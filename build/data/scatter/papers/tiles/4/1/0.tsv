id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
16bd1fbe3694173eda4ad4338a85f8288d19bf02	relational learning of pattern-match rules for information extraction	information extraction;pattern matching;relational learning	Information extraction is a form of shallow text processing which locates a specified set of relevant items in natural language documents. Such systems can be useful, but require domain-specific knowledge and rules, and are time-consuming and difficult to build by hand, making infomation extraction a good testbed for the application of machine learning techniques to natural language processing. This paper presents a system, RAPIER, that takes pairs of documents and filled templates and induces pattern-match rules that directly extract fillers for the slots in the template. The learning algorithm incorporates techniques from several inductive logic programming systems and learns unbounded patterns that include constraints on the words and part-of-speech tags surrounding the filler. Encouraging results are presented on learning to extract information from computer job postings from the newsgroup misc. jobs. offered.	algorithm;inductive logic programming;information extraction;job stream;machine learning;minimal instruction set computer;natural language processing;part-of-speech tagging;star filler;testbed	Mary Elaine Califf;Raymond J. Mooney	1997			statistical relational learning;computer science;artificial intelligence;machine learning;pattern matching;pattern recognition;data mining;information extraction	AI	-26.58005931992679	-69.64673657207543	5079
c1846f2e2a06b1fe387484d87dc1b196dfb5c3c7	neural discrimination of nonprototypical chords in music experts and laymen: an meg study	auditory system;magnetoencephalography;auditory cortex	At the level of the auditory cortex, musicians discriminate pitch changes more accurately than nonmusicians. However, it is not agreed upon how sound familiarity and musical expertise interact in the formation of pitch-change discrimination skills, that is, whether musicians possess musical pitch discrimination abilities that are generally more accurate than in nonmusicians or, alternatively, whether they may be distinguished from nonmusicians particularly with respect to the discrimination of nonprototypical sounds that do not play a reference role in Western tonal music. To resolve this, we used magnetoencephalography (MEG) to measure the change-related magnetic mismatch response (MMNm) in musicians and nonmusicians to two nonprototypical chords, a “dissonant” chord containing a highly unpleasant interval and a “mistuned” chord including a mistuned pitch, and a minor chord, all inserted in a context of major chords. Major and minor are the most frequently used chords in Western tonal music which both musicians and nonmusicians are most familiar with, whereas the other chords are more rarely encountered in tonal music. The MMNm was stronger in musicians than in nonmusicians in response to the dissonant and mistuned chords, whereas no group difference was found in the MMNm strength to minor chords. Correspondingly, the length of musical training correlated with the MMNm strength for the dissonant and mistuned chords only. Our findings provide evidence for superior automatic discrimination of nonprototypical chords in musicians. Most likely, this results from a highly sophisticated auditory system in musicians allowing a more efficient discrimination of chords deviating from the conventional categories of tonal music.	auditory area;cns disorder;categories;clinical act of insertion;magnetoencephalography;pitch (music);pitch discrimination	Elvira Brattico;Karen Johanne Pallesen;Olga Varyagina;Christopher J. Bailey;Irina Anourova;Miika Järvenpää;Tuomas Eerola;Mari Tervaniemi	2009	Journal of Cognitive Neuroscience	10.1162/jocn.2008.21144	psychology;neuroscience;communication;magnetoencephalography	ML	-8.82914987220076	-80.9947028634779	5086
423deaa2d22d2b8bc2b0ea8bc5315d9f65a3f0a5	annotation: textual media for cooperation.		"""! """" # $ %! & '( ' '')# # '( """")$ * + $ #) +'$ !* %)** &)! # , '')# # )' #))* $ , *) """" '#'')# # '( & %)"""" """")! '$ """")! """" )!# '# $ ( # * $)%+"""" '# & %)"""" % '#! * ' .)!/ #+ # )' + )!# 0)! $ * & ! # )' '$ !(+ '( !)+'$ # $)%+"""" '# ! ' $ $' # !# %* . !) ) $ 0 ' # )' )0 '')# # )' """" !( '( """" '# % & , . )0 '')# # )' 1# (( '(2 '$ )% * & , . )0 '')# # )' 1%)"""""""" '# 2! * ) ! '# '( 3 # '( '')# # )' #))* %* 0 $ %%)!$ '( #) # )"""" +# # )' *4 )(' # , $ % )#)""""5 )0 '')# # )' $ 0 ' # )' # # . !) ) '$ %)'%*+$ )' # ' $ #) $ , *) ' '')# # )' #))* """" !( '( # )% * '')# # )' #) ' &* $ %)+! &)+# $)%+"""" '# ' $ #! &+# $ ', !)'"""" '# '$ #) + )!# # &5 . 5 )0 '$ 3 '( 1%)"""" +# # )' * '')# # )'2+! $ (' !)% 0)! # '')# # )' #))* %)' # ' # !# '( 0!)"""" ' '')# # '( """")$ * ' ! $ 0!)"""" +"""" ' # # )! 0# ! ! , . )0 """" ' """")$ * 0 ## '( #) )+! +! ) . %)'%*+$ &5 $ *)! '( # * %/ )0 '# ! %# )' ! ! '# # )' ' %)(' # , """")$ * )0 %)"""" ! ' )' )! # 3# %! # )' '$ # *). 0)%+ )' # '')# # '( !)$+%# )'# ' !) ) """")$ * ' ! $ 0!)"""" # """" $ , * ! #)! % * $ %)+! !)$+%# )'0 ' **5 $ # * # )'() '( """" * """" '# # )' '$ $ %+ ). # 0 # & % 0 #+! )0 %)) ! # , '')# # '("""		Gaëlle Lortal;Myriam Lewkowicz;Amalia Todirascu-Courtier	2005			natural language processing;multimedia;world wide web	ML	-32.78653299016305	-77.15265226222384	5140
febbeacef97e8407d6fb16a4faa2d526da323b13	a domain independent natural language interface to databases capable of processing complex queries	natural language interfaces;base donnee;semantic representation;metadata;query processing;sql;traitement requete;interrogation base donnee;database;interrogacion base datos;base dato;semantics;set theory;semantica;semantique;automatic generation;semantic metadata;natural language;metadonnee;part of speech;natural language interface;tratamiento pregunta;metadatos;query translation;interface langage naturel;database query	We present a method for creating natural language interfaces to databases (NLIDB) that allow for translating natural language queries into SQL. The method is domain independent, i.e., it avoids the tedious process of configuring the NLIDB for a given domain. We automatically generate the domain dictionary for query translation using semantic metadata of the database. Our semantic representation of a query is a graph including information from database metadata. The query is translated taking into account the parts of speech of its words (obtained with some linguistic processing). Specifically, unlike most existing NLIDBs, we take seriously auxiliary words (prepositions and conjunctions) as set theory operators, which allows for processing more complex queries. Experimental results (conducted on two Spanish databases from different domains) show that treatment of auxiliary words improves correctness of translation by 12.1%. With the developed NLIDB 82of queries were correctly translated (and thus answered). Reconfiguring the NLIDB from one domain to the other took only ten minutes.	correctness (computer science);data dictionary;database;information retrieval;natural language user interface;sql;set theory	Rodolfo A. Pazos Rangel;Joaquín Pérez Ortega;Juan Javier González Barbosa;Alexander F. Gelbukh;Grigori Sidorov;M. J. Rodríguez MyriamJ.Rodríguez	2005		10.1007/11579427_85	natural language processing;sql;natural language user interface;part of speech;computer science;database;semantics;natural language;programming language;metadata;set theory	DB	-30.3211743267118	-71.10386777572272	5142
15772874bdfc69d1beef99471a7bd145a5a83af8	human reading knowledge inspired text line extraction	human reading knowledge;text line extraction;directed graph;k-shortest paths optimization	Text in images contains exact semantic information and the text knowledge can be utilized in many image cognition and understanding applications. The human reading habits can provide the clues of text line structure for text line extraction. In this paper, we propose a novel human reading knowledge inspired text line extraction method based on k-shortest paths global optimization. Firstly, the candidate character extraction is reformulated as Maximal Stable Extremal Region (MSER) algorithm on gray, red, blue, and green channels of the target images, and the extracted MSERs are fed into Convolutional Neural Network (CNN) to remove the noise components. Then, the directed graph is built upon the character component nodes with edges inspired by human reading sense. The directed graph can automatically construct the relationship to eliminate the disorder of candidate text components. The text line paths optimization is inspired by the human reading ability in planning of a text line path sequentially. Therefore, the text line extraction problem can be solved using the k-shortest paths optimization algorithm by taking advantage of the human reading sense structure of the directed graph. It can extract the text lines iteratively to avoid the exhaustive searching and obtain global optimized text line number. The proposed method achieves the f-measure of 0.820 and 0.812 on public ICDAR2011 and ICDAR2013 dataset, respectively. The experimental results demonstrate the effectiveness of the proposed human reading knowledge inspired text line extraction method in comparison with state-of-the-art methods This paper presents one human reading knowledge inspired text line extraction method, which approves that the human reading knowledge can benefit the text line extraction and image text discovery.	algorithm;brute-force search;cognition;combinatorial optimization;convolutional neural network;directed graph;f1 score;global optimization;graph (discrete mathematics);line number;loss function;mathematical optimization;maximal set;maximally stable extremal regions;shortest path problem;unary operation	Liuan Wang;Seiichi Uchida;Anna Zhu;Jun Sun	2017	Cognitive Computation	10.1007/s12559-017-9490-4	machine learning;pattern recognition;line number;computer science;artificial intelligence;global optimization;convolutional neural network;directed graph;text graph;communication channel;cognition	AI	-14.029013760899621	-65.4428663337715	5151
6724bb6f8f798d0a8869e23d35d4970508f3f6fb	hardware natural language interface	field programmable gate array;short message service;natural language;natural language interface;hardware implementation;natural language processing;question answering	In this paper an efficient architecture for natural language processing is presented, implemented in hardware using FPGAs (Field Programmable Gate Arrays). The system can receive sentences belonging to a subset of Natural Languages (NL) from the internet or as SMS (Short Message Service). The recognition task of the input string uses Earley’s parallel parsing algorithm and produces intermediate code according to the semantics of the grammar. The intermediate code can be transmitted to a computer, for further processing. The high computational cost of the parsing task in conjunction with a possible large amount of input sentences, to be processed simultaneously, justify the hardware implementation of the grammar (syntax and semantics). An extensive illustrative example is given from the area of question answering, in order to show the feasibility of the proposed system.	algorithm;computational complexity theory;embedded system;field-programmable gate array;glr parser;internet;microprocessor;nl (complexity);natural language processing;natural language user interface;parsing;question answering;speedup	Christos Pavlatos;Alexandros C. Dimopoulos;George K. Papakonstantinou	2007		10.1007/978-0-387-74161-1_33	natural language processing;natural language programming;question answering;natural language user interface;computer science;theoretical computer science;parsing;natural language;programming language;field-programmable gate array;short message service	NLP	-22.996425055112717	-80.17252631225982	5177
61ec60c95b57cbe0365e95e4d30e142796a95a1b	automatic metadata generation using associative networks	particle swarm;information retrieval;digital library;metadata generation;content analysis;associative networks;spreading activation;information service;particle swarms	In spite of its tremendous value, metadata is generally sparse and incomplete, thereby hampering the effectiveness of digital information services. Many of the existing mechanisms for the automated creation of metadata rely primarily on content analysis which can be costly and inefficient. The automatic metadata generation system proposed in this article leverages resource relationships generated from existing metadata as a medium for propagation from metadata-rich to metadata-poor resources. Because of its independence from content analysis, it can be applied to a wide variety of resource media types and is shown to be computationally inexpensive. The proposed method operates through two distinct phases. Occurrence and cooccurrence algorithms first generate an associative network of repository resources leveraging existing repository metadata. Second, using the associative network as a substrate, metadata associated with metadata-rich resources is propagated to metadata-poor resources by means of a discrete-form spreading activation algorithm. This article discusses the general framework for building associative networks, an algorithm for disseminating metadata through such networks, and the results of an experiment and validation of the proposed method using a standard bibliographic dataset.	algorithm;digital data;software propagation;sparse matrix;spreading activation	Marko A. Rodriguez;Johan Bollen;Herbert Van de Sompel	2009	ACM Trans. Inf. Syst.	10.1145/1462198.1462199	digital library;content analysis;computer science;artificial intelligence;machine learning;data mining;database;spreading activation;particle swarm optimization;world wide web;data element;information retrieval;metadata repository	HPC	-26.999735864554747	-58.19569015545882	5190
ef6524dc307b6250a168778e61172afe1c22e81b	cmsof: a structured data organization framework for scanned chinese medicine books in digital libraries	digital library;regional growth;projection method;structured data organization;jie yuan bao gang wei li dong wang wei ming lu yue ting zhuang cmsof a structured data organization framework for scanned chinese medicine books in digital libraries;hybrid method;chinese medicine;image separation;information system;cross media;structured data	Organizing unstructured information from books into a well-defined structure is a significant challenge in digital libraries. Most digital libraries can provide only search services at the granularity of books and few libraries allow books to be accessed at the granularity of chapters, as manually constructing directory information for books is time-consuming. Extracting structured data from scanned books thus remains an urgent and important work. In this paper, we propose a novel structured data organization framework called CMSOF to organize scanned data automatically, and apply it to a Chinese medicine digital library. In the framework, image blocks and text blocks on the scanned page of books are separated based on the gray histogram projection method or a hybrid method of region growth and the Ada-Boosting classifier at first, and then the text structure is obtained from text blocks by text size and font type recognition. Finally, image blocks and structured OCRed text are correlated at the semantic level. By integrating the structured data into a Chinese medicine information system (CMIS), we can organize the Chinese medicine books well and users can access the books with flexibility, which indicates that CMSOF is an efficient framework to organize books mixed with images and text.		Jie Yuan;Baogang Wei;Lidong Wang;Weiming Lu;Yueting Zhuang	2010	Journal of Zhejiang University SCIENCE C	10.1631/jzus.C1001007	digital library;data model;computer science;data science;projection method;information system	Web+IR	-39.50099991186036	-67.346688631127	5211
845fdbc1fa393a0d31dd5bfdea046c3707aa44c5	acoustic modeling using transform-based phone-cluster adaptive training	interpolation;gaussian processes;speaker recognition;hidden markov models vectors adaptation models training context modeling transforms interpolation;phone cluster adaptive training acoustic modeling subspace gaussian mixture models;mixture models;speaker recognition gaussian processes interpolation mixture models;continuous density hidden markov model transform based phone cluster adaptive training acoustic modeling technique context dependent states linear interpolation monophone cluster models linear transformation gaussian mixture model speaker adaptation interpolation vectors	In this paper, we propose a new acoustic modeling technique called the Phone-Cluster Adaptive Training. In this approach, the parameters of context-dependent states are obtained by the linear interpolation of several monophone cluster models, which are themselves obtained by adaptation using linear transformation of a canonical Gaussian Mixture Model (GMM). This approach is inspired from the Cluster Adaptive Training (CAT) for speaker adaptation and the Subspace Gaussian Mixture Model (SGMM). The parameters of the model are updated in an adaptive training framework. The interpolation vectors implicitly capture the phonetic context information. The proposed approach shows substantial improvement over the Continuous Density Hidden Markov Model (CDHMM) and a similar performance to that of the SGMM, while using significantly fewer parameters than both the CDHMM and the SGMM.	acoustic cryptanalysis;acoustic model;context-sensitive language;hidden markov model;linear interpolation;markov chain;subspace gaussian mixture model	Vimal Manohar;Srinivas C. Bhargav;Srinivasan Umesh	2013	2013 IEEE Workshop on Automatic Speech Recognition and Understanding	10.1109/ASRU.2013.6707704	speaker recognition;speech recognition;interpolation;computer science;machine learning;pattern recognition;mixture model;gaussian process;statistics	Vision	-18.064911939734092	-92.00810988382798	5212
67cee3fe1cc08f49eaf340e3195ea2830b71c055	proposed technical architectural framework supporting heterogeneous applications in a hospital	data sharing;shared medical records;hospitals;algeria;hospital information systems;e health;interoperability;mobile communications;heterogeneous applications;his;electronic healthcare	Nowadays, hospital information systems (HISs) play a crucial role in helping medical staffs in accomplishing their daily tasks. In a hospital, actors with different profiles accomplish a variety of activities. In such a context, the processed information is variable in nature and large in quantity. The actors who may be distant, as inside, from the hospital are likely to use heterogeneous application systems. The shared data, which are exchanged in the form of records, require flexibility of access. This paper proposes a new architectural framework for an Algerian hospital system where heterogeneity of applications is involved. The proposed architecture comprises all necessary components for its well functioning.	enterprise architecture framework;heterogeneous computing	Karim Zarour	2016	IJEH	10.1504/IJEH.2016.078740	interoperability;simulation;medicine;telecommunications;computer science;knowledge management;nursing;computer security	Arch	-55.00075498693648	-61.41189629668682	5216
4791cb189d2c07090051f39f2cb63bf4767faa3d	improving a statistical mt system with automatically learned rewrite patterns	linguistic phrase boundary;word order;basic model;broad-coverage rule-based parsers;linguistic phrase;target language;target word;current clump-based statistical mt;bleu measure;source sentence;statistical mt system	• Limitation of current phrase-based SMT • No mechanism for expressing and using linguistic phrases in reordering • Ordering of target words do not respect linguistic phrase boundaries • Xia and McCord’s solution: • Extract linguistic rewrite rules from corpora • Preprocess source sentences so phrase ordering is similar to that of target language • Perform SMT decoding with monotonic ordering constraint	compiler;parse tree;parsing;preprocessor;rewrite (programming);rewriting;text corpus	Fei Xia;Michael C. McCord	2004			natural language processing;computer science;linguistics;algorithm	NLP	-22.33003872910353	-77.2604418043667	5219
adda4894ae606b450fa22dd38e1c1d42fc29df47	a semantic scattering model for the automatic interpretation of english genitives		An important problem in knowledge discovery from text is the automatic extraction of semantic relations. This paper addresses the automatic classification of the semantic relations expressed by English genitives. A learning model is introduced based on the statistical analysis of the distribution of genitives’ semantic relations in a corpus. The semantic and contextual features of the genitive’s noun phrase constituents play a key role in the identification of the semantic relation. The algorithm was trained and tested on a corpus of approximately 20,000 sentences and achieved an f-measure of 79.80 per cent for of-genitives, far better than the 40.60 per cent obtained using a Decision Trees algorithm, the 50.55 per cent obtained using a Naive Bayes algorithm, or the 72.13 per cent obtained using a Support Vector Machines algorithm on the same corpus using the same features. The results were similar for s-genitives: 78.45 per cent using Semantic Scattering, 47.00 per cent using Decision Trees, 43.70 per cent using Naive Bayes, and 70.32 per cent using a Support Vector Machines algorithm. The results demonstrate the importance of word sense disambiguation and semantic generalization/specialization for this task. They also demonstrate that different patterns (in our case the two types of genitive constructions) encode different semantic information and should be treated differently in the sense that different models should be built for different patterns.	algorithm;decision tree;encode;f1 score;naive bayes classifier;ontology components;partial template specialization;support vector machine;text corpus;word sense;word-sense disambiguation	Adriana Badulescu;Dan I. Moldovan	2009	Natural Language Engineering	10.1017/S1351324908004798	natural language processing;semantic similarity;computer science;pattern recognition;data mining	NLP	-25.676310715424396	-70.62162324418397	5256
13b196badbdc55f743df5529d6578164e7561511	unsupervised discovery of phonological categories through supervised learning of morphological rules	symbolic machine;phonological representation;morphological rule;unsupervised discovery;phonological category;morphological task;correct diminutive suffix;dutch noun;supervised rule induction algorithm;phonologically relevant category;linguistic rule;language technology;supervised learning;case study;noun	We describe a case study in tit(', application of symbolic machinc learning techniques for the discow;ry of linguistic rules and categories. A supervised rule induction algorithm is used to learn to predict the. correct dimilmtive suffix given the phonological representation of Dutch nouns. The system produces rules which are comparable, to rules proposed by linguists, l,Slrthermore, in the process of learning this morphological task, the phonemes used are grouped into phonologically relevant categories. We discuss the relevance of our method for linguistics attd language technology. 1 I n t r o d u c t i o n This paper shows how machine lem'ning techniques can be used to induce linguistically relevant rules and categories fl'om data. Statistical, connectionist, and machine learning induction (dataoriented approaches) are currently nsed mainly in language, engineering at)t)lications in order to alleviate the. linguistic knowledge acquisition bottleneck (the fact that lexical an(t grammatical knowledge usually has to be reformulated t'i'()iii scratch whenever a new application has to be built or an existing application ported to a new domain), and to solve problems with robustness and coverage inherent in knowledge-based (the.ory-oriente.d, hand-crafting) approaches. Linguistic relevance. or inspectability of the induced knowledge is usually not an issue in this type of research. ]n linguistics, on the other hand, it is usually agreed that while computer modeling is a useful (or essential) tool for enforcing internal consistency, completeness, and empirical validity of the linguistic theory being modeled, its role in formulating or evaluating linguistic theories is minimal. In this paper, we argue that machine learning techniques can also assist in linguistic theory for*Visiting fl'.llow at NIAS (Netherlands Instituee for Advanced Studies), Wassenaar, The. Netherlands. P e t e r B e r c k a n d S t e v e n G i l l i s L ingu i s t i c s , U n i v e r s i t y of A n t w e r p Unive . r s i t e i t sp le in 1, 2610 W i M j k	algorithm;computer simulation;connectionism;knowledge acquisition;language technology;machine learning;relevance;rule induction;statistical model;supervised learning;theory	Walter Daelemans;Peter Berck;Steven Gillis	1996			natural language processing;unsupervised learning;noun;computer science;machine learning;pattern recognition;linguistics;supervised learning;language technology	NLP	-26.776794154987478	-73.44165952356241	5307
0633a16e100f14928aae0561d1c571882677c527	semantic parsing with neural hybrid trees		We propose a neural graphical model for parsing natural language sentences into their logical representations. The graphical model is based on hybrid tree structures that jointly represent both sentences and semantics. Learning and decoding are done using efficient dynamic programming algorithms. The model is trained under a discriminative setting, which allows us to incorporate a rich set of features. Hybrid tree structures have shown to achieve state-of-the-art results on standard semantic parsing datasets. In this work, we propose a novel model that incorporates a rich, nonlinear featurization by a feedforward neural network. The error signals are computed with respect to the conditional random fields (CRFs) objective using an inside-outside algorithm, which are then backpropagated to the neural network. We demonstrate that by combining the strengths of the exact global inference in the hybrid tree models and the power of neural networks to extract high level features, our model is able to achieve new state-of-the-art results on standard benchmark datasets across different languages.	artificial neural network;benchmark (computing);conditional random field;dynamic programming;feedforward neural network;graphical model;high-level programming language;inside–outside algorithm;natural language;nonlinear system;parsing;tree structure	Raymond Hendy Susanto;Wei Lu	2017			discriminative model;machine learning;computer science;feedforward neural network;parsing;artificial neural network;tree structure;artificial intelligence;conditional random field;inference;pattern recognition;graphical model	AI	-18.131635464691946	-74.3115402539345	5311
d524d7f87c2b7e01d1fb9942216b8e14b4fdc05a	breaking affordance: culture as context	affordance;culture;design context	"""The concept of affordance as it applies to user interface design is widely used and accepted; possibly overused. This paper explores one of the constraints on affordance: culture. Graduate and undergraduate students in the United Kingdom and the United States were surveyed and asked to make judgements about the behaviour of abstracted Western-like objects. The study clearly shows that UK subjects thought the down position of a light switch indicates it is """"ON""""; for their US counterparts it was """"OFF."""" We suggest that context (in the case of this study, culture) is often overlooked, but is central to affordance, to computer interface design, as well as to action and activity more generally."""	user interface design	Lidia Oshlyansky;Harold W. Thimbleby;Paul A. Cairns	2004		10.1145/1028014.1028025	social affordance;human–computer interaction;computer science;artificial intelligence;affordance;culture	HCI	-53.00465103548579	-52.30949357663923	5312
617f6689bbae8a63942686523f238a3cb9ba944e	sentiment analysis for various sns media using naïve bayes classifier and its application to flaming detection	support vector machines;dictionaries companies entropy training data support vector machines media semisupervised learning;companies;media;training data;text analysis bayes methods entropy feature selection learning artificial intelligence natural language processing pattern classification social networking online;comments feature selection sentiment analysis sns media naive bayes classifier model flaming incident detection communication tools backlash company reputation semisupervised learning approach noisy training data twitter facebook blogs japanese textboard 2 channel entropy based criteria tweets feature selection;dictionaries;entropy;semisupervised learning	SNS is one of the most effective communication tools and it has brought about drastic changes in our lives. Recently, however, a phenomenon called flaming or backlash becomes an imminent problem to private companies. A flaming incident is usually triggered by thoughtless comments/actions on SNS, and it sometimes ends up damaging to the company's reputation seriously. In this paper, in order to prevent such unexpected damage to the company's reputation, we propose a new approach to sentiment analysis using a Naïve Bayes classifier, in which the features of tweets/comments are selected based on entropy-based criteria and an empirical rule to capture negative expressions. In addition, we propose a semi-supervised learning approach to relabeling noisy training data, which come from various SNS media such as Twitter, Facebook, blogs and a Japanese textboard called `2-channel'. In the experiments, we use four data sets of users' comments, which were posted to different SNS media of private companies. The experimental results show that the proposed Naïve Bayes classifier model has good performance for different SNS media, and a semi-supervised learning effectively works for the data consisting of long comments. In addition, the proposed method is applied to detect flaming incidents, and we show that it is successfully detected.	blog;experiment;feature selection;flaming (internet);graph labeling;heuristic (computer science);naive bayes classifier;semi-supervised learning;semiconductor industry;sentiment analysis;statistical classification;supervised learning	Shun Yoshida;Jun Kitazono;Seiichi Ozawa;Takahiro Sugawara;Tatsuya Haga;Shogo Nakamura	2014	2014 IEEE Symposium on Computational Intelligence in Big Data (CIBD)	10.1109/CIBD.2014.7011523	computer science;machine learning;pattern recognition;data mining	AI	-20.40084375602305	-56.68809561860595	5320
af2df814f4298675f46d82601ab20c03ccd13889	linking from schema.org microdata to the web of linked data: an empirical assessment	microformats;linked data;schema org;lov;ontologies	Article history: Received 8 June 2015 Received in revised form 31 October 2015 Accepted 17 December 2015 Available online 29 December 2015 The increase of Linked Open Data (LOD) usage has grown in the last few years, and the number of datasets available is considerably higher. Taking this into account, anotherway tomake data available ismicrodata,whose aim is tomake informationmore understandable for search engines to give better results. The Schema.org vocabulary was created for the enrichment ofmicrodata as a way to give more accurate results for user searches. As Schema. org is a kind of ontology, it has the potential to become a bridge to theWeb of Linked Data. In this paper we analyze the potential of mapping Schema.org and theWeb of Linked Data. Concretely, we have obtained mappings between Schema.org terms and the terms provided by the LinkedOpenVocabularies (LOV) collection. In order to measure the limitations of our mappings we have compared the results of our script with some matching tools. Finally, an analysis of the usability of interlinking Schema.org to vocabularies in LOV has been carried out. For this purpose, two studies in whichwe have been presented aggregated information. Results show that new information has been added a substantial number of times. © 2015 Elsevier B.V. All rights reserved.	aggregate data;application programming interface;dbpedia;federated search;gene ontology term enrichment;linked data;microdata (html);microsoft outlook for mac;ontology (information science);ontology alignment;ruby;schema.org;usability;vocabulary;web search engine;webmaster;world wide web	Alberto Nogales;Miguel-Ángel Sicilia;Salvador Sánchez Alonso;Elena García Barriocanal	2016	Computer Standards & Interfaces	10.1016/j.csi.2015.12.003	microdata;computer science;ontology;linked data;data mining;database;world wide web	Web+IR	-29.993283620279925	-61.38546148492229	5331
907efb0fad054951e871d2657f79a35352310e6a	semantic composition of at-location relation with other relations		This paper presents a method for the composition of at-location with other semantic relations. The method is based on inference axioms that combine two semantic relations yielding another relation that otherwise is not expressed. An experimental study conducted on PropBank, WordNet, and eXtended WordNet shows that inferences have high accuracy. The method is applicable to combining other semantic relations and it is beneficial to many semantically intense applications.	extended wordnet;experiment;propbank	Hakki C. Cankaya;Eduardo Blanco;Dan I. Moldovan	2012	Natural Language Engineering	10.1017/S1351324911000222	natural language processing;semantic similarity;semantic computing;computer science;database;information retrieval	AI	-26.057777022171173	-70.5446444962768	5343
5b710b0e6862bcde61a3e69c79440c170a95dbe8	enhanced dialogue markup for crisis talk scenario resources		We present a method of enhancing dialogue markup by mapping HPSG-based discourse category information into XML. The application scenario is crisis talk, specifically cockpit voice recording (CVR) transcripts of aviation disasters. This approach is new both as a source of richly annotated spoken language corpus resources for a little known scenario, and in grammatical theory and language documentation.	documentation;flight recorder;head-driven phrase structure grammar;markup language;real life;text corpus;verification and validation;xml	Claudia Sassen;Dafydd Gibbon	2002			natural language processing;xml;artificial intelligence;cockpit;computer science;language documentation;head-driven phrase structure grammar;markup language;multimedia;spoken language	NLP	-29.386645354540047	-80.96927024144985	5356
02163e9cbebff953680b565e8c248b4ad1c55e82	divrank: the interplay of prestige and diversity in information networks	diversity;text summarization;information network;reinforced random walk;ranking;information networks;ranking algorithm	Information networks are widely used to characterize the relationships between data items such as text documents. Many important retrieval and mining tasks rely on ranking the data items based on their centrality or prestige in the network. Beyond prestige, diversity has been recognized as a crucial objective in ranking, aiming at providing a non-redundant and high coverage piece of information in the top ranked results. Nevertheless, existing network-based ranking approaches either disregard the concern of diversity, or handle it with non-optimized heuristics, usually based on greedy vertex selection.  We propose a novel ranking algorithm, DivRank, based on a reinforced random walk in an information network. This model automatically balances the prestige and the diversity of the top ranked vertices in a principled way. DivRank not only has a clear optimization explanation, but also well connects to classical models in mathematics and network science. We evaluate DivRank using empirical experiments on three different networks as well as a text summarization task. DivRank outperforms existing network-based ranking methods in terms of enhancing diversity in prestige.	automatic summarization;centrality;experiment;greedy algorithm;heuristic (computer science);mathematical optimization;network science;vertex (graph theory)	Qiaozhu Mei;Jian Guo;Dragomir R. Radev	2010		10.1145/1835804.1835931	ranking;ranking;computer science;automatic summarization;machine learning;pattern recognition;data mining;mathematics;statistics	ML	-26.520283140232483	-61.92004388083575	5380
82362bf7825627d4b607ad4f67f6c0fbfe3c3138	the grammar and semantics of disjuncts in world englishes		Adverbs have become the ragbag in grammar in which all uncategorized items are relegated. Over the years, there have been several studies (e.g., Biber et al., 1999; Halliday, 1994; Hasselgard, 2010; Huddleston & Pullum, 2002; Quirk et al., 1985; Sinclair, 1990) that looked into the syntactic and semantic functions of adverbs. This paper focuses on what Quirk et al. (1985) call ‘disjuncts’ (which refer to the overt expression of an author's or speaker's attitudes, feelings, judgments, or commitment concerning the message. There are various terminologies in literature that have emerged: ‘stance adverbs’, ‘conjunctive adjuncts’, ‘evaluative adjuncts’, ‘sentence adverbs’, to name a few. The common denominator of all these adverbs is that, syntactically, they occupy the most peripheral position in the clause and that, semantically, they distinguish how the propositional content of the clause relates to the context. Using 12 matching corpora of the International Corpus of English (ICE), that is, 5 from the Inner Circle (Australia, Canada, Great Britain, Ireland, New Zealand) and 7 from the Outer Circle ( East Africa, Hong Kong, India, Jamaica, Nigeria, the Philippines, and Singapore), the present study aims at presenting the findings on the frequency and distribution of disjuncts across world Englishes. This study supports the disagreement on the labelling of disjuncts as presented in literature in terms of their functions by showing evidence of such claims. Further, it argues that there exist several semantic functions apart from what are presented in literature and that these functions are culture-specific. PACLIC 30 Proceedings 35 30th Pacific Asia Conference on Language, Information and Computation (PACLIC 30) Seoul, Republic of Korea, October 28-30, 2016	existential quantification;information and computation;international corpus of english;peripheral;quirks mode;text corpus;the inner circle;word lists by frequency	Shirley Dita	2016			linguistics;natural language processing;world englishes;semantics;artificial intelligence;computer science;grammar	NLP	-33.90965938942927	-82.76880907784926	5388
cd9b477ba9764f2000753968411ff2732681f8b4	communication of medical images, text, and messages in inter-enterprise systems: a case study in norway	internal medicine;biomedical imaging hospitals medical diagnostic imaging communication networks cardiology information systems picture archiving and communication systems paramagnetic resonance image storage electronics packaging;hospital information system;radiology;physically transporting patients;web based applications;standard compliant web based application;diagnostic imaging;communication system;pacs;user friendly interface;hospital information system his;database management systems decision support systems clinical delivery of health care diagnostic imaging hospital communication systems information dissemination information storage and retrieval internet medical records systems computerized norway radiology information systems remote consultation software teleradiology;rikshospitalet university hospital;cardiology;medical diagnostic image;patient archiving and communication system pacs;picture archiving and communication system;high speed communication network;dicom;inter enterprise system;picture archiving and communication system medical diagnostic image inter enterprise system physically transporting patients high speed communication network pacsflow image data transfer standard compliant web based application user friendly interface digital imaging and communications in medicine dicom rikshospitalet university hospital oslo norway hospital information system radiology information system;oslo;user interfaces diagnostic radiography medical image processing pacs radiology;medical image;norway;community networks;work flow;teleradiology;medical image processing;radiology information system;enterprise system;digital imaging and communications in medicine;work flow hospital information system his patient archiving and communication system pacs teleradiology;user interfaces;high speed;diagnostic radiography;image data transfer;pacsflow	There is an increasing demand to discuss diagnostic images and reports of difficult cases with experienced staff. A possible solution besides physically transporting patients and material is to use high-speed communication networks to transfer images and reports electronically. With the web application PACSflow we have developed a solution to transfer images, reports, and messages as a single package in a one-step procedure. The PACSflow is an interoperable and standard compliant web-based application, which gives clinicians a user-friendly interface for their work on a daily basis. The solution assumes that the diagnostic images are compatible with the digital imaging and communications in medicine (DICOM) format. The Department of Cardiology at the Rikshospitalet University Hospital in Oslo, Norway, and the Department of Internal Medicine at the Soslashrlandet Sykehus in Arendal, Norway, are making clinical use of the system. Initial tests indicate that use of PACSflow has reduced the time required to prepare and transfer data by a factor of 3	cardiology discipline;compliance behavior;dicom;database trigger;digital imaging and communications in medicine (dicom);encryption;enterprise system;generic drugs;information sensitivity;interface device component;interoperability;item unique identification;patients;physics and astronomy classification scheme;picture archiving and communication system;precipitating factors;telecommunications network;usability;virtual private network;web application;world wide web;message	Ilangko Balasingham;H. Ihlen;Wolfgang Leister;P. Roe;Eigil Samset	2007	IEEE Transactions on Information Technology in Biomedicine	10.1109/TITB.2006.879597	medical imaging;simulation;radiology;medicine;computer science;multimedia;dicom;picture archiving and communication system;world wide web	Visualization	-51.687999077447884	-61.67003977650968	5395
f828b401c86e0f8fddd8e77774e332dfd226cb05	lstm recurrent networks learn simple context-free and context-sensitive languages	state space methods;neural networks;context free languages;context free language;bridges;delay effects;context free;regular language;resonance light scattering;learning automata;hidden markov models;recurrent network;context sensitive languages;computational complexity;regular languages;context sensitive language regular languages long short term memory recurrent neural networks context free language;backpropagation algorithms;recurrent neural networks hidden markov models delay effects backpropagation algorithms resonance light scattering learning automata neural networks state space methods bridges computational complexity;context sensitive languages recurrent neural nets learning artificial intelligence context free languages;recurrent neural nets;recurrent neural networks;recurrent neural network;learning artificial intelligence;long short term memory;context sensitive language	Previous work on learning regular languages from exemplary training sequences showed that long short-term memory (LSTM) outperforms traditional recurrent neural networks (RNNs). We demonstrate LSTMs superior performance on context-free language benchmarks for RNNs, and show that it works even better than previous hardwired or highly specialized architectures. To the best of our knowledge, LSTM variants are also the first RNNs to learn a simple context-sensitive language, namely a(n)b(n)c(n).	architecture as topic;artificial neural network;context-free language;context-sensitive language;languages;long short-term memory;neural network simulation;recurrent neural network;regular language	Felix A. Gers;Jürgen Schmidhuber	2001	IEEE transactions on neural networks	10.1109/72.963769	natural language processing;regular language;computer science;artificial intelligence;recurrent neural network;machine learning;context-free language;artificial neural network;hidden markov model	Vision	-14.923987957540854	-76.50492565472312	5405
7c0d1b1aa2e8de812db53db1df5e93061dd43a52	error analysis of the tempeval temporal relation identification task	error analysis	The task to classify a temporal relation between temporal entities has proven to be difficult with unsatisfactory results of previous research. In TempEval07 that was a first attempt to standardize the task, six teams competed with each other for three simple relationidentification tasks and their results were comparably poor. In this paper we provide an analysis of the TempEval07 competition results, identifying aspects of the tasks which presented the systems with particular challenges and those that were accomplished with relative ease.	entity	Chong Min Lee;Graham Katz	2009		10.3115/1621969.1621993	computer science;data mining;communication;social psychology	ML	-12.778364152176252	-76.10549699459247	5406
d77974f671aec112b5d361c1602a1221631f254e	generating a lexicon of errors in portuguese to support an error identification system for spanish native learners		Portuguese is a less resourced language in what concerns foreign language learning. Aiming to inform a module of a system designed to support scientific written production of Spanish native speakers learning Portuguese, we developed an approach to automatically generate a lexicon of wrong words, reproducing language transfer errors made by such foreign learners. Each item of the artificially generated lexicon contains, besides the wrong word, the respective Spanish and Portuguese correct words. The wrong word is used to identify the interlanguage error and the correct Spanish and Portuguese forms are used to generate the suggestions. Keeping control of the correct word forms, we can provide correction or, at least, useful suggestions for the learners. We propose to combine two automatic procedures to obtain the error correction: i) a similarity measure and ii) a translation algorithm based on aligned parallel corpus. The similarity-based method achieved a precision of 52%, whereas the alignment-based method achieved a precision of 90%. In this paper we focus only on interlanguage errors involving suffixes that have different forms in both languages. The approach, however, is very promising to tackle other types of errors, such as gender errors.	algorithm;error detection and correction;lexicon;parallel text;similarity measure	Lianet Sepúlveda Torres;Magali Sanches Duran;Sandra M. Aluísio	2014			natural language processing;speech recognition;computer science;linguistics	NLP	-27.02926125324663	-82.8346854228838	5407
7792958a373d2024d22334842fc252649487c948	lexer: lexicon based emotion analyzer		The huge population of India poses a challenge to government, security and law enforcement. What if we could know beforehand the consequences of any events. Social spaces, such as Twitter, Facebook, and Personal blogs, enable people to show their thoughts regarding public issues and topics. Public emotion regarding future and past events, like public gatherings, governmental policies, shows public beliefs and can be deployed to analyze the measure of support, disorder, or disrupted in such situations. Therefore, emotion analysis of Internet content may be beneficial for various organizations, particularly in government, law enforcement, and security sectors. This paper presents an extension to state-of-art-model for lexicon-based sentiment analysis algorithm for analysis of human emotions.	emotion markup language;lexical analysis;lexicon	Shikhar Sharma;Piyush Kumar;Krishan Kumar	2017		10.1007/978-3-319-69900-4_47	artificial intelligence;internet privacy;machine learning;computer science;sentiment analysis;linguistics;law enforcement;the internet;population;lexicon;government	NLP	-21.79883694567549	-55.406009716016996	5427
0998d47421d1b42556854eaede95cc34c20952e4	definition and recovery of kinematic features for recognition of american sign language movements	human computer interaction;american sign language;apparent motion;motion estimation;linguistic analysis;american sign language asl;human computer interaction hci;gesture recognition;linguistics	An approach to recognizing human hand gestures from a monocular temporal sequence of images is presented. Of concern is the representation and recognition of hand movements that are used in singlehanded American sign language (ASL). The approach exploits previous linguistic analysis of manual languages that decompose dynamic gestures into their static and dynamic components. The first level of decomposition is in terms of three sets of primitives, hand shape, location and movement. Further levels of decomposition involve the lexical and sentence levels and are beyond the scope of the present paper. We propose and subsequently demonstrate that given a monocular gesture sequence, kinematic features can be recovered from the apparent motion that provide distinctive signatures for 14 primitive movements of ASL. The approach has been implemented in software and evaluated on a database of 592 gesture sequences with an overall recognition rate of 86% for fully automated processing and 97% for manually initialized processing. ! 2008 Elsevier B.V. All rights reserved.	algorithm;antivirus software;electronic signature;gesture recognition;human computer;image plane;machine learning;motion field;testbed;type signature;vocabulary	Konstantinos G. Derpanis;Richard P. Wildes;John K. Tsotsos	2008	Image Vision Comput.	10.1016/j.imavis.2008.04.007	natural language processing;computer vision;speech recognition;computer science;motion estimation;gesture recognition	AI	-15.42118769203905	-81.54168212252686	5437
6f77431ded454c04fd2188f9fd53efe8bc32d546	a segment-level confidence measure for spoken document retrieval	transcription indexability;broadcast news;prediction method;search engine;measurement;query processing;search engines;confidence measure;acoustics;automatic transcripts relevance;confidence measures;semantics;transcription indexability segment level confidence measure spoken document retrieval semantic confidence measure automatic transcripts relevance sdr automatic speech recognition confidence measure semantic compacity index french broadcast news corpus ester text queries search engine semantic level information;text analysis;speech;spoken document retrieval speech recognition confidence measures;spoken document retrieval;automatic speech recognition;semantic level information;semantic confidence measure;indexing;indexation;automatic speech recognition confidence measure;segment level confidence measure;sdr;speech recognition;semantic compacity index;speech recognition semantics speech measurement indexing acoustics;french broadcast news corpus ester;text queries;text analysis indexing query processing search engines speech recognition	This paper presents a semantic confidence measure that aims to predict the relevance of automatic transcripts for a task of Spoken Document Retrieval (SDR). The proposed predicting method relies on the combination of Automatic Speech Recognition (ASR) confidence measure and a Semantic Compacity Index (SCI), that estimates the relevance of the words considering the semantic context in which they occurred. Experiments are conducted on the French Broadcast news corpus ESTER, by simulating a classical SDR usage scenario: users submit text-queries to a search engine that is expected to return the most relevant documents regarding the query. Results demonstrate the interest of using semantic level information to predict the transcription indexability.	document retrieval;etsi satellite digital radio;relevance;simulation;speech recognition;transcription (software);web search engine	Grégory Senay;Georges Linarès;Benjamin Lecouteux	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5947616	natural language processing;speech recognition;computer science;semantics;information retrieval;search engine	Vision	-22.808260862055384	-82.9291212085732	5441
22d4d10cc3c709f4e7de2245c74c92e1bbb75af3	building a database of related concepts of mandarin metaphors based on wordnet and sumo	databases;organisms;relational concept database;concept;ontologies relational databases data mining chaos management information systems computational linguistics helium concrete search engines dictionaries;electronic mail;relational concept database mandarin metaphors wordnet sumo ontological concepts english conceptual relatedness semantic networking metaphor concept pairings;finance;probability density function;sumo;semantic networking;database;semantic networks;data mining;metaphor concept pairings;conceptual relatedness;mandarin metaphors;wordnet;english;semantic networks natural language processing relational databases;relational databases;magnetic cores;metaphor;chinese;database metaphor concept english chinese sumo wordnet;natural language processing;ontological concepts	The ontological concepts for 1,256 Mandarin metaphor types (such as cheng2zhang3 ‘grow/growth’ and jian4she4 ‘construct/construction’) were stored in a database in the form of metaphor-concept pairings. In each pair, a Mandarin metaphor type is paired with ontological concepts in English. Based on the relations given in these pairings, a database of relational concept was created. Calculation of co-appearance of concepts can be undertaken using the database. The work will contribute to analysis of conceptual relatedness, semantic networking and relatedness of Mandarin metaphors.	chinese room;database;suggested upper merged ontology;super robot monkey team hyperforce go!;wordnet	Siaw-Fong Chung;Chun-Hung Chen;F. Y. August Chao	2009	2009 IEEE International Conference on Semantic Computing	10.1109/ICSC.2009.71	natural language processing;wordnet;probability density function;computer science;english;data mining;database;linguistics;concept;chinese;suggested upper merged ontology	DB	-38.380722797286765	-69.92306906874543	5446
5c26c43e3ba05b52af13037835ab54e252e88b57	erratum to: a sequential gm-based phd filter for a linear gaussian system		The online version of this research paper unfortunately contained two mistakes. Author names were missing partially or completely from the authorship list and citation, respectively. These mistakes were accidentally caused by the incompatibility of the typesetting software. The printed version is correct as the above.	printing;software incompatibility	Zongxiang Liu;Weixin Xie;Pin Wang;You Yu	2014	Science China Information Sciences	10.1007/s11432-014-5059-y	speech recognition;control theory	ML	-40.32017092193057	-64.99160937582414	5492
15eaab573e7f3a4875e9633c5f29325f90c77ffd	mapping arabic wordnet synsets to wikipedia articles using monolingual and bilingual features		The alignment of WordNet and Wikipedia has received wide attention from researchers of computational linguistics, who are building a new lexical knowledge source or enriching the semantic information of WordNet entities. The main challenge of this alignment is how to handle the synonymy and ambiguity issues in the contents of two units from different sources. Therefore, this paper introduces mapping method that links an Arabic WordNet synset to its corresponding article in Wikipedia. This method uses monolingual and bilingual features to overcome the lack of semantic information in Arabic WordNet. For evaluating this method, an Arabic mapping data set, which contains 1,291 synset–article pairs, is compiled. The experimental analysis shows that the proposed method achieves promising results and outperforms the state-of-the-art methods that depend only on monolingual features. The mapped method has also been used to increase the coverage of Arabic WordNet by inserting new synsets from Wikipedia.	compiler;computational linguistics;entity;synonym ring;wikipedia;wordnet	Abdulgabbar Saif;Mohd Juzaiddin Ab Aziz;Nazlia Omar	2017	Natural Language Engineering	10.1017/S1351324915000376	natural language processing;wordnet;speech recognition;linguistics	NLP	-28.404309050872325	-70.09301472090701	5505
37f81d06b8b2ba09b91f7c2444d87c449f615926	the research and application of the chinese sports information extraction	manuals;sohu com;games abstracts manuals semantics;automatic entity relation extraction;information extraction;data collection;semantics;ie;conditional random fields information extraction named entity recognition entity relation extraction;conditional random fields;unified conditional random fields;sina com;named entity recognition;chinese sports information extraction;re;data structures;abstracts;feature extraction;games;f measure;computational linguistics;sport;entity relation extraction;f measure chinese sports information extraction unified conditional random fields ie named entity recognition ner automatic entity relation extraction re sports game scene data collection sohu com sina com;ner;natural language processing;sports game scene;sport computational linguistics data structures feature extraction natural language processing	By combining the model framework and rules of the unified conditional random fields, a new approach is proposed in this paper. We investigate the two key technologies of Information Extraction (IE), known as Named Entity Recognition (NER) and automatic entity Relation Extraction (RE), and explore several new features of our model on the “sports game” scene. The proposed method is implemented on some data collected from sohu.com and sina.com. Experimental results show that by combining the above new elements, our approach brings some improvements to IE, where the recall, precision and the F-measure are 95.70%, 93.00% and 94.33% respectively.	conditional random field;f1 score;information extraction;named entity;named-entity recognition;relationship extraction	Suxiang Zhang;Luo-Ming Meng;Guoyang Gao;Ying Qin	2012	2012 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2012.6358888	natural language processing;feature extraction;computer science;sport;machine learning;pattern recognition;data mining;information extraction;data collection	NLP	-22.696380177515245	-67.76077110113607	5522
d3837adb8a32e8cfe715cec7eb2d8e368e792521	a corpus-based analysis of mixed code in hong kong speech	code mixing;cantonese;text analysis;speech coding;speech registers taxonomy speech coding tv pragmatics educational institutions;corpus linguistics mixed code hong kong speech corpus based analysis cantonese television program transcription english words identification cantonese utterances code switching speech data;corpus linguistics code mixing code switching cantonese english;code switching;corpus linguistics;english;natural language processing;text analysis linguistics natural language processing speech coding;linguistics	We present a corpus-based analysis of the use of mixed code in Hong Kong speech. From transcriptions of Cantonese television programs, we identify English words embedded within Cantonese utterances, and investigate the motivations for such code-switching. Among the many motivations observed in previous research, we found that four alone account for more than 95% of the use of English words in our speech data across genres, genders, and age groups. We performed analyses over more than 60 hours of transcribed speech, resulting in one of the largest empirical studies to-date on this linguistic phenomenon.	corpus linguistics;embedded system;occam's razor;text corpus	John Lee	2012	2012 International Conference on Asian Language Processing	10.1109/IALP.2012.10	natural language processing;text mining;speech recognition;speech corpus;computer science;english;speech coding;corpus linguistics;text corpus;linguistics	SE	-16.96372081393774	-81.52384032861302	5525
89a279e7d100a830a6cd13a0c62c7426abd64315	trameur: a framework for annotated text corpora exploration		Corpus resources with complex linguistic annotations are becoming increasingly important in the work of language specialists. They often need to perform extensive corpus research, including Natural Language Processing (NLP), statistical modelling and data visualisation. Our software system, called Trameur, aims at making these analyses possible within a single graphical user interface. It relies upon a specific data modelling framework presented in this paper.	corpus linguistics;data modeling;data visualization;graphical user interface;natural language processing;software system;statistical model;text corpus	Serge Fleury;Maria Zimina	2014			natural language processing;computer science;data mining;information retrieval	NLP	-33.15738721746329	-71.97620462030204	5527
583aef90d21360c2a406af2f2323b7cfa86be532	recognizing mentions of adverse drug reaction in social media using knowledge-infused recurrent models.		Recognizing mentions of Adverse Drug Reactions (ADR) in social media is challenging: ADR mentions are contextdependent and include long, varied and unconventional descriptions as compared to more formal medical symptom terminology. We use the CADEC corpus to train a recurrent neural network (RNN) transducer, integrated with knowledge graph embeddings of DBpedia, and show the resulting model to be highly accurate (93.4 F1). Furthermore, even when lacking high quality expert annotations, we show that by employing an active learning technique and using purpose built annotation tools, we can train the RNN to perform well (83.9 F1).	active learning (machine learning);artificial neural network;computer multitasking;dbpedia;display resolution;entity;graph embedding;knowledge graph;long short-term memory;multi-task learning;random neural network;recurrent neural network;social media;test set;transducer	Gabriel Stanovsky;Daniel Gruhl;Pablo N. Mendes	2017		10.18653/v1/e17-1014	computer science;natural language processing;adverse drug reaction;artificial intelligence;social media	NLP	-19.29683403543586	-71.00797536110733	5532
1d7e8bd9c0d3421d10b6a4fbe23e9ade1170a0fc	demonstrations and live evaluation for the gesture recognition challenge	unlabeled data;unsupervised learning;software;human computer interaction;sign language;vocabulary;gesture recognition vocabulary conferences computer vision handicapped aids videos software;data representation;computer vision;handicapped aids;image representation;multimedia databases;transfer learning;vocabulary gesture recognition human computer interaction image representation multimedia databases unsupervised learning visual databases;gesture recognition;conferences;videos;visual databases;hci workshop live evaluation gesture recognition transfer learning unsupervised learning learning data representation video database gesture lexicon gesture vocabulary live competition	We are organizing a competition on gesture recognition. This challenge is part of a series of challenges on the theme of unsupervised and transfer learning. The goal is to push the state of the art in algorithms capable of learning data representations, which may be re-used from task to task, using unlabeled data and/or labeled data from similar domains. In this challenge, the competitors will obtain for training a large database of videos of gestures from various gesture lexicons (religious emblems, sports referee signals, marshalling signals to guide vehicles or machineries, diving signals to communicate under water, signs from sign languages for the deaf, and signs accompanying narratives of hearing people, etc.). They will then be tested on gestures from different domains and different gesture vocabularies, unknown in advance. The final test will be carried out in a live competition in which the systems will be demonstrated at the site of a conference during the summer 2012. We are holding a milestone event at the HCI workshop held in conjunction with ICCV 2011, where the organizers will demonstrate baseline systems and explain the competition protocol to encourage participation. Up to five competitors will demonstrate systems under development.	algorithm;baseline (configuration management);database;gesture recognition;human–computer interaction;iccv;lexicon;marshalling (computer science);organizing (structure);unsupervised learning;vocabulary	Isabelle Guyon;Vassilis Athitsos	2011	2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)	10.1109/ICCVW.2011.6130277	natural language processing;unsupervised learning;computer vision;speech recognition;sign language;transfer of learning;computer science;machine learning;gesture recognition;external data representation	Vision	-13.99787765751047	-71.15949340875486	5553
1caf81f864ed9d0c5f9c9b78c0b7b80b227f174a	male and female voice source characteristics: inverse filtering results	analyse parole;acoustic spectrum;sex;formant;analisis palabra;speech analysis;lenguaje;hombre;sexe;langage;spectre acoustique;espectro acustico;voice;voz;glotis;human;audition;audicion;language;formante;sexo;glotte;hearing;glottis;homme;voix	Detailed data on voicing source characteristics are of interest in both analysis and synthesis of speech. In this study, a time-domain based method of inverse filtering was used to analyze male and female utterances produced with several voice qualities. Fourteen mono-syllabic utterances (ten in normal voicing style, and 2 each in creaky and breathy voice) by each of 8 speakers (4 men, 4 women) were filtered by a set of zero pairs corresponding to the measured formants. The resulting differentiated flow waveforms were analyzed in the time domain and in the frequency domain at voicing onset, near the middle of the vowel, and near the end of voicing. Though there was variability among subjects of the same sex, the women tended to have shorter closed quotients and longer return quotients (the period between maximal rate of change of flow and minimal flow). In the spectral domain, there tended to be less energy at higher frequencies in the middle of the vowel for the women compared to the men. The magnitudes of the male-female differences are similar to those observed for the creaky-normal voicing differences and breathy-normal differences. These differences may arise from a combination of biological, sociological and acoustical effects.	minimum phase	P. J. Price	1989	Speech Communication	10.1016/0167-6393(89)90005-8	speech recognition;formant;computer science;sex;linguistics;language;voice	HPC	-10.13610738837978	-83.18113327379172	5560
5baa63f5adb11e51f9321cd84954dcc83af1bc28	investigating context features hidden in end-to-end tts		Recent studies have introduced end-to-end TTS, which integrates the production of context and acoustic features in statistical parametric speech synthesis. As a result, a single neural network replaced laborious feature engineering with automated feature learning. However, little is known about what types of context information end-to-end TTS extracts from text input before synthesizing speech, and the previous knowledge about context features is barely utilized. In this work, we first point out the model similarity between end-to-end TTS and parametric TTS. Based on the similarity, we evaluate the quality of encoder outputs from an end-to-end TTS system against eight criteria that are derived from a standard set of context information used in parametric TTS. We conduct experiments using an evaluation procedure that has been newly developed in the machine learning literature for quantitative analysis of neural representations, while adapting it to the TTS domain. Experimental results show that the encoder outputs reflect both linguistic and phonetic contexts, such as vowel reduction at phoneme level, lexical stress at syllable level, and part-of-speech at word level, possibly due to the joint optimization of context and acoustic features.	acoustic cryptanalysis;artificial neural network;encoder;end-to-end principle;experiment;feature engineering;feature learning;machine learning;mathematical optimization;netware file system;part-of-speech tagging;speech synthesis;syllable	Kohki Mametani;Tsuneo Kato;Seiichi Yamamoto	2018	CoRR		speech recognition;encoder;vowel reduction;artificial neural network;syllable;feature engineering;feature learning;parametric statistics;artificial intelligence;pattern recognition;speech synthesis;computer science	NLP	-18.02580373359716	-87.15073248975342	5572
69a37741ec706cd4e8f3fa7fb4affb407eb123c6	piction: a system that uses captions to label human faces in newspaper photographs	rule based system;constraint satisfaction;computer vision;natural language parsing;face recognition;face modeling	It is often the case that linguistic and pictorial information are jointly provided to communicate information. In situations where the text describes salient aspects of the picture, it is possible to use the text to direct the interpretation (i.e., labelling objects) in the accompanying picture. This paper focuses on the implementation of a multi-stage system PICTION that uses captions to identify humans in an accompanying photograph. This provides a computationally less expensive alternative to traditional methods of face recognition. It does not require a pre-stored database of face models for all people to be identified. A key component of the system is the utilisation of spatial constraints (derived from the caption)in order to reduce the number of possible labels that could be associated with face candidates (generated by a face locator). A rule-based system is used to further reduce this number and arrive at a unique labelling. The rules employ spatial heuristics as well as distinguishing characteristics of faces (e.g., male versus female). The system is noteworthy since a broad range of AI techniques are brought to bear (ranging from natural-language parsing to constraint satisfaction and computer vision).		Rohini K. Srihari	1991			rule-based system;facial recognition system;computer vision;face detection;speech recognition;constraint satisfaction;computer science;artificial intelligence;machine learning	AI	-8.25765515095473	-72.54453690500848	5605
e7c364f28b94bd23103e4f74f62e2ec98d16fadf	isolated word recognition using models for acoustic phonetic variability by lombard effect	word recognition		acoustic cryptanalysis;heart rate variability	Tadashi Suzuki;Kunio Nakajima;Yoshiharu Abe	1994			speech recognition;artificial intelligence;word recognition;pattern recognition;lombard effect;computer science	NLP	-13.926172077349161	-86.83566548498322	5616
5a18520fc0133a67d38bbef00cb3bf0f0065cfc0	plataforma de generación semiautomática de sistemas de diálogo multimodales y multilingües : proyecto gemini	communication homme machine;traitement automatique des langues naturelles;human computer communication;computacion informatica;automatic dialogue systems;generacion automatica;reconocimiento del habla;filologias;linguistique appliquee;dialogue;automatic generation;info eu repo semantics article;informacion documentacion;linguistica;multimodality;generation automatique;ciencias basicas y experimentales;multilinguality;reconnaissance de la parole;multilingualidad;xml;speech recognition;multimodalidad;sistemas automaticos de dialogo;computational linguistics;grupo a;multilinguisme;ciencias sociales;linguistique informatique;grupo b;natural language processing;multilingualism;applied linguistics	We present in this paper a complete platform for the semiautomatic generation of human-machine dialog systems, that using as input status a description of the database of the service, a flow model with the different states of the final application and a guided interaction step by step with the designer’s intervention, generates dialogs to access the service data in different languages and two modalities, speech and web, simultaneously. We describe every module of the platform that makes this possible, emphasizing the different strategies followed to reduce the time needed to do the design. Finally, and as a demonstration of the platform efficiency and independency, we present the applications that have been developed: a banking application and a citizen care application.	database;dialog manager;dialog system;fourth-generation programming language;harris affine region detector;ieee intelligent systems;interactivity;linear algebra;modality (human–computer interaction);naruto shippuden: clash of ninja revolution 3;semiconductor industry;systems architecture;unique name assumption;vocabulary;voicexml;world wide web;xhtml;xml	Luis Fernando D'Haro;Ricardo de Córdoba;Ignacio Ibarz;Rubén San-Segundo-Hernández;Juan Manuel Montero-Martínez;Javier Macías Guarasa;Javier Ferreiros;José Manuel Pardo	2004	Procesamiento del Lenguaje Natural		natural language processing;xml;computer science;computational linguistics;applied linguistics;linguistics	ML	-28.909404391220914	-81.91971987790552	5637
41326f8a19b7a20983d4aa092fb0318dc683404d	impacts on trust of healthcare ai		Artificial Intelligence and robotics are rapidly moving into healthcare, playing key roles in specific medical functions, including diagnosis and clinical treatment. Much of the focus in the technology development has been on human-machine interactions, leading to a host of related technology-centric questions. In this paper, we focus instead on the impact of these technologies on human-human interactions and relationships within the healthcare domain. In particular, we argue that trust plays a central role for relationships in the healthcare domain, and the introduction of healthcare AI can potentially have significant impacts on those relations of trust. We contend that healthcare AI systems ought to be treated as assistive technologies that go beyond the usual functions of medical devices. As a result, we need to rethink regulation of healthcare AI systems to ensure they advance relevant values. We propose three distinct guidelines that can be universalized across federal regulatory boards to ensure that patient-doctor trust is not detrimentally affected by the deployment and widespread adoption of healthcare AI technologies.		Emily LaRosa;David Danks	2018		10.1145/3278721.3278771	software deployment;machine learning;artificial intelligence;knowledge management;health care;computer science;robotics	AI	-59.25891622868983	-57.92543660848846	5645
78e938619aa0de08987263514a920272473d8e2e	pronunciation of proper names with a joint n-gram model for bi-directional grapheme-to-phoneme conversion	proper names	Pronunciation of proper names is known to be a difficult problem, but one of great practical importance for both speech synthesis and speech recognition. Recently a few data-driven grapheme-to-phoneme conversion techniques have been proposed to tackle this problem. In this paper we apply the joint n-gram model for bi-directional grapheme-to-phoneme conversion, which has already been shown to achieve excellent results on general tasks, to the more specific task of converting between name pronunciations and spellings. The performance of our technique on generating name pronunciations exceeds that of other techniques even when they use additional information. We find the reverse task, of generating orthographic transcriptions from phonemic input, to be a much more difficult task for names than for common words. However, we derive valuable information from our results about the potential of sub-lexical recognition of novel proper names.	n-gram;orthographic projection;speech recognition;speech synthesis	Lucian Galescu;James F. Allen	2002			artificial intelligence;n-gram;speech recognition;pattern recognition;proper noun;computer science;grapheme;pronunciation	NLP	-21.416465414077628	-80.54715818715653	5667
f71be20db5be6e27f0811c09e541779fcddf87c8	an experimental study of boosting model classifiers for chinese text categorization	busqueda informacion;modelizacion;analisis contenido;linguistique;modele agrege;information retrieval;modelo agregado;aprendizaje probabilidades;chino;modelisation;content analysis;biblioteca electronica;linguistica;recherche information;aggregate model;apprentissage probabilites;electronic library;analyse contenu;chinois;chinese;modeling;text categorization;bibliotheque electronique;probability learning;linguistics	Text categorization is a crucial task of increasing importance. Our work focuses on the study of Chinese text categorization on the basis of Boosting model. We chose the People's Daily news from TREC5 as our benchmark datasets. A minor modification to AdaBoost algorithm (Freund and Schapire, 1996, 2000) was applied for this hypothesis. By way of using the F1 measure for its final evaluation, the results of the Boosting model (AdaBoost.MH) is proved to be effective and outperforms most of other algorithms reported for Chinese text categorization.	categorization;document classification	Yibing Geng;Guomin Zhu;Junrui Qiu;Jilian Fan;Jingchang Zhang	2004		10.1007/978-3-540-30544-6_29	natural language processing;brownboost;speech recognition;content analysis;boosting methods for object categorization;artificial intelligence;linguistics;chinese	NLP	-21.56591972787876	-64.29582746238302	5704
64b93fda17d10c445585182e4424a804d0d18bb6	mongolian inflection suffix processing in nlp: a case study	inflection suffix;mongolian;nlp	Inflection suffix is an important morphological characteristic of Mongolian words, since the suffixes express abundant syntactic and semantic meanings. In order to provide an informative introduction of it, this paper implements a case study on it. Through three Mongolian NLP tasks, we disclose the following information: (1) views of inflection suffix in NLP tasks, (2) Inflection suffix processing ways, (3) Inflection suffix effects on system performance and (4) some suffix related conclusion.	information retrieval;natural language processing;parsing;preprocessor	Xiangdong Su;Guanglai Gao;Yupeng Jiang;Jing Wu;Feilong Bao	2015		10.1007/978-3-319-25207-0_29	arithmetic;natural language processing;linguistics	NLP	-29.01733919910876	-74.27605062737797	5706
7cbeb09fcb0c035c72f54b887a768c6d10d3fe3b	iterative training techniques for phonetic template based speech recognition with a speaker-independent phonetic recognizer	phoneme;iterative method;tabla codificacion;modelo markov oculto;evaluation performance;phonetique;performance evaluation;modelo markov;modele markov cache;stochastic method;hidden markov model;evaluacion prestacion;telephone;fonema;intelligence artificielle;metodo iterativo;speaker recognition;markov model;reconocimiento voz;speaker independent;codebook;methode iterative;table codage;reconnaissance locuteur;fonetica;speaker dependent;error rate;speech recognition;methode stochastique;artificial intelligence;modele donnee;phonetics;inteligencia artificial;reconnaissance parole;modele markov;speaker;locutor;telefono;speaker adaptation;locuteur;matching method;data models;metodo estocastico	This paper presents a new method that improves the performance of the speaker specific phonetic template based speech recognizer with the speaker-independent (SI) phoneme HMMs. Since the phonetic template based speech recognizer uses only the phoneme transcription of the input utterance, the performance of the system is worse than that of the speaker dependent system due to the mismatch between the training data and the SI models. In order to solve these problems, a new training method that iteratively estimates the phonetic templates and transformation vectors for the adaptation of the SI phoneme HMMs is presented. The phonetic class based and codebook-based stochastic matching methods are used to estimate the transformation vectors for speaker adaptation. Performance evaluation using the speaker dependent recognition experiments performed over actual telephone line showed a reduction of about 40% in the error rates when compare to the conventional speaker specific phonetic template based speech recognizer.	finite-state machine;speech recognition	Weon-Goo Kim;MinSeok Jang;Chin-Hui Lee	2005		10.1007/11589990_60	loudspeaker;phonetics;speaker recognition;data modeling;speaker diarisation;speech recognition;word error rate;computer science;codebook;pattern recognition;iterative method;markov model;hidden markov model	NLP	-20.077909993007797	-91.40994650407787	5718
d7b9df0669fd43ff45e7939f3e04b61bad465bb9	advances in visual informatics		Family tree is one of the most common ways to trace the genealogy of a certain person. Family trees contain a lot of potential information to be explored especially for research purposes. However, many family trees fail to properly encode all necessary and useful information. Genogram therefore seems to be the most suitable visual representation of medical family tree data as it contains complex information that can be clearly presented in a diagram using genogram symbols and color-coded lines. Some limitations are problems in visu‐ alizing the wealth and complexity of the information represented once a family tree gets bigger. Hence, a new framework for exploring medical family tree data is proposed in this paper. By using genogram as a tool and a few selected visu‐ alization techniques as an enhancement in designing these new framework, which will allows users to maximize usage of data by exploring the data from several different viewpoints. This framework follows the design of advanced graphical user interface guide which is the Visual Information-Seeking Mantra “Overview first, Zoom and Filter, then Details-on Demand”, proposed by Shneiderman in 1996. Using this framework (visualization tool), it is also possible to predict health risk factors based on medical family tree data. This visualization tool can be utilized for personal use or by healthcare professionals.	diagram;encode;family tree;genogram;graphical user interface;informatics;risk factor (computing)	Halimah Badioze Zaman;Peter Robinson;Alan F. Smeaton;Timothy K. Shih;Sergio A. Velastin;A. Jaafar;Nazlena Mohamad Ali	2015		10.1007/978-3-319-25939-0	library science;computer science;data science;engineering physics	HCI	-57.72735255629349	-58.10785884096236	5721
ae9dcaa0fb2b2f495fdd7b7e9533822368b77a56	electronic whiteboards: review of the literature		Electronic whiteboards are being introduced into hospitals to communicate real-time patient information instantly to staff. This paper provides a preliminary review of the current state of evidence for the effect of electronic whiteboards on care processes and patient outcomes. A literature search was performed for the dates 1996 to 2014 on MEDLINE, EMBASE, IEEE Xplore, Science Direct, and the ACM Digital Library. Thirteen papers, describing 11 studies, meeting the inclusion criteria were identified. The majority of studies took place in the Emergency Department. While studies looked at the impact of electronic whiteboards on the process of care, there is an absence of evidence concerning impact on patient outcomes. There is a need for robust research measuring the impact of electronic whiteboards on inpatient care.	aclarubicin;digital library;embase;hospitalization;ieee xplore;interactive whiteboard;medline;occur (action);paper;patients;real-time transcription;inpatient	Rebecca Randell;Joanne Greenhalgh;Jeremy Wyatt;Peter Gardner;Alan Pearman;Stephanie Honey;Dawn Dowding	2015	Studies in health technology and informatics	10.3233/978-1-61499-512-8-389	emergency medicine;knowledge management;inpatient care;emergency department;digital library;multimedia;medline;medicine	HCI	-59.43035395713048	-65.0095635119138	5724
1f4fbf5f123155a1a9b77c2fc41d3e49f67f5770	biological storytelling: a software tool for biological information organization based upon narrative structure	computer supported cooperative work;annotation;information visualization;bioinformatics	"""The main task of molecular biologists seeking to understand the molecular basis of disease is identifying and interpreting the relationships of genes, proteins, and pathways in living organisms. While emerging technologies have provided powerful  analysis  tools to this end, they have also produced an explosion of data, which biologists need to make sense of. We have built software tools to support the  synthesis  activities of molecular biologists, in particular the activities of organizing, retrieving, using, sharing, and reusing diverse biological information. A key aspect of our approach, based upon the findings of user studies, is the use of  narrative structure  as a conceptual framework for developing and representing the """"story"""" of how genes, proteins, and other molecules interact in biological processes.  Biological stories  are represented both textually and graphically within a simple conceptual model of  items, collections , and  stories ."""		Allan Kuchinsky;Kathy Graham;David Moh;Annette Adler;Ketan Babaria;Michael L. Creech	2002		10.1145/1556262.1556315	information visualization;human–computer interaction;computer science;bioinformatics;data science;computer-supported cooperative work;data mining;world wide web	SE	-5.228119635313209	-62.450651719889294	5731
8c7c36502792544e27c5e200472e791b66a1c9c7	speech synthesis method based on application-specific synthetic units and its implementation on a 32-bit microprocessor	text to speech synthesis;speech synthesis;reduced instruction set computing;reduced instruction set computing speech synthesis microprocessor chips;speech synthesis vocabulary microprocessors cache memory information technology research and development large scale integration electronic equipment software quality natural languages;speech synthesis microprocessors vocabulary speech analysis costs information technology research and development large scale integration databases electronic equipment;32 bit speech synthesis method application specific synthesis units microprocessor text to speech synthesis phoneme strings vocabulary speech database memory performance experimental results risc processor;application specific integrated circuits;digital signal processing chips speech synthesis application specific integrated circuits microprocessor chips;text to speech;32 bit speech synthesis method application specific synthetic units 32 bit microprocessor speech synthesis algorithm embedded mcu electronic equipment high quality synthetic speech;digital signal processing chips;microprocessor chips	A novel speech synthesis algorithm with application-specific synthesis units has been implemented in an embedded MCU. This method approximately halves the required amount of memory, making high-quality synthetic speech available in many kinds of electronic equipment.	32-bit;algorithm;dhrystone;embedded system;microprocessor;speech synthesis;synthetic intelligence	Yasushi Ishikawa;Yasuhisa Kisuki;Tadashi Sakamoto;Tomohiro Hase	1999	1999 Digest of Technical Papers. International Conference on Consumer Electronics (Cat. No.99CH36277)	10.1109/30.793661	voice activity detection;natural language processing;reduced instruction set computing;speech recognition;computer hardware;computer science;operating system;speech processing;application-specific integrated circuit;speech synthesis	EDA	-23.987011492149644	-90.23173101296096	5754
0e49333e4bf224947c390194d2a2fb0919a4085a	capturing provenance, evolution and modification of clinical protocols via a heterogeneous, semantic social network		Healthcare delivery is largely based on medical best practices as in clinical protocols. Research so far has addressed the computerized execution of clinical protocols by developing a number of related representation languages, execution engines and integrated platforms to support real time execution. However, much less effort has been put into organizing clinical protocols for use and reuse. In this paper we propose a heterogeneous semantic social network to describe and organize clinical protocols based on their provenance, evolution and modifications. The proposed approach allows semantic tagging and enrichment of clinical protocols so that they can be used and re-used across platforms and also be linked directly to other relevant scientific information, e.g. published works in PubMed or personal health records, and other clinical information systems.	best practice;biological evolution;crew resource management, healthcare;gene ontology term enrichment;genetic heterogeneity;heterogeneous computing;information systems;information system;organizing (structure);programming languages;protocols documentation;pubmed;reuse (action);scientific publication;social network;health records	Nick Portokallidis;George Drosatos;Eleni Kaldoudi	2016	Studies in health technology and informatics	10.3233/978-1-61499-658-3-592	world wide web;semantic social network;reuse;best practice;information system;social media;health care;semantics;information dissemination;computer science	Web+IR	-51.945831924351666	-64.32623902867125	5755
79c29bb6fbb9e622e9a8101a508829c837b2ccd4	automated voice dictation in french	phoneme;vocabulaire;machine ecrire;maquina escribir;reconocimiento palabra;acoustics;speech processing;vocabulary;tratamiento palabra;traitement parole;fonema;phonem;vocabulario;voice;voz;senal vocal;signal vocal;automatic speech processing;speech recognition;voice activated typewriter;reconnaissance parole;acoustic models;typewriter;vocal signal;voice dictation;acoustique;acustica;language models;voix	Abstract   Designing a Voice-Activated Typewriter in French necessitates a study both on how to design the acoustic level recognition, and on how to obtain a model of the French language. Such a project was initiated at LIMSI 15 years ago. This paper presents the different steps that have been completed since the beginning of this project. First, a study on the phoneme-to-grapheme conversion, for continuous, error-free phonemic strings, using a large vocabulary and a natural language syntax was completed in 1979. The corresponding results were then improved, with attempts to convert phoneme strings containing (simulated) errors, while the methodology was adapted to the case of stenotype-to-grapheme conversion. In the ESPRIT project 860 “Linguistic Analysis of the European Languages”, our approach for language modeling was compared with other approaches on 7 different European languages. The link between the acoustic recognition and the language model resulted in a complete system (“Hamlet”), for a limited vocabulary (2,000 words), pronounced in isolation, which was then extended to a vocabulary of 5,000 words, taking advantage of a specialized DTW chip (MuPCD), also designed at LIMSI. This study resulted in the conclusion that dictation in an isolated mode was not acceptable. A speaker-independent continuous speech recognition system is now developed for vocabularies of 5 to 20 KWords.		Joseph-Jean Mariani	1993	Speech Communication	10.1016/0167-6393(93)90069-W	speech recognition;computer science;speech processing;linguistics;voice	NLP	-22.909215734478977	-84.46504558731932	5757
15a92cf48c3721e1e9f5504185cffb7b7df8b971	semeval-2017 task 11: end-user development using natural language		This task proposes a challenge to support the interaction between users and applications, micro-services and software APIs using natural language. It aims to support the evaluation and evolution of the discussions surrounding the application natural language processing techniques within the context of end-user natural language programming, under scenarios of high lexical and semantic heterogeneity.	application programming interface;end-user development;microservices;natural language processing;natural language programming;natural language user interface;open web;precision and recall;semeval;semantic heterogeneity;semantic interpretation;spatial variability;vocabulary	Juliano Efson Sales;Siegfried Handschuh;André Freitas	2017		10.18653/v1/S17-2092	natural language processing;end-user development;task analysis;artificial intelligence;language identification;universal networking language;natural language;computer science;semeval	NLP	-32.29254281458631	-76.83121146185677	5831
94dc02825b80910ffbb1a029e85f8231f3845e21	learning distributed representations of data in community question answering for question retrieval	word vector representation;question retrieval;unsupervised model;community question answering	We study the problem of question retrieval in community question answering (CQA). The biggest challenge within this task is lexical gaps between questions since similar questions are usually expressed with different but semantically related words. To bridge the gaps, state-of-the-art methods incorporate extra information such as word-to-word translation and categories of questions into the traditional language models. We find that the existing language model based methods can be interpreted using a new framework, that is they represent words and question categories in a vector space and calculate question-question similarities with a linear combination of dot products of the vectors. The problem is that these methods are either heuristic on data representation or difficult to scale up. We propose a principled and efficient approach to learning representations of data in CQA. In our method, we simultaneously learn vectors of words and vectors of question categories by optimizing an objective function naturally derived from the framework. In question retrieval, we incorporate learnt representations into traditional language models in an effective and efficient way. We conduct experiments on large scale data from Yahoo! Answers and Baidu Knows, and compared our method with state-of-the-art methods on two public data sets. Experimental results show that our method can significantly improve on baseline methods for retrieval relevance. On 1 million training data, our method takes less than 50 minutes to learn a model on a single multicore machine, while the translation based language model needs more than 2 days to learn a translation table on the same machine.	baseline (configuration management);data (computing);experiment;heuristic;information privacy;language model;loss function;multi-core processor;optimization problem;question answering;relevance	Kai Zhang;Wei Chung Wu;Fang Wang;Ming Zhou;Zhoujun Li	2016		10.1145/2835776.2835786	natural language processing;question answering;computer science;machine learning;data mining;information retrieval	NLP	-17.374143071067262	-67.16248875041434	5840
81dfa45c568d7c1d9771ba2a1f07dad96558cff6	a sequential pattern classifier based on hidden markov kernel machine and its application to phoneme classification	application development;sequential pattern classifier;sufficient nonlinear classification performance;sequential pattern classifiers discriminative training hidden markov models hmms kernel methods;kernel;gaussian mixture;probability;maximum mutual information;maximum likelihood;gaussian processes;hidden markov model;probability density function;training;pdfs;kernel methods;hmm;gmm;hidden markov kernel machine;maximum mutual information procedures;maximum likelihood estimation;sequential pattern classification;gaussian mixture model;kernel machine;hidden markov models;vectors;mixture model;viterbi algorithm;gaussian mixture models;signal classification;hidden markov models kernel training viterbi algorithm vectors speech recognition pattern classification;hidden markov models hmms;pattern classification;speech recognition;kernel method;discriminative training;isolated phoneme classification;sequential pattern;emission probability density functions;speech recognition gaussian processes hidden markov models maximum likelihood estimation pattern classification probability signal classification;maximum mutual information procedures sequential pattern classifier hidden markov kernel machine kernel methods emission probability density functions pdfs hmm sufficient nonlinear classification performance gaussian mixture models gmm sequential pattern classification speech recognition isolated phoneme classification maximum likelihood;sequential pattern classifiers	This paper describes a novel classifier for sequential data based on nonlinear classification derived from kernel methods. In the proposed method, kernel methods are used for enhancing the emission probability density functions (pdfs) of hidden Markov models (HMMs). Because the emission pdfs enhanced by kernel methods have sufficient nonlinear classification performance, mixture models such as Gaussian mixture models (GMMs), which might cause problems of overfitting and local optima, are not necessary in the proposed method. Unlike the methods used in earlier studies on sequential pattern classification using kernel methods, our method can be regarded as an extension of conventional HMMs, and therefore, it can completely model the transition of hidden states with the observed vectors. Therefore, our method can be applied to many applications developed with conventional HMMs, especially for speech recognition. In this paper, we carried out an isolated phoneme classification as a preliminary experiment in order to evaluate the efficiency of the proposed sequential pattern classifier. We confirmed that the proposed method achieved steady improvements as compared to conventional HMMs with Gaussian-mixture emission pdfs trained by the maximum likelihood and the maximum mutual information procedures.	hidden markov model;kernel method;local optimum;markov chain;mixture model;mutual information;nonlinear system;overfitting;speech recognition	Yotaro Kubo;Shinji Watanabe;Atsushi Nakamura;Erik McDermott;Tetsunori Kobayashi	2010	IEEE Journal of Selected Topics in Signal Processing	10.1109/JSTSP.2010.2076030	kernel method;speech recognition;computer science;machine learning;pattern recognition;mixture model;mathematics;hidden markov model;statistics	ML	-19.368715873011936	-92.65386219090271	5843
2f20f682431d71143f17fad34e29f01a03555f34	learning to express left-right & front-behind in a sign versus spoken language		"""Developmental studies show that it takes longer for children learning spoken languages to acquire viewpointdependent spatial relations (e.g., """"left-right"""", """"frontbehind""""), compared to the ones that do not (e.g., """"in"""", """"on"""", """"under""""). The current study investigates how children learn to express viewpoint-dependent relations in a sign language where depicted spatial relations can be communicated in an analogue manner in the space in front of the body or by using body-anchored signs (e.g., tapping the right and left hand/arm to mean LEFT and RIGHT). Our results indicate that visual-spatial modality might have a facilitating effect on learning to express these spatial relations (especially in encoding of """"leftright"""") in a sign language (i.e., Turkish Sign Language) compared to a spoken language (i.e., Turkish)."""	modality (human–computer interaction)	Beyza Sümer;Pamela Perniss;Inge Zwitserlood;Asli Özyürek	2014				NLP	-7.557261972143771	-78.85260223320104	5857
cf6a1c7ca76958d870a24ceec53120c54e40d309	accelerating gene regulatory network modeling using grid-based simulation	simulation ordinateur;distributed system;systeme reparti;biological model;complexite calcul;biologia matematica;simulation;gene regulatory networks;distributed computing;bioinformatique;simulacion;modelo biologico;grid based simulation;biologie mathematique;complejidad computacion;sistema repartido;mathematical biology;modele biologique;computational complexity;performance model;calculo repartido;simulacion computadora;bioinformatica;computational biology;gene regulatory network;computer simulation;calcul reparti;bioinformatics	Modeling gene regulatory networks has, in some cases, enabled biologists to predict cellular behavior long before such behavior can be experimentally validated. Unfortunately, the extent to which biologists can take advantage of these modeling techniques is limited by the computational complexity of gene regulatory network simulation algorithms. This study presents a new platform-independent, grid-based distributed computing environment that accelerates biological model simulation and, ultimately , development. Applying this environment to gene regulatory network simulation shows a significant reduction in execution time versus running simulation jobs locally. To analyze this improvement , a performance model of the distributed computing environment is built. Although this grid-based system was specifically developed for biological simulation, the techniques discussed are applicable to a variety of simulation performance problems.	algorithm;computational complexity theory;distributed computing environment;experiment;gene regulatory network;job stream;run time (program lifecycle phase);simulation	James M. McCollum;Gregory D. Peterson;Chris D. Cox;Michael L. Simpson	2004	Simulation	10.1177/0037549704045051	computer simulation;gene regulatory network;simulation;computer science;bioinformatics;artificial intelligence;simulation modeling	HPC	-4.639401413083207	-53.888671334858564	5876
c4e69a89a9e322ca78ed3f1140e74aee5ab844f1	computational methods for integrating vision and language	affective visual attributes;correspondence ambiguity;visual question answering;image captioning;auto annotation;cross modal disambiguation;region labeling;auto illustration;loosely labeled data;video captioning;aligning visual and linguistic data;language;multimodal translation;vision	Modeling data from visual and linguistic modalities together creates opportunities for better understanding of both, and supports many useful applications. Examples of dual visual-linguistic data includes images with keywords, video with narrative, and figures in documents. We consider two key task-driven themes: translating from one modality to another (e.g., inferring annotations for images) and understanding the data using all modalities, where one modality can help disambiguate information in another. e multiple modalities can either be essentially semantically redundant (e.g., keywords provided by a person looking at the image), or largely complementary (e.g., meta data such as the camera used). Redundancy and complementarity are two endpoints of a scale, and we observe that good performance on translation requires some redundancy, and that joint inference is most useful where some information is complementary. Computational methods discussed are broadly organized into ones for simple keywords, ones going beyond keywords toward natural language, and ones considering sequential aspects of natural language. Methods for keywords are further organized based on localization of semantics, going fromwords about the scene taken as whole, to words that apply to specific parts of the scene, to relationships between parts. Methods going beyond keywords are organized by the linguistic roles that are learned, exploited, or generated. ese include proper nouns, adjectives, spatial and comparative prepositions, and verbs. More recent developments in dealing with sequential structure include automated captioning of scenes and video, alignment of video and text, and automated answering of questions about scenes depicted in images.		Kobus Barnard	2016		10.2200/S00705ED1V01Y201602COV007	natural language processing;vision;computer vision;speech recognition;computer science;machine learning;linguistics;language;information retrieval	ML	-15.133766435742919	-69.07049659990703	5884
837e7b77066f803a63f882f6af36066b93912891	a naive theory of affixation and an algorithm for extraction	random segment;novel approach;random character;example case study;frequent affix;affix extraction algorithm;typologically distant language;salient prefix;naive theory;variable length sequence;salient affix	We present a novel approach to the unsupervised detection of affixes, that is, to extract a set of salient prefixes and suffixes from an unlabeled corpus of a language. The underlying theory makes no assumptions on whether the language uses a lot of morphology or not, whether it is prefixing or suffixing, or whether affixes are long or short. It does however make the assumption that 1. salient affixes have to be frequent, i.e occur much more often that random segments of the same length, and that 2. words essentially are variable length sequences of random characters, e.g a character should not occur in far too many words than random without a reason, such as being part of a very frequent affix. The affix extraction algorithm uses only information from fluctation of frequencies, runs in linear time, and is free from thresholds and untransparent iterations. We demonstrate the usefulness of the approach with example case studies on typologically distant languages.	affix grammar;algorithm;galaxy morphological classification;iteration;time complexity	Harald Hammarström	2006			natural language processing;machine learning;pattern recognition;algorithm	NLP	-24.622023682504228	-77.65292487595717	5887
0b180b4f78cf9a2041bd913dc53ca46bce5b4293	alleviating search uncertainty through concept associations: automatic indexing, co-occurrence analysis, and parallel computing	information extraction;parallel computer;reference materials;information retrieval	In this article, we report research on an algorithmic apgather, process, and retrieve information. These systems proach to alleviating search uncertainty in a large inforprovide a wide variety of information and services, rangmation space. Grounded on object filtering, automatic ing from daily updates of foreign and national news, indexing, and co-occurrence analysis, we performed a movie reviews and clips, law cases, and financial data large-scale experiment using a parallel supercomputer on companies to journal articles, books, trademarks, and (SGI Power Challenge) to analyze 400,000/ abstracts in an INSPEC computer engineering collection. Two sysstatistics. However, gaining access to such information is tem-generated thesauri, one based on a combined oboften difficult. This is due, in large part, to the indeterminject filtering and automatic indexing method, and the ism involved in the process by which information is inother based on automatic indexing only, were compared dexed, and to the latitude searchers have in expressing a with the human-generated INSPEC subject thesaurus. query. Our user evaluation revealed that the system-generated thesauri were better than the INSPEC thesaurus in concept recall, but in concept precision the 3 thesauri were 2. Using Thesauri to Alleviate Search comparable. Our analysis also revealed that the terms suggested by the 3 thesauri were complementary and Uncertainty: Literature Review could be used to significantly increase ‘‘variety’’ in search terms and thereby reduce search uncertainty. 2.1. Indexing and Search Uncertainty	automatic summarization;book;computer engineering;parallel computing;supercomputer;thesaurus	Hsinchun Chen;Joanne Martinez;Amy Kirchhoff;Tobun Dorbin Ng;Bruce R. Schatz	1998	JASIS	10.1002/(SICI)1097-4571(199803)49:3%3C206::AID-ASI3%3E3.0.CO;2-K	supercomputer;computer science;artificial intelligence;data mining;database;accuracy and precision;world wide web;information extraction;information retrieval;statistics	Web+IR	-32.60096959425657	-62.406230032949495	5907
d4fcffda52cdb38c426ff980dde258d4110802df	inesc-id@assin: medição de similaridade semântica e reconhecimento de inferência textual	computacion informatica;filologias;linguistica;ciencias basicas y experimentales	"""EnglishIn this article we present INESC-ID@ASSIN, a system that competed in the 2016 joint evaluation effort entitled Avaliacao de Similaridade Semântica e Inferencia Textual (ASSIN), in the tasks of semantic similarity and textual entailment recognition. INESC-ID@ASSIN addresses the problem of detecting sentence similarity as a regression task, and it addresses textual entailment as a classification task. Although INESC-ID@ASSIN relies mainly on simple lexical features for detecting paraphrases and recognizing textual entailment, promising results were achieved in this joint evaluation. portuguesNeste artigo apresentamos o sistema INESC-ID@ASSIN, o qual competiu no evento """"Avaliacao de Similaridade Semântica e Inferencia Textual"""" (ASSIN) de 2016, nas tarefas de similaridade semântica e reconhecimento de parafrases (i.e., inferencia textual). O sistema INESC-ID@ASSIN aborda o problema de medir a similaridade entre frases como uma tarefa de regressao e aborda a inferencia textual como uma tarefa de classificacao. Embora o INESC-ID@ASSIN seja baseado essencialmente em caracteristicas lexicais simples para detecao de parafrases e reconhecimento de inferencia textual, foram obtidos resultados promissores nesta avaliacao conjunta."""		Pedro Fialho;Ricardo Marques;Bruno Martins;Luísa Coheur;Paulo Quaresma	2016	Linguamática		art;artificial intelligence;algorithm;cartography	Crypto	-26.818773263024276	-77.32663354098176	5910
4f9882eb4e529204449de22dc55fdbc3a7f8f5f3	statistical analysis of the acoustic and prosodic characteristics of different speaking styles.	statistical analysis			Masanobu Abe;Hirokazu Sato	1993			computer science	NLP	-14.254887299041037	-85.50265677352752	5912
b331347e8539de12d946890dafa26ea3350814af	correcting and standardizing crude drug names in traditional medicine formulae by ensemble of string matching techniques		Common problems of representing crude drug names in traditional herbal formulae are spelling errors, grammatical variants, synonyms and various formats. In order to make these names more obvious and useful, correcting and standardizing of these names should be applied. In this work, crude drug names in various forms were corrected and standardized by string matching techniques. A set of experiments were done using crude drug names from a database of registered traditional medicines in Thai Food and Drug Administration as the test set. Two well-known algorithms, i.e., similar text and Levenshtein were investigated. However, the results from each algorithm indicated that crude drug names in the test set were moderately matched with those of the standard set. To increase performance of these single algorithms, the ensemble algorithm was proposed. From the results, the ensemble algorithm outperforms single algo- rithms to match crude drug names, especially crude drug names with the modifier that have no significant meaning.		Duangkamol Pakdeesattayapong;Verayuth Lertnattee	2015		10.1007/978-3-319-22186-1_24	arithmetic;computer science;data mining;algorithm	Theory	-33.983533940506874	-70.88063834421042	5919
dd97b243fc2048dd4ec220d49503ddfaa8908fb5	text-based speaker identification on multiparty dialogues using multi-document convolutional neural networks		We propose a convolutional neural network model for text-based speaker identification on multiparty dialogues extracted from the TV show, Friends. While most previous works on this task rely heavily on acoustic features, our approach attempts to identify speakers in dialogues using their speech patterns as captured by transcriptions to the TV show. It has been shown that different individual speakers exhibit distinct idiolectal styles. Several convolutional neural network models are developed to discriminate between differing speech patterns. Our results confirm the promise of text-based approaches, with the best performing model showing an accuracy improvement of over 6% upon the baseline CNN model.	acoustic cryptanalysis;artificial neural network;baseline (configuration management);convolutional neural network;network model;neural networks;speaker recognition;text-based (computing)	Kaixin Ma;Catherine Xiao;Jinho D. Choi	2017		10.18653/v1/P17-3009	convolutional neural network;artificial intelligence;machine learning;computer science;natural language processing;speech recognition	NLP	-16.756707868893923	-87.48256131502878	5927
1027908b61e603487ef62ebe66d4b6e7733fdc74	speech enhancement based on joint time-frequency segmentation	background noise;non transient coefficient;speech signal;speech intelligibility;band pass filters;speech processing;human listeners;time frequency;speech;testing;psychology;wavelet transforms signal representation speech enhancement speech intelligibility time frequency analysis;psychoacoustic tests;speech enhancement;speech modification;wavelet packet;noise measurement;transient analysis;wavelet transforms;time frequency representation;wavelet packet transform;signal representation;noise cancellation;joint time frequency segmentation;background noise speech enhancement joint time frequency segmentation wavelet packet coefficients speech signal time frequency representation non transient coefficient psychoacoustic tests human listeners speech modification speech intelligibility;wavelet packets;signal to noise ratio;signal processing algorithms;speech enhancement time frequency analysis background noise noise cancellation signal processing algorithms band pass filters wavelet packets speech processing psychology testing;time frequency analysis;wavelet packet transform speech enhancement transient component speech intelligibility;wavelet packet coefficients;transient component	We present an algorithm to decompose speech into transient and non-transient components. Our algorithm, the joint timefrequency segmentation algorithm, uses the wavelet packet coefficients of the speech signal and represents them as tiles of a time-frequency representation adapted to the characteristics of the signal itself. Any wavelet packet coefficient, whose tiling height is larger than or equal to the tiling width is characterized as a transient coefficient and vice versa for the nontransient coefficient. The transient component is selectively amplified and recombined with the original speech to generate the modified speech with energy adjusted to be equal to the energy of the original speech. The psychoacoustic tests performed with fourteen human listeners show that the speech modification significantly improves speech intelligibility in background noise, i.e., for 10% absolute at 0dB to 31% absolute at −30dB.	algorithm;coefficient;crossover (genetic algorithm);intelligibility (philosophy);network packet;psychoacoustics;speech enhancement;tiling window manager;time–frequency representation;wavelet	Charturong Tantibundhit;Franz Pernkopf;Gernot Kubin	2009	2009 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2009.4960673	speech recognition;time–frequency analysis;computer science;speech processing;wavelet packet decomposition	Robotics	-8.993691176458224	-87.24584990790102	5942
1758769bd1c30e487daf587cd5be77b483730159	bridging sentential and discourse-level semantics through clausal adjuncts		It is in PropBank’s ARGM annotation of clausal adjuncts that sentential semantics meets discourse relation annotation in the Penn Discourse TreeBank. This paper discusses complementarities between the two annotation systems: How PropBank ARGM annotation can be used to seed annotation of additional discourse relations in the PDTB, and how PDTB annotation can be used to refine or enrich PropBank ARGM annotation.	bridging (networking);complementarity theory;daisy digital talking book;discourse relation;propbank;treebank	Rashmi Prasad;Bonnie L. Webber;Alan Lee;Sameer Pradhan;Aravind K. Joshi	2015		10.18653/v1/W15-2707	natural language processing;computer science;linguistics;programming language	NLP	-30.300826092974045	-77.06037654153577	5953
e3e550c1863e32382b53ce5b4c228a913f6dad50	development and evaluation of a corrective feedback system using augmented reality for the high-quality cardiopulmonary resuscitation training		In an effort to improve lay rescuers' knowledge and skills in cardiopulmonary resuscitation (CPR), we developed a system that enables high-quality CPR by improving the perceptions of and skills in CPR. The proposed system can evaluate user CPR actions in real time. Particular characteristics of this system are the use of distance and pressure sensors for laypeople to achieve the necessary posture during CPR chest compression and the provision of audiovisual corrective feedback using augmented reality (AR). This system is anticipated to contribute to high-quality CPR training.	augmented reality;poor posture;sensor	Emi Higashi;Keisuke Fukagawa;Rinka Kashimura;Yuima Kanamori;Akinori Minazuki;Hidehiko Hayashi	2017	2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2017.8122692	cardiopulmonary resuscitation;simulation;machine learning;augmented reality;artificial intelligence;corrective feedback;computer science	Robotics	-55.88609253829683	-55.34165267365145	6068
0c8b5d65ef59f34a1617c6cd69c64bc780994d08	"""""""a term is known by the company it keeps"""": on selecting a good expansion set in pseudo-relevance feedback"""	pseudo relevance feedback;information retrieval;expansion terms;term document matrix;relevance feedback;selection criteria;test collection	It is well known that pseudo-relevance feedback (PRF) improves the retrieval performance of Information Retrieval (IR) systems in general. However, a recent study by Cao et al [3] has shown that a non-negligible fraction of expansion terms used by PRF algorithms are harmful to the retrieval. In other words, a PRF algorithm would be better off if it were to use only a subset of the feedback terms. The challenge then is to find a good expansion set from the set of all candidate expansion terms. A natural approach to solve the problem is to make term independence assumption and use one or more term selection criteria or a statistical classifier to identify good expansion terms independent of each other. In this work, we challenge this approach and show empirically that a feedback term is neither good nor bad in itself in general; the behavior of a term depends very much on other expansion terms. Our finding implies that a good expansion set can not be found by making term independence assumption in general. As a principled solution to the problem, we propose spectral partitioning of expansion terms using a specific term-term interaction matrix. We demonstrate on several test collections that expansion terms can be partitioned into two sets and the best of the two sets gives substantial improvements in retrieval performance over model-based feedback.	approximation algorithm;binary space partitioning;expansion pack;information retrieval;interaction;language model;primitive recursive function;relevance feedback;singular value decomposition;statistical classification	Raghavendra Udupa;Abhijit Bhole;Pushpak Bhattacharyya	2009		10.1007/978-3-642-04417-5_10	data mining;mathematics;term discrimination;information retrieval;algorithm	Web+IR	-18.925682228546517	-63.72528752553184	6092
381ec0b65760b8cc8e1924c1713ee55c6196df96	adapting mumble: experience with natural language generation	natural language generation;text generation	This paper describes the construction of a MUMBLE-based [McDonald 83b] tactical component for the TEXT text generation system [McKeown 85]. This new component, which produces fluent English sentences from the sequence of structured message units output from TEXT's strategic component, has produced a 60-fold speed-up in sentence production. Adapting MUMBLE required work on each of the three parts of the MUMBLE framework: the interpreter, the grammar, and the dictionary. It also provided some insight into the organization of the generation process and the consequences of MUMBLE's commitment to a deterministic model. T r a c k : Engineer ing Tol~ic: Na tu ra l Language Generat ion	dictionary;natural language generation;nick mckeown;numerical aperture	Robert Rubinoff	1986		10.3115/1077146.1077171	natural language processing;speech recognition;computer science;linguistics	AI	-29.711408082720908	-81.60988525605129	6101
b76f39b6106ab7601806b227979fd551c4b90edb	robust and scalable differentiable neural computer for question answering		Deep learning models are often not easily adaptable to new tasks and require task-specific adjustments. The differentiable neural computer (DNC), a memoryaugmented neural network, is designed as a general problem solver which can be used in a wide range of tasks. But in reality, it is hard to apply this model to new tasks. We analyze the DNC and identify possible improvements within the application of question answering. This motivates a more robust and scalable DNC (rsDNC). The objective precondition is to keep the general character of this model intact while making its application more reliable and speeding up its required training time. The rsDNC is distinguished by a more robust training, a slim memory unit and a bidirectional architecture. We not only achieve new state-of-the-art performance on the bAbI task, but also minimize the performance variance between different initializations. Furthermore, we demonstrate the simplified applicability of the rsDNC to new tasks with passable results on the CNN RC task without adaptions.	accessibility;artificial neural network;deep learning;differentiable neural computer;direct numerical control;dropout (neural networks);general problem solver;natural language processing;precondition;question answering;scalability	Jörg Franke;Jan Niehues;Alexander H. Waibel	2018			computer science;deep learning;precondition;machine learning;scalability;architecture;artificial neural network;differentiable function;solver;question answering;artificial intelligence	ML	-14.44746369016189	-74.71657982734466	6119
0822a7a3d2b8435fa9e447030d8d725f0e91ab6c	statistical vs. connectionist models of bebop improvisation			connectionism	Dominik Hörnel;Joachim Langnickel;Börje Sieling;Bastian Sandberger	1999			natural language processing;speech recognition;artificial intelligence	NLP	-14.529749646557462	-85.35102547975298	6120
838b524b3fdbe68cd0023d08fe020e740a06fbc7	a coutinuous speech recognition system using a modified lvq2 method and a dependency grammar with semantic constraints	dependency grammar;linguistic processing;dictation system;speech recognition;phoneme recognition;neural network	This paper describes an overview of a continuous speech recognition system composed of an acoustic processor and a linguistic processor. The system deals with 843 conceptual words and 431 functional words. We have constructed an acoustic processor using a modified learning vector quantization method (MLVQ2) for phoneme recognition. The phoneme recognition score was 85.5% for 226 sentences uttered by two male speakers. The linguistic processor is composed of a processor for spotting bunsetsu units (i.e. units similar to a “phrase” in English) and a syntactic processor. The structure of the bunsetsu unit is effectively described by a finite-state automaton, the test-set word-perplexity of which is 230. In the processor for spotting bunsetsu units, using a syntax-driven continuous-DP matching algorithm, the bunsetsu units are spotted from a recognized phoneme sequence and then a bunsetsu unit lattice is generated. In the syntactic processor, the bunsetsu unit lattice is parsed based on the dependency grammar, which is expressed as the correspondence between a FEATURE marker in a modifier-bunsetsu and a SLOT-FILLER marker in a head-bunsetsu. The recognition scores of the bunsetsu units and conceptual words were 75.2% and 88.9% respectively for 226 sentences uttered by the two male speakers.	dependency grammar;speech recognition	Shozo Makino;Akinori Ito;Mitsuru Endo;Ken'iti Kido	1994	IJPRAI	10.1142/S0218001494000097	natural language processing;speech recognition;deep linguistic processing;computer science;artificial neural network;dependency grammar	NLP	-18.302004554791797	-83.9232006710351	6123
0e0ab6ef1494c8ca5fba298847b77a484279c63a	tactical generation in a free constituent order language	constituent order;tactical generation;machine translation;free constituent order language;sentence obey;information structure;carnegie mellon university;genkit system;generation component;default order	This paper describes tactical generation in Turkish, a free con~stituent order language, in which the order of the constituents may change according to the information structure of the sentences to be generated. In the absence of any information regarding the information structure of a sentence (i.e., topic, focus, background , etc.), the constituents of the sentence obey a default order, but the order is almost freely changeable, depending on the constraints of the text flow or discourse. We have used a recursively structured finite state machine for handling the changes in constituent order , implemented as a right-linear grammar backbone. Our implementation environment is the GenKit system, developed at Carnegie Mellon University-Center for Machine Translation. Morphological realization has been implemented using an external morpholggical analy-sis/generation component which performs concrete morpheme selection and handles mor-phographemic processes.	finite-state machine;internet backbone;linear grammar;machine translation;naruto shippuden: clash of ninja revolution 3;recursion	Dilek Zeynep Hakkani;Kemal Oflazer;Ilyas Cicekli	1996	Natural Language Engineering		natural language processing;speech recognition;computer science;linguistics	NLP	-30.29218791334712	-82.04900713349402	6168
60d057172ef1ba9cc79dc8608351cfa2902757be	folk attributions of control and intentionality over mental states		Influential theories in social psychology, philosophy, and linguistics assume that ordinary people judge many mental states as outside voluntary control, yet few studies have directly investigated these claims. We report four studies suggesting that, contrary to several prominent models, ordinary people attribute at least moderate intentional control to others over a wide variety of mental states. Furthermore, it appears that perceived control may vary systematically according to mental state type (e.g. emotions vs. desires vs. beliefs). These results point to several important directions for future research in behavior explanation and moral judgment.	intentionality;mental state;theory	Corey J. Cusimano;Geoffrey P. Goodwin	2017			cognitive psychology;psychology;social psychology;intentionality;attribution	HCI	-52.15034552781311	-53.37098138217817	6248
b5e781ec05b89255bcbf62f57e95641602e85bbe	adapting markov decision process for search result diversification		In this paper we address the issue of learning diverse ranking models for search result diversification. Typical methods treat the problem of constructing a diverse ranking as a process of sequential document selection. At each ranking position, the document that can provide the largest amount of additional information to the users is selected, because the search users usually browse the documents in a top-down manner. Thus, to select an optimal document for a position, it is critical for a diverse ranking model to capture the utility of information the user have perceived from the preceding documents. Existing methods usually calculate the ranking scores (e.g., the marginal relevance) directly based on the query and the selected documents, with heuristic rules or handcrafted features. The utility the user perceived at each of the ranks, however, is not explicitly modeled. In this paper, we present a novel diverse ranking model on the basis of continuous state Markov decision process (MDP) in which the user perceived utility is modeled as a part of the MDP state. Our model, referred to as MDP-DIV, sequentially takes the actions of selecting one document according to current state, and then updates the state for the chosen of the next action. The transition of the states are modeled in a recurrent manner and the model parameters are learned with policy gradient. Experimental results based on the TREC benchmarks showed that MDP-DIV can significantly outperform the state-of-the-art baselines.	baseline (configuration management);benchmark (computing);browsing;diversification (finance);feature engineering;gradient;heuristic;marginal model;markov chain;markov decision process;recurrent neural network;reinforcement learning;relevance;span and div;text retrieval conference;top-down and bottom-up design	Long Xia;Jun Xu;Yanyan Lan;Jiafeng Guo;Wei Zeng;Xueqi Cheng	2017		10.1145/3077136.3080775	information retrieval;data mining;learning to rank;computer science;ranking (information retrieval);ranking svm;baseline (configuration management);markov decision process;machine learning;heuristic;diversification (marketing strategy);artificial intelligence;ranking	Web+IR	-27.119814860750093	-54.490656700808145	6267
0def677e7e20961d59e902559b62e38a197a262f	modeling the on-off patterns in conversational speech, including short silence gaps and the effects of interaction between speaking parties	speech analysis on off patterns conversational speech short silence gaps speaking parties interaction speech interpolation modified brady model wireless communication systems performance;interpolation;wireless communication systems;speech processing;speech analysis interpolation wireless communication systems engineering and theory statistical analysis system performance probability distribution telecommunications telephony communication cables;voice communication;mobile radio;voice communication speech processing interpolation mobile radio	The six-state model for on-off characteristics of conversational speech established by P.T. Brady (1969) does not consider pauses in speech shorter than 200 msec. This paper proposes modifications to the Brady model to represent the effects of the short pauses while preserving the effects of the longer silences and the dynamics of the interaction between speaking parties. The modified Brady model provides a tool for more accurately assessing the performance of new-generation wireless communication systems, which will utilize the short silence pauses as well as the longer periods of silence. >	switch	Harold P. Stern;Samy A. Mahmoud;Kin-Kwok Wong	1994		10.1109/VETEC.1994.345303	voice activity detection;speech recognition;telecommunications;interpolation;computer science;speech processing	HCI	-11.174690975346746	-84.5706064386867	6284
365c8cd8ceb6323a2d09448e9cace1fa333b8c4a	virtual reality, vr digital art website setup and teaching applications	virtual reality	Tinnitus symptoms are treated with aminooxyacetic acid (AOAA) administered orally at dosages of 200-300 mg/day. Clinically significant reductions in tinnitus symptoms were observed.	virtual reality	Chuan-Cheng Chang;Yu-Liang Chang;Ya-Chen Huang;Yu-Huei Su	2008			computer science;multimedia;immersion (virtual reality);computer-mediated reality;tinnitus;mixed reality;aminooxyacetic acid;virtual reality;digital art	Visualization	-56.13469865019162	-56.91527358104167	6319
6466bedd1ffd139592e5dd49d155c765ec2c9e75	understanding images with natural sentences	user needs;large scale;multi stack decoding;canonical correlation analysis;web technology;similarity measure;probabilistic canonical correlation analysis	We propose a novel system which generates sentential captions for general images. For people to use numerous images effectively on the web, technologies must be able to explain image contents and must be capable of searching for data that users need. Moreover, images must be described with natural sentences based not only on the names of objects contained in an image but also on their mutual relations. The proposed system uses general images and captions available on the web as training data to generate captions for new images. Furthermore, because the learning cost is independent from the amount of data, the system has scalability, which makes it useful with large-scale data.	scalability	Yoshitaka Ushiku;Tatsuya Harada;Yasuo Kuniyoshi	2011		10.1145/2072298.2072417	natural language processing;computer vision;canonical correlation;computer science;machine learning;pattern recognition;data mining;world wide web;information retrieval;statistics	ML	-15.625655863954602	-65.25485970744108	6363
aa96fc0d6c6fcefef4fb9f0baa65b6c5c731f788	beatrix: a self-learning system for off-line recognition of handwritten texts	artifical neural networks;handwriting recognition;contextual information;self learning;learning system;system integration;character recognition;artificial neural network;neural network	We present a self-learning system, BEATRIX, for o -line recognition of handwritten texts. The system integrates neural recognition with context analysis techniques. It consists of three main interacting subsystems: the rst is based on an ensemble of neural networks and carries out an approximate pre-recognition of characters; the second carries out a lexical and grammatical analysis of the recognised text. This analysis produces hypotheses about words and sentences in order to correct errors made by the neural networks. Once a su cient number of words have been recognized, the third subsystem retrains one of the neural networks with the hypotheses produced. This enhances the capacity of the system to recognise the speci c handwriting, without losing the capability to recognise other types of handwriting.	approximation algorithm;artificial neural network;interaction;online and offline	Beatrice Lazzerini;Francesco Marcelloni;Leonardo Maria Reyneri	1997	Pattern Recognition Letters	10.1016/S0167-8655(97)00039-1	natural language processing;speech recognition;intelligent character recognition;computer science;artificial intelligence;recurrent neural network;machine learning;pattern recognition;time delay neural network;neocognitron;artificial neural network;system integration	AI	-18.40068992370176	-86.66887099168184	6377
800c7ceba9fe82ff1031297b2e684401538e66d2	acoustic factor analysis for streamed hidden markov modeling	decoding algorithm;analisis acustico;desciframiento;analytical models;modelizacion;optimal solution;processus gauss;streamed hidden markov modeling framework;fashmm speech recognition;solution optimale;streaming;gaussian mixture;chaine markov;cadena markov;acoustic analysis;analisis factorial;common factor;decodage;decoding;speech recognition acoustic signal processing cepstral analysis correlation methods covariance matrices decoding feature extraction gaussian processes hidden markov models speech coding;gaussian processes;taux erreur;hidden markov model;information transmission;speech processing;modele markov variable cachee;tratamiento palabra;traitement parole;speech coding;acoustic signal processing;probabilistic approach;correlation methods;similitude;algorithme;modelisation;algorithm;transmission en continu;cepstral analysis;analyse factorielle;hidden markov models;reconocimiento voz;factor analysis fa;stochastic processes;cepstral feature correlation extraction acoustic factor analysis principle streamed hidden markov modeling framework fashmm speech recognition multiple markov chain covariance matrix decoding algorithm multivariate gaussian mixture;factor analysis;analyse cepstrale;cepstral feature correlation extraction;covariance matrices;viterbi algorithm;enfoque probabilista;approche probabiliste;feature extraction;number of factors;solucion optima;signal classification;speech recognition factor analysis fa markov chain streamed hidden markov model;similarity;streamed hidden markov model;hidden markov models speech recognition cepstral analysis speech enhancement speech processing streaming media topology acoustic noise standards development decoding;classification signal;error rate;speech recognition;transmision fluyente;transmision informacion;gaussian process;reconnaissance parole;similitud;acoustic factor analysis principle;classification automatique;transmission information;proceso gauss;automatic classification;indice error;modeling	This paper presents a novel streamed hidden Markov model (HMM) framework for speech recognition. The factor analysis (FA) principle is adopted to explore the common factors from acoustic features. The streaming regularities in building HMMs are governed by the correlation between cepstral features, which is inherent in common factors. Those features corresponding to the same factor are generated by the identical HMM state. Accordingly, the multiple Markov chains are adopted to characterize the variation trends in different dimensions of cepstral vectors. An FA streamed HMM (FASHMM) method is developed to relax the assumption of standard HMM topology, namely, that all features of a speech frame perform the same state emission. The proposed FASHMM is more flexible than the streamed factorial HMM (SFHMM) where the streaming was empirically determined. To reduce the number of factor loading matrices in FA, we evaluated the similarity between individual matrices to find the optimal solution to parameter clustering of FA models. A new decoding algorithm was presented to perform FASHMM speech recognition. FASHMM carries out the streamed Markov chains for a sequence of multivariate Gaussian mixture observations through the state transitions of the partitioned vectors. In the experiments, the proposed method reduced the recognition error rates significantly when compared with the standard HMM and SFHMM methods.	acoustic cryptanalysis;algorithm;cepstrum;cluster analysis;database;eigen (c++ library);experiment;factor analysis;frame language;hidden markov model;markov chain;model selection;speech recognition;stochastic process;streaming media;transformation matrix;viterbi decoder	Jen-Tzung Chien;Chuan-Wei Ting	2009	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2009.2014891	speech recognition;computer science;machine learning;pattern recognition;gaussian process;mathematics;hidden markov model;statistics	ML	-20.555133205400193	-92.34976134532131	6419
f2f7876815e362afb4eb777ac59e8b23e84ce5e2	computerised decision support in physical activity interventions: a systematic literature review		BACKGROUND The benefits of regular physical activity for health and quality of life are unarguable. New information, sensing and communication technologies have the potential to play a critical role in computerised decision support and coaching for physical activity.   OBJECTIVES We provide a literature review of recent research in the development of physical activity interventions employing computerised decision support, their feasibility and effectiveness in healthy and diseased individuals, and map out challenges and future research directions.   METHODS We searched the bibliographic databases of PubMed and Scopus to identify physical activity interventions with computerised decision support utilised in a real-life context. Studies were synthesized according to the target user group, the technological format (e.g., web-based or mobile-based) and decision-support features of the intervention, the theoretical model for decision support in health behaviour change, the study design, the primary outcome, the number of participants and their engagement with the intervention, as well as the total follow-up duration.   RESULTS From the 24 studies included in the review, the highest percentage (n = 7, 29%) targeted sedentary healthy individuals followed by patients with prediabetes/diabetes (n = 4, 17%) or overweight individuals (n = 4, 17%). Most randomized controlled trials reported significantly positive effects of the interventions, i.e., increase in physical activity (n = 7, 100%) for 7 studies assessing physical activity measures, weight loss (n = 3, 75%) for 4 studies assessing diet, and reductions in glycosylated hemoglobin (n = 2, 66%) for 3 studies assessing glycose concentration. Accelerometers/pedometers were used in almost half of the studies (n = 11, 46%). Most adopted decision support features included personalised goal-setting (n = 16, 67%) and motivational feedback sent to the users (n = 15, 63%). Fewer adopted features were integration with electronic health records (n = 3, 13%) and alerts sent to caregivers (n = 4, 17%). Theoretical models of decision support in health behaviour to drive the development of the intervention were not reported in most studies (n = 14, 58%).   CONCLUSIONS Interventions employing computerised decision support have the potential to promote physical activity and result in health benefits for both diseased and healthy individuals, and help healthcare providers to monitor patients more closely. Objectively measured activity through sensing devices, integration with clinical systems used by healthcare providers and theoretical frameworks for health behaviour change need to be employed in a larger scale in future studies in order to realise the development of evidence-based computerised systems for physical activity monitoring and coaching.		Andreas Triantafyllidis;Dimitris Filos;Jomme Claes;Roselien Buys;Véronique A. Cornelissen;Evangelia Kouidi;Ioanna Chouvarda;Nicos Maglaveras	2018	International journal of medical informatics	10.1016/j.ijmedinf.2017.12.012	knowledge management;randomized controlled trial;health care;physical therapy;decision support system;systematic review;psychological intervention;overweight;coaching;health informatics;medicine	HCI	-61.17831802770852	-62.452856446997195	6497
dd714a02de864ea188f8ea07a93fde0ca1b166e6	syntactic lexicon of polish predicative nouns.		In the paper we report realization of SyntLex project aiming at construction of a full lexicon grammar for Polish. The lexicongrammar based paradigm in computer linguistics is derived from the predicate logic and attributes a central role to the predicative constructions. An important class of syntactic constructions in many languages (French, English, Polish and other Slavonic languages in particular) are those based on verbo-nominal collocations, with the verb playing a support role with respect to the noun considered as carrying the predicative information. In this paper we refer to the former research by one of the authors aiming at full description of verbo-nominal predicative constructions for Polish in the form of an electronic resource for LI applications. We describe procedures to complete and corpus validate the resource obtained so far.	academy;collocation;computational linguistics;computer science;dictionary;impredicativity;lexicon;programming paradigm;text corpus	Grazyna Vetulani;Zygmunt Vetulani;Tomasz Obrêbski	2006			predicate logic;syntax;natural language processing;artificial intelligence;noun;slavic languages;computer science;lexicon;verb;predicative expression;grammar	NLP	-29.39538433670944	-76.5543257943655	6515
aeecec1d2872ee4cb56968a2db5f90795badb44c	deep context model for grammatical error correction		In this paper, we propose a deep context model based on recurrent neural networks (RNN) for grammatical error correction. For a specific error type, we treat the error correction task as a classification problem where the grammatical context representation is learnt from native text data that are largely available. Compared with traditional classifier methods, our model does not require sophisticated feature engineering which usually requires linguistic knowledge and may not cover all context patterns. Experiments on CoNLL-2014 shared task show that our approach significantly outperforms the state-of-the-art classifier and machine translation approaches for grammatical error correction.	artificial neural network;error detection and correction;feature engineering;machine translation;random neural network;recurrent neural network;text corpus	Chuan Wang;Ruobing Li;Hui Lin	2017			speech recognition;natural language processing;artificial intelligence;computer science;error detection and correction;context model	NLP	-18.15865921866636	-74.23179469041219	6519
197a35bd26066a47c830f63f2d13d22c80b8f132	a personal model of trumpery: deception detection in a real-world high-stakes setting		Language use reveals information about who we are and how we feel1-3. One of the pioneers in text analysis, Walter Weintraub, manually counted which types of words people used in medical interviews and showed that the frequency of first-person singular pronouns (i.e., I, me, my) was a reliable indicator of depression, with depressed people using I more often than people who are not depressed4. Several studies have demonstrated that language use also differs between truthful and deceptive statements5-7, but not all differences are consistent across people and contexts, making prediction difficult8. Here we show how well linguistic deception detection performs at the individual level by developing a model tailored to a single individual: the current US president. Using tweets fact-checked by an independent third party (Washington Post), we found substantial linguistic differences between factually correct and incorrect tweets and developed a quantitative model based on these differences. Next, we predicted whether out-of-sample tweets were either factually correct or incorrect and achieved a 73% overall accuracy. Our results demonstrate the power of linguistic analysis in real-world deception research when applied at the individual level and provide evidence that factually incorrect tweets are not random mistakes of the sender.		Sophie van der Zee;Ronald Poppe;Alice Havrileck;Aurélien Baillon	2018	CoRR			NLP	-21.726846645791984	-59.780881691171345	6536
91b7c6095a5c4dc37c462770dec821915f8eaad0	mining concurrent topical activity in microblog streams		Streams of user-generated content in social media exhibit patterns of collective attention across diverse topics, with temporal structures determined both by exogenous factors and endogenous factors. Teasing apart different topics and resolving their individual, concurrent, activity timelines is a key challenge in extracting knowledge from microblog streams. Facing this challenge requires the use of methods that expose latent signals by using term correlations across posts and over time. Here we focus on content posted to Twitter during the London 2012 Olympics, for which a detailed schedule of events is independently available and can be used for reference. We mine the temporal structure of topical activity by using two methods based on non-negative matrix factorization. We show that for events in the Olympics schedule that can be semantically matched to Twitter topics, the extracted Twitter activity timeline closely matches the known timeline from the schedule. Our results show that, given appropriate techniques to detect latent signals, Twitter can be used as a social sensor to extract topical-temporal information on realworld events at high temporal resolution.	cathode ray tube;computation;dynamic problem (algorithms);emergence;hashtag;incremental computing;multiplexing;national transfer format;non-negative matrix factorization;online and offline;pc bruno;social media;timeline;user-generated content	André Panisson;Laetitia Gauvin;Marco Quaggiotto;Ciro Cattuto	2014			data mining;internet privacy;world wide web	Web+IR	-23.939087599764637	-54.08174020288001	6580
5fa1e561118748a87190ac0f8d42c58581e51e74	usability study on two handheld computers to retrieve drug information	drug therapy;computers;handheld;ergonomics;computer assisted	UNLABELLED Objective :Performing a usability study on two handheld computers (personal digital assistant and tablet PC), as tools for retrieving drug information.   MATERIALS AND METHODS A randomised crossover study was performed: 34 students in pharmacy and medicine used the two handheld tools in a randomised order, to answer a questionnaire containing 12 questions covering all the aspects of a drug database and a qualitative analysis on six different items to measure access to drug information. The availability of the drug information database Vidal on PDA and on tablet PC implied our choice of the database. Three main criteria for evaluation were chosen: success rates, time-on-task, and number of clicks.   RESULTS There were no significant differences between the two groups neither on age, sex, medical discipline, study years nor previous computer practice. The success rate is significantly higher with the PDA for only one question. The PDA is significantly faster than the tablet PC on 7 of the 12 questions and generates fewer clicks for 3 questions. Compared to the tablet PC, it appears that the PDA is better in terms of clearness, navigability and usefulness for professional practice and it is the only tool which is significantly preferred to all other supports.   CONCLUSION In this study with students, the PDA is significantly more effective quantitatively and qualitatively than the tablet PC to retrieve drug information.		Simon Letellier;Klervi Leuraud;Philippe Arnaud;Stéfan Jacques Darmoni	2005	Studies in health technology and informatics		pharmacy;drug;crossover study;mobile device;multimedia;usability;medicine	Web+IR	-60.4669944506575	-64.57521702813608	6593
8a532904c969ec7280191d5c8a114ef1bd06c23b	alertnessscanner: what do your pupils tell about your alertness		Alertness is a crucial component of our cognitive performance. Reduced alertness can negatively impact memory consolidation, productivity and safety. As a result, there has been an increasing focus on continuous assessment of alertness. The existing methods usually require users to wear sensors, fill out questionnaires, or perform response time tests periodically, in order to track their alertness. These methods may be obtrusvie to some users, and thus have limited capability. In this work, we propose AlertnessScanner, a computer-vision-based system that collects in-situ pupil information to model alertness in the wild. We conducted two in-the-wild studies to evaluate the effectiveness of our solution, and found that AlertnessScanner passively and unobtrusively assess alertness. We discuss the implications of our findings and present opportunities for mobile applications that measure and act upon changes in alertness.	computer vision;mobile app;response time (technology);semiconductor consolidation;sensor	Vincent W. S. Tseng;Saeed Abdullah;Jean Marcel dos Reis Costa;Tanzeem Choudhury	2018		10.1145/3229434.3229456	alertness;effects of sleep deprivation on cognitive performance;cognitive psychology;continuous assessment;response time;psychology	HCI	-59.063225708998985	-53.53340381748103	6621
6cd1e23813629ac0ee51e9e35b18778556900446	approximating context-free by rational transduction for example-based mt	approximating context-free;weighted rational transduction;weighted context-free transduction;example-based mt;real-time application;reasonable quality	Existing studies show that a weighted context-free transduction of reasonable quality can be effectively learned from examples. This paper investigates the approximation of such transduction by means of weighted rational transduction. The advantage is increased processing speed, which benefits realtime applications involving spoken language.	approximation;context-free language;real-time computing;transducer;transduction (machine learning)	Mark-Jan Nederhof	2001			artificial intelligence;machine learning;communication	NLP	-27.63033501491571	-81.01817208807194	6655
d801ba1fb2cf0b5e53443d6a17f0f9789113ecf0	intelligibility of synthesized voice messages in commercial truck cab noise for normal-hearing and hearing-impaired listeners	speech intelligibility;n ratio;speech synthesis;normal hearing;noise induced hearing loss;noise measurement;hearing impaired;human factors;indexation;physically handicapped persons;hearing loss;trucks	A human factors experiment was conducted to assess the intelligibility of synthesized speech under a variety of noise conditions for both hearing-impaired and normal-hearing subjects. Modified Rhyme Test stimuli were used to determine intelligibility in four speech-to-noise (S/N) ratios (0, 5, 10, and 15 dB), and three noise types, consisting of fiat-by-octaves (pink) noise, interior noise of a currently produced heavy truck, and truck cab noise with added background speech. A quiet condition was also investigated. During recording of the truck noise for the experiment, in-cab noise measurements were obtained. According to OSHA standards, these data indicated that drivers of the sampled trucks have a minimal risk for noise-induced hearing loss due to in-cab noise exposure when driving at freeway speeds because noise levels were below 80 dBA. In the intelligibility experiment, subjects with hearing loss had significantly lower intelligibility than normal-hearing subjects, both in quiet and in noise, but no interaction with noise type or S/N ratio was found. Intelligibility was significantly lower for the noise with background speech than the other noises, but the truck noise produced intelligibility equal to the pink noise. An analytical prediction of intelligibility using Articulation Index calculations exhibited a high positive correlation with the empirically obtained intelligibility data for both groups of subjects.	biconnected component;freeway;human factors and ergonomics;intelligibility (philosophy);pink noise;speech synthesis	H. Boyd Morrison;John G. Casali	1997	I. J. Speech Technology	10.1007/BF02539821	speech recognition;truck;computer science;noise measurement;linguistics;speech synthesis;intelligibility	HCI	-9.920737551245423	-84.2839018402659	6670
3ff2e249009ff9959a80bbe7bd02d715d542c696	the impact of data quality tags on decision-making outcomes and process	protocol analysis;data quality;contextual inquiry	Research Article Rosanne Price Monash University rosanne.price@monash.edu Graeme Shanks The University of Melbourne gshanks@unimelb.edu.au It has been proposed that metadata describing data quality (DQ), termed DQ tags, be made available in situations where decision makers are unfamiliar with the data context, for example, in data warehouses. However, there have been conflicting reports as to the impact of such DQ tags on decision-making outcomes. Early studies did not explicitly consider the usability and semantics of the DQ tag designs used experimentally or the impact of such tags on decision process, except in suggestions for future research. This study addresses these issues, focusing on the design of usable DQ tags whose semantics are explicitly specified and exploring the impact of such DQ tags on decision outcomes and process. We use the information quality framework InfoQual, the interaction design technique of contextual inquiry, and cognitive process tracing to address DQ tag semantics, usability, and impact on decision process, respectively. In distinct contrast to earlier laboratory experiments, there was no evidence that the preferred decision choice changed with DQ tags, but decision time was significantly increased and there were indications of reduced consensus. These results can be explained by understanding the impact of DQ tags on decision process using concurrent protocol analysis, which involves participants verbalizing thoughts while making a decision. The protocol analysis study shows that DQ tags are associated with increased cognitive processing in the earlier phases of decision making, which delays generation of decision alternatives.		Rosanne J. Price;Graeme G. Shanks	2011	J. AIS			HCI	-49.08595197175158	-57.472904837634154	6679
70fbc0f8dbc774763a969e443200da08d9d9da59	distributed deep learning for question answering	deep learning;distributed training;question answering	This paper is an empirical study of the distributed deep learning for question answering subtasks: answer selection and question classification. Comparison studies of SGD, MSGD, ADADELTA, ADAGRAD, ADAM/ADAMAX, RMSPROP, DOWNPOUR and EASGD/EAMSGD algorithms have been presented. Experimental results show that the distributed framework based on the message passing interface can accelerate the convergence speed at a sublinear scale. This paper demonstrates the importance of distributed training. For example, with 48 workers, a 24x speedup is achievable for the answer selection task and running time is decreased from 138.2 hours to 5.81 hours, which will increase the productivity significantly.	algorithm;deep learning;message passing interface;question answering;silent hill: downpour;speedup;time complexity	Minwei Feng;Bing Xiang;Bowen Zhou	2016		10.1145/2983323.2983377	question answering;computer science;machine learning;data mining;deep learning;information retrieval	NLP	-18.67318929358175	-77.95875317784765	6691
350e8db85fdce72230d6e7380807f3e88b9e0c1c	topic by topic performance of information retrieval systems	information retrieval;information retrieval system;system performance	Abstract Formulation of topic properties is the goal of this paper. These properties are to be used in judging the difficulty of topics appropriate to the Text REtrieval Conference (TREC) ad hoc task. Applying statistical methods to both TREC-6 and TREC-7 information retrieval results, we identify topic pa irs that exemplify topic properties useful in relating topic statements to system performance. From two topics that seem the same with respect to the challenge they provide information r etrieval systems, we formulate topic properties by relating the corresponding topic statements to what is known abo ut information retrieval systems. Some properties app arent in the topic pairs identified are linked to topic expansio n. These pairs exemplify both the need for expansion and the dange r in automatic expansion.	adaptive binary optimization;delimiter;exemplification;hoc (programming language);information retrieval;occam's razor;text retrieval conference	Walter Liggett	1998			query expansion;data mining;information retrieval;document clustering;computer science	Web+IR	-29.718659974454358	-64.12883402576053	6699
aeb4e2eab9b47f5622f677ae514fd14a10c0e7b4	durational evidence for syllable boundary of /n/ and /l/ in text-to-speech synthesis	stress;text to speech synthesis;position;coda;ambisyllabicity;duration;syllable;onset;vlnavr;vllavr	The Text-to-Speech (TTS) system does rely on syllable boundary information for segmental duration. However, ambisyllabic consonants always pose a problem to TTS because the system requires clear syllable boundaries to segment and concatenate. In order to provide a possible solution to this problem, /n/ and /l/ in VLCAVR are chosen in this paper as the target to be examined whether their durations behave more like the syllabic onset or coda when comparing with the durational properties of /n/ and /l/ both as onsets in CV and codas in VC. As the syllable boundaries, onset C shows much more sensitivity to stress than coda C while coda C shows more sensitivity to syllabic position than onset C. Moreover, CA in VLCAVR is also influenced by two variables of stress and position as C in CV and VC. The results show that the intervocalic CA holds the properties of both the syllabic onset and coda, which states the possibility that intervocalic consonants should be considered as a rather independent concatenative unit in TTS synthesis.	coda (file system);concatenation;netware file system;onset (audio);speech synthesis;syllable	Fang Tian	2013	Journal of Multimedia	10.4304/jmm.8.2.82-89	coda;speech recognition;position;duration;stress;syllable	AI	-10.547870598501405	-82.09010411324579	6723
ae83eb242cb92be1754083235ad03db29b44231a	chart decoder: generating textual and numeric information from chart images automatically		Abstract Charts are commonly used as a graphical representation for visualizing numerical data in digital documents. For many legacy charts or scientific charts, however, underlying data is not available, which hinders the process of redesigning more effective visualizations and further analysis of charts. In response, we present Chart Decoder, a system that implements decoding of visual features and recovers data from chart images. Chart Decoder takes a chart image as input and generates the textual and numeric information of that chart image as output through applying deep learning, computer vision and text recognition techniques. We train a deep learning based classifier to identify chart types of five categories (bar chart, pie chart, line chart, scatter plot and radar chart), which achieves a classification accuracy over 99%. We also complement a textual information extraction pipeline which detects text regions in a chart, recognizes text content and distinguishes their roles. For generating textual and graphical information, we implement automated data recovery from bar charts, one of the most popular chart types. To evaluate the effectiveness of our algorithms, we evaluate our system on two corpora: 1) bar charts collected from the web, 2) charts randomly made by a script. The results demonstrate that our system is able to recover data from bar charts with a high rate of accuracy.		Wenjing Dai;Meng Wang;Zhibin Niu;Jiawan Zhang	2018	J. Vis. Lang. Comput.	10.1016/j.jvlc.2018.08.005	information extraction;data mining;information retrieval;scatter plot;radar chart;bar chart;pie chart;data recovery;line chart;computer science;chart	NLP	-13.84847655618169	-64.33545737688296	6739
42fc7c3464815fb7bc67af5851d576956ee6b3d8	effect of noise on lexical tone perception in cantonese-speaking amusics		Congenital amusia is a neurogenetic disorder affecting musical pitch processing. It also affects lexical tone perception. It is well documented that noisy conditions impact speech perception in second language learners and cochlear implant users. However, it is yet unclear whether and how noise affects lexical tone perception in the amusics. This paper examined the effect of multi-talker babble noise [1] on lexical tone identification and discrimination in 14 Cantonesespeaking amusics and 14 controls at three levels of signal-tonoise ratio (SNR). Results reveal that the amusics were less accurate in the identification of tones compared to controls in all SNR conditions. They also showed degraded performance in the discrimination, but less severe than in the identification. These results confirmed that amusia influences lexical tone processing. But the amusics were not influenced more by noise than the controls in either identification or discrimination. This indicates that the deficits of amusia may not be due to the lack of native-like language processing mechanisms or are mechanical in nature, as in the case of second language learners and cochlear implant users. Instead, the amusics may be impaired in the linguistic processing of native tones, showing impaired tone perception already under the clear condition.	cochlear implant;depth perception;pitch (music);signal-to-noise ratio	Jing Shao;Caicai Zhang;Gang Peng;Yike Yang;William S.-Y. Wang	2016		10.21437/Interspeech.2016-891	speech recognition;perception;computer science	HCI	-10.06378197267711	-83.81046792021125	6743
d3929604f3d6757e035d2f1774805ba0d050583f	tpematcher: a tool for searching in parsed text corpora	text mining;query formulation;pattern matching;parsed text corpora;tree pattern matching;tree pattern querying;corpus search tool	Recently, due to the widespread on-line availability of syntactically annotated text corpora, some automated tools for searching in such text corpora have gained great attention. Generally, those conventional corpus search tools use a decomposition-matching-merging method based on relational predicates for matching a tree pattern query to the desired parts of text corpora. Thus, their query formulation and expressivity are often complicated due to poorly understood query formalisms, and their searching tasks may require a big computational overhead due to a large number of repeated trials of matching tree patterns. To overcome these difficulties, we present TPEMatcher, a tool for searching in parsed text corpora. TPEMatcher provides not only an efficient way of query formulation and searching but also a good query expressivity based on concise syntax and semantics of tree pattern query. We also demonstrate that TPEMatcher can be effectively used for a text mining in practice with its useful interface providing in-depth details of search results. 2011 Elsevier B.V. All rights reserved.	algorithm;dynamic programming;ethernet over twisted pair;iterative method;online and offline;overhead (computing);parse tree;parsing;pattern matching;text corpus;text mining;time complexity;tree (data structure)	Yong Suk Choi	2011	Knowl.-Based Syst.	10.1016/j.knosys.2011.04.009	natural language processing;query optimization;text mining;computer science;pattern matching;data mining;web search query;information retrieval	DB	-29.924741539683644	-69.66884995290778	6823
334ddf0b3faed26deebbc48a20018f29114eab22	barriers for dissemination of a national health care data network. a list of recommendations for better dissemination.		Establishment of a Danish Nation-wide Health Care Data Network for exchange of health care related information using standardised EDIFACT messages is agreed between the Danish Government, the Hospital Owners and the GPs organisations. The aim is, within year 2000 in the MedCom project, to reach a level of 66% of all messages between the secondary and primary health sector must be send electronically using EDIFACT. There are many barriers for reaching this goal, and to reveal this barriers an investigation was done, and a list of recommendations are set up. The investigation was based upon literature, interviews and most important a questionnaire among 200 GPs and specialists all over the country. After 6 months where the recommendations have been handled there is seen an increase in communicated messages on 40%, and from January 1999 more than 1.1 mill messages are exchanged each month.		Ib Johansen;Lone Kaalund Thiel	1999	Studies in health technology and informatics	10.3233/978-1-60750-912-7-262	data mining;computer security	HCI	-59.65103075155361	-64.30867478745509	6824
1f69dd824fe0333b337779cb609ffffb49511b07	representing and combining calendar information by using finite-state transducers	calendar information;timeline;temporal representation;semantic calendar expressions;finite state transducer;finite state transducers	This paper elaborates a model for representing various types of semantic calendar expressions (SCEs), which correspond to the disambiguated intensional meanings of natural-language calendar phrases. The model uses finite-state transducers (FSTs) to mark denoted periods of time on a set of timelines also represented as an FST. In addition to an overview of the model, the paper presents methods to combine the periods marked on two timeline FSTs into a single timeline FST and to adjust the granularity and span of time of a timeline FST. The paper also discusses advantages and limitations of the model.	transducer	Jyrki Niemi;Kimmo Koskenniemi	2008		10.3233/978-1-58603-975-2-122	timeline;finite state transducer;computer science;artificial intelligence;algorithm	NLP	-35.17971977657988	-80.88196934770468	6831
5fa09b1a23757ebc0b44311c2fab347ae330d44d	towards the development of an automatic readability measurements for arabic language	readability metrics;arabic text automatic readability measurement arabic language educational progress natural language processing english language spanish language automatic readability system;complexity theory;web pages;text analysis computer aided instruction natural language processing;support vector machines;arabic language;computer aided instruction;training;text analysis;machine learning;natural languages educational institutions vocabulary information technology readability metrics computer science natural language processing writing particle measurements medical services;natural language processing	Currently, there are more than 200 readability formulas developed since the 1920s. Only a handful of these formulas are reliable to determine the reading-level of a sample text. Ascertaining the readability of curricula is an important step toward optimizing the effectiveness of the educational progress. Readability measurements are done using manual computation. This is a tedious and time-consuming task. However, nowadays, automatic readability computation has gained much popularity than ever before. This can be attributed to the advancement in the field of natural language processing. Languages such as English and Spanish have benefited from the formation of automatic readability systems. Absence of automated readability measurement of Arabic texts and the large amount of information which are written in Arabic encouraged us to work on finding an automatic readability system for our language; the mother language for millions of Arabs. In this paper we will review the different readability research and then propose a system for automating the readability measurement of Arabic text. Within the paper we also report the results of our pilot experiment carried out on the different well-known Arabic, Swedish and English readability formulas.	computation;natural language processing;programming language	Amani A. Al-Ajlan;Hend Suliman Al-Khalifa;AbdulMalik S. Al-Salman	2008	2008 Third International Conference on Digital Information Management	10.1109/ICDIM.2008.4746711	natural language processing;support vector machine;text mining;speech recognition;computer science;machine learning;arabic;web page	SE	-29.78960437996024	-72.2205436230446	6847
0b56e6a31611364f95e2c607c2365b433966f6c5	the effect of sonority on word segmentation: evidence for the use of a phonological universal	cues;habla;syllables;etude experimentale;acoustics;lenguaje;college students;hombre;speech;langage;segmentation;percepcion;sonority sequencing principle;verbal perception;word segmentation;percepcion verbal;phonology;bias;cognition;human;palabra;language universals;cognicion;audition;fonologia;word;english;sonority;phonologie;audicion;parole;language;perception;auditory stimuli;article;estudio experimental;segmentacion;hearing;mot;homme;perception verbale	It has been well documented how language-specific cues may be used for word segmentation. Here, we investigate what role a language-independent phonological universal, the sonority sequencing principle (SSP), may also play. Participants were presented with an unsegmented speech stream with non-English word onsets that juxtaposed adherence to the SSP with transitional probabilities. Participants favored using the SSP in assessing word-hood, suggesting that the SSP represents a potentially powerful cue for word segmentation. To ensure the SSP influenced the segmentation process (i.e., during learning), we presented two additional groups of participants with either (a) no exposure to the stimuli prior to testing or (b) the same stimuli with pauses marking word breaks. The SSP did not influence test performance in either case, suggesting that the SSP is important for word segmentation during the learning process itself. Moreover, the fact that SSP-independent segmentation of the stimulus occurred (in the latter control condition) suggests that universals are best understood as biases rather than immutable constraints on learning.	document completion status - documented;emoticon;hood method;immutable object;item unique identification;language-independent specification;probability;sessile serrated adenoma/polyp;text segmentation;biologic segmentation	Marc Ettlinger;Amy S. Finn;Carla L. Hudson Kam	2012	Cognitive science	10.1111/j.1551-6709.2011.01211.x	psychology;cognitive psychology;speech recognition;cognition;speech;english;bias;word;linguistics;language;communication;perception;segmentation;phonology	NLP	-10.649219350374514	-77.46657298362511	6866
e725f1c1015a858122c089bb4f2ea231d66e0602	software tools for navigation in document databases - development of information navigation service based on classification schemes		Internet allows accessing large document databases contained in different information centres across the world. Each database has its own search engine which is based on an index or classification scheme. Problems occur when a user tries to search different databases at once: different databases use different classification schemes. This article describes a classification scheme mapping service which is useful in integration of different databases in one search engine.	comparison and contrast of classification schemes in linguistics and metadata;database;information source;internet;scheme;web mapping;web search engine	Pavel Shapkin;Alexander Shapkin	2007			computer science;data mining;database;information retrieval	Web+IR	-31.134376082243858	-57.706786773919326	6904
6c3a89b0fd0b7da9e055982d3f04e769f2bf0f2e	multimedia database of meetings and informal interactions for tracking participant involvement and discourse flow.		At ATR, we are collecting and analysing ‘meetings’ data using a table-top sensor device consisting of a small 360degree camera surrounded by an array of high-quality directional microphones. This equipment provides a stream of information about the audio and visual events of the meeting which is then processed to form a representation of the verbal and non-verbal interpersonal activity, or discourse flow, during the meeting. This paper describes the resulting corpus of speech and video data which is being collected for the above research. It currently includes data from 12 monthly sessions, comprising 71 video and 33 audio modules. Collection is continuing monthly and is scheduled to include another ten sessions.	answer to reset;interaction;microphone	Nick Campbell;Toshiyuki Sadanobu;Masataka Imura;Naoto Iwahashi;Noriko Suzuki;Damien Douxchamps	2006			multimedia database;multimedia;computer science;interpersonal communication;flow (psychology)	HCI	-25.11404820497385	-84.88902208621556	6905
91baf2a814aafe3991247048335edfa6388e872d	a cue for objective speech quality estimation in temporal envelope representations	telephone networks speech processing;telephone networks;nonintrusive estimation speech quality estimation temporal envelope representation human auditory system human speech production system;speech processing;human auditory system;system under test;spectrum;speech processing humans distortion measurement system testing auditory system speech analysis production systems signal processing databases performance evaluation;mean opinion score;speech production;object model	In this paper, we investigate a new paradigm of objective speech quality estimation. The proposed nonintrusive method utilizes only processed speech signal, whereas conventional objective models require source speech applied as an input to the system under test, as well as the processed speech. The proposed method is based on the temporal envelope representation of speech, which reflects the perceptual characteristics of human auditory systems and human speech production systems, and we found it provides a useful cue for nonintrusive objective speech quality estimation. The performance of the proposed method is demonstrated for four different subjective mean opinion score databases.	database;programming paradigm;system under test	Doh-Suk Kim	2004	IEEE Signal Processing Letters	10.1109/LSP.2004.835466	voice activity detection;mean opinion score;speech production;spectrum;audio mining;linear predictive coding;speech recognition;object model;telephone network;computer science;speech coding;speech processing;psqm;system under test	ML	-9.436184926685602	-88.816999163697	6906
56c6f19ea281137cdaf0f1e7c4b17e666afe8197	broadcast news lm adaptation over time	broadcast news;vocabulaire;word error rate;complex dynamics;mathematics;mise a jour;phonetic transcription;out of vocabulary;vocabulary;linguistique appliquee;radio news;journal parle;modele de langage;reconnaissance de la parole;adaptive method;speech recognition;computational linguistics;statistical language model;linguistique informatique;language model;mathematiques;transcription phonetique;applied linguistics	This paper investigates the problem of updating over time the statistical language model (LM) of an Italian broadcast news transcription system. Statistical adaptation methods are proposed which try to cope with the complex dynamics of news by exploiting newswire texts daily available on the Internet. In particular, contemporary news reports are used to extend the lexicon of the LM, to minimize the out-of-vocabulary (OOV) word rate, and to adapt the n-gram probabilities. Experiments performed on 19 news shows, spanning a period of one month, showed relative reductions of 58% in OOV word rate, 16% in perplexity, and 4% in word error rate (WER).		Marcello Federico;Nicola Bertoldi	2004	Computer Speech & Language	10.1016/j.csl.2003.10.001	natural language processing;speech recognition;complex dynamics;word error rate;computer science;computational linguistics;applied linguistics;linguistics;language model	NLP	-25.719357762761888	-78.94446840387964	6907
4434ef718b44a84c9837e32c4015e891b21fcc33	are interface agents scapegoats? attributions of responsibility in human-agent interaction	interface agents;human computer interaction;interface agent;self serving bias;attribution theory;human agent interaction;computer software;software design;human computer interface	This paper presents an investigation of the self-serving biases of interface agent users. An experiment that involved 202 MS Office users demonstrated that, in contrast to the self-serving hypothesis in attribution theory, people do not always attribute the successful outcomes of human–agent interaction to themselves and negative results to interface agents. At the same time, it was found that as the degree of autonomy of MS Office interface agents increases, users tend to assign more negative attributions to agents under the condition of failure and more positive attributions under the condition of success. Overall, this research attempts to understand the behavior of interface agent users and presents several conclusions that may be of interest to human–computer interaction researchers and software designers working on the incorporation of interface agents in end-user systems. 2006 Elsevier B.V. All rights reserved.	human–computer interaction;intelligent user interface	Alexander Serenko	2007	Interacting with Computers	10.1016/j.intcom.2006.07.005	simulation;human–computer interaction;computer science;software design;attribution;self-serving bias	HCI	-52.712163113177006	-52.25589592739501	6909
bbbd96a516d1929e8561b8f616e7114e7f3f0fa6	a case-based approach using behavioural biometrics to determine a user's stress level		[\]^_`a^ b c/. 9: 49-+5 d3 *+,e.34 +/8934 >934*5 f.+*4f 7,9g*.8 1/ 4f. 1/-<34,1+*1h.C9,*13 >+<3.g5 34,.33 ,.*+4.1**/.33.3 i j 1ef 34,.33 *.2.*3 *.+49 ,.-<>.7,9-<>412145 +/799, f.+*4f i =4,.33 7<43 f1ef 34,+1/ 9/ + 7.,39/ d3 7f5319*9e5 +/C 1** .2./4<+**5 *.+49 + 2+,1.45 9: -1k.,./4 1**/.33.3 ,.l<1,1/e 31> *.+2. +/8.-1>+* 4,.+48 ./4 i m/>,.+31/e /<8g., 9: 7 .97*. <3. >987<4.,3 1/ 4f.1, -+1*5 C9,n B .o8 +1* B +>>9</41/e B C ,141/e ,.79,43 B 7,9e,+88 1/e .4> i p/ 1/>,.+31/e /<8g., 9: 499*3 .q 134 4f+4 <3. 7 .,39/+*1h.418 .,3 49 ,.8 1/+ 7.,39/ 49 4+n. g,+n.3 ,.e<*+,*5i rf13 13 ,+,.*5 3<s >1./4 :9, .s >1./4 34,.33 8 +/+e.8 ./4 i t. 7,9793. + 3534.8 4f+4 89/149,3 + 7 .,39/ d3 34,.33 *.2.* +/C 14f 3< 14+g*. 4.>f/1l<.3 +/8.4f9-3 ,.-<>.3 + 7 .,39/ d3 34,.33 49 3+:. *.2.*3 i u 198 .4,1>+* -+4+ 3<>f +3 n.534,9n. -5/+8 1>3 +,. <3.49-+5 1/ 3.><,145 +77*1>+419/3 :9, 1-./41v>+419/ 9: + 7 .,39/ d3 1-./4145i t. 3<ee.34 4f+4 + 318 1*+, +77,9+>f 13 <3.:9, 34,.33 *.2.* -.4.>419/ i rf13 7+7., 7,9793.3 + ;uw +77,9+>f 49 -.4.,8 1/. 7+41./4 d3 34,.33 *.2.* g+3.<79/ g198 .4,1>+* -+4+ i p 7,949457. +/1/141+* 4.343 3f9C 7,98 131/e ,.3< *43 i rf. 3534.8 >+*1g,+4.3 143.*: 49 4f. <3., <31/e <3., :..-g+>n i	biometrics;like button;radio frequency	Johan Andren;Peter Funk	2005			data mining;computer science;biometrics	Crypto	-56.72520539936325	-57.032021200349114	6954
20c016843430bb65c60ea4011a9e8bbb5785c490	which languages do people speak on flickr?: a language and geo-location study of the yfcc100m dataset	flickr;yfcc100m;language detection;nlp;geo location	Recently, the Yahoo Flickr Creative Commons 100 Million (YFCC100m) dataset was introduced to the computer vision and multimedia research community. This dataset consists of millions of images and videos spread over the globe. This geo-distribution hints at a potentially large set of different languages being used in titles, descriptions, and tags of these images and videos. Since the YFCC100m metadata does not provide any information about the languages used in the dataset, this paper presents the first analysis of this kind. The language and geo-location characteristics of the YFCC100m dataset is described by providing (a) an overview of used languages, (b) language to country associations, and (c) second language usage in this dataset. Being able to know the language spoken in titles, descriptions, and tags, users of the dataset can make language specific decisions to select subsets of images for, e.g., proper training of classifiers or analyze user behavior specific to their spoken language. Also, this language information is essential for further linguistic studies on the metadata of the YFCC100m dataset.	computer vision;emoticon;flickr;geolocation;image;tag (metadata)	Alireza Koochali;Sebastian Kalkowski;Andreas Dengel;Damian Borth;Christian Schulze	2016		10.1145/2983554.2983560	natural language processing;computer science;world wide web;information retrieval	NLP	-21.163131323842904	-60.036788656977635	7002
8f196533fe75fb5daa5b8023b9dcae22b01680bc	individualization, globalization and health - about sustainable information technologies and the aim of medical informatics	health enabling technologies;health informatics;medical informatics;ambient intelligence;pervasive computing;information technology;health information system;health information systems;medical informatic;health care system;health care	This paper discusses aspects of information technologies for health care, in particular on transinstitutional health information systems (HIS) and on health-enabling technologies, with some consequences for the aim of medical informatics. It is argued that with the extended range of health information systems and the perspective of having adequate transinstitutional HIS architectures, a substantial contribution can be made to better patient-centered care, with possibilities ranging from regional, national to even global care. It is also argued that in applying health-enabling technologies, using ubiquitous, pervasive computing environments and ambient intelligence approaches, we can expect that in addition care will become more specific and tailored for the individual, and that we can achieve better personalized care. In developing health care systems towards transinstitutional HIS and health-enabling technologies, the aim of medical informatics, to contribute to the progress of the sciences and to high-quality, efficient, and affordable health care that does justice to the individual and to society, may be extended to also contributing to self-determined and self-sufficient (autonomous) life. Reference is made and examples are given from the Yearbook of Medical Informatics of the International Medical Informatics Association (IMIA) and from the work of Professor Jochen Moehr.	academic medical centers;ambient intelligence;autonomous robot;computation (action);contribution;health care;health information systems;informatics (discipline);information system;medical informatics;patients;personalization;science;ubiquitous computing	Reinhold Haux	2006	International journal of medical informatics	10.1016/j.ijmedinf.2006.05.045	health administration informatics;health policy;health informatics;medicine;ambient intelligence;engineering informatics;knowledge management;hrhis;nursing;management science;informatics;health care	HCI	-57.03394507114144	-61.34985033702128	7036
3c31c1d50a2a70132db19ed357a5eba371bf0bfd	paid review and paid writer detection		There has been a surge in opinion-sharing in the public domain. Some opinions greatly influence our decisions, e.g., the choice of purchase. Malicious parties or individuals exploit social media by generating fake reviews for opinion manipulation. This paper aims to investigate the phenomenon of online paid restaurant reviews by bloggers. Our research provides an insight into some characteristics of paid reviews and their authors. We then explore a set of features based on our observations and detect paid reviews and paid bloggers using supervised machine learning techniques. Experimental results show the effectiveness of our approach.	baseline (configuration management);blog;machine learning;social media;social network;supervised learning	Man-Chun Ko;Hen-Hsen Huang;Hsin-Hsi Chen	2017		10.1145/3106426.3106433	public domain;exploit;social media;public relations;phenomenon;engineering	NLP	-20.02255552788764	-53.439128770471896	7042
06d4a7b932e74bc9cc7dd862126f914565998569	using corpus-derived name lists for named entity recognition	corpus-derived name list;standard test set;sophisticated supervised learning technique;entity recognition system;substantial performance benefit;annotated training data;ne recognition;f-measure score;supervised learning	This paper describes experiments to establish the performance of a named entity recognition system which builds categorized lists of names from manually annotated training data. Names in text are then identi ed using only these lists. This approach does not perform as well as state-of-the-art named entity recognition systems. However, we then show that by using simple ltering techniques for improving the automatically acquired lists, substantial performance bene ts can be achieved, with resulting Fmeasure scores of 87% on a standard test set. These results provide a baseline against which the contribution of more sophisticated supervised learning techniques for NE recognition should be measured.	baseline (configuration management);categorization;experiment;named entity;named-entity recognition;supervised learning;test set	Mark Stevenson;Robert J. Gaizauskas	2000			computer science;pattern recognition;data mining;entity linking;information retrieval	NLP	-23.170701228906342	-71.74203966787607	7071
c74c72f88e5759856d85a35270f6479786321e74	an intelligibility metric based on a simple model of speech communication	measurement;information rates;speech;indexes;production;mutual information	Instrumental measures of speech intelligibility typically produce an index between 0 and 1 that is monotonically related to listening test scores. As such, these measures are dimensionless and do not represent physical quantities. In this paper, we propose a new instrumental intelligibility metric that describes speech intelligibility using bits per second. The proposed metric builds upon an existing intelligibility metric that was motivated by information theory. Our main contribution is that we use a statistical model of speech communication that accounts for noise inherent in the speech production process. Experiments show that the proposed metric performs at least as well as existing state-of-the-art intelligibility metrics.	data rate units;experiment;information theory;intelligibility (philosophy);statistical model	Steven Van Kuyk;W. Bastiaan Kleijn;Richard Christian Hendriks	2016	2016 IEEE International Workshop on Acoustic Signal Enhancement (IWAENC)	10.1109/IWAENC.2016.7602933	natural language processing;speech recognition;mathematics;communication	Vision	-9.90164663441587	-87.73616695437563	7072
3df477fc85b34f67f6b50ca08221ba7f16f2cffc	query expansion based on a semantic graph model	semantic similarity;pseudo relevance feedback;information retrieval;semantic network;word sense disambiguation;semantic information;random walk;mutual information;graph model;query expansion;similarity measure;computational semantics;domain specificity;word graph;uniform distribution	Query expansion is a classical topic in the field of information retrieval, which is proposed to bridge the gap between searchers' information intents and their queries. Previous researches usually expand queries based on document collections, or some external resources such as WordNet and Wikipedia [1, 2, 3, 4, 5]. However, it seems that independently using one of these resources has some defects, document collections lack semantic information of words, while WordNet and Wikipedia may not include domain-specific knowledge in certain document collection. Our work aims to combine these two kinds of resources to establish an expansion model which represents not only domain-specific information but also semantic information. In our preliminary experiments, we construct a two-layer word graph and use Random-Walk algorithm to calculate the weights of each term in pseudo-relevance feedback documents, then select the highest weighted term to expand original query. The first layer of the word graph contains terms in related documents, while the second layer contains semantic senses corresponding to these terms. These terms and semantic senses are treated as vertices of the graph and connected with each other by all possible relationships, such as mutual information and semantic similarities. We utilized mutual information, semantic similarity and uniform distribution as the weight of term-term relation, sense-sense relation and word-sense relation respectively. Though these experiments show that our expansion outperform original queries, we are troubled with some difficult problems.  Given the framework of semantic graph model, we need more effort to find out an optimal graph to represent the relationships between terms and their semantic senses. We utilized a two-layer graph model in our preliminary research, where terms from different documents are treated equally. Maybe we can introduce the document as a third layer in future work, where we can differ the same terms in different documents according to document relevance and context.  Then we need appropriately represent initial weights of this words, senses and relationships. Various measures for weights of terms and term relations have been proved effective in other information retrieval tasks, such as TFIDF, mutual information (MI), but there is little research on weights for semantic senses and their relations. For polysemous words, we add all of their semantic senses to the graph and assume that these senses are uniformly distributed. Actually, it is not precise for a word in a special document and query. As we know, a polysemous word may have only one or two senses in a document, and they are not uniformly distributed. Give a word, what we should do is to determine its word senses in a relevant document and estimate the distribution of these senses. Word sense disambiguation may help us in this problem. Then, there are many methods to compute word similarity according to WordNet, which we use to represent the weights of relationships between word senses. Varelas et al implemented some popular methods to compute semantic similarity by mapping terms to an ontology and examining their relationships in that ontology [4]. We also need to know which algorithm for semantic similarity is most suitable for our model.  Additional, WordNet is suitable to calculate word similarity but not suitable to measure word relevance. The inner hyperlinks of Wikipedia could help us to calculate word relevance. We wish to find an effective way to combine the similarity measure from WordNet and relevance measure from Wikipedia, which may completely reflect word relationships.	algorithm;archive;document;domain-specific language;experiment;hyperlink;information retrieval;mutual information;need to know;norm (social);query expansion;relevance feedback;semantic similarity;similarity measure;tf–idf;vertex (graph theory);wikipedia;word sense;word-sense disambiguation;wordnet	Xue Jiang	2011		10.1145/2009916.2010177	natural language processing;semantic similarity;query expansion;semantic integration;explicit semantic analysis;semeval;computer science;data mining;semantic property;uniform distribution;mutual information;semantic network;world wide web;random walk;information retrieval;statistics;computational semantics	Web+IR	-28.466341395483614	-63.561271313594894	7090
5f12e9f8cc6894abcb6c9bf4a1bf2534128a2dab	hausa large vocabulary continuous speech recognition		Africa is the continent with a second largest number of languages in the world. According to [46], 32.8% of the world languages is spoken in Asia, while 30.3% is spoken in Africa. However, a lot of African languages do not have a developed writing system and for those languages, speech is the only way of communication. As a consequence, the disappearance of the native speakers indirectly implies the disappearance of such languages. Speech technologies such as the Automatic Speech Recognition (ASR) allow the communication across language boundaries and also enable the preservation of the authenticity of languages. On the African continent ASR systems can on the one hand support the communication between speakers of different languages or the communication with machines (e.g computer). On the other hand, ASR can preserve the authenticity of African languages. Thus, the need of ASR systems is essential for African languages. In the last few years, a considerable effort has been made to analyze and develop ASR systems for few of the huge number of African languages. For instance, the Meraca Institute and South African universities spend much effort in investigating speech technologies for languages spoken in the Southern part of the continent [39]. In West Africa, the African Languages Technology Initiative (ALT-i) in Nigeria has been investigating speech technology for Yoruba and Igbo [9]. In this part of the African continent, one of the widely spoken language is the Hausa language. To the best of our knowledge, an ASR system for this language has not been investigated so far. In this research project, we investigate and develop a Large Vocabulary Continuous Speech Recognition (LVCSR) system for the Hausa language. We describe the Hausa language and speech database recently collected as a part of our GlobalPhone corpus. We achieve significant improvements by automatically substituting inconsistent or flawed pronunciation dictionary entries, including tone and vowel length information, applying state-of-the art techniques for acoustic modeling, and crawling large quantities of text material from the Internet for language modeling. A system combination of the best graphemeand phoneme-based 2-pass systems achieves a word error rate of 13.16% on the development set and 16.26% on the test set on read newspaper speech.	acoustic cryptanalysis;acoustic model;automated system recovery;dictionary;language model;speech analytics;speech recognition;speech technology;test set;vocabulary;word error rate	Tim Schlippe;Edy Guevara Komgang Djomgang;Ngoc Thang Vu;Sebastian Ochs;Tanja Schultz	2012			word error rate;the internet;natural language processing;test set;pronunciation;computer science;speech recognition;vocabulary;vowel length;hausa;artificial intelligence;language model	NLP	-25.02361136957634	-81.6498965409221	7094
118887acdc4f8466dbf345356a74966d0ed4838e	translation with source constituency and dependency trees		We present a novel translation model, which simultaneously exploits the constituency and dependency trees on the source side, to combine the advantages of two types of trees. We take head-dependents relations of dependency trees as backbone and incorporate phrasal nodes of constituency trees as the source side of our translation rules, and the target side as strings. Our rules hold the property of long distance reorderings and the compatibility with phrases. Large-scale experimental results show that our model achieves significantly improvements over the constituency-to-string (+2.45 BLEU on average) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets.	bleu;dependency grammar;internet backbone;phylogenetic tree;rule induction	Fandong Meng;Jun Xie;Linfeng Song;Yajuan Lü;Qun Liu	2013			bleu;speech recognition;computer science;algorithm	NLP	-21.048580746334245	-76.53132140123371	7143
b898113e0a67ddfbe7934df6403906debb3026fd	"""erratum to: """"the production and recognition of emotions in speech: features and algorithms"""": [int. j. hum.-comput. stud 59 (2003) 157]"""			algorithm	Pierre-Yves Oudeyer	2005	Int. J. Hum.-Comput. Stud.	10.1016/j.ijhcs.2004.10.001	natural language processing;speech recognition	NLP	-30.62526876559295	-79.26568635669257	7156
955ebc12efa05dad23fd8393f73702984a7f0756	complete syntactic analysis bases on multi-level chunking		This paper describes a complete syntactic analysis system based on multi-level chunking. On the basis of the correct sequences of Chinese words provided by CLP2010, the system firstly has a Part-ofspeech (POS) tagging with Conditional Random Fields (CRFs), and then does the base chunking and complex chunking with Maximum Entropy (ME), and finally generates a complete syntactic analysis tree. The system took part in the Complete Sentence Parsing Track of the Task 2 Chinese Parsing in CLP2010, achieved the F-1 measure of 63.25% on the overall analysis, ranked the sixth; POS accuracy rate of 89.62%, ranked the third.	conditional random field;maximum-entropy markov model;parse tree;part-of-speech tagging;principle of maximum entropy;shallow parsing;transfer matrix;weitao yang	Zhipeng Jiang;Yu Zhao;Yi Guan;Chao Li;Sheng Li	2010			parsing;chunking (psychology);pattern recognition;computer science;artificial intelligence	NLP	-25.51838952836154	-77.3070138027014	7175
06d0c1ef9d980c92264f939dbc661c206c9e5656	construction of an electronic timbre dictionary in c++ for multiple platforms			c++;dictionary	Naotoshi Osaka;Hiromichi Tsuda;Yoshitada Yamada	2013			natural language processing;speech recognition;artificial intelligence;timbre;computer science	ML	-14.98898787405389	-86.07239809538783	7186
ea4d90a9e775506ce798825038a3df489338566f	freeflowdb: storage, querying and interacting with structure-activity information from high-throughput drug discovery	high throughput drug discovery;database system;information assimilation;query processing;drug discovery;database management systems;visualization query facilities freeflowdb high throughput drug discovery structure activity information drug like molecules throughput assays user data interaction paradigms information assimilation drug discovery information database system;medical computing;freeflowdb;drug discovery information database system;throughput assays;query processing database management systems medical computing;drugs biological information theory sun information management data visualization genomics bioinformatics chemistry high temperature superconductors image storage;drug like molecules;user data interaction paradigms;high throughput;visualization query facilities;structure activity information;knowledge discovery	The state of the art in modern drug discovery involves investigating a large number of drug-like molecules using medium or high-throughput assays, often being conducted against multiple targets. Managing the information generated in such processes requires the ability to deal 'with complex, multifarious data as well, as the development of new user-data interaction paradigms that help glean patterns hidden in the multitude of data by emphasizing exploration and information assimilation. This paper describes our research in developing FreeFlowDB, a drug discovery information database system that is geared towards storing both structural as well as high-throughput assay information generated as part of a typical drug discovery process. FreeFlowDB supports powerful structural querying facilities that subsume within a common algorithmic framework exact structural matching, sub-structure querying, and in-exact matching. Furthermore, the system supports unified visualization-query facilities that allow interacting with assay as well as structure-activity information. This allows efficacious and intuitive query-analysis of large, amounts of data for knowledge discovery. Case studies and experimental results demonstrate the capabilities of the system	combinatorial chemistry;data assimilation;database;experiment;file spanning;freedom of information laws by country;high-throughput computing;information management system (ims);information model;information retrieval;information visualization;interaction;programming paradigm;throughput;user interface;virtual screening	Rahul Singh;Elinor Velasquez;Preet Vijayant;Emmanuel R. Yera	2006	19th IEEE Symposium on Computer-Based Medical Systems (CBMS'06)	10.1109/CBMS.2006.89	high-throughput screening;computer science;bioinformatics;data science;data mining;database;knowledge extraction;drug discovery	DB	-5.3984371033174	-62.09773367703171	7228
47d8fd59bea51ac978c2ff56b7aa3b32803b5904	lyria pacs: a case study saves ten million dollars in a brazilian hospital	statistical analysis computerised tomography diagnostic radiography pacs;radiology;pacs;dicom;radiography;picture archiving and communication systems hospitals dicom servers medical diagnostic imaging;radiography radiology dicom pacs;computed tomography brazilian hospital computer systems management analysis imaging analysis lyria pacs system infrastructure resource heterogeneity monetary economy statistics	The use of computer systems for management and imaging analysis has brought many advantages to medical organizations. This paper comprises a survey regarding the Lyria PACS system implemented in 2011 at Hospital das Clinicas, Ribeirao Preto USP - Brazil. It presents the benefits of this system, the main challenges faced during its implementation, the heterogeneity of infrastructure resources and the monetary economy obtained after the installation of Lyria PACS in short and medium periods of time. The results are shown using statistics and growth of use.	physics and astronomy classification scheme	Dhiego Fernandes Carvalho;José Antonio Camacho Guerrero;Paulo Mazzoncini de Azevedo Marques;Alessandra Alaniz Macedo	2015		10.1109/CBMS.2015.87	simulation;radiography;radiology;medicine;dicom;picture archiving and communication system;medical physics	HCI	-51.106940536758806	-61.56196438672631	7252
36688a79cc8926f489ccb6e6dadba15afbb4b6a4	linear discriminant analysis for the small sample size problem: an overview	t technology general;institute for integrated and intelligent systems;faculty of science environment engineering and technology;journal article;080109;pattern recognition and data mining	Dimensionality reduction is an important aspect in the pattern classification literature, and linear discriminant analysis (LDA) is one of the most widely studied dimensionality reduction technique. The application of variants of LDA technique for solving small sample size (SSS) problem can be found in many research areas e.g. face recognition, bioinformatics, text recognition, etc. The improvement of the performance of variants of LDA technique has great potential in various fields of research. In this paper, we present an overview of these methods. We covered the type, characteristics and taxonomy of these methods which can overcome SSS problem. We have also highlighted some important datasets and software/ packages.	algorithm;bioinformatics;bioinformatics;british informatics olympiad;categorization;dexter (malware);dimensionality reduction;document classification;facial recognition system;linear discriminant analysis;local-density approximation;optical character recognition;sparse matrix;text corpus	Alok Sharma;Kuldip K. Paliwal	2015	Int. J. Machine Learning & Cybernetics	10.1007/s13042-013-0226-9	speech recognition;computer science;data science;data mining	AI	-9.945257015119392	-64.2138855191659	7259
aa0dd36abdd02d5b819b2da7f64036c1847aeedf	sentiment analysis of social networks statements for the polish language	social network services;computers;testing;internet;dictionaries;sentiment analysis;tagging	The purpose of this paper is to adapt a few sentiment analysis methods to obtain sentiment value for social network statements in the Polish language. Developed methods represent different approaches. Starting with PMI-IR, expansion of dictionary through conjunctive connections method, to determining bigram sentiment by analyzing its neighborhood. Obtained results were compared to the commercial solution - brand24.pl.	bigram;dictionary;sentiment analysis;social network	Lukasz Culer;Olgierd Unold	2016	2016 Third European Network Intelligence Conference (ENIC)	10.1109/ENIC.2016.027	natural language processing;computer science;data mining;world wide web;sentiment analysis	NLP	-22.701308913201757	-61.08116453805829	7271
ce0e9e71a0b09cb9227296ceec26c8654a58e5c9	on-demand injection of lexical knowledge for recognising textual entailment		We approach the recognition of textual entailment using logical semantic representations and a theorem prover. In this setup, lexical divergences that preserve semantic entailment between the source and target texts need to be explicitly stated. However, recognising subsentential semantic relations is not trivial. We address this problem by monitoring the proof of the theorem and detecting unprovable sub-goals that share predicate arguments with logical premises. If a linguistic relation exists, then an appropriate axiom is constructed on-demand and the theorem proving continues. Experiments show that this approach is effective and precise, producing a system that outperforms other logicbased systems and is competitive with state-of-the-art statistical methods.	automated theorem proving;effective method;experiment;formal system;sensor;standalone program;textual entailment	Yusuke Miyao;Pascual Martínez-Gómez;Koji Mineshima;Daisuke Bekki	2017			textual entailment;natural language processing;computer science;artificial intelligence;linguistics	AI	-27.02057541877339	-72.11479022526484	7327
3d28257c0aea633fa51ff85ca2b64ea3bfbcfcb0	comparing distributional semantics models for identifying groups of semantically related words	info eu repo semantics article;cluto;agrupacion semantica de palabras;semantic grouping;dsm;word2vec	Distributional Semantic Models (DSM) are growing in popularity in Computational Linguistics. DSM use corpora of language use to automatically induce formal representations of word meaning. This article focuses on one of the applications of DSM: identifying groups of semantically related words. We compare two models for obtaining formal representations: a well known approach (CLUTO) and a more recently introduced one (Word2Vec). We compare the two models with respect to the PoS coherence and the semantic relatedness of the words within the obtained groups. We also proposed a way to improve the results obtained by Word2Vec through corpus preprocessing. The results show that: a) CLUTO outperforms Word2Vec in both criteria for corpora of medium size; b) The preprocessing largely improves the results for Word2Vec with respect to both criteria.	algorithm;common criteria;complex systems;computational linguistics;display resolution;distributional semantics;emoticon;experiment;lemmatisation;part-of-speech tagging;preprocessor;semantic similarity;skip list;text corpus;word2vec	Venelin Kovatchev;Maria Salamó;Maria Antònia Martí	2016	Procesamiento del Lenguaje Natural		natural language processing;computer science;linguistics	NLP	-25.53799417075416	-73.461959003036	7340
33544abc76ed70eead3e46edc3a3eadb53e091e4	kd strikes back: from keyphrases to labelled domains using external knowledge sources		English. This paper presents L-KD, a tool that relies on available linguistic and knowledge resources to perform keyphrase clustering and labelling. The aim of L-KD is to help finding and tracing themes in English and Italian text data, represented by groups of keyphrases and associated domains. We perform an evaluation of the top-ranked domains using the 20 Newsgroup dataset, and we show that 8 domains out of 10 match with manually assigned labels. This confirms the good accuracy of this approach, which does not require supervision. Italiano. In questo lavoro descriviamo LKD, un sistema che utilizza risorse linguistiche e basate su conoscenza per ragruppare concetti-chiave e categorizzarli. L’obiettivo di L-KD è quello di supportare gli utenti nel rilevare la presenza di specifici temi in documenti italiani e inglesi, rappresentandoli attraverso gruppi di concetti-chiave e relativi domini. Abbiamo valutato l’affidabilità del sistema analizzando i domini più rilevanti nel 20 Newsgroup dataset, e dimostrando che 8 su 10 domini nel gold standard sono assegnati correttamente anche dal sistema. Questa valutazione conferma le buone performance di L-KD, senza il bisogno di	cluster analysis;harsh realm;linear algebra;text corpus	Giovanni Moretti;Rachele Sprugnoli;Sara Tonelli	2016			cognitive science;artificial intelligence;sociology	NLP	-27.10727568918426	-72.67841972881487	7342
71913bff1e448c0e84353e611b193184ed6915bf	characteristics of tissue-centric biomedical researchers using a survey and cluster analysis	database design;cluster analysis;information needs	The objective of this study was to characterize the types of tissue-centric users based on tissue use, requirements, and their job or work-related variables at the University of Pittsburgh Medical Center (UPMC), Pittsburgh, PA. A self-reporting questionnaire was distributed to biomedical researchers at the UPMC. Descriptive and cluster analyses were performed to identify and characterize the complex types of tissue-based researchers. A total of 62 respondents completed the survey, and two clusters were identified based on all variables. Two distinct groups of tissue-centric users made direct use of tissue samples for their research as well as associated information, while a third group of indirect users required only the associated information. The study shows that tissue-centric users were composed of various types. These types were distinguished in terms of tissue use and data requirements, as well as by their work or research-related activities. © 2008 Wiley Periodicals, Inc.	cluster analysis	Sujin Kim;Edie M. Rasmussen	2008	JASIST	10.1002/asi.20807	questionnaire;information needs;scientific method;computer science;cluster analysis;operations research;database design;statistics	SE	-56.23946202242649	-68.86637796519896	7348
6205793621c7e93b753679af4fb8f814432d7582	now i like it, now i don't: delay effects and retrospective judgment		The present paper tests the widely accepted hypothesis that on-line judgment implies functional independence between memory for, and judgment of, verbal stimuli (e.g., Anderson, 1989; Hastie & Park, 1986). In the present study, participants recalled lists of words, after having assessed each for its pleasantness. Presentation position of a negative item within the lists was manipulated. Also, items memorability was manipulated after their presentation – by inserting a filled delay between presentation and the judgment task; in this way, on-line judgment formation was spared. The memory manipulation reduced recall rates for negative items presented in the last position – and their negative influence on pleasantness ratings accordingly. These results contradict the predictions of pure on-line approaches to judgment formation (e.g., Betsch, Plessner, Schwieren, & Gütig, 2001) and suggest that even in on-line judgment tasks, memory plays a	online and offline	Silvio Aldrovandi;Marie Poirier;Petko Kusev;Daniel Heussen;Peter Ayton	2011				HCI	-51.46452459745454	-53.98809515013483	7382
ba8300311330e95148ad5ca8b990dc2501b7d56b	evaluation of piano key vibrations among different acoustic pianos and relevance to vibration sensation		"""Recent studies suggest that vibration of piano keys affect the perceived quality of the instrument, as well as the dynamic control and timing in piano playing. However, the time signals of piano key vibrations and its physical properties have not been analyzed and compared to the threshold of vibration sensation in a real-life playing situation yet. This study investigates piano key vibrations and explores the diversity of vibrations among different pianos with a laser Doppler vibrometer. A pianist was performing single keystrokes, note sequences, and a music piece excerpt on four concert grand pianos, five grand pianos, and two upright pianos. The measurements showed peak displacement levels up to 80 <inline-formula><tex-math notation=""""LaTeX"""">$\mu$</tex-math><alternatives> <inline-graphic xlink:href=""""fluckiger-ieq1-2773099.gif""""/></alternatives></inline-formula>m and the frequency spectrum of the vibrations is dominated by frequencies lower than 500 Hz. Finally, a frequency weighting filter is introduced to show that vibration displacement time signals exceed the threshold of human vibration sensation for all evaluated instruments, when a note sequence is played in the bass to mid range with a single hand at forte level. The conducted experiments demonstrate that the vibration characteristics vary distinctively among the investigated pianos."""	acoustic cryptanalysis;basolateral sorting signal;beneath a steel sky;code for the japanese graphic character set for information interchange (jis x 0208-1990),;displacement mapping;doppler echocardiography;eighty;entity handling - upright;event (computing);experiment;forte 4gl;hertz (hz);instrument - device;key (instrument);physical phenomenon or property;psychologic displacement;real life;relevance;sensation disorders;spectral density;vibration - physical agent;xlink	Matthias Fl&#x00FC;ckiger;Tobias Großhauser;Gerhard Tr&#x00F6;ster	2018	IEEE Transactions on Haptics	10.1109/TOH.2017.2773099	piano;piano playing;laser doppler vibrometer;sensation;acoustics;weighting filter;frequency spectrum;speech recognition;computer science;vibration	Mobile	-5.395673456371968	-82.72017643256687	7397
a8353f8ced9fb3f72e3f81173792e90dca81e022	extracting topic changes through word co-occurrence graphs from japanese subtitles of vod lecture	databases;graph theory;electronic learning;standards;video signal processing computer based training educational institutions graph theory information retrieval internet natural language processing text analysis video on demand;video signal processing;search engines;information retrieval;search engines electronic learning databases educational institutions engines standards search methods;search methods;text analysis;topic search;extracting topic e learning topic search;internet;engines;video on demand;e learning;computer based training;video on demand topic change extraction word cooccurrence graph japanese subtitle vod lecture university e learning system blended learning wbt web based training classroom instruction e learning lecture lecture video topic detection search facility;extracting topic;natural language processing	A number of universities currently utilize VOD in e-Learning systems, a form of blended learning that combines WBT (Web-Based Training) with classroom instruction. Many universities also offer e-Learning lectures, which offer slides and lecture video with VOD. However, because lectures that use standard VOD focus on letting students take the lecture, most do not include advanced search methods or implement topic detection or extraction. To aid for e-Learning users, we have been developing search facilities for VOD lectures. For this purpose, this study proposes a method of extracting the topic by creating a graph of keywords and their co-occurrence words from the lecture's subtitles and then extracting from variations in the word the co-occurrence graphs between sentences.	centrality;graph (discrete mathematics);sensor;tf–idf;versant object database	Nobuyuki Kobayashi;Noboru Koyama;Hiromitsu Shiina;Fumio Kitagawa	2012	2012 IIAI International Conference on Advanced Applied Informatics	10.1109/IIAI-AAI.2012.71	computer science;multimedia;world wide web;information retrieval	DB	-40.34459893361793	-66.94297885953726	7401
487518ced97b4104b08b11133971ca3e9d9d3e02	a minimal-resource transliteration framework for vietnamese		Transliteration converts words in a source language (e.g., English) into phonetically equivalent words in a target language (e.g., Vietnamese). Transliteration is therefore used to handle out-of-vocabulary (OOV) words adopted from foreign languages in automatic speech recognition and keyword search systems. While statistical transliteration approaches have been widely adopted, they may not always be suitable for underresourced languages, where training data is scarce. In this work, we present a rule-based Vietnamese transliteration framework suitable for spoken language applications with minimal linguistic resources. We show that the proposed system outperforms statistical baselines by up to 81.70% relative when there is limited training examples (94 word pairs). In addition, we investigate the trade-off between training corpus size and transliteration performance of different methods on two distinct corpora. We also show that the proposed model outperforms statistical baselines up to 36.76% relative in keyword search tasks.	compiler;logic programming;search algorithm;speech recognition;statistical model;text corpus;vocabulary	Hoang Gia Ngo;Nancy F. Chen;Sunil Sivadas;Bin Ma;Haizhou Li	2014			vietnamese;speech recognition;transliteration;computer science	NLP	-22.107449166466488	-79.44653474660913	7432
0e055f01c7a81c23cc2a2bba6fbdcbf6f5f70b2b	can skos ontologies improve the accuracy of measuring semantic similarity of purchase orders?		The effect of additional domain knowledge provided by a SKOS ontology on the accuracy of semantic similarity calculated from product item lists in purchase orders for a manufacturer of modular building parts is examined. The accuracy of the calculated semantic similarities is evaluated against attribute information of the purchase orders, under the assumption that orders with similar attributes, such as the industrial type of the purchasing entities and the type of application of the modular building, will have similar lists of items. When all attributes of the purchase orders are weighted equally, the SKOS ontology does not appear to increase the accuracy of the calculated item list similarities. However, when only the two attributes that give the highest correlation to item list similarity values are used, the strongest correlation between item list similarity and entity attribute similarity is obtained when the SKOS-ontology is included in the calculation. Still, even the best correlation between item list and entity attribute similarities yields a correlation coefficient of less than 0.01. It is suggested that inclusion of semantic knowledge about the relationship between the set of items in the purchase orders, e.g. via the use of description logics, might increase the accuracy of the calculated semantic similarity values.	coefficient;description logic;entity;ontology (information science);purchasing;semantic similarity;simple knowledge organization system	Steven B. Kraines	2014		10.5220/0005074702480255		AI	-29.18318707097823	-60.44399328090209	7438
50ae082b0c03c6d2447599cc74e2c8c9683d1d23	improving autoregressive hidden markov model recognition accuracy using a non-linear frequency scale with application to speech enhancement.	hidden markov model;speech enhancement	A new method to improve the accuracy of Autoregres-sive Hidden Markov Model (AR-HMM) based recognition systems is proposed. The technique uses the bilin-ear transform to warp the frequency scale of the observation vectors, hence it uses a better perceptual measure to compare the observation vectors to the trained models. Results presented for the E-set letters from the ISOLET database and the rst speaker dependent task of the Resource Management (RM) database show that this technique improves recognition accuracy considerably. However , in the case of the RM system, the recognition results still fall short of those obtained from a similar mel-frequency cepstral (MFCC) based system without delta parameters. Reasons for the inferior performance of the AR-HMM system are proposed and future research directions are suggested. The models built for the RM task are incorporated into an existing enhancement algorithm to form a large vocabulary speaker dependent enhancement system. Preliminary results are presented for this system.	algorithm;ar (unix);autoregressive model;cepstrum;hidden markov model;markov chain;nonlinear system;speech enhancement;vocabulary	Beth Logan;Anthony J. Robinson	1997			forward algorithm;maximum-entropy markov model;speech recognition;viterbi algorithm;machine learning;pattern recognition;markov model;hidden markov model	Robotics	-12.899413282608679	-90.26793953534117	7442
860a353feaa14bf394e813bac29c5b1d6cee2fd9	comprehensive support for chemistry computations in pl-grid infrastructure		InSilicoLab for Chemistry with its experiments and QC Ad- visor are the tools assisting PL-Grid users in chemistry computations on grid infrastructure. The tools are designed to help the user at all stages of calculations - from the software choice and input data preparation, through job submission and monitoring to the retrieval of output files and analysis of results. General Quantum Chemistry experiment helps in launching QC computations on PL-Grid. A specialized tool - Trajec- tory Sculptor - is designed for manipulations with Molecular Dynamics trajectories and large sets of molecular structures in sequential compu- tational experiments. QC Advisor collects information about availability of different computational methods in quantum-chemical programs and supports preparation of input files for the most popular software. The main idea behind the tools described in the paper is to reduce the effort needed to set-up the calculations, allowing users to focus on scientific content of their work.	polish grid infrastructure pl-grid	Andrzej Eilmes;Mariusz Sterzel;Tomasz Szepieniec;Joanna Kocot;Klemens Noga;Maciej Golik	2014		10.1007/978-3-319-10894-0_18	computational science;computer science;theoretical computer science	HPC	-7.037149152710606	-57.57676250675438	7459
4cc07c79ca4922938e0ef29baedf50b4e251770f	fieldtrip: open source software for advanced analysis of meg, eeg, and invasive electrophysiological data	software;electrophysiological phenomena;article letter to editor;numerical analysis computer assisted;magnetoencephalography;humans;user computer interface;electroencephalography;open source software	This paper describes FieldTrip, an open source software package that we developed for the analysis of MEG, EEG, and other electrophysiological data. The software is implemented as a MATLAB toolbox and includes a complete set of consistent and user-friendly high-level functions that allow experimental neuroscientists to analyze experimental data. It includes algorithms for simple and advanced analysis, such as time-frequency analysis using multitapers, source reconstruction using dipoles, distributed sources and beamformers, connectivity analysis, and nonparametric statistical permutation tests at the channel and source level. The implementation as toolbox allows the user to perform elaborate and structured analyses of large data sets using the MATLAB command line and batch scripting. Furthermore, users and developers can easily extend the functionality and implement new algorithms. The modular design facilitates the reuse in other software packages.	algorithm;command-line interface;electroencephalography;frequency analysis;high- and low-level;matlab;magnetoencephalography;modular design;open-source software;resampling (statistics);reuse (action);time–frequency analysis;usability	Robert Oostenveld;Pascal Fries;Eric Maris;Jan-Mathijs Schoffelen	2011		10.1155/2011/156869	psychology;neuroscience;simulation;electroencephalography;computer science;bioinformatics;theoretical computer science;machine learning;data mining;magnetoencephalography	SE	-7.312777864591937	-59.436206163082495	7489
d74756a4860efe80314a2146bdff94a37864f06e	construction of domain dictionary for fundamental vocabulary	is-a relation;japanese fundamental word;japanese domain resource;domain dictionary;document collection;blog categorization;natural language understanding;domain relation;deeper natural language understanding;fundamental vocabulary;semantic relation;it	For natural language understanding, it is essential to reveal semantic relations between words. To date, only the IS-A relation has been publicly available. Toward deeper natural language understanding, we semiautomatically constructed the domain dictionary that represents the domain relation between Japanese fundamental words. This is the first Japanese domain resource that is fully available. Besides, our method does not require a document collection, which is indispensable for keyword extraction techniques but is hard to obtain. As a task-based evaluation, we performed blog categorization. Also, we developed a technique for estimating the domain of unknown words.	ambiguous grammar;archive;blog;categorization;data dictionary;dictionary;keyword extraction;natural deduction;natural language understanding;relation (database);vocabulary	Chikara Hashimoto;Sadao Kurohashi	2007			natural language processing;speech recognition;computer science;linguistics;information retrieval	NLP	-29.180575746901567	-68.03301610966862	7491
d0871009d11ca29abaacd5f06311d5c07da82069	calibrating information users' views on relevance: a social representations approach	core periphery analysis;data collection;search algorithm;internal structure;social representation;relevance;structural analysis;relevance criteria;academic libraries;similarity measure;social representations;structure analysis	The purpose of this study is to investigate how information users view the concept of relevance and make their judgement(s) on relevant information through the framework of social representations theory. More specifically, this study attempts to address the questions of what users view as the constituent concepts of relevance, what are core and peripheral concepts of relevance, and how these concepts are structured by applying a structural analysis approach of social representations theory. We employ a free word association method for data collection. Two hundred and forty four information users of public and academic libraries responded to questionnaires on their relevance judgement criteria. Collected data were content analysed and assessed using weighted frequency, similarity measure, and core/periphery measurements to identify key elements of relevance and to differentiate core and periphery elements of relevance. Results show that four out of 26 emerged elements (concepts) are core and 22 are periphery elements of the concept of relevance. The findings of this study provide a quantitative measure of weighing various elements of relevance and the internal structure of the concept of relevance from users’ perspectives providing enhancements for search algorithms with quantitative metadata support.	library (computing);peripheral;relevance;search algorithm;similarity measure;structural analysis	Boryung Ju;Myke Gluck	2011	J. Information Science	10.1177/0165551511412030	computer science;knowledge management;data mining;structural analysis;law;world wide web;information retrieval;statistics	DB	-40.20484930106922	-57.67647517283049	7534
eb22b0b6a5c6b13e59c08360f05bad2018ca1cc9	a multilingual to polyglot speech synthesizer for indian languages using a voice-converted polyglot speech corpus		A multilingual synthesizer synthesizes speech, for any given monolingual or mixed-language text, that is intelligible to human listeners. The necessity for such synthesizer arises in a country like India, where multiple languages coexist. For the current work, multilingual synthesizers are developed using HMM-based speech synthesis technique. However, for a mixed-language text, the synthesized speech shows speaker switching at language switching points which is quite annoying to the listener. This is due to the fact that, speech data used for training is collected for each language from a different (native) speaker. To overcome the speaker switching at language switching points, a polyglot speech synthesizer is developed using polyglot speech corpus (all the speech data in a single speaker’s voice). The polyglot speech corpus is obtained using cross-lingual voice conversion (CLVC) technique. In the current work, polyglot synthesizer is developed for five languages namely Tamil, Telugu, Hindi, Malayalam and Indian English. The regional Indian languages considered are acoustically similar, to certain extent, and hence, common phoneset and question set is used to build the synthesizer. Experiments are carried out by developing various bilingual polyglot synthesizers to choose the language (thereby the speaker) that can be considered as target for polyglot synthesizer. The performance of the synthesizers is evaluated subjectively for speaker/language switching using perceptual test and quality using mean opinion score. Speaker identity is evaluated objectively using a GMM-based speaker identification system. Further, the polyglot synthesizer developed using polyglot speech corpus is compared with the adaptation-based polyglot synthesizer, in terms of quality of the synthesized speech and amount of data required for adaptation and voice conversion. It is observed that the performance of the polyglot synthesizer developed using polyglot speech corpus obtained from CLVC technique is better or almost similar to that of the adaptation-based polyglot synthesizer.	speech corpus;speech synthesis	P. Vijayalakshmi;B. Ramani;M. P. Actlin Jeeva;T. Nagarajan	2018	CSSP	10.1007/s00034-017-0659-6	malayalam;mean opinion score;polyglot;telugu;natural language processing;speech corpus;hidden markov model;indian english;speech recognition;artificial intelligence;computer science;speech synthesis	NLP	-18.644952284059602	-84.46539587361401	7539
6ef6027647a0b2f5d7dc868d8f5ab63d2e20faf3	investigation of speaker group-dependent modelling for recognition of affective states from speech		For successful human–machine-interaction (HCI) the pure textual information and the individual skills, preferences, and affective states of the user must be known. Therefore, as a starting point, the user’s actual affective state has to be recognized. In this work we investigated how additional knowledge, for example age and gender of the user, can be used to improve recognition of affective state. Two methods from automatic speech recognition are used to incorporate age and gender differences in recognition of affective state: speaker group-dependent (SGD) modelling and vocal tract length normalisation (VTLN). The investigations were performed on four corpora with acted and natural affected speech. Different features and two methods of classification (Gaussian mixture models (GMMs) and multi-layer perceptrons (MLPs)) were used. In addition, the effects of channel compensation and contextual characteristics were analysed. The results are compared with our own baseline results and with results reported in the literature. Two hypotheses were tested. First, incorporation of age information further improves speaker group-dependent modelling. Second, acoustic normalization does not achieve the same improvement as achieved by speaker group-dependent modelling, because the age and gender of a speaker affects the way emotions are expressed.	acoustic cryptanalysis;baseline (configuration management);database normalization;emotion recognition;experiment;heart rate variability;human–computer interaction;little man computer;microsoft outlook for mac;mixture model;multilayer perceptron;rasta filtering;spatial variability;speech recognition;spontaneous order;test data;text corpus;tract (literature)	Ingo Siegert;David Philippou-Hübner;Kim Hartmann;Ronald Böck;Andreas Wendemuth	2014	Cognitive Computation	10.1007/s12559-014-9296-6	psychology;natural language processing;speaker recognition;speaker diarisation;speech recognition;communication	NLP	-12.46932848063588	-84.89035030314479	7545
2cdd6ff06d764de16e5e4f3c307975ec1455207d	learning patient similarity using joint distributed embeddings of treatment and diagnoses		We propose the use of vector-based word embedding models to learn a cross-conceptual representation of medical vocabulary. The learned model is dense and encodes useful knowledge from the training concepts. Applying the embedding to the concepts of diagnoses and medications, we then show that they can then be used to measure similarities among patient prescriptions, leading to the discovery of informative and intuitive relationships between patients.	domain driven data mining;grammar-based code;informatics;information;personalization;vocabulary;word embedding	Christopher J Ormandy;Zina M. Ibrahim;Richard J. B. Dobson	2017			word embedding;medical diagnosis;natural language processing;formative assessment;machine learning;computer science;embedding;vocabulary;artificial intelligence	ML	-47.757419209178344	-67.28611228583712	7548
48a4f2857ad02d3cc523033c71eaa3d8ab36409e	modeling human motion from video for use in gait analysis			gait analysis	Rick Parent;Kathy A. Johnson;Jianxiang Chang;Xiaoning Fu;Suba Varadarajan	1999			data science;gait analysis;information retrieval;text mining;computer science	Vision	-24.659359569088696	-60.748832414345856	7569
39dc9edc1edc3503e17b94750b8a00830d8bd349	improving the accuracy of co-citation clustering using full text	vertical science platform;citation analysis;citation networks;research paper;patents;research platform;full text databases;journals;researchers network	Historically, co-citation models have been based only on bibliographic information. Full text analysis offers the opportunity to significantly improve the quality of the signals upon which these co-citation models are based. In this work we study the effect of citation proximity on the accuracy of co-citation clusters. Using a corpus of 270,521 full text documents from 2007, we compare the results of traditional co-citation clustering using only the bibliographic information to results from co-citation clustering where proximity between reference pairs is factored into the pairwise relationships. We find that accounting for reference proximity from full text can increase the textual coherence (a measure of accuracy) of a co-citation cluster solution by 9-20% over the traditional approach based on bibliographic information. Introduction For the past 45 years, models of the scientific literature have relied solely on bibliographic information. In general, researchers have utilized the very large bibliographic databases, such as the Web of Science, Scopus and Medline, to create many different types of document-level models of the scientific literature. These models have been developed using a variety of citationbased (co-citation, bibliographic coupling, and direct citation) and text-based (e.g., co-word, LSA, topic modelling) methodologies. Hybrid methods (using both citation and textual data) have also been recently been proposed and tested. The fact that these models have been largely limited to the use of bibliographic data is historical—it goes back to the days when very little information about the published literature was available in digital form and computing capabilities were meager. Even as more information has become available electronically and computing capabilities have greatly increased, most studies have continued to be based solely on bibliographic data. We suppose that this is due to a variety of factors—an existing comfort level gained by decades of working with bibliographic data, the existence of methods to work with bibliographic data, and the lack of access to full text of scientific articles being primary among them. 1 This research is supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center (DoI/NBC) contract number D11PC20152. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/NBC, or the U.S. Government. We also gratefully acknowledge Elsevier BV for providing the full text data used in this research.	authorization;bibliographic coupling;bibliographic database;cluster analysis;co-citation;medline;scientific literature;scopus;text corpus;text-based (computing);topic model;web of science;world wide web	Kevin W. Boyack;Henry G. Small;Richard Klavans	2013	JASIST	10.1002/asi.22896	computer science;data science;data mining;citation analysis;world wide web;information retrieval	Web+IR	-41.364694138650336	-65.24891787231279	7582
0e06f8158e3d48f27c1ace41700fa14b8eda5e61	communication-efficient multi-view keyframe extraction in distributed video sensors	video streaming distributed algorithms feature extraction image sensors video signal processing;k farthest neighbors communication efficient multiview keyframe extraction distributed video sensors security monitoring home care communication bandwidth constraint server storage constraint multiview video summarization video streams video data process distributed multiview summarization system maximal marginal relevance ms wave bandwidth efficient distributed algorithm k nearest neighbors;streaming media sensors servers cameras bandwidth visualization educational institutions	Video sensors are widely used in many applications such as security monitoring and home care. However, the growth of the number of sensors makes it impractical to stream all videos back to a central server for further processing, due to communication bandwidth and server storage constraints. Multi-view video summarization allows us to discard redundant data in the video streams taken by a group of sensors. All prior multi-view summarization methods, however, process video data in an off-line and centralized manner, which means that all videos are still required to be streamed back to the server before conducting the summarization. This paper proposes an on-line, distributed multi-view summarization system, which integrates the ideas of Maximal Marginal Relevance (MMR) and MS-Wave, a bandwidth-efficient distributed algorithm for finding k-nearest-neighbors and k-farthest-neighbors. Empirical studies show that our proposed system can discard redundant videos and keep important keyframes as effectively as centralized approaches, while transmitting only 1/6 to 1/3 as much data.	centralized computing;distributed algorithm;information extraction;k-nearest neighbors algorithm;key frame;marginal model;maximal set;multi-master replication;online and offline;relevance;server (computing);streaming media;transmitter;video sensor technology;visual sensor network	Shun-Hsing Ou;Yu-Chen Lu;Jui-Pin Wang;Shao-Yi Chien;Shou-de Lin;Mi-Yen Yeti;Chia-han Lee;Phillip B. Gibbons;V. Srinivasa Somayazulu;Yen-Kuang Chen	2014	2014 IEEE Visual Communications and Image Processing Conference	10.1109/VCIP.2014.7051492	real-time computing;computer science;automatic summarization;video tracking;multimedia;video processing;world wide web	Vision	-8.852562768835796	-63.827753910971374	7583
b94c6d677b496b97ce6b96dd657c5fd9f0881aba	an ontology-supported and fully-automatic annotation technology for semantic portals	semantic indexing;intelligent interfaces;automatic webpage annotation;keyword search;natural language;interaction pattern;automatic annotation;internet applications;semantic portal;part of speech;internet application;ontology construction;ontology	We employ the techniques of ontology and linguistics to develop a fully-automatic annotation technique, which, when coupled with an automatic ontology construction method, can play a key role in the development of semantic portals. Based on this technique, we also demonstrate a semanticportal prototype which defines how a semantic portal is interacting with the user by providing five different types of interaction patterns, including keyword search, synonym search, POS (Part-of-Speech)-constrained keyword search, natural language query, and semantic index search. Our primarily demonstrations show that it can indeed retrieve better semantic-directed information to meet user requests.	interaction;natural language user interface;portals;prototype;search algorithm	Sheng-Yuan Yang	2007		10.1007/978-3-540-73325-6_116	upper ontology;semantic similarity;semantic computing;semantic search;part of speech;computer science;ontology;semantic web stack;database;ontology-based data integration;natural language;temporal annotation;world wide web;information retrieval	Web+IR	-29.980629476963845	-57.379307136169864	7605
052e605f925c477c37761b0f53675b703380efa2	learning language from a large (unannotated) corpus		A novel approach to the fully automated, unsupervised extraction of dependency grammars and associated syntax-to-semantic-relationship mappings from large text corpora is described. The suggested approach builds on the authors’ prior work with the Link Grammar, RelEx and OpenCog systems, as well as on a number of prior papers and approaches from the statistical language learning literature. If successful, this approach would enable the mining of all the information needed to power a natural language comprehension and generation system, directly from a large, unannotated corpus.	dependency grammar;link grammar parser;list comprehension;natural language;opencog;powera;text corpus	Linas Vepstas;Ben Goertzel	2014	CoRR		natural language processing;speech recognition;computer science;machine learning;linguistics	NLP	-25.508381225284648	-74.40703490302553	7625
20df3eba00d2371c9d84eba67a77507ba43fca22	query-focused extractive video summarization		Video data is explosively growing. As a result of the “big video data”, intelligent algorithms for automatic video summarization have (re-)emerged as a pressing need. We develop a probabilistic model, Sequential and Hierarchical Determinantal Point Process (SH-DPP), for query-focused extractive video summarization. Given a user query and a long video sequence, our algorithm returns a summary by selecting key shots from the video. The decision to include a shot in the summary depends on the shot’s relevance to the user query and importance in the context of the video, jointly. We verify our approach on two densely annotated video datasets. The query-focused video summarization is particularly useful for search engines, e.g., to display snippets of videos.	algorithm;automatic summarization;digital photo professional (dpp);digital video;point process;relevance;statistical model;web search engine	Aidean Sharghi;Boqing Gong;Mubarak Shah	2016		10.1007/978-3-319-46484-8_1	computer science;automatic summarization;video tracking;pattern recognition;multimedia;world wide web;information retrieval	Vision	-17.872617847279262	-57.86943630149321	7637
37a4a89419701f03aaad38c8461870ddd188d144	a corpus-based approach to expressive speech synthesis		Human speech communication can be thought of as comprising two channels – the words themselves, and the style in which they are spoken. Each of these channels carries information. Todayu0027s most-advanced text-to-speech (TTS) systems such as [1],[2],[3],[4] fall far short of human speech because they offer only a single, fixed style of delivery, independent of the message. In this paper, we describe the IBM Expressive TTS Engine, which is able to add another channel by offering five speaking styles. These are: neutral declarative, conveying good news, conveying bad news, asking a question, and showing contrastive emphasis. In addition to generating speech in these five styles, our TTS system is also able to generate paralinguistic events such as sighs, breaths, and filled pauses which further enrich the style channel. We describe our methods for generating and evaluating expressive synthetic speech and paralinguistic effects. We show significant perceptual differences between expressive and neutral synthetic speech for each of our speaking styles. In addition, we describe how users have been empowered to easily communicate the desired expression to the TTS engine through our extensions [5] of the Speech Synthesis Markup Language (SSML) [6].	speech synthesis;text corpus	Ellen Eide;Andrew Aaron;Raimo Bakis;Wael Hamza;Michael Picheny;John F. Pitrelli	2004			speech synthesis markup language;paralanguage;speech recognition;speech corpus;perception;speech synthesis;communication channel;computer science	Logic	-25.225445434208947	-86.6064224113445	7671
be26b5d9fbe30f468c70aa685936501c33585db0	a method for automatic extraction of multiword units representing business aspects from user reviews	information retrieval;text processing;natural language processing	The paper describes a semi-supervised approach to extracting multiword aspects of user-written reviews that belong to a given category. The method starts with a small set of seed words representing the target category, and calculates distributional similarity between the candidate and seed words. We compare three distributional similarity measures (Lin’s, Weeds’ and balAPinc), and a document retrieval function, BM25, adapted as a word similarity measure. We then introduce a method for identifying multiword aspects by using a combination of syntactic rules and a co-occurrence association measure. Finally, a method for ranking multiword aspects by the likelihood of belonging to the target aspect category is described. The task used for evaluation is extraction of restaurant dish names from a corpus of restaurant reviews.	algorithm;bottom-up parsing;coefficient;dependency relation;document retrieval;entity;iteration;knowledge base;logic programming;phrase structure rules;pointwise mutual information;semantic similarity;semi-supervised learning;semiconductor industry;similarity learning;similarity measure;syntactic predicate;text corpus;top-down and bottom-up design;user review	Olga Vechtomova	2014	JASIST	10.1002/asi.23052	natural language processing;computer science;data mining;information retrieval	NLP	-25.566689565872313	-69.17387867006849	7723
9f579b91f0fbd6a48be352704bb90a8089df7579	clustering tweets using cellular genetic algorithm	cellular genetic algorithm;clustering;tweet similarity;twitter	As the popularity of Twitter continues to increase rapidly, it is extremely necessary to analyze the h uge amount of data that Twitter users generate. A popul ar method of tweet analysis is clustering. Because most tweets are textual, this study focuses on clusterin g tweets based on their textual content similarity. This study presents tweet clustering using cellular gene tic algorithm cGA. The results obtained by cGA are compared with those obtained by generational geneti c algorithm in terms of average fitness, average ti me required for execution and number of generations. E xperimental results are tested with two sets: One o f 1000 tweets and the second formed of 5000 tweets. T he results show a nearly equal performance for both algorithms in terms of the average fitness of the s olution. On the other hand, cGA shows a much faster performance than generational. These results demonstrate that cellular genetic algorithm outperforms generational genetic algorithm in tweet clustering.	genetic algorithm	Amr Adel;Essam El Fakharany;Amr Badr	2014	JCS	10.3844/jcssp.2014.1269.1280	computer science;machine learning;data mining;internet privacy;cluster analysis;world wide web	NLP	-26.12050614722811	-55.471152040101046	7750
9352c6392ede673f7feb9569865053bd87dc2bb5	active learning with amazon mechanical turk	random selection;annotation example;supervised classification;noisy crowdsourcing scenario;active learning;entity recognition;large amount;annotated training data;amazon mechanical turk;sentiment detection	Supervised classification needs large amounts of annotated training data that is expensive to create. Two approaches that reduce the cost of annotation areactive learningandcrowdsourcing. However, these two approaches have not been combined successfully to date. We evaluate the utility of active learning in crowdsourcing on two tasks, named entity recognition and sentiment detection, and show that active learning outperforms random selection of annotation examples in a noisy crowdsourcing scenario.	amazon mechanical turk;crowdsourcing;experiment;machine learning;named-entity recognition;natural language processing;oracle nosql db;real life;sampling (signal processing);simulation;statistical classification;test set;the turk	Florian Laws;Christian Scheible;Hinrich Schütze	2011			computer science;data science;machine learning;data mining	NLP	-19.953680093257006	-66.542491180194	7783
174944a98033466df036ebd58309c447cc083e80	a novel protocol for the evaluation of motor learning in 3d reching tasks using novint falcon		Motor learning or motor adaptation is the capability to acquire new motor skills or the adaptation of existing motor skills to new environmental conditions. In this paper, a new protocol based on a low-cost haptic device for evaluating the motor adaptation during perturbed 3D reaching tasks was presented. The protocol consisted of three 3D reaching tasks performed using Novint Falcon: a familiarization task in which no force fieldwas applied, an adaptation task in which a perturbing force field occurred, and a wash out task with no force field. Ten healthy subjects were enrolled in the study. Subjects were asked to reach four targets equally distributed along a circumference. During the adaptation task, a constant force perpendicular to the direction of movement was applied and it was randomly removed 40 times out of 160. Trajectories of the end-effector were recorded to calculate the following kinematic indices: duration of movement, length ratio, lateral deviation, speed metric and normalized jerk. The learning index was calculated to study the motor learning during the adaptation task. Two-way repeated measure ANOVA tests were performed for all the indices considering movement directions and tasks as independent variables. Moreover, a one-way repeated measure ANOVA was performed on the learning index to find differences among the 4 target sets. The movement accuracy is influenced from both the perturbed force field and the movement direction. The smoothness of the reaching movement is influenced by the presence of the force field and decreases when it is applied. Learning index showed the capability of the subjects to rapidly adapt to a perturbed force field, generating a compensation strategy in a 3D movement.	falcon (video game series);force field (chemistry);haptic technology;lateral thinking;one-way function;randomness;robot end effector	Emilia Scalona;Francesca Martelli;Zaccaria Del Prete;Eduardo Palermo;Stefano Rossi	2018	2018 7th IEEE International Conference on Biomedical Robotics and Biomechatronics (Biorob)	10.1109/BIOROB.2018.8487735	task analysis;jerk;variables;repeated measures design;motor learning;control theory;kinematics;motor skill;haptic technology;computer science	Robotics	-44.59799958782253	-52.339196523907056	7798
979aede53561a6c158cdc1c370ea210358834dec	incorporating intra-query term dependencies in an aspect query language model	intra query term dependency;information retrieval;intra q;query decomposition;qa75 electronic computers computer science;query language model;期刊论文;aspect hidden markov model	DAWEI SONG,1,2 YANJIE SHI,1 PENG ZHANG,1 QIANG HUANG,3 UDO KRUSCHWITZ,4 YUEXIAN HOU,1 AND BO WANG1 1Tianjin Key Laboratory of Cognitive Computing and Application, School of Computer Science and Technology, Tianjin University, China 2Department of Computing, The Open University, UK 3School of Computing, University of East Anglia, UK 4School of Computer Science and Electronic Engineering, University of Essex, UK	cognitive computing;computer science;document;electronic engineering;ergodicity;hidden markov model;language model;query expansion;query language;robustness (computer science);sparse matrix	Dawei Song;Yanjie Shi;Peng Zhang;Qiang Huang;Udo Kruschwitz;Yuexian Hou;Bo Wang	2015	Computational Intelligence	10.1111/coin.12058	natural language processing;sargable;query optimization;query expansion;web query classification;ranking;boolean conjunctive query;computer science;machine learning;data mining;information retrieval	NLP	-34.54501977245307	-65.51775677721314	7800
6f78e1d45491039e5a199994b01295b90cae66bd	determining compositionality of word expressions using word space models		This research focuses on determining semantic compositionality of word expressions using word space models (WSMs). We discuss previous works employing WSMs and present differences in the proposed approaches which include types of WSMs, corpora, preprocessing techniques, methods for determining compositionality, and evaluation testbeds. We also present results of our own approach for determining the semantic compositionality based on comparing distributional vectors of expressions and their components. The vectors were obtained by Latent Semantic Analysis (LSA) applied to the ukWaC corpus. Our results outperform those of all the participants in the Distributional Semantics and Compositionality (DISCO) 2011 shared task.	distributional semantics;experiment;latent semantic analysis;microsoft word for mac;precision and recall;preprocessor;text corpus	Lubomír Krcmár;Karel Jezek;Pavel Pecina	2013			natural language processing;computer science;machine learning;linguistics	NLP	-26.616185525568838	-71.64071325209996	7810
37e061be1e8933e3045122d054f1af90328a781a	discriminative syntactic language modeling for speech recognition	acoustic input;language model;1000-best output;discriminative syntactic language modeling;baseline recogniser;switchboard speech recognition task;reranking approach;reranking model;syntactic feature;1000-best list;parameter estimation method;error rate;speech recognition	We describe a method for discriminative training of a language model that makes use of syntactic features. We follow a reranking approach, where a baseline recogniser is used to produce 1000-best output for each acoustic input, and a second “reranking” model is then used to choose an utterance from these 1000-best lists. The reranking model makes use of syntactic features together with a parameter estimation method that is based on the perceptron algorithm. We describe experiments on the Switchboard speech recognition task. The syntactic features provide an additional 0.3% reduction in test–set error rate beyond the model of (Roark et al., 2004a; Roark et al., 2004b) (significant at p < 0.001), which makes use of a discriminatively trained n-gram model, giving a total reduction of 1.2% over the baseline Switchboard system.	acoustic cryptanalysis;algorithm;baseline (configuration management);discriminative model;estimation theory;experiment;language model;n-gram;perceptron;point of sale;shallow parsing;speech recognition;synapomorphy;telephone switchboard;word error rate	Michael Collins;Brian Roark;Murat Saraclar	2005			natural language processing;speech recognition;word error rate;computer science;pattern recognition;language model	NLP	-19.349509925542883	-88.43603721152998	7856
c89de346a6cc9699b1943a447b81b96f507e36c7	leveraging the impact of ontology evolution on semantic annotations		This paper deals with the problem of maintenance of semantic annotations produced based on domain ontologies. Many annotated texts have been produced and made available to end-users. If not reviewed regularly, the quality of these annotations tends to decrease over time due to the evolution of the domain ontologies. The quality of these annotations is critical for tools that exploit them (e.g., search engines and decision support systems) and need to ensure an acceptable level of performance. Although the recent advances for ontologybased annotation systems to annotate new documents, the maintenance of existing annotations remains under studied. In this work we present an analysis of the impact of ontology evolution on existing annotations. To do so, we used two well-known annotators to generate more than 66 million annotations from a pre-selected set of 5000 biomedical journal articles and standard ontologies covering a period ranging from 2004 to 2016. We highlight the correlation between changes in the ontologies and changes in the annotations and we discuss the necessity to improve existing annotation formalisms in order to include elements required to support (semi-) automatic annotation maintenance mechanisms.	decision support system;evolution;gate;kolibrios;national center for biomedical ontology;ontology (information science);web search engine	Silvio Domingos Cardoso;Cédric Pruski;Marcos Da Silveira;Ying-Chi Lin;Anika Groß;Erhard Rahm;Chantal Reynaud	2016		10.1007/978-3-319-49004-5_5	natural language processing;upper ontology;bibliographic ontology;ontology inference layer;ontology;ontology-based data integration;world wide web;information retrieval	AI	-34.067816691866646	-67.805218639613	7874
46f58a6ae5853f3c4a57fd44bfb95523fabb9253	designing an annotated longitudinal latvian children's speech corpus			speech corpus	Ilze Auzina;Kristine Levane-Petrova;Guna Rabante-Busa;Roberts Dargis;Antonio Fábregas	2016		10.3233/978-1-61499-701-6-46	linguistics;speech corpus;latvian;psychology	NLP	-15.251368857290036	-84.84734837313907	7905
a96361f203006072fd2a18e2e74a088f8888e056	improved spoken document retrieval by exploring extra acoustic and linguistic cues	speech recognition;query expansion;part of speech;information retrieval;mandarin chinese	In this paper, we explored the use of various extra information to improve the performance of spoken document retrieval (SDR). From the speech recognition perspective, we incorporated the acoustic stress and word confusion information into the audio indexing. From the linguistic perspective, we applied the partof-speech information in both the audio indexing and the query representation. From the information retrieval perspective, we integrated techniques such as the query expansion by word associations and the blind relevance feedback into the retrieval process. The SDR experiments were based on the Topic Detection and Tracking Corpora (TDT-2 and TDT-3). We used the Chinese newswire text stories as query exemplars and the Mandarin Chinese audio news stories as the spoken documents. With all the above acoustic and linguistic cues applied, the average precision was improved from 0.5122 to 0.6312 for the TDT-2 collection and from 0.6216 to 0.7172 for the TDT-3 collection.	acoustic cryptanalysis;document retrieval;etsi satellite digital radio;experiment;information retrieval;query expansion;relevance feedback;speech recognition;super robot monkey team hyperforce go!;text corpus	Berlin Chen;Hsin-Min Wang;Lin-Shan Lee	2001			human–computer information retrieval;speech recognition;query expansion;part of speech;speech analytics;search engine indexing;information retrieval;document retrieval;computer science;relevance feedback;linguistics;document clustering	Web+IR	-23.00462989103259	-82.68413885978319	7915
e2314311f11b06e55f994c88185708154d68ca80	medical image describing behavior: a comparison between an expert and novice	image descriptions;image indexing;eye fixations;area of interest;medical image;eye movement;eye tracking;information need	This preliminary study, as a part of a broader study about the medical image use by experts and novices, examines the differences in image describing behavior between these two categories of users. Eye tracking technique was used to capture the image users' eye movement on the Area of Interests (AOIs). This study found that a domain expert was capable of employing nine levels of image attributes in the descriptions, while a novice was able to use only six levels. Furthermore, the expert showed stronger capability in expressing the image information needs by generating more image attributes in the descriptions than the novice, especially in terms of employing significantly more high-level (semantic levels) image attributes. We also found that the novice attended to more AOIs on every image than the expert.	eye tracking;high- and low-level;information needs;medical imaging;subject-matter expert	Xin Wang;Sanda Erdelez;Carla Allen;Blake Anderson;Hongfei Cao;Chi-Ren Shyu	2011		10.1145/1940761.1940907	computer vision;simulation;computer science;multimedia	HCI	-14.211913256925301	-59.35906001133246	7920
13c771583e2e64500e535bc26677ee2a0089b6f5	elexr: automatic evaluation of machine translation using lexical relationships	datavetenskap datalogi;computer science	This paper proposes ELEXR 1 , a novel metric to evaluate ma- chine translation (MT). In our proposed method, we extract lexical co-occurrence relationships of a given reference translation (Ref) and its corresponding hypothesis sentence using hyperspace analogue to language space matrix. Then, for each term appearing in these two sentences, we convert the co-occurrence information into a conditional probability dis- tribution. Finally, by comparing the conditional probability distributions of the words held in common by Ref and the candidate sentence (Cand) us- ing Kullback-Leibler divergence, we can score the hypothesis. ELEXR can evaluate MT by using only one Ref assigned to each Cand without incorpo- rating any semantic annotated resources like WordNet. Our experiments on eight language pairs of WMT 2011 submissions 2 show that ELEXR outperforms baselines, TER and BLEU, on average at system-level corre- lation with human judgments. It achieves average Spearman's rho corre- lation of about 0.78, Kendall's tau correlation of about 0.66 and Pearson's correlation of about 0.84, corresponding to improvements of about 0.04, 0.07 and 0.06 respectively over BLEU, the best baseline.	evaluation of machine translation	Alireza Mahmoudi;Heshaam Faili;Mohammad Hossein Dehghan;Jalal Maleki	2013		10.1007/978-3-642-45114-0_32	natural language processing;speech recognition;computer science;artificial intelligence;machine learning;statistics	NLP	-26.629299977223624	-68.4637701107224	7926
8ab12f0b15c5447606d2f9b58d4539ee27b005b7	automatic extraction of emergency response process models from chinese plans		Emergency plans can be regarded as the effective guidance of hazard emergency responses, and they include the textual descriptions of emergency response processes in terms of natural language. In this paper, we propose an approach to automatically extract emergency response process models from Chinese emergency plans. First, the emergency plan is represented as a text tree according to its layout markups and sentence-sequential relations. Then, process model elements, including four-level response condition formulas, executive roles, response tasks, and flow relations, are identified by rule-based approaches. Finally, an emergency response process tree is generated from both the text tree and extracted process model elements, and is transformed to an emergency response process that is modeled as business process modeling notation. Extensive experiments on real-world emergency plans demonstrate that the proposed approach is capable of extracting emergency response process models that are highly consistent with both the manually extracted models and the textual plans.		Wenyan Guo;Qingtian Zeng;Hua Duan;G F Yuan;Weijian Ni;Cong Liu	2018	IEEE Access	10.1109/ACCESS.2018.2880515	software engineering;task analysis;response process;natural language;distributed computing;computer science;business process model and notation;process modeling	AI	-40.38795585223284	-70.05000698141346	7940
6da5ba956e413dda211e54f8efc17b802d3f859b	identification of investigator name zones using svm classifiers and heuristic rules	citation analysis;bibliographic information;support vector machines labeling merging libraries accuracy data mining classification algorithms;support vector machines;text analysis bioinformatics citation analysis pattern classification support vector machines;medline;investigator names;text analysis;bibliographic information investigator names medline support vector machine heuristic rules labeling;pattern classification;online biomedical articles investigator name zone identification svm classifiers heuristic rules article authors medline citation automated extraction us national library of medicine support vector machine classifiers svm classifier label text blocks word lists classifier testing classifier training;heuristic rules;support vector machine;labeling;bioinformatics	"""The research reported in biomedical articles often involves large numbers of investigators at different institutions. To properly credit these investigators, an article's authors frequently name them together in some part of the article. These Investigator Names (IN) now constitute a required field in the MEDLINE® citation for the article. The automated extraction of these names is implemented in a system developed by a research group at the U.S. National Library of Medicine, consisting of three modules based on Support Vector Machine (SVM) classifiers and heuristic rules. The SVM classifiers label text blocks (""""zones"""") that possibly contain Investigator Names, and the heuristic rules identify the actual zones. We collect eleven sets of word lists to train and test the classifiers, each set containing 100 to 56,000 words. Experimental results on online biomedical articles show a Precision of 0.90, 0.95 Recall, 0.92 F-Measure, and 0.99 Accuracy."""	dictionary attack;f1 score;heuristic;medline;name;support vector machine	Jongwoo Kim;Daniel X. Le;George R. Thoma	2013	2013 12th International Conference on Document Analysis and Recognition	10.1109/ICDAR.2013.35	support vector machine;computer science;machine learning;pattern recognition;data mining;citation analysis;structured support vector machine;information retrieval	NLP	-36.61650041287536	-69.71296428831843	7945
3a6462c1362989d160b0260a3095e26213f19724	mobile applications for control and self management of diabetes: a systematic review	mobile apps;self management of diabetes;health	Mobile applications (apps) can be very useful software on smartphones for all aspects of people’s lives. Chronic diseases, such as diabetes, can be made manageable with the support of mobile apps. Applications on smartphones can also help people with diabetes to control their fitness and health. A systematic review of free apps in the English language for smartphones in three of the most popular mobile app stores: Google Play (Android), App Store (iOS) and Windows Phone Store, was performed from November to December 2015. The review of freely available mobile apps for self-management of diabetes was conducted based on the criteria for promoting diabetes self-management as defined by Goyal and Cafazzo (monitoring blood glucose level and medication, nutrition, physical exercise and body weight). The Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) was followed. Three independent experts in the field of healthcare-related mobile apps were included in the assessment for eligibility and testing phase. We tested and evaluated 65 apps (21 from Google Play Store, 31 from App Store and 13 from Windows Phone Store). Fifty-six of these apps did not meet even minimal requirements or did not work properly. While a wide selection of mobile applications is available for self-management of diabetes, current results show that there are only nine (5 from Google Play Store, 3 from App Store and 1 from Windows Phone Store) out of 65 reviewed mobile apps that can be versatile and useful for successful self-management of diabetes based on selection criteria. The levels of inclusion of features based on selection criteria in selected mobile apps can be very different. The results of the study can be used as a basis to prvide app developers with certain recommendations. There is a need for mobile apps for self-management of diabetes with more features in order to increase the number of long-term users and thus influence better self-management of the disease.	android;app store;application procedure;burn injury;cns disorder;canonical account;chronic lymphocytic leukemia;customize;diabetes mellitus;diabetes mellitus, non-insulin-dependent;eligibility determination;exercise;global positioning system;glucose;gray platelet syndrome;hematological disease;human body weight;meal (occasion for eating);microsoft windows;mobile applications;mobile app;nsa product types;note (document);nutrition, calories;patients;play store;primary health care;programming languages;promotion (action);requirement;review [publication type];self-management (computer science);smartphone;systematic review;windows phone;accelerometers;ios;sensor (device)	Petra Povalej Brzan;Eva Rotman;Majda Pajnkihar;Petra Klanjsek	2016	Journal of Medical Systems	10.1007/s10916-016-0564-8	health;multimedia;internet privacy;world wide web;computer security	HCI	-58.10825502096795	-58.54837914732894	7952
f6381dadbf74de3052ba36f00180a183253b8d1c	interpretation of an international terminology standard in the development of a logic-based compositional terminology	nursing;health informatics;standards;base composition;international council of nurses;subjects outside of the university themes;health and wellbeing;terminology;rt nursing;internal standard;qa075 electronic computers computer science	PURPOSE Version 1.0 of the International Classification for Nursing Practice (ICNP) is a logic-based compositional terminology. International Organization for Standardization (ISO) 18104:2003 Health Informatics-Integration of a reference terminology model for nursing is an international standard to support the development, testing and implementation of nursing terminologies.   METHODS This study examines how ISO 18104:2003 has been interpreted in the development of ICNP Version 1.0 by identifying mappings between ICNP and the ISO standard. Representations of diagnostic and interventional statements within ICNP are also analyzed according to the requirements mandated by the ISO standard.   RESULTS All structural components of ISO 18104:2003 i.e. semantic categories, semantic domains, qualifiers and semantic links are represented either directly or in interpreted form within ICNP. The formal representations within ICNP of diagnostic and interventional statements meet the requirement of the ISO standard.   CONCLUSIONS The findings of this study demonstrate that ICNP Version 1.0 conforms to ISO 18104:2003. More importantly perhaps, this study provides practical examples of how components of a terminology standard might be interpreted and it examines how such a standard might be used to support the definition of high-level schemata in developing logic-based compositional terminologies.	categories;conformance testing;high- and low-level;informatics (discipline);information model;international classification for nursing practice;nomenclature;requirement;standardized nursing terminology;terminology model;universal instantiation;standards characteristics	Nicholas R. Hardiker;Amy Coenen	2007	International journal of medical informatics	10.1016/j.ijmedinf.2007.05.005	health informatics;medicine;nursing;data mining;internal standard;terminology	SE	-51.13085172461871	-67.32687719982502	8062
000f5039c718f20db8458ac3982a15b981c9ce08	automated domain-aware form understanding with opal : with a case study in the uk real-estate domain		Web forms are the interfaces to the deep web, and automated form understanding is the key to unlock its contents. It is a fundamental problem in many applications and research fields, such as deep web crawling, data integration, or information extraction. It is also essential for improving web usability and accessibility. Form understanding is an inherently empirical problem. Existing form understanding approaches are restricted by exploiting limited and domain independent feature sets leading to overly generic and monolithic algorithms. In response, we present opal (Ontology based web Pattern Analysis with Logic), a domain-aware form understanding approach, that addresses all these limitations through a novel multi-scope approach. opal achieves this through a domain independent form labeling and a domain dependent form interpretation. In form labeling, opal associates texts with fields as labels through three domain independent scopes exploiting textual, structural, and visual information. In form interpretation, opal integrates the form labeling obtained with a layer of high-level domain knowledge to classify form fields and to repair the form model. To ease the task of designing domain schemata, we develop the template language opal-tl to express domain types and their structural constraints. With opal-tl, we describe common design patterns as templates maintained in a library. Thus, the adaption to new domains often requires only instantiation of the templates with corresponding domain types. We conduct extensive experiments, that cover both domain independent crossdomain testing with standard form understanding benchmarks, and a domainaware evaluation with two domain datasets randomly selected from real estate and used car domain. opal outperforms previous works by a significant margin and pushes the state of the art to near perfect accuracy (> 98%). In an effort to integrate opal with an entire data extraction pipeline, we plan to extend opal with form probing and to exploit information obtained by other data extraction components, e.g., result page analysis.	accessibility;algorithm;dark web;deep web;design pattern;domain-specific language;experiment;form (html);high- and low-level;information extraction;opal (programming language);randomness;sim lock;template processor;universal instantiation;web crawler;web usability	Xiaonan Guo	2012				AI	-33.86200513664868	-66.20319997970843	8090
fac34c6b0ed0c0cc68d2b7e3a305d01cdd836775	image evaluation system based on the sound symbolism of brand names	sound symbolic effect image evaluation system sound symbolism brand name registration psychological experiment;psychology image registration marketing;phychological experiments brand name sound symbolism image evaluation system	More than a hundred thousand brands are registered every year in Japan. Therefore, creating effective brand names gets more and more difficult. Psychological experiments conducted by many leading studies of sound symbolism in brand names showed that making good use of sound symbolism in brand names could be very important. No study, however, has attempted to develop any system to use sound symbolic effects of brand names. So, in this article, we developed a system which evaluates the images evoked by brand names based on sound symbolism and supports creation of brand names. We confirmed that outputs of the system were consistent with the evaluation of the participants with an accuracy of 82%.	experiment;genetic algorithm	Ryuichi Doizaki;Yuichiro Shimizu;Maki Sakamoto	2012	The 6th International Conference on Soft Computing and Intelligent Systems, and The 13th International Symposium on Advanced Intelligence Systems	10.1109/SCIS-ISIS.2012.6505236	advertising;machine learning;artificial intelligence;sound symbolism;computer science	SE	-48.50732539165742	-74.6248628169268	8128
f18bfaac5b29fb2f6230f01c3156554b10252e8b	studying expectation violations in socio-technical systems: a case study of the mobile app community	text mining;socio technical systems;analytics;expectation confirmation theory ect;expectancy violation theory evt;norms	With information technology mediating most aspects of contemporary societies, it is important to explore how human-oriented concepts may be leveraged to explore human actions in this new dispensation. One such concept is expectation violations. Expectations govern nearly all aspect of human interactions. However, while this phenomenon has been studied in human-human contexts where violations are expressed through verbal or non-verbal forms, little effort has been dedicated to the study of expectations in human-software contexts in socio-technical systems. We have thus studied expectation violations in one such instance, the mobile app community. Using Expectation Violation and Expectation Confirmation theories, we studied users’ reviews of four apps in the health and fitness domain to understand how this app community responds to expectation violations, and if users in a similar domain will express dissatisfaction about similar expectation violations. Our outcomes confirm that the mobile app community responded to expectation violations, just as individuals do in human-human settings. In addition, we observed that users of different health and fitness apps reported similar expectation violations, as is the case for individuals and groups sharing culturespecific expectations. Beyond being of practical relevance for the app community, our outcomes also highlight opportunities for extending the abovementioned theories.	electroconvulsive therapy;entity;expectation–maximization algorithm;extreme value theory;h2 database engine;interaction;mobile app;relevance;sociotechnical system;t-norm;theory;virtual community	Priyanka Patel;Sherlock A. Licorish;Bastin Tony Roy Savarimuthu;Stephen G. MacDonell	2016			analytics;text mining;computer science;data science;sociotechnical system;data mining;social psychology;norm	HCI	-62.069741111723204	-56.3194128267828	8132
15cbc4994f8cede23473f0b3be8743c9b138b51f	the impact of electronic medical records data sources on an adverse drug event quality measure	clinical data;child health;adverse drug event;system design;clinical practice;health information system;quality measures;electronic medical record;data warehouse;charge order	"""OBJECTIVE To examine the impact of billing and clinical data extracted from an electronic medical record system on the calculation of an adverse drug event (ADE) quality measure approved for use in The Joint Commission's ORYX program, a mandatory national hospital quality reporting system.   DESIGN The Child Health Corporation of America's """"Use of Rescue Agents-ADE Trigger"""" quality measure uses medication billing data contained in the Pediatric Health Information Systems (PHIS) data warehouse to create The Joint Commission-approved quality measure. Using a similar query, we calculated the quality measure using PHIS plus four data sources extracted from our electronic medical record (EMR) system: medications charged, medication orders placed, medication orders with associated charges (orders charged), and medications administered.   MEASUREMENTS Inclusion and exclusion criteria were identical for all queries. Denominators and numerators were calculated using the five data sets. The reported quality measure is the ADE rate (numerator/denominator).   RESULTS Significant differences in denominators, numerators, and rates were calculated from different data sources within a single institution's EMR. Differences were due to both common clinical practices that may be similar across institutions and unique workflow practices not likely to be present at any other institution. The magnitude of the differences would significantly alter the national comparative ranking of our institution compared to other PHIS institutions.   CONCLUSIONS More detailed clinical information may result in quality measures that are not comparable across institutions due institution-specific workflow, differences that are exposed using EMR-derived data."""	adverse reaction to drug;charge (electrical);chart evaluation by healthcare professional;contain (action);dav regimen;data validation;denominator;documentation;electronic health records;electronic billing;electronics, medical;excalibur: morgana's revenge;exclusion;extraction;health information systems;information system;level of detail;mandatory - hl7definedroseproperty;medical records;medical records, problem-oriented;numerator;numerous;oryx (encryption algorithm);review [publication type];web site;orders - hl7publishingdomain	Michael G. Kahn;Daksha Ranade	2010	Journal of the American Medical Informatics Association : JAMIA	10.1136/jamia.2009.002451	health informatics;medicine;computer science;nursing;data warehouse;data mining;database;systems design	DB	-60.38090069687436	-65.65148838405794	8184
269e736628aa290b0dcf815ab00e459497baacd4	techminer: extracting technologies from academic publications		In recent years we have seen the emergence of a variety of scholarly datasets. Typically these capture ‘standard’ scholarly entities and their connections, such as authors, affiliations, venues, publications, citations, and others. However, as the repositories grow and the technology improves, researchers are adding new entities to these repositories to develop a richer model of the scholarly domain. In this paper, we introduce TechMiner, a new approach, which combines NLP, machine learning and semantic technologies, for mining technologies from research publications and generating an OWL ontology describing their relationships with other research entities. The resulting knowledge base can support a number of tasks, such as: richer semantic search, which can exploit the technology dimension to support better retrieval of publications; richer expert search; monitoring the emergence and impact of new technologies, both within and across scientific fields; studying the scholarly dynamics associated with the emergence of new technologies; and others. TechMiner was evaluated on a manually annotated gold standard and the results indicate that it significantly outperforms alternative NLP approaches and that its semantic features improve performance significantly with respect to both recall and precision.	algorithm;emergence;entity;knowledge base;machine learning;natural language processing;ontology (information science);population;precision and recall;scopus;semantic web;semantic search;switzerland;web ontology language	Francesco Osborne;Hélène de Ribaupierre;Enrico Motta	2016		10.1007/978-3-319-49004-5_30	computer science;artificial intelligence;data science;data mining;information retrieval	Web+IR	-32.441838867522584	-66.9309954362287	8191
21273fd5a097579e97a8d25a017a66f1a0f26303	scalable representations of diseases in biomedical ontologies	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;data mining and knowledge discovery;computational biology bioinformatics;uk phd theses thesis;life sciences;algorithms;combinatorial libraries;uk research reports;medical journals;computer appl in life sciences;europe pmc;biomedical research;bioinformatics	BACKGROUND The realm of pathological entities can be subdivided into pathological dispositions, pathological processes, and pathological structures. The latter are the bearer of dispositions, which can then be realized by their manifestations - pathologic processes. Despite its ontological soundness, implementing this model via purpose-oriented domain ontologies will likely require considerable effort, both in ontology construction and maintenance, which constitutes a considerable problem for SNOMED CT, presently the largest biomedical ontology.   RESULTS We describe an ontology design pattern which allows ontologists to make assertions that blur the distinctions between dispositions, processes, and structures until necessary. Based on the domain upper-level ontology BioTop, it permits ascriptions of location and participation in the definition of pathological phenomena even without an ontological commitment to a distinction between these three categories. An analysis of SNOMED CT revealed that numerous classes in the findings/disease hierarchy are ambiguous with respect to process vs. disposition. Here our proposed approach can easily be applied to create unambiguous classes. No ambiguities could be defined regarding the distinction of structure and non-structure classes, but here we have found problematic duplications.   CONCLUSIONS We defend a judicious use of disjunctive, and therefore ambiguous, classes in biomedical ontologies during the process of ontology construction and in the practice of ontology application. The use of these classes is permitted to span across several top-level categories, provided it contributes to ontology simplification and supports the intended reasoning scenarios.	ambiguous grammar;ct scan;categories;class;disjunctive normal form;entity;license;ontology (information science);pet/ct scan;pathologic processes;pathology;software design pattern;systematized nomenclature of medicine clinical terms;text simplification;upper ontology;web ontology language	Stefan Schulz;Kent A. Spackman;Andrew G. James;Cristian Cocos;Martin Boeker	2011		10.1186/2041-1480-2-S2-S6	medical research;computer science;bioinformatics;data science;data mining;algorithm	AI	-50.765631650873885	-67.8392395130022	8204
1befb6047d8e71f16608aa347132b9bc1a07846e	error in natural language dialogue between man and machine	lenguaje natural;erreur;natural language dialogue;man machine dialogue;correction erreur;langage naturel;prevention;error correction;natural language;typology;dialogo hombre maquina;typologie;error;correccion error;tipologia;dialogue homme machine;prevencion	A great deal of research has been done in the field of error processing, but the difficulty of the problem and the diverse ways in which it has been approached have made synthesis and methodological reflection necessary at this point. The present article begins by proposing a typology of errors in natural language dialogue between man and machine. The discussion of possible correction strategies that follows shows that the correction of user competence errors should take precedence over the correction of performance errors. The discussion also emphasizes that error prevention, although generally neglected, is a major way to avoid dialogue dead-ends, and should be taken into account in natural-language interface design. It is proposed that if dialogue systems cannot as yet be adapted to users, then they must be transparent and consistent enough so that users can adapt to them. As an illustration, the paper gives an overview of various methods developed from these methodological principles for correcting and preventing errors at the lexical, syntactic and semantic levels.	natural language	Jean Véronis	1991	International Journal of Man-Machine Studies	10.1016/S0020-7373(05)80148-8	natural language processing;error detection and correction;preventive healthcare;typology;computer science;artificial intelligence;linguistics;natural language	Arch	-30.66123490980449	-84.57047351711556	8218
050a6cc89aa0022b66e957e97451b1606a6eeaa4	a fusion of algorithms in near duplicate document detection	duplicate document detection;web pages;digital library;near duplicate document	"""$O+1=$1=,$5&P+;$;,D,32PQ,#1$29$1=,$O253;$O+;,$O,7($1=,5,$&5,$&$ =""""/,$#""""Q7,5$29$9""""338$25$95&/Q,#1&338$;""""P3+<&1,;$P&/,E$+#$1=,$R#1,5#,1N$S,1""""5#$29$ 1=,E,$#,&5$;""""P3+<&1,;$5,E""""31E$12$1=,$""""E,5E$/5,&138$&99,<1E$""""E,5$,TP,5+,#<,EN$R#$1=,$ P52<,EE$29$;,P328+#/$;+/+1&3$3+75&5+,E($1=,$P521,<1+2#$29$+#1,33,<1""""&3$P52P,518$&#;$ 5,Q2D&3$29$;""""P3+<&1,$<2#1,#1E$#,,;E$12$7,$<2#E+;,5,;N$*=+E$P&P,5$9""""E,E$E2Q,$ UE1&1,$29$1=,$&51V$&3/25+1=QE$12$5,&<=$&$7,11,5$P,5925Q&#<,N$O,$9+5E1$+#152;""""<,$ 1=,$1=5,,$Q&-25$&3/25+1=QE$WE=+#/3+#/($RXQ&1<=($E+Q=&E=Y$+#$;""""P3+<&1,$;2<""""Q,#1$ ;,1,<1+2#$&#;$1=,+5$;,D,32PQ,#1E$+#$1=,$92332Z+#/$;&8EN$O,$1&B,$E,["""",#<,E$29$ Z25;E$WE=+#/3,EY$&E$1=,$9,&1""""5,$29$E+Q=&E=$&3/25+1=QN$O,$1=,#$+QP251$1=,$ 5&#;2Q$3,T+<2#E$7&E,;$Q""""31+$9+#/,5P5+#1E$/,#,5&1+2#$Q,1=2;$+#12$E=+#/3+#/$7&E,$ E+Q=&E=$&3/25+1=Q$&#;$#&Q,;$+1$E=+#/3+#/$7&E,;$Q""""31+$9+#/,5P5+#1E$E+Q=&E=$ &3/25+1=QN$O,$;+;$E2Q,$P5,3+Q+#&58$,TP,5+Q,#1E$2#$1=,$E8#1=,1+<$;&1&E,1$7&E,;$ 2#$1=,$U@=+#&XC>$\+33+2#$F22B$]+/+1&3$6+75&58$A52-,<1VN$*=,$,TP,5+Q,#1$ 5,E""""31$P52D,E$1=,$,99+<+,#<8$29$1=,E,$&3/25+1=QEN$"""		Jun Fan;Tiejun Huang	2011		10.1007/978-3-642-28320-8_20	digital library;computer science;web page;data mining;database;world wide web;information retrieval	NLP	-29.279371928002995	-55.9626383827744	8230
92da013db1179cfa0d0346fd119696ef69f74b5a	computational linguistics and intelligent text processing		In the near-synonym lexical choice task, the best alternative out of a set of near-synonyms is selected to fill a lexical gap in a text. We experiment on an approach of an extensive set, over 650, linguistic features to represent the context of a word, and a range of machine learning approaches in the lexical choice task. We extend previous work by experimenting with unsupervised and semi-supervised methods, and use automatic feature selection to cope with the problems arising from the rich feature set. It is natural to think that linguistic analysis of the word context would yield almost perfect performance in the task but we show that too many features, even linguistic, introduce noise and make the task difficult for unsupervised and semi-supervised methods. We also show that purely syntactic features play the biggest role in the performance, but also certain semantic and morphological features are needed.	computation;computational linguistics;experiment;feature selection;image noise;lexical choice;machine learning;semi-supervised learning;semiconductor industry;supervised learning;unsupervised learning	Josef Kittler	2012		10.1007/978-3-642-28601-8		NLP	-20.915839437611062	-71.69222865311976	8259
a7f7faaec04b8ff1f9747a9f87b07d57dfaffc18	representing action structure through semantic roles of physiotherapy instructions	legged locomotion;neck;semantics;data mining;knee;xml;labeling	Semantic Role Labels can be helpful in deriving action constructs for representing virtual actions from text. Action constructs are structure through which the virtual action from text can be generated easily. The proposed system aims to derive the action structure present in written instructions. This proposed framework describes two aspects of action representation from text; semantic annotation of instructions and generating the logical action structure thereof. A small corpus of physiotherapy instructions, containing nearly 300 instructions, has been formed for this system. The annotation scheme along with representation of action constructs has been explained with some of the physiotherapy instructions from the corpora.	text corpus	Sandeep Kumar Dash;Partha Pakray;Alexander F. Gelbukh	2016	2016 International Conference on Asian Language Processing (IALP)	10.1109/IALP.2016.7875941	labeling theory;xml;computer science;artificial intelligence;theoretical computer science;semantics;linguistics	NLP	-35.15695538733927	-79.5180646884436	8277
5ef1a6404d92103ef3238d3dd06e89a1c60f5921	corpus-based question answering for why-questions		This paper proposes a corpus-based approach for answering why-questions. Conventional systems use hand-crafted patterns to extract and evaluate answer candidates. However, such hand-crafted patterns are likely to have low coverage of causal expressions, and it is also difficult to assign suitable weights to the patterns by hand. In our approach, causal expressions are automatically collected from corpora tagged with semantic relations. From the collected expressions, features are created to train an answer candidate ranker that maximizes the QA performance with regards to the corpus of why-questions and answers. NAZEQA, a Japanese why-QA system based on our approach, clearly outperforms a baseline that uses hand-crafted patterns with a Mean Reciprocal Rank (top-5) of 0.305, making it presumably the best-performing fully implemented why-QA system.	baseline (configuration management);bluetooth;causal filter;causality;expression (computer science);framenet;google questions and answers;propbank;question answering;software quality assurance;text corpus;verification and validation;web page	Ryuichiro Higashinaka;Hideki Isozaki	2008			artificial intelligence;pattern recognition;computer science;question answering;expression (mathematics);mean reciprocal rank	NLP	-25.028056917776432	-70.81115322115832	8292
62b3b93f8bf1b9e00f2b90d91eb7edf405fa8ac5	formant and burst spectral measurements with quantitative error models for speech sound classification.	measurement error;spectrum;spectral measure;electrical engineering and computer science;sound classification;thesis;other;speech production	This thesis demonstrates that acoustic variability, acoustic measurement error, and phoneme classification error can be interpreted as predictable entailments of articulatory variability. Speech production theory is tapped to explain sources of variability in the acoustic signal, including random variation in a turbulent spectrum, increased losses a t the glottis, and coloration of the spectrum by su bglottal and back cavity resonances. Measurements of the burst front cavity resonance, and of formant frequencies, which are defined as the eigenfrequencies of the vocal tract, are developed using both knowledge-based and HMh4 design methods, and are evaluated using the tools of acoustic phonetics and of statistical speech classification. The error or uncertainty of both rule-based and HMM algorithms is evaluated by comparison to the measurements of human judges on a test set. Measurement error of the rule-based algorithm is evaluated using aggregate statistical models, including explicit models of outliers and heteroskedasticity, and a non-parametric model of the effect on error of phonetic context. Measurement uncertainty of the HMM formant tracker is calculated by the HMM itself during the measurement process. The uncertainty models generated by the HMM formant tracker are compared to formants transcribed by human judges, and shown to provide imperfect but generally acceptable predictions of the measurement error. Acoustic variability and acoustic measurement error are evaluated using the tools of phonetic classification. Context-independent linear discriminant classification of stop place, using tokens from the TIMIT multi-speaker database, is shown to be 89% correct using manual formant and burst spectral measurements, but only 76% correct using automatic measurements. I t is demonstrated that the difference between the classification of manual and automatic measurements can be accurately predicted using a heteroskedastic error model. Context-dependent classification experiments using both rule-based and HMM formant measurements result in 83-84% correct classification over the TIMIT TEST database. The pattern of classification errors as a function of phonetic context is shown to be similar to the pattern of errors of human listeners, indicating that the types of acoustic variability which confuse the classifier may be similar to the types of variability which confuse human listeners. Thesis Supervisor: Kenneth N. Stevens Title: Clarence J. LeBel Professor of Electrical Engineering		Mark A. Hasegawa-Johnson	1996			speech recognition;acoustics;computer science;communication	SE	-10.743967508838194	-85.5081297563485	8299
b65377675b76f0b296dd2550a7ddba8f27addb17	using the levenshtein edit distance for automatic lemmatization: a case study for modern greek and english	adaptive nonmonotone conjugate gradient training algorithm;computer science and information systems;recurrent neural nets conjugate gradient methods feedforward neural nets learning artificial intelligence;conjugate gradient;complex data;recurrent network;nonmonotone learning horizon;feedforward neural nets;feedforward network;recurrent neural nets;recurrent neural network;nonmonotone learning horizon adaptive nonmonotone conjugate gradient training algorithm recurrent neural network feedforward network adaptive tuning strategy;learning artificial intelligence;conjugate gradient methods;training algorithm;recurrent neural networks power system modeling equations neural networks feedforward neural networks delay effects artificial intelligence computer science information systems educational institutions;adaptive tuning strategy	In the present work we have implemented the Edit Distance (also known as Levenshtein Distance) on a dictionary-based algorithm in order to achieve the automatic induction of the normalized form (lemma) of regular and mildly irregular words with no direct supervision. The algorithm combines two alignment models based on the string similarity and the most frequent inflexional suffixes. In our experiments, we have also examined the language-independency (i.e. independency of the specific grammar and inflexional rules of the language) of the presented algorithm by evaluating its performance on the Modern Greek and English languages. The results were very promising as we achieved more than 95 % of accuracy for the Greek language and more than 96 % for the English language. This algorithm may be useful to various text mining and linguistic applications such as spell-checkers, electronic dictionaries, morphological analyzers, search engines etc.	algorithm;dictionary;edit distance;experiment;lemmatisation;levenshtein distance;string metric;text mining;web search engine	Dimitrios P. Lyras;Kyriakos N. Sgarbas;Nikos Fakotakis	2007	19th IEEE International Conference on Tools with Artificial Intelligence(ICTAI 2007)	10.1109/ICTAI.2007.41	mathematical optimization;computer science;artificial intelligence;recurrent neural network;machine learning;conjugate gradient method;complex data type	Robotics	-18.842947646422736	-79.37253634578833	8322
4dee6599b4322b6bc0f35a3e275f05f44c5c3221	structural information based term weighting in text retrieval for feature location	accuracy large scale integration indexing probability distribution java benchmark testing computational modeling;information retrieval;information technology;richard b;text analysis;latent dirichlet allocation program comprehension feature location static analysis text retrieval;public domain software;text analysis feature extraction information retrieval java natural language processing public domain software source coding;text retrieval structural term weighting open source java system tr based flt term weight assignment unstructured document natural language term weighting scheme corpus text embedding source code feature location technique;computer science structural information based term weighting in text retrieval for feature location the university of alabama nicholas a kraft bassett;feature extraction;natural language processing;source coding;java	Many recent feature location techniques (FLTs) apply text retrieval (TR) techniques to corpora built from text embedded in source code. Term weighting is a standard preprocessing step in TR and is used to adjust the importance of a term within a document or corpus. Common term weighting schemes such as tf-idf may not be optimal for use with source code, because they originate from a natural language context and were designed for use with unstructured documents. In this paper we propose a new approach to term weighting in which term weights are assigned using the structural information from the source code. We then evaluate the proposed approach by conducting an empirical study of a TR-based FLT. In all, we study over 400 bugs and features from five open source Java systems and find that structural term weighting can cause a statistically significant improvement in the accuracy of the FLT.	argouml;document retrieval;eclipse;embedded system;java;local-density approximation;natural language;open-source software;preprocessor;software bug;text corpus;tf–idf;transistor;jedit;on-line system	Blake Bassett;Nicholas A. Kraft	2013	2013 21st International Conference on Program Comprehension (ICPC)	10.1109/ICPC.2013.6613841	speech recognition;feature extraction;computer science;data mining;term discrimination;programming language;java;information technology;public domain software;information retrieval;source code	SE	-28.38664188530465	-66.7335942794956	8332
4fd240adcc45f637008b493aeb6c23ebee96ee7b	hossur'tech's participation in clef 2009 infile interactive filtering	information retrieval system;information filtering;information search and retrieval;adaptive filter	This paper describes the participation of our company formerly named Cadege / Hossur Tech and called now Geol Semantics in the task of filtering interactive CLEF 2009 INFILE and enhancements added after the experiment.#R##N##R##N#The Interactive filtering is something different from traditional information retrieval systems. In CLEF 2009 INFILE adaptive filtering task we have only the knowledge about the 50 different topics which are used as queries and nothing about the input corpus to filter. Documents are received and filtered one by one.#R##N##R##N#The fact that we know nothing about the corpus of the documents to filter, we were forced to use a linguistic approach for this filtering task. We have performed two CLEF 2009 INFILE interactive filtering French to French and French to English tasks, based on a deep linguistic process by using our own linguistic dictionaries.#R##N##R##N#ACM categories and subject descriptors: H.3.3 Information Search and Retrieval, Information filtering		John Anton Chrisostom Ronald;Aurélie Rossi;Christian Fluhr	2009		10.1007/978-3-642-15754-7_45	natural language processing;adaptive filter;computer science;data mining;information retrieval	HCI	-32.410118847235715	-63.755204317042754	8343
4039f0818dcd0da75d665bc7917d1404be68eb94	validator and preview for the jobposting data model of schema.org		The paper describes a tool for validating and previewing instances of Schema.org JobPosting described in structured data markup embedded in web pages. The validator and preview was developed to assist users of Schema.org to produce data of better quality. The paper discusses the implementation of the tool and design of its validation rules based on SPARQL 1.1. Results of experimental validation of a job posting corpus harvested from the Web are presented. The validation’s findings indicate that publishers of Schema.org JobPosting data often misunderstand precedence rules employed by markup parsers and ignore case-sensitivity of vocabulary names.	data model;schema.org;validator	Jindrich Mynarz	2014		10.1007/978-3-319-10491-1_6	database;schema.org;validator;marketing;data model;data validation;markup language;web page;sparql;computer science;validation rule	DB	-31.421642126640165	-60.331415574589755	8351
9475306136816ecc3d355234cefc64697c5e3484	confidence measures and their applications in music labelling systems based on hidden markov models		Inspired by previous work on confidence measures for tempo estimation in loops, we explore ways to add confidence measures to other music labelling tasks. We start by reflecting on the reasons why the work on loops was successful and argue that it is an example of the ideal scenario in which it is possible to define a confidence measure independently of the estimation algorithm. This requires additional domain knowledge not used by the estimation algorithm, which is rarely available. Therefore we move our focus to defining confidence measures for hidden Markov models, a technique used in multiple music information retrieval systems and beyond. We propose two measures that are oblivious to the specific labelling task, trading off performance for computational requirements. They are experimentally validated by means of a chord estimation task. Finally, we have a look at alternative uses of confidence measures, besides those applications that require a high precision rather than a high recall, such as most query retrievals.	computation;control flow;experiment;genetic algorithm;hidden markov model;information retrieval;markov chain;requirement	Johan Pauwels;Ken O'Hanlon;György Fazekas;Mark B. Sandler	2017			machine learning;speech recognition;labelling;artificial intelligence;usability;computer science;probabilistic logic;hidden markov model;communication channel;upload;chord (music)	ML	-10.80149662573362	-72.03335778549204	8360
443a049d44dbca5489abd3e30e2f1782f17746b5	determination of vocal tract shape for vowels	analyse parole;optimisation;optimizacion;reconocimiento palabra;analisis palabra;vocal tract shape;speech analysis;vocal tract;speech;vocal;desarrollo verbal;canal vocal;trouble articulation parole;inverse problem;conducto vocal;language development;developpement verbal;x ray radiography;speech recognition;voyelle;optimization;trastorno articulacion palabra;reconnaissance parole;radiographie rx;vowel;speech articulation disorder;internal model;radiografia rx	The inverse problem for vocal tract shape, area function and articulatory parameters was solved for steady-state vowels by means of an optimization procedure requiring the conditional minimum of work on the part of the articulatory organs. One to four formant frequencies were used as references. The shape of the tongue was measured with an X-ray microbeam system for male and female speakers. The shapes of the vocal tract calculated for the experiments are very similar to the measured shapes.	tract (literature)	Victor N. Sorokin	1992	Speech Communication	10.1016/0167-6393(92)90064-E	vocal tract;internal model;speech recognition;computer science;inverse problem;speech;linguistics	NLP	-8.758321947330266	-83.98028922431688	8362
636ffa2dd42e9d81f9701033edc31b109a142a38	mhealth stakeholder integration: a gamification-based framework-approach towards behavioural change	health intervention;mhealth;ehealth;gamification;individualisation;framework	Smartphone-based applications are increasingly being used to combat unhealthy habits resulting in obesity or reduced physical mobility. In turn, gamification has been identified as a useful tool to motivate individuals towards behavioural change. However, both health professionals and game developers are currently working on island solutions addressing individual problems due to the lack of efficient frameworks enabling cooperation in this domain. This has resulted in a large number of mHealth apps, each using different motivational techniques, metrics and health data. In this paper we propose a unified user-centred framework which is capable of running third-party developer's apps within a sandbox. Using this approach, we can mitigate often voiced privacy and safety concerns and simultaneously tailor health-interventions using content from developers as well as user preferences and environment in order to facilitate internalisation of healthy lifestyles. Apart from this, we provide indications for operationalizing our conceptual framework in future work.	gamification;mhealth;smartphone;user (computing);video game developer	Christopher Helf;Patrick Zwickl;Helmut Hlavacs;Peter Reichl	2015		10.1145/2837126.2837153	simulation;computer science;knowledge management;software framework;ehealth;computer security	SE	-61.055846998509566	-56.589105252695255	8417
67904b6b445b33f76fb145ca1e9e427dc92ee35b	on the use of an effective boltzmann machine for musical style recognition and harmonisation	arte	This paper describes the application of the Effective Boltzmann Machine (EBM) to musical style recognition. After it has been trained with examples of four-part chorales, the EBM is able to distinguish different styles of chorales in unseen pieces of music. Our earlier studies showed that the EBM has many desirable properties. In particular, it is able to complete arbitrary length contextual sequences from learned local context, and synthesise musical harmony for polyphonic music. ln this paper we show that the EBM is able to distinguish chorales from the Baroque period from Norwegian chorales. In summary, the EBM is able to distinguish, firstly, between chorales present in the training set and those that are not. Secondly, the EBM is also able to distinguish chorales of different styles, where one style was used in the training set. We also show that two different EBMs (one trained from examples from Choralbuch and the other trained from Norwegian chorales) are able to distinguish between two different harmonised styles of the same chorale melody. The EBMX system is a PC-based graphical interface, interactive system implementing the EBM for recognition and harmonisation of short		Matthew I. Bellgard;Chi Ping Tsang	1996			psychology;natural language processing;speech recognition;artificial intelligence	ML	-18.435858387683563	-82.70780524922777	8423
bf01d139799cd23083876283302756b71a93521b	enhancing retrieval and ranking performance for media search engine by deep learning	retrieval;deep semantic model;search engines;search engines information retrieval learning artificial intelligence;vocabulary;semantics;decision support systems semantics search engines media machine learning context vocabulary;media search;media;machine learning;ranking;deep structured semantic model;deep learning;deep learning deep structured semantic model media search engine;decision support systems;media search engine;context;retrieval dssm based similarity features training dataset generic dssm large scale bing web search log xbox media search engine search engine relevance deep structured semantic model information processing deep learning media search engine ranking performance	Deep learning has emerged as a powerful technique to uncover deep structured, nonlinear features for various information processing tasks, such as image processing, speech recognition, and information retrieval. In this paper, we introduce a framework of utilizing a Deep Structured Semantic Model (DSSM) to build similarity features to enhance search engine relevance. To illustrate the effectiveness of the proposed deep learned similarity features, we applied our method to an Xbox Media Search Engine. Specifically, we leverage a large-scale Bing web search log to train a generic DSSM. We then tune the DSSM parameters - making the model specific to media search - by using a training dataset from the Xbox Media Search Engine. Finally, we conduct a series of experiments in building Xbox search engine with and without the proposed DSSM similarity features. Our experiment results show that adding DSSM-based similarity features significantly improves the retrieval and ranking performance.	baseline (configuration management);deep learning;document retrieval;experiment;image processing;information processing;information retrieval;loss function;nonlinear system;optimization problem;refinement (computing);relevance;semantic similarity;speech recognition;supervised learning;web search engine	Xugang Ye;Jingjing Li;Zijie Qi;Xiaodong He	2016	2016 49th Hawaii International Conference on System Sciences (HICSS)	10.1109/HICSS.2016.148	media;ranking;computer science;data mining;database;semantics;deep learning;world wide web;information retrieval;search engine	Web+IR	-17.991320243366122	-67.22628618493346	8456
ce1d4e45de8f46cbbe3a6c986ca15479dabc7b4c	detecting rhetorical figures based on repetition of words: chiasmus, epanaphora, epiphora		Dubremetz, M. 2017. Detecting Rhetorical Figures Based on Repetition of Words: Chiasmus, Epanaphora, Epiphora. Studia Linguistica Upsaliensia 18. 49 pp. Uppsala: Acta Universitatis Upsaliensis. ISBN 978-91-513-0165-5. This thesis deals with the detection of three rhetorical figures based on repetition of words: chiasmus (“Fair is foul, and foul is fair.”), epanaphora (“Poor old European Commission! Poor old European Council.”) and epiphora (“This house is mine. This car is mine. You are mine.”). For a computer, locating all repetitions of words is trivial, but locating just those repetitions that achieve a rhetorical effect is not. How can we make this distinction automatically? First, we propose a new definition of the problem. We observe that rhetorical figures are a graded phenomenon, with universally accepted prototypical cases, equally clear non-cases, and a broad range of borderline cases in between. This makes it natural to view the problem as a ranking task rather than a binary detection task. We therefore design a model for ranking candidate repetitions in terms of decreasing likelihood of having a rhetorical effect, which allows potential users to decide for themselves where to draw the line with respect to borderline cases. Second, we address the problem of collecting annotated data to train the ranking model. Thanks to a selective method of annotation, we can reduce by three orders of magnitude the annotation work for chiasmus, and by one order of magnitude the work for epanaphora and epiphora. In this way, we prove that it is feasible to develop a system for detecting the three figures without an unsurmountable amount of human work. Finally, we propose an evaluation scheme and apply it to our models. The evaluation reveals that, even with a very incompletely annotated corpus, a system for repetitive figure detection can be trained to achieve reasonable accuracy. We investigate the impact of different linguistic features, including length, n-grams, part-of-speech tags, and syntactic roles, and find that different features are useful for different figures. We also apply the system to four different types of text: political discourse, fiction, titles of articles and novels, and quotations. Here the evaluation shows that the system is robust to shifts in genre and that the frequencies of the three rhetorical figures vary with genre.	acta informatica;grams;international standard book number;lazy evaluation;n-gram;part-of-speech tagging;sensor;significant figures	Marie Dubremetz	2017			literature;computational linguistics;chiasmus;rhetorical question;art	NLP	-31.03938729537187	-73.98003694993437	8467
c4ec46f342b0a61b3117570e85af087b85614f61	poseidon - bringing assistive technology to people with down syndrome: results of a three year european project	down syndrome;ict;autonomy;inclusion;integration into society;smart environment;user-centered design	The POSEIDON project aimed to increase the independence and autonomy of people with Down syndrome with the help of technical assistants. It followed a user-centered approach by involving people with Down syndrome and their parents, carers etc. A requirement analysis was the first step of the project. It became clear that people with Down syndrome especially need support in the areas of time management, mobility and money handling. Different applications were developed which were tested and evaluated in two field tests in three countries. Results indicate that POSEIDON can help to overcome daily challenges and that it can increase the autonomy and independence of people with Down syndrome.	assistive technology;autonomy;down syndrome;handling (psychology);requirements analysis;user-centered design	Anne Engler;Eva Schulze	2017	Studies in health technology and informatics	10.3233/978-1-61499-759-7-169	nursing;down syndrome;medicine	HCI	-59.27285006394638	-57.128338803377716	8477
09249262b68facf2ee38e65fde115eca4b771396	machine learning techniques for business blog search and mining	search engine;blog;data mining;weblog;probabilistic model;machine learning;term weighting;latent semantic analysis;probabilistic latent semantic analysis	Weblogs, or blogs, have rapidly gained in popularity over the past few years. In particular, the growth of business blogs that are written by or provide commentary on businesses and companies opens up new opportunities for developing blog-specific search and mining techniques. In this paper, we propose probabilistic models for blog search and mining using two machine learning techniques, latent semantic analysis (LSA) and probabilistic latent semantic analysis (PLSA). We implement the models in our database of business blogs, BizBlogs07, with the aim of achieving higher precision and recall. The probabilistic model is able to segment the business blogs into separate topic areas, which is useful for keywords detection on the blogosphere. Various term-weighting schemes and factor values were also studied in detail, which reveal interesting patterns in our database of business blogs. Our multi-functional business blog system is indeed found to be very different from existing blog search engines, as it aims to provide better relevance and precision of the search. 2007 Elsevier Ltd. All rights reserved.	blogosphere;categorization;corporate blog;data mining;directory (computing);experiment;machine learning;precision and recall;probabilistic latent semantic analysis;real-time transcription;relevance;similarity search;statistical model;web search engine	Yun Chen;Flora S. Tsai;Kap Luk Chan	2008	Expert Syst. Appl.	10.1016/j.eswa.2007.07.015	statistical model;latent semantic analysis;computer science;data science;machine learning;data mining;probabilistic latent semantic analysis;world wide web;search engine	ML	-23.83350761072091	-56.63429811776316	8486
bd73ec7bac51a332e5833bb1a02bcd3bf7a79474	click model-based information retrieval metrics	universiteitsbibliotheek;information retrieval measures;evaluation;user behavior;click models	In recent years many models have been proposed that are aimed at predicting clicks of web search users. In addition, some information retrieval evaluation metrics have been built on top of a user model. In this paper we bring these two directions together and propose a common approach to converting any click model into an evaluation metric. We then put the resulting model-based metrics as well as traditional metrics (like DCG or Precision) into a common evaluation framework and compare them along a number of dimensions.  One of the dimensions we are particularly interested in is the agreement between offline and online experimental outcomes. It is widely believed, especially in an industrial setting, that online A/B-testing and interleaving experiments are generally better at capturing system quality than offline measurements. We show that offline metrics that are based on click models are more strongly correlated with online experimental outcomes than traditional offline metrics, especially in situations when we have incomplete relevance judgements.	definite clause grammar;evaluation function;experiment;forward error correction;information retrieval;online and offline;relevance;web search engine	Aleksandr Chuklin;Pavel Serdyukov;Maarten de Rijke	2013		10.1145/2484028.2484071	computer science;evaluation;machine learning;data mining;multimedia;world wide web;information retrieval	Web+IR	-37.17516972366858	-54.565703887488326	8489
a10ac451d9408f29cc36d276912d6035979e9831	a web-based system for cross-validating optical character recognition (ocr) systems			optical character recognition	Luis E. Selva;Frank Meng;Beth Katcher;Robert Hall;Leonard W. D'Avolio;Saiju Pyarajan	2012			web application;computer vision;optical character recognition;artificial intelligence;intelligent character recognition;intelligent word recognition;computer science	Robotics	-8.745188147403633	-98.11602483952379	8490
fc1001c16a9610ceda76f18dfffbab4ca6186f53	developing automatic articulation, phonation and accent assessment techniques for speakers treated for advanced head and neck cancer	oropharyngeal cancer;outcomes;speech;phonological features;automatic evaluation;universiteitsbibliotheek;features;technology and engineering;ampex;phonemic features;quality;substitution voices;disorders;perceptual evaluation;intelligibility;head and neck cancer	Purpose: To develop automatic assessment models for assessing the articulation, phonation and accent of speakers with head and neck cancer (Experiment 1) and to investigate whether the models can track changes over time (Experiment 2). Method: Several speech analysis methods for extracting a compact acoustic feature set that characterizes a speaker's speech are investigated. The effectiveness of a feature set for assessing a variable is assessed by feeding it to a linear regression model and by measuring the mean difference between the outputs of that model for a set of recordings and the corresponding perceptual scores for the assessed variable (Experiment 1). The models are trained and tested on recordings of 55 speakers treated non-surgically for advanced oral cavity, pharynx and larynx cancer. The perceptual scores are average unscaled ratings of a group of 13 raters. The ability of the models to track changes in perceptual scores over time is also investigated (Experiment 2). Results: Experiment 1 has demonstrated that combinations of feature sets generally result in better models, that the best articulation model outperforms the average human rater's performance and that the best accent and phonation models are deemed competitive. Scatter plots of computed and observed scores show, however, that especially low perceptual scores are difficult to assess automatically. Experiment 2 has shown that the articulation and phonation models show only variable success in tracking trends over time and for only one of the time pairs are they deemed compete with the average human rater (Experiment 2). Nevertheless, there is a significant level of agreement between computed and observed trends when considering only a coarse classification of the trend into three classes: clearly positive, clearly negative and minor differences. Conclusions: A baseline tool to support the multi-dimensional evaluation of speakers treated non-surgically for advanced head and neck cancer now exists. More work is required to further improve the models, particularly with respect to their ability to assess low-quality speech.	biconnected component	Renee Peje Clapham;Catherine Middag;Frans J. M. Hilgers;Jean-Pierre Martens;Michiel W. M. van den Brekel;R. J. J. H. van Son	2014	Speech Communication	10.1016/j.specom.2014.01.003	speech recognition;computer science;speech;linguistics;intelligibility	NLP	-10.998804171034894	-82.83386462386356	8491
5ecd5d1cb8559b65bda4205bad21f942595b2bf7	the bbn byblos continuous speech recognition system	word-pair grammar;recognition system;management database;recognition accuracy;error rate;word error rate;phonetic coarticulation;byblos system;bbn byblos continuous speech;detailed phonetic model;context dependent;resource manager;hidden markov model	In this paper we describe the algorithms used in the BBN BYBLOS Continuous Speech Recognition system. The BYBLOS system uses context-dependent hidden Markov models of phonemes to provide a robust model of phonetic coarticulation. We provide an update of the ongoing research aimed at improving the recognition accuracy. In the first experiment we confirm the large improvement in accuracy that can be derived by using spectral derivative parameters in the recognition. In particular, the word error rate is reduced by a factor of two. Currently the system achieves a word error rate of 2.9% when tested on the speaker-dependent part of the standard 1000-Word DARPA Resource Management Database using the Word-Pair grammar supplied with the database. When no grammar was used, the error rate is 15.3%. Finally, we present a method for smoothing the discrete densities on the states of the HMM, which is intended to alleviate the problem of insufficient training for detailed phonetic models.	algorithm;codebook;context-sensitive language;hidden markov model;markov chain;quantization (signal processing);smoothing;speech recognition;steady state;word error rate	Richard M. Schwartz;Chris Barry;Yen-Lu Chow;Alan Deft;Ming-Whei Feng;Owen Kimball;Francis Kubala;John Makhoul;Jeffrey Vandegrift	1989			natural language processing;speech recognition;word error rate;computer science;resource management;pattern recognition;hidden markov model	ML	-19.38212755145375	-87.89948650492055	8539
d02cf062b4e2d4a7f03fc0fb2f980f3a9b5c7e9b	query-by-example spoken term detection for oov terms	spoken term detection;performance measure;large vocabulary continuous speech recognition systems;named entities;open vocabulary search;query by example search;lattices;query processing;large vocabulary continuous speech recognition;speech stream;out of vocabulary;speech processing;vocabulary;transducers;speech;oov terms;foreign words;data mining;phonetic lattices;speech content;out of vocabulary query terms;actual term weighted value;indexes;lattices vocabulary information retrieval speech recognition transducers indexing music information retrieval natural languages speech processing acoustic signal detection;indexing;region of interest;reference pronunciations query by example spoken term detection oov terms spoken term detection technology open vocabulary search speech content speech stream named entities foreign words large vocabulary continuous speech recognition systems query by example search out of vocabulary query terms finite state transducer indexing system phonetic lattices actual term weighted value;term weighting;indexation;indexing system;vocabulary indexing query processing speech processing speech recognition;speech recognition;query by example;reference pronunciations;finite state transducer;named entity;query by example spoken term detection;spoken term detection technology	The goal of Spoken Term Detection (STD) technology is to allow open vocabulary search over large collections of speech content. In this paper, we address cases where search term(s) of interest (queries) are acoustic examples. This is provided either by identifying a region of interest in a speech stream or by speaking the query term. Queries often relate to named-entities and foreign words, which typically have poor coverage in the vocabulary of Large Vocabulary Continuous Speech Recognition (LVCSR) systems. Throughout this paper, we focus on query-by-example search for such out-of-vocabulary (OOV) query terms. We build upon a finite state transducer (FST) based search and indexing system [1] to address the query by example search for OOV terms by representing both the query and the index as phonetic lattices from the output of an LVCSR system. We provide results comparing different representations and generation mechanisms for both queries and indexes built with word and combined word and subword units [2]. We also present a two-pass method which uses query-by-example search using the best hit identified in an initial pass to augment the STD search results. The results demonstrate that query-by-example search can yield a significantly better performance, measured using Actual Term-Weighted Value (ATWV), of 0.479 when compared to a baseline ATWV of 0.325 that uses reference pronunciations for OOVs. Further improvements can be obtained with the proposed two pass approach and filtering using the expected unigram counts from the LVCSR system's lexicon.	acoustic cryptanalysis;baseline (configuration management);entity;experiment;finite-state transducer;hybrid system;lexicon;n-gram;query by example;region of interest;std bus;speech analytics;speech recognition;substring;text-based (computing);vocabulary	Carolina Parada;Abhinav Sethy;Bhuvana Ramabhadran	2009	2009 IEEE Workshop on Automatic Speech Recognition & Understanding	10.1109/ASRU.2009.5373341	natural language processing;database index;finite state transducer;search engine indexing;speech recognition;transducer;computer science;speech;query by example;pattern recognition;lattice;speech processing;region of interest	NLP	-22.359171916508032	-83.4860050412369	8547
c04688fe30eb96eb0c977f090a87999ee5cfb312	multimedia translation for linking visual data to semantics in videos	automatic image annotation;image annotation;multimedia systems;multimedia data;semantic gap	The semantic gap problem, which can be referred to as the disconnection between low-level multimedia data and high-level semantics, is an important obstacle to build real-world multimedia systems. The recently developed methods that can use large volumes of loosely labeled data to provide solutions for automatic image annotation stand as promising approaches toward solving this problem. In this paper, we are interested in how some of these methods can be applied to semantic gap problems that appear in other application domains beyond image annotation. Specifically, we introduce new problems that appear in videos, such as the linking of keyframes with speech transcript text and the linking of faces with names. In a common framework, we formulate these problems as the problem of finding missing correspondences between visual and semantic data and apply the multimedia translation method. We evaluate the performance of the multimedia translation method on these problems and compare its performance against other auto-annotation and classifier-based methods. The experiments, carried out on over 300 h of news videos from TRECVid 2004 and TRECVid 2006 corpora, show that the multimedia translation method provides a performance that is comparable to the other auto-annotation methods and superior performance compared to other classifier-based methods.	automatic image annotation;experiment;facial recognition system;high- and low-level;key frame;machine learning;machine translation;relevance;statistical machine translation;text corpus	Pinar Duygulu Sahin;Muhammet Bastan	2009	Machine Vision and Applications	10.1007/s00138-009-0217-8	computer vision;computer science;multimedia;automatic image annotation;world wide web;information retrieval;semantic gap	AI	-15.24254134314366	-68.50477100677132	8559
25ca4f4424cf868c2b258cc4e23c46ba329b01ed	generative models for semantic role labeling	generic model;semantic role labeling	This paper describes the four entries from the University of Utah in the semantic role labeling task of SENSEVAL-3. All the entries took a statistical machine learning approach, using the subset of the FrameNet corpus provided by S ENSEVAL-3 as training data. Our approach was to develop a model of natural language generation from semantics, and train the model using maximum likelihood and smoothing. Our models performed satisfactorily in the competition, and can flexibly handle varying permutations of provided versus inferred information.	framenet;generative model;label printer applicator;machine learning;natural language generation;question answering;semantic analysis (compilers);semantic role labeling;shallow parsing;smoothing;sparse matrix	Cynthia Thompson;Siddharth Patwardhan;Carolin Arnold	2004			natural language processing;computer science;pattern recognition;data mining	NLP	-20.091980483388753	-76.74595111614191	8578
291c1020aafa265aaf7d8a99721cc2e1c507d094	analyzing recommender systems for health promotion using a multidisciplinary taxonomy: a scoping review	behavior change;health intervention;health promotion;patient;recommendation;recommender system;tailoring;taxonomy	"""BACKGROUND Recommender systems are information retrieval systems that provide users with relevant items (e.g., through messages). Despite their extensive use in the e-commerce and leisure domains, their application in healthcare is still in its infancy. These systems may be used to create tailored health interventions, thus reducing the cost of healthcare and fostering a healthier lifestyle in the population.   OBJECTIVE This paper identifies, categorizes, and analyzes the existing knowledge in terms of the literature published over the past 10 years on the use of health recommender systems for patient interventions. The aim of this study is to understand the scientific evidence generated about health recommender systems, to identify any gaps in this field to achieve the United Nations Sustainable Development Goal 3 (SDG3) (namely, """"Ensure healthy lives and promote well-being for all at all ages""""), and to suggest possible reasons for these gaps as well as to propose some solutions.   METHODS We conducted a scoping review, which consisted of a keyword search of the literature related to health recommender systems for patients in the following databases: ScienceDirect, PsycInfo, Association for Computing Machinery, IEEExplore, and Pubmed. Further, we limited our search to consider only English-language journal articles published in the last 10 years. The reviewing process comprised three researchers who filtered the results simultaneously. The quantitative synthesis was conducted in parallel by two researchers, who classified each paper in terms of four aspects-the domain, the methodological and procedural aspects, the health promotion theoretical factors and behavior change theories, and the technical aspects-using a new multidisciplinary taxonomy.   RESULTS Nineteen papers met the inclusion criteria and were included in the data analysis, for which thirty-three features were assessed. The nine features associated with the health promotion theoretical factors and behavior change theories were not observed in any of the selected studies, did not use principles of tailoring, and did not assess (cost)-effectiveness.   DISCUSSION Health recommender systems may be further improved by using relevant behavior change strategies and by implementing essential characteristics of tailored interventions. In addition, many of the features required to assess each of the domain aspects, the methodological and procedural aspects, and technical aspects were not reported in the studies.   CONCLUSIONS The studies analyzed presented few evidence in support of the positive effects of using health recommender systems in terms of cost-effectiveness and patient health outcomes. This is why future studies should ensure that all the proposed features are covered in our multidisciplinary taxonomy, including integration with electronic health records and the incorporation of health promotion theoretical factors and behavior change theories. This will render those studies more useful for policymakers since they will cover all aspects needed to determine their impact toward meeting SDG3."""		Santiago Hors-Fraile;Octavio Rivera;Francine Schneider;Luis Fernández-Luque;Francisco Luna-Perejon;Anton Civit-Balcells;Hein de Vries	2018	International journal of medical informatics	10.1016/j.ijmedinf.2017.12.018	recommender system;health intervention;health promotion;knowledge management;behavior change;psycinfo;multidisciplinary approach;health care;population;medicine	HCI	-61.88400376600742	-60.72844422347025	8581
790976d0345f73f63f5d218fa9c6546306db4496	softcardinality: learning to identify directional cross-lingual entailment from cardinalities and smt	feature extraction	In this paper we describe our system submitted for evaluation in the CLTE-SemEval-2013 task, which achieved the best results in two of the four data sets, and finished third in average. This system consists of a SVM classifier with features extracted from texts (and their translations SMT) based on a cardinality function. Such function was the soft cardinality. Furthermore, this system was simplified by providing a single model for the 4 pairs of languages obtaining better (unofficial) results than separate models for each language pair. We also evaluated the use of additional circular-pivoting translations achieving results 6.14% above the best official results.	baudot code;cardinality (data modeling);grams;satisfiability modulo theories;semeval	Sergio Jiménez;Claudia Jeanneth Becerra;Alexander F. Gelbukh	2013			natural language processing;feature extraction;computer science;machine learning;data mining;algorithm	NLP	-22.456122730247657	-72.57823325930288	8587
8fba583cdd0d01622cf6f4d2ea28a9495908bae0	long-distance continuous space language modeling for speech recognition		The n-gram language models has been the most frequently used language model for a long time as they are easy to build models and require the minimum effort for integration in different NLP applications. Although of its popularity, n-gram models suffer from several drawbacks such as its ability to generalize for the unseen words in the training data, the adaptability to new domains, and the focus only on short distance word relations. To overcome the problems of the n-gram models the continuous parameter space LMs were introduced. In these models the words are treated as vectors of real numbers rather than of discrete entities. As a result, semantic relationships between the words could be quantified and can be integrated into the model. The infrequent words are modeled using the more frequent ones that are semantically similar. In this paper we present a long distance continuous language model based on a latent semantic analysis (LSA). In the LSA framework, the word-document co-occurrence matrix is commonly used to tell how many times a word occurs in a certain document. Also, the word-word co-occurrence matrix is used in many previous studies. In this research, we introduce a different representation for the text corpus, this by proposing long-distance word co-occurrence matrices. These matrices to represent the long range co-occurrences between different words on different distances in the corpus. By applying LSA to these matrices, words in the vocabulary are moved to the continuous vector space. We represent each word with a continuous vector that keeps the word order and position in the sentences. We use tied-mixture HMM modeling (TM-HMM) to robustly estimate the LM parameters and word probabilities. Experiments on the Arabic Gigaword corpus show improvements in the perplexity and the speech recognition results compared to the conventional n-gram.	language model;speech recognition	Mohamed Talaat;Sherif Abdou;Mahmoud Shoman	2015		10.1007/978-3-319-18117-2_41	natural language processing;speech technology;speaker recognition;speech recognition;speech corpus;language model	NLP	-19.746510661738878	-79.26172298577761	8604
446f325295f7d3a9dd79f763cc60b6de6b45de30	análisis sintáctico para el español basado en el formalismo de la teoría significado-texto		The application of the Meaning ⇔ Text Theory to Spanish parsing is presented. This formalism is based on dependency grammars. The combinatorial dictionary of this method is employed for the syntactic analysis; it consists of patterns for words, mainly verbs, where all its valences and the way they are realized are described. In this method, no fixed word order in the sentence is considered so it is highly adequate for Spanish parsing. The patterns of the combinatorial dictionary help not only to reduce the number of possible variants obtained from the parser but they include information of the syntactic level related to the semantic of the word which is required in deep levels of language analysis. We include statistical information of realizations for each valence and statistics of valence combinations for each verb in order to increase the efficiency of ambiguity resolution in syntactic analysis.	dependency grammar;dictionary;formal system;linear algebra;parsing	Sofía N. Galicia-Haro;Alexander F. Gelbukh;Igor A. Bolshakov	2002	Procesamiento del Lenguaje Natural			NLP	-27.964140107486074	-77.16914022821497	8610
960c226a871414d1e8b510c0e99007858ddef832	a rich morphological tagger for english: exploring the cross-linguistic tradeoff between morphology and syntax		A traditional claim in linguistics is that all human languages are equally expressive— able to convey the same wide range of meanings. Morphologically rich languages, such as Czech, rely on overt inflectional and derivational morphology to convey many semantic distinctions. Languages with comparatively limited morphology, such as English, should be able to accomplish the same using a combination of syntactic and contextual cues. We capitalize on this idea by training a tagger for English that uses syntactic features obtained by automatic parsing to recover complex morphological tags projected from Czech. The high accuracy of the resulting model provides quantitative confirmation of the underlying linguistic hypothesis of equal expressivity, and bodes well for future improvements in downstream HLT tasks including machine translation.	brill tagger;downstream (software development);encode;galaxy morphological classification;heuristic (computer science);machine translation;mathematical morphology;parsing;scalability	Ryan Cotterell;Christo Kirov;John Sylak-Glassman;Rebecca Knowles;Matt Post	2017			computer science;artificial intelligence;morphology (linguistics);natural language processing;syntax;linguistics	NLP	-26.626831704977892	-75.87272074521373	8640
782a304ecd0c5407693e17c62d3264c8a4f91609	panel: the state of the art in thai language processing		This paper reviews the current state of technology and research progress in the Thai language processing. It resumes the characteristics of the Thai language and the approaches to overcome the difficulties in each processing task. 1 Some Problematic Issues in the Thai Processing It is obvious that the most fundamental semantic unit in a language is the word. Words are explicitly identified in those languages with word boundaries. In Thai, there is no word boundary. Thai words are implicitly recognized and in many cases, they depend on the individual judgement. This causes a lot of difficulties in the Thai language processing. To illustrate the problem, we employed a classic English example. The segmentation of “GODISNOWHERE”. No. Segmentation Meaning (1) God is now here. God is here. (2) God is no where. God doesn’t exist. (3) God is nowhere. God doesn’t exist. With the different segmentations, (1) and (2) have absolutely opposite meanings. (2) and (3) are ambiguous that nowhere is one word or two words. And the difficulty becomes greatly aggravated when unknown words exist. As a tonal language, a phoneme with different tone has different meaning. Many unique approaches are introduced for both the tone generation in speech synthesis research and tone recognition in speech recognition research. These difficulties propagate to many levels in the language processing area such as lexical acquisition, information retrieval, machine translation, speech processing, etc. Furthermore the similar problem also occurs in the levels of sentence and paragraph. 2 Word and Sentence Segmentation The first and most obvious problem to attack is the problem of word identification and segmentation. For the most part, the Thai language processing relies on manually created dictionaries, which have inconsistencies in defining word units and limitation in the quantity. [1] proposed a word extraction algorithm employing C4.5 with some string features such as entropy and mutual information. They reported a result of 85% in precision and 50% in recall measures. For word segmentation, the longest matching, maximal matching and probabilistic segmentation had been applied in the early research [2], [3]. However, these approaches have some limitations in dealing with unknown words. More advanced techniques of word segmentation captured many language features such as context words, parts of speech, collocations and semantics [4], [5]. These reported about 95-99 % of accuracy. For sentence segmentation, the trigram model was adopted and yielded 85% of accuracy [6]. 3 Machine Translation Currently, there is only one machine translation system available to the public, called ParSit (http://www. links.nectec.or.th/services/parsit), it is a service of English-to-Thai webpage translation. ParSiT is a collaborative work of NECTEC, Thailand and NEC, Japan. This system is based on an interlingual approach MT and the translation accuracy is about 80%. Other approaches such as generate-and-repair [7] and sentence pattern mapping have been also studied [8]. 4 Language Resources The only Thai text corpus available for research use is the ORCHID corpus. ORCHID is a 9-MB Thai part-of-speech tagged corpus initiated by NECTEC, Thailand and Communications Research Laboratory, Japan. ORCHID is available at http://www.links.nectec.or.th /orchid. 5 Research in Thai OCR Frequently used Thai characters are about 80 characters, including alphabets, vowels, tone marks, special marks, and numerals. Thai writing are in 4 levels, without spaces between words, and the problem of similarity among many patterns has made research challenging. Moreover, the use of English and Thai in general Thai text creates many more patterns which must be recognized by OCR. For more than 10 years, there has been a considerable growth in Thai OCR research, especially for “printed character” task. The early proposed approaches focused on structural matching and tended towards neural-networkbased algorithms with input for some special characteristics of Thai characters e.g., curves, heads of characters, and placements. At least 3 commercial products have been launched including “ArnThai” by NECTEC, which claims to achieve 95% recognition performance on clean input. Recent technical improvement of ArnThai has been reported in [9]. Recently, focus has been changed to develop system that are more robust with any unclean scanning input. The approach of using more efficient features, fuzzy algorithms, and document analysis is required in this step. At the same time, “Offline Thai handwritten character recognition” task has been investigated but is only in the research phase of isolated characters. Almost all proposed engines were neural network-based with several styles of input features [10], [11]. There has been a small amount of research on “Online handwritten character recognition”. One attempt was proposed by [12], which was also neural networkbased with chain code input. 6 Thai Speech Technology Regarding speech, Thai, like Chinese, is a tonal language. The tonal perception is important to the meaning of the speech. The research currently being done in speech technology can be divided into 3 major fields: (1) speech analysis, (2) speech recognition and (3) speech synthesis. Most of the research in (1) done by the linguists are on the basic study of Thai phonetics e.g. [13]. In speech recognition, most of the current research [14] focus on the recognition of isolated words. To develop continuous speech recognition, a large-scale speech corpus is needed. The status of practical research on continuous speech recognition is in its initial step with at least one published paper [15]. In contrast to western speech recognition, topics specifying tonal languages or tone recognition have been deeply researched as seen in many papers e.g., [16]. For text-to-speech synthesis, processing the idiosyncrasy of Thai text and handling the tones interplaying with intonation are the topics that make the TTS algorithm for the Thai language differrent from others. In the research, the first successful system was accomplished by [14] and later by NECTEC [15]. Both systems employ the same synthesis technique based on the concatenation of demisyllable inventory units.	artificial neural network;c4.5 algorithm;chain code;collocation;concatenation;dictionary;fuzzy concept;handwriting recognition;information retrieval;machine translation;matching (graph theory);maximal set;microsoft word for mac;mutual information;netware file system;online and offline;optical character recognition;printing;probabilistic automaton;speech corpus;speech processing;speech recognition;speech synthesis;speech technology;text corpus;text segmentation;trigram;voice analysis;web page	Virach Sornlertlamvanich;Tanapong Potipiti;Chai Wutiwiwatchai;Pradit Mittrapiyanuruk	2000			natural language processing;artificial intelligence;computer science;speech recognition	NLP	-24.969685474756613	-81.46408953069442	8641
fef6da757d7aefb1e649f89b57dc6054fd420d1a	a comparison of three pre-processing methods for improving main content extraction from hyperlink rich web documents		Most HTML web documents on the World Wide Web contain a lot of hyperlinks in the body of main content area and additional areas. As extraction of the main content of such hyperlink rich web documents is rather complicated, three simple and language-independent pre-processing main content extraction methods are addressed in this paper to deal with the hyperlinks for identifying the main content accurately. To evaluate and compare the presented methods, each of these three methods is combined with a prominent main content extraction method, called DANAg. The obtained results show that one of the methods delivers a higher performance in term of effectiveness in comparison with the other two suggested methods.	html;hyperlink;information systems;language-independent specification;preprocessor;web page;wikipedia;world wide web	Moheb Ghorbani;Hadi Mohammadzadeh;Abdolreza Nazemi	2014		10.5220/0004947503350339	database;world wide web;information retrieval	Web+IR	-28.826563588660004	-56.244528043429	8644
bb3a5e651b55aaa413a4c46694f9085e6cafc26b	using micro-documents for feature selection: the case of ordinal text classification	supervised learning;text classification;feature selection;ordinal regression	Most popular feature selection methods for text classification (TC) are based on binary information concerning the presence/absence of the feature in each training document. As such, these methods do not exploit term frequency information. In order to overcome this drawback we break down each training document of length k into k training “microdocuments”, each consisting of a single word occurrence and endowed with the class information of the original training document. We study the impact of this strategy in the case of ordinal TC; the experiments show that this strategy substantially improves effectiveness.	document classification;experiment;feature selection;ordinal data;statistical classification;tf–idf	Stefano Baccianella;Andrea Esuli;Fabrizio Sebastiani	2011	Expert Syst. Appl.	10.1016/j.eswa.2013.02.010	ordinal regression;computer science;machine learning;linear classifier;pattern recognition;data mining;supervised learning;feature selection	NLP	-20.752221005886415	-64.4561342041205	8645
58b6e5ccff252c07fea221f2c8f792ed2a93cced	standardisation of ergonomic assessment of speech communication	standardisation;speech communication	The purpose of standardisation of the ergonomic assessment of speech communications is to assure a certain level of speech communication quality for various applications. The quality of speech communications is assessed in case of warning, danger, or information messages for work places, public areas, meeting rooms, and auditoria. In many applications direct communication between humans is considered while in other applications the use of electro-acoustic systems (e.g., PA systems) will be the most convenient means of informing and instructing people present. A standard on this subject, under the responsibility of ISO and CEN, is in preparation. The draft version of this standard covers criteria for speech communication quality in various applications, methods to predict the speech transmission quality, and methods to assess the quality (subjective and objective). Examples of several applications are given in annexes of that standard.	acoustic cryptanalysis;human factors and ergonomics;humans	Herman J. M. Steeneken	1999			speech recognition;human factors and ergonomics;computer science	Crypto	-42.9537816638788	-78.24242240556497	8683
fb93fc420504d1ea49a4bdc7c8662b100e88983c	a comparative study on human communication behaviors and linguistic characteristics for speech-to-speech translation.		A large bilingual corpus of English and Japanese is being built at ATR Spoken Language Translation Research Laboratories in order to improve speech translation technology to the level where people can use a portable translation system for traveling abroad, dining and shopping, and hotel situations. As a part of these corpus construction activities, we have been collecting spoken dialogue data by using an experimental translation system between English and Japanese. In a previous study, we found that humans communicate as part of their daily social life, so they prefer using complex sentences and saying than one sentence per utterance. However, corpus-based machine translation systems for conversational expressions tend to be limited to dealing with short simple sentences. To find a way to bridge the gap between human communication behaviors and system performance, we examined the relationship between instructions and linguistic expressions. The experimental results suggest that a state-of-the-art translation system may be useful for subjects who can make their utterance length short by following instructions.	answer to reset;humans;machine translation	Toshiyuki Takezawa;Gen-ichiro Kikui	2004			natural language processing;linguistics	NLP	-26.31830829794921	-84.43333110377223	8696
cacf8ae4ed09bcd101e6ac0f88b1f7a8287273a1	acquisition of character translation rules for supporting snomed ct localizations		Translating huge medical terminologies like SNOMED CT is costly and time consuming. We present a methodology that acquires substring substitution rules for single words, based on the known similarity between medical words and their translations, due to their common Latin / Greek origin. Character translation rules are automatically acquired from pairs of English words and their automated translations to German. Using a training set with single words extracted from SNOMED CT as input we obtained a list of 268 translation rules. The evaluation of these rules improved the translation of 60% of words compared to Google Translate and 55% of translated words that exactly match the right translations. On a subset of words where machine translation had failed, our method improves translation in 56% of cases, with 27% exactly matching the gold standard.		José Antonio Miñarro-Giménez;Johannes Hellrich;Stefan Schulz	2015	Studies in health technology and informatics	10.3233/978-1-61499-512-8-597	data mining;snomed ct;computer science	NLP	-33.93642769807062	-70.62868880745108	8713
c892a77cabbb7085c8be24e64b937ac747e3487b	expedited breast care: a new model in breast health	treatment planning;breast cancer detection;diagnostic imaging;ultrasound;multidisciplinary approach;clinical breast examination;screening test;breast cancer	Women in the U.S. have a 1 in 7 lifetime risk for developing breast cancer. The breast cancer detection process from mammography to clinical consultation with the medical oncologist is often at minimum 28 days. The pathway of diagnosis to treatment generally begins with either an abnormal screening test or an abnormal self or clinical breast examination. This initial warning sign may be followed by diagnostic imaging which may lead to a diagnostic biopsy performed by a breast radiologist under mammography, ultrasound or MRI guidance by a breast surgeon. Once the biopsy is performed, the tissue is processed and read by a pathologist who generates a report and sends it to the clinical site where the biopsy was obtained. If the diagnosis is malignant, the patient is informed and surgical consultation is scheduled. Consultation with the medical oncologist is generally scheduled after the definitive surgery has been performed. The medical oncology consultation is set up to define further treatment planning. Increasingly, the appointment for medical oncology is scheduled prior to definitive surgery to assure a more multidisciplinary approach to breast cancer care.		A. M. Lopez;G. Barker;A. Bhattacharryya;K. Scott;M. Descour;L. Richter;James Davenport;S. Lazarus;L. Kreykes;A. Valencia;R. Weinstein	2008		10.1007/978-3-540-70538-3_72	radiology;medicine;pathology;gynecology	HCI	-58.83432675020646	-66.04373643820587	8720
0562c16e80cd048e7edf49f000597723adf9c1c8	transform query workbench	biological patents;health informatics;biomedical journals;text mining;europe pubmed central;citation search;citation networks;research articles;abstracts;open access;life sciences;clinical guidelines;full text;r medicine general;rest apis;orcids;europe pmc;biomedical research;biomedicine general;bioinformatics;literature search	Description The Query Workbench is a tool of the TRANSFoRm project to support clinical studies and database research. It provides an interface to author, store and deploy queries of clinical data to identify potential subjects for clinical studies and can thus support the clinical study feasibility evaluation. The development of the Query Workbench system is driven by the TRANSFoRm Clinical Research Information Model (CRIM) [1], enabling the creation of a semantically aware software tool for	information model;programming tool;workbench	Theodoros N. Arvanitis;Wolfgang Kuchinke	2015		10.1186/2043-9113-5-S1-S16	health informatics;text mining;medical research;medicine;computer science;bioinformatics;data science;nursing;data mining;information retrieval	SE	-52.20900582986915	-64.97698091183224	8721
0b4b4c109031f7da951f263805431c96dde2055d	inferring intentional agents from violation of randomness		Humans have a strong “cognitive compulsion” to infer intentional agents from violation of randomness and such an agency–nonrandomness link emerges early in development. In two studies, we directly quantified, formalized, and compared both ends of this link for the first time. In Experiment 1, two groups of participants viewed the same 256 binary sequences (e.g., AABAAABA) and classified each as generated by agents/non-agents or by nonrandom/random processes. We found a strong correlation between two judgments: sequences viewed as more agentive also tended to be judged as less random. In Experiment 2, another two groups were asked to produce sequences that others might appreciate as agentive or nonrandom. Participant-generated sequences in the two conditions had a substantial overlap, indicating common guiding principles of agency and nonrandomness generation. Taken together, the present studies provide evidence for a shared cognitive basis of agency detection and subjective randomness.	humans;randomness;stochastic process	Yuan Meng;Tom Griffiths;Fei Xu	2017			cognitive psychology;social psychology;psychology;randomness	SE	-50.50591134364894	-56.56157968822598	8727
178ec08e6475b396631b278e204eed9b6aa1de68	uottawa: system description for semeval 2013 task 2 sentiment analysis in twitter		We present two systems developed at the University of Ottawa for the SemEval 2013 Task 2. The first system (for Task A) classifies the polarity / sentiment orientation of one target word in a Twitter message. The second system (for Task B) classifies the polarity of whole Twitter messages. Our two systems are very simple, based on supervised classifiers with bag-ofwords feature representation, enriched with information from several sources. We present a few additional results, besides results of the submitted runs.	bag-of-words model;cross-validation (statistics);semeval;sentiment analysis;supervised learning;test data	Hamid Poursepanj;Josh Weissbock;Diana Inkpen	2013			computer science;data mining;world wide web	NLP	-22.29484364303493	-69.60563089597116	8730
124cac87031145a0cd16d959c32130052b77ba78	human and dog: an experimental game using unequal communication mechanic	unequal communicative capabilities;puzzle solving;game design;cooperative game	Inequality of communication capabilities may cause an awkward experience in real life. %However, in terms of gaming, unequal communication mechanic can make games more challenging and interesting. However, in terms of gaming, this can be a challenging and interesting mechanic. Charades[1], where one player tries to express a word via body language while others try to guess it, is a good example of such cases. Different from most digital co-op games, which equal communication mechanics have been widely used, we experiment with this unequal communication mechanic. Thus, we have designed a digital co-op game called getGameName, in which 2 avatars, a human and a dog, communicate to solve a series of puzzles with unequal communication capabilities. Ultimately, we conducted an experiment to evaluate our game. Several interesting patterns were discovered from our observation. We then concluded some design suggestions from our collected data for future reference.	avatar (computing);experiment;real life;social inequality;video game design	Hsincheng Hou;Kuan-Ting Chou;Mike Y. Chen	2017		10.1145/3024969.3025089	non-cooperative game;simulation;engineering;artificial intelligence;communication	HCI	-51.59638388705606	-52.98635372923005	8737
0c14ca3a71a27fd42f1c3ead03787f0a5b9cdb18	improving probabilistic latent semantic analysis with principal component analysis		Probabilistic Latent Semantic Analysis (PLSA) models have been shown to provide a better model for capturing polysemy and synonymy than Latent Semantic Analysis (LSA). However, the parameters of a PLSA model are trained using the Expectation Maximization (EM) algorithm, and as a result, the trained model is dependent on the initialization values so that performance can be highly variable. In this paper we present a method for using LSA analysis to initialize a PLSA model. We also investigated the performance of our method for the tasks of text segmentation and retrieval on personal-size corpora, and present results demonstrating the efficacy of our proposed approach.	expectation–maximization algorithm;information retrieval;mathematical optimization;principal component analysis;probabilistic latent semantic analysis;text corpus;text segmentation;time complexity	Ayman Farahat;Francine Chen	2006			machine learning;computer science;principal component analysis;initialization;latent semantic analysis;artificial intelligence;polysemy;probabilistic latent semantic analysis;expectation–maximization algorithm;text segmentation;pattern recognition;latent class model	NLP	-20.048574515630715	-76.86791656580448	8743
a536ae6760efe6f1e5114e0fa33e80dd2fb3f61d	assessing healthcare workforce training needs for health information technology-enabled healthcare transformation				Raven David;Morgan Moy;Virginia Lorenzi;Andrew Flatgard;Angie Lee;Bruce Forman;Lucy Appert;Jennifer Ringler;Gilad J. Kuperman;Victoria Tiase;Rita Kukafka	2016			family medicine;health care;workforce;nursing;workforce planning;teamwork;health information technology;medicine	AI	-57.82504073062219	-63.0139106390898	8747
5af49bcc128dc624738ea095eab672d7971f34e2	the electronic patient record in general practice - a south-african perspective		A research project was undertaken to design an electronic medical record system which will cater for the specific needs of South African general practitioners and their unique requirements. The project was launched by sending out a questionnaire to general practitioners in the Free State, trying to determine their specific requirements and to gain insight into their attitudes and opinions regarding the use of computers, especially the computer-based patient record, in a general practice setting. The results indicated that there was a need to replace the current manual patient record with an electronic version, and that the attitudes of the general practitioners in South Africa were generally favourable regarding implementing and using such a system. Given these results, we can proceed to design a computer-based patient record system, specifically addressing the needs and requirements of the South African general practitioner.		Lizette de Wet;Theo McDonald;Gawie Pistorius	1998	Studies in health technology and informatics	10.3233/978-1-60750-896-0-9		HCI	-59.78242373545628	-64.5187349492267	8760
b15e9ecb94015192b06936436789b636dafa59b8	web searching in chinese: a study of a search engine in hong kong	non-english web;search log;hong kong;search-query log;character-based analysis;n-gram analysis;mean number;search query;search topic;search engine;english search engine;chinese language	been conducted on the query logs in search engines that are primarily English-based (e.g., Excite and AltaVista), only a few of them have studied the information-seeking behavior on the Web in non-English languages. In this article, we report the analysis of the search-query logs of a search engine that focused on Chinese. Three months of search-query logs of Timway, a search engine based in Hong Kong, were collected and analyzed. Metrics on sessions, queries, search topics, and character usage are reported. N-gram analysis also has been applied to perform character-based analysis. Our analysis suggests that some characteristics identified in the search log, such as search topics and the mean number of queries per sessions, are similar to those in English search engines; however, other characteristics, such as the use of operators in query formulation, are significantly different. The analysis also shows that only a very small number of unique Chinese characters are used in search queries. We believe the findings from this study have provided some insights into further research in non-English Web searching.	chinese input methods for computers;excite;general-purpose markup language;information seeking behavior;internet;log analysis;n-gram;text-based (computing);web search engine;web search query;world wide web	Michael Chau;Xiao Fang;Christopher C. Yang	2007	JASIST	10.1002/asi.20592	web query classification;organic search;metasearch engine;semantic search;computer science;phrase search;data mining;linguistics;search analytics;web search query;world wide web;chinese;information retrieval;search engine	Web+IR	-34.53224048308642	-62.12208671933467	8792
445ddeb2d786ab4f9a4c6a14a1f14ebd06db481d	arabizi detection and conversion to arabic		Arabizi is Arabic text that is written using Latin characters. Arabizi is used to present both Modern Standard Arabic (MSA) or Arabic dialects. It is commonly used in informal settings such as social networking sites and is often with mixed with English. In this paper we address the problems of: identifying Arabizi in text and converting it to Arabic characters. We used word and sequence-level features to identify Arabizi that is mixed with English. We achieved an identification accuracy of 98.5%. As for conversion, we used transliteration mining with language modeling to generate equivalent Arabic text. We achieved 88.7% conversion accuracy, with roughly a third of errors being spelling and morphological variants of the forms in ground truth.	american and british english spelling differences;analog-to-digital converter;arabic chat alphabet;ground truth;label printer applicator;language identification;language model;sensor	Kareem Darwish	2014		10.3115/v1/W14-3629	natural language processing;speech recognition;modern arabic mathematical notation;linguistics	NLP	-25.284300836401368	-77.29284885039459	8793
986db8bed79a6ef31ad782da98341e1b106c9232	enhancing semantic web by semantic annotation: experiences in building an automatic conference calendar	semantic annotation;semantic information;semantic annotations;semantic web;patent mining;ontology;natural language processing	In this paper, we describe a Semantic Web application that builds a customizable conference calendar. In contrast to previous works aiming at manually creating a list of upcoming/current and past conferences, in this work we aim at providing a semantic conference calendar which automatically extracts information from the web using semantic annotation. In this system, to build a calendar, the user simply needs to specify what conferences he/she is interested in. The system finds, extracts, and updates the semantic information from the Web. We propose a unified approach for semantic annotation of the conference calendar. We also present evaluations of our approach on real-world data.	semantic web;web application;world wide web	Tsuyoshi Murata;Sakiko Moriyasu	2007	IEEE/WIC/ACM International Conference on Web Intelligence (WI'07)	10.1109/WI.2007.52	semantic data model;semantic interoperability;semantic computing;semantic integration;semantic web rule language;data web;explicit semantic analysis;semantic search;semantic grid;image retrieval;computer science;semantic web;ontology;social semantic web;data mining;semantic web stack;semantic technology;world wide web;owl-s;information retrieval;semantic analytics	DB	-29.94724456294816	-56.72222734373626	8819
e7aa9b9a655e7cd3e261c43ccc873d3dfb1a200a	idiomatic syntactic constructions and language learning	syntax;idiomatic constructions;cues;construction grammar;cross sentential learning cues;language patterns;syntaxe;language acquisition;statistical learning;acquisition du langage;phrase structure;experiments;language learning;second language learning;sentences;psycholinguistique;psycholinguistics	This article explores the influence of idiomatic syntactic constructions (i.e., constructions whose phrase structure rules violate the rules that underlie the construction of other kinds of sentences in the language) on the acquisition of phrase structure. In Experiment 1, participants were trained on an artificial language generated from hierarchical phrase structure rules. Some participants were given exposure to an idiomatic construction (IC) during training, whereas others were not. Under some circumstances, the presence of an idiomatic construction in the input aided learners in acquiring the phrase structure of the language. Experiment 2 provides a replication of the first experiment and extends the findings by showing that idiomatic constructions that strongly violate the predictive dependencies that define the phrase structure of the language do not aid learners in acquiring the structure of the language. Together, our data suggest that (a) idiomatic constructions aid learners in acquiring the phrase structure of a language by highlighting relevant structural elements in the language, and (b) such constructions are useful cues to learning to the extent that learners can keep their knowledge of the idiomatic construction separate from their knowledge of the rest of the language.	bond issues, construction;emoticon;languages;phrase structure rules;powerset construction;programming idiom;rule (guideline);sentence	Michael P. Kaschak;Jenny R. Saffran	2006	Cognitive science	10.1207/s15516709cog0000_44	language acquisition;natural language processing;computer science;linguistics;psycholinguistics;communication	NLP	-10.469658557210096	-77.32937975152262	8825
1988fe60e978e80b64e16e8109cea81629eda720	learning user communities for improving the services of information providers	unsupervised learning;learning algorithm;red www;fournisseur;data collection;large data sets;besoin utilisateur;necesidad usuario;supplier;algorithme apprentissage;service;communaute d usager;internet;user need;user community;world wide web;reseau www;information system;algoritmo aprendizaje;systeme information;user model;proveedor;sistema informacion;servicio	In this paper we propose a methodology for organising the users of an information providing system into groups with common interests (communities). The communities are built using unsupervised learning techniques on data collected from the users (user models). We examine a system that filters news on the Internet, according to the interests of the registered users. Each user model contains the user’s interests on the news categories covered by the information providing system. Two learning algorithms are evaluated: COBWEB and ITERATE. Our main concern is whether meaningful communities can be constructed. We specify a metric to decide which news categories are representative for each community. The construction of meaningful communities can be used for improving the structure of the information providing system as well as for suggesting extensions to individual user models. Encouraging results on a large data-set lead us to consider this work as a first step towards a method that can easily be integrated in a variety of information systems.	algorithm;cluster analysis;conceptual clustering;experiment;general material designation;information system;internet;machine learning;personally identifiable information;statistical model;supervised learning;unsupervised learning;user experience;user modeling	Georgios Paliouras;Christos Papatheodorou;Vangelis Karkaletsis;Constantine D. Spyropoulos;Victoria Malaveta	1998		10.1007/3-540-49653-X_22	unsupervised learning;the internet;simulation;user modeling;service;computer science;artificial intelligence;machine learning;data mining;database;world wide web;computer security;information system;statistics;data collection	Web+IR	-36.439843116762404	-57.29473317219503	8864
6021002577cff614b62de260673b191ad90e72aa	almost) automatic semantic feature extraction from technical text	information retrieval;semantics;feature extraction;natural language;information processing;acquisition	"""Acquisition of semantic information is necessary for proper understanding of natural language text. Such information is often domain-speclfic in nature and must be acquized from the domain. This causes a problem whenever a natural fanguage processing (NLP) system is moved from one domain to another. The portability of an NLP system can be improved if these semantic features can be acquired with ];mited human intervention. This paper proposes an approach towards (almost) automatic semantic feature extraction. I. I N T R O D U C T I O N Acquisition of semantic information is necessary for proper understanding of natural language text. Such information is often domaln-specific in nature and must be acquized from the domain. When an NLP system is moved from one domain to another, usually a substantial amount of time has to be spent in tailoring the system to the new domain. Most of this time is spent on acquiring the semantic features specific to that domain. It is important to automate the process of acquisition of semantic information as much as possible, and facilitate whatever human intervention is absolutely necessary. Portability of NLP systems has been of concern to researchers for some time [8, 5, 11, 9]. This paper proposes an approach to obtain the domaln-dependent semantic features of any given domain in a domain-independent manner. The next section will describe an existing NLP system (KUDZU) which has been developed at Mississippi State University. Section 3 will then present the motimstion behind the automatic acquisition of the semantic features of a domain, and a brief outline of the methodology proposed to do it. Section 4 will describe the dlf[ereut steps in this methodology in detail Section 5 will focus on the app]icatlons of the semantic features. Section 6 compares the proposed approach to R;ml]ar research efforts. The last section presents some final comments. 2. THE EXISTING KUDZU SYSTEM The research described in this paper is part of a larger ongoing project called the KUDZU (Knowledge Under Development from Zero Understanding) project. This project is aimed at exploring the automation of extraction of information from technical texts. The KUDZU system has two primary components an NLP component, and a Knowledge Analysis (KA) component. This section desoribes this system in order to facilitate understanding of the approach described in this paper. The NLP component consists of a tagger, a semi-purser, a prepositional phrase attachment specialist, a conjunct identifier for coordinate conjunctions, and a restructuzer. The tagger is an u-gram based program that currently generates syntactic/semantic tags for the words in the corpus. The syntactic portion of the tag is mandatory and the semantic portion depends upon whether the word has any special domain-specific classification or not. Currently only nouns, gerunds, and adjectives are assigned semantic tags. For example, in the domain of veterinary medicine, adog"""" would be assigned the tag """"nounmpatient, m """"nasal"""" would be """"adj--body-part, m etc. The parsing strategy is based on the initial identification of simple phrases, which are later converted to deeper structures with the help of separate specialist programs for coordinate conjunct identification and prepositional phrase attachment. The result for a given sentence is a single parse, many of whose elements are comparatively underspecified. For example, the parses generated lack clause boundaries. Nevertheless, the results are surprisingly useful in the extraction of relationships from the corpus. The semi-parser recognises noun-, verb-, prepositional-, gerund-, infinitive-, and s~iectival-phrases. The prepositional phrase attachment specialist [2] uses case grammar analysis to disambiguate the attachments of prepositional phrases and is highly domain-dependent. The current iraplemcntation of this subcomponent is highly specific to the domnin of veterinary medicine, the initial testbed for the KUDZU system. Note that all the examples presented in this paper will be taken from this domain. The coordinate conjunction specialist identifies pairs of appropriate conjuncts for the coordinate conjunctions in the text and is domainindependent in nature [1]. The restructurer puts together the information acquired by the specialist programs in order to provide a better (and deeper) structure to the parse. """"This research is supported by the NSP-ARPA grant number IRI 9314963. Before being passed to the knowledge analysis portion of the system, some parses undergo manual modification, which is"""	attachments;brill tagger;feature extraction;identifier;information extraction;natural language processing;parsing;semiconductor industry;software portability;testbed;text corpus	Rajeev Agarwal	1994		10.3115/1075812.1075899	natural language processing;semantic computing;explicit semantic analysis;information processing;feature extraction;computer science;pattern recognition;semantic compression;semantics;linguistics;natural language;information extraction;information retrieval	NLP	-34.27184223467751	-76.99876782459665	8865
1e486715fdd84a3e4fa37c6e5d5b4d5abe5232cc	correcting chinese spelling errors with word lattice decoding	chinese spelling error checking;unknown word detection;word lattice;noisy channel model;word segmentation;computer assisted language learning	Chinese spell checkers are more difficult to develop because of two language features: 1) there are no word boundaries, and a character may function as a word or a word morpheme; and 2) the Chinese character set contains more than ten thousand characters. The former makes it difficult for a spell checker to detect spelling errors, and the latter makes it difficult for a spell checker to construct error models. We develop a word lattice decoding model for a Chinese spell checker that addresses these difficulties. The model performs word segmentation and error correction simultaneously, thereby solving the word boundary problem. The model corrects nonword errors as well as real-word errors. In order to better estimate the error distribution of large character sets for error models, we also propose a methodology to extract spelling error samples automatically from the Google web 1T corpus. Due to the large quantity of data in the Google web 1T corpus, many spelling error samples can be extracted, better reflecting spelling error distributions in the real world. Finally, in order to improve the spell checker for real applications, we produce n-best suggestions for spelling error corrections. We test our proposed approach with the Bakeoff 2013 CSC Datasets; the results show that the proposed methods with the error model significantly outperform the performance of Chinese spell checkers that do not use error models.	character encoding;error detection and correction;google search;spell checker;text segmentation;use error	Yu-Ming Hsieh;Ming-Hong Bai;Shu-Ling Huang;Keh-Jiann Chen	2015	ACM Trans. Asian & Low-Resource Lang. Inf. Process.	10.1145/2791389	natural language processing;speech recognition;word error rate;computer science;noisy channel model;communication	NLP	-23.71197524288685	-79.75649930829225	8890
87d5503f2d3b9b5f6292d259754732cd7405a93b	choosing better seeds for entity set expansion by leveraging wikipedia semantic knowledge		Entity Set Expansion, which refers to expanding a human-input seed set to a more complete set which belongs to the same semantic category, is an important task for open information extraction. Because human-input seeds may be ambiguous, sparse etc., the quality of seeds has a great influence on expansion performance, which has been proved by many previous researches. To improve seeds quality, this paper proposes a novel method which can choose better seeds from original input ones. In our method, we leverage Wikipedia semantic knowledge to measure semantic relatedness and ambiguity of each seed. Moreover, to avoid the sparseness of the seed, we use web corpus to measure its population. Lastly, we use a linear model to combine these factors to determine the final selection. Experimental results show that new seed sets chosen by our method can improve expansion performance by up to average 13.4% over random selected seed sets.	information extraction;information science;linear model;neural coding;random seed;semantic similarity;sparse matrix;wikipedia	Zhenyu Qi;Kang Liu;Jian Zhao	2012		10.1007/978-3-642-33506-8_80	computer science;machine learning;pattern recognition;data mining	NLP	-26.116520215611676	-66.04032679283083	8912
25bdf3a5227aff4d7d8398d6c7e9436286fc151e	automatically discovering the number of clusters in web page datasets	hierarchical clustering;web pages;bepress selected works;information retrieval;classification;general methods;clustering;web mining clustering classification information retrieval knowledge discovery;cluster system;number of clusters;web mining;knowledge discovery	Clustering is well suited for Web mining by automatically organizing Web pages into categories each of which contains Web pages having similar contents. However, one problem in clustering is the lack of general methods to automatically determine the number of categories or clusters. For the Web domain in particular, currently there is no such method suitable for Web page clustering. In an attempt to address this problem, we discover a constant factor that characterizes the Web domain, based on which we propose a new method for automatically determining the number of clusters in Web page datasets. We discover that the measure of average inter-cluster similarity reaches a constant of 1.7 when all our experiments produced the best results for clustering Web pages. We determines the number of clusters by using the constant as the stopping factor in our clustering process by arranging individual Web pages into clusters and then arranging the clusters into larger clusters and so on until the average inter-cluster similarity approaches the constant. Having the new method described in this paper together with our new Bidirectional Hierarchical Clustering algorithm reported elsewhere, we have developed a clustering system suitable for mining the Web.	algorithm;cluster analysis;experiment;hierarchical clustering;organizing (structure);web mining;web page;world wide web	Zhongmei Yao;Ben Choi	2005			correlation clustering;web mining;data stream clustering;fuzzy clustering;biological classification;computer science;machine learning;consensus clustering;cure data clustering algorithm;web page;data mining;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;world wide web;information retrieval;affinity propagation	Web+IR	-27.87446176174464	-56.898647409932934	8923
c56442f4d74b415010da69c66b2315d3bc8ac43a	an improved hierarchical word sequence language model using word association	word association;modified kneser ney mkn;language modeling;hierarchical word sequence hws;generalized language model glm	Language modeling is a fundamental research problem that has applications for many NLP tasks. For estimating probabilities, most research on language modeling uses n-gram approach to factor sentence probabilities. However, the assumption of n-gram is too simple to cope with the data sparseness problem, which affects the final performance of language models. At the point, Hierarchical Word Sequence abbreviated as HWS language model, which uses word frequency information to convert raw sentences into special n-gram sequences, can be viewed as an effective alternative to normal n-gram method.#R##N##R##N#In this paper, we improve upon the basic HWS approach by generalizing it to exploit not only word frequencies but word association.#R##N##R##N#For evaluation, we compare word association based HWS models to normal HWS models and normal n-gram models. Both intrinsic and extrinsic experiments verify that word association based HWS models can achieve better performance.	language model	Xiaoyi Wu;Yuji Matsumoto;Kevin Duh;Hiroyuki Shindo	2015		10.1007/978-3-319-25789-1_26	natural language processing;speech recognition;computer science;linguistics	NLP	-22.7815159422249	-78.43692416419226	8932
0f005d0c68507f67f2501c314ba0ccd8da291b14	keyword extraction using backpropagation neural networks and rule extraction	document handling;document analysis;neural nets;information retrieval;digital libraries;knowledge management;rule extraction;backpropagation;statistical analysis;document analysis backpropagation rule extraction keyword extraction;feature extraction;keyword extraction;decision trees keyword extraction backpropagation networks knowledge management system information retrieval system digital libraries web browsing document processing methods process automation statistics based methods word related features neural network backpropagation network tf idf rule extraction method symbolic data extraction;statistical analysis backpropagation digital libraries document handling feature extraction information retrieval knowledge management neural nets;feature extraction neural networks data mining training backpropagation accuracy abstracts	Keyword extraction is vital for Knowledge Management System, Information Retrieval System, and Digital Libraries as well as for general browsing of the web. Keywords are often the basis of document processing methods such as clustering and retrieval since processing all the words in the document can be slow. Common models for automating the process of keyword extraction are usually done by using several statistics-based methods such as Bayesian, K-Nearest Neighbor, and Expectation-Maximization. These models are limited by word-related features that can be used since adding more features will make the models more complex and difficult to comprehend. In this research, a Neural Network, specifically a backpropagation network, will be used in generalizing the relationship of the title and the content of articles in the archive by following word features other than TF-IDF, such as position of word in the sentence, paragraph, or in the entire document, and formats such as heading, and other attributes defined beforehand. In order to explain how the backpropagation network works, a rule extraction method will be used to extract symbolic data from the resulting backpropagation network. The rules extracted can then be transformed into decision trees performing almost as accurate as the network plus the benefit of being in an easily comprehensible format.	archive;artificial neural network;backpropagation;cluster analysis;course (navigation);decision tree;digital library;document processing;expectation–maximization algorithm;k-nearest neighbors algorithm;keyword extraction;knowledge management;library (computing);management system;rule induction;tf–idf	Arnulfo P. Azcarraga;Michael David Liu;Rudy Setiono	2012	The 2012 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2012.6252618	digital library;feature extraction;computer science;backpropagation;machine learning;pattern recognition;data mining;information retrieval;artificial neural network	AI	-25.25200537091176	-64.21110716374658	8967
412aa234f900086d482dec2498d89229663c3241	incorporating linguistics constraints into inductive logic programming	candidate grammar rule;linguistic property;linguistics constraint;linguistically plausible grammar rule;incomplete grammar;linguistic knowledge;grammar induction;linguistic constraint;inductive logic programming;grammar rule;general rule;general generalisation;bottom up;search space	We report work on effectively incorporating linguistic knowledge into grammar induction. We use a highly interactive bot tom-up inductive logic programming (ILP) algorithm to learn 'missing' grammar rules from an :incomplete grammar. Using linguistic constraints on, for example, head features and gap threading, reduces the search space to such an extent that, in the small-scale experiments reported here, we can generate and store all candidate grammar rules together with information about their coverage and linguistic properties. This allows an appealingly simple and controlled method for generating linguistically plausible grammar rules. Starting from a base of highly specific rules, we apply least general generalisation and inverse resolution to generate more general rules. Induced rules are ordered, for example by coverage, for easy inspection by the user and at any point, the user can commit to a hypothesised rule and add it to the grammar. Related work in ILP and computational linguistics is discussed.	algorithm;computational linguistics;experiment;grammar induction;inductive logic programming;inductive reasoning;thread (computing);tom	James Cussens;Stephen G. Pulman	2000			natural language processing;id/lp grammar;synchronous context-free grammar;link grammar;operator-precedence grammar;regular grammar;computer science;affix grammar;machine learning;stochastic grammar;phrase structure rules;emergent grammar;relational grammar;attribute grammar;unrestricted grammar;adaptive grammar;grammar-based code;mildly context-sensitive grammar formalism;algorithm;lexical grammar	NLP	-27.743126511775156	-79.86664524132786	8974
c6cccbb552459bdd0dfa5cb334dfde9a74d0ec34	vector model based indexing and retrieval of handwritten medical forms	erroneous output;ir performance;mean average pre;interpolated 11-point pre;information retrieval;ordinary hr text;handwritten medical forms;recognition proba;vector model;tor model;ir tests show;image recognition;image segmentation;word segmentation;probability;handwriting recognition;mean average precision	A vector model based information retrieval of handwritten medical forms is presented in this paper. In order to improve the IR performance on the erroneous output of handwriting recognition (HR) systems, a variation of the vector model is made to estimate the number of occurrences of terms from word segmentation and recognition probabilities. IR Tests show that our approach outperforms the retrieval of ordinary HR text in terms of mean average precision (MAP), R-Precision, and interpolated 11-point precisions.	algorithm;handwriting recognition;information retrieval;interpolation;text segmentation;tf–idf	Huaigu Cao;Venu Govindaraju	2007	Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)	10.1109/ICDAR.2007.262	text segmentation;computer vision;speech recognition;computer science;pattern recognition;probability;handwriting recognition;image segmentation;vector space model	Vision	-22.559361491362843	-82.58656027310055	8996
54ba65b7737bed08d5718ec49eb6eb41f397d936	cross-document event ordering through temporal, lexical and distributional knowledge	cross document event coreference;temporal information processing;distributional semantics;info eu repo semantics article;timelines;cross document temporal relation;event ordering	In this paper we present a system that automatically builds ordered timelines of events from different written texts in English. The system deals with problems such as automatic event extraction, cross-document temporal relation extraction and cross-document event coreference resolution. Its main characteristic is the application of three different types of knowledge: temporal knowledge, lexical-semantic knowledge and distributional-semantic knowledge, in order to anchor and order the events in the timeline. It has been evaluated within the framework of SemEval 2015. The proposed system improves the current state-of-the-art systems in all measures (up to eight points of F1-score over other systems) and shows a significant advance in the Cross-document event ordering task.		Borja Navarro-Colorado;Estela Saquete Boró	2016	Knowl.-Based Syst.	10.1016/j.knosys.2016.07.032	natural language processing;timeline;computer science;data mining;information retrieval	NLP	-26.832565555590275	-66.17951818181389	8999
5694d58f784f82137d44c9638d7bf0c3d7be09db	a constraint approach to lexicon induction for low-resource languages		Bilingual lexicon is a useful language resource, but such data rarely available for lower-density language pairs, especially for those that are closely related. The lack or absence of parallel and comparable corpora makes bilingual lexicon extraction becomes a difficult task. Using a third language to link two other languages is a well-known solution in low-resource situation, which usually requires only two input bilingual lexicons to automatically induce the new one. This approach, however, is weak in measuring semantic distance between bilingual word pairs because it has never been demonstrated to utilize the complete structures of the input bilingual lexicons as dropped meanings negatively influence the result. This research discuss a constraint approach to pivot-based lexicon induction in case the target language pair are closely related. We create constraints from language similarity and model the structures of the input dictionaries as an optimization problem whose solution produces optimally correct target bilingual lexicon. In addition, we enable created bilingual lexicons of low-resource languages accessible through service grid federation.	lexicon	Wushouer Mairidan;Donghui Lin;Toru Ishida;Yohei Murakami	2018		10.1007/978-981-10-7793-7_7	grid;natural language processing;semantic similarity;pivot language;constraint satisfaction problem;bilingual lexicon;lexicon;optimization problem;artificial intelligence;computer science	NLP	-28.80195136046308	-71.75493003697227	9014
617ed09bc09d73cbd4e7a78213d04c6da567249f	introducing a nationally shared electronic patient record: case study comparison of scotland, england, wales and northern ireland	electronic patient record;change management;medicina estatal;escocia;pais de gales;irlanda do norte;humanos;comportamento cooperativo;human factors;health information exchange;registros eletronicos de saude;inglaterra	AIM To compare the experience of the four UK countries in introducing nationally accessible electronic summaries of patients' key medical details, intended for use in emergency and unscheduled care episodes, and generate transferable lessons for other countries.   METHOD Secondary analysis of data collected previously on all four schemes; cross-case comparison using a framework derived from diffusion of innovations theory.   MAIN FINDINGS Whilst all four programmes shared a similar vision, they differed widely in their strategy, budget, implementation plan, approach to clinical and public engagement and approach to evaluation and learning. They also differed, for various reasons, in stakeholder alignments, the nature and extent of resistance to the programme and the rate at which records were created. A nationally shared, widely accessible electronic record has powerful symbolic meaning; it may or may not be perceived as improving the quality and safety of care or (alternatively) as threatening patient confidentiality or the traditional role of the doctor or nurse. 'Hard' project management oriented to achieving specific milestones and deadlines sometimes appeared counterproductive when it cut across the 'softer' aspects of the programmes.   CONCLUSION When designing and implementing complex technologies with pervasive implications, policymakers must consider not only technical issues but also the personal, social and organisational aspects of the programme. A judicious blend of 'hard' and 'soft' management appears key to managing such programmes.	aim alliance;confidentiality;episode of care;medical records;neoplasm metastasis;patients;pervasive informatics;software project management;while	Trisha Greenhalgh;Libby Morris;Jeremy C. Wyatt;Gwyn Thomas;Katey Gunning	2013	International journal of medical informatics	10.1016/j.ijmedinf.2013.01.002	medicine;telecommunications;nursing;change management;management;law	HCI	-59.327272861991815	-62.2065408809036	9027
1d79ea41bc69bfa9545b3251eee1f032ee4e2275	language model estimations and representations for real-time continuous speech recognition	search space;real time;continuous speech recognition;speaker independent;linear interpolation;language model	This paper compares different ways of estimating bigram language models and of representing them in a finite state network used by a beam-search based, continuous speech, and speaker independent HMM recognizer. Attention is focused on the n-gram interpolation scheme for which seven models are considered. Among them, the Stacked estimated linear interpolated model favourably compares with the best known ones. Further, two different static representations of the search space are investigated: “linear” and “tree-based”. Results show that the latter topology is better suited to the beam-search algorithm. Moreover, this representation can be reduced by a network optimization technique, which allows the dynamic size of the recognition process to be decreased by 60%. Extensive recognition experiments on a 10,000-word dictation task with four speakers are described in which an average word accuracy of 93% is achieved with real-time response.	beam search;bigram;experiment;finite-state machine;flow network;hidden markov model;language model;linear interpolation;mathematical optimization;n-gram;online and offline;perplexity;real-time clock;real-time locating system;search algorithm;speech recognition;text corpus	Giuliano Antoniol;Fabio Brugnara;Mauro Cettolo;Marcello Federico	1994			natural language processing;cache language model;speaker recognition;speaker diarisation;speech recognition;computer science;pattern recognition;acoustic model;linear interpolation;language model	NLP	-20.960224226637454	-87.98323060728114	9030
bddf217b68ad3f3324b2ece5dc35858814310f15	speechdat-car. a large speech database for automotive environments		The aims of the SpeechDat-Car project are to develop a set of speech databases to support training and testing of multilingual speech recognition applications in the car environment. As a result, a total of ten (10) equivalent and similar resources will be created. The 10 languages are Danish, each language 600 sessions will be recorded (from at least 300 speakers) in seven characteristic environments (low speed, high speed with audio equipment on, etc.). This paper gives an overview of the project with a focus on the production phases (recording platforms, speaker recruitment, annotation and distribution). Automatic speech recognition (ASR) appears to be a particularly well adapted technology for providing voice-based interfaces (based on hands-free mode) that will enable new in-car applications to develop while taking care of safety aspects. However, the car environment is known to be particularly noisy (street noise, car engine noise, vibration noises, bubble noise, etc...). To obtain an optimal performance for speech recognition, it is necessary to train the system on large corpora of speech data recorded in context (i.e. directly in the car). For this reason, language-specific initiatives for database collections have been developed since about 1990 [Langmann (1998)]. The European project SpeechDat-Car 1 aims at providing a set of uniform, coherent databases for nine European languages and for American English. in developing large-scale speech resources for a wide range of languages and for in-car applications participation of external partners to the original consortium is also possible. Siemens is an 'external' partner. It is also important to note that SpeechDat-Car commits itself to a strict validation protocol to ensure 1 SpeechDat-Car started in April 1998 in the 4th EC framework under project code LE4-8334 with a 30 months' project duration. optimal quality and exchangeability of the databases [Van den Heuvel (1999)]. This paper gives an overview of the project with a focus on production phases. It is organised as follows: the next section describes the database specifications (database content, recording platforms and validation procedures). Then, Section 3 provides additional information on speaker recruitment and an extensive description of the annotation procedure and tools is given in section 4. The paper then concludes with a short section about database availability and dissemination. Each database produced in the SpeechDat-Car project is intended to provide enough data to adapt speaker independent recognition systems to the automotive environment. Database contents were designed to cope with different applications. The design …	audio equipment;care-of address;coherence (physics);database;hoare logic;speech recognition;text corpus	Asunción Moreno;Børge Lindberg;Christoph Draxler;Gaël Richard;Khalid Choukri;Stephan Euler;Jeffrey Allen	2000				DB	-24.20411266437145	-84.45165286294338	9037
15a4c52156290d53363a2b5de1014554acc74a11	domain-initial strengthening on french vowels and phonological contrasts: evidence from lip articulation and spectral variation	french;initial strengthening;vowel contrast;labial articulation	Domain-initial strengthening has primarily been studied for consonants. This paper examines whether vowels also undergo boundary-induced phonetic variation and questions how this effect interacts with phonological contrast in a dense vowel system such as that of French. The labial articulation and the acoustic properties of the 10 French oral vowels /i, e, ɛ, a, y, ø, œ, u, o, ɔ/ are examined in Intonational Phrase-initial vs. Word-initial position. The vowels' phonetic properties are found to be affected by position but not in a uniform way. First, while all vowels are found to have a larger lip opening and width in IP-initial position, the effect is larger and more robust for unrounded vowels than rounded vowels leading to an enhanced distinction between vowels contrasting in rounding. No effect is found on lip protrusion. The distinction between these vowels is also found to be increased in IP-initial position by the enhancement of the spectral characteristics making unrounded vowels more ‘unrounded-like’ and – to a lesser degree – by the enhancement of the properties making rounded vowels more ‘rounded-like’. The contrast between front and back vowels is also maximized by a tendency toward a higher F2 for front vowels and a lower F2 and F2–F1 for back vowels. Open and mid-open vowels also tend toward a higher F1. These results suggest that initial strengthening indeed contributes to maximizing phonetic contrasts between vowels in IP-initial position. & 2014 Elsevier Ltd. All rights reserved.	acoustic cryptanalysis;biconnected component;mike lesser;rounding	Laurianne Georgeton;Cécile Fougeron	2014	J. Phonetics	10.1016/j.wocn.2014.02.006	speech recognition;acoustics;philosophy;french;relative articulation;linguistics;sociology;communication;nasal vowel	HCI	-10.780104267211941	-81.48116862540134	9041
4a91bd3af87bf073c855027875e306c810107d4b	empirically assessing effects of the right frontier constraint	relative position;discourse structure	In a questionnaire study the effects of discourse structural information on resolving inter-sentential anaphora were investigated. The Right Frontier Constraint, first proposed by Polanyi (1988), states that potential antecedents of an anaphor that are placed at the right frontier of a discourse unit can be accessed more easily than antecedents that are placed somewhere else. Participants (N=36) received written experimental passages of six lines each that contained a pronominal anaphor in the last line and two potential antecedents in the preceding text, one introduced in the first, one in the fourth line of a passage. Antecedents' relative position to the right frontier was manipulated through the discourse relation between the first and the second antecedent and through the filler information interposed between the second antecedent and the anaphor. The two potential antecedents either had the same or different grammatical gender. In the latter case only the first antecedent was gender congruent to the anaphor. Participants' task was to name the anaphor's antecedent. Results show that in case of unequal gender antecedents, participants almost always chose the gender congruent first antecedent, irrespective of its position relative to the Right Frontier. In case of equal gender antecedents choice patterns point to an influence of an antecedent's position relative to the Right Frontier. Alternative theoretical approaches such as centering theory or situational models cannot account for the found results. The findings in the same gender antecedent condition are therefore interpreted as an effect of the Right Frontier Constraint.	anaphora (linguistics);discourse relation;star filler	Anke Holler;Lisa Irmen	2007		10.1007/978-3-540-71412-5_2	computer science;artificial intelligence;social psychology	NLP	-37.861932287897005	-83.34291476987616	9052
24ba91d8052661fb04c5566df4e5cabdade15e14	computational analysis of medieval manuscripts: a new tool for analysis and mapping of medieval documents to modern orthography	mpeg spelling variations;computational analysis of medieval manuscripts camm;phonetic algorithms;article	Medieval manuscripts or other written documents from that period contain valuable information about people, religion, and politics of the medieval period, making the study of medieval documents a necessary pre-requisite to gaining in-depth knowledge of medieval history. Although tool-less study of such documents is possible and has been ongoing for centuries, much subtle information remains locked such manuscripts unless it gets revealed by effective means of computational analysis. Automatic analysis of medieval manuscripts is a non-trivial task mainly due to non-conforming styles, spelling peculiarities, or lack of relational structures (hyper-links), which could be used to answer meaningful queries. Natural Language Processing (NLP) tools and algorithms are used to carry out computational analysis of text data. However due to high percentage of spelling variations in medieval manuscripts, NLP tools and algorithms cannot be applied directly for computational analysis. If the spelling variations are mapped to standard dictionary words, then application of standard NLP tools and algorithms becomes possible. In this paper we describe a web-based software tool CAMM (Computational Analysis of Medieval Manuscripts) that maps medieval spelling variations to a modern German dictionary. Here we describe the steps taken to acquire, reformat, and analyze data, produce putative mappings as well as the steps taken to evaluate the findings. At the time of the writing of this paper, CAMM provides access to 11275 manuscripts organized into 54 collections containing a total of 242446 distinctly spelled words. CAMM accurately corrects spelling of 55% percent of the verifiable words. CAMM is freely available at http://researchworks.cs.athabascau.ca/	algorithm;computation;dictionary;document;map;natural language processing;programming tool;text corpus;web application	Mushtaq Ahmad;Stefan Gruner;Muhammad Tanvir Afzal	2012	J. UCS	10.3217/jucs-018-20-2750	natural language processing;computer science;artificial intelligence;world wide web;algorithm	NLP	-31.32295844748726	-74.56633798980953	9089
a490a5380c49af5bf7158c03750852ca9f2dff25	how to build a system that manages bioinformatics content	front end	We discuss how to build systems with sophisticated features based on a core set of bioinformatics tool s. Beside from being the front-end server to the core analyses, such a system can manage users, bioinform atics content and shared data. Using tools available in the open-source community, we show how to build a syste m with such sophisticated features with a relatively minimal effort, especially in comparison to building such a system from scratch. We discuss the notion of Model-Conte nt Management-View, under whose framework similar applications with such desirable features beyond th e field Bioinformatics can be built. Our system can be vi ewed at http://cetus.cs.memphis.edu/rnamotif	bioinformatics;content management system;open-source software;server (computing)	Vinhthuy T. Phan;Allen Thomas	2007			operating system;world wide web;database;front and back ends;bicyclic molecule;computer science	OS	-5.931759919609397	-59.39854588330521	9094
97a13d7c8508190f28a100cdaed1729b3f041c9b	dynamic cataloguing of the old arabic manuscripts by automatic extraction of metadata		PurposernrnrnrnrnThe purpose of this paper is to obtain online access to the digitised Arabic manuscripts images, which need to use a catalogue. The bibliographic cataloguing is unsuitable for old Arabic manuscripts, and it is imperative to establish a new cataloguing model. In the research, the authors propose a new cataloguing model based on manuscript annotations and transcriptions. This model can be an effective solution to dynamic catalogue old Arabic manuscripts. In this field, the authors used the automatic extraction of the metadata that is based on the structural similarity of the documents.rnrnrnrnrnDesign/methodology/approachrnrnrnrnrnThis work is based on experimental methodology. The whole proposed concepts and formulas were tested for validation. This, allows the authors to make concise conclusions.rnrnrnrnrnFindingsrnrnrnrnrnCataloguing old Arabic manuscripts faces problem of unavailability of information. However, this information may be found in another place in a copy of the original manuscript. Thus, cataloguing Arabic manuscript cannot be done in one time, it is a continual process which require information updating. The idea is to make a pre-cataloguing of a manuscript, then try to complete and improve it through a specific platform. Consequently, in the research work, the authors propose a new cataloguing model, which the authors call “Dynamic cataloguing”.rnrnrnrnrnResearch limitations/implicationsrnrnrnrnrnThe success of the proposed model is confronted with the involvement of all actors of the model. It is based on the conviction and the motivation of actors of the collaborative platform.rnrnrnrnrnPractical implicationsrnrnrnrnrnThe model can be used in several cataloguing fields, where the encoding model is based on XML. The model is innovative and implements a smart cataloguing model. The model is useful by using a web platform. It allows an automatic update of a catalogue.rnrnrnrnrnSocial implicationsrnrnrnrnrnThe model prompts the user to participate and enrich the catalogue. The user could improve his social status from a passive to an active.rnrnrnrnrnOriginality/valuernrnrnrnrnThe dynamic cataloguing model is a new concept. It has never been proposed in the literature until now. The proposed cataloguing model is based on automatic extraction of metadata from user annotations/transcription. It is a smart system which automatically updates or fills the catalogue with the extracted metadata.	imperative programming;spatial variability;structural similarity	Mohammed Ourabah Soualah;Yassine Ait Ali Yahia;Abdelkader Keita;Abderrezak Guessoum	2017	Library Hi Tech	10.1108/LHT-07-2016-0076	world wide web;xml;arabic;data mining;digital library;information retrieval;structural similarity;metadata;transcription (linguistics);computer science;originality;smart system	NLP	-37.5441656590949	-68.27417517656545	9120
39c26afd9bf91c8b4456111ae486a381d70e5868	putting language into language modeling	language model	In this paper we describe the statistical Structured Language Model (SLM) that uses grammatical analysis of the hypothesized sentence segment (prefix) to predict the next word. We first describe the operation of a basic, completely lexicalized SLM that builds up partial parses as it proceeds left to right. We then develop a chart parsing algorithm and with its help a method to compute the prediction probabilities P (wi+1jWi): We suggest useful computational shortcuts followed by a method of training SLM parameters from text data. Finally, we introduce more detailed parametrization that involves non-terminal labeling and considerably improves smoothing of SLM statistical parameters. We conclude by presenting certain recognition and perplexity results achieved on standard corpora.	algorithm;computation;itil;language model;parsing;smoothing;technical standard;text corpus	Frederick Jelinek;Ciprian Chelba	1999			perplexity;first-generation programming language;artificial intelligence;universal networking language;speech recognition;object language;language model;comprehension approach;parsing;smoothing;pattern recognition;computer science	NLP	-21.953060382756462	-78.91353609701531	9126
3ca95a855ae9abf8b1c40f48a6640d1770b8e5a0	negation detection in swedish clinical text	detects negation;swedish health record;trigger phrase;trigger phrase approach;english clinical text;english version;swedish clinical text;clinical text;negation detection;swedish context;swedish adaptation;english trigger phrase	NegEx, a rule-based algorithm that detects negations in English clinical text, was translated into Swedish and evaluated on clinical text written in Swedish. The NegEx algorithm detects negations through the use of trigger phrases, which indicate that a preceding or following concept is negated. A list of English trigger phrases was translated into Swedish, taking grammatical differences between the two languages into account. This translation was evaluated on a set of 436 manually classified sentences from Swedish health records. The results showed a precision of 70% and a recall of 81% for sentences containing the trigger phrases and a negative predictive value of 96% for sentences not containing any trigger phrases. The precision was significantly lower for the Swedish adaptation than published results on the English version, but since many negated propositions were identified through a limited set of trigger phrases, it could nevertheless be concluded that the same trigger phrase approach is possible in a Swedish context, even though it needs to be further developed.	algorithm;logic programming	Maria Skeppstedt	2010			natural language processing;speech recognition;computer science;linguistics	NLP	-26.464548874242798	-75.2177710222158	9134
1afa673e6fe519f0feb3e2e2693178b4169f0e0c	an automatic approach to feature extraction		The pervasive diffusion of social networks as common way to communicate and share information is becoming a valuable resource for analysts and decision makers. Reviews are used every day by common people or by companies who need to make decisions. It is evident that even the opinion monitoring is essential for listening to and taking advantage of the conversations of possible customers in a decision making process. Opinion Mining is a way to analyse opinions related to specific topics: products, services, tourist locations, etc. In this paper we propose an automatic approach to the extraction of feature terms, applying our experience in the semantic analysis of textual resources to Opinion Mining task and performing a contextualisation by means of semantic categorisation, and by a set of qualities associated to the sense expressed by adjectives and adverbs.	categorization;feature extraction;pervasive informatics;social network	Manuela Angioni;Franco Tuveri	2012			feature extraction	Web+IR	-22.8107306952749	-56.07924105470887	9148
3e83fbeb089c58fc4d74d4d51914d08c45b83b7d	do relationality and aptness influence conventionalization?		The conventionalization of figurative comparisons is one source of lexical evolution. For example, anchor once only meant a device for mooring a ship, but may now be used to describe any source of stability or confidence. Our goal is to understand this process. Following the Career of Metaphor framework, figurative mappings are interpreted through a structure-mapping process, rendering common structure salient. As figurative terms become conventionalized, (1) the figurative sense becomes associated with the base term; and (2) there is shift from simile form to metaphor form. In two studies we investigated psycholinguistic properties that may influence this process: relationality and aptness. We use relative preference for the metaphor form as an estimate of degree of conventionalization; by determining the preferred form for a set of figuratives, we find evidence that both aptness and relationality influence this process. We speculate that figurative comparisons may give rise to new relational terms.	figurative system of human knowledge;simile	Francisco Maravilla;Dedre Gentner	2017				AI	-8.573006446872267	-77.08698999438597	9150
26fb8e6fd498ce4fe7e37b329e5f17f96eeb4188	experiments of 991-word speaker independent continuous speech recognition on darpa rm task			speech recognition	J. M. Song;T. Thomas;M. Patel	1991			speech recognition;pattern recognition;speaker recognition;speaker diarisation;natural language processing;computer science;artificial intelligence	ML	-15.297999305072047	-86.7328631299887	9155
181ec35e1da386cd5e38ef54c20af63e41def3b0	search engine guided non-parametric neural machine translation.		In this paper, we extend an attention-based neural machine translation (NMT) model by allowing it to access an entire training set of parallel sentence pairs even after training. The proposed approach consists of two stages. In the first stage– retrieval stage–, an off-the-shelf, black-box search engine is used to retrieve a small subset of sentence pairs from a training set given a source sentence. These pairs are further filtered based on a fuzzy matching score based on edit distance. In the second stage–translation stage–, a novel translation model, called search engine guided NMT (SEG-NMT), seamlessly uses both the source sentence and a set of retrieved sentence pairs to perform the translation. Empirical evaluation on three language pairs (En-Fr, En-De, and En-Es) shows that the proposed approach significantly outperforms the baseline approach and the improvement is more significant when more relevant sentence pairs were retrieved.	baseline (configuration management);black box;edit distance;linear stage;neural machine translation;test set;web search engine	Jiatao Gu;Yong Wang;Kyunghyun Cho;Victor O. K. Li	2017	CoRR		machine learning;natural language processing;artificial intelligence;machine translation;edit distance;approximate string matching;computer science;search engine;training set;nonparametric statistics;translation memory;sentence	AI	-22.497697425193856	-73.05924393428695	9173
f341bb79e4270c718019482e08c840603f676be7	an intelligent language tutoring system	grammar;lenguaje natural;assistance intelligente;representacion conocimientos;computer assisted teaching;aleman;prolog ii;relacion hombre maquina;langage naturel;base connaissance;man machine relation;ensenanza asistida por computador;intelligence artificielle;intelligent language tutoring system;systeme conversationnel;asistencia inteligente;interactive system;grammaire;natural language;sistema conversacional;allemand;artificial intelligence;base conocimiento;relation homme machine;inteligencia artificial;knowledge representation;intelligence assistance;german;representation connaissances;gramatica;enseignement assiste ordinateur;knowledge base	In this paper, we present the theoretical background and describe the design and implementation of an intelligent language tutoring system (ILTS). The most important properties of our system are: (1) The system is based on a very complete and “objective” grammar knowledge base; (2) Students can at any moment during an exercise ask the system questions about the grammar, and they are immediately answered without losing the exercise context. Thus the normal behaviour of a tutor is better simulated, which contributes to a user-friendly interface; and (3) It allows for individual correction of errors and reaction to errors. This is due to the fact that the system is firmly based on a linguistically well-founded analysis. The sentences formulated by the students are  parsed  and analysed. They are not simply matched against predefined answers as is still the case with many other more classically oriented systems.		Camilla Schwind	1990	International Journal of Man-Machine Studies	10.1016/S0020-7373(05)80053-7	natural language processing;knowledge base;german;computer science;artificial intelligence;grammar;linguistics;natural language	Arch	-36.33610277667445	-78.91761665531163	9175
349f1ce57a7ae325a221df1c4e33a585578247bf	automatic diacritic restoration for resource-scarce languages	language resources;machine learning;languages and literatures;article	The orthography of many resource-scarce languages includes diacritically marked characters. Falling outside the scope of the standard Latin encoding, these characters are often represented in digital language resources as their unmarked equivalents. This renders corpus compilation more difficult, as these languages typically do not have the benefit of large electronic dictionaries to perform diacritic restoration. This paper describes experiments with a machine learning approach that is able to automatically restore diacritics on the basis of local graphemic context. We apply the method to the African languages of Cilubà, Gı̃kũyũ, Kı̃kamba, Maa, Sesotho sa Leboa, Tshivenda and Yoruba and contrast it with experiments on Czech, Dutch, French, German and Romanian, as well as Vietnamese and Chinese Pinyin.	circuit restoration;compiler;dictionary;experiment;machine learning;rendering (computer graphics)	Guy De Pauw;Peter Waiganjo Wagacha;Gilles-Maurice de Schryver	2007		10.1007/978-3-540-74628-7_24	natural language processing;speech recognition;computer science;artificial intelligence;machine learning;linguistics	NLP	-28.38994493853695	-81.30983019960888	9188
5e1cbd054501860daa74d1a34035e2b6eeb94b1d	helmholtz principle based supervised and unsupervised feature selection methods for text mining	helmholtz principle;text mining;attribute selection;text classification;machine learning;feature selection;article	One of the important problems in text classification is the high dimensionality of the feature space. Feature selection methods are used to reduce the dimensionality of the feature space by selecting the most valuable features for classification. Apart from reducing the dimensionality, feature selection methods have potential to improve text classifiers’ performance both in terms of accuracy and time. Furthermore, it helps to build simpler and as a result more comprehensible models. In this study we propose new methods for feature selection from textual data, called Meaning Based Feature Selection (MBFS) which is based on the Helmholtz principle from the Gestalt theory of human perception which is used in image processing. The proposed approaches are extensively evaluated by their effect on the classification performance of two well-known classifiers on several datasets and compared with several feature selection algorithms commonly used in text mining. Our results demonstrate the value of the MBFS methods in terms of classification accuracy and execution time. © 2016 Published by Elsevier Ltd.	algorithm;document classification;feature selection;feature vector;gestalt psychology;image processing;run time (program lifecycle phase);supervised learning;text corpus;text mining	Melike Tutkan;Murat Can Ganiz;Selim Akyokus	2016	Inf. Process. Manage.	10.1016/j.ipm.2016.03.007	feature extraction;computer science;machine learning;linear classifier;pattern recognition;data mining;feature selection;feature;feature model;dimensionality reduction	ML	-20.0369241130012	-64.0491352643021	9205
4e7dd05b75add873d618bc2c7ea5003829e449b6	extracting relations with integrated information using kernel methods	entity relation detection;different level;processing error;ace relation detection task;kernel method;syntactic processing;predefined relation;useful information;integrated information;relation detection approach;extracting relation;different kernel;information extraction;dependence analysis	Entity relation detection is a form of information extraction that finds predefined relations between pairs of entities in text. This paper describes a relation detection approach that combines clues from different levels of syntactic processing using kernel methods. Information from three different levels of processing is considered: tokenization, sentence parsing and deep dependency analysis. Each source of information is represented by kernel functions. Then composite kernels are developed to integrate and extend individual kernels so that processing errors occurring at one level can be overcome by information from other levels. We present an evaluation of these methods on the 2004 ACE relation detection task, using Support Vector Machines, and show that each level of syntactic processing contributes useful information for this task. When evaluated on the official test data, our approach produced very competitive ACE value scores. We also compare the SVM with KNN on different kernels.	ace;dependence analysis;entity;information extraction;information source;k-nearest neighbors algorithm;kernel method;parsing;support vector machine;test data;tokenization (data security)	Shubin Zhao;Ralph Grishman	2005			natural language processing;kernel method;string kernel;computer science;machine learning;pattern recognition;data mining;tree kernel;dependence analysis	NLP	-24.18165336850682	-69.07009930420604	9208
057135ed0c8825cb28ee5ecab78198e4f787f672	a study on task-independent subword selection and modeling for speech recognition	modeling approach;degradation;spanish task independent subword selection speech recognition task independent training subword units application vocabulary training corpus recognition task context dependent units basis phone sets training criterion phone selection modeling approach american english context dependent phone models unseen tasks mandarin chinese;word processing speech recognition natural languages;training corpus;application vocabulary;vocabulary;context dependent phone models;natural languages;testing;speech recognition vocabulary hidden markov models context modeling testing training data multimedia communication natural languages degradation laboratories;training data;training criterion;mandarin chinese;a priori knowledge;hidden markov models;recognition task;context dependent units;basis phone sets;spanish;multimedia communication;subword units;speech recognition;phone selection;task independent training;context modeling;word processing;unseen tasks;american english;task independent subword selection	We study two key issues in task-independent training, namely selection of a universal set of subword units and modeling of the selected units. Since no a priori knowledge about the application vocabulary and syntax was used in the collection of the training corpus and the recognition task is frequently changing, the conventional strategy can no longer provide the best performance across many di erent tasks. We present a new approach that use the complete sets of right and left context-dependent units as the basis phone sets. Training of these models is accomplished by a new training criterion that maximizes phone separation between competing models. The proposed phone selection and modeling approach was evaluated across di erent tasks in American English. Good recognition results were obtained for both context-independent and context-dependent phone models even for unseen tasks. The same strategy has also been applied to two other languages, Mandarin Chinese and Spanish, with similar success.	context-sensitive language;linuxmce;speech recognition;substring;super robot monkey team hyperforce go!;triphone;vocabulary	Chin-Hui Lee;Biing-Hwang Juang;Wu Chou;J. J. Molina-Perez	1996		10.1109/ICSLP.1996.607984	natural language processing;speech recognition;computer science;communication	NLP	-20.21246296420084	-87.28001497081559	9209
fc1aecfd2b8deb443f344963a802806395359995	extraction of displayed objects corresponding to demonstrative words for use in remote transcription	or phrases;hearing impaired	A previously proposed system for extracting target objects displayed during lectures by using demonstrative words and phrases and pointing gestures has now been evaluated. The system identifies pointing gestures by analyzing the trajectory of the stick pointer and extracts the objects to which the speaker points. The extracted objects are displayed on the transcriber's monitor at a remote location, thereby helping the transcriber to translate the demonstrative word or phrase into a short description of the object. Testing using video of an actual lecture showed that the system had a recall rate of 85.7% and precision of 84.8%. Testing using two extracted scenes showed that transcribers replaced significantly more demonstrative words with short descriptions of the target objects when the extracted objects were displayed on the transcriber's screen. A transcriber using this system can thus transcribe speech more easily and produce more meaningful transcriptions for hearing-impaired listeners.	medical transcription	Yoshinori Takeuchi;Hajime Ohta;Noboru Ohnishi;Daisuke Wakatsuki;Hiroki Minagawa	2010		10.1007/978-3-642-14100-3_24	natural language processing;speech recognition;computer science;communication	HCI	-26.84851199736378	-83.4456109705257	9212
77d20c5660351cc3df73dd49ebc38080092d401a	embedding syntactic tree structures into cnn architecture for relation classification		Relation classification is an important task in natural language processing (NLP) fields. State-of-the-art methods are mainly based on deep neural networks. This paper proposes a new convolutional neural network (CNN) architecture which combines the syntactic tree structure and other lexical level features together for relation classification. In our method, each word in the input sentence is first represented as a k-size word sequence which contains the context information of the considering word. Then each of such word sequence is parsed into a syntactic tree structure and this kind of tree structure is further mapped into a real-valued vector. Finally, concatenated with the attention features for the words among the marked entities, all of these features are fed into a CNN model for relation decision. We evaluate our method on the SemEval 2010 relation classification task and experimental results show that our method outperforms previous state-of-the-art methods under the condition of without using external linguistic resources like WordNet.		Feiliang Ren;Rongsheng Zhao;Xiao Hu;Yongcheng Li;Di Zhou;Cunxiang Wang	2017		10.1007/978-981-10-7359-5_11	architecture;convolutional neural network;semeval;artificial neural network;parsing;syntax;tree structure;wordnet;computer science;pattern recognition;artificial intelligence	NLP	-18.532345878682385	-71.78640542820604	9222
a30dc16c821128a553d286c6d58e530b85adba38	a comparison of several speech parameters for speaker independent speech recognition and speaker recognition	speech recognition;speaker recognition		speaker recognition;speech recognition	John Sirigos;Nikos Fakotakis;George K. Kokkinakis	1995			speech recognition;artificial intelligence;pattern recognition;speaker diarisation;voice analysis;computer science;speaker recognition	ML	-14.306110052187526	-87.58759786367632	9223
32887505b348adcf8499a8c8f937ee2dae4f6b01	distributional semantics for medical information extraction		This report describes two methods implemented for the CLEF eHealth 2016 Task 1 challenge. They consist of: a) a feed forward neural network; and b) a random forest for classification and a feed forward neural net, applied to automatically fill in medical handover forms using synthetic medical records as inputs. Both approaches are interesting because they rely on word embeddings, are domain independent, and are feature engineering free. We discuss the complexity of the task, and the impact in our models, having too many output classes and a limited amount of training data. The performance of the methods are based on traditional classification metrics (e.g. precision, recall, and f1-score) on the macro and micro averaged level, and focus on two sets of labels: a) the ”NA” tag, which recognize data that is irrelevant and therefore should be excluded from the form; and b) all other tags, which account for the different fields of the form. The neural network achieved an F1-score of 0.8 (for the ”NA” tag) and a macro-averaged F1-score of 0.308 and a micro-averaged result of 0.514 (for the remaining categories), while the ensemble pipeline got 0.813 (for the ”NA” tag) and 0.345 and 0.503 for the macroand micro-averaged rates on the rest of the labels.	artificial neural network;distributional semantics;emoticon;f1 score;feature engineering;information extraction;random forest;relevance;synthetic intelligence;word embedding	Lautaro Quiroz;Lydia Mennes;Mostafa Dehghani;Evangelos Kanoulas	2016			information extraction;data mining;distributional semantics;computer science	NLP	-19.33907423331753	-71.25141254957929	9265
eb66861d02d58a86a10b7915ed6931f812a9728e	a tensor neural network with layerwise pretraining: towards effective answer retrieval		In this paper we address the answer retrieval problem in community-based question answering. To fully capture the interactions between question-answer pairs, we propose an original tensor neural network to model the relevance between them. The question and candidate answers are separately embedded into different latent semantic spaces, and a 3-way tensor is then utilized to model the interactions between latent semantics. To initialize the network layers properly, we propose a novel algorithm called denoising tensor autoencoder (DTAE), and then implement a layerwise pretraining strategy using denoising autoencoders (DAE) on word embedding layers and DTAE on the tensor layer. The experimental results show that our tensor neural network outperforms various baselines with other competitive neural network methods, and our pretraining DTAE strategy improves the system’s performance and robustness.	algorithm;artificial neural network;autoencoder;deep learning;embedded system;interaction;machine translation;money;noise reduction;question answering;relevance;word embedding	Xinqi Bao;Yunfang Wu	2016	Journal of Computer Science and Technology	10.1007/s11390-016-1689-4	artificial intelligence;machine learning;tensor product network;algorithm	AI	-17.937020115242117	-72.31658982821534	9293
1a4fb46971558b7962ad8a4a11f095ad55ff3ae2	investigating variation in english vowel-to-vowel coarticulation in a longitudinal phonetic corpus	intra speaker variation;inter speaker variation;corpus phonetics;coarticulation	Understanding the nature of individual variation in speech, particularly the mechanism underlying such variability, is increasingly important, especially for research on sound change, since such investigations might help explain why sound change happens at all and, conversely, why sound change is so rarely actuated even though the phonetic preconditions are always present in speech. The present study contributes to the literature on interand intraspeaker variation in coarticulation, a major precursor to sound change, by focusing on the degree of coarticulation stressed vowels have on neighboring unstressed vowels using recordings from a longitudinal phonetic corpus of oral arguments before the Supreme Court of the United States. Significant inter-speaker variation in height coarticulation, both anticipatory and carryover, is observed, while no evidence for systematic inter-speaker variability in backness coarticulation is found. There is also no evidence for intra-speaker variation in coarticulation over the course of 205 days.	heart rate variability;precondition;spatial variability;text corpus	Alan C. L. Yu;Carissa Abrego-Collier;Jacob Phillips;Betsy Pillion	2015			speech recognition;acoustics;linguistics	NLP	-10.84268737326646	-81.60570868131215	9296
c02a7dd94d3743a6401aadb79f707b1fc53324ea	active learning based weak supervision for textual survey response classification		Analysing textual responses to open-ended survey questions has been one of the challenging applications for NLP. Such unstructured text data is a rich data source of subjective opinions about a specific topic or entity; but it is not amenable to quick and comprehensive analysis. Survey coding is the process of categorizing such text responses using a pre-specified hierarchy of classes (often called a code-frame). In this paper, we identify the factors constraining the automation approaches to this problem and observe that a completely supervised learning approach is not feasible in practice. We then present details of our approach which uses multi-label text classification as a first step without requiring labeled training data. This is followed by the second step of active learning based verification of survey response categorization done in first step. This weak supervision using active learning helps us to optimize the human involvement as well as to adapt the process for different domains. Efficacy of our method is established using the high agreement with real-life, manually annotated benchmark data.	active learning (machine learning);algorithm;benchmark (computing);categorization;cluster analysis;document classification;iterative method;knowledge base;machine learning;multi-label classification;natural language processing;nonlinear gameplay;real life;supervised learning;text corpus;unsupervised learning	Sangameshwar Patil;Balaraman Ravindran	2015		10.1007/978-3-319-18117-2_23	noisy text;supervised learning;coding (social sciences);natural language processing;machine learning;active learning;artificial intelligence;computer science;automation;hierarchy;categorization;training set	NLP	-26.530802893575782	-69.73628393876533	9307
b347c1a7fac1a88a93c9188a33d26a07706f8cca	tangible rhythm: sensorimotor representations of metrical structure and learning musical rhythm with gesture		When we listen to music, we can mentally control how we perceive the beat. This ability is thought to be subserved by sensorimotor imagery, having top-down effects on attentional-allocation and perception. Here, we examine whether imagined “up and down” gestures can support an internal generation of metrical accent in rhythmic sequences. We also examine how this type of motor imagery interacts with either metrically congruent or incongruent auditory imagery. This is explored using EEG with a frequency-tagging approach, quantifying the strength of metrical accent with the amplitude of beat-related SSEPs. Gesture supports our ability to think and learn by fostering an alignment between sensorimotor representations and more abstract conceptual structure. Therefore, the imagined gestures may act as a bridge between perceptual and action-oriented understandings of metrical structure and the more abstract conceptual ones that musicians struggle with in their training. These imagery strategies may then be beneficial to music education.	electroencephalography;tangible user interface;top-down and bottom-up design	Courtney Hilton;Micah B. Goldwater;Michael Jacobson	2017			cognitive psychology;psychology;gesture;rhythm	ML	-7.767632028378911	-79.28650672789679	9313
0593e562eeab5a642ce97f469994d0688c9ee3a0	on the web communication assist aide based on the bilingual sign language dictionary	conference paper	We discuss the basic ideas behind a Japanese to American Sign Language Translation System for the Japanese users, which assists Japanese Deaf people to communicate. Our discussion covers two main points. The first describes the necessity of a Sign Language Translation System. Since there is no “universal sign language” or real “international sign language,” if Deaf people should learn at least three languages: they want to talk to people whose mother tongue is different from their owns, the mother sign language, the mother spoken language as an intermediate language, and the sign language in which they want to communicate. The second describes the use of computer, especially WWW which is very popular today. As the use of computers becomes widespread, it is increasingly convenient to study through computer software or Internet facilities. Our translation system provides Deaf people with an easy means of access using their mother-spoken language. It also provides a way for people who are going to learn American sign language to look up new vocabulary. We are further planning to examine how our system could be used to educate and assist Deaf people.	computer;dictionary;internet;machine translation;vocabulary;www	Emiko Suzuki;Kyoko Kakihana	2005			natural language processing;speech recognition;computer science;linguistics	HCI	-29.024486551957096	-84.49196800839391	9371
74b9fa769f331538baee0dc62cdb9dcae08af50e	an approach for predicting the popularity of online security news articles		Online social platform has become the main way for people to access information and the main platform for news delivery. Journalists and editors face with the task of determining which article will receive a significant amount of user attention. The reasons that impact on the popularity of news articles are varying from complex factors including the category of news articles, author and many more. To cast the popularity prediction, we extract features from title, summary and publication date. Then a novel framework composed of four parts is designed to predict the popularity. We apply a recurrent neural network model called Gated Recurrent Unit(GRU) to our framework, experiments on our dataset including about 25939 Chinese security news articles in China’s websites and 7654 English security news articles and blogs in foreign countries. The result of experiment demonstrates that our framework outperforms the baseline model. The average mean absolute error(MAE) of Chinese security news articles is reduced by 54.6315% and the average root mean square error(RMSE) of English security news is decreased to 0.21677.	artificial neural network;baseline (configuration management);blog;experiment;mean squared error;network model;recurrent neural network;vii	Junli Kong;Baocang Wang;Caiyun Liu;Gaofei Wu	2018	2018 IEEE Conference on Communications and Network Security (CNS)	10.1109/CNS.2018.8433152	computer security;computer science;data mining;metadata;feature extraction;recurrent neural network;mean absolute error;popularity;mean squared error	Web+IR	-18.18325269912677	-69.32653941987589	9376
480aed4d3f56e5bbb83390af6cd413b79fb22dcc	l2 english rhythm in read speech by chinese students		L2 English speech produced by Mandarin Chinese speakers is usually perceived to be intermediate between stress-timed and syllable-timed in rhythm. However, previous studies seldom employed comparable data of target language, source language and L2 interlanguage in one investigation, which may lead to discrepant results. Thus, in this study we conducted a contrastive investigation of 10 Chinese students and 10 native English speakers. We measured the rhythmic correlates in passage readings of Mandarin and L2 English produced by the native Chinese subjects, and those of English by the native British speakers. Comparison of the widely used rhythmic metrics %V, ΔC, ΔV , nPVI, rPVI, VarcoV, and VarcoC confirmed that Mandarin Chinese is a highly syllable-timed language. Results suggested that vowel-related metrics were better indexes to classify L2 English rhythm produced by Chinese speakers as being more syllable-timed than stress-timed. Analysis showed that vowel epenthesis, non-reduction of vowels, and no stressed/unstressed contrast could contribute to the auditory impression of syllable-timed rhythm of their L2 English. This investigation could shed some light on the Chinese accent of L2 English and provided support to facilitate the rhythmic acquisition of stress-timed languages for Chinese students.	compiler;super robot monkey team hyperforce go!;syllable	Hongwei Ding;Xinping Xu	2016		10.21437/Interspeech.2016-427	speech recognition;linguistics	NLP	-11.376961196126455	-82.13775671405632	9393
6277439dde3e62062b6d169bc6eeae8f15236326	exploiting bilingual information to improve web search	ranking function;state-of-the-art monolingual ranking algorithm;ranking technique;bilingual data;bilingual ranking;bilingual feature;linear ranking function;bilingual query;bilingual information;monolingual ranking;web search;corresponding monolingual query log;information need	Web search quality can vary widely across languages, even for the same information need. We propose to exploit this variation in quality by learning a ranking function on bilingual queries: queries that appear in query logs for two languages but represent equivalent search interests. For a given bilingual query, along with corresponding monolingual query log and monolingual ranking, we generate a ranking on pairs of documents, one from each language. Then we learn a linear ranking function which exploits bilingual features on pairs of documents, as well as standard monolingual features. Finally, we show how to reconstruct monolingual ranking from a learned bilingual ranking. Using publicly available Chinese and English query logs, we demonstrate for both languages that our ranking technique exploiting bilingual data leads to significant improvements over a state-of-the-art monolingual ranking algorithm.	cluster analysis;cynthia dwork;document classification;formal system;granular computing;information needs;ranking (information retrieval);search algorithm;thrust;web search engine	Wei Gao;John Blitzer;Ming Zhou;Kam-Fai Wong	2009			natural language processing;information needs;ranking;computer science;data mining;web search query;ranking svm;information retrieval;query language	Web+IR	-27.587726971482866	-66.46256439809177	9397
cd5d86324a5d382dd163cdd5eee0c06393fe0b33	identifying news from tweets		Informal genres such as tweets provide large quantities of data in real time, which can be exploited to obtain, through ranking and classification, a succinct summary of the events that occurred. Previous work on tweet ranking and classification mainly focused on salience and social network features or rely on web documents such as online news articles. In this paper, we exploit language independent journalism and content based features to identify news from tweets. We propose a novel newsworthiness classifier trained through active learning and investigate human assessment and automatic methods to encode it on both the tweet and trending topic levels. Our findings show that content and journalism based features proved to be effective for ranking and classifying content on Twitter.	encode;real-time computing;social network;web page	Jesse Freitas;Heng Ji	2016		10.18653/v1/W16-5602	computer science	Web+IR	-24.2115647489816	-54.923406922326	9434
56b2a0bbe7b757d12a3277311c662c76a5d317af	learning type-driven tensor-based meaning representations		This paper investigates the learning of 3rd-order tensors representing the semantics of transitive verbs. The meaning representations are part of a type-driven tensor-based semantic framework, from the newly emerging field of compositional distributional semantics. Standard techniques from the neural networks literature are used to learn the tensors, which are tested on a selectional preference-style task with a simple 2-dimensional sentence space. Promising results are obtained against a competitive corpus-based baseline. We argue that extending this work beyond transitive verbs, and to higher-dimensional sentence spaces, is an interesting and challenging problem for the machine learning community to consider.	artificial neural network;baseline (configuration management);distributional semantics;machine learning;plausibility structure;preference learning;sentence boundary disambiguation	Tamara Polajnar;Luana Fagarasan;Stephen Clark	2013	CoRR		natural language processing;machine learning;linguistics	NLP	-18.84769181392994	-72.25060092901326	9451
3ec7fdbcfb9cf867455ea04fc575705d25240f6b	hybrid hmm/blstm-rnn for robust speech recognition	robust speech recognition;layered architecture;context information;noise robustness;automatic speech recognition;dynamic bayesian network;hybrid system;hybrid hmm rnn;artificial neural net;long range;recurrent neural network;article in monograph or in proceedings;virtual evidence	The question how to integrate information from different sources in speech decoding is still only partially solved (layered architecture versus integrated search). We investigate the optimal integration of information from Artificial Neural Nets in a speech decoding scheme based on a Dynamic Bayesian Network for noise robust ASR. A HMM implemented by the DBN cooperates with a novel Recurrent Neural Network (BLSTM-RNN), which exploits long-range context information to predict a phoneme for each MFCC frame. When using the identity of the most likely phoneme as a direct observation, such a hybrid system has proved to improve noise robustness. In this paper, we use the complete BLSTM-RNN output which is presented to the DBN as Virtual Evidence. This allows the hybrid system to use information about all phoneme candidates, which was not possible in previous experiments. Our approach improved word accuracy on the Aurora 2 Corpus by 8%.	artificial neural network;aurora;dynamic bayesian network;experiment;hidden markov model;hybrid system;pattern recognition;recurrent neural network;signal-to-noise ratio;speech recognition;support vector machine	Yang Sun;Louis ten Bosch;Lou Boves	2010		10.1007/978-3-642-15760-8_51	speech recognition;computer science;recurrent neural network;multitier architecture;machine learning;pattern recognition;time delay neural network;dynamic bayesian network;hybrid system	NLP	-17.139040566761818	-88.51011428420364	9464
b2461cc58b141acca8b31979b43a8e91ae49fe1a	extracting significant repeating figures in music by using quantized melody contour	music retrieval;signal sampling;multimedia systems;quantisation signal;multiple signal classification music information retrieval content based retrieval query processing data mining cities and towns feature extraction multimedia databases research and development microphones;signal representation;music object representation significant repeating figure extraction music quantized melody contour music feature extraction music indices music retrieval repeating pattern musical segment;multimedia systems music quantisation signal signal representation signal sampling content based retrieval;content based retrieval;music	To extract music features from the raw data of music object and organize them as the music indices is important for music retrieval. A repeating pattern is a series of notes which appear more than once. Most of the repeating patterns are themes or tones for people to remember easily. A figure is defined as a melody contour of a musical segment. A sequence pattern is a melody segment that has the same figure with other melody segments. In this paper, we use the idea of interval between two adjacent notes to form quantized melody contour for representing music objects. This representation differs from existing methods that use notes to form a melody string. We also propose a method to find significant repeating figures. These figures could cover most of the repeating patterns that are in a noted melody string. In addition, the number of repeating figures found in samples of music is less than that of repeating patterns. As a result, the total execution time and memory space can be dramatically decreased.	contour line;quantization (signal processing)	Chuan-Wang Chang;Hewijin Christine Jiau	2003		10.1109/ISCC.2003.1214255	melody;speech recognition;computer science;music;multimedia	Vision	-6.757985807537782	-94.14909296467644	9475
acc5f983e0cf39544112b341f5eda3e63a5826a8	perception and production of l2 mandarin tones by swedish learners		This study presents the results of perception and production of L2 Mandarin tones in mono- and di-syllabic words by Swedish learners at the beginner level. Although studies of perception and production on Mandarin tones are many, those by speakers of lexical-pitch accent language such as Swedish are still very limited. The result reveals both discrepancy and agreement between perception and production. Swedish learners perform best in discriminating a level tone (T1) from contour tones (T2, T3, T4) both in perception and production. Discrepancy between perception and production was noted for T3. In perception, the identification of T3 was second best after the level tone (T1), but the production of T3 was found to be difficult.	contour line;discrepancy function;super robot monkey team hyperforce go!	Yasuko Nagano-Madsen;Xinzheng Wan	2017	2017 25th European Signal Processing Conference (EUSIPCO)	10.23919/EUSIPCO.2017.8081273	mandarin chinese;perception;psychology;pragmatics;speech recognition	HCI	-11.525574601870558	-81.85645428823837	9482
90221884fe2643b80203991686af78a9da0f9791	high level describable attributes for predicting aesthetics and interestingness	tamara berg dhar sagnik;aesthetics prediction;animals;query estimation high level describable attribute aesthetics prediction digital camera visual data image collection aesthetic quality estimation sky illumination attribute natural lighting condition baseline method human quality judgment flickr photo;query estimation;image processing;support vector machines;query processing;estimation method;aesthetic quality estimation;baseline method;query processing cameras image processing internet philosophical aspects;training;natural lighting condition;digital camera;human quality judgment;power method;visualization;image collection;internet;flickr photo;image color analysis;philosophical aspects;sky illumination attribute;humans;lighting;high level describable attribute;visual data;image color analysis animals humans support vector machines visualization training lighting;cameras;web studies information science computer science high level describable attributes for predicting aesthetics and interestingness state university of new york at stony brook dimitris samaras	With the rise in popularity of digital cameras, the amount of visual data available on the web is growing exponentially. Some of these pictures are extremely beautiful and aesthetically pleasing, but the vast majority are uninteresting or of low quality. This paper demonstrates a simple, yet powerful method to automatically select high aesthetic quality images from large image collections. Our aesthetic quality estimation method explicitly predicts some of the possible image cues that a human might use to evaluate an image and then uses them in a discriminative approach. These cues or high level describable image attributes fall into three broad types: 1) compositional attributes related to image layout or configuration, 2) content attributes related to the objects or scene types depicted, and 3) sky-illumination attributes related to the natural lighting conditions. We demonstrate that an aesthetics classifier trained on these describable attributes can provide a significant improvement over baseline methods for predicting human quality judgments. We also demonstrate our method for predicting the “interestingness” of Flickr photos, and introduce a novel problem of estimating query specific “interestingness”.	baseline (configuration management);digital camera;flickr;high-level programming language;image;statistical classification	Sagnik Dhar;Vicente Ordonez;Tamara L. Berg	2011	CVPR 2011	10.1109/CVPR.2011.5995467	support vector machine;computer vision;the internet;visualization;power iteration;image processing;computer science;machine learning;data mining;lighting;multimedia	Vision	-15.782790744116282	-62.1906756544411	9487
612667da339fd2828c7d11602c2265e1a81c4d09	short-text representation using diffusion wavelets	text classification;representation discovery	Usual text document representations such as tf-idf do not work well in classification tasks for short-text documents and across diverse data domains. Optimizing different representations for different data domains is infeasible in a practical setting on the Internet. Mining such representations from the data in an unsupervised manner is desirable. In this paper, we study a representation based on the multi-scale harmonic analysis of term-term co-occurrence graph. This representation is not only sparse, but also leads to the discovery of semantically coherent topics in data. In our experiments on user-generated short documents e.g., newsgroup messages, user comments, and meta-data, we found this representation to outperform other representations across different choice of classifiers. Similar improvements were also observed for data sets in Chinese and Portuguese languages.	coherence (physics);diffusion wavelets;experiment;internet;optimizing compiler;sparse matrix;tf–idf;unsupervised learning;user-generated content;wavelet	Vidit Jain;Jay Mahadeokar	2014		10.1145/2567948.2577345	representation term;computer science;machine learning;pattern recognition;data mining	ML	-18.88716954047369	-65.84814687945905	9494
d3f3761166359d5b7e6a58d157883c5e46c4b27f	seeing several stars: a rating inference task for a document containing several evaluation criteria	rating inference;or phrases;evaluation criteria;review mining;sentiment analysis;feature selection	"""In this paper we address a novel sentiment analysis task of rating inference. Previous rating inference tasks, which are sometimes referred to as """"seeing stars"""", estimate only one rating in a document. However reviewers judge not only the overall polarity for a product but also details for it. A document in this new task contains several ratings for a product. Furthermore the range of the ratings is zero to six points (i.e., stars). In other words this task denotes """"seeing several stars in a document"""". If significant words or phrases for evaluation criteria and their strength as positive or negative opinions are extracted, a system with the knowledge can recommend products for users appropriately. For example, the system can output a detailed summary from review documents. In this paper we compare several methods to infer the ratings in a document and discuss a feature selection approach for the methods. The experimental results are useful for new researchers who try this new task."""		Kazutaka Shimada;Tsutomu Endo	2008		10.1007/978-3-540-68125-0_106	computer science;machine learning;pattern recognition;data mining;feature selection;information retrieval;sentiment analysis	NLP	-22.7626090705797	-60.031906501477366	9581
1e8caf585629e8f6ab90a4c631f41116eae8fd16	improving spatial semantic analysis by a combining model	pattern classification information analysis learning artificial intelligence natural language processing;classifier combination;training;niobium;mixture of experts;semantics;multiple pretraining classifiers;machine learning;semantics classification algorithms support vector machine classification labeling niobium training;combining model;gating mechanism;classification algorithms;pattern classification;support vector machine classification;combination base machine learning;classifier combination spatial semantic analysis mixture of experts;learning artificial intelligence;spatial semantic analysis;em algorithm combining model combination base machine learning spatial semantic analysis chinese language multiple pretraining classifiers gating mechanism;em algorithm;chinese language;information analysis;natural language processing;labeling;semantic analysis	This paper presents a combination base machine learning approach to spatial semantic analysis in Chinese. The model consists of multiple pre-training classifiers and a gating mechanism for integrating the outputs of these classifiers. Then we use EM algorithm to train the parameters of the combining model. Finally the experimental results show an overall improvement on the standard corpus CPB.	cycles per byte;effective method;expectation–maximization algorithm;k-nearest neighbors algorithm;machine learning;naive bayes classifier;semantic role labeling;windows me	Shiqi Li;Tiejun Zhao;Hanjing Li	2010	2010 International Conference on E-Business and E-Government	10.1109/ICEE.2010.363	natural language processing;niobium;labeling theory;expectation–maximization algorithm;computer science;machine learning;pattern recognition;semantics;data analysis;chinese	NLP	-17.951080537708386	-78.99615953168195	9583
bb1a86300b2eb551b85d927f63c114d56f2fb993	grafon: a grapheme-to-phoneme conversion system for duth	recent pattern recognition;concatenative compounding;fast morphological parser;phonological rule;lexical database;grapheme-to-phoneme conversion system;different phonological theory;phonological knowledge base;central unit;scandinavian language;grapheme-to-phoneme coversion system;part of speech;pattern recognition;rule based;user interface;knowledge base	"""We (k;~,.c,ibe a set of rt,o(lttle.,i that together tuake up a grapheme-to phoneme conversion system for Dutch. Modules include a syflabificatiou program, a fast morphological parser, a lexical database, a phonological knowledg*: base, transliteration rules, and phonological rnles. Knowledge and procedures were intlflenmnted object-orientedly. We contrast GRAFON to recent p . t e r n recognitkm and rule. compiler approaches mid t i t to show that the first fails for languages with concatenative comlmtmding (like Dutch, Get,nan, and Scandinavian languages) while the second lack.,; the flexibility to model different phonological theories. It is claimed that sylhtbles (and not graphemes/phonemes or ulorphemes) should be central units in a rtde-based phonemisatkm algorithm. Furthermore, the architectnre of GRA!:""""ON and its nser interface make it ideally suited as a rule testing tool fol phonologists."""	algorithm;cal;compiler;computation;computer-assisted telephone interviewing;dictionary;display resolution;error detection and correction;lexical database;nan;syllable;test automation;theory;transcription (software)	Walter Daelemans	1988				NLP	-28.464069831055145	-81.15319773409871	9595
d68b2ee867ae110b813b046d7aa477ba1a3f9fec	parsing syntactic and semantic dependencies with two single-stage maximum entropy models	overall task;single-stage system;syntactic dependency;semantic parsing;joint parsing;closed track;brown test set;shared task;syntactic parsing;semantic dependency;single-stage maximum entropy model;parsing syntactic;maximum entropy model	This paper describes our system to carry out the joint parsing of syntactic and semantic dependencies for our participation in the shared task of CoNLL-2008. We illustrate that both syntactic parsing and semantic parsing can be transformed into a word-pair classification problem and implemented as a single-stage system with the aid of maximum entropy modeling. Our system ranks the fourth in the closed track for the task with the following performance on the WSJ+Brown test set: 81.44% labeled macro F1 for the overall task, 86.66% labeled attachment for syntactic dependencies, and 76.16% labeled F1 for semantic dependencies.	attachments;entity–relationship model;parsing;principle of maximum entropy;test set	Hai Zhao;Chunyu Kit	2008			natural language processing;speech recognition;computer science;bottom-up parsing;syntactic predicate;s-attributed grammar;pattern recognition	NLP	-22.386256948104606	-74.06162099875444	9665
4c163cd92c8d1c4f54356ef86867d0b4c7e9d389	multiple documents summarization based on evolutionary optimization algorithm	diversity;multi document summarization;self adaptive crossover;content coverage;differential evolution algorithm;optimization model	This paper proposes an optimization-based model for generic document summarization. The model generates a summary by extracting salient sentences from documents. This approach uses the sentence-to-document collection, the summary-to-document collection and the sentence-to-sentence relations to select salient sentences from given document collection and reduce redundancy in the summary. To solve the optimization problem has been created an improved differential evolution algorithm. The algorithm can adjust crossover rate adaptively according to the fitness of individuals. We implemented the proposed model on multi-document summarization task. Experiments have been performed on DUC2002 and DUC2004 data sets. The experimental results provide strong evidence that the proposed optimization-based approach is a viable method for document summarization. 2012 Elsevier Ltd. All rights reserved.	archive;automatic summarization;differential evolution;evolutionary algorithm;mathematical optimization;multi-document summarization;optimization problem	Rasim M. Alguliyev;Ramiz M. Aliguliyev;Nijat R. Isazade	2013	Expert Syst. Appl.	10.1016/j.eswa.2012.09.014	multi-document summarization;computer science;automatic summarization;machine learning;data mining;information retrieval	NLP	-26.534169119693594	-61.78820187075993	9670
534451922950b405b0ca01685c07fb7ba0299374	spatio-temporal semantic representation of cardiac mri in heart attack patients	spatiotemporal phenomena biomedical mri health care medical image processing ontologies artificial intelligence semantic web;ontology cardiac mri spatio temporal representation;t technology general;sarawak general hospital heart centre spatio temporal semantic representation heart attack patients semantic web technologies life science health care data exchange semantic representations spatio temporal ontologies cardiac mri images;ontologies magnetic resonance imaging biomedical imaging heart myocardium semantics hospitals	Semantic Web technologies, applications and tools have made great steps forward in the life science and health care data exchange. However, developing appropriate semantic representations, including designing spatio-temporal ontologies, remains difficult and challenging. In this paper, we describe a framework to engineer a spatio-temporal semantic representation for the Cardiac MRI images using the current existing case studies conducted in Sarawak General Hospital Heart Centre.	ontology (information science);semantic web	D. N. F. Awang Iskandar;Hamimah Ujir	2015	2015 9th International Conference on IT in Asia (CITA)	10.1109/CITA.2015.7349841	computer science;knowledge management;data science;data mining	DB	-50.36622846462105	-65.18470897565989	9671
1137f3311649811f9cd2b51f63bce1ebe892d70a	interpretable rationale augmented charge prediction system		This paper proposes a neural based system to solve the essential interpretability problem existing in text classification, especially in charge prediction task. First, we use a deep reinforcement learning method to extract rationales which mean short, readable and decisive snippets from input text. Then a rationale augmented classification model is proposed to elevate the prediction accuracy. Naturally, the extracted rationales serve as the introspection explanation for the prediction result of the model, enhancing the transparency of the model. Experimental results demonstrate that our system is able to extract readable rationales in a high consistency with manual annotation and is comparable with the attention model in prediction accuracy.	design rationale;document classification;experiment;human-readable medium;introspection;reinforcement learning	Xin Jiang;Hai Ye;Zhunchen Luo;Wen-Han Chao;Wenjia Ma	2018			natural language processing;artificial intelligence;computer science	NLP	-19.023871016238555	-68.73929947767449	9674
962689783282cef4ce15587f47d0d592bd503904	precise environmental searches: integrating hierarchical information search with envirodaemon	search engine;medio ambiente;hierarchy;context specific seaerch;red www;bepress selected works;information retrieval;hierarchical information search;audio video;information search;motor investigacion;internet;recherche information;environment;indexation;jerarquia;information retrieval context specific seaerch environmental search engine hierarchical information search;environmental search engine;world wide web;environnement;reseau www;recuperacion informacion;moteur recherche;hierarchie	Information retrieval has evolved from searches of references, to abstracts, to documents. Search on the Web involves search engines that promise to parse full-text and other ®les: audio, video, and multimedia. With the indexable Web at 320 million pages and growing, dif®culties with locating relevant information have become apparent. The most prevalent means for information retrieval relies on syntax-based methods: keywords or strings of characters are presented to a search engine, and it returns all the matches in the available documents. This method is satisfactory and easy to implement, but it has some inherent limitations that make it unsuitable for many tasks. Instead of looking for syntactical patterns, the user often is interested in keyword meaning or the location of a particular word in a title or header. This paper describes some precise search approaches in the environmental domain that locate information according to syntactic criteria, augmented by the utilization of information in a certain context. The main emphasis of this paper lies in the treatment of structured knowledge, where essential aspects about the topic of interest are encoded not only by the individual items, but also by their relationships among each other. Examples for such structured knowledge are hypertext documents, diagrams, logical and chemical formulae. Bene®ts of this approach are enhanced precision and approximate search in an already focused, context-speci®c search engine for the environment: EnviroDaemon.	approximation algorithm;diagram;don't-care term;html;hypertext;information retrieval;intranet;parsing;query language;sql;standard generalized markup language;string (computer science);surface web;www;web search engine;world wide web;xml	George Jyh-Shian Chang;Gunjan Samtani;Marcus L. Healey;Franz J. Kurfess;Jason Tsong-Li Wang	2001	Journal of Systems Integration	10.1023/A:1011206028302	search-oriented architecture;the internet;computer science;artificial intelligence;phrase search;data mining;database;natural environment;search analytics;world wide web;information retrieval;search engine;hierarchy	Web+IR	-32.83327861340974	-57.14274917237295	9685
37663dc595c0ff7539a41afd0082eecc6ad04dba	patient centered healthcare informatics		 Abstract—The healthcare system is undergoing a transformation from reactive care to proactive and preventive care. Patients or health consumers are actively acquiring knowledge to manage their health and seeking supports from their peers in addition to receiving healthcare support from healthcare professionals. 74% of American adults use the internet, of which 80% have looked online for healthcare information [6]. With the popularity of social media, many health consumers are also exchanging informational and emotional support with peers who have similar health conditions or diseases. The large volume of health consumer contributed content provides valuable resources for healthcare informatics research. It is worth to note that the information in the health consumer contributed content is timelier than the traditional resources such as electronic health records, centralized reporting systems, and pharmaceutical databases because health consumers often discuss their concerns with peers before any of them are reported in the traditional resources [22]. In this article, we review a few important healthcare informatics research issues that are centered on the patient contributed content and concerns.	centralized computing;database;informatics;internet;social media	Christopher C. Yang	2014	IEEE Intelligent Informatics Bulletin		health administration informatics;health informatics;family medicine;medicine	HCI	-62.52277589464866	-60.02849836075176	9694
f828c01b8826c7e4fb37ba5fecb6f1d88739fe51	web-based information retrieval using agent and ontology	filtering;traitement texte;search engine;filtrage;ontologie;buscador;navegacion informacion;red www;query processing;information source;surveillance;source information;information retrieval;navigation information;filtrado;information filtering;reseau web;information browsing;vigilancia;internet;monitoring;recherche information;tratamiento textos;world wide web;recuperacion informacion;monitorage;moteur recherche;monitoreo;ontology;word processing;fuente informacion	With notoriously large and ever-increasing number of websites, users face the challenge of searching, filtering and monitoring ever-changing information of astronomical magnitude. This research has engineered a society of agents for: (1) locating desirable number of URLs, (2) browsing multiple websites simultaneously and (3) monitoring changes in websites. Since WWW is being used across many cultures, text documents may use different terms for the same concept. By using ontological relations of words, (i) a query processing agent (QPA) assists users in selecting desired number of URLs and (ii) information filtering agents (IFAs) are used to retrieve information. Additionally, information monitoring agents are used to constantly monitor and report changes in websites. Experimental results demonstrated that the QPA can find appropriate number of URLs and IFAs are successful in filtering relevant information in many instances.	information retrieval;ontology (information science)	Kwang Mong Sim;Pui Tak Wong	2001		10.1007/3-540-45490-X_48	filter;the internet;computer science;ontology;data mining;database;world wide web;information retrieval;search engine	Web+IR	-34.595822341085814	-57.39451061372871	9708
fc71d72871ab2e5dcf4d93b82a166a9b250632b8	a case study for monitoring fires with twitter		This paper presents a user configurable monitoring system to track in near-realtime tweets describing fire events. The system targets fire related words in a user defined region of interest published on Twitter which are further processed by a text classifier to determine if they describe a known fire event of interest. The system was motivated from a case study that examined a corpus of tweets posted during active bushfires. This demonstrated that useful information is available on Twitter about fire events from people who are in the vicinity. We present an overview of the system describing how it is initially configured by a user to focus on specific fire events in Australia, the development of a text classifier to identify tweets of interest, especially those with accompanying photos, and the monitoring system that can track multiple events at once.	region of interest;text corpus	Robert Power;Bella Robinson;John Colton;Mark A. Cameron	2015			region of interest;classifier (linguistics);internet privacy;computer science	ML	-23.14670763951394	-55.19744601619232	9728
12cbf917b9ff1d6927555f74203c9b656e841d1e	chimera -- three heads for english-to-czech translation		This paper describes our WMT submissions CU-BOJAR and CU-DEPFIX, the latter dubbed “CHIMERA” because it combines on three diverse approaches: TectoMT, a system with transfer at the deep syntactic level of representation, factored phrase-based translation using Moses, and finally automatic rule-based correction of frequent grammatical and meaning errors. We do not use any off-the-shelf systemcombination method.	chimera (software library);google translate;logic programming;mental representation;moses;postediting;tip (unix utility)	Ondrej Bojar;Rudolf Rosa;Ales Tamchyna	2013			speech recognition;computer science;communication;algorithm	NLP	-23.581278777323217	-74.89787775602923	9745
6a075cbeb020df53f57ea886b5001c111b08bfe4	deepcx: a transition-based approach for shallow semantic parsing with complex constructional triggers		This paper introduces the SURFACE CONSTRUCTION LABELING (SCL) task, which expands the coverage of Shallow Semantic Parsing (SSP) to include frames triggered by complex constructions. We present DeepCx, a neural, transition-based system for SCL. As a test case for the approach, we apply DeepCx to the task of tagging causal language in English, which relies on a wider variety of constructions than are typically addressed in SSP. We report substantial improvements over previous tagging efforts on a causal language dataset. We also propose ways DeepCx could be extended to still more difficult constructions and to other semantic domains once appropriate datasets become available.	causal filter;parsing;structured text;test case	Jesse Dunietz;Jaime G. Carbonell;Lori S. Levin	2018			machine learning;artificial intelligence;natural language processing;computer science;parsing	NLP	-20.951367113514358	-74.77942044859944	9762
aace447ac3d53190a16c410133a3f25e68a15a48	sentence length bias in trec novelty track judgements	sentence length;query biased summaries;sentence ranking	"""The Cranfield methodology for comparing document ranking systems has also been applied recently to comparing sentence ranking methods, which are used as pre-processors for summary generation methods. In particular, the TREC Novelty track data has been used to assess whether one sentence ranking system is better than another. This paper demonstrates that there is a strong bias in the Novelty track data for relevant sentences to also be longer sentences. Thus, systems that simply choose the longest sentences will often appear to perform better in terms of identifying """"relevant"""" sentences than systems that use other methods. We demonstrate, by example, how this can lead to misleading conclusions about the comparative effectiveness of sentence ranking systems. We then demonstrate that if the Novelty track data is split into subcollections based on sentence length, comparing systems on each of the subcollections leads to conclusions that avoid the bias."""	ranking (information retrieval)	Lorena Leal Bando;Falk Scholer;Andrew Turpin	2012		10.1145/2407085.2407093	natural language processing;computer science;data mining;information retrieval	NLP	-33.334759615407876	-62.74755600446751	9790
e7037566931d459a060e2832b923594e805ac178	using multi-stream hierarchical deep neural network to extract deep audio feature for acoustic event detection		Extraction of effective audio features from acoustic events definitely influences the performance of Acoustic Event Detection (AED) system, especially in adverse audio situations. In this study, we propose a framework for extracting Deep Audio Feature (DAF) using multi-stream hierarchical Deep Neural Network (DNN). The DAF outputted from the proposed framework fuses the potential complementary information of multiple input feature streams and thus could be more discriminative than those input features for AED. We take two input feature streams and the hierarchical DNNs with two stages as an example for showing the extraction of DAF. The effectiveness of different audio features for AED is evaluated on two audio corpora, i.e. BBC (British Broadcasting Corporation) audio dataset and TV audio dataset with different signal-to-noise ratios. Experimental results show that DAF outperforms other features for AED under several experimental conditions.	acoustic cryptanalysis;artificial neural network;convolutional neural network;decibel;deep learning;delayed auditory feedback;hidden markov model;monkey's audio;nonlinear system;signal-to-noise ratio;text corpus	Yanxiong Li;Xue Zhang;Hai Jin;Xianku Li;Qin Wang;Qianhua He;Qian Huang	2016	Multimedia Tools and Applications	10.1007/s11042-016-4332-z	speech recognition;computer science;data mining	AI	-15.719872088120315	-89.0165736047197	9804
96f21935f7ee1e6cf84dd62baa56af3310a7e19d	a framework for automated pop-song melody generation with piano accompaniment arrangement		We contribute a pop-song automation framework for lead melody generation and accompaniment arrangement. The framework reflects the major procedures of human music composition, generating both lead melody and piano accompaniment by a unified strategy. Specifically, we take chord progression as an input and propose three models to generate a structured melody with piano accompaniment textures. First, the harmony alternation model transforms a raw input chord progression to an altered one to better fit the specified music style. Second, the melody generation model generates the lead melody and other voices (melody lines) of the accompaniment using seasonal ARMA (Autoregressive Moving Average) processes. Third, the melody integration model integrates melody lines (voices) together as the final piano accompaniment. We evaluate the proposed framework using subjective listening tests. Experimental results show that the generated melodies are rated significantly higher than the ones generated by bi-directional LSTM, and our accompaniment arrangement result is comparable with a state-of-the-art commercial software, Band in a Box.		Ziyu Wang;Gus Xia	2018	CoRR			ML	-14.42761253197601	-83.05247607062572	9820
82b4f044edb9115bd7d95ad20a69913bd09ee478	synonym discovery for structured entities on heterogeneous graphs	structured entity;synonym discovery;heterogeneous graph	"""With the increasing use of entities in serving people's daily information needs, recognizing synonyms---different ways people refer to the same entity---has become a crucial task for many entity-leveraging applications. Previous works often take a """"literal"""" view of the entity, i.e., its string name. In this work, we propose adopting a """"structured"""" view of each entity by considering not only its string name, but also other important structured attributes. Unlike existing query log-based methods, we delve deeper to explore sub-queries, and exploit tailed synonyms and tailed web pages for harvesting more synonyms. A general, heterogeneous graph-based data model which encodes our problem insights is designed by capturing three key concepts (synonym candidate, web page and keyword) and different types of interactions between them. We cast the synonym discovery problem into a graph-based ranking problem and demonstrate the existence of a closed-form optimal solution for outputting entity synonym scores. Experiments on several real-life domains demonstrate the effectiveness of our proposed method."""	data model;entity;information needs;interaction;literal (mathematical logic);real life;web page	Xiang Ren;Tao Cheng	2015		10.1145/2740908.2745396	synonym;computer science;data mining;database;world wide web	ML	-27.638529338882098	-63.09964427360721	9829
e701feebb9779b8d85f2de89660c18d51eaaa109	using nlp techniques for tagging events in arabic text	relevant information;information retrieval;text snippet automatic answer validation question answering precise information retrieval large document collection relevant information;precise information retrieval;large document collection;automatic answer validation;text snippet;question answering;information retrieval artificial intelligence text recognition resumes humans search engines pattern analysis	In any text Events represent a rich source of information, such as proper names that specify the people, organizations, locations, etc., that participate in the event, dates that specify the time of the event, noun phrases that describe the event, and keywords that indicate that an interesting event has occurred. In this paper our focus is on those elements in terms of how we use natural language processing techniques to capture, analyze and identify the role each element plays in the event and understand how they are linked to form events as well as how they form the relationships between related events.	information source;natural language processing	Saleem Abuleil	2007	19th IEEE International Conference on Tools with Artificial Intelligence(ICTAI 2007)	10.1109/ICTAI.2007.179	question answering;computer science;data mining;database;information retrieval	DB	-29.695697236663037	-67.64391223749126	9838
40242a0afdbdde785371df13027c1f7055eb2425	experiments on sentence boundary detection	human performance;sentence boundary detection;sentence boundary;memory-based computational approach;automatic speech recognition system;automatic speech recognition	"""This paper explores the problem of identifying sentence boundaries in the transcriptions produced by automatic speech recognition systems. An experiment which determines the level of human performance for this task is described as well as a memorybased computational approach to the problem. 1 T h e P r o b l e m This paper addresses the problem of identifying sentence boundaries in the transcriptions produced by automatic speech recognition (ASR) systems. This is unusual in the field of text processing which has generally dealt with well-punctuated text: some of the most commonly used texts in NLP are machine readable versions of highly edited documents such as newspaper articles or novels. However, there are many types of text which are not so-edited and the example which we concentrate on in this paper is the output from ASR systems. These differ from the sort of texts normally used in NLP in a number of ways; the text is generally in single case (usually upper), unpunctuated and may contain transcription errors. 1 Figure 1 compares a short text in the format which would be produced by an ASR system with a fully punctuated version which includes case information. For the remainder of this paper errorfree texts such as newspaper articles or novels shall be referred to as """"standard text"""" and the output from a speech recognition system as """"ASR text"""". There are many possible situations in which an NLP system may be required to process ASR text. The most obvious examples are NLP systems which take speech input (eg. Moore et al. (1997)). Also, dictation software programs do not punctuate or capitalise their output but, if this information could be added to ASR text, the results would be far more usable. One of the most important pieces of inform1 Speech recognition systems are often evaluated in terms of word error rate (WER), the percentage of tokens which are wrongly transcribed. For large vocabulary tasks and speakerindependent systems, WER varies between 7% and 50%, depending upon the quality of the recording being recognised. See, e.g., Cole (1996). G00D EVENING GIANNI VERSACE ONE OF THE WORLDS LEADING FASHION DESIGNERS HAS BEEN MURDERED IN MIAMI POLICE SAY IT WAS A PLANNED KILLING CARRIED OUT LIKE AN EXECUTION SCHOOLS INSPECTIONS ARE GOING TO BE TOUGHER TO FORCE BAD TEACHERS OUT AND THE FOUR THOUSAND COUPLES WH0 SHARED THE QUEENS GOLDEN DAY Good evening. Gi~nni Versace, one of the world's leading fashion designers, has been murdered in Miami. Police say it was a planned killing carried out like an execution. Schools inspections are going to be tougher to force bad teachers out. And the four thousand couples who shared the Queen's golden"""	automated system recovery;automatic system recovery;baseline (configuration management);computation;human reliability;human-readable medium;inter-rater reliability;kappa calculus;natural language processing;software inspection;speech recognition;transcription (software);vocabulary;word error rate	Mark Stevenson;Robert J. Gaizauskas	2000			natural language processing;speech recognition;computer science;pattern recognition	NLP	-26.030824941968003	-80.58367295460037	9855
420c9c24aa0911c50df9bfddb7ba52dd1e19fa7e	automatic idiom recognition with word embeddings		Expressions, such as add fuel to the fire, can be interpreted literally or idiomatically depending on the context they occur in. Many Natural Language Processing applications could improve their performance if idiom recognition were improved. Our approach is based on the idea that idioms and their literal counterparts do not appear in the same contexts. We propose two approaches: (1) Compute inner product of context word vectors with the vector representing a target expression. Since literal vectors predict well local contexts, their inner product with contexts should be larger than idiomatic ones, thereby telling apart literals from idioms; and (2) Compute literal and idiomatic scatter (covariance) matrices from local contexts in word vector space. Since the scatter matrices represent context distributions, we can then measure the difference between the distributions using the Frobenius norm. For comparison, we implement [8,16,24] and apply them to our data. We provide experimental results validating the proposed techniques.	algorithm;expression (computer science);language-independent specification;literal (computer programming);literal (mathematical logic);microsoft word for mac;natural language processing;parsing;programming idiom;supervised learning;word embedding	Jing Peng;Anna Feldman	2016		10.1007/978-3-319-55209-5_2	natural language processing;vector space;distributional semantics;matrix (mathematics);covariance;matrix norm;mathematics;expression (mathematics);pattern recognition;artificial intelligence	NLP	-25.238405103817332	-72.71808662348495	9869
0b0dc14b8a8dcccbfc62a38355fff2f6a361e9d2	effective lstms for target-dependent sentiment classification		Target-dependent sentiment classification remains a challenge: modeling the semantic relatedness of a target with its context words in a sentence. Different context words have different influences on determining the sentiment polarity of a sentence towards the target. Therefore, it is desirable to integrate the connections between target word and context words when building a learning system. In this paper, we develop two target dependent long short-term memory (LSTM) models, where target information is automatically taken into account. We evaluate our methods on a benchmark dataset from Twitter. Empirical results show that modeling sentence representation with standard LSTM does not perform well. Incorporating target information into LSTM can significantly boost the classification accuracy. The target-dependent LSTM models achieve state-of-the-art performances without using syntactic parser or external sentiment lexicons.1	benchmark (computing);end-to-end principle;long short-term memory;parsing;performance;semantic similarity;sentiment analysis	Duyu Tang;Bing Qin;Xiaocheng Feng;Ting Liu	2016			natural language processing;computer science;pattern recognition;information retrieval	NLP	-18.57261414668036	-71.7712054383133	9872
d63f1641549abb9b8463eeaddec2151856927954	automated question answering from lecture videos: nlp vs. pattern matching	electronic learning;automatic testing;videos pattern matching educational programs natural language processing learning systems natural languages speech automatic testing electronic learning knowledge management;knowledge management;speech;natural languages;text extraction;learning systems;pattern matching;educational programs;natural language processing;question answering;videos	This paper explores the feasibility of automated question answering from lecture video materials used in conjunction with PowerPoint slides. Two popular approaches to question answering are discussed, each separately tested on the text extracted from videotaped lectures: 1) the approach based on Natural Language Processing (NLP) and 2) a self-learning probabilistic pattern matching approach. The results of the comparison and our qualitative observations are presented. The advantages and shortcomings of each approach are discussed in the context of video applications for e-learning or knowledge management.	knowledge management;natural language processing;pattern matching;question answering	Jinwei Cao;Jose Antonio Robles-Flores;Dmitri Roussinov;Jay F. Nunamaker	2005	Proceedings of the 38th Annual Hawaii International Conference on System Sciences	10.1109/HICSS.2005.113	natural language processing;speech recognition;question answering;computer science;speech;pattern matching;natural language;information retrieval	NLP	-35.490514941398764	-72.48189004019103	9881
04f6b597d94f5e1b5f2d72a79f2bb2a502851f73	utilizing global and path information with language modelling for hierarchical text classification	hierarchical text classification;web taxonomy;language models	Hierarchical text classification of a Web taxonomy is challenging because it is a very large-scale problem with hundreds of thousand categories and associated documents. Furthermore, the conceptual levels and training data availabilities of categories vary widely. The narrow-down approach is the state-of-the-art that utilizes a search engine for generating candidates from the taxonomy and builds a classifier for the final category selection. In this paper, we take the same approach but address the issue of using global information in a language modelling framework to improve effectiveness. We propose three methods of using non-local information for the task: a passive way of utilizing global information for smoothing, an aggressive way where a top-level classifier is built and integrated with a local model, and a method of using label terms associated with the path from a category to the root, which is based on our systematic observation that they are underrepresented in the documents. For evaluation, we constructed a document collection from Web pages in the Open Directory Project (ODP). A series of experiments and their results show the superiority of our methods and reveal the role of global information in hierarchical text classification.	apple open directory;archive;baseline (configuration management);document classification;experiment;information retrieval;information science;language model;percussion mallet;rm-odp;recursion;smoothing;statistical classification;tf–idf;web page;web search engine;web-based taxonomy	Heung-Seon Oh;Sung-Hyon Myaeng	2014	J. Information Science	10.1177/0165551513507415	taxonomy;computer science;data science;machine learning;data mining;database;world wide web;information retrieval;language model	Web+IR	-23.730262512785902	-64.37888002912774	9920
920796cf6e1904dac5a5d906929340efca643e4c	assisting therapists in assessing small animal phobias by computer analysis of video-recorded sessions	clinical diagnostics;info eu repo semantics article;computer vision;visual representations;small animal phobia;therapists	Behavioural Avoidance Tests (BATs) are commonly used for assessing phobias. While easy to deploy, these tests have some practical difficulties. For instance, therapists have to make distance estimations that are hard to do with accuracy and objectivity; or information regarding the performance of the patients (e.g. their walking pattern) is lost. To alleviate these difficulties, a computerized tool has been developed to extract the walking pattern of patients while approaching the phobic stimulus. From a video-recorded BAT session, two visual representations have been explored to compactly summarize the patient’s behavior: a static one (an image) and a dynamic one (an animation). A proof-of-concept prototype has been tested with 23 therapists. Most of the therapists preferred the animated representation, since it provides them with a better sense of the dynamics of how the patient really behaved. The participants agreed that this tool might be useful in assisting therapist when assessing phobia through BATs, since diagnostics could be made in a more accurate and objective way.	batch file;computer vision;feedback;information;objectivity/db;prototype;refinement (computing);usability testing	Vicente Castelló;V. Javier Traver;Berenice Serrano;Raúl Montoliu;Cristina Botella	2016	Multimedia Tools and Applications	10.1007/s11042-016-3997-7	computer vision;simulation;computer science	HCI	-41.897905903959504	-53.473934826587254	9930
520ff3927cec828b342fbc8bc962c9c903740642	load characteristics of ground reaction force in gait training system with real-time audiovisual feedback function	legged locomotion;training;measurement uncertainty;force;visualization;force measurement;real time systems	In gait training for walking rehabilitation, it is essential to maintain a good balance during walking (i.e., both legs equally loaded). Therefore, scales are frequently used in order to feed back the load of the lower limbs to the trainee. However, in this situation, the trainee has to look down to check whether the scale and the trainee's walking posture is affected in an undesirable manner. In addition, the trainee generally cannot read the load value accurately. In this study, we propose a gait training system with a real-time audiovisual feedback function for each leg's load. For this system, the feedback of the lower limb load is calculated from the ground reaction force, allowing the trainee to use audio and visual information in real time. This study reports the load characteristics of the ground reaction force of the walking path plate on force plates.	load profile;poor posture;real-time transcription	Kei Fukuyama;Isao Abe Hidetaka Ikeuchi	2016	2016 IEEE/SICE International Symposium on System Integration (SII)	10.1109/SII.2016.7844017	control engineering;simulation;engineering;physical therapy	Robotics	-46.95110720270623	-57.63709274490739	9965
529f3d3caa670615ecdecd23a16c02bd604d5a6b	supporting strategic planning with interactive visualization - a case study of patient flow through a large hospital		Hospitals collect large amounts of data during their daily operation. Next to its immediate primary purpose, this data also contains implicit information that can be used to improve clinical and administrative processes. We present a case study of how strategic infrastructure planning can be supported by the analysis of enriched patient flow through a hospital. Data from various hospital information systems was collected, enriched with topographical and organizational data, and integrated into a coherent data store. Common analysis tools and methods do not support exploration and sense-making well for such large and complex problems. We therefore developed a highly interactive visual analytics application that offers various views onto the data, and provides fast access to details in order to show them in context. The analysts were able to validate their experiences, confirm hypotheses and generate new insights. As a result, several sub-systems of clinics were identified that will play a central role on the future hospital campus. This approach was successful enough that we envision to extend it towards other process optimization tasks in hospitals.	coherence (physics);data store;information system;interactive visualization;mathematical optimization;process optimization;sensemaking;topography;visual analytics	Dominique Brodbeck;Markus Degen;Andreas Walter	2013			human–computer interaction;computer science;knowledge management;multimedia	HCI	-58.91644382720616	-59.95167910211225	9971
4c79e6c04d1f633a2261a5fe33bd305bff78aef1	training data optimization for language model adaptation		Language model (LM) adaptation is a necessary step when the LM is applied to speech recognition. The task of LM adaptation is to use out-domain data to improve in-domain model’s performance since the available in-domain (taskspecific) data set is usually not large enough for LM training. LM adaptation faces two problems. One is the poor quality of the out-domain training data. The other is the mismatch between the n-gram distribution in out-domain data set and that in in-domain data set. This paper presents two methods, filtering and distribution adaptation, to solve them respectively. First, a bootstrapping method is presented to filter suitable portion from two large variable quality out-domain data sets for our task. Then a new algorithm is proposed to adjust the ngram distribution of the two data sets to that of a task–specific but small data set. We consider preventing over-fitting problem in adaptation. All resulting models are evaluated on the realistic application of email dictation. Experiments show that each method achieves better performance, and the combined method achieves a perplexity reduction of 24% to 80%.	algorithm;bootstrapping (compilers);domain model;email;experiment;filter (signal processing);language model;mathematical optimization;n-gram;overfitting;perplexity;regular expression;speech recognition	Xiaoshan Fang;Jianfeng Gao;Jianfeng Li;Huanye Sheng	2003			speech recognition;language model;natural language processing;training set;computer science;artificial intelligence	NLP	-20.014421634040925	-78.82868942346965	10009
870833c5f74f18118aea7141536b4058888f9928	copying equations to assess mathematical competence: an evaluation of pause measures using graphical protocol analysis	attention;mental fatigue;genius;thought and thinking;bf0309 consciousness cognition including learning;imagination;intelligence;psycholinguistics;memory;comprehension	Can mathematical competence be measured by analyzing the patterns of pauses between written elements in the freehand copying of mathematical equations? Twenty participants of varying levels of mathematical competence copied sets of equations and sequences of numbers on a graphics tablet. The third quartile of pauses is an effective measure, because it reflects the greater number of chunks and the longer time spent per chunk by novices as they processed the equations. To compensate for individual differences in speeds of elementary operations and skill in writing basic mathematical symbols, variants on the measure were devised and tested.	adobe freehand;chunking (computing);graphics tablet;tablet computer	Peter Cheng	2014			psychology;cognitive psychology;comprehension;genius;intelligence;developmental psychology;attention;computer science;artificial intelligence;mathematics;linguistics;imagination;psycholinguistics;memory;communication;cognitive science	HCI	-50.529242480844864	-53.430518776011986	10013
69cdc6458897578e3f4ec493c734e677ce20ce12	an architecture for finding entities on the web	erbium;entity centric index;web documents;document handling;entity retrieval;web pages;information extraction;internet document handling information retrieval;information retrieval;entity ranking;web collections;indexes;large scale;internet;web document index;natural language;indexation;large scale entity retrieval;world wide web;web search;service oriented architecture web search erbium search engines wikipedia web pages data mining information retrieval image retrieval natural language processing;natural language processing entity retrieval web search;electronic publishing;entity extraction;encyclopedias;natural language processing;entity centric index world wide web information extraction information retrieval large scale entity retrieval web collections entity extraction entity ranking web documents web document index	Recent progress in research fields such as Information Extraction and Information Retrieval enables the creation of systems providing better search experiences to web users. For example, systems that retrieve entities instead of just documents have been built. In this paper we present an approach for large-scale Entity Retrieval using web collections as underlying corpus. We propose an architecture for entity extraction and entity ranking starting from web documents. This is obtained (1) using an existing web document index and (2) creating an entity centric index. We describe advantages and feasibility of our approach using state-of-the-art tools.	entity;information extraction;information retrieval;named-entity recognition;web page;world wide web	Gianluca Demartini;Claudiu S. Firan;Mihai Georgescu;Tereza Iofciu;Ralf Krestel;Wolfgang Nejdl	2009	2009 Latin American Web Congress	10.1109/LA-WEB.2009.14	web service;database index;web development;web modeling;erbium;the internet;web mapping;web standards;computer science;web page;database;electronic publishing;web intelligence;natural language;world wide web;information extraction;information retrieval;encyclopedia	Web+IR	-29.678623455636274	-57.312240771397484	10019
356b24b65f30992bad51a9ab1017ba3aa98040bb	integrated adaptation with multi-factor joint-learning for far-field speech recognition	integrated adaptation;factor representation;integrated adaptation far field speech recognition deep neural network factor representation multi task learning;deep neural network;far field speech recognition;multi task learning;ami sdm task far field automatic speech recognition distant talking scenario integrated adaptation with multifactor joint learning speaker extraction deep neural network asr dnn classification accuracy improvement factor extraction improvement factor extractor dnn word error rate reduction wer i vector adaptation ami single distant microphone;feature extraction speech recognition training speech adaptation models data mining hidden markov models;speech recognition learning artificial intelligence microphones neural nets signal classification	Although great progress has been made in automatic speech recognition (ASR), significant performance degradation still exists in distant talking scenarios due to significantly lower signal power. In this paper, a novel adaptation framework, named integrated adaptation with multi-factor joint-learning, is proposed to improve the recognition accuracy for distant speech recognition. We explore and extract speaker, phone and environment factor representations using deep neural networks (DNNs), which are integrated into the main ASR DNN to improve classification accuracy. In addition, the hidden activations in the main ASR DNN are used to improve the factor extraction, which in turn helps the ASR DNN. All the model parameters, including those in the ASR DNN and factor extractor DNNs, are jointly optimized under the multi-task learning framework. Further more, unlike prior techniques, our novel approach requires no explicit separate stages for factor extraction and adaptation. Experiments on the AMI single distant microphone (SDM) task show that the proposed architecture can significantly reduce word error rate (WER) and additional improvement can be achieved by combining it with the i-vector adaptation. Our best configuration obtained more than 15% and 10% relative reduction on WER over the baselines using the SDM and close-talk data generated alignments, respectively.	artificial neural network;automated system recovery;computer multitasking;deep learning;elegant degradation;microphone;multi-task learning;randomness extractor;speech recognition;word error rate	Yanmin Qian;Tian Tan;Dong Yu;Yu Zhang	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7472783	multi-task learning;speech recognition;computer science;machine learning;pattern recognition	Robotics	-17.052556909690264	-89.78948986628039	10036
362a8d6a4a5dfc4616412b28db094c46e8bf9a58	speech for multimedia information retrieval	video information retrieval;image processing;bepress selected works;information retrieval;video compression;multimedia information retrieval;continuous speech recognition;natural language understanding;indexation;video on demand;multimedia indexing and search;speech recognition;multimedia indexing;informedia;news on demand;digital video library;video information retrieval speech recognition news on demand multimedia indexing and search informedia	"""We describe the Informediatm News-on-Demand system. News-on-Demand is an innovative example of indexing and searching broadcast video and audio material by text content. The fully-automatic system monitors TV news and allows selective retrieval of news items based on spoken queries. The user then plays the appropriate video """"paragraph"""". The system runs on a Pentium PC using MPEG-I video compression and the Sphinx-II continuous speech recognition system [6]."""	data compression;information retrieval;mpeg-1;moving picture experts group;speech recognition;sphinx4	Alexander G. Hauptmann;Michael J. Witbrock;Alexander I. Rudnicky	1995		10.1145/215585.215667	data compression;image processing;computer science;multimedia;world wide web;information retrieval;human–computer information retrieval;speech analytics	Web+IR	-14.979105773324846	-55.524459581491854	10045
6c580aba46896c63c844d4cc24470940ffbc9c8a	production and multi-channel distribution of news	cycle time;news production;production process;technology and engineering;multimedia mastering;file based production;archiving;new products	News production is characterised by complex and dynamic workflows as it is important to produce and distribute news as soon as possible and in an audiovisual quality as good as possible. In this paper, we present news production as it has been implemented at the Flemish Radio and Television (Vlaamse radio en televisie, VRT). Driven by the dynamic nature of news content, the VRT news department is optimized for short cycle times and characterised by a highly parallel production process, i.e. product engineering (news bulletin composition or “organise” and story-editing or “construct message”), various material procurement (“create media asset”) processes, mastering (“publish”), and the distribution processes largely run in parallel. The formal expression of news operations in terms of canonical processes has allowed us to disambiguate the overall process, and it will help us developing meaningful and reusable interfaces.	bus mastering;formal language;procurement;product engineering	Erik Mannens;Maarten Verwaest;Rik Van de Walle	2008	Multimedia Systems	10.1007/s00530-008-0138-9	simulation;telecommunications;cycle time variation;electrical engineering;operating system;scheduling;multimedia;world wide web	Web+IR	-33.19430349677089	-75.28194550470661	10056
a6b904b68a1c5f258b76afaa2998fc8ab964f536	a face recognition system that simulates perception impairments of autistic children	handicapped aids face recognition;face face recognition autism neurons testing performance analysis abstracts;handicapped aids;face recognition;horizontal invariants face recognition system perception impairment simulation autistic children	We use a face recognition algorithm to model differences in perception between autistic and non autistic children. With our model it is possible to reproduce several phenomena of autism by assuming that autistic children lack the ability to abstract from horizontal invariants. In particular, we can explain why autistic children are able to better recognize faces from parts of the face while the overall recognition of faces is worse than in non autistic children.	facial recognition system	Horng Jason Wu;Wen-Chun Chen;Norbert Michael Mayer	2012		10.1109/DevLrn.2012.6400577	psychology;cognitive psychology;developmental psychology;communication	Robotics	-6.109625310047356	-78.432101695516	10087
19745b317151c4a3db33f4b293ca80127561c511	text entry performance of state of the art unconstrained handwriting recognition: a longitudinal user study	longitudinal study;handwriting recognition;user study;text entry;software keyboard;error rate;handwriting	We report on a longitudinal study of unconstrained handwriting recognition performance. After 250 minutes of practice, participants had a mean text entry rate of 24.1 wpm. For the first four hours of usage, entry and error rates of handwriting recognition are about the same as for a baseline QWERTY software keyboard. Our results reveal that unconstrained handwriting is faster than what was previously assumed in the text entry literature.	baseline (configuration management);handwriting recognition;usability testing;virtual keyboard;words per minute	Per Ola Kristensson;Leif C. Denby	2009		10.1145/1518701.1518788	natural language processing;speech recognition;word error rate;computer science;multimedia;handwriting recognition	HCI	-24.852070816010553	-87.50072154884842	10107
c554c424e09244b713115b0686565e69471df1d5	trigger word mining for relation extraction based on activation force	trigger word;pattern learning;relation extraction;activation force;structure feature	SUMMARYrnrnIn this paper, relation extraction is characterized as structured feature learning, and activation force (AF) is employed to extract and construct structured features. Trigger word is a low-level feature, and it is very crucial in relation extraction. We define the trigger word as a word that is most likely to form a structure corresponding to a special relation. To extract trigger words, firstly, posteriori probability weighted frequency AF model, in which AF is regarded as posteriori probability weighted frequency, is presented. Secondly, a generative AF is introduced. Then, a higher level structured feature, named trigger-word dependency pair (TWDP), is extracted by a reduced AF model. Based on the trigger words and TWDPs, the most advanced patterns, the shortest dependency paths (SDPs), are optimized. The evaluation corpus of Knowledge Base Population in Text Analysis Conference is adopted to test our methods. Experiments show that 87.30% of Stanford manual trigger words appearing in the training sentences can be found by G-AF. The experimental results also verified that the modified SDP patterns are superior to original SDP patterns. Copyright © 2014 John Wiley u0026 Sons, Ltd.	relationship extraction	Weiran Xu;Chunyun Zhang	2016	Int. J. Communication Systems	10.1002/dac.2897	relationship extraction;speech recognition;computer science;pattern recognition;data mining	NLP	-24.223653977738593	-65.8875360586434	10115
b055d69b449a1dc553b3dd32df47c7ddc4b1e409	analysis of the usage of japanese segmented texts in ntcir workshop 2	ntcir;japanese text segmentation;morphological analyzer.	In this paper, we report on the usage of Japanese segmented texts and analyze the submitted search results to NTCIR Workshop 2, which used these texts. In these texts, each sentence is segmented into terms and term components (similar to phrases and words). However, the sizes of terms are inconsistent in the texts; e.g., some terms that should be decomposed into term components remain as terms. We analyze the effect of this inconsistency from the viewpoint of comparison between word-based indexing and phrasal indexing. Based on this analysis, we propose the desired specification of a morphological analyzer for Information Retrieval.	alloy analyzer;information retrieval;morphological pattern;random indexing;text segmentation	Masaharu Yoshioka;Kazuko Kuriyama;Noriko Kando	2001			natural language processing;search engine indexing;artificial intelligence;sentence;computer science	NLP	-29.102278784416455	-67.71477113141248	10133
026fc9f885600ac2562fe54c05c0c0f9a27fd20c	sentiment analysis and the complex natural language	simulation and modeling;operation research decision theory;operations research management science;complex systems	Introduction Sentiment analysis (Pang and Lillian 2008) is a type of text classification that deals with subjective statements. It is also known as opinion mining, since it processes opinions in order to learn about public perception. Sentiment analysis and opinion mining are the same, and are used interchangeably throughout the document. It uses natural language processing (NLP) to collect and examine opinion or sentiment words. SA is explained as identifying the sentiments of people about a topic and its features (Pang and Lillian 2008). The reason for the popularity of opinion mining is because people prefer to take advice from others in order to invest sensibly. Determining subjective attitudes in big social data is a hotspot in the field of data mining and NLP (Hai et al. 2014). Abstract	data mining;document classification;natural language processing;sentiment analysis	Muhammad Taimoor Khan;Mehr Yahya Durrani;Armughan Ali;Irum Inayat;Shehzad Khalid;Kamran Habib Khan	2016	CASM	10.1186/s40294-016-0016-9	natural language processing;computer science;data science;data mining;sentiment analysis	NLP	-22.563924376312347	-59.12115567536214	10161
6e25fbb0b04e3cd39e5c71fa06211bb200b0b163	application of different learning methods to hungarian part-of-speech tagging	linguistica matematica;algorithm analysis;systeme apprentissage;tratamiento lenguaje;logical programming;learning systems;analyse syntaxique;learning methods;programmation logique;language processing;computational linguistic;analisis sintaxico;syntactic analysis;part of speech tagging;traitement langage;part of speech;linguistique mathematique;analyse algorithme;computational linguistics;point of view;programacion logica;analisis algoritmo	From the point of view of computational linguistics, Hungarian is a diffcult language due to its complex grammar and rich morphology. This means that even a common task such as part-of-speech tagging presents a new challenge for learning when looked at for the Hungarian language, especially given the fact that this language has fairly free word order. In this paper we therefore present a case study designed to illustrate the potential and limits of current ILP and non-ILP algorithms on the Hungarian POS-tagging task. We have selected the popular C4.5 and Progol systems as propositional and ILP representatives, adding experiments with our own methods AGLEARN, a C4.5 preprocessor based on attribute grammars, and the ILP approaches PHM and RIBL. The systems were compared on the Hungarian version of the multilingual morphosyntactically annotated MULTEXT-East TELRI corpus which consists of about 100.000 tokens. Experimental results indicate that Hungarian POS-tagging is indeed a challenging task for learning algorithms, that even simple background knowledge leads to large differences in accuracy, and that instance-based methods are promising approaches to POS tagging also for Hungarian. The paper also includes experiments with some different cascade connections of the taggers.	part-of-speech tagging	Tamás Horváth;Zoltán Alexin;Tibor Gyimóthy;Stefan Wrobel	1999		10.1007/3-540-48751-4_13	natural language processing;part of speech;computer science;artificial intelligence;computational linguistics;parsing;linguistics;algorithm	NLP	-26.39044871696717	-77.45830615712777	10192
e27d81521dc4e8b6ea93947c05ffccf06784f569	audio chord recognition with recurrent neural networks		In this paper, we present an audio chord recognition system based on a recurrent neural network. The audio features are obtained from a deep neural network optimized with a combination of chromagram targets and chord information, and aggregated over different time scales. Contrarily to other existing approaches, our system incorporates acoustic and musicological models under a single training objective. We devise an efficient algorithm to search for the global mode of the output distribution while taking long-term dependencies into account. The resulting method is competitive with state-of-the-art approaches on the MIREX dataset in the major/minor prediction task.	acoustic cryptanalysis;algorithm;artificial neural network;chroma feature;deep learning;recurrent neural network	Nicolas Boulanger-Lewandowski;Yoshua Bengio;Pascal Vincent	2013			speech recognition;artificial intelligence;machine learning;artificial neural network;time delay neural network;recurrent neural network;computer science;chord (music);pattern recognition	ML	-16.830213381398934	-88.36289298663917	10206
0a49824b83d4be04d43f54857b72653f7b62c9ef	university of glasgow at trec 2010: experiments with terrier in blog and web tracks		In TREC 2010, we continue to build upon the Voting Model and experiment with our novel xQuAD framework within the auspic es of the Terrier IR Platform. In particular, our focus is the de velopment of novel applications for data-driven learning in the B log and Web tracks, with experimentation spanning hundreds of feat ur s. In the Blog track, we propose novel feature sets for the ranki ng of blogs, news stories and blog posts. In the Web track, we propo se novel selective approaches for adhoc and diversity search.	blog;file spanning;world wide web	Rodrygo L. T. Santos;Richard McCreadie;Craig MacDonald;Iadh Ounis	2010			information retrieval;computer science;voting	Web+IR	-30.41522946294812	-62.770661058741396	10211
2efd0531bb0a64aaceac4db373d512fc0088c283	linear time algorithms for finding maximal forward references	information resources;web pages;linear time complexity linear time algorithms maximal forward reference finding very large web logs;time complexity;information retrieval;linear time algorithm;computational complexity;information retrieval computational complexity information resources;algorithm design and analysis performance analysis computer science web mining web pages navigation web server prefetching delay personnel	In this paper, two algorithms are designed for finding maximal forward references from very large Web logs, longest sequences of Web pages visited by a user without revisiting some previously visited page in the sequence, an d their performance is comparatively analyzed. It is shown that the two algorithms have linear (hence optimal) time complexity.	algorithm;blog;maximal set;paging;time complexity;web page	Zhixiang Chen;Richard H. Fowler;Ada Wai-Chee Fu	2003		10.1109/ITCC.2003.1197519	probabilistic analysis of algorithms;computer science;theoretical computer science;worst-case complexity;world wide web;information retrieval	Web+IR	-31.014871461393227	-55.95138002882801	10217
7844af167f2b004f5ebdb8d582759063e3643163	implicit relevance feedback from eye movements	busqueda informacion;ojo;modelo markov oculto;eye;fisiologia;modele markov cache;hidden markov model;information retrieval;implicit feedback;movimiento ocular;time series;physiologie;physiology;hidden markov models;recherche information;eye movements;eye movement;serie temporelle;serie temporal;retroaction pertinence;implicit relevance feedback;mouvement oculaire;relevance feedback;oeil	We explore the use of eye movements as a source of implicit relevance feedback information. We construct a controlled information retrieval experiment where the relevance of each text is known, and test usefulness of implicit relevance feedback with it. If perceived relevance of a text can be predicted from eye movements, eye movement signal must contain information on the relevance. The result is that relevance can be predicted to a considerable extent with discriminative hidden Markov models, and clearly better than randomly already with simple linear models of time-averaged data.	hidden markov model;information retrieval;linear model;markov chain;randomness;relevance feedback	Jarkko Salojärvi;Kai Puolamäki;Samuel Kaski	2005		10.1007/11550822_80	computer vision;simulation;computer science;artificial intelligence;machine learning;hidden markov model;eye movement	NLP	-7.2722002666815415	-71.47032070274604	10219
4561af16d026134fb6c772a71919198cfd1abdfd	cross-lingual terminology extraction for translation quality estimation		We explore ways of identifying terms from monolingual texts and integrate them into investigating the contribution of terminology to translation quality.The researchers proposed a supervised learning method using common statistical measures for termhood and unithood as features to train classifiers for identifying terms in cross-domain and cross-language settings. On its basis, sequences of words from source texts (STs) and target texts (TTs) are aligned naively through a fuzzy matching mechanism for identifying the correctly translated term equivalents in student translations. Correlation analyses further show that normalized term occurrences in translations have weak linear relationship with translation quality in term of usefulness/transfer, terminology/style, idiomatic writing and target mechanics and nearand above-strong relationship with the overall translation quality. This method has demonstrated some reliability in automatically identifying terms in human translations. However, drawbacks in handling low frequency terms and term variations shall be dealt in the	programming idiom;supervised learning;terminology extraction	Yu Yuan;Yuze Gao;Yue Zhang;Serge Sharoff	2018			natural language processing;artificial intelligence;speech recognition;computer science;terminology extraction	NLP	-25.748432522454465	-72.7804347139588	10220
0d5fdac610b9dc5670c8c93704d32450eff0d3aa	phase-based dual-microphone robust speech enhancement	microphones;perceptual quality;time varying;reverberation time;speech processing;time frequency;robustness speech enhancement speech recognition filters speech coding time frequency analysis time difference of arrival microphones distortion reverberation;prior knowledge;speech enhancement;indexing terms;time frequency analysis speech enhancement speech recognition microphones;single channel;microphone array;algorithms computer simulation information storage and retrieval models biological pattern recognition automated signal processing computer assisted sound spectrography speech intelligibility speech production measurement stochastic processes voice quality;speech recognition;time frequency analysis;speech recognition dual microphone speech signal enhancement time frequency phase error filter microphone array speech processing;time difference of arrival	A dual-microphone speech-signal enhancement algorithm, utilizing phase-error based filters that depend only on the phase of the signals, is proposed. This algorithm involves obtaining time-varying, or alternatively, time-frequency (TF), phase-error filters based on prior knowledge regarding the time difference of arrival (TDOA) of the speech source of interest and the phases of the signals recorded by the microphones. It is shown that by masking the TF representation of the speech signals, the noise components are distorted beyond recognition while the speech source of interest maintains its perceptual quality. This is supported by digit recognition experiments which show a substantial recognition accuracy rate improvement over prior multimicrophone speech enhancement algorithms. For example, for a case with two speakers with a 0.1 s reverberation time, the phase-error based technique results in a 28.9% recognition rate gain over the single channel noisy signal, a gain of 22.0% over superdirective beamforming, and a gain of 8.5% over postfiltering.	algorithm;beamforming;block cipher;digit structure;dual;experiment;hoc (programming language);large;microphone device component;multilateration;signal-to-noise ratio;speech enhancement;benefit;contents - htmllinktype	Parham Aarabi;Guangji Shi	2004	IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)	10.1109/TSMCB.2004.830345	voice activity detection;speech recognition;time–frequency analysis;computer science;speech processing	ML	-12.9797909842739	-91.1630645203039	10226
b5f5b43f13f3ab8aa24162e116d4105310106ad3	pitch period determination of speech sounds	pitch period determination	A computer procedure which determines pitch periods by the recognition of the peak structure of the speech waveform is described. Speech sounds were sampled by a microphone and an analog-to-digital converter attached to an interconnected IBM 7090-PDP-1 system. These utterances were recorded at the normal noise level of the computer room but were not band-compressed or phase-distorted in any manner. A sequence of operations defined on the speech wave selects a list of points along the waveform as candidates for pitch markers. These markers are validated by an error detection and correction procedure. About 95 percent of the pitch periods were recognized correctly within 1 to 2 times real-time on the IBM 7090.	analog-to-digital converter;error detection and correction;ibm 7090;microphone;noise (electronics);pitch (music);real-time transcription;waveform	Raj Reddy	1967	Commun. ACM	10.1145/363332.363340	pitch (music)	Graphics	-15.683395873256956	-84.45939429327589	10298
75505b9ad77ebd3888ee531008f09744af8cdced	ufrgs@pan2010: detecting external plagiarism - lab report for pan at clef 2010		This paper presents our approach to detect plagiar ism in the PAN’10 competition. To accomplish this task we applied a m ethod which aims at detecting external plagiarism cases. The method is specia lly designed to detect crosslanguage plagiarism and is composed by five phases: language normalization, retrieval of candidate documents, classifier traini ng, plagiarism analysis, and post-processing. Our group got the seventh place in the competition with an overall score of 0.5175. It is important to notice that the final score was affected by our low recall (0.4036) which arose as a result of not detecting intrinsic plagiarism cases, which were also present in the compe tition corpus.	sensor;video post-processing	Rafael Corezola Pereira;Viviane Pereira Moreira;Renata de Matos Galante	2010				ML	-31.360314204406848	-65.31153338772842	10305
842cbc5ffcdd6f20f365a6f9f5a32ae52e30ea53	discovering structure in islamist postings using systemic nets	electronic mail;frequency measurement;artificial neural networks;matrix decomposition;matrix converters;clustering algorithms;context;conference proceeding	Textual analytics based on representations of documents as bags of words has been extremely successful. However, analysis that requires deeper insight into language, into author properties, or into the contexts in which documents were created requires a richer representation. Systemic nets are one such representation. The jihadist groups AQAP, ISIS, and the Taliban have all produced English magazines designed to influence Western sympathizers. Using a model of jihadi language, we construct a systemic functional net for these magazines, and contrast the structures revealed by clustering using words versus clustering using the choices implicit in systemic functional nets. We then show that the systemic functional net derived from the magazines is consistent with the structure present in two Islamist forums, and therefore reveals two different mindsets, one that is political and another that is religious, that seem widely held within the relevant communities.	cluster analysis;isis	Nasser Alsadhan;David B. Skillicorn	2016	2016 IEEE Conference on Intelligence and Security Informatics (ISI)	10.1109/ISI.2016.7745459	computer science;electrical engineering;artificial intelligence;machine learning;data mining;cluster analysis;matrix decomposition;computer security;artificial neural network	Comp.	-20.324373672393463	-67.61736047241638	10313
15ef19d626576c1772708607facaf677ffeff1fc	exemplar-based pitch accent categorisation using the generalized context model		This paper presents the results of a pitch accent categorisation simulation which attempts to classify L*H and H*L accents using a psychologically motivated exemplar-theoretic model of categorisation. Pitch accents are represented in terms of six linguistically meaningful parameters describing their shape. No additional information is employed in the categorisation process. The results indicate that these accents can be successfully categorised, via exemplar-based comparison, using a limited number of purely tonal features.	categorization;simulation;theory	Michael Walsh;Katrin Schweitzer;Nadja Schauffler	2013			speech recognition;pattern recognition;computer science;natural language processing;artificial intelligence;pitch accent;context model;exemplar theory	AI	-13.85753202489802	-82.73982724954635	10363
84dcf2edfec7597439847d12c26ca88a9cae0633	search and retrieval experiments in real-time information retrieval	search and retrieval;information retrieval;real time;technical report;computer science	Future operating document retrieval systems may be based on fully-automatic information analysis methods instead of manual indexing, and on real-time search procedures which allow the user to interact with the system during the search process. Performance characteristics are first given for fully-automatic information retrieval systems, and comparisons are made with presently operating partly-manual systems. Thereafter, various user-controlled search strategies are described, and the potential of these strategies in improving systems performance is discussed. The evaluation results for the real-time retrieval procedures are used to derive design criteria for future automatic information systems.	document retrieval;experiment;information retrieval;information system;real-time data;real-time locating system;real-time transcription;real-time web	Gerard Salton	1968			document retrieval;query expansion;relevance;image retrieval;concept search;adversarial information retrieval;retrievability;data retrieval;information retrieval;search engine;human–computer information retrieval	Web+IR	-34.44594185947363	-56.56484334009758	10391
38b12100ca810eb142aa8d6f74c5ace211465ac2	multilingual language identification: altw 2010 shared task data		While there has traditionally been strong interest in the task of monolingual language identification, research on multilingual language identification is underrepresented in the literature, partly due to a lack of standardised datasets. This paper describes an artificially-generated dataset for multilingual language identification, as used in the 2010 Australasian Language Technology Workshop shared task.	australasian conference on information systems;baseline (configuration management);benchmark (computing);language identification;language technology;the australian	Timothy Baldwin;Marco Lui	2010			natural language processing;language identification;speech recognition;computer science;language industry;linguistics	NLP	-24.545411234951388	-82.64790289222917	10392
067dd63a291caa7901f723ba56d1a8b8d1b4876e	extraction of proper names from myanmar text using latent dirichlet allocation	resource management;data mining;hidden markov models;large scale integration;feature extraction;dictionaries;organizations	This paper proposes a method for proper names extraction from Myanmar text by using latent Dirichlet allocation (LDA). Our method aims to extract proper names that provide important information on the contents of Myanmar text. Our method consists of two steps. In the first step, we extract topic words from Myanmar news articles by using LDA. In the second step, we make a post-processing, because the resulting topic words contain some noisy words. Our post-processing, first of all, eliminates the topic words whose prefixes are Myanmar digits and suffixes are noun and verb particles. We then remove the duplicate words and discard the topic words that are contained in the existing dictionary. Consequently, we obtain the words as candidate of proper names, namely personal names, geographical names, unique object names, organization names, single event names, and so on. The evaluation is performed both from the subjective and quantitative perspectives. From the subjective perspective, we compare the accuracy of proper names extracted by our method with those extracted by latent semantic indexing (LSI) and rule-based method. It is shown that both LS] and our method can improve the accuracy of those obtained by rule-based method. However, our method can provide more interesting proper names than LSI. From the quantitative perspective, we use the extracted proper names as additional features in K-means clustering. The experimental results show that the document clusters given by our method are better than those given by LSI and rule-based method in precision, recall and F-score.	cluster analysis;dictionary;f1 score;k-means clustering;latent dirichlet allocation;latent semantic analysis;least squares;logic programming;newton's method;video post-processing	Yuzana Win;Tomonari Masada	2016	2016 Conference on Technologies and Applications of Artificial Intelligence (TAAI)	10.1109/TAAI.2016.7880176	speech recognition;computer science;pattern recognition;data mining	AI	-23.944324842773888	-65.88976823118342	10393
12ba9d55a5a06112635fb70f48bc2e8c52aa5672	an architecture for effective music information retrieval	busqueda informacion;music retrieval;business information management incl records;and intelligence;information retrieval;musica;dynamic program;music similarity;musique;recherche information;music information retrieval;knowledge and information management;string matching;music;test collection	We have explored methods for music information retrieval for polyphonic music stored in the MIDI format. These methods use a query, expressed as a series of notes that are intended to represent a melody or theme, to identify similar pieces. Our work has shown that a three-phase architecture is appropriate for this task, in which the first phase is melody extraction, the second is standardisation, and the third is query-to-melody matching. We have investigated and systematically compared algorithms for each of these phases. To ensure that our results are robust, we have applied methodologies that are derived from text information retrieval: we developed test collections and compared different ways of acquiring test queries and relevance judgements. In this paper we review this program of work, compare to other approaches to music information retrieval, and identify outstanding issues.	algorithm;approximate string matching;information retrieval;midi;matching (graph theory);n-gram;relevance;string searching algorithm;the australian	Alexandra L. Uitdenbogerd;Justin Zobel	2004	JASIST	10.1002/asi.20057	relevance;cognitive models of information retrieval;computer science;music;data mining;multimedia;pop music automation;world wide web;information retrieval;human–computer information retrieval	Web+IR	-34.762529767846736	-62.68002720885909	10404
b14815f331e762b28e26f43f7642d0b780caad39	cepstral domain modification of audio signals for data embedding: preliminary results	databases;quantization;base donnee;frecuencia audible;frequence audible;signal audio;bit error rate;data embedding;audio signal;cepstre;database;masquage perceptif;base dato;vocal tract;spectrum;taux erreur bit;cepstrum;analyse spectrale;audiofrequency;signal acoustique;analisis espectral;acoustic signal;spectral analysis;tasa error bit;audio acoustics;senal acustica;senal audio;acoustique audio	A method of embedding data in an audio signal using cepstral domain modification is described. Based on successful embedding in the spectral points of perceptually masked regions in each frame of speech, first the technique was extended to embedding in the log spectral domain. This extension resulted at approximately 62 bits /s of embedding with less than 2 percent of bit error rate (BER) for a clean cover speech (from the TIMIT database), and about 2.5 percent for a noisy speech (from an air traffic controller database), when all frames – including silence and transition between voiced and unvoiced segments – were used. Bit error rate increased significantly when the log spectrum in the vicinity of a formant was modified. In the next procedure, embedding by altering the mean cepstral values of two ranges of indices was studied. Tests on both a noisy utterance and a clean utterance indicated barely noticeable perceptual change in speech quality when lower range of cepstral indices – corresponding to vocal tract region – was modified in accordance with data. With an embedding capacity of approximately 62 bits /s – using one bit per each frame regardless of frame energy or type of speech – initial results showed a BER of less than 1.5 percent for a payload capacity of 208 embedded bits using the clean cover speech. BER of less than 1.3 percent resulted for the noisy host with a capacity was 316 bits. When the cepstrum was modified in the region of excitation, BER increased to over 10 percent. With quantization causing no significant problem, the technique warrants further studies with different cepstral ranges and sizes. Pitch-synchronous cepstrum modification, for example, may be more robust to attacks. In addition, cepstrum modification in regions of speech that are perceptually masked – analogous to embedding in frequency masked regions – may yield imperceptible stego audio with low BER.	bit error rate;cepstrum;embedded system;steganography;timit;tract (literature);x.690	Kaliappan Gopalan	2004		10.1117/12.525800	speech recognition;acoustics;mathematics;communication	ML	-8.929578421106166	-89.32133847023898	10468
10743d3de8fb42f50d209fc0ac7678de58f81327	generalized context modeling with multi-directional structuring and mdl-based model selection for heterogeneous data compression	data compression;model selection context modeling heterogeneous data compression minimum description length model redundancy;maximum likelihood estimation;context modeling context predictive models data models data compression adaptation models maximum likelihood estimation;predictive models;adaptation models;context modeling;context;data models	This paper proposes generalized context modeling (GCM) for heterogeneous data compression. The proposed model extends the suffix of predicted subsequences in classic context modeling to arbitrary combinations of symbols in multiple directions. To address the selection of contexts, GCM constructs a model graph with a combinatorial structuring of finite order combination of predicted symbols as its nodes. The estimated probability for prediction is obtained by weighting over a class of context models that contain all the occurrences of nodes in the model graph. Moreover, separable context modeling in each direction is adopted for efficient prediction. To find optimal class of context models for prediction, the normalized maximum likelihood (NML) function is developed to estimate their structures and parameters, especially for heterogeneous data with large sizes. Furthermore, it is refined by context pruning to exclude the redundant models. Such model selection is optimal in the sense of minimum description length (MDL) principle, whose divergence is proven to be consistent with the actual distribution. It is shown that upper bounds of model redundancy for GCM are irrelevant to the size of data. GCM is validated in an extensive field of applications, e.g., Calgary corpus, executable files, and genomic data. Experimental results show that it outperforms most state-of-the-art context modeling algorithms reported.	algorithm;calgary corpus;data compression;google cloud messaging;mdl (programming language);minimum description length;model selection;relevance	Wenrui Dai;Hongkai Xiong;Jia Wang;Samuel Cheng;Yuan F. Zheng	2015	IEEE Transactions on Signal Processing	10.1109/TSP.2015.2458784	data compression;data modeling;computer science;machine learning;pattern recognition;data mining;mathematics;predictive modelling;maximum likelihood;context model;statistics	ML	-22.693606160870242	-92.34566891287655	10473
dbf3ae3555807c207e714749b44a39968e955e6b	sam: a system for iteratively building marker maps	dna;computadora;computer program;chromosome g22;langage c;computerized processing;tratamiento informatico;cromosoma g22;ordinateur;hombre;computer;genetic mapping;genetic marker;marqueur genetique;algorithme;algorithm;c language;human;carte genetique;computer aid;asistencia ordenador;mapa genetico;programa computador;traitement informatique;marcador genetico;assistance ordinateur;programme ordinateur;lenguaje c;homme;algoritmo	SAM (system for assembling markers) is a system which supports man-machine problem solving for iteratively ordering a set of markers. SAM aids the user in partially ordering a set of markers based on incomplete and uncertain data. As data is added and modified, SAM aids the user in updating the previously assembled maps. The input is a file of clones and for each clone, a list of the markers contained within it. The objective is to order the set of markers such that the markers contained in each clone are consecutive. The user directs the map building by selecting functions to assemble a region of markers, order the clones to fit the order of the markers and position new markers within an ordered set of markers. The user can edit the input data, edit the assembled map and add clones to the map based on their marker content. The results are displayed graphically and can be saved in a solution file. Based on the partial map, the user designs new experiments or edits the existing data to fill gaps and resolve ambiguities. When a previously assembled map is loaded into SAM, it is automatically updated with the new or altered data. SAM treats all markers as points, but has special features for multiple copy and long markers so that they can be used in the map building process. This system has supported the building of a YAC map of human chromosome 22 at the Sanger Centre, where use of Alu-PCR product markers is a major component in determining clone overlap and where we have an on-going effort to accumulate data from various sources. SAM is also being used at various other laboratories.		Carol Soderlund;I. Dunham	1995	Computer applications in the biosciences : CABIOS	10.1093/bioinformatics/11.6.645	biology;gene mapping;artificial intelligence;genetic marker;genetics;dna;algorithm	Graphics	-4.927892141595228	-56.498819480345944	10481
49f1422d1c2e2dcc11d3f90f0ed25d3125f42f1a	using centroids of word embeddings and word mover's distance for biomedical document retrieval in question answering		We propose a document retrieval method for question answering that represents documents and questions as weighted centroids of word embeddings and reranks the retrieved documents with a relaxation of Word Mover’s Distance. Using biomedical questions and documents from BIOASQ, we show that our method is competitive with PUBMED. With a top-k approximation, our method is fast, and easily portable to other domains and languages.	approximation;document retrieval;linear programming relaxation;ontology (information science);pubmed;query expansion;question answering;textual entailment;tf–idf;word embedding	Georgios-Ioannis Brokos;Prodromos Malakasiotis;Ion Androutsopoulos	2016		10.18653/v1/W16-2915	natural language processing;pattern recognition;information retrieval	NLP	-27.025899362318807	-67.56958255934578	10492
37150d14294a76ad06f2c20426efba1ea124daf8	adaptation of hidden markov models using multiple stochastic transformations	linear transformation;hidden markov model	The recognition accuracy in recent large vocabulary Automatic Speech Recognition (ASR) systems is highly related to the existing mismatch between the training and test sets. For example, dialect di erences across the training and testing speakers result to a signi cant degradation in recognition performance. Some popular adaptation approaches improve the recognition performance of speech recognizers based on hidden Markov models with continuous mixture densities by using linear transforms to adapt the means, and possibly the covariances of the mixture Gaussians. In this paper, we propose a novel adaptation technique that adapts the means and, optionally, the covariances of the mixture Gaussians by using multiple stochastic transformations. We perform both speaker and dialect adaptation experiments, and we show that our method signi cantly improves the recognition accuracy and the robustness of our system. The experiments are carried out with SRI's DECIPHER speech recognition system.	decipher;elegant degradation;experiment;finite-state machine;hidden markov model;markov chain;speech recognition;vocabulary	Vassilios Diakoloukas;Vassilios Digalakis	1997			artificial intelligence;viterbi algorithm;pattern recognition;markov algorithm;hidden semi-markov model;hidden markov model;forward algorithm;markov model;maximum-entropy markov model;computer science;variable-order markov model	ML	-18.258180168146524	-91.90384345951298	10493
06173806d8606cfd8efaee24543b6abc55a616ae	creation of a japanese adverb dictionary that includes information on the speakers communicative intention using machine learning.		Japanese adverbs are classified as either declarative or normal; the former declare the communicative intention of the speaker, while the latter convey a manner of action, a quantity, or a degree by which the adverb modifies the verb or adjective that it accompanies. We have automatically classified adverbs as either declarative or not declarative using a machine-learning method such as the maximum entropy method. We defined adverbs having positive or negative connotations as the positive data. We classified adverbs in the EDR dictionary and IPADIC used by Chasen using this result and built an adverb dictionary that contains descriptions of the communicative intentions of the speaker.	bluetooth;chasen;declarative programming;degree (graph theory);dictionary;machine learning;principle of maximum entropy	Toshiyuki Kanamaru;Masaki Murata;Hitoshi Isahara	2006			natural language processing;speech recognition;artificial intelligence;computer science;adverb	NLP	-14.778700868102238	-78.69503190095692	10494
1eaae4452a07f6e7d9bebae64076b2b842916a80	prosody generation using frame-based gaussian process regression and classification for statistical parametric speech synthesis	hidden markov models ground penetrating radar speech synthesis kernel speech context context modeling;kernel;speech synthesis;speech synthesis gaussian processes regression analysis;speech;ground penetrating radar;hidden markov models;target frame prosody generation frame based gaussian process regression statistical parametric speech synthesis phone durations gaussian process classification spectral feature modeling kernel function phonetic information accent phrases temporal acoustic accent phrase;kernel function statistical parametric speech synthesis prosody gaussian process regression gaussian process classification;context modeling;context	This paper proposes novel models of F0 contours and phone durations using Gaussian process regression and classification (GPR and GPC) for statistical parametric speech synthesis. Although the use of frame-based GPR has shown the effectiveness of spectral feature modeling in previous studies, the application of GPR to prosodic features, i.e., F0 and phone duration, was not investigated sufficiently because the kernel function was designed for phonetic information only. In this paper, therefore, we propose a kernel function available for multiple units such as syllables, moras, and accent phrases. The proposed kernel function is based on temporal acoustic events like the beginning of accent phrase and the relative position between the target frame and the event is utilized for the kernel function. Experimental results of objective and subjective tests show that the GPR/GPC-based F0 and duration modeling improves the prediction accuracy of acoustic features compared with HMM-based speech synthesis.	acoustic cryptanalysis;computer;gaussian process;hidden markov model;kriging;semantic prosody;speech synthesis	Tomoki Koriyama;Takao Kobayashi	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7178908	natural language processing;kernel;speech recognition;ground-penetrating radar;computer science;speech;pattern recognition;context model;speech synthesis	Robotics	-17.309829731872433	-85.38820255501734	10500
8dcb4665cfdecbabba8114a61b627e6657cebd8c	benchmarking the extraction and disambiguation of named entities on the semantic web	eurecom ecole d ingenieur telecommunication centre de recherche graduate school research center communication systems;named entity recognition named entity linking machine learning newswire microposts	Named entity recognition and disambiguation are of primary importance for extracting information and for populating knowledge bases. Detecting and classifying named entities has traditionally been taken on by the natural language processing community, whilst linking of entities to external resources, such as those in DBpedia, has been tackled by the Semantic Web community. As these tasks are treated in different communities, there is as yet no oversight on the performance of these tasks combined. We present an approach that combines the state-of-the art from named entity recognition in the natural language processing domain and named entity linking from the semantic web community. We report on experiments and results to gain more insights into the strengths and limitations of current approaches on these tasks. Our approach relies on the numerous web extractors supported by the NERD framework, which we combine with a machine learning algorithm to optimize recognition and linking of named entities. We test our approach on four standard data sets that are composed of two diverse text types, namely newswire and microposts.	algorithm;dbpedia;entity linking;experiment;knowledge base;machine learning;named entity;natural language processing;population;semantic web;while;word-sense disambiguation	Guiseppe Rizzo;Marieke van Erp;Raphaël Troncy	2014			natural language processing;computer science;data science;data mining	NLP	-26.06773368155832	-68.73322292298484	10531
3418b8ab66cb194371e78d3aae50fce5f0611fa2	comprehensive analysis of clinical information flow in an integrated delivery network	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;uk phd theses thesis;life sciences;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	Abstract#R##N#In developing a Regional Electronic Health Record (EHR) Strategy, Calgary Health Region identified a significant gap in evidence to guide decisions regarding the investments, policies, and infrastructure required to support clinical information flow in an integrated delivery network. The Clinical Informatics group conducted a comprehensive analysis to identify the clinical priority of individual data elements in a cross-continuum EHR, followed by a thorough study of the referral and discharge processes, and the attributes of effective referral notes and discharge summaries.	information flow	Jeremy Smith;Tom Rosenal;Lucy Reyes;Adrienne Cousins	2002			library science;medicine;data mining	HPC	-56.89487823142549	-66.08076499919659	10533
2d883f6da2bbbe77b00ca0f904294ca3ebea76f9	i-ring: a system for humming transcription and chord generation	signal processing;music;application software;dynamic programming;audio signal processing;speech processing;probability;computer science;multiple signal classification;statistics	This paper describes the construction of a system called i-Ring that can generate a polyphonic ringtone based on a user's humming input. Algorithms used in the system for music transcription and chord generation include various techniques in speech and music signal processing, such as pitch tracking and dynamic programming, which are explained in the paper. Experimental results demonstrate the feasibility of i-Ring as a handy tool for computer assisted music composition for polyphonic ringtones	dynamic programming;handy board;pitch detection algorithm;signal processing;transcription (software)	Hong-Ru Lee;Jyh-Shing Roger Jang	2004	2004 IEEE International Conference on Multimedia and Expo (ICME) (IEEE Cat. No.04TH8763)		application software;speech recognition;audio signal processing;computer science;multiple signal classification;dynamic programming;signal processing;probability;music;speech processing;statistics	Robotics	-7.54099050485719	-92.80379236679445	10565
62e6a8311fb1fb1c0291a74583349d218515c405	aggressive morphology and lexical relations for query expansion	elargissement question;information retrieval;cuestion respuesta;performance;automatisation;automatizacion;recherche information;question reponse;evaluation;recuperacion informacion;evaluacion;rendimiento;query expansion;question answering;automation	Our submission to TREC this year is based on a combination of systems. The first is the conceptual indexing and retrieval system that was developed at Sun Microsystems Laboratories (Woods et al., 2000a; Woods et al., 2000b). The second is the MultiText system developed at the University of Waterloo (Clarke et al., 2000; Cormack et al., 2000). The conceptual indexing system was designed to help people find specific answers to specific questions in unrestricted text. It uses a combination of syntactic, semantic, and morphological knowledge, together with taxonomic subsumption techniques, to address differences in terminology between a user’s queries and the material that may answer them. At indexing time, the system builds a conceptual taxonomy of all the words and phrases in the indexed material. This taxonomy is based on the morphological structure of words, the syntactic structure of phrases, and semantic relations between meanings of words that it knows in its lexicon. It was not, however, designed as a question answering system. Our results from last year, while encouraging, showed that we needed more work in the area of question analysis (i.e., “What would constitute an answer to this question?”) and answer determination (i.e., “Does this retrieved passage actually answer the question?”) to support our relaxation ranking passage retrieval algorithm. After conversations with the researchers at the University of Waterloo, we decided to submit a run where we would provide front-end processing consisting of query formulation and query expansion using our automatically derived taxonomy and Waterloo would provide the back-end processing via their MultiText passage retrieval system and their answer selection component. The result is a direct comparison of two question answering systems that differ only in the query formulation component.	algorithm;answer set programming;edmund m. clarke;elegant degradation;lexicon;linear programming relaxation;mathematical morphology;query expansion;question answering;refinement (computing);subsumption architecture;text retrieval conference	William A. Woods;Stephen J. Green;Paul Alan Martin;Ann Houston	2001			natural language processing;query expansion;question answering;performance;computer science;evaluation;automation;world wide web;information retrieval	NLP	-34.91821202487735	-63.599972195381106	10588
36250147569cfabae149995fe159052de374a366	on the importance of parameter tuning in text categorization	document representation;parameter tuning;winnow;rocchio;automatic classification;text categorization	Text Categorization algorithms have a large number of parameters that determine their behaviour, whose effect is not easily predicted objectively or intuitively and may very well depend on the corpus or on the document representation. Their values are usually taken over from previously published results. In this article we investigate the effect of parameter tuning on the accuracy of two Text Categorization algorithms: the well-known Rocchio algorithm and the lesser-known Winnow. It turns out that the optimal parameter values are sometimes very different from those found in literature. We show that parameter tuning can greatly improve the accuracy of both algorithms, much more so than Term Selection. We also show that a non-Euclidian variant of the Vector Space Model may improve the accuracy but that at optimal parameter values this effect practically disappears.	categorization;document classification;ising model;mike lesser;rocchio algorithm;whole earth 'lectronic link;winnow (algorithm)	Cornelis H. A. Koster;Jean Beney	2006		10.1007/978-3-540-70881-0_24	winnow;computer science;machine learning;pattern recognition;information retrieval	Web+IR	-22.205699839298727	-65.67991881323839	10592
906dcda9128e2132dc5d301790a0cf176cbb6631	lexical triggers and latent semantic analysis for cross-lingual language model adaptation	word error rate;automatic speech recognition;lexical trigger;statistical language modeling;language model adaptation;target language;speech recognition;multilingual processing;statistical language model;latent semantic analysis;language model	In-domain texts for estimating statistical language models are not easily found for most languages of the world. We present two techniques to take advantage of in-domain text resources in other languages. First, we extend the notion of <i>lexical triggers</i>, which have been used monolingually for language model adaptation, to the cross-lingual problem, permitting the construction of sharper language models for a target-language document by drawing statistics from related documents in a resource-rich language. Next, we show that <i>cross-lingual latent semantic analysis</i> is similarly capable of extracting useful statistics for language modeling. Neither technique requires explicit translation capabilities between the two languages! We demonstrate significant reductions in both perplexity and word error rate on a Mandarin speech recognition task by using these techniques.	expect;experiment;formal language;language model;latent semantic analysis;mutual information;perplexity;principle of maximum entropy;speech recognition;super robot monkey team hyperforce go!;test data;text corpus;word error rate	Woosung Kim;Sanjeev Khudanpur	2004	ACM Trans. Asian Lang. Inf. Process.	10.1145/1034780.1034782	natural language processing;language identification;cache language model;speech recognition;universal networking language;latent semantic analysis;object language;factored language model;word error rate;language construct;computer science;computational linguistics;language transfer;linguistics;modeling language;natural language;probabilistic latent semantic analysis;language model	NLP	-21.852267232421063	-79.83107714720855	10596
3e2e0c3fff7ccf230a6eda3032d97a1350d71efe	combining cohort and ubm models in open set speaker identification	cohort;speaker identification;support vector machines;training;single global model;universal background;speaker verification;speaker recognition;training data;accuracy;computational modeling;cepstral analysis;cohort model;machine learning;indexing;ubm;support vector machines speaker recognition;support vector machine classification;support vector machines training data machine learning support vector machine classification indexing speaker recognition cepstral analysis computer science educational institutions sampling methods;computer science;support vector machine;ubm speaker identification speaker verification cohort;sampling methods;ubm model;buildings;decision rule;decision rule cohort model ubm model open set speaker identification single global model universal background support vector machine;data models;open set speaker identification	In open set speaker identification it is important to build an alternative model against which to compare scores from the 'target' speaker model. Two alternative strategies for building an alternative model are to build a single global model by sampling from a pool of training data, the Universal Background  (UBM), or to build a cohort of models from selected individuals in the training data for the target speaker. The main contribution in this paper is to show that these approaches can be unified by using a Support Vector Machine (SVM) to learn a decision rule in the score space made up of the output scores of the client, cohort and UBM model.	decision boundary;sampling (signal processing);speaker recognition;support vector machine	Anthony Brew;Padraig Cunningham	2009	2009 Seventh International Workshop on Content-Based Multimedia Indexing	10.1109/CBMI.2009.30	speaker recognition;support vector machine;speech recognition;computer science;machine learning;pattern recognition	ML	-15.839861532425624	-94.50550716424813	10602
f4a4990b25e508c66fd83ab786e2974757bdb045	the color between two others	color appearance;difference set	A psychophysical experiment was performed to determine the color that had the appearance halfway between two other colors with equal chroma and separated by either 40o or 60o of hue angle. Four color centers were used and a QUEST threshold procedure was employed to find the midpoint between two flanking colors. Rather than choosing colors that shared the same chroma attribute as the flanking color pair, observers chose colors that had less chroma and were closer to the midpoint of the line connecting the flanking pair in Cartesian coordinates. In a separate control experiment using the same stimulus configuration but a different set of instructions, subjects determined the color between the flanking colors that had the same chroma. In this experiment, subjects were able to choose the color with the same chroma as the flanks. The results of this experiment are discussed in relation to the usefulness of defining color appearance attributes.	chroma subsampling;color	Ethan D. Montag	2003			color model;lightness;computer science;color difference;brightness;difference set;statistics	ML	-42.13542125266246	-52.529883704438724	10616
42605dca59a3aafe2e5b33741a98dad9ba117395	improving graph-walk-based similarity with reranking: case studies for personal information management	random graph;similarity metric;personal information management;name disambiguation;learning;graph walk;semistructured data;pim;high performance	Relational or semistructured data is naturally represented by a graph, where nodes denote entities and directed typed edges represent the relations between them. Such graphs are heterogeneous, describing different types of objects and links. We represent personal information as a graph that includes messages, terms, persons, dates, and other object types, and relations like sent-to and has-term. Given the graph, we apply finite random graph walks to induce a measure of entity similarity, which can be viewed as a tool for performing search in the graph. Experiments conducted using personal email collections derived from the Enron corpus and other corpora show how the different tasks of alias finding, threading, and person name disambiguation can be all addressed as search queries in this framework, where the graph-walk-based similarity metric is preferable to alternative approaches, and further improvements are achieved with learning. While researchers have suggested to tune edge weight parameters to optimize the graph walk performance per task, we apply reranking to improve the graph walk results, using features that describe high-level information such as the paths traversed in the walk. High performance, together with practical runtimes, suggest that the described framework is a useful search system in the PIM domain, as well as in other semistructured domains.	email;enron corpus;entity;high- and low-level;object type (object-oriented programming);personal information management;personally identifiable information;random graph;runtime system;text corpus;thread (computing);web search query;word-sense disambiguation	Einat Minkov;William W. Cohen	2010	ACM Trans. Inf. Syst.	10.1145/1877766.1877770	random graph;null model;computer science;artificial intelligence;theoretical computer science;machine learning;personal information management;data mining;database;graph;world wide web;personal information manager;graph database;information retrieval	Web+IR	-27.77962999119434	-61.73775338872639	10657
eae10d4c8ab7d68f4c0aafc01378824ca1f99d6f	a neural network classifier for junk e-mail	bayes estimation;document structure;commercial use;largeur bande;interes comercial;bayesian classifier;keyword;electronic mail;decision tree;analisis datos;bayesian approach;estructura documental;structure document;rule based;serveur informatique;hombre;palabra clave;correo electronico;neural network classifier;linear discriminate analysis;mot cle;classification;or phrases;courrier electronique;captador medida;refinement method;online community;data analysis;estimacion bayes;test preliminaire;measurement sensor;capteur mesure;spam filtering;interet commercial;preliminary test;filter;anchura banda;human;filtre;bandwidth;servidor informatico;analyse donnee;test preliminar;support vector machine;methode raffinement;reseau neuronal;metodo afinamiento;filtro;clasificacion;red neuronal;estimation bayes;homme;neural network;open source;computer server	Most e-mail readers spend a non-trivial amount of time regularly deleting junk e-mail (spam) messages, even as an expanding volume of such e-mail occupies server storage space and consumes network bandwidth. An ongoing challenge, therefore, rests within the development and refinement of automatic classifiers that can distinguish legitimate e-mail from spam. A few published studies have examined spam detectors using Naïve Bayesian approaches and large feature sets of binary attributes that determine the existence of common keywords in spam, and many commercial applications also use Naïve Bayesian techniques. Spammers recognize these attempts to thwart their messages and have developed tactics to circumvent these filters, but these evasive tactics are themselves patterns that human readers can often identify quickly. Therefore, in contrast to earlier approaches, our feature set uses descriptive characteristics of words and messages similar to those that a human reader would use to identify spam. This preliminary study tests this alternative approach using a neural network (NN) classifier on a corpus of e-mail messages from one user. The results of this study are compared to previous spam detectors that have used Naïve Bayesian classifiers. Also, it appears that commercial spam detectors are now beginning to use descriptive features as proposed here.	algorithm;anti-spam techniques;artificial neural network;bayesian network;bounce address;email filtering;naive bayes classifier;naivety;refinement (computing);sensor;server (computing);spamming;vocabulary;whitelist	Ian Stuart;Sung-Hyuk Cha;Charles C. Tappert	2004		10.1007/978-3-540-28640-0_42	rule-based system;support vector machine;naive bayes classifier;speech recognition;biological classification;filter;bayesian probability;computer science;artificial intelligence;document structure description;machine learning;decision tree;data mining;data analysis;artificial neural network;bandwidth;server	Security	-35.80670312078529	-56.804013415037744	10675
10f3268df0bb1b7ca9e5d096b4847c85735e919e	in search of common ground in handoff documentation in an intensive care unit	handoff;intensive care units;patient transfer;continuity of patient care;humans;nurses;patient centered;communication;documentation	OBJECTIVE Handoff is an intra-disciplinary process, yet the flow of critical handoff information spans multiple disciplines. Understanding this information flow is important for the development of computer-based tools that supports the communication and coordination of patient care in a multi-disciplinary and highly specialized critical care setting. We aimed to understand the structure, functionality, and content of nurses' and physicians' handoff artifacts.   DESIGN We analyzed 22 nurses' and physicians' handoff artifacts from a Cardiothoracic Intensive Care Unit (CTICU) at a large urban medical center. We combined artifact analysis with semantic coding based on our published Interdisciplinary Handoff Information Coding (IHIC) framework for a novel two-step data analysis approach.   RESULTS We found a high degree of structure and overlap in the content of nursing and physician artifacts. Our findings demonstrated a non-technical, yet sophisticated, system with a high degree of structure for the organization and communication of patient data that functions to coordinate the work of multiple disciplines in a highly specialized unit of patient care.   LIMITATIONS This study took place in one CTICU. Further work is needed to determine the generalizability of the results.   CONCLUSIONS Our findings indicate that the development of semi-structured patient-centered interdisciplinary handoff tools with discipline specific views customized for specialty settings may effectively support handoff communication and patient safety.	care of intensive care unit patient;coronary care units;documentation;morphologic artifacts;numerous;occur (action);patients;scientific publication;semiconductor industry	Sarah A. Collins;Lena Mamykina;Desmond A. Jordan;Daniel M. Stein;Alisabeth Shine;Paul A Reyfman;David R. Kaufman	2012	Journal of biomedical informatics	10.1016/j.jbi.2011.11.007	medicine;documentation;computer science;knowledge management;nursing	HCI	-52.69087349064734	-65.57441885963387	10678
f24b1fa42fd17dff5f06ee91f2c6116058827ea9	arabic named entity recognition: a bidirectional gru-crf approach		The previous Named Entity Recognition (NER) models for Modern Standard Arabic (MSA) rely heavily on the use of features and gazetteers, which is time consuming. In this paper, we introduce a novel neural network architecture based on bidirectional Gated Recurrent Unit (GRU) combined with Conditional Random Fields (CRF). Our neural network uses minimal features: pretrained word representations learned from unannotated corpora and also character-level embeddings of words. This novel architecture allowed us to eliminate the need for most of handcrafted engineering features. We evaluate our system on a publicly available dataset where we were able to achieve comparable results to previous best-performing systems.	bidirectional associative memory;conditional random field;named-entity recognition	Mourad Gridach;Hatem Haddad	2017		10.1007/978-3-319-77113-7_21	modern standard arabic;architecture;natural language processing;arabic;artificial intelligence;artificial neural network;computer science;conditional random field;named-entity recognition	NLP	-18.921271106142875	-73.11231507858999	10681
f675fc7f6c837f373817feb4df17506f0ed54e59	from where to what: metadata sharing for digital photographs with geographic coordinates	busqueda informacion;utilisation information;uso informacion;text;metadata;information use;fotografia digital;information retrieval;digital libraries;localization;photographie numerique;localizacion;texte;system performance;digital photography;user cooperation;localisation;recherche information;metadonnee;metadatos;information system;texto;systeme information;sistema informacion	We describe LOCALE, a system that allows cooperating information systems to share labels for photographs. Participating photographs are enhanced with a geographic location stamp – the latitude and longitude where the photograph was taken. For a photograph with no label, LOCALE can use the shared information to assign a label based on other photographs that were taken in the same area. LOCALE thus allows (i) text search over unlabeled sets of photos, and (ii) automated label suggestions for unlabeled photos. We have implemented a LOCALE prototype where users cooperate in submitting labels and locations, enhancing search quality for all users in the system. We ran an experiment to test the system in centralized and distributed settings. The results show that the system performs search tasks with surprising accuracy, even when searching for specific landmarks.	basic stamp;centralized computing;emoticon;future search;geographic coordinate system;geographic information system;image retrieval;landmark point;multi-user;prototype;search algorithm;semiconductor industry	Mor Naaman;Andreas Paepcke;Hector Garcia-Molina	2003		10.1007/978-3-540-39964-3_14	digital photography;digital library;internationalization and localization;computer science;database;computer performance;multimedia;metadata;world wide web;information system	HCI	-32.83326312421417	-55.554491057765375	10697
96ba71db35a1f3b0816a212ab5d34d2735f8d33a	designing sound: towards a system for designing audio interfaces using timbre spaces	principal component analysis;fourier analysis	The creation of audio interfaces is currently hampered by the difficulty of designing sounds for them. This paper presents a novel system for generating and manipulating non-speech sounds. The system is designed to generate Auditory Icons and Earcons through a common interface. Using a timbre space representation of the sound, it generates output via an FM synthesiser. The timbre space has been compiled in both Fourier and Constant Q Transform versions using Principal Components Analysis (PCA). The design of the system and initial evaluations of these two versions are discussed, showing that the Fourier analysis appears to produce higher quality results, contrary to initial expectations.	auditory processing disorder;compiler;fm broadcasting;fourier analysis;principal component analysis;sound card;spaces	Craig Nicol;Stephen A. Brewster;Philip D. Gray	2004			fourier transform;computer science;constant q transform;principal component analysis;fourier analysis;speech recognition;timbre	HCI	-7.007409449134197	-86.63228977466747	10701
5cd1f0b9dc8ce23d1f79f7c41619c149e0e343ce	building a chinese shallow parsed treebank for collocation extraction	traduccion automatica;methode collocation;metodo colocacion;chino;large scale;analyse syntaxique;traduction automatique;analisis sintaxico;syntactic analysis;palabra;controle qualite;analizador sintaxico;word;parser;collocation method;quality control;chinois;chinese;analyseur syntaxique;shallow parsing;control calidad;mot;automatic translation	To automatically extract Chinese collocations and build a large-scale collocation bank, we are developing a one-million-word Chinese shallow parsed treebank. The treebank can be used not only as a training set for our shallow parser, but also as processed data from which collocations are extracted. This paper presents several issues related to this on-going project, such as our definition of shallow parsing used in Chinese collocation extraction, guideline preparation, and quality control.	chinese wall;collocation extraction;shallow parsing;test set;treebank	Baoli Li;Qin Lu;Li Yin	2003		10.1007/3-540-36456-0_41	natural language processing;quality control;speech recognition;computer science;treebank;parsing;collocation method;word;programming language;chinese	NLP	-27.362842974191583	-78.09614151939323	10717
8039784b5ed4f21b81e6414d466ccc05309b344d	automatic reading of aeronautical meteorological messages	data flow diagrams;aeronautical meteorological messages;natural language generation;interlingua;voice generation	This paper describes the architecture developed to produce an automatic reader of aeronautical meteorological messages. An interlingua has been used and a whole process of natural language generation has been implemented. The system Festival has been used with a modified voice to read the messages generated. The presented system is able to translate the meteorological messages into a natural language text and read it.	data flow diagram;dataflow;natural language generation;speech synthesis	Luis Delgado;Núria Castell	2007			speech recognition;telecommunications;computer science;communication	NLP	-29.076351291289644	-81.32863536260719	10732
26fc663688ba6304f006c1970ab9748dc475fbd0	from toys to brain: virtual reality applications in neuroscience	rehabilitation;virtual reality;neuroscience;virtual environment;neurosurgery;reference standard;assessment	While many virtual reality (VR) applications have emerged in the areas of entertainment, education, military training, physical rehabilitation, and medicine, only recently have some research projects begun to test the possibility of using virtual environments (VEs) for research in neuroscience, neurosurgery and for the study and rehabilitation of human cognitive and functional activities. Virtual reality technology could have a strong impact on neuroscience. The key characteristic of VEs is the high level of control of the interaction with the tool without the constraints usually found in computer systems. VEs are highly flexible and programmable. They enable the therapist to present a wide variety of controlled stimuli and to measure and monitor a wide variety of responses made by the user. However, at this stage, a number of obstacles exist which have impeded the development of active research. These obstacles include problems with acquiring funding for an almost untested new treatment modality, the lack of reference standards, the non-interoperability of the VR systems and, last but not least, the relative lack of familiarity with the technology on the part of researchers in these fields.	computer;high-level programming language;interoperability;modality (human–computer interaction);toys;virtual reality;web standards	Giuseppe Riva	1998	Virtual Reality	10.1007/BF01408706	simulation;neurosurgery;human–computer interaction;computer science;virtual machine;artificial intelligence;virtual reality;educational assessment	Visualization	-54.94214204842072	-55.49675160506838	10740
4c01f4a8e3f4ec9e8556a10b393d97b7c8140f27	streaming aspect-sentiment analysis	analytical models;convergence;data mining;sentiment analysis;inference algorithms;probabilistic logic;algorithm design and analysis	Sentiment analysis of online users has been attracting significant interests in both academics and industry, but is always challenging. In this paper, we propose an effective algorithm that can work with text streams and big text collections, without human supervision. This method is based on the state-of-the-art model, namely Aspect and Sentiment Unification Model (ASUM). Our method has several advantages compared to existing approaches: (i) it is a streaming algorithm which is able to deal with large or streaming data, with whom existing approaches cannot work; (ii) at the core of our algorithm is an inference method which has a theoretical guarantee about quality and convergence rate, whereas existing methods do not have; (iii) by further proposing to keep prior knowledge when learning, our method can do sentiment classification better. Extensive experiments on different review datasets show the effectiveness and efficiency of our algorithm in practice.	4000 series;experiment;han unification;random seed;rate of convergence;scalability;sentiment analysis;statistical classification;statistical model;streaming algorithm;streaming media	Vu Le Anh;Chien Phung Van;Cuong Vu Cao;Ngo Van Linh Khoat Than	2016	2016 IEEE RIVF International Conference on Computing & Communication Technologies, Research, Innovation, and Vision for the Future (RIVF)	10.1109/RIVF.2016.7800291	computer science;data science;machine learning;data mining	SE	-22.283928333660494	-57.946872557590495	10748
4fc5ccb41b4a8738a6ac6476b9c0979dc465da68	deriving appearance scales		The concept of color space has come to be an unquestioned threedimensional representation of color stimuli, or color appearance, intended to simplify the relationships among physically measurable attributes of light, mathematical formulae, and human sensations and perceptions. The notion of three-dimensional mathematical spaces as adjuncts for color is often helpful, but perhaps also misleading at times. Color appearance models requiring five or six dimensions to represent color appearance illustrate some of the limitations of historic spaces. This paper poses the question of whether color appearance would be better represented by independent appearance scales with no requirement that they be related as a higher-dimensional space. In other words, is color better represented by six one-dimensional color scales than one or two three-dimensional color spaces. A framework for implementing such appearance scales is described and one implementation is presented along with discussion of the ramifications for color difference metrics. Introduction Color scientists and engineers have become accustomed to the fundamental concept of color space to the point that the concept itself goes unquestioned. Much like most accept the fact that the earth is nearly spherical, those in the color-related fields proceed merrily along without a doubt that color space is three dimensional. Further, some continue to seek the holy grail of a three-dimensional color space in which perceived color differences can be expressed as uniform Euclidean distances despite an apparent lack of psychophysical evidence that such a space might exist. Perhaps it is time to, once again, step back and ask the question of whether the concept of a Euclidean distance metric in three dimensions really makes sense for describing color, even approximately. Perhaps some insight into appropriate descriptions of color appearance can be gained from a cursory examination of the other human senses.[1,2] Our perception of taste has at least five distinct dimensions, sweetness, bitterness, sourness, saltiness, and umami, and seldom does anyone speak of changes in taste perceptions as a Euclidean difference space. Similarly our sense of smell is served by something on the order of 1000 different receptor types. Some have tried to reduce the dimensionality to approximately six including flowery, foul, fruity, spicy, burnt, and resinous. Our sense of hearing is actually spectral (plus intensity) in terms familiar to color scientists as humans are able to detect frequencies within sounds (no aural metamerism) and the relative intensities of each frequency. Finally our sense of touch might well be too complex to even attempt to summarize in a sentence or two. None of the perceptions arising from any of these senses are commonly expressed in terms of multi-dimensional spaces with Euclidean (or similar) difference metrics. Given these similarities in our other senses, why should we think color is different? Is it the relatively low dimensionality? Is it the seemingly simple perceptual relationships such as color opponency? Is it the nature of additive color mixing? Additive color mixture under photopic conditions provides ample evidence for trichromacy, the three-dimensional nature of color matching/mixture. Adding Grassmann’s laws allows expression of color matches in various sets of primaries via simple 3x3 linear transformations analogous to a change of basis in a three-dimensional, linear space (where Euclidean distances mean something mathematically). Perhaps it is this property of color matching, which is not a direct representation of perception or appearance, that leads to an almost irresistible next step to start expressing color matches in three-dimensional Euclidean spaces. And then, apparently without clear justification, the concept is carried forward in attempts to express appearances and differences in similar three-dimensional Euclidean spaces such as the CIELAB color space. Perhaps those attempts were always as doomed as any explorers who might have set out to “circumnavigate” a flat earth. Color science is not devoid of examples typically described as color spaces that are actually descriptions of color perception one dimension at a time.[3] For example, the Munsell system, despite its common embodiments, was derived as a system of three independent perceptual dimensions, hue, value, and chroma. Similarly, Guth’s ATD model of visual perception was typically described in terms of independent dimensions, although the temptation to plot some of them together for some examples proved irresistible. Likewise, color appearance models such as CIECAM02 were developed with independent predictors of the six perceptual dimensions of brightness, lightness, colorfulness, saturation, chroma, and hue. This was somewhat compromised by requests for rectangular color space dimensions which appeared as CIECAM97s evolved to CIECAM02. However it should be noted that cylindrical representations of the appearance spaces were common even before the requests for rectangular coordinates. Lastly, the NCS system provides a useful example of hue being treated separately from whiteness-blackness and chromaticness. And while NCS whiteness-blackness and chromaticness are plotted in two-dimensional trilinear form, the dimensions are largely independent since the anchor of maximal chromaticness appropriately varies from hue to hue. All of this insight leads to the hypothesis that perhaps color space is actually a one-dimensional space, rather than a threedimensional space, and that Euclidean distance metrics might indeed be successful in such a space. Of course, color appearance cannot be properly described in a single one-dimensional space. Instead six of them are required. There are three fundamental appearance attributes for related colors, lightness, saturation, and hue. Combined with information about absolute luminance, colorfulness and brightness can be derived from these and are important and useful appearance attributes. Lastly, chroma can be derived from lightness and saturation if desired as an alternative relative colorfulness metric. Thus, color is rightfully and fully described with six one-dimensional appearance spaces (or scales), four of which are fundamental for related colors and two of which are derived from the fundamental scales. This paper provides some detail of the conceptual framework of a color model made up of one-dimensional spaces and an implementation of that framework for future application and investigation. Note: One-dimensional “spaces” are more commonly referred to as “scales” in color science, thus the term “scale” is used preferentially for the remainder of the paper. Conceptual Framework A set of color appearance scales (or dimensions, or spaces) following these principles has been derived and an implementation is presented in the next section. This section provides the general framework that could be easily adapted to different specific implementations of the concept. The first step is to apply a chromatic adaptation model to compute corresponding colors for reference viewing conditions (e.g. D65, 315 cd/m2). Then the IPT model, derived specifically for accurate hue representations, is used to compute a hue angle (h) and then a hue composition (H) can be computed based on NCS unique hues. For the defined hue, saturation (S) is computed using the classical formula for excitation purity applied along lines of constant h in the u’v’ chromaticity diagram. For that chromaticity, the luminance for zero gray content, G0, is defined as the reference for lightness (L) computations that follow a power function with offset model found to perform well in recent research for high-dynamic-range lightness-brightness scaling. The remaining dimensions are then derived from L and S along with luminance information. Brightness (B) is lightness (L) scaled by a factor derived from the classic work of Stevens and Stevens that illustrated terminal brightness as a function of adapting luminance. The derived scales are colorfulness (C), which is simply saturation (S) scaled by brightness (B), and chroma (Ch) which is saturation (S) times lightness (L). This type of formulation allows accurate description of color appearance for lights and objects across a variety of adaptation conditions and for lowor high-dynamic-range scenes. To the degree that each perceptual scale is accurate, differences on each of the dimensions should be easily calculated and, as long as the temptation to combine those differences into a single Euclidean distance metric is resisted, quite effective results can be obtained. The next section steps through a proposed implementation in detail. Implementation Fairchild,[4] at ISCC/IS&T/SID meeting on color spaces, outlined a methodology for computing the set of three fundamental appearance attributes of hue, saturation, and lightness for related colors from which the attributes of brightness and colorfulness can be derived as a function of the absolute luminance along with chroma. As the hue-linearized space IPT, based in opponent color theory, is considered exceptionally uniform in hue, the hue scale (h) is computed as a simple hue angle using the IPT model.[5] The required inputs for the IPT hue angle computation are the CIE tristimulus values in XYZ for the corresponding colors in CIE Illuminant D65. A chromatic adaptation transform is required to obtain corresponding colors for Illuminant D65 if the stimuli of interest are viewed under a different state of adaptation. The CAT02 transformation imbedded in the CIECAM02 color appearance model is recommended with a simple von Kries transformation on cone fundamentals a second choice. If luminance information is available and impacted by the selected chromatic adaptation transformation, then transformation to a white-point luminance of cd/m2 is recommended. Hue composition (H) can be obtained by recognizing th	additive model;color space;color vision;computability in europe;computation;diagram;euclidean distance;image scaling;information processes and technology;international solid-state circuits conference;maximal set;network computing system;norm (social);opponent process;pure function;utility functions on indivisible goods;xyz file format	Mark D. Fairchild;Rodney L. Heckaman	2012			computer vision;artificial intelligence;computer science	Vision	-41.82520339129462	-52.620128726246726	10750
42879a44724c1adb743cf4a5565c7a453707d170	tunisian dialect recognition based on hybrid techniques			hybrid kernel	Mohamed Hassine;Lotfi Boussaid;Hassani Massaoud	2018	Int. Arab J. Inf. Technol.		machine learning;artificial intelligence;computer science	AI	-15.138143639364303	-87.0868718454126	10754
b7399ac9728b07b7d194106d74602bf064e2b9f4	generating hypotheses from the web	hypothesis generation;search engine;scientific discovery;automatic generation;link analysis;web search	Hypothesis generation is a crucial initial step for making scientific discoveries. This paper addresses the problem of automatically discovering interesting hypotheses from the web. Given a query containing one or two entities of interest, our algorithm automatically generates a semantic profile describing the specified entity or provides the potential connections between two entities of interest. We implemented a prototype on top of the Google search engine and the experimental results demonstrate the effectiveness of our algorithms.	algorithm;entity;prototype;web search engine	Wei Jin;Rohini K. Srihari;Abhishek Singh	2008		10.1145/1367497.1367731	link analysis;computer science;data mining;web search query;world wide web;information retrieval;search engine	Web+IR	-30.06770641751788	-54.113482509769206	10764
3267db63c4723076a531cb5d576da88c1bac4b5d	probabilistic ensemble learning for vietnamese word segmentation	performance;syllable syllable frequency index;word segmentation algorithm;multi segmenter;algorithms;experimentation;vietnamese;probabilistic ensemble learning;languages	Word segmentation is a challenging issue, and the corresponding algorithms can be used in many applications of natural language processing. This paper addresses the problem of Vietnamese word segmentation, proposes a probabilistic ensemble learning (PEL) framework, and designs a novel PEL-based word segmentation (PELWS) algorithm. Supported by the data structure of syllable-syllable frequency index, the PELWS algorithm combines multiple weak segmenters to form a strong segmenter within the PEL framework. The experimental results show that the PELWS algorithm can achieve the state-of-the-art performance in the Vietnamese word segmentation task.	algorithm;data structure;ensemble learning;natural language processing;syllable;text segmentation	Wuying Liu;Li Lin	2014		10.1145/2600428.2609477	natural language processing;speech recognition;vietnamese;performance;computer science;machine learning;pattern recognition;scale-space segmentation	NLP	-22.291146942156544	-77.23152793221551	10770
c2dbb5bc547158bf4cc9a48fcbbef6e27a963237	automated evaluation of website navigability: an empirical validation of multilevel quality models	website navigability;website improvements;empirical study;quality evaluation	Websites are an important and efficient means of communication for companies wishing to interact with their clients. Therefore, research has focused on evaluating how websites should be structured to ensure their quality. The majority of this research has focused on evaluating the quality of individual pages or that of a site as a whole. In this article, we propose the use of two-level models that combine evaluations at the page level with evaluations at the site level, and applied them to the problem of evaluating the navigability of websites. To test our models, we conducted a study with 21 subjects who had to complete navigation tasks on several websites, and compared their quality judgments to those produced by single and two-level quality models. We found that two-level models are better predictors of navigability. Finally, we show how two-level models are able to suggest modifications to improve site navigability. Copyright © 2012 John Wiley & Sons, Ltd. Received 28 February 2011; Revised 11 May 2012; Accepted 28 May 2012	john d. wiley;mathematical model	Stéphane Vaucher;Antoine Moulart;Houari A. Sahraoui;Naji Habra	2013	Journal of Software: Evolution and Process	10.1002/smr.1562	engineering;data mining;empirical research;world wide web;information retrieval	SE	-37.12337520986857	-53.616741330235826	10783
8a61396ab3eae09d5a6b6bd020bb4433dec52473	evento 360: social event discovery from web-scale multimedia collection	social event detection;event summarization;topic models	We present Evento 360 (URL: http://evento360.info), an online interactive social event browser, which allows the user to explore events detected within a web-scale multimedia corpus. The system addresses five key aspects of social multimedia event detection and summarization: multimodality, scale, diversity of representations, noise of multimedia items, and missing metadata. The detection algorithm uses unsupervised clustering approach that exploits temporal, spatial and textual metadata. For each detected event cluster, to choose the best subset of photos that meet both relevance and diversity criteria, the system uses hierarchical clustering that exploits both visual and audio information. Evento 360's user interface provides a search feature that is not limited to a certain set of events, but rather can handle an arbitrary event query. It allows the user to retrieve and explore relevant events. The system scales well and is effective in producing high-quality summaries of the detected events.	algorithm;cluster analysis;hierarchical clustering;relevance;user interface;web search engine	Jaeyoung Choi;Eungchan Kim;Martha Larson;Gerald Friedland;Alan Hanjalic	2015		10.1145/2733373.2809934	computer science;machine learning;data mining;topic model;world wide web;information retrieval	Web+IR	-26.351474023120446	-53.98800719364161	10792
8f4a0c18d8ec3bc9efb6e55c21f88698531bcd01	prediction of black box warning by mining patterns of convergent focus shift in clinical trial study populations using linked public data	convergent focus shift;clinical trial;random forest;black box warning;patient selection	OBJECTIVE To link public data resources for predicting post-marketing drug safety label changes by analyzing the Convergent Focus Shift patterns among drug testing trials.   METHODS We identified 256 top-selling prescription drugs between 2003 and 2013 and divided them into 83 BBW drugs (drugs with at least one black box warning label) and 173 ROBUST drugs (drugs without any black box warning label) based on their FDA black box warning (BBW) records. We retrieved 7499 clinical trials that each had at least one of these drugs for intervention from the ClinicalTrials.gov. We stratified all the trials by pre-marketing or post-marketing status, study phase, and study start date. For each trial, we retrieved drug and disease concepts from clinical trial summaries to model its study population using medParser and SNOMED-CT. Convergent Focus Shift (CFS) pattern was calculated and used to assess the temporal changes in study populations from pre-marketing to post-marketing trials for each drug. Then we selected 68 candidate drugs, 18 with BBW warning and 50 without, that each had at least nine pre-marketing trials and nine post-marketing trials for predictive modeling. A random forest predictive model was developed to predict BBW acquisition incidents based on CFS patterns among these drugs. Pre- and post-marketing trials of BBW and ROBUST drugs were compared to look for their differences in CFS patterns.   RESULTS Among the 18 BBW drugs, we consistently observed that the post-marketing trials focused more on recruiting patients with medical conditions previously unconsidered in the pre-marketing trials. In contrast, among the 50 ROBUST drugs, the post-marketing trials involved a variety of medications for testing their associations with target intervention(s). We found it feasible to predict BBW acquisitions using different CFS patterns between the two groups of drugs. Our random forest predictor achieved an AUC of 0.77. We also demonstrated the feasibility of the predictor for identifying long-term BBW acquisition events without compromising prediction accuracy.   CONCLUSIONS This study contributes a method for post-marketing pharmacovigilance using Convergent Focus Shift (CFS) patterns in clinical trial study populations mined from linked public data resources. These signals are otherwise unavailable from individual data resources. We demonstrated the added value of linked public data and the feasibility of integrating ClinicalTrials.gov summaries and drug safety labels for post-marketing surveillance. Future research is needed to ensure better accessibility and linkage of heterogeneous drug safety data for efficient pharmacovigilance.	accessibility;appetite depressants;area under curve;black box warning;ct scan;chronic fatigue syndrome;climate forecast system;clinicaltrials.gov;genetic heterogeneity;information privacy;kerrison predictor;linkage (software);mental association;mined;patients;pharmacovigilance;population;predictive modelling;random forest;robustness (computer science);shift-work sleep disorder;spleen focus-forming virus;structured product labeling marketing status terminology;substance abuse detection;systematized nomenclature of medicine;tracer	Han-dong Mao;Chunhua Weng	2016	Journal of biomedical informatics	10.1016/j.jbi.2016.01.015	random forest;alternative medicine;medicine;computer science;machine learning;clinical trial;data mining	ML	-58.15770946705162	-67.49469571349753	10799
5de6e6cca7d0253934716391535d0b7fe684de34	location2vec: generating distributed representation of location by using geo-tagged microblog posts		This paper proposes a method to represent the characteristics of a place (i.e., use of the venue, atmosphere of the area) by using geo-tagged microblog posts around the place. It enables a vector representation of a location similar to the distributed representation of a term in Word2Vec. Our method uses a simple neural network that is trained through the task of estimating the terms that appear in tweets posted from the area. The effectiveness of our method is illustrated through an experiment of a comparison of similar locations in Tokyo and Kyoto.	artificial neural network	Yoshiyuki Shoji;Katsurou Takahashi;Martin J. Dürst;Yusuke Yamamoto;Hiroaki Ohshima	2018		10.1007/978-3-030-01159-8_25	data mining;word2vec;artificial neural network;geotagging;computer science;microblogging;social media	NLP	-24.528117275330068	-53.14922422393935	10839
0de5dba1f7d0f01cfa88501aeb8f42be153bfbca	complementary tasks for context-dependent deep neural network acoustic models		We have previously found that context-dependent DNN models for automatic speech recognition can be improved with the use of monophone targets as a secondary task for the network. This paper asks whether the improvements derive from the regularising effect of having a much small number of monophone outputs – compared to the typical number of tied states – or from the use of targets that are not tied to an arbitrary stateclustering. We investigate the use of factorised targets for left and right context, and targets motivated by articulatory properties of the phonemes. We present results on a large-vocabulary lecture recognition task. Although the regularising effect of monophones seems to be important, all schemes give substantial improvements over the baseline single task system, even though the cardinality of the outputs is relatively high.	acoustic cryptanalysis;acoustic model;artificial neural network;baseline (configuration management);context-sensitive language;deep learning;speech recognition;vocabulary	Peter Bell;Steve Renals	2015			cardinality;artificial neural network;artificial intelligence;pattern recognition;small number;left and right;computer science	NLP	-17.58028448204693	-89.44309639845223	10840
3be0d1378490c1ada643dec382894e75bbc80dd1	surgical process modelling: a review	surgical skill evaluation;surgical workflow;procedural knowledge;computer assisted surgery	Surgery is continuously subject to technological and medical innovations that are transforming daily surgical routines. In order to gain a better understanding and description of surgeries, the field of surgical process modelling (SPM) has recently emerged. The challenge is to support surgery through the quantitative analysis and understanding of operating room activities. Related surgical process models can then be introduced into a new generation of computer-assisted surgery systems. In this paper, we present a review of the literature dealing with SPM. This methodological review was obtained from a search using Google Scholar on the specific keywords: “surgical process analysis”, “surgical process model” and “surgical workflow analysis”. This paper gives an overview of current approaches in the field that study the procedural aspects of surgery. We propose a classification of the domain that helps to summarise and describe the most important components of each paper we have reviewed, i.e., acquisition, modelling, analysis, application and validation/evaluation. These five aspects are presented independently along with an exhaustive list of their possible instantiations taken from the studied publications. This review allows a greater understanding of the SPM field to be gained and introduces future related prospects.	gain;google scholar;operating room;operative surgical procedures;process modeling;radiotherapy, computer-assisted;super paper mario	Florent Lalys;Pierre Jannin	2013	International Journal of Computer Assisted Radiology and Surgery	10.1007/s11548-013-0940-5	simulation;medicine;computer science;procedural knowledge;surgery	HCI	-56.209803534841775	-62.49673354428677	10854
ee427ae04d5ce3bf7e310d7aee7c1a3c6df2c3cb	latvian newswire information extraction system and entity knowledge base			information extraction;knowledge base	Peteris Paikens	2014		10.3233/978-1-61499-442-8-119	information retrieval;information extraction;knowledge base;natural language processing;latvian;artificial intelligence;computer science	AI	-30.823218138075823	-76.69906290147985	10856
31c9c69982700bb2aaa27ba0e443170ffc7c88e6	how realistic is artificially added noise?		Evaluations of algorithms for robust automatic speech recognition (ASR) are often based on artificial noisy speech instead of realistic noisy speech. In this paper we compare the ASR performance of speech with artificial additive noise to the performance of realistic noisy speech. All data was recorded during the same recording campaign and with nearly identical channel characteristics. The simulation process takes into account all major characteristics of the noisy reference data. Clean speech, noisy speech and simulated speech are compared for different aspects of robust ASR including noise reduction by Spectral Subtraction and the ETSI robust front end. The results show, that artificial noisy speech even in very controlled simulation environments is not very similar and not a full substitute for realistic noisy data. While the tendencies of the improvement for artificial and realistic data are similar for the evaluated approaches, the magnitude can be quite different.	additive white gaussian noise;algorithm;noise reduction;signal-to-noise ratio;simulation;speech recognition;utility functions on indivisible goods	Thomas Winkler	2011				NLP	-12.778421145293189	-90.24480515204809	10873
6a6b9a99a8b30d48e7b34ac705e86735ec6802f5	wednesday: parsing flexible word order languages	natural language sentence;lexicon linguistic knowledge;cognitive representation;flexible word order language;syntactic memory management;search space;syntactic knowledge;semantic network;morphological data;global space;semantic description;natural language;memory management;word order	"""A parser for """"flexible"""" word order languages must be substantially data driven. In our view syntax has two distinct roles in this connection: (i) to give impulses for assembling cognitive representations, (ii) to structure the space of search for fillers. WEDNESDAY is an interpreter for a language describing the lexicon and operating on natural language sentences. The system operates from left to right, interpreting the various words comprising the sentence one at a time. The basic ideas of the approach are the following: a) to introduce into the lexicon linguistic knowledge that in other systems is in a centralized module. The lexicon therefore carries not only morphological data and semantic descriptions. Also syntactic knowledge is distributed throughout it, partly of a procedural kind. b) to build progressively a cognitive representation of the sentence in the form of a semantic network, in a global space, accessible from all levels of the analysis. c) to introduce procedures invoked by the words themselves for syntactic memory management. Simply stated, these procedures decide on the opening, closing, and mantaining of search spaces; they use detailed constraints and take into account the active expectations. WEDNESDAY is implemented in MAGMA-LISP and with a stress on the non-deterministic mechanism. I. Parsing typologically diverse languages emphasizes aspects that are absent or of little importance in English. By taking these problems into account, some light may be shed on: a) insufficiently treated psycholinguistic aspects b) a design which is less language-dependent c) extraand non-grammatical aspects to be taken into consideration in designing a friendly English The work reported here has largely involved problems with parsing Italian. One of the typical features of Italian is a lower degree of word order rigidity in sentences. For instance, """"Paolo ama Maria"""" (Paolo loves Maria) may be rewritten without any significant difference in meaning (leaving aside questions of context and pragmatics) in any the six possible permutations: Paolo ama Maria, Paolo Maria ama, Maria ama Paolo, Maria Paolo ama, ama Paolo Maria, ama Maria Paolo. Although Subject-Verb-Object is a statistically prevalent construction, all variations in word order can occur inside a component, and they may depend on the particular words which are used. 2. In ATNSYS (Cappelli, Ferrari, Moretti, Prodanof and Stock, 1978), a previously constructed ATN based system (Woods, 1970), a special dynamic reordering mechanism was introduced in order to get sooner to a correct syntactic analysis, when parsing sentences of a coherent text (Ferrari and Stock, 1980). Besides psycholinguistic motivations, the main reason for the introduction such heuristics lay in the large number of alternative arcs that has to be introduced in networks for parsing Italian sentences. As a matter of fact, ATN's were not originally conceived for flexible word order languages. (In the extreme free word order case, an ATN would have one single node and a large number of looping arcs, losing its meaningfulness). Work has been done on ATN parsers for the parsing of non-grammatical or extragrammatical sentences in English, a problem related to our one. For instance Weischedel and Black (1981) have proposed a system of information passing in the case of parsing failure. Kwasny and Sondheimer (1981) have suggested the relaxation of constraints on the arcs under certain circumstances. Nevertheless, these problems, together with that of treating idiosyncratic phenomena related to words and flexible idioms, are not easy to solve within the ATN approach. At least two other parsers should be mentioned here."""	automatic message accounting;centralized computing;closing (morphology);coherence (physics);emoticon;heuristic (computer science);identification friend or foe;interpreter (computing);lexicon;linear programming relaxation;magma;memory management;microsoft word for mac;natural language;parsing;procedural programming;programming idiom;semantic network	Oliviero Stock;Cristiano Castelfranchi;Domenico Parisi	1983			word order;natural language processing;semantic role labeling;computer science;semantic compression;linguistics;natural language;semantic network;programming language;memory management	NLP	-32.22500938405833	-82.51627080172824	10896
5da53320345ed5509dc5601cf61a4c8f2af07d4e	a digital humanities approach to the history of science - eugenics revisited in hidden debates by means of semantic text mining	universiteitsbibliotheek	Comparative historical research on the the intensity, diversity and fluidity of public discourses has been severely hampered by the extraordinary task of manually gathering and processing large sets of opinionated data in news media in different countries. At most 50,000 documents have been systematically studied in a single comparative historical project in the subject area of heredity and eugenics. Digital techniques, like the text mining tools WAHSP and BILAND we have developed in two successive demonstrator projects, are able to perform advanced forms of multi-lingual text-mining in much larger data sets of newspapers. We describe the development and use of WAHSP and BILAND to support historical discourse analysis in large digitized news media corpora. Furthermore, we argue how text mining techniques overcome the problem of traditional historical research that only documents explicitly referring to eugenics issues and debates can be incorporated. Our tools are able to provide information on ideas and notions about heredity, genetics and eugenics that circulate in discourses that are not directly related to eugenics (e.g., sport, education and economics).	digital humanities;text corpus;text mining	Pim Huijnen;Fons Laan;Maarten de Rijke;Toine Pieters	2013		10.1007/978-3-642-55285-4_6	computer science;artificial intelligence;algorithm	ML	-36.844270807038136	-72.5376405551489	10899
7dfbbeaa8c06308c519695c1e6edec887434bb28	enhancing text clustering model based on truncated singular value decomposition, fuzzy art and cross validation	text analysis adaptive resonance theory fuzzy set theory pattern clustering singular value decomposition;model selection;pattern clustering;model selection learning data mining nlp tsvd variable selection semantic analysis;learning;singular value decomposition;text analysis;data mining;fuzzy set theory;mathematical model abstracts marine vehicles approximation methods equations accuracy entropy;variable selection;variable selection schemes fuzzy adaptive resonance theory cross validation fuzzy art truncated singular value decomposition text clustering model multidisciplinary applications optimal conceptual model automatic conceptual model clustering accuracy semantic based scheme;tsvd;nlp;adaptive resonance theory;semantic analysis	Numerical schemes research on clustering model has been quite intensive in the past decade. The difficulties associated with curse of dimensionality and cost functions to reflect the general knowledge about internal structures and distributions of target data. Traditional computational clustering and variables selection schemes are struggling to estimate at high level of accuracy for this type of problem. Hence, in the present study, a novel semantic-based scheme was proposed to enhance the clustering accuracy. The results show that our conceptual model is automatic and optimal. Good comparisons with the experimental studies demonstrate the multidisciplinary applications of our approach.	cluster analysis;cross-validation (statistics);curse of dimensionality;high-level programming language;numerical linear algebra;singular value decomposition	Choukri Djellali	2013	2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2013)	10.1145/2492517.2500317	correlation clustering;constrained clustering;text mining;fuzzy clustering;computer science;artificial intelligence;adaptive resonance theory;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;data mining;mathematics;fuzzy set;cluster analysis;brown clustering;singular value decomposition;feature selection;model selection;statistics;conceptual clustering	EDA	-17.306125019906716	-62.23121079381067	10957
7aefbddae482da31e103ff1031c087b99e303669	group delay based phone segmentation for hts	hidden markov models high temperature superconductors speech delays context speech synthesis labeling	HMM based speech synthesis (HTS) is a state-of-the art approach to text-to-speech synthesis. Segmentation of the training data is essential for building any text-to-speech system. Most conventional text-to-speech systems use phones as the basic unit of synthesis and use a speech recogniser to automatically segment the data at the phone level. As Indian languages are low resource languages, accurate transcriptions are difficult to obtain owing to paucity of data. Manual labeling at the phone level is not only laborious but also inaccurate. HMM based flat start segmentation doesn't work well at the sentence level. In this paper we propose an event driven approach to obtain better phone boundaries. Syllable-like events are detected in the speech signal and matched with syllabified transcription of the text. The syllables are converted to phoneme sequences and Baum-Welch embedded re-estimation is restricted to the syllable-level. Subjective evaluations indicate that the proposed system has a lower word error rate compared to that of a conventional system that uses flat start for obtaining phone boundaries.	baum–welch algorithm;embedded system;group delay and phase delay;hidden markov model;high-throughput satellite;medical transcription;signal processing;speech synthesis;syllable;transcription (software);welch's method;word error rate	Sangaraju Shanmugam;Hema A. Murthy	2014	2014 Twentieth National Conference on Communications (NCC)	10.1109/NCC.2014.6811273	natural language processing;speech recognition;computer science;communication	NLP	-20.8541422556028	-83.74663002295512	10970
5a823590c0b453a4163de4770f0e7b18aec7fa46	an approach for building a semi-automatic online consultancy system	support vector machines;training;text recommendation;text classification;message classification;training data;systems architecture;classification algorithms;semi automatic question answering system;buildings;data models	This study proposes an approach for building a semi-automatic consultancy (question-answering) system via mobile/Internet networks. This approach is a combination of natural language (text) processing and machine learning method. For building the system, at first, we need to build modules for sending and receiving messages via SMS/Email/Webpage. These modules are used for users to send/receive their questions that need to be consulted. While waiting for answering from the system, the user will be recommended similar questions which have been answered in the past by using Cosine similarity. Next, a message classification module is built using a combination of text processing (e.g., word segmentation, stop word deletion) and machine learning method (e.g., SVM). Finally, a whole web-based system is conducted to integrate these modules. The proposed approach is applied for a case study of consulting on Vietnam National Entrance Test, which is an important test for the pupils to get into universities. Initial results show that the system can automatically classify the questions at 82.33% of accuracy, thus, this approach could be promising for (semi) automatic online consultancy systems.	cosine similarity;email;internet;machine learning;natural language;question answering;semiconductor industry;text segmentation;web application;web page	Nguyen Thai-Nghe;Quoc Dinh Truong	2015	2015 International Conference on Advanced Computing and Applications (ACOMP)	10.1109/ACOMP.2015.11	computer science;data science;data mining;world wide web	SE	-30.846664510836636	-69.1650393797052	10973
a7168b3460891ed781f9ba8e7790aafbee4be884	triphone analysis: a combined method for the correction of orthographical and typographical errors	typographical error;existing system;orthographical error;word level error;combined method;trigram analysis;new correction strategy;phonemic transcription;triphone analysis	Most existing systems for the correction of word level errors are oriented toward either typographical or orthographical errors. Triphone analysis is a new correction strategy which combines phonemic transcription with trigram analysis. It corrects both kinds of errors (also in combination) and is superior for orthographical errors.	transcription (software);trigram;triphone	Brigitte van Berkel;Koenraad De Smedt	1988			trigram;typographical error;triphone;speech recognition;computer science	NLP	-21.54371410083235	-81.07837178228056	10978
6ef3b7c4a898fe7377d0654ce915857f5830e091	the perceptual and cognitive role of visual and auditory channels in conveying emotional information	comparative analysis;expressed emotion;emotional expressions;cognitive load;point of view;foreign language;visual and auditory information;emotional expression	In expressing emotional states, we generate a synaesthesic experience in our interlocutor when we transmit information about our feelings by the simultaneous use of several sensorial channels. These are referred to as verbal (the semantic content of the message) and non-verbal (gesture, gaze, tonal expression) modalities. From an engineering point of view the transmission of the same information by more than one sensorial channel should be considered as redundant. Is this true or does each channel transmit a specific piece of information? How much emotional information is transmitted by each channel and which plays a major role? As an attempt to address these questions, here we present a comparative analysis of the subjective perception of emotional states by the visual and auditory channels considered either singularly or in combination, always in the non-verbal modality. The results reveal that the audio and visual components of emotional messages convey much the same amount of information either separately or in combination, hence suggesting that each channel performs a robust encoding of the emotional features. Redundancy probably facilitates the recovery of emotional information in case one of the channels is impaired. This conclusion is challenged by language cultural specificity, since when tested on a foreign language the addressers rely more on visual information.	computer multitasking;information;modality (human–computer interaction);point of view (computer hardware company);qualitative comparative analysis;sensitivity and specificity	Anna Esposito	2009	Cognitive Computation	10.1007/s12559-009-9017-8	psychology;cognitive psychology;emotional expression;communication;social psychology	HCI	-7.167453343334829	-79.94605721802142	10982
71d90a60b9b59c201cc25fc49645840b15b53efb	visual acceptance of library-generated citygml lod3 building models	citygml;3d modelling;modelisation 3d;approbation visuelle;visual acceptance;building modelling;modelisation d immeubles;geovisualisation;geovisualization	The acceptance of 3D building models is critical to all urban 3D visualization projects. Building models that are identified as unacceptable can increase the cost of the project, delay the delivery, and, in some cases, cancel the acceptance of the entire project. A 3D modelling approach of using representative textures and geometry rather than actual photorealistic textures and geometry was conducted to determine whether participants who frequent the building multiple times a week over a period of a year would be able to identify the visual difference. Three focus groups were established and used to evaluate the visual quality of the 3D building models. Participants were asked to rank the visual quality of the building, as well as identifying any geometry, texture, or overall visual quality problems. The participants from the three focus groups did not identify any texture or geometry mistakes present in the building models. The overall visual quality identified by the participants from the three focus groups was above average, suggesting that the 3D modelling approach is an effective means for modelling buildings with high visitation and significance.	3d modeling;citygml;focus group;volume rendering	Ryan Garnett;Jason T. Freeburn	2014	Cartographica	10.3138/carto.49.4.2522	geovisualization;geography;artificial intelligence;algorithm;cartography	HCI	-43.77988415985768	-55.46242909261356	11015
348b84ac7077c4bd7bfa7ed5c6d0085ce9c92336	induction of treebank-aligned lexical resources		We describe the induction of lexical resources from unannot ated corpora that are aligned with treebank grammars, provi ding a systematic correspondence between features in the lexical resource an d a treebank syntactic resource. We first describe a methodol ogy based on parsing technology for augmenting a treebank database with linguistic features. A PCFG containing these features is cr eated from the augmented treebank. We then use a procedure based on the insi de-outside algorithm to learn lexical resources aligned wi th the treebank PCFG from large unannotated corpora. The method has been app lied in creating a feature-annotated English treebank base d on the Penn Treebank. The unsupervised estimation procedure give s a substantial error reduction (up to 31.6%) on the task of le arning the subcategorization preference of novel verbs that are not pr esent in the annotated training sample.	algorithm;parsing;stochastic context-free grammar;text corpus;treebank	Tejaswini Deoskar;Mats Rooth	2008			speech recognition;natural language processing;syntax;artificial intelligence;subcategorization;word error rate;treebank;lexicon;verb;computer science;rule-based machine translation;context-free grammar	NLP	-22.80826338970023	-74.92601812488276	11032
b3a5394bb3d78b0fbe2e7886527507445c5dd06e	convolutional density estimation in hidden markov models for speech recognition	parameter estimation speech recognition hidden markov models convolution probability gaussian processes deconvolution decoding genetic algorithms computational complexity;gaussian mixture;probability;decoding;gaussian processes;convolution;hidden markov model;probability density function;k means;density estimation;hidden markov models;compact representation;computational complexity;deconvolution;speech recognition;genetic algorithm;genetic algorithms;performance convolutional density estimation hmm speech recognition continuous density hidden markov models probability density function pdf gaussians mixture m gaussian mixture impulses reestimation formulae convolutional model parameters residual k means approach deconvolution genetic algorithm optimal gaussians assignment compact representation storage space training time decoding time 1996 hub 4 development test;mixture of gaussians;parameter estimation;continuous density hidden markov model;hidden markov models gaussian processes decoding speech recognition probability density function convolution deconvolution genetic algorithms testing standards development	In continuous density Hidden Markov Models (HMMs) for speech recognition, the probability density function (pdf) for each state is usually expressed as a mixture of Gaussians. In this paper, we present a model in which the pdf is expressed as the convolution of two densities. We focus on the special case where one of the convolved densities is aM -Gaussian mixture, and the other is a mixture ofN impulses. We present the reestimation formulae for the parameters of theM N convolutional model, and suggest two ways for initializing them, the residual K-Means approach, and the deconvolution from a standard HMM with MN Gaussians per state using a genetic algorithm to search for the optimal assignment of Gaussians. Both methods result in a compact representation that requires onlyO(M +N) storage space for the model parameters, andO(MN) time for training and decoding. We explain how the decoding time can be reduced to O(M + kN), wherek < M . Finally, results are shown on the 1996 Hub-4 Development test, demonstrating that a 32 2 convolutional model can achieve performance comparable to that of a standard 64-Gaussian per state model.	convolution;deconvolution;genetic algorithm;hidden markov model;k-means clustering;markov chain;mixture model;portable document format;speech recognition	Spyridon Matsoukas;George Zavaliagkos	1999		10.1109/ICASSP.1999.758075	speech recognition;genetic algorithm;computer science;machine learning;pattern recognition;mathematics;hidden markov model;statistics	ML	-19.466906302557348	-92.48311512218687	11039
e26a191c19f7b57eccaa88a89c687f90c92d2c5f	an investigation of search processes in collaborative exploratory web search		People are often engaged in collaboration in workplaces or daily life due to the complexity of tasks. In the information seeking and retrieval environment, the task can be as simple as fact-finding or a known-item search, or as complex as exploratory search. Given the complex nature of the information needs, exploratory searches may require the collaboration among multiple people who share the same search goal. For instance, students may work together to search for information in a collaborative course project; friends may search together while planning a vacation. #R##N#There are demands for collaborative search systems that could support this new format of search (Morris, 2013). Despite the recognized importance of understanding search process for designing successful search system (Bates, 1990; M. Hearst, 2009), it is particularly difficult to study collaborative search process because of the complex interactions involved. #R##N#In this dissertation, I propose and demonstrate a framework of investigating search processes in the collaborative exploratory search. I designed a laboratory-based user study to collect the data, compared two search conditions: individual search and collaborative search as well as two task types through the study. I first applied a novel Hidden Markov Model approach to analyze the search states in the collaborative search process, the results of which provide a holistic picture of the collaborative search process. I then investigated two important components in the collaborative search process – query behaviors and communications. The findings reveal the characteristics of query and communication patterns in the collaborative search. It also suggests that although the collaboration between two people on search did not achieve a higher performance than two individuals, the collaboration indeed make people feel more satisfied with their performance and less stressed. The results of this study not only provide implications for designing effective collaborative search systems, but also show valuable research directions and methodologies for other researchers.	web search engine	Zhen Yue;Shuguang Han;Daqing He	2012		10.1002/meet.14504901386	collaborative learning;simulation;semantic search;computer science;knowledge management;search analytics;world wide web	Web+IR	-35.834344738318904	-52.976272995708285	11041
02442dc868f823a78c0ec2d59109f7c09493912e	multimodal fusion by adaptive compensation for feature uncertainty with application to audiovisual speech recognition	multimodal fusion;environmental conditions;audio visual speech recognition;pattern recognition;speech recognition;uncertainty speech recognition noise visualization face shape speech;measurement noise multimodal fusion adaptive compensation feature uncertainty pattern recognition audio visual speech recognition;measurement noise	In pattern recognition one usually relies on measuring a set of informative features to perform tasks such as classification. While the accuracy of feature measurements heavily depends on changing environmental conditions, studying the consequences of this fact has received relatively little attention to date. In this work we explicitly take into account uncertainty in feature measurements and we show in a rigorous probabilistic framework how the models used for classification should be adjusted to compensate for this effect. Our approach proves to be particularly fruitful in multimodal fusion scenarios, such as audio-visual speech recognition, where multiple streams of complementary features are integrated. For such applications, provided that an estimate of the measurement noise uncertainty for each feature stream is available, we show that the proposed framework leads to highly adaptive multimodal fusion rules which are widely applicable and easy to implement. We further show that previous multimodal fusion methods relying on stream weights fall under our scheme if certain assumptions hold; this provides novel insights into their applicability for various tasks and suggests new practical ways for estimating the stream weights adaptively. Preliminary experimental results in audio-visual speech recognition demonstrate the potential of our approach.	audio-visual speech recognition;information;multimodal interaction;oracle fusion architecture;oracle fusion middleware;pattern recognition	Athanasios Katsamanis;George Papandreou;Vassilis Pitsikalis;Petros Maragos	2006	2006 14th European Signal Processing Conference		speaker recognition;speech recognition;feature;computer science;pattern recognition;communication	Vision	-10.645018993416313	-95.71752067136137	11055
b01e8982a8c59b23817284fa2ebd868b8066bbdb	mrd: a mashup resource discovery approach applying semantics indexing	mashup searching;semantic annotation;semantic indexing;external ontology base;mashups;resource discovery;statistical analysis dictionaries indexing information retrieval ontologies artificial intelligence semantic web;semantics annotation;information retrieval;r p curve statistics;semantics;ontologies artificial intelligence;r p curve statistics mashup resource discovery semantics indexing mashup searching index library external ontology base synonym base semantics annotation mashup description file;statistical analysis;indexing;synonym base;indexation;dictionaries;mashups semantics indexing ontologies mobile communication;mobile communication;mashup description file;mashup resource discovery;index library;semantic web;ontologies;high performance;semantics indexing	The mashup is a hot research topic in recent years. In this paper, we propose an efficient Mashup Resource Discovery(MRD) approach to facilitating mashup searching. Basing on an index library, MRD acts high performance in response time. By an external ontology base and a synonym base, we apply a semantics discovery, avoiding semantics annotation in mashup description files. By evaluating the prestige of mashups(especially for mashup api), we rank the results and make mashups with high prestige get higher ranking. The experimental evaluation shows that our MRD act high performance both in time consuming and r-p curve statistics.	application programming interface;experiment;machine-readable dictionary;mashup (web application hybrid);response time (technology);subject matter expert turing test;time complexity	Changbao Li;Bo Cheng;Junliang Chen;Pingli Gu;Na Deng;Desheng Li	2011	2011 IEEE International Conference on Communications (ICC)	10.1109/icc.2011.5962809	search engine indexing;mobile telephony;computer science;ontology;semantic web;database;semantics;world wide web;information retrieval;mashup	DB	-29.48661125077744	-57.407691762755725	11099
9862bb02e66815762c1d86664863471ad18e7c92	an adaptive classification method for multimedia retrieval	multimedia retrieval;classification tree analysis support vector machines support vector machine classification multimedia databases state feedback content based retrieval training data information retrieval computer science multimedia systems;machine learning algorithm adaptive classification method content based multimedia retrieval systems relevance feedback approach;classification;multimedia systems;machine learning;random forest;k nearest neighbor;relevance feedback;content based retrieval;relevance feedback classification content based retrieval multimedia systems	Relevance feedback can effectively improve the performance of content-based multimedia retrieval systems. To be effective, a relevance feedback approach must be able to efficiently capture the user’s query concept from a very limited number of training samples. To address this issue, we propose a novel adaptive classification method using random forests, which is a machine learning algorithm with proven good performance on many traditional classification problems. With random forests, our method reduces the relevance feedback to a two-class classification problem and classifies database objects as relevant or irrelevant. From the relevant object set, our approach returns the top k nearest neighbors of the query to the user. Briefly speaking, our relevance feedback method has the following dominant features. First, our method is able to address the multimodal distribution of relevant points, because it trains a nonparametric and nonlinear classifier, i.e., random forests, for relevance feedback. Second, it does not overfit training data because it uses an ensemble of tree classifiers to classify multimedia objects. Experiments on a Corel image set (with 31,438 images) show that our method significantly outperforms the state-of-the-art relevance feedback approaches.	information retrieval;k-nearest neighbors algorithm;machine learning;multimodal interaction;nonlinear system;overfitting;random forest;relevance feedback	Yimin Wu;Aidong Zhang	2003		10.1109/ICME.2003.1221028	random forest;biological classification;computer science;machine learning;pattern recognition;k-nearest neighbors algorithm;information retrieval	Web+IR	-18.10474999065264	-62.58239871103595	11120
6fe2f459502dc15ed71736b2eb1e1fa335a0c185	clinical terminology support for a national ambulatory practice outcomes research network	database management systems;information management	"""The Medical Quality Improvement Consortium (MQIC) is a nationwide collaboration of 74 healthcare delivery systems, consisting of 3755 clinicians, who contribute de-identified clinical data from the same commercial electronic medical record (EMR) for quality reporting, outcomes research and clinical research in public health and practice benchmarking. Despite the existence of a common, centrally-managed, shared terminology for core concepts (medications, problem lists, observation names), a substantial """"back-end"""" information management process is required to ensure terminology and data harmonization for creating multi-facility clinically-acceptable queries and comparable results. We describe the information architecture created to support terminology harmonization across this data-sharing consortium and discuss the implications for large scale data sharing envisioned by proponents for the national adoption of ambulatory EMR systems."""	consortium;delivery of health care;electronic health records;electronics, medical;epidemiologic research design;excalibur: morgana's revenge;information architecture;information management;medical records;name;nomenclature;numerous	Thomas N. Ricciardi;Michael I. Lieberman;Michael G. Kahn;Fred E. Masarie	2005	AMIA ... Annual Symposium proceedings. AMIA Symposium		information architecture;information management;health care;benchmarking;quality management;terminology;data sharing;knowledge management;outcomes research;medicine	Arch	-59.43738390397433	-63.14173258375818	11133
d81a9e2335decf25e3dfd76b9ea12c05bd9288f8	research paper: use of electronic information systems in nursing homes: united states, 2004	medical records;minimum data set;research paper;cross section;information system;nursing home	OBJECTIVES This study sought to define the extent of utilization of 12 types of electronic information system (EIS) function in U.S. nursing homes (NH), to relate EIS utilization to selected facility characteristics and to contrast these findings to previous estimates of EIS use in NH.   DESIGN This study used data from the National Nursing Home Survey (NNHS), a nationally representative, cross-sectional sample of U.S. NH.   MEASUREMENTS Data on current use of EIS in 12 functional areas, including administrative and resident care activities, were collected. Information was also collected on facility characteristics including ownership, bed size, and whether the facility was a member of a chain.   RESULTS Essentially all (99.6%) U.S. NH had >or=1 EIS, a figure that was driven by the nearly universal use of EIS for Minimum Data Set (MDS) reporting (96.4%) and for billing (95.4%). Nearly 43% of U.S. NH had EIS for medical records, including nurse's notes, physician notes, and MDS forms. EIS use ranged from a high of 79.6% for admission, transfer, and discharge to a low of 17.6% for daily care by certified nursing assistants (CNAs). Ownership, membership in a chain, and bed size were associated with use of selected EIS. Larger facilities and those that were part of a chain used more EIS than smaller standalone facilities.   CONCLUSION In 2004, NH use of EIS for functions other than MDS and billing was highly variable, but considerably higher than previous estimates.	breast feeding;cns disorder;cross-sectional data;discharger;discipline of nursing;electronic billing;estimated;information system;large;minimum data set;nethack;note (document);nursing homes;radiology information systems;small	Helaine E. Resnick;Barbara B. Manard;Robyn I. Stone;Majd Alwan	2009	Journal of the American Medical Informatics Association : JAMIA	10.1197/jamia.M2955	medicine;nursing;cross section;information system;medical record	HCI	-61.11010791728618	-65.6563083208851	11170
d3b46a0fd3f463186c8156746cf42666c729c59f	what's the date? high accuracy interpretation of weekday names		In this paper we present a study on the interpretation of weekday names in texts. Our algorithm for assigning a date to a weekday name achieves 95.91% accuracy on a test data set based on the ACE 2005 Training Corpus, outperforming previously reported techniques run against this same data. We also provide the first detailed comparison of various approaches to the problem using this test data set, employing re-implementations of key techniques from the literature and a range of additional heuristic-based approaches.	ace;constraint satisfaction problem;han unification;heuristic (computer science);hybrid algorithm;machine learning;self-replicating machine;test data	Pawel P. Mazur;Robert Dale	2008			digital library;computer science;data science;data mining;operations research	SE	-30.89202211093948	-64.71213892134489	11226
d73519d1924436485e2be7fdcb7ccfdfd7664c1b	blind men and elephants: six approaches to trec data	performance measure;information retrieval;cluster analysis;elephants;multidimensional scaling;text retrieval;analysis of variance;rank correlation	The paper reviews six recent efforts to better understand performance measurements on information retrieval (IR) systems within the framework of the Text REtrieval Conferences (TREC): analysis of variance, cluster analyses, rank correlations, beadplots, multidimensional scaling, and item response analysis. None of this work has yielded any substantial new insights. Prospects that additional work along these lines will yield more interesting results vary but are in general not promising. Some suggestions are made for paying greater attention to richer descriptions of IR system behavior but within smaller, better controlled settings.	additive model;aggregate data;algorithm;cluster analysis;experiment;image scaling;information retrieval;item response theory;multidimensional scaling;systems design	David Banks;Paul Over;Nien-Fan Zhang	1999	Information Retrieval	10.1023/A:1009984519381	analysis of variance;multidimensional scaling;computer science;data science;machine learning;data mining;cluster analysis;rank correlation;information retrieval	Web+IR	-38.003842955111594	-61.884311136352416	11235
7f46d6f72c35fc822a2b914ce6e8665ba3361a2a	codemagic: semi-automatic assignment of icd-10-am codes to patient records		There is huge amount of data generated in modern hospitals, such as medical records, lab reports and doctor notes. Health care systems employ a large number of categorization and classification systems to assist data management for a variety of tasks such as patient care, record storage and retrieval, statistical analysis, insurance and billing. One of these systems is the International Classification of Diseases (ICD), which is the official system of assigning codes to diagnoses and procedures associated with hospital utilization. After a patient’s clinical treatment, ICD codes are assigned to that patient’s health records and insurance companies take these labels into account for reimbursement. On the other hand, label assignment by human annotators (clinic coders) may not be reliable. For example, in a Turkish hospital, two human experts examined 491 pre-labeled patient records and indicated that more than half of the records are assigned wrong ICD codes. An ICD-10-AM code indicates a classification of a disease, symptom, procedure or injury. Codes are organized hierarchically, where top level entries are general groupings and bottom level codes indicate specific symptoms or diseases and their location [9]. Each specific, low-level code consists of 4 or 5 digits, with a decimal after the third. Overall, there are thousands of codes that cover a broad range of medical conditions [9]. Only trained experts can properly perform the task, making the process of coding documents both expensive and unreliable since a coder must	bag-of-words model;categorization;code;electronic billing;embedded entertainment system;experiment;high- and low-level;preprocessor;recommender system;semiconductor industry;spell checker;tokenization (data security);web search engine	Damla Arifoglu;Onur Deniz;Kemal Aleçakir;Meltem Turhan Yöndem	2014		10.1007/978-3-319-09465-6_27	recommender system;computer science;icd-10;distributed computing;documentation;data mining;borda count;search engine;standardization;voting;schema (psychology)	ML	-48.095148178795895	-68.8352335215701	11270
8cc59035dd9e0c59fdff5c316e818e83207cafc9	"""two-parameter model of word length """"language - genre"""""""	word length	A two-parameter model of word length measured by the number of syllables comprising it is proposed. The first parameter is dependent on language type, the second one - on text genre and reflects the degree of completion of synergetic processes of language optimization.		Victor Kromer	2001	CoRR		natural language processing;speech recognition;computer science;word lists by frequency;linguistics	NLP	-12.872320422018104	-79.75636093227281	11286
b11ce9796b932be637bba043bf4a33064df74fb3	a semi-automatic wizard of oz technique for let'sfly spoken dialogue system	human computer interaction;mixed initiative;spoken dialogue system;automatic speech recognition;wizard of oz;call centers;spoken dialogue systems;wizard of oz study;call center;dialogue manager;spoken language understanding	The paper presents Let'sFly spoken dialogue system intended for natural human-computer interaction via telephone lines in travel planning domain. The system uses ASR, keyword spotting and TTS methods for continuous Russian speech and a dialogue manager with mixed-initiative strategy. Semi- automatic Wizard of Oz technique used for collecting speech data and real dialogues is described. Semi-automatic model is a tradeoff between a fully automatic spoken dialogue system and an interaction with a hidden human-operator working like a computer system. The experimental data obtained with this technique are discussed in the paper.	dialog system;semiconductor industry;spoken dialog systems	Alexey Karpov;Andrey Ronzhin;Anastasia Leontyeva	2008		10.1007/978-3-540-87391-4_74	natural language processing;speech recognition;computer science	NLP	-25.687317828096077	-85.36333431551975	11350
ee57ba19b665e8aec4bd46cd03c222738efa82bb	the effects of recommendations’ presentation on persuasion and satisfaction in a movie recommender system	filtering;filtrage;algorithm performance;articulo sintesis;article synthese;implementation;efficient algorithm;filtrado;information filtering;recommender system;internet;senal video;signal video;resultado algoritmo;performance algorithme;video signal;internet application;implementacion;review;user satisfaction	The explosive growth of Internet applications and content, during the last decade, has revealed an increasing need for information filtering and recommendation. Most research in the area of recommendation systems has focused on designing and implementing efficient algorithms that provide accurate recommendations. However, the selection of appropriate recommendation content and the presentation of information are equally important in creating successful recommender applications. This paper addresses issues related to the presentation of recommendations in the movies domain. The current work reviews previous research approaches and popular recommender systems, and focuses on user persuasion and satisfaction. In our experiments, we compare different presentation methods in terms of recommendations’ organization in a list (i.e. top N-items list and structured overview) and recommendation modality (i.e. simple text, combination of text and image, and combination of text and video). The most efficient presentation methods, regarding user persuasion and satisfaction, proved to be the “structured overview” and the “text and video” interfaces, while a strong positive correlation was also found between user satisfaction and persuasion in all experimental conditions.	algorithm;cluster analysis;experiment;information filtering system;interaction;internet;modality (human–computer interaction);recommender system;self-replicating machine;simpletext;smartphone;the movies;usability	Theodora Nanou;George Lekakos;Konstantinos G. Fouskas	2010	Multimedia Systems	10.1007/s00530-010-0190-0	filter;the internet;simulation;telecommunications;computer science;multimedia;implementation;law;world wide web;recommender system	Web+IR	-37.5536107461705	-56.74769271849235	11362
dff85302b85b63843ba53914114268df313e4968	barriers and facilitators that affect public engagement with ehealth services.	subjects outside of the university themes;health and wellbeing;rt nursing;qa075 electronic computers computer science	It is commonly accepted that public engagement with eHealth is beneficial. However, engagement is also variable. This article presents the findings of a review of published evaluation studies around eHealth services. A targeted search of MEDLINE, CINAHL and EMBASE returned 2622 unique abstracts. 50 articles met the inclusion criteria and were subjected to further analysis. 6 review articles were used for post hoc validation. Four main types of eHealth service or resource were identified: health information on the Internet; custom-built online health information; online support; and telehealth. 5 key themes emerged in terms of facilitators or barriers to engagement: characteristics of users; technological issues; characteristics of eHealth services; content issues; social aspects of use; and eHealth services in use. Recommendations arising from the review include: targeting efforts to engage those underserved by eHealth; maximizing exposure to eHealth across all sections of society; improving access to computers and the internet; appropriate design and delivery; ensuring content is relevant to different audiences; capitalizing on the interest in social computing; and clarifying the role of health workers in the delivery of eHealth.		Nicholas R. Hardiker;Maria J. Grant	2010	Studies in health technology and informatics	10.3233/978-1-60750-588-4-13	public relations;medicine;ehealth;multimedia;world wide web	HCI	-62.25948655760199	-60.32694758946888	11367
4137943e01a55df21b71ffff4d5416f029e826f6	spam mail filtering through dynamically updating url statistics.	spam filtering	This paper presents a unique spam mail filtering technique based on a deep analysis of statistics on URL’s included in various e-mails gathered from a laboratory in a university for about six months. Since the proposed mail filtering technique searches only URL’s in mail, the overhead introduced by searching all mail contents or black list utilized by many other mail filtering algorithms is significantly reduced. In addition, the proposed filtering technique dynamically updates URL list through client feedback, and the bias possibly introduced by selecting bad training mail set can be eliminated as the filtering process is progressed.	algorithm;email;email filtering;moe;overhead (computing);server (computing);spamming	Jangbok Kim;Kyunghee Choi;Gihyun Jung	2005			spam and open relay blocking system;internet privacy;world wide web;information retrieval	Networks	-29.780926667857592	-55.26371730490818	11392
848f9491045d768d62b6784473ba4ea5a3d744bc	domain adaptation with topical correspondence learning	domain adaptation;topical models;non negative matrix factorization	A serious and ubiquitous issue in machine learning is the lack of sufficient training data in a domain of interest. Domain adaptation is an effective approach to dealing with this problem by transferring information or models learned from related, albeit distinct, domains to the target domain. We develop a novel domain adaptation method for text document classification under the framework of Nonnegative Matrix Factorization. Two key ideas of our method are to construct a latent topic space where a topic is decomposed into common words shared by all domains and words specific to individual domains, and then to establish associations between words in different domains through the common words as a bridge for knowledge transfer. The correspondence between cross-domain topics leads to more coherent distributions of source and target domains in the new representation while preserving the predictive power. Our new method outperformed several state-of-the-art domain adaptation methods on several benchmark datasets.	benchmark (computing);coherence (physics);document classification;domain adaptation;machine learning;non-negative matrix factorization	Zheng Chen;Weixiong Zhang	2013			speech recognition;computer science;problem domain;machine learning;non-negative matrix factorization	AI	-17.098238872187963	-66.50378938146923	11456
b7f2258e9f0ad776a3745f8fbfc5be010f4f2ccf	a unified and discriminative model for query refinement	generic model;query refinement;conditional random fields;error correction;conditional random field;web search;discriminative model	This paper addresses the issue of query refinement, which involves reformulating ill-formed search queries in order to enhance relevance of search results. Query refinement typically includes a number of tasks such as spelling error correction, word splitting, word merging, phrase segmentation, word stemming, and acronym expansion. In previous research, such tasks were addressed separately or through employing generative models. This paper proposes employing a unified and discriminative model for query refinement. Specifically, it proposes a Conditional Random Field (CRF) model suitable for the problem, referred to as Conditional Random Field for Query Refinement (CRF-QR). Given a sequence of query words, CRF-QR predicts a sequence of refined query words as well as corresponding refinement operations. In that sense, CRF-QR differs greatly from conventional CRF models. Two types of CRF-QR models, namely a basic model and an extended model are introduced. One merit of employing CRF-QR is that different refinement tasks can be performed simultaneously and thus the accuracy of refinement can be enhanced. Furthermore, the advantages of discriminative models over generative models can be fully leveraged. Experimental results demonstrate that CRF-QR can significantly outperform baseline methods. Furthermore, when CRF-QR is used in web search, a significant improvement of relevance can be obtained.	baseline (configuration management);conditional random field;discriminative model;error detection and correction;generative model;mathematical model;query optimization;refinement (computing);relevance;stemming;text segmentation;web search engine;web search query	Jiafeng Guo;Gu Xu;Hang Li;Xueqi Cheng	2008		10.1145/1390334.1390400	natural language processing;query optimization;query expansion;computer science;machine learning;pattern recognition;data mining;conditional random field;information retrieval;discriminative model	Web+IR	-21.774748924735494	-73.63840624879224	11476
544fa241fa6e6743191a4644358c0fb2a5ce153d	semi-supervised bootstrapping approach for neural network feature extractor training	statistical analysis feature extraction hidden markov models learning artificial intelligence neural nets;neural nets;bottle neck features semi supervised training bootstrapping;training artificial neural networks hidden markov models feature extraction accuracy training data labeling;hidden markov models;statistical analysis;feature extraction;learning artificial intelligence;automatically transcribed segments semisupervised bootstrapping approach neural network feature extractor training neural network training bottle neck feature extractor gmm hmm recognizer transcription confidence assignment untranscribed data supervised data	This paper presents bootstrapping approach for neural network training. The neural networks serve as bottle-neck feature extractor for subsequent GMM-HMM recognizer. The recognizer is also used for transcription and confidence assignment of untranscribed data. Based on the confidence, segments are selected and mixed with supervised data and new NNs are trained. With this approach, it is possible to recover 40-55% of the difference between partially and fully transcribed data (3 to 5% absolute improvement over NN trained on supervised data only). Using 70-85% of automatically transcribed segments with the highest confidence was found optimal to achieve this result.	artificial neural network;bootstrapping (compilers);finite-state machine;hidden markov model;randomness extractor;semi-supervised learning;semiconductor industry;supervised learning;transcription (software)	Frantisek Grézl;Martin Karafiát	2013	2013 IEEE Workshop on Automatic Speech Recognition and Understanding	10.1109/ASRU.2013.6707775	speech recognition;feature extraction;computer science;machine learning;pattern recognition;artificial neural network;hidden markov model	ML	-19.103900312207053	-89.20147589477612	11478
6e68fdfbae281253d093a6b9f7947a80ec6b36be	can a more neutral position and support of the forearms at the table top reduce pain for vdu operators. laboratory and field studies	field study			Arne Aarås;Ola Ro;Gunnar Horgen	1999			simulation;computer science;field research	HCI	-55.71219097365875	-57.91694924943459	11488
754493032bd101a77830181cf9059b6383c17f26	tactile morse code using locational stimulus identification		This research investigated several haptic interfaces designed to reduce mistakes in Morse code reception. Results concluded that a bimanual setup, discriminating dots/dashes by left/right location, reduced the amount of errors to only 56.6 percent of the errors compared to a unimanual setup that used temporal discrimination to distinguish dots and dashes.	haptic device component;haptic technology;percent (qualifier value);preparation	Michael Walker;Kyle B. Reed	2018	IEEE Transactions on Haptics	10.1109/TOH.2017.2743713	morse code;visualization;computer vision;computer science;judgement;artificial intelligence;perception;haptic technology	Visualization	-6.79496854385787	-81.64788629509701	11490
18c69cd43108c697faf3d5c431ac9da798dabded	ml-net: multi-label classification of biomedical texts with deep neural networks		In multi-label text classification, each textual document can be assigned with one or more labels. Due to this nature, the multi-label text classification task is often considered to be more challenging compared to the binary or multi-class text classification problems. As an important task with broad applications in biomedicine such as assigning diagnosis codes, a number of different computational methods (e.g. training and combining binary classifiers for each label) have been proposed in recent years. However, many suffered from modest accuracy and efficiency, with only limited success in practical use. We propose ML-Net, a novel deep learning framework, for multi-label classification of biomedical texts. As an end-to-end system, ML-Net combines a label prediction network with an automated label count prediction mechanism to output an optimal set of labels by leveraging both predicted confidence score of each label and the contextual information in the target document. We evaluate ML-Net on three independent, publicly-available corpora in two kinds of text genres: biomedical literature and clinical notes. For evaluation, example-based measures such as precision, recall and f-measure are used. ML-Net is compared with several competitive machine learning baseline models. Our benchmarking results show that ML-Net compares favorably to the state-of-the-art methods in multi-label classification of biomedical texts. ML-NET is also shown to be robust when evaluated on different text genres in biomedicine. Unlike traditional machine learning methods, ML-Net does not require human efforts in feature engineering and is highly efficient and scalable approach to tasks with a large set of labels (no need to build individual classifiers for each separate label). Finally, ML-NET is able to dynamically estimate the label count based on the document context in a more systematic and accurate manner.	artificial neural network;deep learning;informatics;multiclass classification;netware loadable module;yang	Jingcheng Du;Qingyu Chen;Yifan Peng;Yang Xiang;Cui Tao;Zhiyong Lu	2018	CoRR		computer science;diagnosis code;data mining;multi-label classification;benchmarking;scalability;artificial neural network;deep learning;feature engineering;binary number;artificial intelligence;pattern recognition	ML	-19.512693495221704	-71.45154173026431	11517
ec5a0718d3f820e73c7b51101c7e409a7f2edd11	hypothetical outcome plots help untrained observers judge trends in ambiguous data		Animated representations of outcomes drawn from distributions (hypothetical outcome plots, or HOPs) are used in the media and other public venues to communicate uncertainty. HOPs greatly improve multivariate probability estimation over conventional static uncertainty visualizations and leverage the ability of the visual system to quickly, accurately, and automatically process the summary statistical properties of ensembles. However, it is unclear how well HOPs support applied tasks resembling real world judgments posed in uncertainty communication. We identify and motivate an appropriate task to investigate realistic judgments of uncertainty in the public domain through a qualitative analysis of uncertainty visualizations in the news. We contribute two crowdsourced experiments comparing the effectiveness of HOPs, error bars, and line ensembles for supporting perceptual decision-making from visualized uncertainty. Participants infer which of two possible underlying trends is more likely to have produced a sample of time series data by referencing uncertainty visualizations which depict the two trends with variability due to sampling error. By modeling each participant's accuracy as a function of the level of evidence presented over many repeated judgments, we find that observers are able to correctly infer the underlying trend in samples conveying a lower level of evidence when using HOPs rather than static aggregate uncertainty visualizations as a decision aid. Modeling approaches like ours contribute theoretically grounded and richly descriptive accounts of user perceptions to visualization evaluation.		Alex M. Kale;Francis Nguyen;Matthew Kay;Jessica Hullman	2018	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2018.2864909	data visualization;error bar;sampling error;evidence-based medicine;time series;task analysis;theoretical computer science;visualization;multivariate statistics;machine learning;computer science;artificial intelligence	Visualization	-48.093247595024614	-53.7920322931422	11529
85e313c9f7de364ed423df5eadd38802017a5f59	i want to believe: journalists and crowdsourced accuracy assessments in twitter		"""Evaluating information accuracy in social media is an increasingly important and well-studied area, but limited research has compared journalist-sourced accuracy assessments to their crowdsourced counterparts. This paper demonstrates the differences between these two populations by comparing the features used to predict accuracy assessments in two Twitter data sets: CREDBANK and PHEME. While our findings are consistent with existing results on feature importance, we develop models that outperform past research. We also show limited overlap exists between the features used by journalists and crowdsourced assessors, and the resulting models poorly predict each other but produce statistically correlated results. This correlation suggests crowdsourced workers are assessing a different aspect of these stories than their journalist counterparts, but these two aspects are linked in a significant way. These differences may be explained by contrasting factual with perceived accuracy as assessed by expert journalists and non-experts respectively. Following this outcome, we also show preliminary results that models trained from crowdsourced workers outperform journalist-trained models in identifying highly shared """"fake news"""" stories."""	crowdsourcing	Cody Buntain;Jennifer Golbeck	2017	CoRR		data mining;social psychology	NLP	-38.0228926970269	-54.667800460136625	11530
008b0134b1d0e4ef81f943d8fff78baf6bdba62a	probabilistic model of two-dimensional rhythm tree structure representation for automatic transcription of polyphonic midi signals	bayes methods;rhythm production vocabulary estimation speech recognition hidden markov models multiple signal classification;tree data structures;bayesian approach parameter inference algorithm 2 dimensional rhythm tree structure representation note onset positions musically natural tempo curve generative process modeling tempo variations rhythm patterns beat location tempo estimation rhythm recognition musical notes generative onset occurrence modeling polyphonic midi signals automatic music transcription;tree data structures bayes methods music pattern recognition signal representation;signal representation;pattern recognition;music	This paper proposes a Bayesian approach for automatic music transcription of polyphonic MIDI signals based on generative modeling of onset occurrences of musical notes. Automatic music transcription involves two subproblems that are interdependent of each other: rhythm recognition and tempo estimation. When we listen to music, we are able to recognize its rhythm and tempo (or beat location) fairly easily even though there is ambiguity in determining the individual note values and tempo. This may be made possible through our empirical knowledge about rhythm patterns and tempo variations that possibly occur in music. To automate the process of recognizing the rhythm and tempo of music, we propose modeling the generative process of a MIDI signal of polyphonic music by combining the sub-process by which a musically natural tempo curve is generated and the sub-process by which a set of note onset positions is generated based on a 2-dimensional rhythm tree structure representation of music, and develop a parameter inference algorithm for the proposed model. We show some of the transcription results obtained with the present method.	algorithm;experiment;generative modelling language;generative model;interdependence;midi;onset (audio);statistical model;transcription (software);tree structure;vocabulary	Masato Tsuchiya;Kazuki Ochiai;Hirokazu Kameoka;Shigeki Sagayama	2013	2013 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference	10.1109/APSIPA.2013.6694308	speech recognition;computer science;pattern recognition;communication	ML	-16.01127531670761	-83.79182769044093	11531
0adbde16c9530d457a058356534852f8135c5968	computational intelligence for condition monitoring	hidden markov model;computational intelligence;mel frequency cepstral coefficient;gaussian mixture model;condition monitoring;feature extraction;support vector machine;neural network	Condition monitoring techniques are described in this chapter. Two aspects of condition monitoring process are considered: (1) feature extraction; and (2) condition classification. Feature extraction methods described and implemented are fractals, Kurtosis and Melfrequency Cepstral Coefficients. Classification methods described and implemented are support vector machines (SVM), hidden Markov models (HMM), Gaussian mixture models (GMM) and extension neural networks (ENN). The effectiveness of these features were tested using SVM, HMM, GMM and ENN on condition monitoring of bearings and are found to give good results.	artificial neural network;cepstrum;coefficient;computation;computational intelligence;feature extraction;fractal;google map maker;hidden markov model;markov chain;mixture model;statistical classification;support vector machine	Tshilidzi Marwala;Christina B. Vilakazi	2007	CoRR		support vector machine;speech recognition;feature extraction;computer science;machine learning;computational intelligence;pattern recognition;mixture model;artificial neural network;hidden markov model	ML	-7.031091863562936	-90.3173521722391	11533
5b14f8aa412c915f056a8250a4dd33c32f438f0b	a trainable classifier via k nearest neighbors	k nearest neighbor		k-nearest neighbors algorithm	I. Mora-Jiménez;Abdelouahid Lyhyaoui;Jerónimo Arenas-García;Ángel Navia-Vázquez;Aníbal R. Figueiras-Vidal	2002			nearest-neighbor chain algorithm;mathematics;artificial intelligence;k-nearest neighbors algorithm;classifier (linguistics);pattern recognition	Vision	-15.495575938399496	-87.41994315993296	11552
13519851ed6d9739f3b914bddb0d88c65c5bb90b	an intelligent discussion-bot for answering student queries in threaded discussions	discussion board;answer extraction;information retrieval;threaded discussion;natural language processing;discussion bot;online learning environment	This paper describes a discussion-bot that provides answers to students' discussion board questions in an unobtrusive and human-like way. Using information retrieval and natural language processing techniques, the discussion-bot identifies the questioner's interest, mines suitable answers from an annotated corpus of 1236 archived threaded discussions and 279 course documents and chooses an appropriate response. A novel modeling approach was designed for the analysis of archived threaded discussions to facilitate answer extraction. We compare a self-out and an all-in evaluation of the mined answers. The results show that the discussion-bot can begin to meet students' learning requests. We discuss directions that might be taken to increase the effectiveness of the question matching and answer extraction algorithms. The research takes place in the context of an undergraduate computer science course.	algorithm;archive;computer science;information retrieval;mined;natural language processing;unobtrusive javascript	Donghui Feng;Erin Shaw;Jihie Kim;Eduard H. Hovy	2006		10.1145/1111449.1111488	natural language processing;human–computer interaction;computer science;artificial intelligence;machine learning;data mining;world wide web;information retrieval	NLP	-31.99370378136966	-61.064391969969726	11566
064b1653607883e587742f3f5eac852be4c71e88	cross-validation based decision tree clustering for hmm-based tts	sufficient statistic;mdl criterion;pattern clustering;generalization error;generation error;decision tree;speech synthesis;training;mdl hmm based speech synthesis cross validation context clustering;speech;statistics cross validation decision tree clustering hmm based tts speech synthesis linguistically rich speech units mdl criterion generation error contexts training algorithm;training data;hmm based speech synthesis;hidden markov models;statistical analysis;statistical analysis decision trees hidden markov models pattern clustering speech synthesis;statistics;context dependent;linguistically rich speech units;cross validation;hmm based tts;decision trees;decision tree clustering;context;training algorithm;context clustering;data models;contexts;decision trees speech synthesis hidden markov models context modeling training data clustering algorithms asia statistics stress predictive models;mdl	In HMM-based speech synthesis, we usually use complex, context dependent models to characterize prosodically and linguistically rich speech units. It is therefore difficult to prepare training data which can cover all combinatorial possibilities of contexts. A common approach to cope with this insufficient training data problem is to build a clustered tree via the MDL criterion. However, an MDL-based tree still tends to be inadequate in its power to predict unseen data. In this paper, we adopt the cross-validation principle to build such a decision tree to minimize the generation error of unseen contexts. An efficient training algorithm is implemented by exploiting the sufficient statistics. Experimental results show that the proposed method can achieve better speech synthesis results, both objectively and subjectively, than the baseline results of the MDL-based decision tree.	algorithm;baseline (configuration management);cluster analysis;cross-validation (statistics);decision tree;hidden markov model;mdl (programming language);netware file system;speech synthesis	Yu Zhang;Zhi-Jie Yan;Frank K. Soong	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495560	speech recognition;computer science;machine learning;decision tree;pattern recognition;incremental decision tree;speech synthesis	Robotics	-19.719263757852165	-90.33222340549503	11600
1a735b4d8e5f3c7d5f61a810e4dd42aa08f235d5	correlating words - approaches and applications		The determination of characteristic and discriminating terms as well as their semantic relationships plays a vital role in text processing applications. As an example, term clustering techniques heavily rely on this information. Classic approaches for this means such as statistical cooccurrence analysis however usually only consider relationships between two terms that co-occur as immediate neighbours or on sentence level. This article presents flexible approaches to find statistically significant correlations between two or more terms using co-occurrence windows of arbitrary sizes. Their applicability will be discussed in detail by presenting solutions to improve the interactive and image-based search in the World Wide Web. Moreover, approaches to determine directed term associations and applications for them will be explained, too.		Mario Kubek;Herwig Unger;Jan Dusik	2015		10.1007/978-3-319-23192-1_3	computer science;machine learning;pattern recognition;artificial intelligence;cluster analysis;text processing;sentence	Web+IR	-27.27424549398208	-67.37998682829209	11612
7dffa56366440bb9fa7eb9e836af2bfd9dfaef23	visualising the past: annotating a life with linked open data	trust;data sharing;ws33 accessibility;decision networks;web science 2011;data quality	This paper introduces a novel lifelogging system. The novelty of our system is that it uses open linked data to reduce the burden on the user to an absolute minimum by combining the user's private logs with data obtained automatically from the web. This in turn is annotated with data generated by the latest version of the author's portable 'life annotation' software to generate a detailed, interactive, user-friendly visual representation of the user's life experiences with little or no manual work required. This paper covers several key parts of the system that make it work. Specifically, the analysis of linked data, entity comparison with known objects and events from the user's own logs in order to determine relevance, and careful annotation of the user's data ensuring that the data's provenance is stored alongside the annotations generated.	experience;lifelog;linked data;relevance;usability	Ashley Smith;Kieron O'Hara;Paul H. Lewis	2011		10.1145/2527031.2527038	computer science;data mining;world wide web;information retrieval	HCI	-44.33822245239814	-64.9913858967105	11630
3b8a6badf115c0aa1d661597f1dfe4ed60fea335	testing analogical proportions with google using kolmogorov information theory	information theory;conference proceeding	Analogical reasoning is considered as one of the main mechanisms underlying creativity. “Thinking out of the box” allows the paradigm shift essential to a creative process. More common is the concept of analogical proportion (“2 is to 4 as 4 is to 8”) which can be described within an algebraic framework. When it comes to concepts (“engine is to the car as heart is to the human”), we need to investigate a new way to understand this analogical ratio. In this paper, we take inspiration from the formal framework of information theory for proposing a new approach to the evaluation of analogy between concepts. Using Kolmogorov complexity as a backbone providing a clear semantics, we give a practical interpretation for analogy between words viewed as labeling concepts. Making use of Google as a linguistic resource, we provide an implementation of our definitions: experiments show that the accuracy of our definition is quite acceptable and justify the approach.	analogical modeling;distortion;experiment;information theory;internet backbone;knowledge base;kolmogorov complexity;mutual information;out of the box (feature);programming paradigm;relevance;signal-to-noise ratio;thinking outside the box;web search engine;world wide web	Henri Prade;Gilles Richard	2009			computer science;artificial intelligence;statistics	AI	-36.13911377149686	-70.4526515496707	11634
844e7d31c3177f51b40071005e9c3ff206dec9e3	what a difference a tag cloud makes: effects of tasks and cognitive abilities on search results interface use	cognitive style;short term memory;human computer interaction;cognitive ability;multivariate analysis;individual differences;information science;multivariate analysis of variance;search engines;information retrieval;repeated measures;virtual classrooms;ls search engines;informacion documentacion;lk software methodologies and engineering;internet;mixed model;search strategies;grupo a;computer interfaces;ciencias sociales;clickthrough data;grupo b;cb user studies;memory	Introduction. The goal of this study is to expand our understanding of the relationships between selected tasks, cognitive abilities and search result interfaces. The underlying objective is to understand how to select search results presentation for tasks and user contexts Method. Twenty three participants conducted four search tasks of two types and used two interfaces (List and Overview) to refine and examine search results. Clickthrough data were recorded. This controlled study employed a mixed model design with two within-subject factors (task and interface) and two between-subject factors (two cognitive abilities: memory span and verbal closure). Analysis. Quantitative analyses were carried out by means of the statistical package SPSS. Specifically, multivariate analysis of variance with repeated measures and non-parametric tests were performed on the collected data. Results.The overview of search results appeared to have benefited searchers in several ways. It made them faster; it facilitated formulation of more effective queries and helped to assess search results. Searchers with higher cognitive abilities were faster in the Overview interface and in less demanding situations (on simple tasks), while at the same time they issued about the same number of queries as lower-ability searchers. In more demanding situations (on complex tasks and in the List interface), the higher ability searchers expended more search effort, although they were not significantly slower than the lower ability people in these situations. The higher search effort, however, did not result in a measurable improvement of task outcomes for high-ability searchers. Conclusions. These findings have implications for the design of search interfaces. They suggest benefits of providing result overviews. They also suggest the importance of considering cognitive abilities in the design of search results' presentation and interaction. Abstract	cognition;list of statistical packages;mixed model;spss;tag cloud	Jacek Gwizdka	2009	Inf. Res.		multivariate analysis of variance;psychology;library science;mixed model;repeated measures design;the internet;simulation;cognition;cognitive style;information science;computer science;artificial intelligence;data mining;mathematics;short-term memory;multivariate analysis;memory;world wide web;statistics	HCI	-37.70671702854356	-53.03614003528885	11641
c58c6b68ad74691563504d0933dbd8584605f050	detecting dementia through retrospective analysis of routine blog posts by bloggers with dementia		We investigate if writers with dementia can be automatically distinguished from those without by analyzing linguistic markers in written text, in the form of blog posts. We have built a corpus of several thousand blog posts, some by people with dementia and others by people with loved ones with dementia. We use this dataset to train and test several machine learning methods, and achieve prediction performance at a level far above the base-	baseline (configuration management);blog;cluster analysis;machine learning;preprocessor;text corpus;vocabulary	Vaden Masrani;Gabriel Murray;Thalia Shoshana Field;Giuseppe Carenini	2017		10.18653/v1/W17-2329	multimedia;dementia;medicine	NLP	-20.508023834679957	-59.16249529025508	11661
c7aae96b52602b819e15e6a08544e457f2e3251d	comparing encounter and demographics data elements among different healthcare enterprises using a common data dictionary	biomedical research;bioinformatics		data dictionary	Lee Min Lau;Siew Hong Lam;Stanley M. Huff	1998			health care;data science;data dictionary;demographics;text mining;computer science	ML	-54.802792913209906	-64.53832497255274	11702
0025b963134b1c0b64c1389af19610d038ab7072	learning to order things	search engine;greedy algorithm;query expansion;domain specificity;on line learning;learning preference	"""There are many applications in which it is desirable to order rather than classify instances. Here we consider the problem of learning how to order, given feedback in the form of preference judgments, i.e., statements to the effect that one instance should be ranked ahead of another. We outline a two-stage approach in which one first learns by conventional means a preference Junction, of the form PREF( u, v), which indicates whether it is advisable to rank u before v. New instances are then ordered so as to maximize agreements with the learned preference function. We show that the problem of finding the ordering that agrees best with a preference function is NP-complete, even under very restrictive assumptions. Nevertheless, we describe a simple greedy algorithm that is guaranteed to find a good approximation. We then discuss an on-line learning algorithm, based on the """"Hedge"""" algorithm, for finding a good linear combination of ranking """"experts."""" We use the ordering algorithm combined with the on-line learning algorithm to find a combination of """"search experts,"""" each of which is a domain-specific query expansion strategy for a WWW search engine, and present experimental results that demonstrate the merits of our approach."""	approximation;emoticon;feedback;greedy algorithm;np-completeness;online and offline;online machine learning;query expansion;www;web search engine	William W. Cohen;Robert E. Schapire;Yoram Singer	1997	J. Artif. Intell. Res.	10.1613/jair.587	instance-based learning;preference learning;greedy algorithm;query expansion;computer science;artificial intelligence;machine learning;data mining;mathematics;search engine	ML	-27.53499792718782	-53.6744749315179	11719
a013daed0d82666add23ca0761ffd545bd917a96	computer aided intelligent breast cancer detection: second opinion for radiologists - a prospective study			prospective search	J. Dheeba;N. Albert Singh	2015		10.1007/978-3-319-11017-2_16	gynecology;oncology;multimedia	Robotics	-56.57312474507355	-64.80109215610514	11727
cfef297978c3632ff3412aba6f701dd9b969d214	matching patient and physician preferences in designing a primary care facility network	access;physician allocation;journal of the operational research society;physician preference;turkey;article;public facility location	This paper introduces an integer programming model for planning primary care facility networks, which accounts for the interests of different stakeholders while maximizing access to health care. Physician allocation to health-care facilities is explicitly modelled, which allows consideration of physician incentives in the planning phase. An illustrative case study in the Turkish primary care system is presented to show the implications of focusing on patient or physician preferences in the planning phase. A discussion of trade-offs between the different stakeholder preferences and some recommendations for modelling choices to match these preferences are provided. In the context of this case, we found that using an access measure that decays with distance, and incorporating nearest allocation constraints improves performance for all stakeholders. We also show that increasing the number of physicians may have adverse affects on access measures when physician preferences are addressed. Journal of the Operational Research Society (2014) 65, 483–496. doi:10.1057/jors.2012.71 Published online 4 July 2012	integer programming;programming model	Evrim Didem Günes;H. Yaman;Bora Çekyay;Vedat Verter	2014	JORS	10.1057/jors.2012.71	knowledge management	AI	-61.783962345073185	-63.75719905599948	11729
536bb0ad8aceed1e651f34c9598213a88a34e573	a multilingual phonetic representation and analysis system for different speech databases	speech processing;natural languages;speech synthesis;database management systems;signal analysis;speech recognition;lattices;labeling;signal processing	A multilingual phonetic representation and analysis system for different speech databases is presented. The need for such a system is first justified and then one is proposed based on the Worldbet phonetic alphabet. A phonetic class hierarchy is developed and a description of the hierarchical structural representation follows. Database access is based on the latter and is accomplished by defining predicate search functions and applying them to a database. Immediate signal analysis of the results is possible since the multilingual phonetic representation system is seamlessly integrated into a digital signal processing environment.	class hierarchy;database;digital signal processing	Toomas Altosaar;Matti Karjalainen;Martti Vainio	1996			natural language processing;speech recognition;speech corpus;computer science;phonetic search technology;speech processing;linguistics;speech synthesis;transcription	DB	-21.449092150144786	-83.22910521129837	11742
978a4d806d2a79c8186f1a97ca312049c749a400	patients screening for clinical trials using ehr representation similarities		Introduction Randomized controlled trials generate quality medical evidence but suffer from longstanding recruitment problems [1]. Existing solutions mostly focus on eligibility criteria processing to facilitate trial search by patients [2, 3] or patient search using electronic health records (EHR) by investigators [4]. An untapped opportunity is using “casebased reasoning” to identify patients similar to known trial participants. This study tested the feasibility of this idea.	case-based reasoning;randomized algorithm	Riccardo Miotto;Chunhua Weng	2014			physical therapy;clinical trial;alternative medicine;medicine	AI	-57.14948230690575	-65.58575191800018	11790
b02f3cd16bc85b0cfc083f8295882fe385a8da50	automatic identification of mild cognitive impairment through the analysis of italian spontaneous speech productions		This paper presents some preliminary results of the OPLON project. It aimed at identifying early linguistic symptoms of cognitive decline in the elderly. This pilot study was conducted on a corpus composed of spontaneous speech sample collected from 39 subjects, who underwent a neuropsychological screening for visuo-spatial abilities, memory, language, executive functions and attention. A rich set of linguistic features was extracted from the digitalised utterances (at phonetic, suprasegmental, lexical, morphological and syntactic levels) and the statistical significance in pinpointing the pathological process was measured. Our results show remarkable trends for what concerns both the linguistic traits selection and the automatic classifiers building.	automatic identification and data capture;lexicon;spontaneous order;text corpus	Daniela Beltrami;Laura Calzà;Gloria Gagliardi;Enrico Ghidoni;Norina Marcello;Rema Rossini Favretti;Fabio Tamburini	2016			speech recognition;artificial intelligence;natural language processing;computer science;cognition	NLP	-13.582067918171788	-81.07974555346165	11810
e308df1d6cccd31544a1e20497a4fba3990af19c	towards semi-automatic generation of proposition banks for low-resource languages		Annotation projection based on parallel corpora has shown great promise in inexpensively creating Proposition Banks for languages for which high-quality parallel corpora and syntactic parsers are available. In this paper, we present an experimental study where we apply this approach to three languages that lack such resources: Tamil, Bengali and Malayalam. We find an average quality difference of 6 to 20 absolute F-measure points vis-avis high-resource languages, which indicates that annotation projection alone is insufficient in low-resource scenarios. Based on these results, we explore the possibility of using annotation projection as a starting point for inexpensive data curation involving both experts and non-experts. We give an outline of what such a process may look like and present an initial study to discuss its potential and challenges.	data curation;digital curation;experiment;microsoft outlook for mac;parallel text;parsing;propbank;semiconductor industry;text corpus	Alan Akbik;Vishwajeet Kumar;Yunyao Li	2016			natural language processing;artificial intelligence;computer science;machine learning;proposition	NLP	-29.661169812503804	-73.45928793550843	11827
e8628c6b1338fa3033ce1bcc0359be2443e777f5	text content reliability estimation in web documents: a new proposal	text reliability;content based trust;textual entailment	This paper illustrates how a combination of information retrieval, machine learning, and NLP corpus annotation techniques was applied to a problem of text content reliability estimation in Web documents. Our proposal for text content reliability estimation is based on a model in which reliability is a similarity measure between the content of the documents and a knowledge corpus. The proposal includes a new representation of text which uses entailment-based graphs. Then we use the graph-based representations as training instances for a machine learning algorithm allowing to build a reliability model. Experimental results illustrate the feasibility of our proposal by performing a comparison with a state-of-the-art method.	algorithm;information retrieval;machine learning;natural language processing;similarity measure	Luis Sanz;Héctor Allende;Marcelo Mendoza	2012		10.1007/978-3-642-28601-8_37	natural language processing;text graph;textual entailment;computer science;data mining;linguistics;information retrieval	NLP	-26.919287985649426	-65.17593589593042	11834
3673fac89e13c5ece733d44c30834f32698950ba	evaluation of french and english mesh indexing systems with a parallel corpus	knowledge bases;medical subject headings;abstracting and indexing as topic;algorithms;language;automatic data processing;natural language processing	OBJECTIVE This paper presents the evaluation of two MeSH indexing systems for French and English on a parallel corpus.   MATERIAL AND METHODS We describe two automatic MeSH in-dexing systems - MTI for English, and MAIF for French. The French version of the evaluation resources has been manually indexed with MeSH keyword/qualifier pairs. This professional indexing is used as our gold standard in the evaluation of both systems on keyword retrieval.   RESULTS The English system (MTI) obtains significantly better precision and recall (78% precision and 21% recall at rank 1, vs. 37%. precision and 6% recall for MAIF ). Moreover, the performance of both systems can be optimised by the break-age function used by the French system (MAIF), which selects an adaptive number of descriptors for each resource indexed.   CONCLUSION MTI achieves better performance. However, both systems have features that can benefit each other.	body of uterus;computer performance;email;index;indexes;keyword;large;linguistics;moving target indication;parallel text;precision and recall	Aurélie Névéol;James G. Mork;Alan R. Aronson;Stéfan Jacques Darmoni	2005	AMIA ... Annual Symposium proceedings. AMIA Symposium		natural language processing;computer science;database;information retrieval	NLP	-33.206195613722265	-64.46483236666229	11836
467c0eb72d56a175e176200fcae0df96f39954c0	speech recognition using hmm with decreased intra-group variation in the temporal structure	speech recognition		hidden markov model;speech recognition	Keikichi Hirose	1994			speech recognition;artificial intelligence;pattern recognition;hidden markov model;computer science	Vision	-13.305884617167294	-88.130978148006	11885
7ea683ee15952f1515eb0b10cb7618c9f98f9927	towards comprehensive computational representations of arabic multiword expressions		A successful computational treatment of multiword expressions (MWEs) in natural languages leads to a robust NLP system which considers the long-standing problem of language ambiguity caused primarily by this complex linguistic phenomenon. The first step in addressing this challenge is building an extensive reliable MWEs language resource LR with comprehensive computational representations across all linguistic levels. This forms the cornerstone in understanding the heterogeneous linguistic behaviour of MWEs in their various manifestations. This paper presents a detailed framework for computational representations of Arabic MWEs (ArMWEs) across all linguistic levels based on the state-of-the-art lexical mark-up framework (LMF) with the necessary modifications to suit the distinctive properties of Modern Standard Arabic (MSA). This work forms part of a larger project that aims to develop a comprehensive computational lexicon of ArMWEs for NLP and language pedagogy LP (JOMAL project).		Ayman Alghamdi;Eric Atwell	2017		10.1007/978-3-319-69805-2_29	modern standard arabic;natural language processing;linguistics;natural language;ambiguity;lexicon;language pedagogy;expression (mathematics);phenomenon;computer science;artificial intelligence;annotation	NLP	-30.003401963837046	-74.27208955195152	11927
66e80e3db6f1f16a4dd8b78e04d2367998e096c8	the limsi continuous speech dictation system: evaluation on the arpa wall street journal task	graph search;duration model;graph theory;arpa wall street journal task;trigram language model;time synchronous graph search;gaussian mixture;wall street journal;acoustic analysis;limsi continuous speech dictation system;nov93 arpa tests;continuous density hmm;gaussian processes;second forward pass;acoustic modeling;speech analysis;vocabulary;language modeling;context dependent phone models;sex dependent models;nov93 arpa tests limsi continuous speech dictation system arpa wall street journal task speaker independent speech dictation large vocabulary csr corpus continuous density hmm gaussian mixture acoustic modeling newspaper texts language modeling time synchronous graph search bigram back off language models second forward pass word graph trigram language model cepstrum based features context dependent phone models phone duration models sex dependent models nov92 arpa tests;csr corpus;materials testing;natural languages;time synchronization;cepstrum based features;training data;system evaluation;dictation;smoothing methods;hidden markov models;large vocabulary;speaker independent;speech recognition equipment;speech analysis hidden markov models context modeling vocabulary speech recognition training data text recognition statistics materials testing smoothing methods;newspaper texts;statistics;speaker independent speech dictation;speech recognition;nov92 arpa tests;natural languages dictation speech recognition speech recognition equipment hidden markov models gaussian processes acoustic analysis graph theory search problems;context dependent;search problems;phone duration models;text recognition;bigram back off language models;context modeling;statistical estimation;language model;word graph	In this paper we report progress made at LIMSI in speakerindependent large vocabulary speech dictation using the ARPA Wall Street Journal-based CSR corpus. The recognizer makes use of continuous density HMM with Gaussian mixture for acoustic modeling and n-gram statistics estimated on the newspaper texts for language modeling. The recognizer uses a time-synchronous graph-search strategy which is shown to still be viable with vocabularies of up to 20K words when used with bigram back-off language models. A second forward pass, which makes use of a word graph generated with the bigram, incorporates a trigram language model. Acoustic modeling uses cepstrum-based features, context-dependent phone models (intra and interword), phone duration models, and sex-dependent models. The recognizer has been evaluated in the Nov92 and Nov93 ARPA tests for vocabularies of	acoustic cryptanalysis;acoustic model;bigram;cepstrum;context-sensitive language;finite-state machine;hidden markov model;language model;n-gram;the wall street journal;trigram;vocabulary	Jean-Luc Gauvain;Lori Lamel;Gilles Adda;Martine Adda-Decker	1994		10.1109/ICASSP.1994.389233	natural language processing;training set;speech recognition;computer science;context-dependent memory;gaussian process;context model;natural language;statistics;language model	NLP	-20.478402868488665	-86.94254723656486	11948
5e191b0fa92089c3097358e4d245ec85c45106f0	advanced sentiment classification of tibetan microblogs on smart campuses based on multi-feature fusion		Sentiment analysis is an important problem in natural language processing, which plays an important role in many fields, such as information forecasting, knowledge classification, and product review. Because Tibetan microblogs have their own unique form, particularly the heterogeneous features, such as the emoticons, the grammatical relations, and the speech, the existing sentiment analysis method has difficulty in analyzing the emotions that such microblogs express. In this paper, we propose a sentiment classification method for Tibetan microblogs based on multi-feature fusion. To better study the affection of affective features, this paper first determines the theme of Weibo texts and chooses smart campuses as theme of Weibo texts for analyzing the influence of each feature on the sentiment of the microblog. Then, these features are fused as a multi-feature, and the sentiment of the Tibetan microblog is classified according to the multi-feature fusion. The experimental results demonstrated that the sentiment classification algorithm based on feature fusion improved the accuracy of microblog sentiment classification.	algorithm;emoticon;natural language processing;sentiment analysis;smart tv	Lirong Qiu;Qiao Lei;Zhen Zhang	2018	IEEE Access	10.1109/ACCESS.2018.2820163	distributed computing;data mining;fusion;semantics;feature extraction;computer science;microblogging;social media;sentiment analysis	NLP	-22.426549755358298	-57.75393115636561	11951
c664e81f4591dcd7c339db5447a6ccbf83b9583b	semi-supervised policy recommendation for online social networks	authorisation;data privacy;human computer interaction;iterative methods;learning (artificial intelligence);network theory (graphs);recommender systems;social networking (online);facebook;access control;active learning;automated privacy policy setting;fine grained policy recommendation system;human computer interaction;information leakage;information sharing;information uploading;iterative semi-supervised learning approach;online social network site;semi-supervised policy recommendation;social graph;training set;user privacy management;active learning;graph-based propagation;policy recommendation;semi-supervised learning	Fine-grained policy settings in social networking sites are becoming important for managing user privacy. Incorrect privacy policy settings can easily lead to leaks in private and personal information. At the same time, being too restrictive would reduce the benefits of online social networks. This is further complicated due to the growing adoption of social networks and the rapid growth in information uploading and sharing. The problem of facilitating policy settings has attracted the attention of numerous access control, and human–computer interaction researchers. The proposed solutions range from usable interfaces for policy settings to automated policy settings. We propose a fine-grained policy recommendation system that is based on an iterative semi-supervised learning approach which leverages the social graph propagation properties. Active learning and social graph properties are used to detect the most informative instances to be labeled as training sets. We implemented and tested our approach using both participant-labeled Facebook dataset and their real policy dataset extracted using the Facebook API. We compared our proposed approach to supervised learning and random walk-based approaches. Our approach provided higher accuracy and precision for both datasets. Collaborative active learning further improved the performance of our approach. Moreover, the accuracy and precision of our approach were maintained with the addition of new friends in the social graph.	access control;active learning (machine learning);experiment;facebook messenger;graph property;human–computer interaction;iterative method;personally identifiable information;privacy policy;recommender system;semi-supervised learning;semiconductor industry;social graph;social network;software propagation;supervised learning;upload	Mohamed Shehab;Hakim Touati;Yousra Javed	2012	2012 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining	10.1007/s13278-016-0370-9	computer science;knowledge management;data mining;world wide web	ML	-19.503777629232488	-54.71570829511712	11960
d63850735fee07b166d625c5138a2b77da38dc53	linked disambiguated distributional semantic networks	004 informatik	We present a new hybrid lexical knowledge base that combines the contextual information of distributional models with the conciseness and precision of manually constructed lexical networks. The computation of our countbased distributional model includes the induction of word senses for single-word and multi-word terms, the disambiguation of word similarity lists, taxonomic relations extracted by patterns and context clues for disambiguation in context. In contrast to dense vector representations, our resource is human readable and interpretable, and thus can be easily embedded within the Semantic Web ecosystem. Resource type: Lexical Knowledge Base Permanent URL: https://madata.bib.uni-mannheim.de/id/eprint/171	computation;ecosystem;embedded system;focused crawler;human-readable medium;knowledge graph;knowledge base;semantic web;semantic network;word-sense disambiguation	Stefano Faralli;Alexander Panchenko;Christian Biemann;Simone Paolo Ponzetto	2016		10.1007/978-3-319-46547-0_7	natural language processing;computer science;pattern recognition;information retrieval	NLP	-29.381203624022927	-66.73729370271963	11962
a57a3a88499f3b6ecc74268de7584e70ef3450d7	a fast speaker indexing using vector quantization and second order statistics with adaptive threshold computation	indexation;adaptive thresholding	This paper describes an effective unsupervised speaker indexing approach. We suggest a two stage algorithm to speed-up the state-of-the-art algorithm based on the Bayesian Information Criterion (BIC). In the first stage of the merging process a computationally cheap method based on the vector quantization (VQ) is used. Then in the second stage a more computational expensive technique based on the BIC is applied. In the speaker indexing task a turning parameter or a threshold is used. We suggest an on-line procedure to define the value of a turning parameter without using development data. The results are evaluated using ESTER corpus.	algorithm;bayesian information criterion;computation;online and offline;pc speaker;vector quantization	Konstantin Biatov	2010			speech recognition;search engine indexing;order statistic;computer science;pattern recognition;computation;machine learning;vector quantization;thresholding;indexation;artificial intelligence	NLP	-17.434731638603854	-93.58380363407598	11995
20e0c70b36671bc83bf32f8953c03749a973e354	isa & ica - two web interfaces for interactive alignment of bitexts alignment of parallel texts		ISA and ICA are two web interfaces for interactive alignment of parallel texts. ISA provides an interface for automatic and manual sentence alignment. It includes cognate filters and uses structural markup to improve automatic alignment and provides intuitive tools for editing them. Alignment results can be saved to disk or sent via e-mail. ICA provides an interface to the clue aligner from the Uplug toolbox. It allows one to set various parameters and visualizes alignment results in a two-dimensional matrix. Word alignments can be edited and saved to disk.	email;independent computing architecture;markup language;sequence alignment;user interface	Jörg Tiedemann	2006			natural language processing;artificial intelligence;computer science;theoretical computer science	NLP	-8.266469298877785	-59.96693658358331	12000
70f8ef993a06614eb642ce81605d6e41898192e5	"""the """"honest broker"""" method of integrating interdisciplinary research data."""	information systems;michigan;heart diseases;medical records systems computerized;humans;medical record linkage;depression;biomedical research	"""Multiple clinical informatics systems have been developed within separate departments of the University of Michigan Medical School. We are in the process of creating an """"Honest Broker"""" method of safely and securely linking together data from different clinical systems for a research project studying the co-morbidity of depression and cardiovascular disease. The Michigan Clinical Research Collaboratory (MCRC) is an NIH/NHLBI Roadmap initiative funded to re-engineer the clinical research enterprise."""	cardiovascular diseases;depressive disorder;genetic annotation initiative;honest broker;informatics (discipline);morbidity - disease rate;secure communication;united states national institutes of health	Andrew Dallas Boyd;Dale A. Hunscher;Adam J. Kramer;Charles Hosner;Paul R. Saxman;Brian D. Athey;John F. Greden;Daniel J. Clauw	2005	AMIA ... Annual Symposium proceedings. AMIA Symposium		medicine;data science;data mining	Embedded	-53.75569183283925	-63.67045982232159	12006
f41a9cece252199dcaf3c71a34b62a59a6af376a	metamorphosed characters in dreams: constraints of conceptual structure and amount of theory of mind	categories;dreams;representation;theory of mind;content analysis;conceptual structure;substitutability;metamorphoses	Dream reports from 21 dreamers in which a metamorphosis of a person-like entity or animal occurred were coded for characters and animals and for inner states attributed to them (Theory of Mind). In myths and fairy tales, Kelly and Keil (1985) found that conscious beings (people, gods) tend to be transformed into entities nearby in the conceptual structure of Keil (1979). This also occurred in dream reports, but perceptual nearness seemed more important than conceptual nearness. In dream reports, most inanimate objects involved in metamorphoses with person-like entities were objects such as statues that ordinarily resemble people physically, and moreover represent people. A metamorphosis of a person-like entity or animal did not lead to an increase in the amount of Theory of Mind attribution. We propose that a character-line starts when a character enters a dream; properties and Theory of Mind attributions tend to be preserved along the line, regardless of whether, metamorphoses occur on it.		Richard Schweickert;Zhuangzhuang Xi	2010	Cognitive science	10.1111/j.1551-6709.2009.01082.x	psychology;cognitive psychology;philosophy;content analysis;computer science;mathematics;sociology;communication;social psychology;representation;cognitive science	AI	-7.387492486880457	-77.44315676749564	12026
4b9fe100958eab1ae7e4bf4e1736b1c6fa594bb0	smart glasses in nursing - situation change and further usages exemplified on a wound care application	nursing;evaluation studies as topic;information technology	Smart Glasses are a promising technology that can be leveraged to improve flexible service processes. Especially in the field of nursing where practitioners are facing complex tasks and challenges. Introducing such pervasive computing devices in service processes may have both positive and negative consequences. This leads us to the following research questions: How does the usage of the Smart Glass applications change the caring situation? Which ideas for future usage of Smart Glasses do nurses have? To answer these questions we followed a design science research approach to design a prototype for support of wound care management in nursing. We evaluated the prototype in a real life situation. Five nurses used the application in a real world setting to perform a wound documentation. Afterwards, we conducted semi-structured interviews with the nurses. The intent of the interviews was not only to get information on the current prototype, but to generate knowledge about dimensions of changing the caring situation which should be considered further. The nurses gave the application an overall positive evaluation. They stated that they would expect an improvement of the quality of the wound documentation when using the device. In addition, they mentioned a change in the communication structure with the patient would be necessary. Furthermore, opinions regarding further use cases did differ.	documentation;glass;interviews;patients;prototype;real life;semiconductor industry;smart tv;smartglasses;ubiquitous computing;wound care management	Hanna Wüller;Jonathan Behrens;Kai Klinker;Manuel Wiesche;Helmut Krcmar;Hartmut Remmers	2018	Studies in health technology and informatics	10.3233/978-1-61499-896-9-191		HCI	-60.78131782504034	-59.098175802686654	12035
5f38de93e1079060d2a68c8703d11ed6193b3ce6	extracting personal conceptual structures from pc documents and its application to web search personalization	electronic mail;personal communication networks;personal computer;application software;search engines;data mining;internet;web search;informatics;classification tree analysis;web search informatics data mining application software microcomputers personal communication networks classification tree analysis internet electronic mail search engines;microcomputers	"""This paper proposes a method of extracting personal conceptual structures from documents on a personal computer that contain a great deal of personal information, and applying them to personalize Web searches. Everybody has differing ideologies, concepts, and knowledge and there is a lot of personal information stored on PCs. While it is easy for a person to determine what a PC user thinks and knows, computers cannot. In this work, two types of personal conceptual structures are represented, i.e., a """"personal concept tree"""" and """"relationships between terms"""". The """"personal concept tree"""" indicates personal concept classification. Relationships between terms"""" indicates how the user thinks of a term, and how it is created from the """"personal concept tree"""" based on deviations in the appearance of the term. This paper also proposes the personalization of Web searches, i.e., expanding query keywords and re-ranking of search results using personal conceptual structures."""	coefficient;connection-oriented ethernet;directory (computing);feature vector;informatics;knowledge society;personal computer;personalization;personally identifiable information;web search engine;world wide web	Hiroaki Ohshima;Satoshi Oyama;Katsumi Tanaka	2005	21st International Conference on Data Engineering Workshops (ICDEW'05)	10.1109/ICDE.2005.220	application software;the internet;computer science;operating system;personal information management;data mining;database;microcomputer;informatics;world wide web;personal information manager	DB	-29.98007051428777	-52.841205148194284	12049
36762e61491b1d377af1015850b0cc7648408263	tagging complex nes with maxent models: layered structures versus extended tagset	complex structure;layered structure;maximum entropy model	The paper discusses two policies for recognizing NEs with complex structures by maximum entropy models. One policy is to develop cascaded MaxEnt models at different levels. The other is to design more detailed tags with human knowledge in order to represent complex structures. The experiments on Chinese organization names recognition indicate that layered structures result in more accurate models while extended tags can not lead to positive results as expected. We empirically prove that the {start, continue, end, unique, other} tag set is the best tag set for NE recognition with MaxEnt models.	experiment;principle of maximum entropy;propagation of uncertainty;software propagation	Deyi Xiong;Hongkui Yu;Qun Liu	2004		10.1007/978-3-540-30211-7_57	computer science;artificial intelligence;principle of maximum entropy;machine learning;data mining;generalized complex structure	NLP	-18.61677543079281	-72.95222189041246	12057
ae36d64cdcf3de20cf5d0a239ea6f13817257716	where to put the search concepts in the search result page		This paper looks at where to put the search concepts in the search result pages by asking over 40 subjects. Four (layout) designs are used where the search concepts are placed differently, and subjects are asked to rank them. Results show that there is preference to place the search concepts near the snippets.		K. T. Tong;Robert Wing Pong Luk	2011		10.1007/978-3-642-22098-2_120	semantic search;phrase search;data mining;world wide web;information retrieval;search engine;exponential search	Theory	-32.00337483016743	-53.480060646139506	12091
5816cb81f615042db1da3ab5924baf7e022f8b29	talen: tool for annotation of low-resource entities		We present a new web-based interface, TALEN, designed for named entity annotation in low-resource settings where the annotators do not speak the language. To address this non-traditional scenario, TALEN includes such features as in-place lexicon integration, TF-IDF token statistics, Internet search, and entity propagation, all implemented so as to make this difficult task efficient and frictionless. We conduct a small user study to compare against a popular annotation tool, showing that TALEN achieves higher precision and recall against ground-truth annotations, and that users strongly prefer it over the alternative. TALEN is available at: github.com/CogComp/talen.	authorization;brat;ground truth;in-place algorithm;lexicon;named entity;precision and recall;screenshot;software propagation;tf–idf;usability testing;web application	Stephen D. Mayhew;Dan Roth	2018			natural language processing;computer science;artificial intelligence;transcription activator-like effector nuclease;annotation	NLP	-32.27495994566137	-72.69109784564257	12117
b4c822055568f86c7d421ef8977f7077f76df63d	a two-level morphological analyser and generator for irish using finite-state transducers		Computational morphology is an important part of natural language processing. Finite-state techniques have been applied successfully in computational phonology and morphology to many of the world’s major languages. Celtic languages such as Modern Irish present challenging morphological features that to date have not been addressed using finite-state technology. This paper presents a finite-state two-level morphology of Irish developed using Xerox Finite-State Tools. The system encodes the inflectional morphology of all inflected parts-of-speech in Modern Irish. The morphotactics of stems and affixes are encoded in the lexicon and word mutations are implemented as a series of replace rules encoded as regular expressions. Both the lexicons and rules are compiled into finite state transducers and combined to produce a single lexical transducer for the language. A major advantage of finite-state two-level implementations of morphology is their inherent bi-directionality; the same system is used for both analysis and generation of word forms in the language. This resource can be used as a component part in many NLP applications such as spelling checkers/correctors, stemmers, and text to speech synthesisers. It can also be used in tokenising, lemmatising and part-of-speech tagging of a corpus of text. The system, which is designed for broad coverage of the language, is evaluated against the most frequently used words in a corpus of contemporary Irish texts. Finally, possible extensions to the system are suggested, such as derivational morphology and the inclusion of dialectal or historical word-forms.	bi-directional text;compiler;computation;finite-state transducer;galaxy morphological classification;lexicon;mathematical morphology;morphological parsing;natural language processing;part-of-speech tagging;regular expression;speech synthesis;text corpus	Elaine Uí Dhonnchadha	2002			speech recognition;analyser;computer science;transducer;irish	NLP	-30.392909275432036	-75.72788232255724	12151
1471c0a6b7022553259bbf070a95e5b917c9aff7	multimodal communication from multimodal thinking - towards an integrated model of speech and gesture production	multimodal communication;integrable model;gesture;speech;production model;multimodal semantics	A computational model for the automatic production of combined speech and iconic gesture is presented. The generation of multimodal behavior is grounded in processes of multimodal thinking, in which a propositional representation interacts and interfaces with an imagistic representation of visuo-spatial imagery. An integrated architecture for this is described, in which the planning of content and the planning of form across both modalities proceed in an interactive manner. Results from an empirical study are reported that inform the on-the-spot formation of gestures.	computational model;german research centre for artificial intelligence;heart rate variability;modal logic;multimodal interaction;simulation	Stefan Kopp;Kirsten Bergmann;Ipke Wachsmuth	2008	Int. J. Semantic Computing	10.1142/S1793351X08000361	natural language processing;speech recognition;computer science;speech;productivity model;linguistics;gesture	AI	-13.361857822930585	-77.463783618158	12158
236bfea55bbe0b5bd7da413575c13c5916d58960	indexing umls semantic types for medical question-answering	umls;information retrieval;non-us government;language;france;funding;natural language processing	Open-domain Question-Answering (QA) systems heavily rely on named entities, a set of general-purpose semantic types which generally cover names of persons, organizations and locations, dates and amounts, etc. If we are to build medical QA systems, a set of medically relevant named entities must be used. In this paper, we explore the use of the UMLS (Unified Medical Language System) Semantic Network semantic types for this purpose. We present an experiment where the French part of the UMLS Metathesaurus, together with the associated semantic types, is used as a resource for a medically-specific named entity tagger. We also explore the detection of Semantic Network relations for answering specific types of medical questions. We present results and evaluations on a corpus of French-language medical documents that was used in the EQueR Question-Answering evaluation forum. We show, using statistical studies, that strategies for using these new tags in a QA context are to take in account the individual origin of documents.	body of uterus;brill tagger;evaluation;general-purpose markup language;indexes;name;named entity;question answering;quinolinic acid;semantic network;software quality assurance;umls metathesaurus;unified medical language system	Thierry Delbecque;Pierre Jacquemart;Pierre Zweigenbaum	2005	Studies in health technology and informatics		natural language processing;named entity;data mining;search engine indexing;question answering;unified medical language system;artificial intelligence;semantics;semantic network;umls metathesaurus;medicine	NLP	-33.92459125619909	-70.28148141886513	12181
17ea7b982e1ba39461ede82ce0d737289aa572e8	a multi-lingual dictionary of dirty words.		We present a multi-lingual dictionary of dirty words. We have collected about 3,200 dirty words in several languages and built a database of these. The language with the most words in the database is English, though there are several hundred dirty words in for instance Japanese too. Words are classified into their general meaning, such as what part of the human anatomy they refer to. Words can also be assigned a nuance label to indicate if it is a cute word used when speaking to children, a very rude word, a clinical word etc. The database is available online and will hopefully be enlarged over time. It has already been used in research on for instance automatic joke generation and emotion detection.	dictionary;emotion recognition	Jonas Sjöbergh;Kenji Araki	2008			natural language processing;speech recognition;artificial intelligence;joke;computer science	NLP	-26.863769001399064	-81.86834975647226	12184
1ec5be3d92e0375785d6dfc9194996e1713a558c	irmail: a minimal interface for a retrieval system	information retrieval system	One of the primary problems with using information retrieval systems are problems of access. The access problems for IR can be categorized into: problems of physically accessing the IR system, problems of using the correct retrieval syntax to access the stored information, and problems of using the correct keywords to access the stored information. The goals for this project were to develop a simple retrieval system that would contain much of the functionality of existing retrieval systems, while minimizing the problems of access. Thus, it serves as a system that can be easily accessed and used with minimal effort or instruction. This poster provides a brief description of the motivation for, and implementation of IRMail, and then describes how HCI researchers may perform searches on the HCIBIB archives with IRMail.	archive;categorization;human–computer interaction;information retrieval	Peter W. Foltz	1992		10.1145/1125021.1125025	relevance;cognitive models of information retrieval;computer science;theoretical computer science;data mining;world wide web;information retrieval;human–computer information retrieval	Web+IR	-34.17850209952403	-60.551600434691096	12194
5a0576f5f014d8cdce56407c4a0316298cde890e	authoring multimedia documents using wysiwym editing	semantic modeling;wysiwym editing;authoring multimedia document;working system;ideal system;multimedia document;significant part;high level;semantic model	(1) This paper outlines a future `ideal' multimedia document authoring system that allows authors to specify content and form of the document independently of each other and at a high level of abstraction; (2) It describes a working system that implements a small but signi cant part of the functionality of such an ideal system, based on semantic modeling of the pictures as well as the text of the document; and (3) It explains what needs to be done to bridge the gap between the implemented system and the ideal one. 1 A Future `Ideal' Multimedia Document Authoring System A Document Authoring System is a tool that helps an author to write documents. If the system supports the authoring of documents that combine `presentations' in di erent media (text and images, for example), we will speak of a multimedia document authoring system. Ideally, a multimedia document authoring system would allow authors to specify the content and form of a high-quality document in ways that are both simple and e cient. More speci cally, an ideal system would a ord the following options to the author: 1. Easy determination of content. `Content' is taken to mean the factual (i.e., propositional) content of the document { in other words, the content of the Knowledge Base (kb) that forms the input to the document authoring system. 2. Easy determination of style and layout. In the absence of speci c instructions from the author, style and layout should be determined using intelligent defaults. (For example, the standard settings may require the document to be informal, with avoidance of technical terms, lists and footnotes, without maximum paragraph length, and with numbered sections.) Defaults can be overridden by the author, whereupon other defaults may become relevant. 3. Easy allocation of media. As in the case of style and layout, the system has to use judiciously chosen defaults: perhaps using illustrative pictures wherever suitable pictures are available, and graphs or tables wherever large amounts of homogeneously structured quantitative information are involved. As above, defaults may be overruled by speci c requests from the author; if a request is impossible to full l, an appropriate error message should be generated. 4. Easy annotation of non-generated presentations. In some cases, it will be possible for the system to generate presentations. In other cases, this may be impossible: Literally quoted texts, for example, or historic photographs, may predate the use of the system, in which case it may be necessary to treat them as `canned' and to annotate them to allow the system to make intelligent use of them. 5. Easy post-editing. Once the system has produced a document according to the speci cations of the author, the ideal system would o er tools to address remaining inadequacies using post-editing. `Easy' means e cient, protected against inconsistencies, and not requiring specialist skills or knowledge. A domain specialist { who may not know anything about knowledge representation, logic, or linguistics { could use such a system to build kbs that the system can turn into documents in any desired language using any desired combination of media. The production and updating of complex documents would be greatly simpli ed as a result. In present-day practice, these requirements tend to be far from realized: authoring documents by means of such tools as ms Word or Powerpoint requires much low-level interaction, such as the typing of characters on a keyboard and the dragging of gures from one physical location to another. In some cases, an Intelligent Multimedia Presentation System (immps e.g., Bordegoni et al. 1997) can be used (see air 1995, Maybury and Wahlster 1998 for some surveys), which employs techniques from Articial Intelligence to allow higher-level interaction. Present immps, however, meet few of the requirements mentioned above. Most of them, for example, require input of a highly specialized nature (e.g., the complex logical formulas entered in the wip system, Andr e and Rist 1995) and they allow an author little control over the form (e.g., layout, textual style, media allocation) of the document. The issue of easy annotation (4) is never even addressed, to the best of our knowledge. The next section describes an implemented system for the authoring of textual documents that can be argued to ful ll requirements (1) and (2) and which forms a suitable starting point for working towards the `ideal' multimedia system outlined above. Section 3 describes an extension of this system in which signi cant aspects of requirements 3-5 have also been implemented. Key features of this new system are its ability to use semantic representations that are common to the di erent media, and the ability to construct natural language feedback texts to help the author understand the content and the form of the document while it is still under construction. The concluding section explains what needs to be done to ll the gap between the implemented system and the ideal one. An exception is alfresco which takes natural language input, requiring the system to interpret unconstrained natural language (Stock 1991). Avoiding the need for doing this is an important design motivation for wysiwym-based systems. 2 A WYSIWYM-based System for the Authoring of Textual	canned response;consistency (database systems);drag and drop;error message;high- and low-level;high-level programming language;knowledge base;knowledge representation and reasoning;natural language processing;object-relational database;postediting;requirement;wysiwym	Kees van Deemter;Richard Power	2000			semantic data model;natural language processing;computer science;linguistics;multimedia;world wide web	Web+IR	-31.72546020332635	-83.40501817066684	12202
6d4254b9a8caeaafce81ac37f80fdffca14e67a4	ksu kdd: word sense induction by clustering in topic space	unlabeled data;ksu;space use	We describe our language-independent unsupervised word sense induction system. This system only uses topic features to cluster different word senses in their global context topic space. Using unlabeled data, this system trains a latent Dirichlet allocation (LDA) topic model then uses it to infer the topics distribution of the test instances. By clustering these topics distributions in their topic space we cluster them into different senses. Our hypothesis is that closeness in topic space reflects similarity between different word senses. This system participated in SemEval-2 word sense induction and disambiguation task and achieved the second highest V-measure score among all other systems.	brill tagger;centrality;cluster analysis;f1 score;language-independent specification;latent dirichlet allocation;part-of-speech tagging;point of sale;roland gs;semeval;topic model;wafer-scale integration;word sense;word-sense disambiguation;word-sense induction	Wesam Elshamy;Doina Caragea;William H. Hsu	2010			natural language processing;machine learning;pattern recognition	AI	-24.59846676432572	-68.41681631745477	12220
40ffd9a99405e1af48c56bd9cd95579e69ca7aeb	the value of monolingual crowdsourcing in a real-world translation scenario: simulation using haitian creole emergency sms messages	human computation;real-world translation scenario;translation system;haitian creole;english translation task;google translate;monolingual crowdsourcing;haitian creole emergency sms;monolingual source;machine translation	MonoTrans2 is a translation system that combines machine translation (MT) with human computation using two crowds of monolingual source (Haitian Creole) and target (English) speakers. We report on its use in the WMT 2011 Haitian Creole to English translation task, showing that MonoTrans2 translated 38% of the sentences well compared to Google Translate’s 25%.	creole (markup);crowdsourcing;google translate;human-based computation;machine translation;simulation	Chang Hu;Philip Resnik;Yakov Kronrod;Vladimir Eidelman;Olivia Buzek;Benjamin B. Bederson	2011			natural language processing;speech recognition;computer science;communication	NLP	-23.48559575783432	-82.39993489825596	12230
fb3fc7b1374fb0cd0b546b4c3bc9c3fd84ed95e4	voice identity recognition: functional division of the right sts and its behavioral relevance		The human voice is the primary carrier of speech but also a fingerprint for person identity. Previous neuroimaging studies have revealed that speech and identity recognition is accomplished by partially different neural pathways, despite the perceptual unity of the vocal sound. Importantly, the right STS has been implicated in voice processing, with different contributions of its posterior and anterior parts. However, the time point at which vocal and speech processing diverge is currently unknown. Also, the exact role of the right STS during voice processing is so far unclear because its behavioral relevance has not yet been established. Here, we used the high temporal resolution of magnetoencephalography and a speech task control to pinpoint transient behavioral correlates: we found, at 200 msec after stimulus onset, that activity in right anterior STS predicted behavioral voice recognition performance. At the same time point, the posterior right STS showed increased activity during voice identity recognition in contrast to speech recognition whereas the left mid STS showed the reverse pattern. In contrast to the highly speech-sensitive left STS, the current results highlight the right STS as a key area for voice identity recognition and show that its anatomical-functional division emerges around 200 msec after stimulus onset. We suggest that this time point marks the speech-independent processing of vocal sounds in the posterior STS and their successful mapping to vocal identities in the anterior STS.	fingerprint;magnetoencephalography;neural pathways;neuroimaging;onset (audio);relevance;sense of identity (observable entity);speech processing;speech recognition;voice;sodium thiosulfate	Sonja Schall;Stefan J. Kiebel;Burkhard Maess;Katharina von Kriegstein	2014	Journal of Cognitive Neuroscience	10.1162/jocn_a_00707	psychology;developmental psychology;communication	ML	-7.733273227166372	-81.19184121308996	12233
066d3e597528e856c1597100e0a69d910c4c1431	"""supporting physicians in taking decisions in clinical guidelines: the glare """"what if"""" facility"""	physicians;expert systems	"""GLARE (GuideLine Acquisition, Representation and Execution) is a domain-independent system for the acquisition, representation and execution of clinical guidelines. GLARE is unique in its approach to supporting the decision-making process of users/physicians faced with various alternatives in the guidelines. In many cases, the best alternative cannot be determined on the basis of """"local information"""" alone (i.e., by considering just the selection criteria associated with the decision at hand), but must also take into account information stemming from relevant alternative pathways. Exploitation of """"global information"""" available in the various pathways is made possible by GLARE through the """"what if"""" facility, a form of hypothetical reasoning which allows users to gather relevant decision parameters (e.g., costs, resources, times) from selected parts of the guideline in a semi-automatic fashion. In particular, the extremely complex task of coping with temporal information involves the extension and adaptation of various techniques developed by the Artificial Intelligence (AI) community."""	acclimatization;artificial intelligence;coping behavior;decision making;glare - eye symptom;semiconductor industry;stemming;what if	Paolo Terenziani;Stefania Montani;Alessio Bottrighi;Mauro Torchio;Gianpaolo Molino	2002	Proceedings. AMIA Symposium		coping (psychology);management science;guideline;expert system;knowledge management;computer science	AI	-52.86655210922274	-67.75519705682323	12236
87e15a6982b4b08757eb32d2b9bd4766ce999fa6	speaker recognition for speech under face cover		Speech under face cover constitute a case that is increasingly met by forensic speech experts. Wearing face cover mostly happens when an individual strives to conceal his or her identity. Based on the material of face cover and the level of contact with speech production organs, speech production becomes affected by face mask and a part of speech energy gets absorbed in the mask. There has been little research on how speech acoustics is affected by different face masks and how face covers might affect performance of automatic speaker recognition systems. In the present paper, we have collected speech under face mask with the aim of studying the effects of wearing different masks on state-of-the-art text-independent automatic speaker recognition system. The preliminary speaker recognition rates along with mask identification experiments are presented in this paper.	experiment;speaker recognition;time-compressed speech	Rahim Saeidi;Tuija Niemi;Hanna Karppelin;Jouni Pohjalainen;Tomi Kinnunen;Paavo Alku	2015			speaker recognition;speaker diarisation;speech recognition;pattern recognition	Vision	-11.646742965933726	-85.75142608136011	12243
bfcbf4495da2bff459f048036a83d3ee6bbda1e8	improving short utterance speaker recognition by modeling speech unit classes	short utterance;acoustics;speech processing;speech;speech data models speaker recognition acoustics speech recognition speech processing adaptation models;speaker recognition;speech recognition;model synthesis short utterance speaker recognition subregion model;model synthesis;speaker recognition error statistics maximum likelihood estimation regression analysis;eer short utterance speaker recognition modeling speech unit classes susr speech data universal background model ubm speech signals acoustic subregions data driven approach robust subregion models maximum likelihood linear regression mllr equal error rate;adaptation models;subregion model;data models	Short utterance speaker recognition (SUSR) is highly challenging due to the limited enrollment and/or test data. We argue that the difficulty can be largely attributed to the mismatched prior distributions of the speech data used to train the universal background model (UBM) and those for enrollment and test. This paper presents a novel solution that distributes speech signals into a multitude of acoustic subregions that are defined by speech units, and models speakers within the subregions. To avoid data sparsity, a data-driven approach is proposed to cluster speech units into speech unit classes, based on which robust subregion models can be constructed. Further more, we propose a model synthesis approach based on maximum likelihood linear regression (MLLR) to deal with no-data speech unit classes. The experiments were conducted on a publicly available database SUD12. The results demonstrated that on a text-independent speaker recognition task where the test utterances are no longer than 2 seconds and mostly shorter than 0.5 seconds, the proposed sub-region modeling offered a 21.51% relative reduction in equal error rate (EER), compared with the standard GMM-UBM baseline. In addition, with the model synthesis approach, the performance can be greatly improved in scenarios where no enrollment data are available for some speech unit classes.	acoustic cryptanalysis;baseline (configuration management);cluster analysis;enhanced entity–relationship model;experiment;simulation;sparse matrix;speaker recognition;speech synthesis;test data	Lantian Li;Dong Wang;Chenhao Zhang;Thomas Fang Zheng	2016	IEEE/ACM Transactions on Audio, Speech, and Language Processing	10.1109/TASLP.2016.2544660	voice activity detection;speaker recognition;data modeling;speaker diarisation;speech recognition;computer science;speech;pattern recognition;speech processing;acoustic model	NLP	-17.32793118318237	-91.58779797836353	12288
4c930a1ad14e472f4d609ec27935d4baef185ced	causal reasoning with continuous outcomes	bf psychology	We describe an attempt to understand causal reasoning in situations where a binary cause produces a change on a continuous magnitude dimension. We consider established theories of binary probabilistic causal inference – ΔP and Power PC – and adapt them to continuous non-probabilistic outcomes. While ΔP describes causal strength as the difference of effect occurrence between the presence and absence of the cause, Power PC normalizes this difference with the effect base-rate to obtain a proportional measure of causal power, relative to the maximum possible strength. Two experiments compared the applicability of each approach by creating scenarios where binary probabilistic scenarios were directly mapped onto inference problems involving continuous magnitude dimensions. Results from counterfactual judgments tentatively indicate that people reason about causal relations with continuous outcomes by adopting a proportional approach when evaluation preventive causal powers, and a difference approach in generative scenarios.	base rate;causal filter;causal inference;causality;counterfactual conditional;entity;experiment;powerpc;probabilistic causation;quantum key distribution;theory	Ahmad Azad Ab Rashid;Marc Buehner	2013			psychology;developmental psychology;artificial intelligence;mathematics;social psychology;statistics	Web+IR	-5.003361194523174	-77.57716878291293	12301
35c4f18cb54ef29914b77d095de7a1c68d024636	supporting interoperability of genetic data with loinc	vocabulary;genetics;medical records systems;controlled;clinical laboratory information systems;loinc	Electronic reporting of genetic testing results is increasing, but they are often represented in diverse formats and naming conventions. Logical Observation Identifiers Names and Codes (LOINC) is a vocabulary standard that provides universal identifiers for laboratory tests and clinical observations. In genetics, LOINC provides codes to improve interoperability in the midst of reporting style transition, including codes for cytogenetic or mutation analysis tests, specific chromosomal alteration or mutation testing, and fully structured discrete genetic test reporting. LOINC terms follow the recommendations and nomenclature of other standards such as the Human Genome Organization Gene Nomenclature Committee's terminology for gene names. In addition to the narrative text they report now, we recommend that laboratories always report as discrete variables chromosome analysis results, genetic variation(s) found, and genetic variation(s) tested for. By adopting and implementing data standards like LOINC, information systems can help care providers and researchers unlock the potential of genetic information for delivering more personalized care.	chromosome aberrations;code;conferences;doctor of law;entity name part qualifier - adopted;experiment;gene nomenclature;genetic algorithm;genetic screening method;health level 7;identifier;information systems;information system;interoperability;jd - java decompiler;laboratory procedures;logical observation identifiers names and codes;manuscripts;molecular pathology (discipline);mutation testing;personalization;rodent nomenclature name;sim lock;science of genetics;vocabulary;format;mutation analysis;standards characteristics	Jamalynne Deckard;Clement J. McDonald;Daniel J. Vreeman	2015	Journal of the American Medical Informatics Association : JAMIA	10.1093/jamia/ocu012	medcin;medicine;computer science;bioinformatics;loinc;nursing;data mining;database;world wide web	SE	-55.442658933686495	-65.94244477312012	12305
6edaa9acc59a8e9997124548c605b05fd62abadb	a default hierarchy for pronouncing english	default rules;hierarchical structure;speech intelligibility;spelling knowledge representation speech synthesis speech intelligibility learning systems default hierarchy knowledge acquisition default rules english pronunciation;default hierarchy;speech synthesis;natural languages;learning systems;neural networks artificial intelligence knowledge representation knowledge acquisition artificial neural networks laboratories learning speech synthesis investments databases;knowledge acquisition;spelling;knowledge representation;english pronunciation;speech synthesis knowledge acquisition knowledge representation learning systems natural languages speech intelligibility;neural network	The authors study the principles governing the power and efficiency of the default hierarchy, a system of knowledge acquisition and representation. The default hierarchy trains automatically, yet yields a set of rules which can be easily assessed and analyzed. Rules are organized in a hierarchical structure containing general (default) and specific rules. In training the hierarchy, general rules are learned before specific rules. In using the hierarchy, specific rules are accessed first, with default rules used when no specific rules apply. The main results concern the properties of the default hierarchy architecture, as revealed by its application to English pronunciation. Evaluating the hierarchy as a pronouncer of English, the authors find that its rules capture several key features of English spelling. The default hierarchy pronounces English better than the neural network NETtalk, and almost as well as expert-devised systems. >		Judith Hochberg;Susan M. Mniszewski;T. Calleja;G. J. Papcun	1991	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.93813	natural language processing;speech recognition;computer science;natural language;speech synthesis;intelligibility	Vision	-23.507609391422147	-87.95912937051125	12320
e1c3954ad18f65366b2644cd22e25113b9464097	modeling tones in hakka on the basis of the command-response model.		As one of the major Chinese dialects, Hakka typically has a tone system with six lexical tones. The traditional 5-level notation of tones in Hakka varies in previous references due to its subjective and relative nature. In order to overcome the limitations of the traditional approach, the command-response model for the process of F0 contour generation is employed to analyze quantitatively the tones in continuous speech of two varieties of Hakka, spoken in Meixian and in Shataukok, respectively. By providing both phonological descriptions to each tone type and quantitative approximations to continuous F0 contours, the model-based approach provides an efficient connection between phonetics and phonology of Hakka tones.	approximation	Wentao Gu;Rerrario Shui-Ching Ho;Tan Lee	2007			speech recognition;command response;computer science	NLP	-13.741456828430746	-83.67100980136539	12321
3c6e886fa6f7c5954e13fbae3d763b184775d1a4	loss minimization in parse reranking	discriminative model;discriminative rerankers;various baseline model;voted perceptron algorithm;probabilistic model;loss minimization model;probable candidate;expected loss approximation;expected loss;parse reranking task;neural network	We propose a general method for reranker construction which targets choosing the candidate with the least expected loss, rather than the most probable candidate. Different approaches to expected loss approximation are considered, including estimating from the probabilistic model used to generate the candidates, estimating from a discriminative model trained to rerank the candidates, and learning to approximate the expected loss. The proposed methods are applied to the parse reranking task, with various baseline models, achieving significant improvement both over the probabilistic models and the discriminative rerankers. When a neural network parser is used as the probabilistic model and the Voted Perceptron algorithm with data-defined kernels as the learning algorithm, the loss minimization model achieves 90.0% labeled constituents F1 score on the standard WSJ parsing task.	approximation algorithm;artificial neural network;baseline (configuration management);dmz (computing);discriminative model;f1 score;kernel (operating system);parse tree;parsing;perceptron;statistical model;the wall street journal	Ivan Titov;James Henderson	2006			statistical model;computer science;machine learning;pattern recognition;data mining;artificial neural network;discriminative model	NLP	-20.373787604885397	-76.21657908647241	12328
3f5414c94de849cf3baba80173c91725d13d6e48	validation and calibration of dietary intake in chronic kidney disease: an ontological approach		This study develops a pilot knowledge-based system (KBS) for addressing validation and calibration of dietary intake in chronic kidney disease (CKD). The system is constructed by using Web Ontology Language (OWL) and Semantic Web Rule Language (SWRL) to demonstrate how a KBS approach can achieve sound problem solving modeling and effective knowledge inference. In terms of experimental evaluation, data from 36 case patients are used for testing. The evaluation results show that, excluding the interference factors and certain non-medical reasons, the system has achieved the research goal of CKD dietary consultation. For future studies, the problem solving scope can be expanded to implement a more comprehensive dietary consultation system.		Yu-Liang Chi;Tsang-Yao Chen;Wan-Ting Tsai	2015		10.1007/978-3-319-19027-3_9	engineering;knowledge management;data mining;management science	AI	-54.00212307449702	-66.44225188677926	12367
229861bb448a96b7e0047585c3070e0f0e129fe1	jufit: a configurable rule engine for filtering and generating new multilingual umls terms	biological patents;biomedical journals;text mining;europe pubmed central;citation search;citation networks;research articles;abstracts;open access;life sciences;clinical guidelines;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	We here describe JuFiT, an easily adjustable rule engine which allows to filter non-natural terms (i.e., ones usually not occurring in running citation texts) from the Umls metathesaurus and even adds new terms to the UMLS (by rewriting non-natural terms). Unlike previous attempts (with MetaMap or Casper), JuFiT serves multilingual purposes in that it runs for English, Spanish, French, German and Dutch documents, as well - the most prominent European languages in terms of UMLS coverage. We evaluated JuFiT under a variety of experimental conditions and found evidence that it increases annotation quality for English, and most likely also for German and Spanish.	annotation;business rules engine;cflar gene;classification;embedded system;embedding;experiment;information extraction;machine translation;multilingualism;natural language processing;open-source software;programming languages;rewriting;rule (guideline);schedule (computer science);solutions;tnm staging system;the nameless mod;unified medical language system;citation	Johannes Hellrich;Stefan Schulz;Sven Buechel;Udo Hahn	2015	AMIA ... Annual Symposium proceedings. AMIA Symposium		computer science;data mining;world wide web;information retrieval	NLP	-29.8375164376807	-72.17011620103679	12377
8f8ac2a8b7cc8a60b1eea229db5ef190c72f7c8b	acoustic probing for estimating the storage time and firmness of tomatoes and mandarin oranges		This paper introduces an acoustic probing technique to estimate the storage time and firmness of fruits; we emit an acoustic signal to fruit from a small speaker and capture the reflected signal with a tiny microphone. We collect reflected signals for fruits with various storage times and firmness conditions, using them to train regressors for estimation. To evaluate the feasibility of our acoustic probing, we performed experiments; we prepared 162 tomatoes and 153 mandarin oranges, collected their reflected signals using our developed device and measured their firmness with a fruit firmness tester, for a period of 35 days for tomatoes and 60 days for mandarin oranges. We performed cross validation by using this data set. The average estimation errors of storage time and firmness for tomatoes were 0.89 days and 9.47 g/mm2. Those for mandarin oranges were 1.67 days and 15.67 g/mm2. The estimation of storage time was sufficiently accurate for casual users to select fruits in their favorite condition at home. In the experiments, we tested four different acoustic probes and found that sweep signals provide highly accurate estimation results. All data sets and software are available online (URL will be available upon acceptance).	acoustic cryptanalysis;cross-validation (statistics);experiment;microphone;signal reflection;super robot monkey team hyperforce go!	Hidetomo Kataoka;Takashi Ijiri;Kohei Matsumura;Jeremy White;Akira Hirabayashi	2018	CoRR		mandarin chinese;computer science;speech recognition;cross-validation;microphone;data set	Metrics	-6.5653795350605355	-88.27814795905822	12387
2a81748031adafbe32248e135cea2dc22dc2712f	a 40 nm 144 mw vlsi processor for real-time 60-kword continuous speech recognition	cmos integrated circuits;speech recognition random access memory viterbi algorithm real time systems hidden markov models power demand frequency measurement;power 144 mw vlsi processor real time 60 kword continuous speech recognition low power vlsi chip context dependent hidden markov model hmm cache architecture beam pruning dynamic threshold two stage language model searching highly parallel gaussian mixture model variable frame look ahead scheme elastic pipeline operation viterbi transition gmm processing bandwidth reduction cmos technology on chip memory 60 kword real time continuous speech recognition size 40 nm;vlsi cmos integrated circuits hidden markov models real time systems speech recognition;hidden markov models;vlsi;speech recognition;real time systems	We have developed a low-power VLSI chip for 60-kWord real-time continuous speech recognition based on a context-dependent hidden Markov model (HMM). Our implementation includes a cache architecture using locality of speech recognition, beam pruning using a dynamic threshold, two-stage language model searching, highly parallel Gaussian mixture model (GMM) computation based on the mixture level, a variable-frame look-ahead scheme, and elastic pipeline operation between the Viterbi transition and GMM processing. The accuracy degradation of the important parameters in Viterbi computation is strictly discussed. Results show that our implementation achieves 95% bandwidth reduction (70.86 MB/s) and 78% required frequency reduction (126.5 MHz) comparing to the referential Julius system. The test chip, fabricated using 40 nm CMOS technology, contains 1.9 M transistors for logic and 7.8 Mbit on-chip memory. It dissipates 144 mW at 126.5 MHz and 1.1 V for 60-kWord real-time continuous speech recognition.	cmos;computation;context-sensitive language;elegant degradation;google map maker;hidden markov model;julius;language model;locality of reference;low-power broadcasting;markov chain;mebibyte;megabit;mixture model;real-time clock;real-time transcription;speech recognition;transistor;very-large-scale integration	Guangji He;Takanobu Sugahara;Tsuyoshi Fujinaga;Yuki Miyamoto;Hiroki Noguchi;Shintaro Izumi;Hiroshi Kawaguchi;Masahiko Yoshimoto	2012	IEEE Transactions on Circuits and Systems I: Regular Papers	10.1109/ASPDAC.2013.6509561	electronic engineering;real-time computing;speech recognition;viterbi algorithm;computer science;very-large-scale integration;cmos;hidden markov model	EDA	-24.115953531194876	-91.32669978373553	12407
3ce4c40c14a6e7ed9832ddabd0a4340216fccfe4	tonal complexity as a dialectal feature: 25 different citation tones from four zhejiang wu dialects		Acoustic and auditory data are presented from an ongoing large-scale investigation into the tones and tone sandhi of the Wu dialects of Zhejiang province in East Central China. The citation tones from 4 sites (3 hitherto undescribed) in the little known Central Zhejiang area are described: Pujiang, Tonglu, Shengxian and Tiantai. Mean F0 and duration data are presented for the tones of these dialects. The data demonstrate a high degree of complexity, having no less than 25 Linguistic-tonetically different tones, including 3 different falling tones, and 4 different falling-level tones. The nature of the complexity of these forms is discussed.	acoustic cryptanalysis	Sean Zhu;Phil Rose	1998			speech recognition;natural language processing;citation;computer science;artificial intelligence	ML	-12.214068253318697	-81.84566974117187	12443
50e507262d65c407bd252ca9a3565dfbff53c794	formalizing invariances for content-based music retrieval	similarity measure;content based music retrieval	Invariances are central concepts in content-based music retrieval. Musical representations and similarity measures are designed to capture musically relevant invariances, such as transposition invariance. Though regularly used, their explicit definition is usually omitted because of the heavy formalism required. The lack of explicit definition, however, can result in misuse or misunderstanding of the terms. We discuss the musical relevance of various musical invariances and develop a set-theoretic formalism, for defining and classifying them. Using it, we define the most common invariances, and give a taxonomy which they inhabit. The taxonomy serves as a useful tool for idetinfying where work is needed to address real world problems in content-based music retrieval.	academy;algorithmic efficiency;anna lubiw;centrality;chou's invariance theorem;database;error-tolerant design;kjell samuelson;onset (audio);pitch (music);relevance;requirement;semantics (computer science);set theory;sparse matrix;strict function;string searching algorithm;taxonomy (general)	Kjell Lemström;Geraint A. Wiggins	2009			speech recognition;computer science;artificial intelligence;mathematics	ML	-5.874640868191058	-73.87786343830348	12452
f6c0bb31da9ae2165ab13c5ee6f1929148c38ebc	structures or texts? a dynamic gating method for expert finding in cqa services		Expert finding plays an important role in community question answering websites. Previously, most works focused on assessing the user expertise scores mainly from their past question-answering semantic features. In this work, we propose a gating mechanism to dynamically combine structural and textual representations based on past question-answering behaviors. We also use some user activities including temporal behaviors as the features, which determine the gate values. We evaluate the performance of our method on the well-known question answering sites Stackexchange and Quora. Experiments show that our approach can improve the performance on expert finding tasks.		Zhiqiang Liu;Yan Zhang	2018		10.1007/978-3-319-91458-9_12	data mining;computer science;question answering;gating;feature learning	AI	-18.45487456381529	-67.86333558633324	12466
da7a8544f1d110c2ba1d430350810af61e0b1a35	single document summarization based on nested tree structure		Many methods of text summarization combining sentence selection and sentence compression have recently been proposed. Although the dependency between words has been used in most of these methods, the dependency between sentences, i.e., rhetorical structures, has not been exploited in such joint methods. We used both dependency between words and dependency between sentences by constructing a nested tree, in which nodes in the document tree representing dependency between sentences were replaced by a sentence tree representing dependency between words. We formulated a summarization task as a combinatorial optimization problem, in which the nested tree was trimmed without losing important content in the source document. The results from an empirical evaluation revealed that our method based on the trimming of the nested tree significantly improved the summarization of texts.	abstract syntax tree;automatic summarization;combinatorial optimization;fork (software development);integer programming;intel matrix raid;linear programming;mathematical optimization;optimization problem;parsing;rouge (metric);text corpus;tree (data structure);tree structure;upsampling	Yuta Kikuchi;Tsutomu Hirao;Hiroya Takamura;Manabu Okumura;Masaaki Nagata	2014			natural language processing;speech recognition;computer science;automatic summarization;pattern recognition	NLP	-20.01140071908192	-75.9013257296329	12467
950b99dad0b0454f6df78a428095de1867e0d7b0	a method for user profile learning in document retrieval system using bayesian network		User modeling methods are developed by many researches in area of document retrieval systems. The main reason is that the system can not present the same results for every user. Each user can have different information needs even if he uses the same terms to formulate his query. In this paper we present the solution for the problem. We propose a method for user profile building and updating using Bayesian network approaches which allows to discover dependencies between terms. Additionally, we use domain ontology of terms to simplify the calculations. Performed experiments have shown that the quality of presented methods is promising.		Bernadetta Maleszka	2017		10.1007/978-3-319-54472-4_26	document retrieval;pattern recognition;data mining;information retrieval	AI	-30.255400082441703	-58.98132093216045	12474
5d9f40c9845c90d1b0b0ff6b6f7af887ca80908a	selection problems in the presence of implicit bias		Over the past two decades, the notion of implicit bias has come to serve as an important component in our understanding of discrimination in activities such as hiring, promotion, and school admissions. Research on implicit bias posits that when people evaluate others – for example, in a hiring context – their unconscious biases about membership in particular groups can have an effect on their decision-making, even when they have no deliberate intention to discriminate against members of these groups. A growing body of experimental work has pointed to the effect that implicit bias can have in producing adverse outcomes. Here we propose a theoretical model for studying the effects of implicit bias on selection decisions, and a way of analyzing possible procedural remedies for implicit bias within this model. A canonical situation represented by our model is a hiring setting: a recruiting committee is trying to choose a set of finalists to interview among the applicants for a job, evaluating these applicants based on their future potential, but their estimates of potential are skewed by implicit bias against members of one group. In this model, we show that measures such as the Rooney Rule, a requirement that at least one of the finalists be chosen from the affected group, can not only improve the representation of this affected group, but also lead to higher payoffs in absolute terms for the organization performing the recruiting. However, identifying the conditions under which such measures can lead to improved payoffs involves subtle trade-offs between the extent of the bias and the underlying distribution of applicant characteristics, leading to novel theoretical questions about order statistics in the presence of probabilistic side information.	rule 90;theory	Jon M. Kleinberg;Manish Raghavan	2018		10.4230/LIPIcs.ITCS.2018.33	social psychology;data mining;probabilistic logic;computer science	ML	-50.42576484037228	-56.63808108292072	12480
66bf6f206feb51fd187210a5b39de454f8a5a813	applying operational research and data mining to performance based medical personnel motivation system		This paper presents the methodology suitable for creation of a performance related remuneration system in healthcare sector, which would meet requirements for efficiency and sustainable quality of healthcare services. Methodology for performance indicators selection, ranking and a posteriori evaluation has been proposed and discussed. Priority Distribution Method is applied for unbiased performance criteria weighting. Data mining methods are proposed to monitor and evaluate the results of motivation system.We developed a method for healthcare specific criteria selection consisting of 8 steps; proposed and demonstrated application of Priority Distribution Method for the selected criteria weighting. Moreover, a set of data mining methods for evaluation of the motivational system outcomes was proposed. The described methodology for calculating performance related payment needs practical approbation. We plan to develop semi-automated tools for institutional and personal performance indicators monitoring. The final step would be approbation of the methodology in a healthcare facility.	cns disorder;data mining;health care facility;operations research;quality of health care;remuneration;requirement;semiconductor industry	Olegas Niaksu;Jonas Zaptorius	2014	Studies in health technology and informatics	10.3233/978-1-61499-397-1-63	knowledge management;data mining;management science;medicine	HPC	-56.05421322510537	-62.763823006451645	12482
8d9d3acefa8524f600a7ceabc93f0f3a72b0bd08	pp attachment: where do we stand?		Prepositional phrase (PP) attachment is a well known challenge to parsing. In this paper, we combine the insights of different works, namely: (1) treating PP attachment as a classification task with an arbitrary number of attachment candidates; (2) using auxiliary distributions to augment the data beyond the hand-annotated training set; (3) using topological fields to get information about the distribution of PP attachment throughout clauses and (4) using state-of-the-art techniques such as word embeddings and neural networks. We show that jointly using these techniques leads to substantial improvements. We also conduct a qualitative analysis to gauge where the ceiling of the task is in a realistic setup.	artificial neural network;attachments;parsing;test set;word embedding	Erhard W. Hinrichs;Corina Dima;Daniël de Kok;Jianqiang Ma	2017			computer science;natural language processing;artificial intelligence	NLP	-17.209918768905027	-73.00586933745396	12483
116e4e70e40eff3584fb6aff850613833a7c0f42	the probabilistic analysis of language acquisition: theoretical, computational, and experimental analysis	probabilistic models;experimental analysis;p philology linguistics;probability;generic model;qa mathematics;complexity;evaluation methods;qa76 electronic computers computer science computer software;language acquisition;data analysis;childrens;natural language;minimum description length;experiments;identification in the limit;science education;no negative evidence;child language acquisition;entrenchment;language learning;bayesian models;poverty of the stimulus;prediction;simplicity principle;models;natural language processing;linguistics	There is much debate over the degree to which language learning is governed by innate language-specific biases, or acquired through cognition-general principles. Here we examine the probabilistic language acquisition hypothesis on three levels: We outline a novel theoretical result showing that it is possible to learn the exact generative model underlying a wide class of languages, purely from observing samples of the language. We then describe a recently proposed practical framework, which quantifies natural language learnability, allowing specific learnability predictions to be made for the first time. In previous work, this framework was used to make learnability predictions for a wide variety of linguistic constructions, for which learnability has been much debated. Here, we present a new experiment which tests these learnability predictions. We find that our experimental results support the possibility that these linguistic constructions are acquired probabilistically from cognition-general principles.	cognition;construction grammar;generative model;language development;learnability;linguistics;natural language;probabilistic analysis of algorithms;programming languages	Anne S. Hsu;Nick Chater;Paul M. B. Vitányi	2011	Cognition	10.1016/j.cognition.2011.02.013	psychology;language acquisition;science education;linguistics;communication;cognitive science	ML	-10.791387076945437	-75.97397305952065	12491
8901483ff7f69653b1437422976b87f8b86337de	on the usefulness of compression models for authorship verification		Compression models represent an interesting approach for different classification tasks and have been used widely across many research fields. We adapt compression models to the field of authorship verification (AV), a branch of digital text forensics. The task in AV is to verify if a questioned document and a reference document of a known author are written by the same person. We propose an intrinsic AV method, which yields competitive results compared to a number of current state-of-the-art approaches, based on support vector machines or neural networks. However, in contrast to these approaches our method does not make use of machine learning algorithms, natural language processing techniques, feature engineering, hyperparameter optimization or external documents (a common strategy to transform AV from a one-class to a multi-class classification problem). Instead, the only three key components of our method are a compressing algorithm, a dissimilarity measure and a threshold, needed to accept or reject the authorship of the questioned document. Due to its compactness, our method performs very fast and can be reimplemented with minimal effort. In addition, the method can handle complicated AV cases where both, the questioned and the reference document, are not related to each other in terms of topic or genre. We evaluated our approach against publicly available datasets, which were used in three international AV competitions. Furthermore, we constructed our own corpora, where we evaluated our method against state-of-the-art approaches and achieved, in both cases, promising results.	algorithm;artificial neural network;distortion;experiment;fastest;feature engineering;kinetic data structure;machine learning;mathematical optimization;multiclass classification;natural language processing;parsing;prediction by partial matching;reference work;regular expression;support vector machine;text corpus;tokenization (data security);verification and validation	Oren Halvani;Christian Winter;Lukas Graner	2017		10.1145/3098954.3104050	data mining;support vector machine;hyperparameter optimization;artificial neural network;reference document;computer science;machine learning;artificial intelligence;feature engineering	NLP	-20.444882377015258	-70.21490541728224	12500
abf53f97c7fd65ff6bd044459a367fcfea90226c	retrieval effectiveness of surname-title-word searches for known items by academic library users	relevance information retrieval;information retrieval	This article reports the findings of an experiment using a simulated title page, author surnames, and title words, one-third of which were selected by each of the three authors, to determine the frequency of one-screen displays when used to search for known items in an implied boolean retrieval system. Searches comprising surname plus one significant title word produced one-screen displays 78% of the time; surname plus two words 97% of the time; and surname plus three words 98.5%. Three-quarters of the significant words were nouns.		Frederick G. Kilgour;Barbara B. Moran;John R. Barden	1999	JASIS	10.1002/(SICI)1097-4571(1999)50:3%3C265::AID-ASI9%3E3.0.CO;2-R	library science;relevance;titer;computer science;evaluation;database;law;world wide web;auteur theory;information retrieval;title	DB	-34.17445460093217	-61.62564350132733	12532
5371ad7c52c802bfd649b94ff8b3a5cb5b5e4292	bary at the ntcir-11 mednlp-2 task for complaints and diagnosis recognition		This paper describes a machine-learning based approach to recognizing diagnosed disease names and corresponding temporal expressions. Using CRFs (conditional random fields) to learn and predict tags, the systems described in this paper are characterized by a character-level formulation and heuristic features extracted from medical terminologies. Experimental results on the NTCIR-11 MedNLP-2 datasets suggest that the approach effectively exploit terminological resources and combine them with other NLP (natural language processing) resources including morphological analyzers.	conditional random field;heuristic;machine learning;maximal set;morphological parsing;natural language processing;temporal expressions	Yusuke Matsubara;Mizuki Morita;Kôiti Hasida	2014			natural language processing;computer science;artificial intelligence;data mining	NLP	-24.258140490186413	-70.34626064256182	12547
4868ade370665be49ec8e69f727e852d333830fe	discordances between patient-reported family history and family histories in ehr				Jung Hoon Son;Chunhua Weng	2017			family history;family medicine;medicine	ML	-56.56950161756483	-64.40651498551752	12548
388c1ac595136fb24f9c2b017379f6e0469701f6	semanticizing search engine queries: the university of amsterdam at the erd 2014 challenge	wikipedia;entity linking;data fusion;universiteitsbibliotheek;semantic search;knowledge base	This paper describes the University of Amsterdam's participation in the short track of the Entity Recognition & Disambiguation Challenge 2014 (ERD 2014). We describe how we adapt the Semanticizer---an open-source entity linking framework developed primarily at the University of Amsterdam---to the task of the ERD challenge: linking named entities in search engine queries. We steer the Semanticizer's linking towards named entities by adapting an existing training corpus, and extend the Semanticizer's set of features with contextual features that aim to leverage the limited context provided by search queries. With an F1 score of 0.6062 our final system run achieves median performance, and better than mean performance (0.5329).	entity linking;entity–relationship model;f1 score;named entity;open-source software;web search engine;web search query;word-sense disambiguation	David Graus;Daan Odijk;Manos Tsagkias;Wouter Weerkamp;Maarten de Rijke	2014		10.1145/2633211.2634354	computer science;data mining;database;information retrieval	NLP	-30.81743309020451	-63.05971606760601	12558
a80d531862916f6e4f33c9c0cd574cf03e147b58	the bag-of-frames approach: a not so sufficient model for urban soundscapes	acoustic signal processing;audio coding;statistical distributions;pattern recognition	"""The """"bag-of-frames"""" (BOF) approach, which encodes audio signals as the long-term statistical distribution of short-term spectral features, is commonly regarded as an effective and sufficient way to represent environmental sound recordings (soundscapes). The present paper describes a conceptual replication of a use of the BOF approach in a seminal article using several other soundscape datasets, with results strongly questioning the adequacy of the BOF approach for the task. As demonstrated in this paper, the good accuracy originally reported with BOF likely resulted from a particularly permissive dataset with low within-class variability. Soundscape modeling, therefore, may not be the closed case it was once thought to be."""	aclarubicin;acoustics;audio media;auditory processing disorder;categorization;classification;dacarbazine;frame (physical object);hearing problem;ieee transactions on multimedia;information systems;information retrieval;mathematical morphology;multimodal interaction;neuroscience discipline;pl/i;page (document);percent (qualifier value);repeatability;reuse (action);silo (dataset);spatial variability;sturm's theorem;user-centered design;yang	Mathieu Lagrange;Grégoire Lafay;Boris Defreville;Jean-Julien Aucouturier	2015	The Journal of the Acoustical Society of America	10.1121/1.4935350	probability distribution;acoustics	ML	-8.970035377111099	-84.19565867309815	12574
0213474ee928803b36cc95713b57ec4c8b1868d9	turku neural parser pipeline: an end-to-end system for the conll 2018 shared task		In this paper we describe the TurkuNLP entry at the CoNLL 2018 Shared Task on Multilingual Parsing from Raw Text to Universal Dependencies. Compared to the last year, this year the shared task includes two new main metrics to measure the morphological tagging and lemmatization accuracies in addition to syntactic trees. Basing our motivation into these new metrics, we developed an end-to-end parsing pipeline especially focusing on developing a novel and state-of-the-art component for lemmatization. Our system reached the highest aggregate ranking on three main metrics out of 26 teams by achieving 1st place on metric involving lemmatization, and 2nd on both morphological tagging and parsing.	aggregate data;end system;end-to-end principle;lemmatisation;parser;urban dictionary	Jenna Kanerva;Filip Ginter;Niko Miekka;Georg Heinz Helmut Borner;Tapio Salakoski	2018				NLP	-22.784647741445767	-73.8710495224132	12582
b80b042fc9e5432344eef014560dd0cc9b73a198	a novel website structure optimization model for more effective web navigation	optimisation;distance education;web navigation;web page group;structure optimization;web pages;information retrieval;navigation web pages topology optimization methods couplings web sites information resources costs data mining distance learning;web sites information retrieval optimisation;average distance;web site structure optimization model;web sites;user navigation;user navigation web site structure optimization model web navigation web page group web site access efficiency;web site access efficiency	A novel website structure optimization model for more effective web navigation is proposed. First, web page group with low access efficiency is discovered by its support and its topology average distance; Then a measure degree, website topology interest, which can overall indicate the website access efficiency is proposed as guidance rule to optimize the website hyperlink structure; Finally, users' navigation are facilitated by optimizing website linkage structure that reduces the number of steps to locate their target web pages. Experiments result on a distance education website show that our approach is efficient and practical for adaptive website.	dynamic programming;hyperlink;linkage (software);mathematical optimization;network topology;self-information;user experience;www;web navigation;web page;world wide web	Wen-long Lin;Ye-zheng Liu	2008	First International Workshop on Knowledge Discovery and Data Mining (WKDD 2008)	10.1109/WKDD.2008.77	web service;distance education;web application security;web mining;static web page;web development;web modeling;site map;data web;web analytics;web mapping;web design;web standards;computer science;web navigation;web page;data mining;web 2.0;world wide web;website parse template;information retrieval;web server	Web+IR	-29.97334135566347	-52.67398651625999	12598
113313d0a1e91416f49eb6926b910d9d06a0e902	the stylometric impacts of ageing and life events on identity		AbstractUsing data containing stylometric markers for depression and Alzheimer’s disease, the 45 novels of Iris Murdoch and P.D. James are examined to see if a signature of an individual, their personality, changes over time due to life events and natural ageing. We use variants of the critical slowing down 1-lag autocorrelation and coefficient of skewness techniques with a multivariate identity measure, RPAS to visualize these changes. We find that life events such as depression, anxiety, and Alzheimer’s disease might be identified outside of natural ageing through a tipping point phenomenon. We believe these techniques might be a useful self-help tool to aid in the signalling of depressive episodes, such as averting suicide, and the early identification of Alzheimer’s disease, or for law enforcement personnel monitoring terrorists on watch lists.		David Kernot;Terry Bossomaier;Roger Bradbury	2019	Journal of Quantitative Linguistics	10.1080/09296174.2017.1405719	ageing;social psychology;natural language processing;artificial intelligence;computer science;law enforcement;anxiety;disease;personality;phenomenon	NLP	-45.193113183216774	-73.97973391036814	12685
120abd00970175b726347a4c69fe5cc1edc67dba	tools for image annotation using context-awareness, nfc and image clustering		Annotation of images is crucial for enabling keyword based image search. However, the enormous amount of available digital photos makes manual annotation impractical, and requires methods for automatic image annotation. This paper describes two complementary approaches to automatic annotation of images depicting some public attraction. The LoTagr system provides annotation information for already captured, geo-positioned images, by selecting nearby, previously tagged images from a source image collection, and subsequently collect the most frequently used tags from these images. The NfcAnnotate system enables annotation at image capture time, by using NFC (Near Field Communication) and NFC information tags provided at the site of the attraction. NfcAnnotate enables clustering of topically related images, which makes it possible annotate a set of images in one annotation operation. In cases when NFC information tags are not available, NfcAnnotate image clustering can be combined with LoTagr to conveniently annotate every image in the cluster in a single operation.	automatic image annotation;cluster analysis;context awareness;flickr;geographic coordinate system;global positioning system;image retrieval;near field communication	Randi Karlsen;Anders Andersen	2014			image retrieval;computer science;data mining;automatic image annotation;world wide web;information retrieval	Vision	-16.83269232817147	-57.9409947824119	12714
f7db36880b2a5f59edda88c7273dc307bdc552b0	missing data mask estimation with frequency and temporal dependencies	acoustic modeling;time frequency;automatic speech recognition;missing data recognition;frequency and temporal dependencies;missing data masks estimation;missing data;computational linguistics;linguistique informatique;bayesian model	Automatic speech recognition (ASR) has reached a very high level of performance in controlled situations. However, the performance degrades drastically when environmental noise occurs during recognition. Nowadays, the major challenge is to reach a good robustness to adverse conditions. Missing data recognition has been developed to deal with this challenge. Unlike other denoising methods, missing data recognition does not match the whole data with the acoustic models, but instead considers part of the signal as missing, i.e. corrupted by noise. The main challenge of this approach is to identify accurately missing parts (also called masks). The work reported here focuses on this issue. We start from developing Bayesian models of the masks, where every spectral feature is classified as reliable or masked, and is assumed independent of the rest of the signal. This classification strategy results in sparse and isolated masked features, like the squares of a chess-board, while oracle reliable and unreliable features tend to be clustered into consistent time–frequency blocks. We then propose to take into account frequency and temporal dependencies in order to improve the masks’ estimation accuracy. Integrating such dependencies leads to a new architecture of a missing data mask estimator. The proposed classifier has been evaluated on the noisy Aurora2 (digits recognition) and Aurora4 (continuous speech) databases. Experimental results show a significant improvement of recognition accuracy when these dependencies are considered. 2008 Elsevier Ltd. All rights reserved.	acoustic cryptanalysis;acoustic fingerprint;bayesian network;codebook;database;ergodicity;experiment;frequency band;google map maker;hidden markov model;high-level programming language;markov chain;missing data;noise reduction;robustness (computer science);sparse matrix;spectral density;spectrogram;speech recognition;variable shadowing;vocabulary	Sébastien Demange;Christophe Cerisara;Jean Paul Haton	2009	Computer Speech & Language	10.1016/j.csl.2008.02.002	speech recognition;time–frequency analysis;missing data;computer science;computational linguistics;machine learning;pattern recognition;data mining;bayesian inference;statistics	Vision	-14.307798821295906	-92.18375348086887	12719
227d88df9fa48bd2de917e53523fedfaaab3127e	beyond actions: exploring the discovery of tactics from user logs	log analysis;search behaviour;search tactics;electronic computers computer science	Search log analysis has become a common practice to gain insights into user search behaviour; it helps gain an understanding of user needs and preferences, as well as an insight into how well a system supports such needs. Currently, log analysis is typically focused on low-level user actions, i.e. logged events such as issued queries and clicked results, and often only a selection of such events are logged and analysed. However, types of logged events may differ widely from interface to interface, making comparison between systems difficult. Further, the interpretation of the meaning of and subsequent analysis of a selection of events may lead to conclusions out of context—e.g. the statistics of observed query reformulations may be influenced by the existence of a relevance feedback component. Alternatively, in lab studies user activities can be analysed at a higher level, such as search tactics and strategies, abstracted away from detailed interface implementation. Unfortunately, until now the required manual codings that map logged events to higher-level interpretations have prevented large-scale use of this type of analysis. In this paper, we propose a new method for analysing search logs by (semi-)automatically identifying user search tactics from logged events, allowing large-scale analysis that is comparable across search systems. In addition, as the resulting analysis is at a tactical level we reduce potential issues surrounding the need for interpretation of low-level user actions for log analysis. We validate the efficiency and effectiveness of the proposed tactic identification method using logs of two reference search systems of different natures: a product search system and a video search system. With the identified tactics, we perform a series of novel log analyses in terms of entropy rate of user search tactic sequences, demonstrating how this type of analysis allows comparisons of user search behaviours across systems of different nature and design. This analysis provides insights not achievable with traditional log analysis.	complement (complexity);entropy (information theory);entropy rate;high- and low-level;log analysis;markov chain;relevance feedback;stationary process;vigor (software)	Jiyin He;Pernilla Qvarfordt;Martin Halvey;Gene Golovchinsky	2016	Inf. Process. Manage.	10.1016/j.ipm.2016.05.007	simulation;computer science;data mining;world wide web	Web+IR	-34.43759217527217	-53.18200217136037	12726
2b0555b53d9f2dd46b9ab579b8156e8d516d4919	audio recurrence contribution to a video-based tv program structuring approach	structure methods;audio signal processing;tv content indexing tv program structuring non linear browsing audio and video recurrence detection;image segmentation;detection algorithms;video signal processing;hidden markov model;television applications;automatic video indexing audio recurrence contribution video based tv program structuring approach unsupervised tv programs structuring news entertainment programs tv shows tv magazines video recurrence detection automatic video segmentation;tv program structuring non linear browsing audio and video recurrence detection tv content indexing;visualization;particle separators tv games detection algorithms hidden markov models feature extraction visualization;hidden markov models;audio and video recurrence detection;indexing;feature extraction;eurecom ecole d ingenieur telecommunication centre de recherche graduate school research center communication systems;games;indexation;video signal processing audio signal processing image segmentation indexing television applications;detection algorithm;particle separators;tv;tv program structuring;non linear browsing;tv content indexing	This paper addresses the problem of unsupervised TV programs structuring. Program structuring allows direct and non linear access to the desired parts of a program. Our work addresses the structuring of recurrent TV programs like news, entertainment programs, TV shows, TV magazines. In a previous work we proposed a program structuring method based on the detection of video recurrences. In this paper we extend our study to audio recurrences and verify their influence on the final structuring. We evaluate the structuring results on both approaches (audio and video) separately and jointly. We use for evaluation a 62 hours dataset corresponding to 97 episodes of TV programs.	algorithm;cluster analysis;natural language;precision and recall;recurrence relation;video	Alina Elma Abduraman;Sid-Ahmed Berrani;Bernard Mérialdo	2011	2011 IEEE International Symposium on Multimedia	10.1109/ISM.2011.15	games;computer vision;search engine indexing;visualization;audio signal processing;feature extraction;computer science;machine learning;multimedia;image segmentation;hidden markov model;computer graphics (images)	Arch	-14.070132956097506	-56.16671494935993	12743
a7dedfb7b831ebd1ef3889edaae483253dc2d167	authors vs. readers: a comparative study of document metadata and content in the www	google;web documents;search engine;social bookmarking;metadata;random sampling;qualitative analysis;web service;pagerank;dmoz100k06;authoring;world wide web;icra;document engineering;collaborative tagging;dmoz;www;tagging;del icio us	Collaborative tagging describes the process by which many users add metadata in the form of unstructured keywords to shared content. The recent practical success of web services with such a tagging component like Flickr or del.icio.us has provided a plethora of user-supplied metadata about web content for everyone to leverage.  In this paper, we conduct a quantitative and qualitative analysis of metadata and information provided by the authors and publishers of web documents compared with metadata supplied by end users for the same content. Our study is based on a random sample of 100,000 web documents from the Open Directory, for which we examined the original documents from the World Wide Web in addition to data retrieved from the social bookmarking service del.icio.us, the content rating system ICRA, and the search engine Google. To the best of our knowledge, this is the first study to compare user tags with the metadata and actual content of documents in the WWW on a larger scale and to integrate document popularity information in the observations. The data set of our experiments is freely available for research.	apple open directory;experiment;flickr;folksonomy;icra;sports rating system;www;web content;web page;web search engine;web service;world wide web	Michael G. Noll;Christoph Meinel	2007		10.1145/1284420.1284465	web service;document engineering;sampling;computer science;qualitative research;database;database catalog;internet privacy;web 2.0;metadata;world wide web;data element;meta data services;information retrieval;search engine;metadata repository	Web+IR	-30.749988838613227	-56.26612359741598	12745
f2ba5f5dfa5703b577f60f6b6af257c3e37203d0	coping with pacs downtime in digital radiology	hospital information system;pacs;server;failure;radiology information systems;radiology department hospital;equipment failure;picture archiving and communication system	As radiology departments become increasingly reliant on picture archiving and communication systems, they become more vulnerable to computer downtime that can paralyze a smoothly running department. The experiences and strategies developed during various types of picture archiving and communication system (PACS) downtime in a large radiology department that has completely converted to soft copy interpretation in all modalities except mammography are presented. Because these failures can be minimized but not eliminated, careful planning is necessary to minimize their impact.	archive;cns disorder;coping behavior;downtime;experience;mammography;picture archiving and communication systems;picture archiving and communication system;radiology;smoothing	Mike McBiles;Anna K. Chacko	2000	Journal of Digital Imaging	10.1007/BF03168387	simulation;radiology;medicine;computer science;multimedia;picture archiving and communication system	HCI	-52.655080070235925	-61.79077142313757	12794
250fc560788312077b20edf0c20f966f21a35434	textual complexity as a predictor of difficulty of listening items in language proficiency tests		In this paper we explore to what extent the difficulty of listening items in an English language proficiency test can be predicted by the textual properties of the item text. We show that a system based on multiple text complexity features can predict item difficulty for several different item types and for some items achieves higher accuracy than human estimates of item difficulty.	kerrison predictor;population;printing;text editor;vocabulary	Anastassia Loukina;Su-Youn Yoon;Jennifer Sakano;Youhua Wei;Kathy Sheehan	2016			natural language processing;artificial intelligence;computer science;language proficiency;active listening	NLP	-26.870614286222416	-83.4647640869371	12805
9b0983480eb24813cfa3aff840acf93bbea962b0	investigation on the pleasantness of music perception in monolateral and bilateral cochlear implant users by using neuroelectrical source imaging: a pilot study	pilot study;rhythm;neuroelectrical imaging music perception bilateral cochlear implant monolateral cochlear implant pleasantness eeg rhythms;healthy subjects;brain modeling;imaging;music perception;neurophysiology cochlear implants electroencephalography music;auditory perception brain mapping cerebral cortex child cochlear implants electroencephalography female humans male music pilot projects;neurophysiology;electroencephalography;spectral analysis;cochlear implant;music;cochlear implants;cochlear implants electroencephalography rhythm spectral analysis imaging brain modeling	There is a debate in the specialized literature about the quality of fruition of music for patients that received a cochlear implant. Interestingly, very few studies have investigated the hypothesis that patients that use a bilateral cochlear implant could perceive the music in a more pleasant way as compared to unilaterally implanted patients. Previous observations in healthy subjects have indicated that variations of particular EEG rhythms correlated with the pleasantness of the perceived music. The aim of the present pilot study is then to apply the state of the art neuroelectrical imaging and the analysis of cortical representation of EEG rhythms to monitor the perceived pleasantness during the observation of a simple videoclip in one patient with a unilateral cochlear implant and in one receiving a bilateral cochlear implant. Results of this pilot study showed that on the base of such previously validated EEG rhythms, the fruition of music and video, in terms of pleasantness, is statistically higher in the bilaterally implanted patient when compared to the monolateral implanted patient.	bilateral filter;cochlear implant;electroencephalography;patients;video clip	Giovanni Vecchiato;Jlenia Toppi;Laura Astolfi;Donatella Mattia;Paolo Malerba;Alessandro Scorpecci;Pasquale Marsella;Fabio Babiloni	2011	2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/IEMBS.2011.6092000	psychology;neuroscience;radiology;acoustics;electroencephalography;rhythm;music;communication;audiology;neurophysiology	Visualization	-5.5445325689309515	-84.39492552143254	12806
232fc5ff8557a92c801cf899ed08e6b80b1a3ec4	aligning medical domain ontologies for clinical query extraction	medical domain ontology;corresponding semantic search engine;medical imaging;clinical query extraction;complementary knowledge;medical image;related type;related patient text data;related application;multiple medical ontology;medical ontology alignment;medical knowledge;semantic integration;semantic search;ontology alignment	Often, there is a need to use the knowledge from multiple ontologies. This is particularly the case within the context of medical imaging, where a single ontology is not enough to provide the complementary knowledge about anatomy, radiology and diseases that is required by the related applications. Consequently, semantic integration of these different but related types of medical knowledge that is present in disparate domain ontologies becomes necessary. Medical ontology alignment addresses this need by identifying the semantically equivalent concepts across multiple medical ontologies. The resulting alignments can then be used to annotate the medical images and related patient text data. A corresponding semantic search engine that operates on these annotations (i.e. alignments) instead of simple keywords can, in this way, deliver the clinical users a coherent set of medical image and patient text data.	coherence (physics);medical imaging;ontology (information science);ontology alignment;radiology;semantic integration;semantic search;text corpus;web search engine	Pinar Wennerberg	2009			upper ontology;idef5;open biomedical ontologies;ontology alignment;semantic integration;semantic search;computer science;ontology;data mining;database;linguistics;ontology-based data integration;information retrieval;process ontology	ML	-48.71063119859118	-67.37321362082174	12819
2d45f4faca9aeacfab32a4be6fd0bc6a4f4ed862	high performance on-line session-adaptation for handling inter-session speaker variability in variable-text speaker-recognition	databases;unsupervised learning;unsupervised speaker recognition inter session variability session adaptation on line;speech synthesis;unsupervised learning speaker recognition speech synthesis;on line;probability density function;inter session variability;training;session adaptation;testing;unsupervised;data mining;speaker verification;speaker recognition;accuracy;indexes;adaptive algorithm;unsupervised learning online session adaptation algorithm variable text speaker recognition system inter session speaker variability handling speaker verification template updation;system testing concatenated codes pattern recognition robustness biometrics fingerprint recognition iris retina training data performance evaluation;modes of operation;high performance	We propose an on-line session-adaptation algorithm for making variable-text speaker-recognition systems robust to inter-session variability, by a continuous update of registered speakers' multiple templates from test utterances during actual usage of the system. The algorithm is set in a speaker-verification mode of operation and uses an enhanced verification step to ensure reliable selection of input utterances for template updation in an unsupervised way.	algorithm;block cipher mode of operation;heart rate variability;online and offline;speaker recognition	S. Thiyagarajan;V. Ramasubramanian	2009	2009 Seventh International Conference on Advances in Pattern Recognition	10.1109/ICAPR.2009.87	speech recognition;computer science;machine learning;pattern recognition	Robotics	-19.805093974372102	-90.96783230251917	12821
c6120ac39c36f24320aff30ea8792220725fee79	complex interventions in healthcare and health informatics: a scoping review		Complex interventions are pervasive in healthcare. There is a need to make sense of complex interventions to support their better development, implementation and evaluation. This paper summarizes the results of a scoping review undertaken to identify attributes of, and logistical considerations for complex interventions in healthcare. Results suggest five distinct attributes that can assist researchers to identify, conduct, and appraise complex interventions studies. Considerations for applying results to evaluate complex health informatics interventions are discussed.	clinical informatics;health occupations;informatics (discipline);logistics;pervasive informatics;scope (computer science)	Julie Kim	2013	Studies in health technology and informatics	10.3233/978-1-61499-203-5-263	health administration informatics;health care;psychological intervention;health informatics;medicine;nursing	HCI	-60.25385759108818	-61.80024691683156	12831
5c5a27eba409679a3e79e9507191253ec325e9df	biomedical question-focused multi-document summarization: ilsp and aueb at bioasq3		Question answering systems aim to find answers to natural language questions by searching in document collections (e.g., repositories of scientific articles or the entire Web) and/or structured data (e.g., databases, ontologies). Strictly speaking, the answer to a question might sometimes be simply ‘yes’ or ‘no’, a named entity, or a set of named entities. In practice, however, a more elaborate answer is often also needed, ideally a summary of the most important information from relevant documents and structured data. In this paper, we focus on generating summaries from documents that are known to be relevant to particular questions. We describe the joint participation of AUEB and ILSP in the corresponding subtask of the bioasq3 competition, where participants produce multi-document summaries of given biomedical articles that are relevant to English questions prepared by biomedical experts.	automatic summarization;database;multi-document summarization;named entity;natural language;ontology (information science);question answering;scientific literature	Prodromos Malakasiotis;Emmanouil Archontakis;Ion Androutsopoulos;Dimitrios Galanis;Harris Papageorgiou	2015			information retrieval;named entity;ontology (information science);question answering;natural language;data model;multi-document summarization;computer science	Web+IR	-33.817980619302	-67.66790044951364	12834
77c891fb78071c3a5ea307741d99a2cb21d42803	mitre at semeval-2016 task 6: transfer learning for stance detection		We describe MITRE’s submission to the SemEval-2016 Task 6, Detecting Stance in Tweets. This effort achieved the top score in Task A on supervised stance detection, producing an average F1 score of 67.8 when assessing whether a tweet author was in favor or against a topic. We employed a recurrent neural network initialized with features learned via distant supervision on two large unlabeled datasets. We trained embeddings of words and phrases with the word2vec skip-gram method, then used those features to learn sentence representations via a hashtag prediction auxiliary task. These sentence vectors were then finetuned for stance detection on several hundred labeled examples. The result was a high performing system that used transfer learning to maximize the value of the available training data.	artificial neural network;experiment;f1 score;hashtag;n-gram;recurrent neural network;semeval;test set;word2vec	Guido Zarrella;Amy Marsh	2016			speech recognition;computer science;artificial intelligence;machine learning;pattern recognition	NLP	-21.66918685403075	-70.07184462112266	12863
707c845f5d847b104a67142bd44d2189c8ac7a6c	the choice of outpatient computerized patient record systems: questions to ask	biomedical research;bioinformatics			Judith R. Logan;Jon A. Blackman	1999			ask price;data science;medicine	Logic	-56.82849690663503	-65.00672577976826	12869
daeab92707c9c6b689c6671d4b9216e00b823b29	an entity-driven recursive neural network model for chinese discourse coherence modeling		Chinese discourse coherence modeling remains a challenge taskin Natural Language Processing field.Existing approaches mostlyfocus on the need for feature engineering, whichadoptthe sophisticated features to capture the logic or syntactic or semantic relationships acrosssentences within a text.In this paper, we present an entity-drivenrecursive deep modelfor the Chinese discourse coherence evaluation based on current English discourse coherenceneural network model. Specifically, to overcome the shortage of identifying the entity(nouns) overlap across sentences in the currentmodel, Our combined modelsuccessfully investigatesthe entities information into the recursive neural network freamework.Evaluation results on both sentence ordering and machine translation coherence rating task show the effectiveness of the proposed model, which significantly outperforms the existing strong baseline.	artificial neural network;baseline (configuration management);entity;feature engineering;machine translation;natural language processing;network model;recursion;recursive neural network	Fan Xu;Shujing Du;Maoxi Li;Mingwen Wang	2017	CoRR	10.5121/ijaia.2017.8201	natural language processing;speech recognition;computer science;artificial intelligence;machine learning;pattern recognition	NLP	-18.894221838279186	-73.13768346766491	12912
25dce02e1c836ef15d5f4f9b9e0fd79f63e544f1	going to extremes: the influence of unsupervised categories on the mental caricaturization of faces and asymmetries in perceptual discrimination	social and behavioral sciences	Recent re-analysis of traditional Categorical Perception (CP) effects show that the advantage for between category judgments may be due to asymmetries of within-category judgments (Hanley & Roberson, 2011). This has led to the hypothesis that labels cause CP effects via these asymmetries due to category label uncertainty near the category boundary. In Experiment 1 we demonstrate that these “within-category” asymmetries exist before category training begins. Category learning does increase the within-category asymmetry on a category relevant dimension but equally on an irrelevant dimension. Experiment 2 replicates the asymmetry found in Experiment 1 without training and shows that it does not increase with additional exposure in the absence of category training. We conclude that the within-category asymmetry may be a result of unsupervised learning of stimulus clusters that emphasize extreme instances and that category training increases this caricaturization of stimulus representations.	experiment;relevance;unsupervised learning	Andrew Hendrickson;Paulo F. Carvalho;Robert L. Goldstone	2012			psychology;cognitive psychology;artificial intelligence;mathematics;communication;social psychology;cognitive science	ML	-7.4374275237295	-76.0249452121371	12947
9cfbd1641866f0693689ca6f87f7c6a0cb89f296	improving query spelling correction using web search results	web search	Traditional research on spelling correction in natural language processing and information retrieval literature mostly relies on pre-defined lexicons to detect spelling errors. But this method does not work well for web query spelling correction, because there is no lexicon that can cover the vast amount of terms occurring across the web. Recent work showed that using search query logs helps to solve this problem to some extent. However, such approaches cannot deal with rarely-used query terms well due to the data sparseness problem. In this paper, a novel method is proposed for use of web search results to improve the existing query spelling correction models solely based on query logs by leveraging the rich information on the web related to the query and its top-ranked candidate. Experiments are performed based on realworld queries randomly sampled from search engine’s daily logs, and the results show that our new method can achieve 16.9% relative F-measure improvement and 35.4% overall error rate reduction in comparison with the baseline method.	baseline (configuration management);experiment;information retrieval;lexicon;natural language processing;neural coding;pagerank;randomness;spell checker;web search engine	Mu Li;Ming Zhou	2007			natural language processing;sargable;query optimization;query expansion;web query classification;ranking;computer science;data mining;web search query;world wide web;information retrieval;query language;search engine	Web+IR	-31.012764702807978	-59.20272997300184	12965
8f3ff24a73ffb788545429ba82c78faa920e1529	learning semantics with deep belief network for cross-language information retrieval		This paper introduces a cross-language information retrieval (CLIR) framework that combines the state-of-the-art keyword-based approach with a latent semantic-based retrieval model. To capture and analyze the hidden semantics in cross-lingual settings, we construct latent semantic models that map text in different languages into a shared semantic space. Our proposed framework consists of deep belief networks (DBN) for each language and we employ canonical correlation analysis (CCA) to construct a shared semantic space. We evaluated the proposed CLIR approach on a standard ad hoc CLIR dataset, and we show that the cross-lingual semantic analysis with DBN and CCA improves the state-of-the-art keyword-based CLIR performance.	bayesian network;cross-language information retrieval;deep belief network;hoc (programming language)	Jungi Kim;Jinseok Nam;Iryna Gurevych	2012			natural language processing;semantic computing;computer science;data mining;probabilistic latent semantic analysis;information retrieval	Web+IR	-18.973723497026736	-67.35528790305878	12975
8a7bcc5e8842585bb0eeff4a4a006b56533685cc	cvtron web: a versatile framework for online computer vision services		Recently, computer vision has aroused the greatest interest. Many companies have developed online inference system in the field of computer vision associated with web services, however, it still lacks easy-to-use online training system. This paper introduces CVTron Web system, an open source framework that serves as a web-based system and a dashboard for handling the training task, testing the inference, and checking hardware capabilities. By clicking on the web dashboard, developers or those having little programming knowledge will be able to complete several computer vision tasks such as image classification, object detection, and image segmentation for both training and inference.	computer vision	Yingying Chen;Xiaozhe Yao	2018		10.1007/978-3-319-94472-2_5	dashboard (business);online computer;database;training system;image segmentation;object detection;computer science;web service;human–computer interaction;inference;contextual image classification	Vision	-46.689486390411226	-60.66406020906567	12996
a253f215833a0533bf2749964ba95577f122e33c	unsupervised semantic role induction with global role ordering	local role assignment decision;probabilistic generative model;unsupervised semantic role induction;primary role;syntactic constituent;global role;unified model;secondary role;local feature;role sequence	We propose a probabilistic generative model for unsupervised semantic role induction, which integrates local role assignment decisions and a global role ordering decision in a unified model. The role sequence is divided into intervals based on the notion of primary roles, and each interval generates a sequence of secondary roles and syntactic constituents using local features. The global role ordering consists of the sequence of primary roles only, thus making it a partial ordering.	generative model;unified model;unsupervised learning	Nikhil Garg;James Henserdon	2012			knowledge management;machine learning	NLP	-24.62094034201623	-72.85540340971393	13011
391dde266e2d931cc80c2e2370ba22885ffa0bb9	a fusion approach to cluster labeling	fusion;cluster labeling	We present a novel approach to the cluster labeling task using fusion methods. The core idea of our approach is to weigh labels, suggested by any labeler, according to the estimated labeler's decisiveness with respect to each of its suggested labels. We hypothesize that, a cluster labeler's labeling choice for a given cluster should remain stable even in the presence of a slightly incomplete cluster data. Using state-of-the-art cluster labeling and data fusion methods, evaluated over a large data collection of clusters, we demonstrate that, overall, the cluster labeling fusion methods that further consider the labeler's decisiveness provide the best labeling performance.	label printer applicator	Haggai Roitman;Shay Hummel;Michal Shmueli-Scheuer	2014		10.1145/2600428.2609465	fusion;computer science;data mining	DB	-18.668471412130884	-64.13519293943381	13016
59fe07a7f315e563e89e6402184d11083311d1c8	a simple approach to make dialogue systems incremental (vers une approche simplifiée pour introduire le caractère incrémental dans les systèmes de dialogue) [in french]		Incremental dialogue is at the heart of current research in the field of dialogue systems. Several architectures and models have been published such as (Allen et al., 2001; Schlangen & Skantze, 2011). This work has made it possible to understand many aspects of incremental dialogue, however, in order to implement these solutions, one needs to start from scratch as the existing architectures are inherently different. Our approach is different as it tends towards a new generation of incremental systems that behave incrementally but work internally in a traditional way. This paper suggests inserting a new module, called the Scheduler, between the service and the client. This Scheduler manages the asynchronous events, hence reproducing the behaviour of incremental systems from the client’s point of view. On the other end, the service does not work incrementally. Mots-clés : Systèmes de Dialogue, Traitement Incrémental, Architecture des Systèmes de Dialogue.	dialog system;word lists by frequency	Hatim Khouzaimi;Romain Laroche;Fabrice Lefèvre	2014				NLP	-34.791042795991466	-81.37391078338898	13030
c29b56325567ff66390860ba4faa6c373aadcff4	un método independiente del idioma para responder preguntas de definición	h;information systems	This paper describes a method for answering definition questions that is exclusively based on the use of lexical patterns, and, therefore, that is language independent. This method applies two main text-mining steps. The first step focuses on the discovery of a set of surface lexical patterns from definition examples downloaded from the Web. Subsequently, it uses these patterns to extract a set of concept-description pairs from a given target document collection. The second step applies a text-mining algorithm to determine the most adequate answer to each specific question. Experimental results were obtained using the datasets from the CLEF 2005 and 2006 for the monolingual tasks in Spanish, French and Italian. These results demonstrate the relevance of the method which showed very high precisions for the three languages.	algorithm;archive;relevance;text mining;world wide web	Claudia Denicia-Carral;Luis Villaseñor Pineda;Manuel Montes-y-Gómez	2010	Computación y Sistemas			NLP	-29.652971499337475	-71.19324660968896	13036
05da78077791218eb294f6a51e9084b72c62a26c	a graph-based ranking model for automatic keyphrases extraction from arabic documents		Automatic keyphrases extraction is to extract a set of phrases that are related to the main topics discussed in a document. They have served in several areas of text mining such as information retrieval and classification of a large text collection. Consequently, they have proved their effectiveness. Due to its importance, automatic keyphrases extraction from Arabic documents has received a lot of attention. For instance, the KP-Miner system was proposed to extract Arabic keyphrases, and demonstrates through experimentation and comparison with other systems its effectiveness. In this paper, we introduce TextRank, a graph-based ranking model, used successfully in many tasks of text processing, to compute term weights from graphs of documents. Vertices represent the document’s terms, and edges represent term co-occurrence within a fixed window. It is an innovative unsupervised method that we have adapted to extract Arabic keyphrases, and assess its effectiveness. The obtained results with TextRank are compared with those obtained with KPMiner, owing to the fact that both systems do not need a training step.	information retrieval;pixel;text mining;unsupervised learning	Mohamed Salim El Bazzi;Driss Mammass;Taher Zaki;Abdellatif Ennaji	2017		10.1007/978-3-319-62701-4_25	arabic;computer science;artificial intelligence;text processing;pattern recognition;ranking;text mining;graph	Web+IR	-26.57453246428335	-64.51895808337648	13047
5fae99c5103d79646be53386c5b876f7c0352431	evaluation of multilingual and multi-modal information retrieval, 7th workshop of the cross-language evaluation forum, clef 2006, alicante, spain, september 20-22, 2006, revised selected papers	cross language evaluation forum;information retrieval;universiteitsbibliotheek	"""What Happened in CLEF 2006.- Scientific Data of an Evaluation Campaign: Do We Properly Deal with Them?.- I: Multilingual Textual Document Retrieval (Ad Hoc).- CLEF 2006: Ad Hoc Track Overview.- Hindi, Telugu, Oromo, English CLIR Evaluation.- Amharic-English Information Retrieval.- The University of Lisbon at CLEF 2006 Ad-Hoc Task.- Query and Document Translation for English-Indonesian Cross Language IR.- Passage Retrieval vs. Document Retrieval in the CLEF 2006 Ad Hoc Monolingual Tasks with the IR-n System.- The PUCRS NLP-Group Participation in CLEF2006: Information Retrieval Based on Linguistic Resources.- NLP-Driven Constructive Learning for Filtering an IR Document Stream.- ENSM-SE at CLEF 2006 : Fuzzy Proxmity Method with an Adhoc Influence Function.- A Study on the Use of Stemming for Monolingual Ad-Hoc Portuguese Information Retrieval.- Benefits of Resource-Based Stemming in Hungarian Information Retrieval.- Statistical vs. Rule-Based Stemming for Monolingual French Retrieval.- A First Approach to CLIR Using Character N-Grams Alignment.- SINAI at CLEF 2006 Ad Hoc Robust Multilingual Track: Query Expansion Using the Google Search Engine.- Robust Ad-Hoc Retrieval Experiments with French and English at the University of Hildesheim.- Comparing the Robustness of Expansion Techniques and Retrieval Measures.- Experiments with Monolingual, Bilingual, and Robust Retrieval.- Local Query Expansion Using Terms Windows for Robust Retrieval.- Dublin City University at CLEF 2006: Robust Cross Language Track.- JHU/APL Ad Hoc Experiments at CLEF 2006.- II: Domain-Specific Information Retrieval (Domain-Specific).- The Domain-Specific Track at CLEF 2006: Overview of Approaches, Results and Assessment.- Reranking Documents with Antagonistic Terms.- Domain Specific Retrieval: Back to Basics.- Monolingual Retrieval Experiments with a Domain-Specific Document Corpus at the Chemnitz University of Technology.- III: Interactive Cross-Langauge Information Retrieval (i-CLEF).- iCLEF 2006 Overview: Searching the Flickr WWW Photo-Sharing Repository.- Are Users Willing to Search Cross-Language? An Experiment with the Flickr Image Sharing Repository.- Providing Multilingual Access to FLICKR for Arabic Users.- Trusting the Results in Cross-Lingual Keyword-Based Image Retrieval.- IV: Multiple Language Question Answering (QA@CLEF).- Overview of the CLEF 2006 Multilingual Question Answering Track.- Overview of the Answer Validation Exercise 2006.- Overview of the WiQA Task at CLEF 2006.- Re-ranking Passages with LSA in a Question Answering System.- Question Types Specification for the Use of Specialized Patterns in Prodicos System.- Answer Translation: An Alternative Approach to Cross-Lingual Question Answering.- Priberam's Question Answering System in a Cross-Language Environment.- LCC's PowerAnswer at QA@CLEF 2006.- Using Syntactic Knowledge for QA.- A Cross-Lingual German-English Framework for Open-Domain Question Answering.- Cross Lingual Question Answering Using QRISTAL for CLEF 2006.- CLEF2006 Question Answering Experiments at Tokyo Institute of Technology.- Quartz: A Question Answering System for Dutch.- Experiments on Applying a Text Summarization System for Question Answering.- N-Gram vs. Keyword-Based Passage Retrieval for Question Answering.- Cross-Lingual Romanian to English Question Answering at CLEF 2006.- Finding Answers in the xdipe System by Extracting and Applying Linguistic Patterns.- Question Answering Beyond CLEF Document Collections.- Using Machine Learning and Text Mining in Question Answering.- Applying Dependency Trees and Term Density for Answer Selection Reinforcement.- Interpretation and Normalization of Temporal Expressions for Question Answering.- Relevance Measures for Question Answering, The LIA at QA@CLEF-2006.- Monolingual and Cross-Lingual QA Using AliQAn and BRILI Systems for CLEF 2006.- The Bilingual System MUSCLEF at QA@CLEF 2006.- MIRACLE Experiments in QA@CLEF 2006 in Spanish: Main Task, Real-Time QA and Exploratory QA Using Wikipedia (WiQA).- A First Step to Address Biography Generation as an Iterative QA Task.- The Effect of Entity Recognition on Answer Validation.- A Knowledge-Based Textual Entailment Approach Applied to the AVE Task.- Automatic Answer Validation Using COGEX.- Paraphrase Substitution for Recognizing Textual Entailment.- Experimenting a """"General Purpose"""" Textual Entailment Learner in AVE.- Answer Validation Through Robust Logical Inference.- University of Alicante at QA@CLEF2006: Answer Validation Exercise.- Towards Entailment-Based Question Answering: ITC-irst at CLEF 2006.- Link-Based vs. Content-Based Retrieval for Question Answering Using Wikipedia.- Identifying Novel Information Using Latent Semantic Analysis in the WiQA Task at CLEF 2006.- A Bag-of-Words Based Ranking Method for the Wikipedia Question Answering Task.- University of Alicante at WiQA 2006.- A High Precision Information Retrieval Method for WiQA.- QolA: Fostering Collaboration Within QA.- V: Cross-Language Retrieval in Image Collections (ImageCLEF).- Overview of the ImageCLEF 2006 Photographic Retrieval and Object Annotation Tasks.- Overview of the ImageCLEFmed 2006 Medical Retrieval and Medical Annotation Tasks.- Text Retrieval and Blind Feedback for the ImageCLEFphoto Task.- Expanding Queries Through Word Sense Disambiguation.- Using Visual Linkages for Multilingual Image Retrieval.- Approaches of Using a Word-Image Ontology and an Annotated Image Corpus as Intermedia for Cross-Language Image Retrieval.- Dublin City University at CLEF 2006: Experiments for the ImageCLEF Photo Collection Standard Ad Hoc Task.- Image Classification with a Frequency-Based Information Retrieval Scheme for ImageCLEFmed 2006.- Grayscale Radiograph Annotation Using Local Relational Features.- MorphoSaurus in ImageCLEF 2006: The Effect of Subwords On Biomedical IR.- Medical Image Retrieval and Automated Annotation: OHSU at ImageCLEF 2006.- MedIC at ImageCLEF 2006: Automatic Image Categorization and Annotation Using Combined Visual Representations.- Medical Image Annotation and Retrieval Using Visual Features.- Baseline Results for the ImageCLEF 2006 Medical Automatic Annotation Task.- A Refined SVM Applied in Medical Image Annotation.- Inter-media Concept-Based Medical Image Indexing and Retrieval with UMLS at IPAL.- UB at ImageCLEFmed 2006.- Translation by Text Categorisation: Medical Image Retrieval in ImageCLEFmed 2006.- Using Information Gain to Improve the ImageCLEF 2006 Collection.- CINDI at ImageCLEF 2006: Image Retrieval & Annotation Tasks for the General Photographic and Medical Image Collections.- Image Retrieval and Annotation Using Maximum Entropy.- Inter-media Pseudo-relevance Feedback Application to ImageCLEF 2006 Photo Retrieval.- ImageCLEF 2006 Experiments at the Chemnitz Technical University.- VI: Cross-Language Speech Retrieval (CLSR).- Overview of the CLEF-2006 Cross-Language Speech Retrieval Track.- Benefit of Proper Language Processing for Czech Speech Retrieval in the CL-SR Task at CLEF 2006.- Applying Logic Forms and Statistical Methods to CL-SR Performance.- XML Information Retrieval from Spoken Word Archives.- Experiments for the Cross Language Speech Retrieval Task at CLEF 2006.- CLEF-2006 CL-SR at Maryland: English and Czech.- Dublin City University at CLEF 2006: Cross-Language Speech Retrieval (CL-SR) Experiments.- VII: Multilingual Web Track (WebCLEF).- Overview of WebCLEF 2006.- Improving Web Pages Retrieval Using Combined Fields.- A Penalisation-Based Ranking Approach for the Mixed Monolingual Task of WebCLEF 2006.- Index Combinations and Query Reformulations for Mixed Monolingual Web Retrieval.- Multilingual Web Retrieval Experiments with Field Specific Indexing Strategies for WebCLEF 2006 at the University of Hildesheim.- Vocabulary Reduction and Text Enrichment at WebCLEF.- Experiments with the 4 Query Sets of WebCLEF 2006.- Applying Relevance Feedback for Retrieving Web-Page Retrieval.- VIII: Cross-Language Geographical Retrieval (GeoCLEF).- GeoCLEF 2006: The CLEF 2006 Cross-Language Geographic Information Retrieval Track Overview.- MIRACLE's Ad-Hoc and Geographical IR Approaches for CLEF 2006.- GIR Experimentation.- GIR with Geographic Query Expansion.- Monolingual and Bilingual Experiments in GeoCLEF2006.- Experiments on the Exclusion of Metonymic Location Names from GIR.- The University of New South Wales at GeoCLEF 2006.- GEOUJA System. The First Participation of the University of Jaen at GEOCLEF 2006.- R2D2 at GeoCLEF 2006: A Combined Approach.- MSRA Columbus at GeoCLEF 2006.- Forostar: A System for GIR.- NICTA I2D2 Group at GeoCLEF 2006.- Blind Relevance Feedback and Named Entity Based Query Expansion for Geographic Retrieval at GeoCLEF 2006.- A WordNet-Based Indexing Technique for Geographical Information Retrieval.- University of Twente at GeoCLEF 2006: Geofiltered Document Retrieval.- TALP at GeoCLEF 2006: Experiments Using JIRS and Lucene with the ADL Feature Type Thesaurus.- GeoCLEF Text Retrieval and Manual Expansion Approaches.- UB at GeoCLEF 2006.- The University of Lisbon at GeoCLEF 2006."""	information retrieval;modal logic		2007		10.1007/978-3-540-74999-8	natural language processing;computer science;data mining;database;information retrieval	NLP	-32.22288950170296	-63.67559378003636	13068
23712ec08701cfaead675254b3ea632272c56a03	geographic information extraction, disambiguation and ranking techniques	geographic ranking;toponym disambiguation strategies;geographic similarity measures;geographic information retrieval;geographic ontologies	An important part of textual information around the world contains some kind of geographic features. User queries with geographic references are becoming very common and human expectations from a search engine are even higher. Although several works have been focused on this area, the interpretation of the geographic information in order to better satisfy the user needs continues being a challenge. This work proposes different techniques which are involved in the process of identifying and analyzing the geographic information in textual documents and queries in natural languages. A geographic ontology GeoNW has been built by combining GeoNames, WordNet and Wikipedia resources. Based on the information stored in GeoNW, geographic terms are identified and an algorithm for solving the toponym disambiguation problem is proposed. Once the geographic information is processed, we obtain a geographic ranking list of documents which is combined with a standard textual ranking list of documents for producing the final results. GeoCLEF test collection is used for evaluating the accuracy of the result.	algorithm;geonames;geographic coordinate system;information extraction;natural language;toponym resolution;web search engine;wikipedia;word-sense disambiguation;wordnet	Yisleidy Linares Zaila;Danilo Montesi	2015		10.1145/2837689.2837695	local information systems;computer science;geospatial analysis;data mining;volunteered geographic information;database;information retrieval	Web+IR	-30.499135446067992	-60.090261952277864	13108
0f0d843097dfe00c926673e0f9edd310836efb9b	parser engineering and performance profiling	processing system;performance profiling;system performance;performance profile;detailed technical level;system refinement;detailed performance profile;parsing system;parsing strategy;parser engineering;salient performance metrics	We describe and argue for a strategy of performance profiling and comparison in the engineering of parsing systems for wide-coverage linguistic grammars. A performance profile is a precise, rich and structured snapshot of system (and grammar) behaviour at a given development point. The aim is to characterize system performance at a very detailed technical level, but at the same time to abstract away from idiosyncracies of particular processors. Profiles are obtained with minimal effort by applying a specialized profiling tool to a set of structured reference data (taken from both existing test suites and corpora), in conjunction with a uniform format for test data and processing results. The resulting profiles can be analyzed and visualized at various levels of granularity in order to highlight different aspects of system performance, thus providing a solid empirical basis for system refinement and optimization. Since profiles are stored in a database, comparison with earlier versions, different parameter settings, or other processing systems is straightforward. We apply several salient performance metrics in a contrastive discussion of various (one-pass, bottom-up, chart-based) parsing strategies (viz. passive vs. active and uni- vs. bidirectional approaches). Based on insights gained from detailed performance profiles, we outline and evaluate a novel ‘hyper-active’ parsing strategy. We also present preliminary profiles for techniques for ‘packing’ of local ambiguities with respect to (partial) subsumption of feature structures.	parser;profiling (computer programming);software performance testing	Stephan Oepen;John A. Carroll	2000	Natural Language Engineering		natural language processing;computer science;data mining;computer performance;algorithm	SE	-37.958322928471034	-80.72264979824699	13123
8d62b296b3374f242ef081255a208e2be2240acd	health 2.0: the digital health revolution		INIT (e.g., the increasing afford-ability and sophistication of sensor technology) have only served to more deeply embed Health 2.0 into our everyday environments. Beyond ubiquity, however , Health 2.0 has dramatically reformed the role patients play in their own healthcare. Just like Jane, many of us are active participants in managing our own health: We turn to technology to help us lose weight, improve long-term memory capacity, figure out what triggers our asthma attacks, and to collaborate and share experiences with people who have the same health concerns as us. And as a result of our participation in Health 2.0, health-related data—whether in I t's the crack of dawn, and Jane is on her morning run with her smartphone. She uses a running app to pace herself through the full six miles without stopping. She notices her split times are a bit slower than usual, and wonders if it has something to do with her restless night; she had a tough time falling asleep. When she gets home, Jane examines the charts her wireless scale generates. Over the past two weeks, her body fat percentage has dropped, and her weight has gone up: She's built some muscle. Looks like incorporating those strength sessions into her morning workouts is paying off! While it's hard today to imagine a world in which technology and health are not enmeshed, the phenomenon is somewhat recent. The tech + health (or Health 2.0) movement is rooted in the mid '90s, with the advent of the commercial web. The Internet made medical information available to anyone behind a browser: No longer did people have to overcome financial, physical , or geographic barriers to learn about diabetes treatments, for example, or to research Lyme Disease symptoms. Since then, other technological advances The idea that we can leverage technology on novel data to improve both our own well-being as well as that of those around us is compelling. the form of plain text, survey responses, or even raw biometric data—is both abundant and easier to collect , store, and share than ever before. In the past few decades, Health 2.0 has grown into one of the most vibrant and active topics in computer science, and it is easy to intuit why. In a world in which almost anything can be measured, we have all become, to some degree, direct beneficiaries of the Health 2.0 movement. The idea that …	biometrics;chart;computer science;experience;health 2.0;init;internet;jane (software);lyme (software bundle);smartphone;world wide web	Diana L. MacLean	2014	ACM Crossroads	10.1145/2676580	internet privacy;digital health;computer science	HCI	-58.7631610001656	-54.44826124378294	13129
0bbbd275426967851cc230f8617e3f2cc64cd210	panel: shared resources, shared code, and shared activities in clinical natural language processing			natural language processing	Guergana K. Savova;Wendy W. Chapman;Noémie Elhadad;Martha Palmer	2013			natural language processing;computer science;artificial intelligence	HCI	-32.411197005637014	-78.28744204352319	13145
001ba3193d04b84fa061c057bf0e4c0239878c75	a cross-sectional study assessing determinants of the attitude to the introduction of ehealth services among patients suffering from chronic conditions	health research;uk clinical guidelines;biological patents;health informatics;europe pubmed central;citation search;information systems and communication service;uk phd theses thesis;management of computing and information systems;life sciences;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	BACKGROUND Provision of care to patients with chronic diseases remains a great challenge for modern health care systems. eHealth is indicated as one of the strategies which could improve care delivery to this group of patients. The main objective of this study was to assess determinants of the acceptance of the Internet use for provision of chosen health care services remaining in the scope of current nationwide eHealth initiative in Poland.   METHODS The survey was carried out among patients with diagnosed chronic conditions who were treated in three health care facilities in Krakow, Poland. Survey data was used to develop univariate and multivariate logistic regression models for six outcome variables originating from the items assessing the acceptance of specific types of eHealth applications. The variables used as predictors were related to the sociodemographic characteristics of respondents, burden related to chronic disease, and the use of the Internet and its perceived usefulness in making personal health-related decisions.   RESULTS Among 395 respondents, there were 60.3% of Internet users. Univariate logistic regression models developed for six types of eHealth solutions demonstrated their higher acceptance among younger respondents, living in urban areas, who have attained a higher level of education, used the Internet on their own, and were more confident about its usefulness in making health-related decisions. Furthermore, the duration of chronic disease and hospitalization due to chronic disease predicted the acceptance of some of eHealth applications. However, when combined in multivariate models, only the belief in the usefulness of the Internet (five of six models), level of education (four of six models), and previous hospitalization due to chronic disease (three of six models) maintained the effect on the independent variables.   CONCLUSIONS The perception of the usefulness of the Internet in making health-related decision is a key determinant of the acceptance of provision of health care services online among patients with chronic diseases. Among sociodemographic factors, only the level of education demonstrates a consistent impact on the level of acceptance. Interestingly, a greater burden of chronic disease related to previous hospitalizations leads to lower acceptance of eHealth solutions.	chronic disease;cross-sectional data;health care;health services;hospitalization;internet;logistic regression;patients;solutions	Mariusz Duplaga	2015		10.1186/s12911-015-0157-3	health informatics;alternative medicine;medical research;medicine;nursing;ehealth;health care	HCI	-61.793620460029416	-64.87117040924483	13152
1506b8dfff1401c5a1dd3264408573768934f2bd	finding the importance of facial features in social trait perception		We are constantly making very fast attributions from faces, such as whether a person is trustworthy or threatening, that influence our behavior towards people. In this work, we present a method to automatically tell the importance of facial features on social trait perception. We employ an unsupervised clustering method to group the facial features by similarity and then create a model which explains the contribution of each facial feature to each social trait by means of a Genetic Algorithm. Our model deals with the difficulties associated to quantifying social impression using judgments from human observers (low inter- and intra-observer agreement) and obtains significant correlations greater than 0.7 for all social impressions, which justifies the method developed. Finally, the weights of the eyebrows, eyes, nose, mouth, jawline and facial feature distances are shown and discussed. This work poses a step forward in social trait impression understanding, as to the date, there is no other work quantifying the effects of facial features on social trait perception.		Félix Fuentes-Hurtado;Jose Antonio Diego-Mas;Valery Naranjo;Mariano Alcañiz	2018		10.1007/978-3-030-03493-1_5	genetic algorithm;machine learning;trustworthiness;trait;computer science;cluster analysis;artificial intelligence;impression;cognitive psychology;attribution;perception	ML	-51.086740704128765	-55.31787012889716	13160
a7ec16094004149f83c5f906b35d3b6e04d00fd8	formalization and computation of quality measures based on electronic medical records	general practice;netherlands;technology;practice guidelines as topic;teknikvetenskap;article letter to editor;engineering and technology;secondary use of patient data;teknik och teknologier;identification of patient cohorts;quality indicators;quality indicators health care;emr driven phenotyping;humans;quality measures;electronic medical record;natural language processing;electronic health records;programming languages	OBJECTIVE Ambiguous definitions of quality measures in natural language impede their automated computability and also the reproducibility, validity, timeliness, traceability, comparability, and interpretability of computed results. Therefore, quality measures should be formalized before their release. We have previously developed and successfully applied a method for clinical indicator formalization (CLIF). The objective of our present study is to test whether CLIF is generalizable--that is, applicable to a large set of heterogeneous measures of different types and from various domains.   MATERIALS AND METHODS We formalized the entire set of 159 Dutch quality measures for general practice, which contains structure, process, and outcome measures and covers seven domains. We relied on a web-based tool to facilitate the application of our method. Subsequently, we computed the measures on the basis of a large database of real patient data.   RESULTS Our CLIF method enabled us to fully formalize 100% of the measures. Owing to missing functionality, the accompanying tool could support full formalization of only 86% of the quality measures into Structured Query Language (SQL) queries. The remaining 14% of the measures required manual application of our CLIF method by directly translating the respective criteria into SQL. The results obtained by computing the measures show a strong correlation with results computed independently by two other parties.   CONCLUSIONS The CLIF method covers all quality measures after having been extended by an additional step. Our web tool requires further refinement for CLIF to be applied completely automatically. We therefore conclude that CLIF is sufficiently generalizable to be able to formalize the entire set of Dutch quality measures for general practice.	am broadcasting;common logic;computability;computation (action);denominator;document completion status - documented;electronic health records;electronics, medical;elfacos ow 100;experiment;first draft of a report on the edvac;genetic heterogeneity;information model;julius;natural language;peer review;question (inquiry);refinement (computing);revision procedure;sql;structured query language;traceability;web application;anonymized;general practice (field);interest;quality measures;replication compartment	Kathrin Dentler;Mattijs E. Numans;Annette ten Teije;Ronald Cornet;Nicolette de Keizer	2014	Journal of the American Medical Informatics Association : JAMIA	10.1136/amiajnl-2013-001921	natural language processing;computer science;artificial intelligence;data science;data mining;database;technology	Comp.	-51.423690015532614	-67.46340153645134	13164
4020bb19c14d36d788195969b1b8281157d74384	combining supervised and unsupervised polarity classification for non-english reviews	non-english corpus;main approach;training data;non-english review;english linguistic resource;sentiment polarity;unsupervised polarity classification;better result;unsupervised method;arabic corpus;unsupervised technique;linguistic resource	Twomain approaches are used in order to detect the sentiment polarity from reviews. The supervised methods apply machine learning algorithms when training data are provided and the unsupervised methods are usually applied when linguistic resources are available and training data are not provided. Each one of them has its own advantages and disadvantages and for this reason we propose the use of meta-classifiers that combine both of them in order to classify the polarity of reviews. Firstly, the non-English corpus is translated to English with the aim of taking advantage of English linguistic resources. Then, it is generated two machine learning models over the two corpora (original and translated), and an unsupervised technique is only applied to the translated version. Finally, the three models are combined with a voting algorithm. Several experiments have been carried out using Spanish and Arabic corpora showing that the proposed combination approach achieves better results than those obtained by using the methods separately.	algorithm;experiment;machine learning;supervised learning;tree-meta;text corpus;unsupervised learning	José Manuel Perea Ortega;Eugenio Martínez-Cámara;María Teresa Martín-Valdivia;Luis Alfonso Ureña López	2013		10.1007/978-3-642-37256-8_6	natural language processing;unsupervised learning;computer science;machine learning;pattern recognition	NLP	-23.712532636281104	-69.41130786836071	13168
9fc2c33ae020eab0479a052f80fe5c96c9395900	completeness and timeliness of notifiable disease reporting: a comparison of laboratory and provider reports submitted to a large county health department	completeness;disease notification;electronic laboratory reporting;health information exchange;public health surveillance;timeliness	BACKGROUND Most public health agencies expect reporting of diseases to be initiated by hospital, laboratory or clinic staff even though so-called passive approaches are known to be burdensome for reporters and produce incomplete as well as delayed reports, which can hinder assessment of disease and delay recognition of outbreaks. In this study, we analyze patterns of reporting as well as data completeness and timeliness for traditional, passive reporting of notifiable disease by two distinct sources of information: hospital and clinic staff versus clinical laboratory staff. Reports were submitted via fax machine as well as electronic health information exchange interfaces.   METHODS Data were extracted from all submitted notifiable disease reports for seven representative diseases. Reporting rates are the proportion of known cases having a corresponding case report from a provider, a faxed laboratory report or an electronic laboratory report. Reporting rates were stratified by disease and compared using McNemar's test. For key data fields on the reports, completeness was calculated as the proportion of non-blank fields. Timeliness was measured as the difference between date of laboratory confirmed diagnosis and the date the report was received by the health department. Differences in completeness and timeliness by data source were evaluated using a generalized linear model with Pearson's goodness of fit statistic.   RESULTS We assessed 13,269 reports representing 9034 unique cases. Reporting rates varied by disease with overall rates of 19.1% for providers and 84.4% for laboratories (p < 0.001). All but three of 15 data fields in provider reports were more often complete than those fields within laboratory reports (p <0.001). Laboratory reports, whether faxed or electronically sent, were received, on average, 2.2 days after diagnosis versus a week for provider reports (p <0.001).   CONCLUSIONS Despite growth in the use of electronic methods to enhance notifiable disease reporting, there still exists much room for improvement.	extraction;fax;generalized linear model;health information exchange;laboratory;state or local health department	Brian E. Dixon;Zuoyi Zhang;Patrick Lai;Uzay Kirbiyik;Jennifer Williams;Rebecca A. Hills;Debra Revere;P. Joseph Gibson;Shaun J. Grannis	2017		10.1186/s12911-017-0491-8	health department;disease notification;data mining;disease;public health;health information exchange;public health surveillance;health informatics;notifiable disease;medicine	AI	-59.920496760844244	-66.1535335470752	13191
87f7029ab9eaab486409395c8082236e076ac2c7	an improved fortran 77 recombinant dna database management system with graphic extensions in gks	fortran;database management system	We have improved an existing clone database management system written in FORTRAN 77 and adapted it to our software environment. Improvements are that the database can be interrogated for any type of information, not just keywords. Also, recombinant DNA constructions can be represented in a simplified 'shorthand', whereafter a program assembles the full nucleotide sequence from the contributing fragments, which may be obtained from nucleotide sequence databases. Another improvement is the replacement of the database manager by programs, running in batch to maintain the databank and verify its consistency automatically. Finally, graphic extensions are written in Graphical Kernel System, to draw linear and circular restriction maps of recombinants. Besides restriction sites, recombinant features can be presented from the feature lines of recombinant database entries, or from the feature tables of nucleotide databases. The clone database management system is fully integrated into the sequence analysis software package from the Pasteur Institute, Paris, and is made accessible through the same menu. As a result, recombinant DNA sequences can directly be analysed by the sequence analysis programs.	base sequence;clone;contribution;dna database;data base management;data table;database management systems;fortran;graphical kernel system;map;nucleotides;recombinant dna;sequence analysis	L. L. Van Rompuy;C. Lesage;M. E. Vanderhaegen;M. P. Telemans;M. F. Zabeau	1986	Computer applications in the biosciences : CABIOS	10.1093/bioinformatics/2.4.251	biology;computer architecture;computer science;database;programming language	DB	-4.54925904725884	-57.09761013863438	13198
9597b75455e8786534beec64303cb93dd1bebd7d	a deep linguistic analysis for cross-language information retrieval		Cross-language information retrieval consists in providing a query in one language and searching documents in one or different languages. These documents are ordered by the probability of being relevant to the user's request. The highest ranked document is considered to be the most likely relevant document. The LIC2M cross-language information retrieval system is a weighted Boolean search engine based on a deep linguistic analysis of the query and the documents. This system is composed of a linguistic analyzer, a statistic analyzer, a reformulator, a comparator and a search engine. The linguistic analysis processes both documents to be indexed and queries to extract concepts representing their content. This analysis includes a morphological analysis, a part-of-speech tagging and a syntactic analysis. In this paper, we present the deep linguistic analysis used in the LIC2M cross-lingual search engine and we will particularly focus on the impact of the syntactic analysis on the retrieval effectiveness.	boolean algebra;comparator;cross-language information retrieval;deep linguistic processing;parsing;part-of-speech tagging;text mining;web search engine	Nasredine Semmar;Meriama Laïb;Christian Fluhr	2006			boolean algebra;natural language processing;artificial intelligence;search engine;statistic;cross-language information retrieval;parsing;linguistics;computer science;deep linguistic processing;ranking	Web+IR	-29.429709498165778	-65.41424810946646	13199
b729fb3cfcff75a7384ac0887fd2bfaf673cb1cd	a trust-based approach for security policies enhancement in dynamic social networks	security policies;betweenness centrality;social networking;text mining;trust score;suspicious conversations;lexical features classification;privacy preservation;privacy protection;dynamic social networks;pagerank;sexual predators;influential users;bees algorithm;behavioural features classification	The sensitive information revealed in social networks is a real threat menacing users' privacy. Hence, trust is required to enhance the security policies in these networks. Motivated by the privacy problems in particular the danger of sexual predators and the disregard of the dynamic aspect of social networks, users popularity and activity which may affect the precision of the expected results of influential users detection, we aim to present a generic model to improve security policies. To do so, we use text mining techniques to distinguish suspicious conversations using lexical and behavioural features classification and to determine influential users having the maximum trust score value. This trust score is calculated using an algorithm called DynamicInflu inspired by the honey bee's foraging behaviour based on popularity and activity parameters. The experiments are performed on real data and the comparison of the DynamicInflu algorithm with commonly used approaches showed a good performance.	social network	Zeineb Dhouioui;Jalel Akaichi	2016	Int. J. Auton. Comp.	10.1504/IJAC.2016.10002939	text mining;computer science;security policy;bees algorithm;data mining;internet privacy;betweenness centrality;computer security;social network	AI	-20.173283308548	-55.4653155654308	13217
a5f9cfb10b7c024bfe8fe7c58ed873c3c618effc	a top-down method for program plagiarism detecting system	top down		sensor	Thi-Thu-Thao Phan;Hoang-Tru Cao	2004			information retrieval;plagiarism detection;computer science	ML	-32.13926179950614	-77.07289399092835	13226
564049721e97ee01c4f6b315bf4c98293f1b2fa6	segment-level display time as implicit feedback: a comparison to eye tracking	display time;search engine;pseudo relevance feedback;perforation;implicit feedback;personalization;web search engine;eye tracking;query expansion	We examine two basic sources for implicit relevance feedback on the segment level for search personalization: eye tracking and display time. A controlled study has been conducted where 32 participants had to view documents in front of an eye tracker, query a search engine, and give explicit relevance ratings for the results. We examined the performance of the basic implicit feedback methods with respect to improved ranking and compared their performance to a pseudo relevance feedback baseline on the segment level and the original ranking of a Web search engine.  Our results show that feedback based on display time on the segment level is much coarser than feedback from eye tracking. But surprisingly, for re-ranking and query expansion it did work as well as eye-tracking-based feedback. All behavior-based methods performed significantly better than our non-behavior-based baseline and especially improved poor initial rankings of the Web search engine.  The study shows that segment-level display time yields comparable results as eye-tracking-based feedback. Thus, it should be considered in future personalization systems as an inexpensive but precise method for implicit feedback.	baseline (configuration management);eye tracking;personalization;query expansion;relevance feedback;web search engine;world wide web	Georg Buscher;Ludger van Elst;Andreas Dengel	2009		10.1145/1571941.1571955	query expansion;simulation;web search engine;eye tracking;computer science;personalization;multimedia;world wide web;information retrieval;search engine	Web+IR	-34.13628244401162	-52.52891336968065	13249
d138284dcdcd8a5f870fb28ae488eb0d15534aac	effectiveness of ipms tool for handling chronic low back pain with sitting workplace employees		Effectiveness of IPMS tool for handling chronic low back pain with sitting workplace employees.	ability to sit question;handling (psychology);low back pain	Hille Maas	2015	Studies in health technology and informatics	10.3233/978-1-61499-566-1-907	low back pain;sedentary lifestyle;physical therapy;physical medicine and rehabilitation;sitting;medicine	HCI	-57.64175145714855	-55.5334210911611	13254
66f685de26e4e2c684069639ced73c0252a55149	social media analysis for product safety using text mining and sentiment analysis	product safety;naive bayes text mining sentiment analysis product safety social media machine learning;text mining;active surveillance systems negative sentiment brand monitoring enforcement agencies regulatory agencies product manufacturers sentiment orientation social media data drug sentiment prediction cosmetic product sentiment prediction machine learning classifier modeling training data product safety lexicon brand analysis twitter facebook comments sentiment analysis text mining cosmetic products drug product counterfeiting product allergies anticounterfeiting fight;naive bayes;text analysis cosmetics data mining drugs learning artificial intelligence natural language processing pattern classification social networking online;sentiment analysis drugs media text mining facebook twitter monitoring;machine learning;sentiment analysis;social media	The growing incidents of counterfeiting and associated economic and health consequences necessitate the development of active surveillance systems capable of producing timely and reliable information for all stake holders in the anti-counterfeiting fight. User generated content from social media platforms can provide early clues about product allergies, adverse events and product counterfeiting. This paper reports a work in progress with contributions including: the development of a framework for gathering and analyzing the views and experiences of users of drug and cosmetic products using machine learning, text mining and sentiment analysis; the application of the proposed framework on Facebook comments and data from Twitter for brand analysis, and the description of how to develop a product safety lexicon and training data for modeling a machine learning classifier for drug and cosmetic product sentiment prediction. The initial brand and product comparison results signify the usefulness of text mining and sentiment analysis on social media data while the use of machine learning classifier for predicting the sentiment orientation provides a useful tool for users, product manufacturers, regulatory and enforcement agencies to monitor brand or product sentiment trends in order to act in the event of sudden or significant rise in negative sentiment.	experience;lexicon;machine learning;sentiment analysis;social media;text mining;user-generated content	Haruna Isah;Paul R. Trundle;Daniel Neagu	2014	2014 14th UK Workshop on Computational Intelligence (UKCI)	10.1109/UKCI.2014.6930158	text mining;naive bayes classifier;social media;computer science;machine learning;data mining;world wide web;sentiment analysis	AI	-21.416480768022083	-56.006146787443555	13262
03dccaeadd93795118e2ed1af2a8e6ae0e3ba7fc	semantic object based retrieval from surveillance videos	video surveillance algorithm theory closed circuit television data acquisition image retrieval;semantic object based retrieval;video indexing algorithms;video surveillance;motion tracking developments;indexing semantic objects;surveillance;increased data acquisition;multimedia analysis;video indexing algorithms semantic object based retrieval surveillance videos closed circuit television monitoring improved personal safety improved personal security increased data acquisition multimedia analysis extracting novel architecture indexing semantic objects scale invariant feature transform features motion tracking developments;technological development;closed circuit television monitoring;event detection;data mining;motion tracking;surveillance videos;content based retrieval sift features sift matching;video indexing;closed circuit television;visualization;motion segmentation;improved personal security;indexing;algorithm theory;scale invariant feature transform;sift matching;feature extraction;indexation;sift features;performance analysis;extracting novel architecture;scale invariant feature transform features;data acquisition;content based retrieval;videos;semantic retrieval;image retrieval;improved personal safety;surveillance videos space technology indexing tv monitoring safety data security data acquisition event detection	In recent years, due to technological developments, the use of Closed-Circuit Television monitoring has been widely used not only in public areas but also in confined and/or private spaces for improved personal safety and security. The increased data acquisition has naturally resulted in the critical need for multimedia analysis for semantic object and event detection. Addressing this research problem, in this paper we present an novel architecture for extracting and indexing semantic objects with Scale Invariant Feature Transform features. The proposed approach exploits the developments of motion tracking and video indexing algorithms. The proposed framework is an ongoing development with the objective to enable the semantic retrieval of objects. The preliminary performance analysis of the proposed approach has been evaluated on a set of surveillance videos.	algorithm;closed-circuit television;data acquisition;object-based language	Virginia Fernandez Arguedas;Krishna Chandramouli;Ebroul Izquierdo	2009	2009 Fourth International Workshop on Semantic Media Adaptation and Personalization	10.1109/SMAP.2009.20	computer vision;semantic computing;computer science;multimedia;information retrieval	Vision	-12.546967513408326	-55.32255506260526	13263
cc70ef3d0889370afda16e428f674e78bbc05d15	test data likelihood for plsa models	busqueda informacion;tratamiento automatico;lenguaje natural;probability;algoritmo busqueda;plsa;information retrieval;algorithme recherche;search algorithm;langage naturel;latent variable model;probabilistic model;modele variable latente;modelo variable latente;automatic processing;recherche information;natural language;probabilidad;probabilite;expectation maximization algorithm;modele probabiliste;it evaluation;traitement automatique;empirical evaluation;latent class model;likelihood;probabilistic latent semantic analysis;test collection;plsa probabilistic latent semantic analysis;modelo probabilista;collection test	Probabilistic Latent Semantic Analysis (PLSA) is a statistical latent class model that has recently received considerable attention. In its usual formulation it cannot assign likelihoods to unseen documents. Furthermore, it assigns a probability of zero to unseen documents during training. We point out that one of the two existing alternative formulations of the Expectation-Maximization algorithms for PLSA does not require this assumption. However, even that formulation does not allow calculation ofthe actual likelihood values. We therefore derive a new test-data likelihood substitute for PLSA and compare it to three existing likelihood substitutes. An empirical evaluation shows that our new likelihood substitute produces the best predictions about accuracies in two different IR tasks and is therefore best suited to determine the number of EM steps when training PLSA models. The new likelihood measure and its evaluation also suggest that PLSA is not very sensitive to overfitting for the two tasks considered. This renders additions like tempered EM that especially address overfitting unnecessary.	expectation–maximization algorithm;latent class model;overfitting;probabilistic latent semantic analysis;rendering (computer graphics);test data	Thorsten Brants	2005	Information Retrieval	10.1007/s10791-005-5658-8	computer science;machine learning;pattern recognition;probabilistic latent semantic analysis;statistics	ML	-23.940910761774006	-79.52660903673895	13273
a27dbc11161fa60ade9b5c33573db962d399428b	when is it better to give up?: towards autonomous action selection for robot assisted asd therapy	cognitive robotics;action selection;social robotics;rat;asd	Robot Assisted Therapy (RAT) for children with ASD has found promising applications. In this paper, we outline an autonomous action selection mechanism to extend current RAT approaches. This will include the ability to revert control of the therapeutic intervention to the supervising therapist. We suggest that in order to maintain the goals of therapy, sometimes it is better if the robot gives up.	action selection;autonomous robot	Emmanuel Senft;Paul Baxter;James Kennedy;Tony Belpaeme	2015		10.1145/2701973.2702715	asd;action selection;computer science;artificial intelligence;social robot;cognitive robotics	AI	-44.09306937985	-53.16555466784438	13279
84e7fa1fd0071c6d0088b413f806757e49ea59c9	development of an asthma management system in a pediatric emergency department	medical order entry systems;asthma;therapy computer assisted;guideline adherence;practice guidelines as topic;diagnosis computer assisted;child;algorithms;emergency service hospital;humans;electronic health records	Asthma is the leading chronic childhood disease with exacerbations resulting in urgent and emergency care visits. Guidelines adherence improves patient care but is suboptimal. A computerized guideline system can help improve compliance through automatic initiation and reminders to increase adherence. We designed a computerized management system for asthma care in the pediatric emergency department (ED). The system will be evaluated in two phases. The first phase evaluates a computerized diagnostic prompt using the ED's existing asthma protocol. The second phase evaluates a computerized asthma management system including temporal reminder elements for scoring and medication orders. The system was developed in conjunction with the pediatric ED multidisciplinary care team. The computerized system is entirely automatic and a prospective evaluation of the diagnostic component is ongoing.		Judith W. Dexheimer;Donald H. Arnold;Thomas J. Abramo;Dominik Aronsky	2009	AMIA ... Annual Symposium proceedings. AMIA Symposium		intensive care medicine;medicine;emergency medicine;medical emergency	Embedded	-58.42179979534261	-65.10837682989988	13280
1001f9da0232cda5e2babf30fd728106b48f2584	towards interoperability of heterogeneous health databases: application to a tumor samples bank		OBJECTIVES to define principles and methods that allow heterogeneous database in the health sector to be interoperable.   MATERIAL AND METHOD to design a component-based middleware able to provide flexible and efficient means of communication between end-users and databases, and that exploits the standard nomenclatures of the health sector.   RESULTS according to these principles, to implement a prototype of a tumor sample bank in the University hospitals of Marseille, France.   DISCUSSION to discuss the benefits that the approach brings and the progress in prototyping.	academic medical centers;component-based software engineering;genetic heterogeneity;heterogeneous database system;interoperability;middleware;neoplasms;nomenclature;prototype;published database;benefit;tumor tissue	Michel Joubert;Jean-Charles Dufour;Laurent Falco;Sylvain Aymard;Marius Fieschi	2004	Studies in health technology and informatics	10.3233/978-1-60750-949-3-1251	data mining;interoperability;database;middleware;exploit;medicine	HPC	-54.27845609594415	-63.63718821638471	13307
eac40e9dbd1b3ca31eb94b7ac02b18e945b3e4be	comparison of rule-based and statistical methods for grapheme to phoneme modelling		Grapheme to phoneme modelling is one of the key features in automated speech recognition and speech synthesis. In this paper, the authors compare two different approaches: a statistical machine translation based method using the phonetically transcribed Latvian Speech Recognition Corpus and a rule-based method for phonetic transcription of words from grammatically correct forms. The paper provides 10-fold cross-validation results and error analysis for both methods.		Ilze Auzina;Marcis Pinnis;Roberts Dargis	2014		10.3233/978-1-61499-442-8-57	machine translation;linguistics;phonetic transcription;rule-based system;grapheme;speech synthesis;artificial intelligence;pattern recognition;latvian;computer science	Vision	-21.276851074665643	-82.02927469319903	13335
219373fc10851a8c2cc48f574875d3fc7502d877	a new virtual environment for testing and hardware implementation of closed-loop control algorithms in the artificial pancreas	protocols;computer simulation diabetes mellitus type 1 equipment design equipment failure analysis feedback physiological humans models biological pancreas artificial therapy computer assisted user computer interface;control algorithm;type 1 diabetes;biological system modeling;diabetes;closed loop control;blood glucose;graphical programming;insulin sugar diabetes algorithm design and analysis protocols hardware biological system modeling;graphic user interface;insulin;sugar;virtual environment;electronic control unit;simulation tool;algorithm design;hardware implementation;algorithm design and analysis;artificial pancreas;hardware	This article presents a new simulation tool for designing and testing blood glucose control algorithms in patients with type 1 diabetes. The control algorithms can be designed and implemented either with textual or graphical programming languages or by importing them from several frameworks. Realistic scenarios and protocols can be customized and built through graphical user interfaces, where several outcomes are available to evaluate control performance. Sophisticated models of the glucose-insulin system, as well as representative models of the instrumentation, have been included. Unlike existing systems, this simulation tool allows integrating the control algorithms into an electronic control unit, thus reusing the entire code in a straightforward way.	algorithm;control theory;customize;deploy;diabetes mellitus;diabetes mellitus, insulin-dependent;electronic control unit;engine control unit;experiment;glucose;graphical user interface;hematological disease;pancreas extract;pancreas, artificial;patients;programming languages;protocols documentation;simulation;software deployment;virtual reality;visual programming language	Fabian León-Vargas;Germán Prados;Jorge Bondia;Josep Vehí	2011	2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/IEMBS.2011.6090124	control engineering;algorithm design;simulation;computer science;computer engineering	Visualization	-8.554720058282822	-58.77066410141977	13374
9e0c68e74bac44efd924059c6fa59d8714436dc0	hybrid long- and short-term models of folk melodies		In this paper, we present the results of a study on dynamic models for predicting sequences of musical pitch in melodies. Such models predict a probability distribution over the possible values of the next pitch in a sequence, which is obtained by combining the prediction of two components (1) a long-term model (LTM) learned offline on a corpus of melodies, as well as (2) a short-term model (STM) which incorporates context-specific information available during prediction. Both the LTM and the STM learn regularities in pitch sequences solely from data. The models are combined in an ensemble, wherein they are weighted by the relative entropies of their respective predictions. Going by previous work that demonstrates the success of Connectionist LTMs, we employ the recently proposed Recurrent Temporal Discriminative Restricted Boltzmann Machine (RTDRBM) as the LTM here. While it is indeed possible for the same model to also serve as an STM, our experiments showed that n-gram models tended to learn faster than the RTDRBM in an online setting and that the hybrid of an RTDRBM LTM and an n-gram STM gives us the best predictive performance yet on a corpus of monophonic chorale and folk melodies.	connectionism;experiment;n-gram;online and offline;pitch (music);restricted boltzmann machine;software transactional memory	Srikanth Cherla;Son N. Tran;Tillman Weyde;Artur S. d'Avila Garcez	2015			melody;speech recognition;computer science	ML	-16.78463764512157	-75.38494695444403	13388
3820a2ee5bfee8eafd342d802a4d920a9d16e90f	ein expertensystem verbessert die qualität der medizinischen praxis		The knowledge base fort he practice of medicine is growing exponentially. To digest the available new information, a practitioner had to read almost all of his time and would not be able to treat patients. Since the economic burden is pressing doctors to see more patients in a shortage of time, the quality of the so-called evidence based medicine in practical medicine is declining. Additionally we see more complex patients with co morbidities due to the demographic shift with more older patients with a multiplicity of chronic diseases. However, the growing knowledgebase is giving the doctor a broad array of different factors for single decisions, which overwhelm the short-term memory of the human brain. To bridge the gap and to present the best available data to the point of decisionmaking, from our point of view there is a strong need for expert systems for decision support to guarantee a high level of medical quality. Most of medical IT solutions are developed out of the needs of hospital administrations or handling of simple digital data, as laboratory values. However, the patient should be in the centre of the process of executing high quality medicine. Therefore we see a lot of arguments for IT systems which covering medicine in the core of the IT processes. With CLEOS we have a solution available that could bridge the gap.	altran praxis;cryptographic hash function;decision support system;digital data;display resolution;expert system;high-level programming language;knowledge base	Mark Dominik Alscher	2009			praxis;psychoanalysis;medicine	DB	-56.05220779893585	-64.62416688881301	13401
80188854b633cef69072e215ff15aec23f74a1aa	from an acoustic tube to speech production	modelizacion;tube acoustique;acoustic tube;speech processing;tratamiento palabra;traitement parole;vocal tract;product model;modelisation;production parole;modeling;speech production	The general aim of this paper is to find out if speech production characteristics may be explained by the physical and permanent properties of an acoustic tube that is 18 cm long. In a way, we intend to generalize the deductive approach used by Lindblom [Phonetic Universal in Vowel Systems, in: J.J. Ohala, J.J. Jaeger (Eds.), Experimental Phonology, Academic Press, Orlando, p. 13] to explain vowel systems from production and perception properties. However, the question here is not to account for vowel systems but rather to explain certain characteristics of the speech production system. In the present research, these characteristics are not observed per se and used as unquestionable ‘‘constraints’’. In order to answer this question related to the characteristics of the speech production system, the properties of the acoustic tube are first studied to build an acoustic production model having the following specific property: the shape of the tube is deformed to perform maximum acoustic changes, i.e., a minimum area deformation provokes a maximum acoustic variation. Following this approach, a set of distinctive deformation gestures involving corresponding distinctive acoustic changes is obtained and used to set up the intrinsic ‘‘phonological’’ system of the tube designed for communication needs. Then, it is shown that the Distinctive Region Model (DRM) summarizes the main results obtained without any constraints. Finally, model and speech production characteristics are compared. With a limited number of constraints, which can be explained, included into the model (such as a fixed larynx cavity), they fit surprisingly well. Thus, can these speech production characteristics be explained by the proposed deductive approach, i.e., are the main characteristics of the vocal tract, and of speech production in general, consequences of specific deformations of the shape of the tube to perform maximum acoustic changes? Implications of the findings are discussed. 2003 Elsevier B.V. All rights reserved.	acoustic cryptanalysis;cognition;complex system;experiment;optimizing compiler;perturbation theory;production system (computer science);robot;tract (literature);williams tube	René Carré	2004	Speech Communication	10.1016/j.specom.2003.12.001	vocal tract;speech production;speech recognition;systems modeling;computer science;speech processing;linguistics	Web+IR	-9.419572639945475	-83.18746646871223	13408
4f613156c45780ec94df3f3b86dc87f1889cf984	speaker adaptive training of deep neural network acoustic models using i-vectors	ieee transactions;deep neural networks dnns;acoustics;acoustic modeling;speech processing;vectors acoustic signal processing gaussian processes learning artificial intelligence mixture models neural nets signal representation speaker recognition;training;speech;speaker adaptive training sat acoustic modeling deep neural networks dnns;testing;negligible wer loss speaker adaptive training feature space sat deep neural network acoustic models i vectors gaussian mixture models gmm speaker representations speaker normalized features feature types network structures sat dnn models large scale acoustic modeling tasks model space dnn adaptation frame skipping training data size reduction word error rates;speaker adaptive training sat;adaptation models training speech acoustics testing ieee transactions speech processing;adaptation models	In acoustic modeling, speaker adaptive training (SAT) has been a long-standing technique for the traditional Gaussian mixture models (GMMs). Acoustic models trained with SAT become independent of training speakers and generalize better to unseen testing speakers. This paper ports the idea of SAT to deep neural networks (DNNs), and proposes a framework to perform feature-space SAT for DNNs. Using i-vectors as speaker representations, our framework learns an adaptation neural network to derive speaker-normalized features. Speaker adaptive models are obtained by fine-tuning DNNs in such a feature space. This framework can be applied to various feature types and network structures, posing a very general SAT solution. In this paper, we fully investigate how to build SAT-DNN models effectively and efficiently. First, we study the optimal configurations of SAT-DNNs for large-scale acoustic modeling tasks. Then, after presenting detailed comparisons between SAT-DNNs and the existing DNN adaptation methods, we propose to combine SAT-DNNs and model-space DNN adaptation during decoding. Finally, to accelerate learning of SAT-DNNs, a simple yet effective strategy, frame skipping, is employed to reduce the size of training data. Our experiments show that compared with a strong DNN baseline, the SAT-DNN model achieves 13.5% and 17.5% relative improvement on word error rates (WERs), without and with model-space adaptation applied respectively. Data reduction based on frame skipping results in 2× speed-up for SAT-DNN training, while causing negligible WER loss on the testing data.	acoustic cryptanalysis;acoustic model;artificial neural network;baseline (configuration management);boolean satisfiability problem;deep learning;experiment;feature model;feature vector;mixture model;word error rate	Yajie Miao;Hao Zhang;Florian Metze	2015	IEEE/ACM Transactions on Audio, Speech, and Language Processing	10.1109/TASLP.2015.2457612	speaker recognition;speech recognition;acoustics;computer science;speech;machine learning;pattern recognition;speech processing;linguistics;software testing	AI	-17.396630401269345	-90.01132260833961	13432
ac2e79cd00f1639afee75769205366c033fd3718	image annotation using search and mining technologies	hash indexing;image database;image annotation;data mining;large scale;indexation;search result clustering	In this paper, we present a novel solution to the image annotation problem which annotates images using search and data mining technologies. An accurate keyword is required to initialize this process, and then leveraging a large-scale image database, it 1) searches for semantically and visually similar images, 2) and mines annotations from them. A notable advantage of this approach is that it enables unlimited vocabulary, while it is not possible for all existing approaches. Experimental results on real web images show the effectiveness and efficiency of the proposed algorithm.	algorithm;automatic image annotation;data mining;vocabulary	Xin-Jing Wang;Lei Zhang;Feng Jing;Wei-Ying Ma	2006		10.1145/1135777.1136007	image retrieval;computer science;data mining;database;automatic image annotation;world wide web;information retrieval	Vision	-16.846277299177768	-58.054451131361844	13455
10ab3d1aa3d3c0750fd466c29438ced35a9d2118	word level lyrics-audio synchronization using separated vocals		The massive amount of digital music data available necessitates automated methods for processing, classifying and organizing large volumes of songs. As music discovery and interactive music applications become commonplace, the ability to synchronize lyric text information with an audio recording has gained interest. This paper presents an approach for lyric-audio alignment by comparing synthesized speech with a vocal track removed from an instrument mixture using source separation. We take a hierarchical approach to solve the problem, assuming a set of paragraph-music segment pairs is given and focus on within-segment lyric alignment at the word level. A synthesized speech signal is generated to reflect the properties of the music signal by controlling the speech rate and gender. Dynamic time warping finds the shortest path between the synthesized speech and separated vocal. The resulting path is used to calculate the timestamps of words in the original signal. The system results in approximately half a second of misalignment error on average. Finally, we discuss the challenges and suggest improvements to the method.	dynamic music;dynamic time warping;organizing (structure);shortest path problem;source separation;speech synthesis	Sang Won Lee;Jeffrey Scott	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7952235	voice activity detection;artificial intelligence;sound recording and reproduction;digital audio;dynamic time warping;pattern recognition;lyrics;computer science;source separation;speech processing;speech recognition;linear predictive coding	Robotics	-10.44558783759272	-90.88533595814954	13493
3a6e91c02fe4c899a1404165f5ea3e75ac76fee6	on the marriage of information retrieval and information extraction	information extraction;information retrieval	The techniques of information retrieval and information extraction are complementary, but to date there has been little work aimed at integrating the two. We describe how each of these techniques contributes to the process of transferring information from generator to user, summarise the issues which must be addressed if they are to work together, and report the results of some preliminary experiments on coupling them.	experiment;information extraction;information retrieval	Alexander M. Robertson;Robert J. Gaizauskas	1997			relevance;cognitive models of information retrieval;computer science;information filtering system;data mining;database;information quality;information extraction;information retrieval;human–computer information retrieval	AI	-31.286750799623846	-60.359673861954704	13508
d7f84654f414bb25b21ab3901871380ee04e34b2	deep context modeling for web query entity disambiguation		In this paper, we presented a new study for Web query entity disambiguation (QED), which is the task of disambiguating different candidate entities in a knowledge base given their mentions in a query. QED is particularly challenging because queries are often too short to provide rich contextual information that is required by traditional entity disambiguation methods. In this paper, we propose several methods to tackle the problem of QED. First, we explore the use of deep neural network (DNN) for capturing the character level textual information in queries. Our DNN approach maps queries and their candidate reference entities to feature vectors in a latent semantic space where the distance between a query and its correct reference entity is minimized. Second, we utilize the Web search result information of queries to help generate large amounts of weakly supervised training data for the DNN model. Third, we propose a two-stage training method to combine large-scale weakly supervised data with a small amount of human labeled data, which can significantly boost the performance of a DNN model. The effectiveness of our approach is demonstrated in the experiments using large-scale real-world datasets.	artificial neural network;baseline (configuration management);deep learning;entity bean;experiment;knowledge base;map;teaching method;web search engine;word-sense disambiguation;world wide web	Zhen Liao;Xinying Song;Yelong Shen;Saekoo Lee;Jianfeng Gao;Ciya Liao	2017		10.1145/3132847.3132856	query expansion;web query classification;data mining;information retrieval;web search query;labeled data;computer science;knowledge base;artificial neural network;feature vector;context model	Web+IR	-18.171983750294046	-67.1310614530942	13528
6d6579e66d927be8379f3c2280fa6a0ba9d925f9	robust generation of symbolic prosody by a neural classifier based on autoassociators	english language;probability;history;neural networks;speech synthesis;symbolic prosody;neural nets;training;natural languages;testing;neural classifier;german language robust generation symbolic prosody neural classifier autoassociators speech synthesis two stage approach error information training class conditional probabilities english language;autoassociators;robust generation;two stage approach;class conditional probabilities;hidden markov models;classification algorithms;learning artificial intelligence speech synthesis probability neural nets;robust method;robustness;learning artificial intelligence;conditional probability;robustness hidden markov models speech synthesis natural languages neural networks history testing classification algorithms costs regression tree analysis;german language;error information;regression tree analysis	In this paper a highly robust method to predict symbolic prosody labels for speech synthesis is proposed. This method is based on a two stage approach. In the fist stage the characteristics of each symbolic prosody label are captured by autoassociative models, which are trained independently. In the second stage detailed error information obtained from the different autoassociative models is used to train a neuraliclassifier yielding class conditional probabilities. The method. has been successfully applied for German and English language. For the latter the exact same data bases as in (21 and [9] were used to test the method. The results obtained are superior to those reported for the HMMbased and CART-based approaches [2] [9]. Further experiments also demonstrate considerable generalizing ability, yielding high robustness in sparse training material conditions.	database;experiment;rom cartridge;robustness (computer science);semantic prosody;sparse matrix;speech synthesis	Achim F. Müller;Hans-Georg Zimmermann;Ralph Neuneier	2000		10.1109/ICASSP.2000.861812	natural language processing;speech recognition;conditional probability;german;computer science;english;machine learning;probability;software testing;natural language;artificial neural network;statistics;robustness	NLP	-19.924203695887662	-89.50961462072098	13550
7f5e3a78ff96905ad68456bb047f42a3d6ca6555	towards named entity annotation of latvian national library corpus				Peteris Paikens;Ilze Auzina;Ginta Garkaje;Madara Paegle	2012		10.3233/978-1-61499-133-5-169	named entity;information retrieval;natural language processing;computer science;latvian;annotation;artificial intelligence	NLP	-31.116496344369118	-76.55092361919479	13591
7d0e6dc820dac8c88f7c9a9fa9884563f4180348	query initialization by virtual query image and pre-initial query feedback	relevance feedback content based retrieval image retrieval;binary decision;virtual query image content based image retrieval query initialization relevance feedback binary decision;query initialization;feedback image retrieval information retrieval content based retrieval indexing image databases visual databases multimedia databases image segmentation spatial databases;page zero problem;virtual query image;content based image retrieval;relevance feedback;page zero problem image retrieval relevance feedback query initialization;content based retrieval;image retrieval	"""One of the main problems in content-based image retrieval is the query initialization and so-called """"page-zero problem"""". The early solutions range from the use of similar images to definition of a user sketch (using color, texture, or shape information). The latter implies user's choice of a starting point, which can be achieved in different ways. In this paper a new approach to this problem is proposed based on relevance feedback. The objective is to provide a very simple interface, where at each iteration the user should make a binary decision based on two presented images. Experimental results show that the proposed approach performs very well compared to existing methods, while providing a very simple and intuitive interaction"""	content-based image retrieval;iteration;radio frequency;relevance feedback;texture mapping	Anelia Grigorova;Francesco G. B. De Natale	2005	2005 IEEE 7th Workshop on Multimedia Signal Processing	10.1109/MMSP.2005.248657	sargable;computer vision;query optimization;query expansion;visual word;ranking;image retrieval;computer science;concept search;database;web search query;automatic image annotation;binary decision diagram;information retrieval;query language;human–computer information retrieval	Vision	-12.211491626987396	-58.116551768468746	13621
fab5828ac72c3cf10b55b5819296a65baa4aca32	multi-cast attention networks for retrieval-based question answering and response prediction		Attention is typically used to select informative sub-phrases that are used for prediction. This paper investigates the novel use of attention as a form of feature augmentation, i.e, casted attention. We propose Multi-Cast Attention Networks (MCAN), a new attention mechanism and general model architecture for a potpourri of ranking tasks in the conversational modeling and question answering domains. Our approach performs a series of soft attention operations, each time casting a scalar feature upon the inner word embeddings. The key idea is to provide a real-valued hint (feature) to a subsequent encoder layer and is targeted at improving the representation learning process. There are several advantages to this design, e.g., it allows an arbitrary number of attention mechanisms to be casted, allowing for multiple attention types (e.g., co-attention, intra-attention) and attention variants (e.g., alignment-pooling, max-pooling, mean-pooling) to be executed simultaneously. This not only eliminates the costly need to tune the nature of the coattention layer, but also provides greater extents of explainability to practitioners. Via extensive experiments on four well-known benchmark datasets, we show that MCAN achieves state-of-the-art performance. On the Ubuntu Dialogue Corpus, MCAN outperforms existing state-of-the-art models by 9%. MCAN also achieves the best performing score to date on the well-studied TrecQA dataset.	benchmark (computing);convolutional neural network;data structure alignment;encoder;experiment;feature learning;information;machine learning;question answering;ubuntu;whole earth 'lectronic link	Yi Tay;Luu Anh Tuan;Siu Cheung Hui	2018	CoRR		architecture;machine learning;encoder;artificial intelligence;question answering;computer science;ranking;feature learning	ML	-17.11412258762271	-73.77597177018954	13635
b3c2e8ac2872ee95c3e27f31f3466de7fb69bb70	off-line handwritten signature gpds-960 corpus	current need;automatic biometric recognition system;large databases;off-line handwritten signature gpds-960;preliminary verification result;acquisition protocol;gpds-960 corpus;off-line handwritten signature database;genuine signature;gpds signature corpus;gpds data;digital signatures;database management systems	The current need for large databases to evaluate automatic biometric recognition systems has motivated the developing of the GPDS-960 corpus, an off-line handwritten signature database which contains 24 genuine signatures and 30 forgeries of 960 individuals. This paper describes the GPDS signature corpus, gives details about the acquisition protocols and presents preliminary verification results obtained using the GPDS data.	biometrics;database;online and offline;signature	Jesus Francisco Vargas Bonilla;Miguel Angel Ferrer-Ballester;Carlos Manuel Travieso-González;Jesús B. Alonso	2007	Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)	10.1109/ICDAR.2007.190	digital signature;speech recognition;computer science;data mining;database;signature recognition	Vision	-16.120548604521687	-96.93080563122541	13649
caf953f5987cc4692e7d4fffb33b9adbb29d9ab3	compilação de corpos comparáveis especializados: devemos sempre confiar nas ferramentas de compilação semi-automáticas?	comparable corpora;distributional similarity measures manual and semi automatic compilation natural language processing;computacion informatica;filologias;linguistica;ciencias basicas y experimentales;computational linguistics	Decisions at the outset of compiling a comparable corpus are of crucial importance for how the corpus is to be built and analysed later on. Several variables and external criteria are usually followed when building a corpus but little is been said about textual distributional similarity in this context and the quality that it brings to research. In an attempt to fulfil this gap, this paper aims at presenting a simple but e cient methodology capable of measuring a corpus internal degree of relatedness. To do so, this methodology takes advantage of both available natural language processing technology and statistical methods in a successful attempt to access the relatedness degree between documents. Our findings prove that using a list of common entities and a set of distributional similarity measures is enough not only to describe and assess the degree of relatedness between the documents in a comparable corpus, but also to rank them according to their degree of relatedness within the corpus.	compiler;entity;natural language processing;semiconductor industry;similarity learning;text corpus	Hernani Costa;Isabel Dúran Muñoz;Gloria Corpas Pastor;Ruslan Mitkov	2016	Linguamática		art;art history;cartography	NLP	-29.163298938117894	-73.65719365401459	13663
f34996d5f24fe83ba4e802976d079f0ad84c6b75	discourse prosody planning in native (l1) and nonnative (l2) (l1-bengali) english: a comparative study		This paper conducts a comparative study between L1 and L2 (L1 Bengali) English discourse level speech planning to investigate differences between L1 and L2 English speaker groups in the organization of discourse-level speech planning. For this purpose, English speech of 10 L1 English and 40 L1 Bengali speakers of the same discourse are analyzed in terms of using prosodic and acoustic cues by applying hierarchical discourse prosody framework. From this analysis, between-group differences in discourse level speech planning are found through the speech rate, locations of discourse boundary breaks as well as size and scope of speech planning and chunking units. Result of analysis shows that the speech rate of L1 English speakers is higher than that of L2 English speakers, L2 English speakers contain more break boundary than that of the L1 English speakers at every discourse level in the organization, which exhibit the fact that L2 English speakers use more intermediate chunking units and larger scale planning units than that of L1 English speakers. Between-group differences are also found through the analysis of phrase component at prosodic phrase level and accent component at the prosodic word level. These findings can be attributed to L2 English speakers’ improper phrasing, improper word level prominence and the ambiguous difference between content words and function words. The study concludes that the deficiencies in English strategy for L1 Bengali speakers’ discourse-level speech planning compared to L1 English speakers are due to the influence of L1 (Bengali) prosody at the L2 discourse level.	domain of discourse;semantic prosody	Shambhu Nath Saha;Shyamal Kr. Das Mandal	2017	I. J. Speech Technology	10.1007/s10772-017-9409-1	natural language processing;speech recognition;linguistics	NLP	-11.5708173488004	-81.14059951137023	13671
bdbb8cf11926bb843823c4ef306aa5af76a15926	differences between social media and regulatory databases in adverse drug reaction discovery	regulatory databases;pharmacovigilance;drug adverse event discovery;medical forum	Information extraction from social media for a variety of applications, such as collecting people opinion about a product or a political party, has been widely studied and justified. Extracting information for health related applications however is less justified especially because of sensitivity of health issues, difficulty in establishing the value and trust in lay people to judge their health problems. Using social media to discover adverse drug reactions is one of the most controversial topics. It is difficult to establish the causality between an adverse drug reaction and a drug when the context information such as patient condition is missing. We compare official reports of adverse drug reactions with reports on medical forums related to two different drugs to discuss the potential and challenges in this research area.	causality;database;information extraction;social media	Chen Wang;Sarvnaz Karimi	2014		10.1145/2632188.2632201	pharmacology;public relations;medicine;data mining;pharmacovigilance	AI	-21.60607709677739	-55.50623733046493	13674
ab531b367a353b74f62c5a51711e470c0eadaaad	visual information and the perception of prosody		This paper reports the results of a phonetic experiment carried out to explore the potential importance of visual information for the perception of Japanese prosody and its relationship with phonological structure. It specifically examines whether and to what extent native speakers of Japanese are able to accurately perceive the temporal structure of their native language on the sole basis of visual information. It has been found that the word-final contrast between short and long vowels is totally invisible to the native speakers although other types of t emporal differences characteristic of mora-timed languages are readily visible as a crucial distinction in the mora-timed language. Interestingly, essentially the same confusion occurs in Japanese phonology, where word-final long vowels tend to be shortened and neutralized with their short counterparts.	japanese input methods;semantic prosody	Haruo Kubozono;Shosuke Haraguchi	2000			speech recognition;prosody;perception;computer science	NLP	-10.116563493042518	-81.20280743179842	13676
0190d5a707cece7aa18748ddc6f813b350420121	name disambiguation using semantic association clustering	databases testing grid computing computer science robustness information retrieval couplings clustering algorithms frequency bibliographies;clusting semantic association name disambiguation;clusting;pattern clustering;document handling;heterogeneous literature database;sand;name disambiguation;website sand semantic association based name disambiguation method semantic association clustering e document heterogeneous literature database name specification information retrieval dblp libra citesseer;information retrieval;name specification;probability density function;web sites document handling information retrieval pattern clustering;dblp;data mining;website;citesseer;accuracy;e document;web sites;merging;mathematical model;clustering algorithms;libra;semantic association based name disambiguation method;named entity;semantic association clustering;semantic association	Due to homonyms, abbreviations, etc., name ambiguity is widely available in web and e-document. For example, when integrating heterogeneous literature databases, because there are different name specifications, different authors may be thought of as the same author, and vice versa. Therefore, name ambiguity makes data robust even dirty and lowers the precision of information retrieval. In this paper, we present an approach, named as Semantic Association based Name Disambiguation method (SAND), to solve person name ambiguity. The basic idea of SAND is to explore the semantic association of name entities and cluster name entities according to their associations. Finally, the name entities in the same group are considered as the same entities. We test SAND using data from CitesSeer, DBLP and Libra. The test results show that SAND is an effective approach to solve the problem of name ambiguity.	database;entity;information retrieval;microsoft academic search;word-sense disambiguation	Hai Jin;Li Huang;Pingpeng Yuan	2009	2009 IEEE International Conference on e-Business Engineering	10.1109/ICEBE.2009.16	probability density function;fully qualified name;computer science;mathematical model;data mining;database;accuracy and precision;cluster analysis;world wide web;information retrieval;sand;statistics	SE	-27.040423403879323	-58.51408177555009	13687
66700f1f35e1b5df25210722dabf812db085b46d	weighted word2vec based on the distance of words		Word2vec is a novel technique for the study and application of natural language processing(NLP). It trains a word embedding neural network model with a large training corpus. After the model is trained, each word is represented by a vector in the specified vector space. The vectors obtained possess many interesting and useful characteristics that are implicitly embedded with the original words. The idea of word2vec is that there are relations between the words if they appear in the neighborhood. These relations are employed by considering various context windows in training the network model. However, word2vec doesn't consider the influence of distance between the words. It only considers whether or not the words appear in the same context window. We consider that word distances in the context bear certain semantic sense which can be exploited to better train the network model. To formalize the influence of different distances in the context, the fuzzy concept is adopted. Various experiments show that our proposed improvement can result in better language models than Word2Vec.	artificial neural network;computation;embedded system;experiment;fuzzy concept;language model;microsoft windows;natural language;network model;newton's method;text corpus;time complexity;window function;word embedding;word2vec	Chia-Yang Chang;Shie-Jue Lee;Chih-Chin Lai	2017	2017 International Conference on Machine Learning and Cybernetics (ICMLC)	10.1109/ICMLC.2017.8108974	fuzzy concept;word embedding;inverse distance weighting;word2vec;machine learning;artificial intelligence;computer science;pattern recognition;artificial neural network;network model;language model;vocabulary	NLP	-18.10827441959755	-72.29580118027175	13689
e6e825773d4ea15451c16ddc9e4a694c3c41a5f5	the effects of exclusive user choice of decision aid features on decision making	decision aid	Decision Support Systems (DSS) frequently have multiple decision aid (DA) features, causing users to engage in exclusive choice behavior; i.e., choice between alternative DA features that results in one feature being used to the exclusion of all others. We hypothesize that: (1) users choose the least effective (least accurate) DA feature in certain predictability environments; (2) users choose the DA feature that they believe they are most competent with; and (3) choice between DA features improves performance compared to those assigned the same DA feature. We test these hypotheses in an experiment in which 164 participants act as loan officers who chose between two decision aids (a database aid and a regression aid). The results support our hypotheses. Users employed a choice heuristic that caused them to choose the least effective DA feature for the task more than or as often as the most effective DA feature. Results also indicate a positive relationship between perceived competence and DA feature choic...		Patrick R. Wheeler;Donald R. Jones	2003	J. Information Systems	10.2308/jis.2003.17.1.63	computer science;data mining;communication;social psychology	ECom	-50.321835267192476	-56.62712506288956	13702
e6c251329d84e68fa171d6c97046159aa62f43c5	feature combination approaches for discriminative language models		This paper focuses on feature combination approaches for discriminative language models (DLMs). DLM is a feature-based log-linear language modeling approach where the feature parameters are estimated discriminatively. DLM allows for easy integration of various knowledge sources into language modeling. Choosing the proper strategy when combining features coming from different information sources is important. We investigated three approaches for combining lexical, word class, and acoustic features in DLMs. The three approaches are joint parameter estimation, cascade training, and model score combination. The cascade approach is an interesting approach that finally gave the best test set performance, improving the word error rate by 0.49% absolute (3% relative) on transcription of English Broadcast News. The word class features and state duration features were found to be very complementary, and their combination provided most of the improvement.	acoustic cryptanalysis;discriminative model;estimation theory;language model;linear grammar;log-linear model;test set;transcription (software);word error rate	Ebru Arisoy;Bhuvana Ramabhadran;Hong-Kwang Jeff Kuo	2011			discriminative model;pattern recognition;artificial intelligence;computer science;language model	NLP	-19.466495194115904	-88.65344759177658	13705
5625e1c7ac5bd1fd6db17fa1b236326ef89fed40	learning to explain entity relationships in knowledge graphs		We study the problem of explaining relationships between pairs of knowledge graph entities with human-readable descriptions. Our method extracts and enriches sentences that refer to an entity pair from a corpus and ranks the sentences according to how well they describe the relationship between the entities. We model this task as a learning to rank problem for sentences and employ a rich set of features. When evaluated on a large set of manually annotated sentences, we find that our method significantly improves over state-of-the-art baseline models.	baseline (configuration management);entity;human-readable medium;knowledge graph;learning to rank;text corpus;web page;wikipedia	Nikos Voskarides;Edgar Meij;Manos Tsagkias;Maarten de Rijke;Wouter Weerkamp	2015			natural language processing;knowledge management;machine learning	NLP	-27.500414942566515	-65.12963566595269	13727
255a56a7d5a74e329270a39d1ad0488bfe584d88	a modular mobile exergaming system with an adaptive behavior	biomedical monitoring;human smartphone interaction;exergames;pediatrics;heart monitor modular mobile exergaming system adaptive behavior obesity epidemic physical health mental health obese children exergames physical activity playing mobile based exergaming system running exercises jumping exercises modular structure foot interface;smart phones;games heart rate obesity pediatrics monitoring biomedical monitoring smart phones;human smartphone interaction exergames obesity tangible user interface;tangible user interface;heart rate;monitoring;games;obesity;user interfaces computer games gait analysis medical computing medical disorders mobile computing paediatrics patient care smart phones;conference proceeding	Obesity rates in the world, especially in the developed countries are alarming. This has forced scientists to consider obesity as an epidemic due to its huge negative consequences on the societies' physical and mental health. Obese Children constitute a large portion of those affected by this epidemic and researchers are striving to find solutions which can curb its spread. Exergames have emerged as promising tools that can help in the fight against obesity because it promotes physical activity through playing. In this paper, we present a mobile-based exergaming system that targets children of different ages and that aims to encourage them to do running and jumping exercises in an enjoyable manner. It currently incorporates 1 game but its modular structure enables to easily accept even more games. The system uses a novel foot interface and a heart monitor that allow interacting with a special game that can adapt to the user's performance. The preliminary evaluations with two children have shown that the system can be an effective tool that engages users into physical activity.	adaptive behavior;interaction;the fight: lights out	Ali Karime;Basim Hafidh;Wail Gueaieb;Abdulmotaleb El-Saddik	2015	2015 IEEE International Symposium on Medical Measurements and Applications (MeMeA) Proceedings	10.1109/MeMeA.2015.7145261	simulation;human–computer interaction;engineering;multimedia	HCI	-58.33388656209021	-55.236543102817144	13751
3f38bada02a7d87d2b385ebdc745337f5134bdf1	sources of performance in crf transfer training: a business name-tagging case study		This paper explores methods for increasing performance of CRF models, with a particular concern for transfer learning. We consider in particular the transfer case from political news to hard-to-tag business news, and show the effectiveness of several methods, including a novel semi-supervised approach.	conditional random field;semi-supervised learning;semiconductor industry	Marc B. Vilain;Jonathan Huggins;Ben Wellner	2009			multimedia	NLP	-20.89680080189968	-68.402344596017	13761
8fdcd4bfd0ae67b3b0ad1f65cb6d2ecbd24ce227	ternary twitter sentiment classification with distant supervision and sentiment-specific word embeddings		The paper proposes the Ternary Sentiment Embedding Model, a new model for creating sentiment embeddings based on the Hybrid Ranking Model of Tang et al. (2016), but trained on ternary-labeled data instead of binary-labeled, utilizing sentiment embeddings from datasets made with different distant supervision methods. The model is used as part of a complete Twitter Sentiment Analysis system and empirically compared to existing systems, showing that it outperforms Hybrid Ranking and that the quality of the distantsupervised dataset has a great impact on the quality of the produced sentiment embeddings.	binary classification;clutter;emoticon;experiment;gilbert cell;lexicon;library (computing);open-source software;prototype;semeval;sentiment analysis;text corpus;word embedding;eric	Mats Byrkjeland;Frederik Gørvell de Lichtenberg;Björn Gambäck	2018			machine learning;natural language processing;artificial intelligence;ternary operation;computer science	NLP	-20.969816647809736	-69.81456493388743	13777
ebd3e89db26092b0fff2324ad6a20ae26f0b1d4a	design of a mobile, safety-critical in-patient glucose management system		Diabetes mellitus is one of the most widespread diseases in the world. People with diabetes usually have long stays in hospitals and need specific treatment. In order to support in-patient care, we designed a prototypical mobile in-patient glucose management system with decision support for insulin dosing. In this paper we discuss the engineering process and the lessons learned from the iterative design and development phases of the prototype. We followed a user-centered development process, including real-life usability testing from the outset. Paper mock-ups in particular proved to be very valuable in gaining insight into the workflows and processes, with the result that user interfaces could be designed exactly to the specific needs of the hospital personnel in their daily routine.		Bernhard Höll;Stephan Spat;Johannes Plank;Lukas Schaupp;Katharina Neubauer;Peter Beck;Franco Chiarugi;Vasilis Kontogiannis;Thomas R. Pieber;Andreas Holzinger	2011	Studies in health technology and informatics	10.3233/978-1-60750-806-9-950	knowledge management;management system;medicine	HCI	-59.05098758535403	-59.58127052436747	13789
36f268a97e95ad42e5670f62f7ec710eaa08255a	modelling derivational morphology: a case of prefix stacking in russian		In order to automatically analyse Russian texts, one needs to model complex verb formation, as it is a productive mechanism and dictionary data is not sufficient. In this paper I discuss two implementations that aim to produce all and only the existing complex verbs built from the available morpheme inventory for the same fragment of Russian grammar. The first implementation is based on the syntactic theory approach to prefix combinatorics by Tatevosov (2009) and the other one uses the combination of basic syntactic restrictions and frame semantics to construct all possible combinations. I show that a combination of basic syntactic and semantic restrictions provides better results than a set of elaborated syntactic restrictions, especially for the complex verbs that are not normally tested by introspection.	mathematical morphology;stacking	Yulia Zinova	2017		10.1007/978-3-662-56343-4_8	morphology (linguistics);implementation;natural language processing;syntax;mathematics;morpheme;frame semantics;russian grammar;prefix;artificial intelligence;verb	Vision	-28.402683725172743	-77.6097210791035	13790
8e535c6154882ef881013cd923e1814e0c940a00	systematic comparison of question target classification taxonomies towards question answering		Question target classification is one of the essential research topics in question answering. Accurate identification and classification of question targets can help understand questions for retrieving relevant passages and assist answer extraction and ranking for improving answer retrieval accuracy and user satisfaction with return answers. This paper presents a systematic analysis on question target classification taxonomy. We investigate existing definitions of the classification and propose a concise definition. We then compare the existing classification taxonomies. The relevancies of the taxonomies are analyzed, inspiring us to propose a new taxonomy classification strategy. We finally summarize the characteristics and tendency of the current research of question target classification taxonomy. The systematic comparison is expected to provide consistent and meaningful guidance in the research of question target understanding in question answering.		Tianyong Hao;Wenxiu Xie;Chun Chen;Yuming Shen	2015		10.1007/978-981-10-0080-5_12	data mining;question answering;biological classification;computer science;ranking	NLP	-34.46937533929414	-55.7838069052233	13803
1f364e66ffd0c2d49ba6e18f34f63b3e8cfeb46a	a generative perspecl: ive on verbs and their readings		"""We sketch the architecture of a sentence generation module that maps a language-neutral """"deep"""" representation to a language-specific sentence-semantic specification, which is given to a front-end generator. Lexicalizat, ion is tlm main instrument tbr the mapl~ing step, and we examine the role of verb semantics in the process. In particular, we propose a set of rules that derive a range of verb alternations from a single base form, which is one source of lexical paraphrasing in the system."""	generative grammar;lexicon;moose;map;rewriting	Manfred Stede	1996			generative grammar;linguistics;computer science	AI	-29.20428949459567	-78.9819173284744	13810
0ec724a5a8837e36522f1aba890ddebc347180eb	deceiving google’s cloud video intelligence api built for summarizing videos		"""Despite the rapid progress of the techniques for image classification, video annotation has remained a challenging task. Automated video annotation would be a breakthrough technology, enabling users to search within the videos. Recently, Google introduced the Cloud Video Intelligence API for video analysis. As per the website, the system can be used to """"separate signal from noise, by retrieving relevant information at the video, shot or per frame"""" level. A demonstration website has been also launched, which allows anyone to select a video for annotation. The API then detects the video labels (objects within the video) as well as shot labels (description of the video events over time).,,,,,,In this paper, we examine the usability of the Google's Cloud Video Intelligence API in adversarial environments. In particular, we investigate whether an adversary can subtly manipulate a video in such a way that the API will return only the adversary-desired labels. For this, we select an image, which is different from the video content, and insert it, periodically and at a very low rate, into the video. We found that if we insert one image every two seconds, the API is deceived into annotating the video as if it only contained the inserted image. Note that the modification to the video is hardly noticeable as, for instance, for a typical frame rate of 25, we insert only one image per 50 video frames. We also found that, by inserting one image per second, all the shot labels returned by the API are related to the inserted image. We perform the experiments on the sample videos provided by the API demonstration website and show that our attack is successful with different videos and images."""	adversary (cryptography);algorithm;application programming interface;computer vision;digital video;experiment;frame (video);usability;video content analysis	Hossein Hosseini;Baicen Xiao;Radha Poovendran	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2017.171	computer vision;video capture;smacker video;artificial intelligence;world wide web;microsoft video 1;frame rate;video tracking;video processing;video production;computer science;contextual image classification	Vision	-11.88797585678415	-54.745742046242	13812
27c0dcf46b321f151f63aa8630c244ccebd379e0	robust excitation-based features for automatic speech recognition	mel filter banks robust excitation automatic speech recognition noise robust features speech excitation signal vocal tract asr excitation based features ebf state of the art deep neural network dnn hybrid acoustic model excitation features broad phonetic classes ami meeting transcription system error rate reduction;speech excitation signal neural networks automatic speech recognition;acoustics;training;speech recognition feature extraction filtering theory neural nets;speech;hidden markov models;feature extraction;speech robustness feature extraction speech recognition acoustics training hidden markov models;speech recognition;robustness	In this paper we investigate the use of noise-robust features characterizing the speech excitation signal as complementary features to the usually considered vocal tract based features for Automatic Speech Recognition (ASR). The proposed Excitation-based Features (EBF) are tested in a state-of-the-art Deep Neural Network (DNN) based hybrid acoustic model for speech recognition. The suggested excitation features expand the set of periodicity features previously considered for ASR, expecting that these features help in a better discrimination of the broad phonetic classes (e.g., fricatives, nasal, vowels, etc.). Our experiments on the AMI meeting transcription system showed that the proposed EBF yield a relative word error rate reduction of about 5% when combined with conventional PLP features. Further experiments led on Aurora4 confirmed the robustness of the EBF to both additive and convolutive noises, with a relative improvement of 4.3% obtained by combinining them with mel filter banks.	acoustic cryptanalysis;acoustic model;deep learning;experiment;filter bank;pl/p;quasiperiodicity;speech recognition;tract (literature);transcription (software);utility functions on indivisible goods;word error rate	Thomas Drugman;Yannis Stylianou;Langzhou Chen;Xie Chen;Mark J. F. Gales	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7178855	voice activity detection;speech recognition;feature extraction;computer science;speech;machine learning;pattern recognition;speech processing;acoustic model;hidden markov model;robustness	Robotics	-13.290958076183623	-90.55106469032869	13815
e6b765e4c146e838578b8609a47caabc2929fd08	an empirical study of required dimensionality for large-scale latent semantic indexing applications	empirical study;lsi;dimensionality;large scale;latent semantic indexing;multiple valued	The technique of latent semantic indexing is used in a wide variety of commercial applications. In these applications, the processing time and RAM required for SVD computation, and the processing time and RAM required during LSI retrieval operations are all roughly linear in the number of dimensions, k, chosen for the LSI representation space. In large-scale commercial LSI applications, reducing k values could be of significant value in reducing server costs. This paper explores the effects of varying dimensionality.  The approach taken here focuses on term comparisons. Pairs of terms are considered which have strong real-world associations. The proximities of members of these pairs in the LSI space are compared at multiple values of k. The testing is carried out for collections of from one to five million documents. For the five million document collection, a value of k ≈ 400 provides the best performance.  The results suggest that there is something of an 'island of stability' in the k = 300 to 500 range. The results also indicate that there is relatively little room to employ k values outside of this range without incurring significant distortions in at least some term-term correlations.	archive;computation;distortion;latent semantic analysis;random-access memory;server (computing);singular value decomposition	Roger B. Bradford	2008		10.1145/1458082.1458105	latent semantic indexing;curse of dimensionality;computer science;data science;machine learning;data mining;database;empirical research;world wide web;information retrieval	Web+IR	-15.066822207841462	-63.64601713315848	13824
0a416b58d76d6734bd27e5aa41adf1555804b3bf	schwa-assimilation in danish synthetic speech		Assimilation of schwa into surrounding sonorant consonants is a vital feature of natural Danish speech. It varies with speaking rate and speaking style and is more likely to occur in some phonological contexts than in others. This presents some problems for the implementation of the process into a Danish textto-speech system.	data assimilation;speech synthesis;synthetic intelligence	Christian Jensen	2001			speech recognition;linguistics	AI	-11.16462529849547	-81.69772569653766	13839
a0dd5c5d01372432a947048504ca6dd09dfffda4	do people understand irony from computers?		In this paper, we empirically investigate whether people understand irony from computers in order to test the recent argument for an egocentric tendency in irony comprehension. In the experiment, participants took a timed math test comprising 10 questions of 3-digit by 2-digit multiplication. After that, they received a feedback comment on their performance (including potentially ironic sentences) from either an intelligent evaluation system with an AI engine (AI condition), a nonintelligent automatic evaluation system (Auto condition), or a human judge connected via the network (Human condition). The result was that the participants in the AI and Auto conditions understood the comment as ironic as those in the Human condition, and the participants in the AI condition perceived more sarcasm than other participants. Because people know that computers cannot think just as humans do, these results can be regarded as evidence for the egocentric tendency in irony comprehension, indicating that participants understood irony egocentrically from their own perspective without taking into account the mental state of the ironic speaker. These findings are also consistent with the “media equation” theory, from which we can suggest implications for the media equation, anthropomorphism, and computer-mediated communication of irony.	computer;computer-mediated communication;irony;mental state	Akira Utsumi;Yu Watanabe;Yusuke Wakayama	2013				HCI	-51.32586135688435	-53.308083519519904	13840
76450639a518701df45d25c7c708ac9014b46d8f	how different mental workload levels affect the take-over control after automated driving	manuals;phase measurement;visualization;monitoring;vehicles;wheels;automation	Automated driving represents a new way of driving and, therefore, significant changes in driver behaviour are expected. In this study, we analysed the impact of different levels of mental workload associated with a non-driving task in response to a take-over request (TOR). Thirty participants drove in a driving simulator through three experimental conditions: once in manual driving and twice following an automated driving during which a non-driving task was performed. The impact of the mental workload was evaluated by manipulating the mental workload induced by the non-driving task (low versus high mental workload). The results showed a negative effect of automated driving on take-over performance regardless of the level of mental workload. These results point out that engagement in non-driving task leads to reduced situation awareness. Interpretations of results and research prospects are discussed.	autonomous car;driving simulator;human factors and ergonomics;human–computer interaction;simulation	M. Bueno;E. Dogan;F. Hadj Selem;Eric Monacelli;S. Boverie;A. Guillaume	2016	2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2016.7795886	embedded system;real-time computing;simulation;engineering	SE	-46.89359606126145	-52.66688778638228	13842
72cebece9ff6d4981b67662fb40ae3efbf9d496d	toward real-time recognition of acoustic musical instruments	second order;dynamic change;spectral function;standard deviation;real time;spectrum;musical instruments;reference point;learning system;distribution function;first order;spatial distribution;feature weighting;random variable;genetic algorithm;k nearest neighbor;method of moment;steady state	A real-time timbre recognition system based of Miller Puckette’s fiddle program was tested using the attack portions of acoustic musical instruments. The dynamically changing spectra are quantified by the velocities of the integral, the centroid, the standard deviation, and the skewness of the spectra and the velocity of the estimated pitches. The mean and the standard deviation of the five parameters were also calculated. These features were stored in the database for an exemplar-based learning system, which is based on a k-nearest neighbor classifier. The system is enhanced by a genetic algorithm, which finds the optimal set of feature weights to improve the recognition rate.	acoustic cryptanalysis;genetic algorithm;k-nearest neighbors algorithm;nearest neighbour algorithm;real-time clock;real-time transcription;velocity (software development)	Angela Fraser;Ichiro Fujinaga	1999			speech recognition;pattern recognition;mathematics;statistics	AI	-9.825678266457464	-96.54235816582212	13871
6fe8526948163574d280c873b352cb6338e5fdbf	detecting music in ambient audio by long-window autocorrelation	svm music detection long window autocorrelation audio recording aperiodic noise lpc gmm;lpc;support vector machines audio recording audio signal processing correlation methods gaussian processes music;audio signal processing;long window autocorrelation;support vector machines;gaussian processes;speech analysis;signal detection;correlation speech analysis music acoustic signal detection;gmm;audio recording;correlation methods;indexing terms;music detection;autocorrelation acoustic noise music noise shaping rhythm multiple signal classification audio recording acoustic signal detection video recording data mining;acoustic signal detection;svm;correlation;electrical engineering;high performance;aperiodic noise;music	We address the problem of detecting music in the background of ambient real-world audio recordings such as the sound track of consumer-shot video. Such material may contain high levels of noises, and we seek to devise features that will reveal music content in such circumstances. Sustained, steady musical pitches show significant, structured autocorrelation at when calculated over windows of hundreds of milliseconds, where autocorrelation of aperiodic noise has become negligible at higher-lag points if a signal is whitened by LPC. Using such features, further compensated by their long-term average to remove the effect of stationary periodic noise, we produce GMM and SVM based classifiers with high performance compared with previous approaches, as verified on a corpus of real consumer video.	autocorrelation;google map maker;microsoft windows;sensor;stationary process;support vector machine	Keansub Lee;Daniel P. W. Ellis	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4517533	support vector machine;speech recognition;computer science;machine learning;mathematics;statistics	Visualization	-10.092049024763993	-92.80785703092677	13872
0fc62026ddeb10657e555856267f2aadf4d33ee6	advances in transcription of broadcast news and conversational telephone speech within the combined ears bbn/limsi system	cable television;broadcast news;desciframiento;modelizacion;general modeling techniques;appareillage essai;word error rate;news;document audiovisuel;system combination hidden markov models hmms large training corpora speech recognition;speech synthesis;decodage;generic model;learning;decoding;analyse linguistique;taux erreur;hidden markov model;defense advanced research projects agency ears program;combined ears bbn limsi system;speech processing;modele markov variable cachee;tratamiento palabra;system combination strategies broadcast news transcription conversational telephone speech combined ears bbn limsi system word error rate reduction effective affordable reusable speech to text program defense advanced research projects agency ears program general modeling techniques recognition accuracy improvements fast decoding architectures;traitement parole;conversacion;speech coding;probabilistic approach;word error rate reduction;telephony;fast decoding architectures;linguistic analysis;aprendizaje;modelisation;accuracy;apprentissage;precision;system combination strategies;hidden markov models;reconocimiento voz;telephony broadcasting speech coding speech recognition speech synthesis;system combination;enfoque probabilista;approche probabiliste;aparato ensayo;documento audiovisual;analisis linguistico;speech to text;hidden markov models hmms;conversation;audiovisual document;teledistribution;testing equipment;error rate;speech recognition;noticias;large training corpora;effective affordable reusable speech to text program;sintesis palabra;reconnaissance parole;broadcasting;indice error;defense advanced research project agency;actualites;modeling;broadcasting telephony speech ear collaboration error analysis data engineering training data decoding system testing;teledistribucion;synthese parole;recognition accuracy improvements;conversational telephone speech;broadcast news transcription	This paper describes the progress made in the transcription of broadcast news (BN) and conversational telephone speech (CTS) within the combined BBN/LIMSI system from May 2002 to September 2004. During that period, BBN and LIMSI collaborated in an effort to produce significant reductions in the word error rate (WER), as directed by the aggressive goals of the Effective, Affordable, Reusable, Speech-to-text [Defense Advanced Research Projects Agency (DARPA) EARS] program. The paper focuses on general modeling techniques that led to recognition accuracy improvements, as well as engineering approaches that enabled efficient use of large amounts of training data and fast decoding architectures. Special attention is given on efforts to integrate components of the BBN and LIMSI systems, discussing the tradeoff between speed and accuracy for various system combination strategies. Results on the EARS progress test sets show that the combined BBN/LIMSI system achieved relative reductions of 47% and 51% on the BN and CTS domains, respectively	acoustic cryptanalysis;algorithm;carpal tunnel syndrome;feature extraction;language model;super robot monkey team hyperforce go!;text corpus;transcription (software);word error rate	Spyridon Matsoukas;Jean-Luc Gauvain;Gilles Adda;Thomas Colthurst;Chia-Lin Kao;Owen Kimball;Lori Lamel;Fabrice Lefèvre;Jeff Z. Ma;John Makhoul;Long Nguyen;Rohit Prasad;Richard M. Schwartz;Holger Schwenk;Bing Xiang	2006	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2006.878257	simulation;speech recognition;word error rate;computer science;machine learning;accuracy and precision;hidden markov model	Visualization	-20.908391503904387	-86.54604489131033	13878
04c28c9275604f04ddee12c051ecef9b62337597	automating rule generation for grammar checkers		In this paper, I describe several approaches to automatic or semiautomatic development of symbolic rules for grammar checkers from the information contained in corpora. The rules obtained this way are an important addition to manually-created rules that seem to dominate in rulebased checkers. However, the manual process of creation of rules is costly, time-consuming and error-prone. It seems therefore advisable to use machine-learning algorithms to create the rules automatically or semiautomatically. The results obtained seem to corroborate our initial hypothesis that symbolic machine learning algorithms can be useful for acquiring new rules for grammar checking. It turns out, however, that for practical uses, error corpora cannot be the sole source of information used in grammar checking. We suggest therefore that only by using different approaches, grammar-checkers, or more generally, computer-aided proofreading tools, will be able to cover most frequent and severe mistakes and avoid false alarms that seem to distract users. In what follows, I will show how Transformation-Based Learning (TBL) algorithms may be used to acquire rules. Before doing that, I will discuss the pros and cons of three approaches to creating rules and show the need to make use of them all in a successful grammar-checking tool. The results obtained seem to suggest that the machine-learning approach is actually fruitful, and I will point to some future work related to the reported research.	algorithm;cognitive dimensions of notations;grammar checker;information source;machine learning;spell checker;text corpus	Marcin Milkowski	2011	CoRR		natural language processing;algorithm	ML	-27.851283595441814	-82.92625778734428	13952
afd92dcd64d1590dcd1d69330c72e1aeeb565ba8	multimedia database system for tv newcasts and newspapers	multimedia database system	It is important to use pattern information (e.g. TV newscasts) and textual information (e.g. newspapers) together. For this purpose, we describe a method for aligning articles in TV newscasts and newspapers. Also, we describe a method for extracting a newspaper article and its follow-ups. In order to align articles, the alignment system uses words extracted from telops in TV newscasts. The recall and the precision of the alignment process are 97% and 89%, respectively. On the other hand, in order to obtain a newspaper and its follow-ups, the system uses typical expressions which give signs of subsequent articles. The recall and precision are 80% and 85%, respectively. Using the results of these processes, we develop a browsing and retrieval system for articles in TV newscasts and newspapers.		Yasuhiko Watanabe;Yoshihiro Okada;Kengo Kaneji;Yoshitaka Sakamoto	1998		10.1007/3-540-48962-2_15	computer science;multimedia;world wide web;information retrieval	DB	-30.891772845364	-65.62722780524905	13973
714537cf5ec70d1b913dc994c439f85976b5fe0e	handling granularity differences in knowledge integration		Knowledge integration is a process of combining two different knowledge representations together. This task is important especially in learning where new information is combined with prior knowledge or in understanding where a coherent knowledge representation should be generated out of several knowledge fragments. A challenging problem in KI is handling granularity differences, i.e. combining together two knowledge representations with granularity differences. This paper presents an algorithm to find such correspondences between two representations with a granularity difference and to combine the two representations together based on the correspondences. The algorithm uses coarsening operators which generate coarse-grained representations from a representation. At the end, we introduce a large scale project in which the algorithm will be used. Knowledge Integration (KI) is a process of combining two pieces of knowledge together. KI is an important step in human activities such as learning, reading comprehension, reasoning and others. For example, in learning, one must combine the new knowledge with the old, and in reading comprehension, one must relate together individual sentences in a text which often differ in level of detail. Just as in human activities, KI is also important in many AI tasks such as knowledge acquisition, story understanding, and multidocument summarization. All these tasks commonly require the combination of several pieces of knowledge to generate a coherent knowledge representation. A challenging problem in KI is integrating together two knowledge representations with granularity differences. Simply putting the representations together without considering the granularity differences could result in an incorrect representation. Ideally, the two representations should be aligned together based on identification of which parts in the fine-grained correspond to which parts in the coarsegrained. This paper presents an algorithm to find the correspondences between two representations with a granularity difference and to combine the representations together based Copyright c © 2007, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. In this paper, by knowledge, we mean factual knowledge which can be represented by logic. We will use conceptual graphs or logical predicates as our notation. on the correspondences. The algorithm uses coarsening operators which generate coarse-grained representations from a representation. Example This section compares a good and a bad KI on the two texts below about blood circulation. These texts used in the learning-by-reading project (Barker et al. 2007) were excerpted from a biology textbook. Fig. 1a and fig. 1b show the encoding of the long and the short text respectively. The Component Library (Barker, Porter, u0026 Clark 2001) was used as the ontology for the encoding. Text1: Hearts pump blood through the body. Blood carries oxygen to organs throughout the body. Blood leaves the heart, then goes to the lungs where it is oxygenated. The oxygen given to the blood by the lungs is then burned by organs throughout the body. Eventually the blood returns to the heart, depleted of oxygen. It is then pumped by the heart back to the lungs. Text2: Heart pumps blood to circulate around the body. Fig. 2 shows a bad combination by a simple graph join (Sowa 1984) on the two representations in fig. 1. In this combination, Move3 and Move1 are unified together. Notice that Move3 is from the coarse-grained representation (fig. 1a), and Move1 is from the fine-grained representation(fig. 1b). Consequently, Move13, resulting from the unification of Move1 and Move3, comes to have two destinations (Lung1 and Heart12), such that its English interpretation would be “Blood moves from the Heart to the Heart and the Lung.” In contrast, fig. 3 illustrates a good combination in which Move1 is identified as a superevent of Obtain1, Move1, Carry1, Move2, and thus subevent relations are estabilished between Move1 and Obtain1, Move1, Carry1, Move2. The English interpretation for this combination would be Blood moves from Heart to Heart. This Due to the limitation of space, some information in the long text is not encoded.	knowledge integration	Doo Soon Kim;B. Porter	2007			knowledge representation and reasoning;natural language processing;machine learning;automatic summarization;computer science;knowledge acquisition;ontology;granularity;notation;artificial intelligence;knowledge integration;unification	Robotics	-38.78090920854778	-74.63491167741482	13975
dfa9ebfc136243eba7ffd22a4842dfeeb4ca4424	utilizing typed dependency subtree patterns for answer sentence generation in question answering systems		Question Answering over Linked Data (QALD) refer to the use of Linked Data by question answering systems, and in recent times this has become increasingly popular as it opens up a massive Linked Data cloud which is a rich source of encoded knowledge. However, a major shortfall of current QALD systems is that they focus on presenting a single fact or factoid answer which is derived using SPARQL (SPARQL Protocol and RDF Query Language) queries. There is now an increased interest in development of human-like systems which would be able to answer questions and even hold conversations by constructing sentences akin to humans. In this paper, we introduce a new answer construction and presentation system, which utilizes the linguistic structure of the source question and the factoid answer to construct an answer sentence which closely emanates a human-generated answer. We employ both semantic Web technology and the linguistic structure to construct the answer sentences. The core of the research resides on extracting dependency subtree patterns from the questions and utilizing them in conjunction with the factoid answer to generate the answer sentence with a natural feel akin to an answer from a human when asked the question. We evaluated the system for both linguistic accuracy and naturalness using human evaluation. These evaluation processes showed that the proposed approach is able to generate answer sentences which have linguistic accuracy and natural readability quotients of more than 70%. In addition, we also carried out a feasibility analysis on using automatic metrics for answer sentence evaluation. The results from this phase showed that the there is not a strong correlation between the results from automatic metric evaluation and the human ratings of the machine-generated answers.	bleu;human-readable medium;information source;linked data;meteor;machine translation;question answering;rdf query language;sparql;semantic web;software quality assurance;tag cloud;tree (data structure);ural (computer)	Rivindu Perera;Parma Nand;Asif Naeem	2017	Progress in Artificial Intelligence	10.1007/s13748-017-0113-9	natural language processing;computer science;data mining;information retrieval	NLP	-28.261981437156425	-66.30305902850232	13985
a4593b326d2f14918a549dfc74488691d2fc6342	semantic pattern for user-interactive question answering	answer extraction;user interface;question answering information retrieval;semantic pattern;pattern generation;null;user satisfaction semantic pattern user interactive question answering qa system question structure analysis pattern matching pattern generation pattern classification answer extraction user interface;user interfaces pattern classification question answering information retrieval;tagger ontology;pattern matching;pattern classification;user interaction;user satisfaction;user interfaces;question answering;structure analysis	A new semantic pattern is proposed in this paper, which can be used by users to post questions and answers in user-interactive question answering (QA) system. The necessary procedures of using semantic pattern in a QA system are also presented, which include question structure analysis, pattern matching, pattern generation, pattern classification and answer extraction. A user interface of using semantic pattern is also implemented in our QA system, which allows users to effectively post and answer questions. It gains good overall user satisfaction.	computer user satisfaction;google questions and answers;pattern matching;question answering;user interface	Tianyong Hao;Qingtian Zeng;Wenyin Liu	2006		10.1109/SKG.2006.89	question answering;computer science;operating system;data mining;database;programming language;user interface;information retrieval	Web+IR	-31.968499503939146	-54.50426277101269	14011
96d1905df3903fae9c18028e64045f75454c59ee	automated intelligent system for sound signalling device quality assurance	audio signal processing;artificial neural networks;manufacturing;non speech sound recognition;auto encoder;automation	This paper presents a novel approach to the detection and recognition of faulty audio signalling devices as part of an automated industrial manufacturing quality assurance process. The proposed system outperforms other well-established automated systems based on mel-frequency cepstrum coefficients (MFCC) and multi-layer perceptron (MLP). It uses both unlabelled sound data and labelled historical data acquired from human experts in detecting faulty signalling devices. The unlabelled data is used to train a deep neural network generative model to create multiple levels of hierarchical feature extractors which are used to train an MLP classifier, with the intent to model the human reasoning and judging processes in respect to sound classification. This paper presents the results of real world experiments based on data pertaining to the audio signalling quality assurance process for car instrument cluster manufacturing. These results show that the proposed system is able to successfully classify speakers into two groups: ‘‘Good’’ and ‘‘No good’’ depending on the part quality. The proposed system proves to be capable enough to eliminate the need for a manual inspection within the manufacturing process and is shown to be able to diagnose a fault with a high degree of accuracy. This work can be extended to other areas of automotive inspection where there is a need for a robust solution to sound detection and where an output signal is represented by a complex and changing frequency spectrum even with significant environmental noise. 2014 Elsevier Inc. All rights reserved.	artificial intelligence;artificial neural network;coefficient;deep learning;experiment;generative model;layer (electronics);mel-frequency cepstrum;memory-level parallelism;multilayer perceptron;sensor;spectral density	Tomasz Maniak;Chrisina Jayne;Rahat Iqbal;Faiyaz Doctor	2015	Inf. Sci.	10.1016/j.ins.2014.09.042	simulation;speech recognition;audio signal processing;computer science;artificial intelligence;automation;machine learning;manufacturing;artificial neural network;algorithm;autoencoder	AI	-10.697395063150044	-97.48021993832477	14022
1821c28f8505e6d43eba10c3ebd39b53abfd37f9	ontology development for a pharmacogenetics knowledge base	knowledge base;knowledge representation	Research directed toward discovering how genetic factors influence a patient's response to drugs requires coordination of data produced from laboratory experiments, computational methods, and clinical studies. A public repository of pharmacogenetic data should accelerate progress in the field of pharmacogenetics by organizing and disseminating public datasets. We are developing a pharmacogenetics knowledge base (PharmGKB) to support the storage and retrieval of both experimental data and conceptual knowledge. PharmGKB is an Internet-based resource that integrates complex biological, pharmacological, and clinical data in such a way that researchers can submit their data and users can retrieve information to investigate genotype-phenotype correlations. Successful management of the names, meaning, and organization of concepts used within the system is crucial. We have selected a frame-based knowledge-representation system for development of an ontology of concepts and relationships that represent the domain and that permit storage of experimental data. Preliminary experience shows that the ontology we have developed for gene-sequence data allows us to accept, store, and query data submissions.	attribute-value system;clinical data;computation;experiment;frame language;internet;knowledge base;knowledge representation and reasoning;name;ontology;organizing (structure);patients;pharmgkb;pharmacogenetics;pharmacogenomic analysis;pharmacology	Diane E. Oliver;Daniel L. Rubin;Joshua M. Stuart;Micheal Hewett;Teri E. Klein;Russ B. Altman	2002	Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing		knowledge base;computer science;bioinformatics;knowledge management;knowledge-based systems;data mining	AI	-49.17106099554752	-65.39047322555743	14032
919966f56743d365424df0fe5d56f6b4e028c1e2	tc-star: specifications of language resources and evaluation for speech synthesis	speech synthesis	In the framework of the EU funded project TC-STAR (Technology and Corpora for Speech to Speech Translation), research on TTS aims on providing a synthesized voice sounding like the source speaker speaking the target language. To progress in this direction, research is focused on naturalness, intelligibility, expressivity and voice conversion both, in the TC-STAR framework. For this purpose, specifications on large, high quality TTS databases have been developed and the data have been recorded for UK English, Spanish and Mandarin. The development of speech technology in TC-STAR is evaluation driven. Assessment of speech synthesis is needed to determine how well a system or technique performs in comparison to previous versions as well as other approaches (systems & methods). Apart from testing the whole system, all components of the system are evaluated separately. This approach grants better assessment of each component as well as identification of the best techniques in the different speech synthesis processes. This paper describes the specifications of Language Resources for speech synthesis and the specifications for evaluation of speech synthesis activities.	automatic sounding;compiler;database;display resolution;intelligibility (philosophy);netware file system;speech synthesis;speech technology;super robot monkey team hyperforce go!;text corpus	Antonio Bonafonte;Harald Höge;Imre Kiss;Asunción Moreno;Ute Ziegenhain;Henk van den Heuvel;Horst-Udo Hain;Xia S. Wang;M. N. Garcia	2006			artificial intelligence;speech translation;natural language processing;language technology;speech recognition;speech technology;chinese speech synthesis;speech corpus;telegraphic speech;computer science;intelligibility (communication);speech synthesis	NLP	-23.688259948096725	-84.04512333017406	14034
0db7bae970d09a2ad1d8032f83e0b341513eab3a	broadcast news subtitling system in portuguese	broadcast news;portuguese public broadcast company;broadcast news subtitling system;multimedia systems speech processing speech recognition;speech processing;indexing terms;multimedia systems;automatic speech recognition;automatic speech recognition natural languages tv broadcasting broadcast technology multimedia communication vocabulary pipelines filters streaming media teletext;speech recognition;automatic speech recognition broadcast news subtitling system portuguese public broadcast company	The subtitling of broadcast news programs are starting to become a very interesting application due to the technological advances in automatic speech recognition and associated technologies. However, to build this kind of systems, several advances are necessary both in terms of the technological components and on main blocks integration. In this paper, we are presenting the overall architecture of a subtitling system running daily at RTP (the Portuguese public broadcast company). The goal is to integrate our components in a system for the subtitling of RTP programs. The global system includes the subtitling of recorded and direct programs.	real-time computing;speech recognition	João Paulo da Silva Neto;Hugo Meinedo;Márcio Viveiros;Renato Cassaca;Ciro Martins;Diamantino Caseiro	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4517921	speech recognition;index term;computer science;broadcast journalism;speech processing;multimedia	Robotics	-23.350704284784435	-86.92277944094259	14052
4b8851567df0e9883e829cf3d78b4744167a5e38	xml data representation in document image analysis	xml data representation;chinese character;document image analysis;document rec;technical advance;key technical development;digital library;xml;data representation;digital libraries;reverse engineering;xslt;image recognition	This paper presents the XML-based formats ALTO, TEI, METS used for digital libraries and their interest for data representation in a document image analysis and recognition (DIAR) process. In the first part we briefly present these formats with focus on their adequacy for structural representation and modeling of DIAR data. The second part shows how these formats can be used in a reverse engineering process. Their implementation as a data representation framework will be shown.	common data representation;data (computing);digital library;image analysis;image file formats;library (computing);metadata encoding and transmission standard;reverse engineering;text encoding initiative;xml;xerox alto	Abdel Belaïd;Yves Rangoni;I. Falk	2007	Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)	10.1109/ICDAR.2007.272	digital library;xml;xslt;computer science;database;external data representation;world wide web;information retrieval;reverse engineering	Vision	-13.525130809357218	-58.04223653226725	14055
145ada3128c349ce8d3f925bc3280e8b18049405	improvement of non-native speech recognition by effectively modeling frequently observed pronunciation habits	acoustic modeling;target language;speech recognition;non native speaker	In this paper, two techniques are proposed to enhance the nonnative (Japanese English) speech recognition performance. The first technique effectively integrates orthographic representation of a phoneme as an additional context in state clustering in training tied-state triphones. Non-native speakers often learned the target language not through their ears but through their eyes and it is easily assumed that their pronunciation of a phoneme may depend upon its grapheme. Here, correspondence between a vowel and its grapheme is automatically extracted and used as an additional context in the state clustering. The second technique elaborately couples a Japanese English acoustic model using triphones, mapping between the two models should be carefully trained because phoneme sets of both the models are different. Here, several phoneme recognition experiments are done to induce the mapping, and based upon the mapping, a tentative method of the coupling is examined. Results of LVCSR experiments show high validity of both the proposed methods.	acoustic cryptanalysis;acoustic model;cluster analysis;compiler;decision tree;experiment;orthographic projection;speech analytics;speech recognition;the 3-d battles of worldrunner;triphone;vocabulary	Koichi Osaki;Keikichi Hirose	2003			natural language processing;speech recognition;computer science;linguistics	NLP	-19.249899761853186	-84.94414680361099	14072
6e35bb49b28f8edac6ca48287cac6b5ef097b4ca	assigning documents to master sites in distributed search	document assignment;distributed index;web search engine;performance improvement;indexation;multi site web search engine;distributed search;web search;distributed architecture	An appealing solution to scale Web search with the growth of the Internet is the use of distributed architectures. Distributed search engines rely on multiple sites deployed in distant regions across the world, where each site is specialized to serve queries issued by the users of its region. This paper investigates the problem of assigning each document to a master site. We show that by leveraging similarities between a document and the activity of the users, we can accurately detect which site is the most relevant to place a document. We conduct various experiments using two document assignment approaches, showing performance improvements of up to 20.8% over a baseline technique which assigns the documents to search sites based on their language.	baseline (configuration management);distributed web crawling;experiment;internet;web search engine	Roi Blanco;Berkant Barla Cambazoglu;Flavio Paiva Junqueira;Ivan Kelly;Vincent Leroy	2011		10.1145/2063576.2063591	search engine indexing;site map;web search engine;semantic search;computer science;data mining;database;search analytics;web search query;world wide web;information retrieval;search engine	Web+IR	-32.44855460261268	-55.45117261958449	14075
a611b4770e872092c927ad484f76d9787ab05f42	myhealthtoday: helping patients with their healthschedule using a 24-hour clock visualization		We propose a variation on the 24-hour clock visualization to represent daily health schedules. The area inside the clock is used to display a graph network which helps patients explore and understand the rationale for each health-related scheduled task, such as taking medication. We investigate whether this visualization can be leveraged to increase patient comprehension of personal health schedules. Two low and one high-fidelity prototype have been designed and evaluated. Participants in our study included both general practitioners and patients. Results are promising and indicate that our visualization can be an effective means to explore and understand health schedules. Moreover, our results suggest there is an actual need for visual exploration of health schedules. Finally, participants perceive that our proof-of-concept provides useful feedback and can help both patients and physicians to discuss and explore health schedules.	24-hour clock;design rationale;e-patient;fits;interaction;iteration;prototype;schedule (computer science);smartwatch;tablet computer	Robin De Croon;Bruno De Lemos Ribeiro Pinto Cardoso;Katrien Verbert	2017	2017 IEEE International Conference on Healthcare Informatics (ICHI)	10.1109/ICHI.2017.32	24-hour clock;data visualization;visualization;multimedia;taking medication;information visualization;schedule;comprehension;computer science;graph	Visualization	-61.672825582771054	-56.07112828006128	14090
28ef385643cfa7728227029b62f712459ede5280	prominent features of rumor propagation in online social media	time series social networking online;conference;time series;sentiment analysis rumor social media time series diffusion network;rumor classification rumor propagation online social media online social networks temporal characteristics periodic time series model daily shock cycles external shock cycles structural differences linguistic differences;social networking online;electric shock pragmatics twitter mathematical model time series analysis psychology adaptation models	The problem of identifying rumors is of practical importance especially in online social networks, since information can diffuse more rapidly and widely than the offline counterpart. In this paper, we identify characteristics of rumors by examining the following three aspects of diffusion: temporal, structural, and linguistic. For the temporal characteristics, we propose a new periodic time series model that considers daily and external shock cycles, where the model demonstrates that rumor likely have fluctuations over time. We also identify key structural and linguistic differences in the spread of rumors and non-rumors. Our selected features classify rumors with high precision and recall in the range of 87% to 92%, that is higher than other states of the arts on rumor classification.	baseline (configuration management);online and offline;precision and recall;social media;social network;software propagation;time series	Sejeong Kwon;Meeyoung Cha;Kyomin Jung;Wei Chen;Yajun Wang	2013	2013 IEEE 13th International Conference on Data Mining	10.1109/ICDM.2013.61	artificial intelligence;time series;mathematics;statistics	DB	-21.011053060001526	-58.80626132077887	14099
1603109ca4f910fa8b8b19912cf2b211316a3aa1	text search of surnames in some slavic and other morphologically rich languages using rule based phonetic algorithms	information retrieval;natural languages;natural language processing information retrieval;search recall surname text search slavic language morphologically rich language rule based phonetic algorithm natural identifiers information systems phonetic search algorithm surname string matching communications service providers person registries social networks genealogy databases slovak language czech language polish language ukrainian language russian language german language hungarian language jewish language search precision;algorithm design and analysis materials ieee transactions speech speech processing databases europe;algorithms;natural languages algorithms information retrieval	Surnames play a key role as person natural identifiers, essentially in present information systems. This paper deals with the topic of optimizing a phonetic search algorithm as a string matching of surnames usable for communications service providers, person registries, social networks or genealogy databases. It describes a proposed solution for the phonetic searching of Slovak and (territorial) neighboring languages (Czech, Polish, Ukrainian, Russian, German, Hungarian, Jewish) surnames. This solution was designed to improve search precision and recall when searching for people by their surnames originating in these languages.	daitch–mokotoff soundex;data store;database;identifier;information system;metaphone;phonetic algorithm;precision and recall;search algorithm;social network;string searching algorithm;syllable	Dusan Zahoransky;Ivan Polásek	2015	IEEE/ACM Transactions on Audio, Speech, and Language Processing	10.1109/TASLP.2015.2393393	natural language processing;language identification;speech recognition;universal networking language;computer science;linguistics;natural language	Web+IR	-33.681114368619305	-71.11905507286576	14104
17ff6ebd5c5174f0a303cf747ac10c15f0d3abb9	a preliminary study on the use of demisyllables in automatic speech recognition	prototypes;vocabulary;automatic speech recognition vocabulary prototypes copper speech recognition mediation concatenated codes linear predictive coding system testing robustness;automatic speech recognition;linear predictive coding;mediation;pattern recognition;speech recognition;system testing;robustness;concatenated codes;copper	A speech recognition system is described for recognizing isolated words from reference templates created by concatenating demisyllables from a corpus of about 1000 demisyllables. The composition (in terms of demisyllables) of each reference word is specified in a lexicon with one or more entries for each word of the vocabulary. Experiments were carried out, using a 100-word vocabulary, to investigate the usefulness of such a representation and the effect on performance of some simple modifications in demisyllable specification and durations of reference patterns. Recognition accuracy of 97.6% was obtained using 132 reference templates for the 100-word vocabulary.	speech recognition	Aaron E. Rosenberg;Lawrence R. Rabiner;Stephen E. Levinson;Jay G. Wilpon	1981		10.1109/ICASSP.1981.1171360	natural language processing;linear predictive coding;speech recognition;computer science;prototype;mediation;copper;system testing;robustness	NLP	-21.533518640397613	-83.42635327067043	14107
51270181dc7bc59a3542402ef5fbad284f0dcca5	quantifying sentence complexity based on eye-tracking measures.		Eye-tracking reading times have been attested to reflect cognitive processes underlying sentence comprehension. However, the use of reading times in NLP applications is an underexplored area of research. In this initial work we build an automatic system to assess sentence complexity using automatically predicted eye-tracking reading time measures and demonstrate the efficacy of these reading times for a well known NLP task, namely, readability assessment. We use a machine learning model and a set of features known to be significant predictors of reading times in order to learn per-word reading times from a corpus of English text having reading times of human readers. Subsequently, we use the model to predict reading times for novel text in the context of the aforementioned task. A model based only on reading times gave competitive results compared to the systems that use extensive syntactic features to compute linguistic complexity. Our work, to the best of our knowledge, is the first study to show that automatically predicted reading times can successfully model the difficulty of a text and can be deployed in practical text processing applications.	experiment;eye tracking;feedback;machine learning;natural language processing;parsing;word lists by frequency	Abhinav Deep Singh;Poojan Mehta;Samar Husain;Rajkumar Rajakrishnan	2016			artificial intelligence;computer science;eye tracking;pattern recognition;sentence	NLP	-25.36967635412776	-72.92587380698745	14113
94a1d35720180ff7efe7c42be08958a26e98d656	designing an exploratory text analysis tool for humanities and social sciences research	information retrieval;literature;digital humanities;information science;computer science;user interfaces;natural language processing	Author(s): Shrikumar, Aditi | Advisor(s): Hearst, Marti A.; Hartmann, Bjoern | Abstract: This dissertation presents a new tool for exploratory text analysis that attempts to improve the experience of navigating and exploring text and its metadata. The design of the tool was motivated by the unmet need for text analysis tools in the humanities and social sciences. In these fields, it is common for scholars to have hundreds or thousands of text-based source documents of interest from which they extract evidence for complex arguments about society and culture. These collections are difficult to make sense of and navigate. Unlike numerical data, text cannot be condensed, overviewed, and summarized in an automated fashion without losing significant information. And the metadata that accompanies the documents -- often from library records -- does not capture the varied content of the text within.Furthermore, adoption of computational tools remains low among these scholars despite such tools having existed for decades. A recent study found that the main culprits were poor user interfaces and lack of communication between tool builders and tool users. We therefore took an iterative, user-centered approach to the development of the tool. From reports of classroom usage, and interviews with scholars, we developed a descriptive model of the text analysis process, and extracted design guidelines for text analysis systems. These guidelines recommend showing overviews of both the content and metadata of a collection, allowing users to separate and compare subsets of data according to combinations of searches and metadata filters, allowing users to collect phrases, sentences, and documents into custom groups for analysis, making the usage context of words easy to see without interrupting the current activity, and making it easy to switch between different visualizations of the same data.WordSeer, the system we implemented, supports highly flexible slicing and dicing, as well as easier transitions than in other tool between visual analyses, drill-downs, lateral explorations and overviews of slices in a text collection. The tool uses techniques from computational linguistics, information retrieval and data visualization.The contributions of this dissertation are the following. First, the design and source code of WordSeer Version 3, an exploratory text analysis system. Unlike other current systems for this audience, WordSeer 3 supports collecting evidence, isolating and analyzing sub-sets of a collection, making comparisons based on collected items, and exploring a new idea without interrupting the current task. Second, we give a descriptive model of how humanities and social science scholars undertake exploratory text analysis during the course of their work. We also identify pain points in their current workflows and give suggestions on how systems can address these problems. Third, we describe a set of design principles for text analysis systems aimed at addressing these pain points. For validation, we contribute a set of three real-world examples of scholars using WordSeer 3, which was designed according to those principles. As a measure of success, we show how the scholars were able to conduct analyses yielding otherwise inaccessible results useful to their research.	exploratory testing	Aditi Shrikumar	2013			humanities;usability;data science;computational linguistics;world wide web;metadata;source code;workflow;social science;social studies;user interface;computer science;source document	HCI	-38.78570214505247	-53.86370913083872	14138
8a6f128157054528316bdb66ecfa6627b22581a2	cost-effectiveness comparison of manual and on-line retrospective bibliographic searching	line search;general and miscellaneous mathematics computing and information science;comparative analysis;information sources;information retrieval;investigations;on line systems 990300 information handling;graphs;mathematical models;online systems;indexation;comparative evaluations;tables data;cost effectiveness;economics;cost;automation	A study to compare the cost effectiveness of retrospective manual and on-line bibliographic searching is described. Forty search queries were processed against seven abstracting-indexing publications and the corresponding SDC/ORBIT data bases. Equivalent periods of coverage and searcher skill levels were used for both search models. Separate task times were measured for question analysis, searching, photocopying, shelving, and output distribution. Component costs were calculated for labor, information, reproduction, equipment, physical space, and telecommunications. Results indicate that on-line searching is generally faster, less costly, and more effective than manual searching. However, for certain query/information-source combinations, manual searching may offer some advantages in precision and turn-around time. The results of a number of related studies are reviewed.		Dennis R. Elchesen	1978	Journal of the American Society for Information Science. American Society for Information Science	10.1002/asi.4630290204	library science;qualitative comparative analysis;cost-effectiveness analysis;computer science;data science;automation;mathematical model;data mining;database;graph;line search;world wide web;information retrieval	Web+IR	-37.03485601181457	-61.30682338886945	14153
f23441eee6553f7a713fe63291e432058efb2329	practical approach to automatic text summarization	itemsets;text;document summarization;abstracting;automatic text summarization;summarizer;term frequency;classification;sentence selection;classifier;machine learning;condensation;array;evaluation;highlight;heuristics;abstract;extraction;categorization	The significance of automatic document summarization increases with the threat of information overload we are facing. Short summaries can be presented to users, for example, in place of fulllength documents found by a search engine in response to a user’s query. We have analyzed various approaches to document summarization, using some existing algorithms and combining these with a novel use of itemsets. The resulting summarizer is evaluated by comparing classification of original documents and that of abstracts generated automatically. Despite highly promising results achieved by this evaluation, readability of abstracts must be further improved by integrating additional heuristic approaches.	algorithm;automatic summarization;dictionary;heuristic;information overload;markov switching multifractal;web page;web search engine	Jiri Hynek;Karel Jezek	2003			extraction;multi-document summarization;classifier;biological classification;computer science;evaluation;automatic summarization;heuristics;pattern recognition;data mining;tf–idf;information retrieval;condensation;categorization	Web+IR	-26.545189638056826	-63.90204478968815	14163
307821e26b8c4475ff9cf114539b726539d24b52	intelligibility measurement of processed reverberant speech	reverberation;speech processing;speech analysis;filters;acoustic signal processing;acoustic testing;signal processing;speech processing speech analysis signal processing filters australia reverberation acoustic measurements acoustic testing pathology acoustic signal processing;acoustic measurements;pathology;australia	Speech intelligibility is reduced by reverberation, and this is a topic of interest in several disciplines. The literature generally falls into three categories linguistic analysis of the effect of reverberation on speech perception, acoustical aspects of auditorium design for better speech delivery, and signal processing techniques for reducing reverberation of recorded speech. Just as each of these research areas has quite different aims, each generally uses a different method of quantifying the effects of reverberation. Linguistic analysis for speech pathology uses subject response based intelligibility tests with diagnostic analysis of the results. The preference for acoustics design is for an objective measure which predicts intelligibility well, preferably one which can be applied theoretically before construction. Signal processing researchers, when dealing with dereverberation, have rarely quantified the effect of processing on reverberant speech, probably due to the difficulty both of improving the characteristics of reverberant speech, and of the administration of intelligibility tests. This study evaluates alternative methods of intelligibility measurement for application to processed reverberant speech. For this purpose, an objective measure (similar to the predictors often used in acoustics) would be ideal because of their ease of implementation; however the results of this study show that these are of limited use. 2. PROCEDURE	intelligibility (philosophy);signal processing;speech synthesis	David R. Cole;Miles Moody;Sridha Sridharan	1996	Fourth International Symposium on Signal Processing and Its Applications	10.1109/ISSPA.1996.615682	speech recognition;reverberation;computer science;signal processing;speech processing	Arch	-9.866104753241288	-85.28501881020597	14204
f08f269b715c31bfa02dc4df31f5561fd74222f0	bi-weighting domain adaptation for cross-language text classification	traditional machine;novel transfer;feature set;internet world;iterative feature;text classification;satisfied classification performance;bi-weighting domain adaptation;cross-language text classification problem;feature space;traditional data mining method	Text classification is widely used in many realworld applications. To obtain satisfied classification performance, most traditional data mining methods require lots of labeled data, which can be costly in terms of both time and human efforts. In reality, there are plenty of such resources in English since it has the largest population in the Internet world, which is not true in many other languages. In this paper, we present a novel transfer learning approach to tackle the cross-language text classification problems. We first align the feature spaces in both domains utilizing some on-line translation service, which makes the two feature spaces under the same coordinate. Although the feature sets in both domains are the same, the distributions of the instances in both domains are different, which violates the i.i.d. assumption in most traditional machine learning methods. For this issue, we propose an iterative feature and instance weighting (Bi-Weighting) method for domain adaptation. We empirically evaluate the effectiveness and efficiency of our approach. The experimental results show that our approach outperforms some baselines including four transfer learning algorithms.	algorithm;align (company);baseline (configuration management);data mining;document classification;domain adaptation;internet;iterative method;machine learning;online and offline;semi-supervised learning;semiconductor industry;supervised learning	Chang Wan;Rong Pan;Jiefei Li	2011		10.5591/978-1-57735-516-8/IJCAI11-258	computer science;artificial intelligence;machine learning;data mining;mathematics;statistics	AI	-18.002287593305997	-66.49334620023134	14224
90945e6b18e3f8349bb3a7fb597cba2056350682	tree kernel-based svm with structured syntactic knowledge for btg-based phrase reordering	btg constraint-based phrase reordering;structured syntactic feature;structured feature;source phrase;proposed phrase reordering model;btg-based phrase reordering;structured syntactic knowledge;composite kernel;convolution tree kernel;phrase reordering	Structured syntactic knowledge is important for phrase reordering. This paper proposes using convolution tree kernel over source parse tree to model structured syntactic knowledge for BTG-based phrase reordering in the context of statistical machine translation. Our study reveals that the structured syntactic features over the source phrases are very effective for BTG constraint-based phrase reordering and those features can be well captured by the tree kernel. We further combine the structured features and other commonly-used linear features into a composite kernel. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods.	algorithm;baseline (configuration management);convolution;evaluation function;experiment;feature engineering;feature vector;heuristic (computer science);kernel (operating system);kernel method;linux;parse tree;parsing;statistical machine translation	Min Zhang;Haizhou Li	2009			natural language processing;speech recognition;computer science;pattern recognition	NLP	-20.90722384164877	-75.20159123798301	14285
4625665ae979c390900d3771d4712dc2b7d92d80	speech recognition using hidden markov models: a cmu perspective	modelizacion;lenguaje natural;habla continua;cmu;modelo markov;learning;independance locuteur;reconocimiento palabra;hidden markov model;langage naturel;proceso adquisicion;acquisition process;aprendizaje;modelisation;independencia locutor;apprentissage;markov model;hidden markov models;continuous speech;natural language;speaker independency;speech recognition;sphinx system;reconnaissance parole;parole continue;modele markov;sphinx;modeling;processus acquisition	This chapter discusses that Hidden Markov models (HMMs) have become the predominant approach for speech recognition systems. It makes certain structural assumptions, and then tries to learn two sets of parameters from training data. The forward-backward learning algorithm adjusts these model parameters so as to improve the probability that the models generated the training data. This seemingly simple technique has worked surprisingly well, and has led to many state-of-the-art systems. One example of an HMM-based system is SPHINX, a large-vocabulary, speaker-independent, continuous-speech recognition system developed at CMU. The chapter introduces hidden Markov modeling techniques, analyze the reason for their success, and describe some improvements to the standard HMM used is SPHINX. It is believed that these have benefited greatly from the use of detailed subword models, large training databases, and powerful learning techniques.	hidden markov model;markov chain;speech recognition	Kai-Fu Lee;Hsiao-Wuen Hon;Mei-Yuh Hwang;Xuedong Huang	1990	Speech Communication	10.1016/0167-6393(90)90025-5	speech recognition;systems modeling;computer science;artificial intelligence;machine learning;markov model;natural language;hidden markov model	Vision	-21.231946445785745	-91.13096175153436	14294
6bebf15de5ee64ba6bb2fe321c889193f310f623	structuring mined knowledge for the support of hypothesis generation in molecular biology		Hypothesis generation in the life sciences is an empirical process in which obtaining and structuring knowledge from literature plays a significant role. Text mining and Information Extraction techniques are seen as key for programmatically accessing the knowledge captured in the form of free text. We describe progress towards an application that supports the task of generating a hypothesis about biomolecular mechanisms using Semantic Web technologies and a workflow to carry out text mining in a service-oriented architecture. The output is a semantic model with putative biological relationships that have been extracted from literature, with each relationship linked to the corresponding evidence. We present preliminary data that extends a model for chromatin (de)condensation. The methodology can be used to bootstrap the process of human-guided construction of semantically rich biological models using the results of knowledge extraction processes.	information extraction;mined;semantic web;service-oriented architecture;service-oriented device architecture;text mining	Marco Roos;M. Scott Marshall;Andrew P. Gibson;Pieter W. Adriaans	2008			structuring;bioinformatics;biology	AI	-4.54438779660292	-63.54420226665552	14330
dd9cb6d3a116315dfcf13305bf077d7637c7343e	quadratic programming approach to glottal inverse filtering by joint norm-1 and norm-2 optimization	closed phase analysis glottal inverse filtering quadratic programming;quadratic programming;ieee transactions;speech processing;speech;computational modeling;lips;speech quadratic programming lips computational modeling speech processing ieee transactions;glottal inverse filtering;quadratic programming qpr;closed phase analysis	This study proposes an approach for glottal inverse filtering of acoustic speech signals using quadratic programming QPR. The method aims to jointly model the effect of vocal tract and lip radiation with a single filter whose coefficients are optimized using QPR. This optimization is based on the principles of closed phase analysis, where the contribution of the glottal source is attenuated in optimizing the inverse model of the vocal tract. By expressing the optimization problem in terms of the output of a filter, we can apply physically motivated optimization such as flatness of the closed phase. The proposed method was objectively evaluated using a synthetic Liljencrants-Fant model based test set of sustained vowels, as well as a real speech test set where the glottal flow estimates' closed phases were compared in terms of their flatness. The results based on synthetic speech indicate that the proposed method is robust to changes in f0, and state-of-the-art quality results were obtained for high-pitched voices, when f0 is in the range 330-450 Hz. The results based on real speech indicate that the proposed method produces glottal flow estimates that have flatter closed phases with less formant ripple in comparison to estimates computed with known reference methods.	acoustic cryptanalysis;coefficient;inverse filter;mathematical optimization;optimization problem;quadratic programming;ripple effect;synthetic intelligence;test set;tract (literature)	Manu Airaksinen;Tomas Bäckström;Paavo Alku	2017	IEEE/ACM Transactions on Audio, Speech, and Language Processing	10.1109/TASLP.2016.2620718	mathematical optimization;speech recognition;acoustics;computer science;speech;speech processing;mathematics;linguistics;computational model;quadratic programming	EDA	-9.316131714168783	-86.84806494772101	14346
2ae1090c16f8c0cad20530dcedc5d5bf4a56e28f	availability of necessary electronic infrastructure to support open access initiative to literature in academic libraries in delta state, nigeria	delta state;open access;electronic infrastructure;open access initiative;nigeria;academic libraries	Rationale for the study is to ascertain the state of necessary electronic infrastructure that are available in academic libraries in Delta State, Nigeria which would be used to support Open Access to Literature Initiative. The research method adopted for the study was the descriptive survey design. The instrument for data collection was the questionnaire. The population consisted of 66 Librarians (those working in dedicated IT units in the libraries). No sample was used because the population was small. The study revealed that a majority of the respondents are aware of the essential electronic infrastructure which will enable them benefit maximally from Open Access to Literature Initiative(OALI); and e-infrastructure facilities are available but in a poor state among others. The scheme is laudable considering the fact that libraries would be assisted in their collection development activities. This implies that funding bodies of libraries have much to do in term of making provision for the sustenance of OAI through adequate funding. Availability of Necessary Electronic Infrastructure to Support Open Access Initiative to Literature in Academic Libraries in Delta State, Nigeria	cyberinfrastructure;design rationale;librarian;library (computing)	Joseph Chukwusa	2014	IJDLS	10.4018/ijdls.2014010101	multimedia;knowledge management;data collection;computer science;survey research;population;collection development;sustenance	HCI	-59.53468458902092	-61.71461764518386	14348
7f55031b14d129c49fd69509b20f005737e136d4	factors impacting the label denoising of neural relation extraction		The goal of relation extraction is to obtain relational facts from plain text, which can benefit a variety of natural language processing tasks. To address the challenge of automatically labeling large-scale training data, a distant supervision strategy is introduced to relation extraction by heuristically aligning entity pairs in plain text with the knowledge base. Unfortunately, the method is vulnerable to the noisy label problem due to the incompletion of the exploited knowledge base. Existing works focus on the specific algorithms, but few works summarize the commonalities between different methods and the influencing factors of these denoising mechanisms. In this paper, we propose three main factors that impact the label denoising of distantly supervised relation extraction, including labeling assumption, prior knowledge and confidence level. In order to analyze how these factors influence the denoising effectiveness, we build a unified neural framework with word, sentence and label denoising modules for relation extraction. Then we conduct experiments to evaluate and compare these factors according to ten neural schemes. In addition, we discuss the typical cases of these factors and find that influential word-level prior knowledge and partial confidence for distantly supervised labels can significantly affect the denoising performance. These implicational findings can provide researchers with more insight of distantly supervised relation extraction.	noise reduction;relationship extraction	Tingting Sun;Chunhong Zhang;Yang Ji	2018		10.1007/978-3-030-04618-7_2	mathematical optimization;relationship extraction;noise reduction;knowledge base;machine learning;heuristic;computer science;training set;plain text;sentence;artificial intelligence	NLP	-17.920111541373117	-72.42172577049371	14388
482cc84c36053d977379f20fe153ded9fead6394	constructing a corpus-based ontology using model bias		Recent work in lexical resource construction has recognized the importance of contextualizing the knowledge in existing resources and ontologies with information derived from text corpora. This paper describes the integration of a corpus-based lexical acquisition process with a large, linguistically motivated lexical ontology. This semi-automatic bootstrapping process is used to produce refinements, additions, and modifications to the type specifications for the arguments to predicates in this ontology. In addition, the procedure is used to verify and modify the lexical extensions of the entity types	lexical analysis;ontology (information science);semiconductor industry;text corpus;type system;word sense	Anna Rumshisky;Patrick Hanks;Catherine Havasi;James Pustejovsky	2006			natural language processing;artificial intelligence;predicate (grammar);ontology;bootstrapping;ontology (information science);computer science;text corpus	AI	-28.582995574298995	-72.51939991817203	14399
0a6b9bf2779e7dc49e2cfe836da9b15ce8f0671c	resource2vec: linked data distributed representations for term discovery in automatic speech recognition		Abstract In this work we present a neural network embedding we call Resource2Vec, which is able to represent the resources that make up some Linked Data (LD) corpora. A vector representation of these resources allows more advantageous processing (in computational terms) as is the case with known word or document embeddings. We give a quantitative analysis for their study. Furthermore, we employ them in an Automatic Speech Recognition (ASR) task to demonstrate their functionality by designing a strategy for term discovery. This strategy permits out-of-vocabulary (OOV) terms in a Large Vocabulary Continuous Speech Recognition (LVCSR) system to be discovered and then put into the final transcription. First, we detect where a potential OOV term may have been uttered in the LVCSR output speech segments. Second, we carry out a candidate OOV search in some LD corpora. This search is oriented by distance measurements between the transcription context around the potential-OOV speech segment and the resources of the LD corpora in Resource2Vec format, obtaining a set of candidates. To rank them, we mainly depend on the phone transcription of that segment. Finally, we decide whether or not to incorporate a candidate into the final transcription. The results show we are able to improve the transcription in Word Error Rate (WER) terms significantly, after our strategy is used on speech in Spanish.	linked data;speech recognition	Alejandro Coucheiro-Limeres;Javier Ferreiros;Rubén San-Segundo-Hernández;Ricardo de Córdoba	2018	Expert Syst. Appl.	10.1016/j.eswa.2018.06.039	phone;linked data;artificial intelligence;machine learning;artificial neural network;speech recognition;word error rate;vocabulary;computer science	NLP	-21.550609149876358	-83.2336211788072	14401
56ab86a8fe880f7764e580317a2e80e49af692a2	reducing model complexity for dnn based large-scale audio classification		Audio classification is the task of identifying the sound categories that are associated with a given audio signal. This paper presents an investigation on large-scale audio classification based on the recently released AudioSet database. AudioSet comprises 2 millions of audio samples from YouTube, which are human-annotated with 527 sound category labels. Audio classification experiments with the balanced training set and the evaluation set of AudioSet are carried out by applying different types of neural network models. The classification performance and the model complexity of these models are compared and analyzed. While the CNN models show better performance than MLP and RNN, its model complexity is relatively high and undesirable for practical use. We propose two different strategies that aim at constructing low-dimensional embedding feature extractors and hence reducing the number of model parameters. It is shown that the simplified CNN model has only 1/22 model parameters of the original model, with only a slight degradation of performance.	elegant degradation;experiment;feature extraction;memory-level parallelism;random neural network;sound card;test set	Yuzhong Wu;Tan Lee	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8462168	task analysis;artificial neural network;machine learning;artificial intelligence;speech recognition;feature extraction;computer science;pattern recognition;audio signal;training set;embedding	Vision	-4.5885128311623955	-87.37876099965216	14409
087b291dec65710c4e93378657269371282a3da7	classifying text with statistically selected features to closely related categories	chi square max method text classification electronic document digital data document classification text categorization feature selection chi square statistics naive bayes classifier;supervised learning;bayes methods;training;text analysis;classification;text classification;naive bayes classifier;chi square statistics;accuracy;statistical analysis;feature extraction;dictionaries;classification algorithms;feature selection;digital data;text analysis bayes methods classification feature extraction statistical analysis;document classification;bayes classifier;text categorization;text categorization statistics educational institutions information filtering information filters communications technology computer science data engineering supervised learning information technology;chi square max method;electronic document	Text Classification is continuing to be one of the most researched problems due to continuously-increasing amount of electronic documents and digital data. Classifying documents to closely related categories is the most complex task in text categorization. Feature selection is an essential preprocessing step for improving the efficiency and accuracy of the text classifiers by removing redundant and irrelevant terms from the training corpus. In this paper, a novel feature selection algorithm based on chi-square statistics, have been proposed for Naïve Bayes classifier. The proposed feature selection method not only identifies the related features for a class, but also determines the type of dependency between the feature and category. The performance of the classifier with the features selected by the proposed method and the features selected by conventional chi-square max method are compared for closely related categories. Experiments were conducted with randomly chosen training documents from six closely related categories of 20Newsgroup Benchmarks. Experimental results show that the classifier has better classifying accuracy with positive features selected by the proposed method.	chi;categorization;cluster analysis;digital data;dimensionality reduction;document classification;experiment;feature selection;naive bayes classifier;preprocessor;randomness;relevance;selection algorithm	M. Janaki Meena;K. R. Chandran	2009	2009 International Conference on Advances in Recent Technologies in Communication and Computing	10.1109/ARTCom.2009.67	bayes classifier;naive bayes classifier;chi-square test;feature extraction;biological classification;computer science;machine learning;pattern recognition;data mining;accuracy and precision;supervised learning;feature selection	AI	-21.25756375963328	-63.750024640906936	14415
b0c25194ba548bfe1ff19a104bde0e086de49079	how effective are teleconsults to persuade patients of pulmonary tuberculosis of avoiding to use public transport at lima city?	software;urban areas mathematical model hospitals telemedicine market research software;market research;telemedicine;telemedicine diseases electronic messaging human factors public transport;hospitals;urban areas;mathematical model;pulmonary tuberculosis;telemedicine pulmonary tuberculosis wells riley equation;wells riley equation;human assistance teleconsults patient persuasion pulmonary tuberculosis public transport use avoidance lima city peri urban zones hospitals health centers phone calls text messages tb patient nurse visit ehealth system	We have selected 12 pulmonary Tuberculosis patients from Peri-urban zones of Lima city where all of them are still using public transport to visit hospitals and health centers. Patients were involved in the testing of a prototype of teleconsult which was based in phone calls and text messages giving as result that the number of hours/month of mobility of TB patient in Lima city using buses and minibuses was reduced in average in a 6.4±3%. However, when teleconsult is combined to an aggressive program of nurse's visit it might be possible that the number of hours/month would be reduced by a 44.7± 6 %. This result might indicate that eHealth system applied to TB patients would need of human assistance in a first phase of implementation, until patient becomes well trained in the usage of the eHealth software and related details.	prototype;terabyte	Huber Nieto-Chaupis	2016	2016 IEEE 29th International Symposium on Computer-Based Medical Systems (CBMS)	10.1109/CBMS.2016.75	market research;simulation;medicine;mathematical model;statistics	SE	-62.69174684101946	-65.64898594345547	14417
bcd619c26005726e9a006fdfa546ef26b748bb6c	change detection in xml documents for fixed structures using exclusive-or (xor)	change detection;xml document	XML is an emerging standard for the representation and exchange of Internet data. The characteristics of XML, tree-structured (i.e. a collection of nodes) and self-descriptive, facilitate the detection of changes in an XML document in minute detail and at a finer grain than obtainable at the document level. Furthermore, for a fixed schema (structure) changes may frequently happen in the content or data values of XML documents in the Web. We wish to propose a method that effectively detects this content or data value changes. Rather than inspecting all nodes between two versions of XML documents, we propose an effective algorithm, called top-down, which will detect changes in XML documents by exploring a subset of nodes in the tree. We would like to be certain that if a leaf node changes the algorithm will detect these changes, not only by inspecting the node itself, but also its parent node, grand parent node, and so on. For this, a signature for each node will be constructed which is basically an abstraction of the information stored in a node. There are several ways we can construct such a signature. We will choose exclusive-or (XOR) to construct node signatures which will prevent a user from getting irrelevant information/change and make certain that the user does not miss relevant information. Note that for the web, along with being able to access huge quantities of information, the relevancy of information/change is more important than missing of relevant information/change. For this, in this paper we propose an automatic change detection algorithm which will identify changes between two versions of an XML document based on these signatures using XOR. Our proposed algorithm will traverse the least number of nodes necessary to detect these changes. We demonstrate that our algorithm outperforms the traditional algorithm which exhaustively searches the entire space. We will also demonstrate analytically and empirically that the miss of relevant change is within tolerable range.	algorithm;antivirus software;digital signature;exclusive or;quantities of information;relevance;traverse;top-down and bottom-up design;tree (data structure);type signature;world wide web;xml	Latifur Khan;Lei Wang	2004	JDIM		xml validation;xml;computer science;operating system;data mining;xml schema;database;xml signature;world wide web;change detection;information retrieval;statistics	Web+IR	-28.784683119370047	-55.35850361446377	14426
059157907ed082ad5c4e6786266649f4b6a761cd	a speech recognition system that integrates neural nets and hmm	hidden markov model;multilayer perceptron;neural net;speech recognition;clustered data;neural network	In this paper we present a speech recognition system based on neural networks and on Hidden Markov Models. This system makes use of the discriminating properties of the multilayer perceptron, the properties of the fonotropical maps of Kohonen for clustering data and the properties for dealing with sequentiallity of the Hidden Markov Models (HMM). We also present preliminary results.	artificial neural network;hidden markov model;speech recognition	Enric Monte-Moreno;José B. Mariño	1991		10.1007/BFb0035916	neural gas;probabilistic neural network;speech recognition;computer science;recurrent neural network;machine learning;pattern recognition;time delay neural network;multilayer perceptron;artificial neural network	ML	-15.895603587791802	-87.6415395773742	14442
43cb191b39013239e88d735e29d16ab08cbcd4ba	an event-extraction approach for business analysis from online chinese news		Abstract Extracting events from business news aids users to perceive market trends, be aware of competitors’ strategies, and to make valuable investment decisions. Prior research lacks event extraction in the area of business and event based business analysis, especially in Chinese language. We propose a novel business event-extraction approach integrating patterns, machine learning models and word embedding technology in deep learning, which is applied to extract events from online Chinese news. Word embedding and a semantic lexicon are utilized to extend an event trigger dictionary with high accuracy. Then the trigger features in the dictionary are introduced into a machine learning classification algorithm to implement more refined event-type recognition. Based on a scalable pattern tree, the event type that is discovered is used to find the best-suited pattern for extracting event elements from online news. Experimental results show the effectiveness of the proposed approach. In addition, empirical studies demonstrate the practical value of extracted events, especially in finding the relationships between news events and excess returns for stock, and analyzing industry trends based on events in China.		Songqiao Han;Xiaoling Hao;Hailiang Huang	2018	Electronic Commerce Research and Applications	10.1016/j.elerap.2018.02.006	word embedding;computer science;semantic lexicon;empirical research;marketing;information retrieval;deep learning;scalability;statistical classification;investment decisions;artificial intelligence;business analysis	Web+IR	-21.81284560278293	-57.31220291925587	14461
eba98aecead7dc0093ea86eaeaf3457c65c98c21	simtools - a new paradigm in high fidelity simulation		We demonstrate a prototype set of clinical devices that deliver simulated information to the clinician when used with standardized patients or simple manikins.	manikins;patients;prototype;simple endometrial hyperplasia;simulation	Cyle Sprick;Karen J. Reynolds;H. Owen	2008	Studies in health technology and informatics			HCI	-53.824052005830396	-59.72178397210911	14476
d56fa14e39c6804e73edda589ce670b46f884467	a method of automated semantic parser generation with an application to language technology	language technology		language technology;parser	Gregers Koch	1999			universal networking language;mathematics;natural language processing;discrete mathematics;semantic computing;language technology;compiler-compiler;parsing;semantic web rule language;semantic technology;artificial intelligence	NLP	-30.643667976888516	-77.61685366272707	14484
d2dbb9662edccca40544d070eb72fa051f72e78f	speech emotion recognition and intensity estimation	hidden markov model;emotion recognition;feature vector	In this paper, a system for speech emotion analysis is presented. On a corpus of over 1700 utterances from an individual, the feature vector stream is extracted for each utterance based on short time log frequency power coefficients (LFCC). Using the feature vector streams, we trained Hidden Markov Models (HMMs) to recognize seven basic categories emotions: neutral, happiness, anger, sadness, surprise, fear. Furthermore, the intensity of the basic emotion is divided into 3 levels. And we trained 18 sub-HMMs to identify the intensity of the recognized emotions. Experiment result shows that the emotion recognition rate and the estimation of intensity performed by our system are of good and convincing quality.	coefficient;emotion recognition;ergodicity;facial recognition system;feature vector;hidden markov model;markov chain;sadness;viterbi algorithm	Mingli Song;Chun Chen;Jiajun Bu;Mingyu You	2004		10.1007/978-3-540-24768-5_43	speaker recognition;speech recognition;feature vector;feature;computer science;machine learning;pattern recognition;hidden markov model;signature recognition	NLP	-12.502630313401728	-88.41143540848294	14505
84439d293e48e20081eeb3e74c9a4234a470bc83	vision-based system for continuous arabic sign language recognition in user dependent mode	motion analysis;sign language recognition;image video processing and sign language;pattern recognition	Existing work on Arabic Sign Language recognition focuses on finger spelling and isolated gestures. In this work we extend vision-based existing solutions to recognition of continuous signing. As such we have collected and labeled the first video-based continuous Arabic Sign Language dataset. We intend to make the collected dataset available for the research community. The proposed solution extracts the motion from the video-based sentences by means of thresholding the forward prediction error between consecutive images. Such prediction errors are then transformed into the frequency domain and Zonal coded. We use Hidden Markov Models for model training and classification. The experimental results show an average word recognition rate of 94%, keeping in the mind the use of a high perplexity vocabulary and unrestrictive grammar.	american and british english spelling differences;digital signature;hidden markov model;markov chain;perplexity;thresholding (image processing);vocabulary;word error rate	Khaled Assaleh;Tamer Shanableh;M. Fanaswala;F. Amin;H. Bajaj	2008	2008 5th International Symposium on Mechatronics and Its Applications	10.4236/jilsa.2010.21003	natural language processing;speech recognition;computer science;gesture recognition	Vision	-5.882901668656009	-87.55766751721335	14523
4c053cea33b901b706fbef1b91d5a538d28cbc09	using rough sets to determine construct importance in a dynamic hci environment	workload;communication system;rough set theory;affective interface;airborne warning and control system;intelligent agent;rough set;interpersonal interaction	Rough set analysis is used as a methodology to identify the relative importance of variables for individuals who interact with various computer and other display and communication systems aboard Airborne Warning and Control Systems (AWACS). A goal of the analysis is to determine optimal information display and interpersonal interaction strategies to minimize workload and maximize coordination among team members, including intelligent agents. Rough sets analysis yielded the rank order of importance of 15 variables on three constructs (individual experience, personality, team process) for each of three different types of AWACS operators.	airborne ranger;control system;display device;human–computer interaction;intelligent agent;rough set	Michael D. Coovert;Dawn Riddle;Linda R. Elliott;Samuel G. Schiflett	2000		10.1145/633292.633375	simulation;rough set;computer science;intelligent agent	HCI	-47.52346189531174	-53.69522680453244	14537
798c1473128d7b7c861b6711cd3154465bd3c144	economic analysis of the reduction of blood transfusions during surgical procedures while continuous hemoglobin monitoring is used	cost savings;hemoglobin;photoplethysmography;real-time monitoring;spectrophotometry;transfusions	BACKGROUND Two million transfusions are performed in Spain every year. These come at a high economic price for the health system, increasing the morbidity and mortality rates. The way of obtaining the hemoglobin concentration value is via invasive and intermittent methods, the results of which take time to obtain. The drawbacks of this method mean that some transfusions are unnecessary. New continuous noninvasive hemoglobin measurement technology can save unnecessary transfusions.   METHODS A prospective study was carried out with a historical control of two homogeneous groups. The control group used the traditional hemoglobin measurement methodology. The experimental group used the new continuous hemoglobin measurement technology. The difference was analyzed by comparing the transfused units of the groups. The economic savings was calculated by multiplying the cost of a transfusion by the difference in units, taking into account measurement costs.   RESULTS The percentage of patients needing a transfusion decreased by 7.4%, and the number of transfused units per patient by 12.56%. Economic savings per patient were €20.59. At the national level, savings were estimated to be 13,500 transfusions (€1.736 million).   CONCLUSIONS Constant monitoring of the hemoglobin level significantly reduces the need for blood transfusions. By using this new measurement technology, health care facilities can significantly reduce costs and improve care quality.	blood transfusion;experiment;financial cost;hemoglobin measurement;morbidity - disease rate;patients;prospective search;quality of health care	Borja Ribed-Sánchez;Cristina González-Gaya;Sara Varea-Díaz;Carlos Corbacho-Fabregat;Jaime Pérez-Oteyza;Cristóbal Belda-Iniesta	2018		10.3390/s18051367	emergency medicine;electronic engineering;prospective cohort study;health care;hemoglobin measurement;engineering;homogeneous;hemoglobin;mortality rate	HCI	-61.050217840392676	-66.62754276773883	14549
3cbd1f0f75caee4a2dee1f2331c0c06ab636af61	effective language identification of forum texts based on statistical approaches		This investigation deals with the problem of language identification of noisy texts, which could represent the primary step of many natural language processing or information retrieval tasks. Language identification is the task of automatically identifying the language of a given text. Although there exists several methods in the literature, their performances are not so convincing in practice. In this contribution, we propose two statistical approaches: the high frequency approach and the nearest prototype approach. In the first one, 5 algorithms of language identification are proposed and implemented, namely: character based identification (CBA), word based identification (WBA), special characters based identification (SCA), sequential hybrid algorithm (HA1) and parallel hybrid algorithm (HA2). In the second one, we use 11 similarity measures combined with several types of character N-Grams. For the evaluation task, the proposed methods are tested on forum datasets containing 32 different languages. Furthermore, an experimental comparison is made between the proposed approaches and some referential language identification tools such as: LIGA, NTC, Google translate and Microsoft Word. Results show that the proposed approaches are interesting and outperform the baseline methods of language identification on forum texts. © 2015 Elsevier Ltd. All rights reserved.	baseline (configuration management);google translate;grams;hybrid algorithm;information retrieval;language identification;microsoft word for mac;n-gram;natural language processing;performance;prototype;why–because analysis	Kheireddine Abainia;Siham Ouamour;Halim Sayoud	2016	Inf. Process. Manage.	10.1016/j.ipm.2015.12.003	natural language processing;language identification;speech recognition;universal networking language;computer science;machine learning;linguistics;world wide web;information retrieval	NLP	-28.009229044019015	-68.6633667319459	14597
b95ce39dfeafbdf86dc11c377339897e52571f75	computation of word associations based on co-occurrences of words in large corpora		A statistical model is presented which predicts the strengths of word-associations from the relative frequencies of the common occurrences of words in large bodies of text. These predictions are compared with the Minnesota association norms for 100 stimulus words. The average agreement between the predicted and the observed responses is only slightly weaker than the agreement between the responses of an arbitrary subject and the responses of the other subjects. It is shown that the approach leads to equally good results for both English and German.	computation;statistical model;text corpus	Manfred Wettler;Reinhard Rapp	1993			machine learning;natural language processing;artificial intelligence;computer science;computation	NLP	-13.086388712457707	-78.99726196804279	14599
3fc228f1ef6cdc227026ddc666a922fb8cac5ff7	a logic-based approach to named-entity disambiguation in the web of data		Semantic annotation aims at linking parts of rough data (e.g., text, video, or image) to known entities in the Linked Open Data (LOD) space. When several entities could be linked to a given object, a Named-Entity Disambiguation (NED) problem must be solved. While disambiguation has been extensively studied in Natural Language Under- standing (NLU), NED is less ambitious—it does not aim to the meaning of a whole phrase, just to correctly link objects to entities—and at the same time more peculiar since the target must be LOD-entities. Inspired by semantic similarity in NLU, this paper illustrates a way to solve dis- ambiguation based on Common Subsumers of pairs of RDF resources related to entities recognized in the text. The inference process proposed for resolving ambiguities leverages on the DBpedia structured semantics. We apply it to a TV-program description enrichment use case, illustrat- ing its potential in correcting errors produced by automatic text anno- tators (such as errors in assigning entity types and entity URIs), and in extracting a description of the main topics of a text in form of common- alities shared by its entities.	entity linking;named-entity recognition;semantic web;word-sense disambiguation;world wide web	Silvia Giannini;Simona Colucci;Francesco M. Donini;Eugenio Di Sciascio	2015		10.1007/978-3-319-24309-2_28	natural language processing;computer science;data mining;information retrieval	NLP	-29.48874637962649	-66.0874675183091	14607
769e2bff50b63ef8e047fa6c2321f61a1d16079d	positive mood induction procedures for virtual environments designed for elderly people	icts;virtual reality;positive emotions;info eu repo semantics article;mood induction procedures;elderly people;e health	Positive emotions have a significant influence on mental and physical health. Their role in the elderly’s wellbeing has been established in numerous studies. It is therefore worthwhile to explore ways in which elderly people can increase the number of positive experiences in their daily lives. This paper describes two Virtual Environments (VEs) that were used as mood induction procedures (MIPs) for this population. In addition, the VEs’ efficacy at increasing joy and relaxation in elderly users is analyzed. The VEs contain exercises for generating positive-autobiographic memories, mindfulness and slow breathing rhythms. The total sample comprised 18 participants over 55 years old who used the VEs on two occasions. Twelve of them used the joy environment, while 16 used the relaxation environment. Moods before and after each session were assessed using Visual Analogical Scales. After using both VEs, results indicated significant increases in joy and relaxation and significant decreases in sadness and anxiety. The participants also indicated low levels of difficulty of use and high levels of satisfaction and sense of presence. Hence, the VEs demonstrate their usefulness at promoting positive affects and enhancing the wellbeing of	anomalous experiences;experience;linear programming relaxation;mathematical induction;sadness;virtual reality	Rosa María Baños;Ernestina Etchemendy;Diana Castilla;Azucena García-Palacios;Soledad Quero;Cristina Botella	2012	Interacting with Computers	10.1016/j.intcom.2012.04.002	computer science;artificial intelligence;virtual reality	HCI	-57.684520855864584	-54.07730889787267	14611
32acfd0bbdabffe4c0c0b24e29b17a02cd1ea54e	inspire: an integrated agent based system for hypothesis generation within cancer datasets	databases;manuals;cancer;information retrieval;multi agent support vector machine;tumours;heating;data mining;data analysis;multi agent systems;medical information systems;tumors;patient treatment;cancer data analysis testing performance evaluation project management batteries tumors biochemical analysis immune system pathology;statistical testing;tumours cancer data analysis information retrieval medical diagnostic computing medical information systems multi agent systems patient treatment statistical testing;medical diagnostic computing;breast cancer;personalised treatment integrated agent based system hypothesis generation cancer dataset tumour test pathological parameter manual data analysis procedure multiagent system statistical test automated data retrieval	Cancer research has become an extremely data rich environment, with huge batteries of tests performed to quantify and categorise tumours. Multiple analyses of biochemical, molecular and immunohistological markers on tissue samples generate large and complex data sets to compare with clinical and pathological parameters. The manual data analysis procedures by scientists have become impractical and automation is becoming the only method for complete and comprehensive analysis of the search space. To this end, a multi-agent system has been developed to automate the data analysis process. Project agents are tasked with overseeing the analysis. Initially, they create a list of hypotheses based on all parameters associated with the project. Agents then commence data collection and aggregation by directly interfacing with the various data sources. Conventional statistical tests can then be performed under agent control to determine the significance of these hypotheses. The final stage is to present collated results online. INSPIRE uses automated data retrieval and analysis on large and diverse cancer datasets to allow for the quick identification of significant results amongst the noise of the large dataset. This results in a more streamlined research process, which makes large cohort, multivariate projects easier to manage in a secure, user-friendly, web-based data management system.	categorization;data retrieval;database;multi-agent system;software agent;usability;web application	Philip Roy Quinlan;Chris Reed;Alastair Thompson	2008	2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WIIAT.2008.286	statistical hypothesis testing;computer science;bioinformatics;data science;breast cancer;machine learning;multi-agent system;data mining;data analysis;cancer	Robotics	-7.250635321664757	-61.769613991496854	14626
edd21cc9270f172eb438264288f66e67a267f98d	building evidence graph for clinical decision support		Evidence-based medicine intends to optimize clinical decision making by using evidence. Semantic query answering could help to find the most relevant evidence. However, at point of care, it still lacks time for clinicians to do literature study. In this poster, we propose to build an evidence graph for clinical decision support, in which an evidence ontology is defined with extension of SWRL rules. On top of this graph, we do evidence query and evidence fusion to generate the ranking list of decision options. Our prototype implementation of the evidence graph demonstrates its assistance to decision making, by combining a variety of knowledge-driven and data-driven decision services.	clinical decision support system;prototype;semantic web rule language;semantic query	Jing Mei;Wen Sun;Jing Li;Haifeng Liu;Xiang Li;Yiqin Yu;Guo Tong Xie	2016			clinical decision support system;data mining;decision tree;computer science;machine learning;artificial intelligence;graph	Web+IR	-52.50257350033525	-67.07332693857221	14630
4c99cc51de0cb5ffb501408eebc1630715b4b380	dnn-based speech recognition system dealing with motor state as auxiliary information of dnn for head shaking robot		In this paper, a deep neural network (DNN) based integrated background noise suppression and acoustic modeling for speech recognition proposed in which on/off state of the motor for the head shaking robot is employed as the relevant auxiliary information of the DNN input. Since the motor sound being generated when the robot is moving or shaking its head severely degrades the performance of the speech recognition accuracy, we propose to use the motor on/off state as additional information when designing the DNN-based recognition system. Our speech recognition algorithm consists of two parts including the feature mapping model for feature enhancement and the acoustic model for phoneme recognition. As for the feature mapping, the stacked DNN is designed for the precise feature enhancement such that the lower DNN and upper DNN are trained separately and combined after which the motor state is plugged into both the lower DNN and upper DNN in addition to the input noisy speech. Then, the acoustic model is trained upon the feature enhancement model in which the motor state is again used as the augmented feature. The proposed technique to suppress the acoustic and motor noises was evaluated in term of the phoneme error rate (PER) and showed a significant improvement over the conventional system.		Moa Lee;Joon-Hyuk Chang	2018	2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2018.8593396	feature extraction;artificial neural network;noise measurement;computer science;acoustic model;word error rate;mel-frequency cepstrum;background noise;speech recognition;speech enhancement	Robotics	-16.18714031081016	-89.2579375610726	14650
329b7aa5406284b49889ecb6720294ce99e32fe4	analysis of data from a longitudinal study on risk factors of atherosclerosis				Marie Tomecková;Jindra Reissigová;Karel Hrach;Jana Zvárová	2001		10.3233/978-1-60750-928-8-996	demographic economics;data analysis;psychology;longitudinal study	HCI	-56.712321989982044	-64.90158691319127	14661
34cfcf12925ea8498720ff75b60140b85906e139	best-answer selection using a machine learning tool at ntcir8 cqa pilot task			machine learning	Kazuko Kuriyama	2010			artificial intelligence;machine learning;computer science	NLP	-23.700935068771358	-61.332582628211995	14676
49c7c9e1d557c007caa5c37b97929e0eb0e636a6	application of technology: development of a replicated database of dhcp data for evaluation of druguuse	case report;pilot project;structured query language;drug therapy;drug use;relational database system;database query;veterans affairs	This case report describes development and testing of a method to extract clinical information stored in the Veterans Affairs (VA)Decentralized Hospital Computer System (DHCP) for the purpose of analyzing data about groups of patients. The authors used a microcomputer-based, structured query language (SQL)-compatible, relational database system to replicate a subset of the Nashville VA Hospital's DHCP patient database. This replicated database contained the complete current Nashville DHCP prescription, provider, patient, and drug data sets, and a subset of the laboratory data. A pilot project employed this replicated database to answer questions that might arise in drug-use evaluation, such as identification of cases of polypharmacy, suboptimal drug regimens, and inadequate laboratory monitoring of drug therapy. These database queries included as candidates for review all prescriptions for all outpatients. The queries demonstrated that specific drug-use events could be identified for any time interval represented in the replicated database.	replication (computing)	Stanley E. Graber;John A. Seneker;Archie A. Stahl;Karen O. Franklin;Thomas E. Neel;Randolph A. Miller	1996	JAMIA	10.1136/jamia.1996.96236283	pharmacotherapy;sql;relational database management system;computer science;data mining;database;world wide web	DB	-53.18806761654429	-66.05615473068335	14729
c7bfaef6c657be8910e9ff232bceeddf3fb50ddc	discovering relevant topics using dbpedia: providing non-obvious recommendations	content management;dbpedia;keyword recommendation semantic web dbpedia topic discovery;technological innovation;expert systems;keyword recommendation;real problem broadcast nonobvious recommendation open innovation based expert identification system dbpedia concept graph;internet;web sites;semantic web;electronic publishing;topic discovery;web sites content management expert systems internet recommender systems;recommender systems;internet encyclopedias electronic publishing semantics technological innovation ontologies	In this paper we propose an alternative method for generating topic suggestions to be used in an Open Innovation based expert identification system. An important requirement within this challenge is the identification of topics lateral to a given innovation problem, and use them to broaden the broadcast of the problem without compromising on relevancy. We propose an approach based on DBPedia, using which we can recommend topics on certain proximity in the DBPedia concept graph. We show the important impact of the use of suggested lateral keywords to the raised awareness about the problem in a real problem broadcast.	dbpedia;lateral thinking;open innovation;recommender system;relevance	Milan Stankovic;Werner Breitfuss;Philippe Laublet	2011	2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2011.32	the internet;content management;computer science;artificial intelligence;semantic web;data mining;electronic publishing;world wide web;information retrieval	AI	-28.215374384779604	-56.55172691887729	14730
617696b656a2289729515be99f51d6692a1c383c	ict tools for the discovery of semantic relations in legal documents	search and retrieval;semantic annotation;settore inf 01 informatica;information retrieval;col;settore ing inf 05 sistemi di elaborazione delle informazioni;semantic relations	This paper reports the experience of the development and the evaluation of a set of pre-competitive tools to support legal professionals in exploring a complex corpus of norms and documents in the legal domain. The research addresses two complementary goals: using ICT to support the simplification of the corpus of norms and using ICT to facilitate search and retrieval of information in large archives in the legal domain. The contribution of this work is in the development of tools beyond state of the art for the e-discovery of relationships between sections of norms or other legal documents. To reach the best results in terms of effectiveness, the tools combine statistical and the semantic approaches. The effectiveness of the statistical tool has been measured in terms of precision, through an assessment procedure that involved some experts of the legal domain.	archive;electronic discovery;text corpus;text simplification	Marco Bianchi;Mauro Draoli;Giorgio Gambosi;Maria Teresa Pazienza;Noemi Scarpato;Armando Stellato	2009			computer science;data science;data mining;information retrieval	Web+IR	-34.00759013912991	-67.13762026676733	14761
aeb4217c0315fcd078a7358323d71700804269f9	nlforspec: translating natural language descriptions into formal test case specifications	software testing;specification language;natural language	This paper describes the NLForSpec, a Natural Language (NL) processing tool to translate software test cases descriptions in NL into a formal representation in CSP specification language. NLForSpec is part of a larger project which aims to automate test case generation, selection and evaluation for mobile phone applications. Our tool can be used in the process of update or partially generate requirements documents from test cases (one of the project’ s main goals). The NLForSpec architecture follows the traditional pipeline NL interpretation approach, counting on a lexicon, a case grammar (to represent semantic information) and a domain ontology. The prototype was tested with a corpus of 100 test cases descriptions, obtaining a performance rate of 91%. This is an original and innovative work.	brill tagger;communicating sequential processes;experiment;formal language;intelligent user interface;lexicon;mobile app;mobile phone features;nl (complexity);natural language processing;ontology (information science);part-of-speech tagging;prototype;requirement;software development;software testing;specification language;test case;text corpus	Daniel Leitao;Dante Torres;Flávia de Almeida Barros	2007			natural language programming;formal specification;natural language;computer science;universal networking language;test case;natural language processing;specification language;programming language;object language;programming language specification;artificial intelligence	SE	-33.082966523599175	-78.12186012283466	14776
84372852886ddcb49e9adf9c5facf53e1bde9696	extracting content structure for web pages based on visual representation	document structure;extraction information;estructura de documento;web pages;red www;information extraction;recherche image;top down;information retrieval;structure document;reseau web;internet;percepcion visual;visual representation;recherche information;perception visuelle;world wide web;visual perception;recuperacion informacion;content based retrieval;extraccion informacion;langage html;recherche par contenu;html language;lenguaje html;image retrieval	A new web content structure based on visual representation is proposed in this paper. Many web applications such as information retrieval, information extraction and automatic page adaptation can benefit from this structure. This paper presents an automatic top-down, tag-tree independent approach to detect web content structure. It simulates how a user understands web layout structure based on his visual perception. Comparing to other existing techniques, our approach is independent to underlying documentation representation such as HTML and works well even when the HTML structure is far different from layout structure. Experiments show satisfactory results.	algorithm;digital distribution;document object model;documentation;html;information extraction;information retrieval;mobile device;scalability;top-down and bottom-up design;web application;web content;web page	Deng Cai;Shipeng Yu;Ji-Rong Wen;Wei-Ying Ma	2003		10.1007/3-540-36901-5_42	web modeling;the internet;web mapping;html;web design;visual perception;image retrieval;computer science;document structure description;top-down and bottom-up design;web page;database;world wide web;website parse template;information extraction;information retrieval	Web+IR	-35.86317353488834	-58.49608669269036	14793
98bbbd4728e91bafc5fe39c4f758b59023691966	unsupervised training with directed manual transcription for recognising mandarin broadcast audio	broadcast news;character error rate;maximum likelihood;indexing terms;broadcast conversation;speech recognition;discriminative training;random set	The performance of unsupervised discriminative training has been found to be highly dependent on the accuracy of the initial automatic transcription. This paper examines a strategy where a relatively small amount of poorly recognised data are manually transcribed to supplement the automatically transcribed data. Experiments were carried out on a Mandarin broadcast transcription task using both Broadcast News (BN) and Broadcast Conversation (BC) data. A range of experimental conditions are compared for both maximum likelihood and discriminative training using directed manual transcription. For BC data, using fully unsupervised discriminative training, only 17% of the reduction in character error rate (CER) from supervised training is obtained. By automatically selecting 18% of the data for manual transcription yields 50% of the CER gain from supervised training. The directed approach to selecting data outperforms the use of a random set of data for manual transcription.	discriminative model;super robot monkey team hyperforce go!;transcription (software);unsupervised learning	Kai Yu;Mark J. F. Gales;Philip C. Woodland	2007			speech recognition;index term;computer science;pattern recognition;maximum likelihood;statistics	NLP	-20.147912010015173	-85.9782557202973	14797
8e76eb7fe778cbb7230b6ca00fa3955c85d561ff	disambiguating dynamic sentiment ambiguous adjectives	challenging task;web data;sentiment expectation;previous work;dynamic sentiment;ambiguous adjective;document level;semantic orientation;knowledge-based method;sentiment analysis;sentiment classification	Dynamic sentiment ambiguous adjectives (DSAAs) like “large, small, high, low” pose a challenging task on sentiment analysis. This paper proposes a knowledge-based method to automatically determine the semantic orientation of DSAAs within context. The task is reduced to sentiment classification of target nouns, which we refer to sentiment expectation instead of semantic orientation widely used in previous researches. We mine the Web using lexico-syntactic patterns to infer sentiment expectation of nouns, and then exploit character-sentiment model to reduce noises caused by the Web data. At sentence level, our method achieves promising result with an f-score of 78.52% that is substantially better than baselines. At document level, our method outperforms previous work in sentiment classification of product reviews.	baseline (configuration management);f1 score;lexico;sentiment analysis;world wide web	Yunfang Wu;Miaomiao Wen	2010			natural language processing;computer science;pattern recognition;data mining;sentiment analysis	NLP	-25.626514545595963	-69.4841316363379	14821
296a029461a0f9c5aa307a29a51fa9366b8f481d	yamraj: binary-class and multi-class based textual entailment system for japanese (ja) and chinese simplified (cs)		The article presents the experiments carried out as part of the participation in Recognizing Inference in TExt and Validation (RITE-VAL) 1 at NTCIR-11 for Japanese. RITE-VAL has two subtasks i.e. Fact Validation and System Validation subtask for Chinese-Simplified (CS), ChineseTraditional (CT), English (EN), and Japanese (JA) and semantic relation between two texts such as entailment, contradiction, and independence. We have submitted run for Japanese (JA) System Validation (one run BC and one for MC), Chinese Simplified (CS) System Validation (one run). The Textual Entailment system used the web based Google translator system 2 for Machine Translation purpose. The system is based on Support Vector Machine that uses features from lexical similarity, lexical distance, and syntactic similarity.	experiment;google translate;machine translation;ontology components;semantic similarity;support vector machine;textual entailment;variable assembly language	Partha Pakray	2014			natural language processing;speech recognition;computer science;linguistics	NLP	-23.37712812654152	-70.8233986333476	14822
6664b4616b545388da228c839dd0502d89d76ebd	a sensing platform for physiological and contextual feedback to tennis athletes	biomedical monitoring;tennis;sensor phenomena and characterization;frequency modulation;multimedia;body sensor networks;image processing;contextual feedback;sensors;feedback biomedical monitoring multimodal sensors digital cameras performance analysis body sensor networks sensor phenomena and characterization streaming media games content based retrieval;information retrieval;data stream;training;wearable computers;multimodal sensing platform;multimodal sensors;data mining;digital cameras;wearable sensing technology;feedback;sports;heart rate;streaming media;games;physiological feedback;performance analysis;video camera network;visual sensing;content management system;tennis athletes;visual sensing physiological feedback contextual feedback tennis athletes multimodal sensing platform video camera network wearable sensing technology content based retrieval;information need;camera network;multimedia tennis sensors content management system sports;content based retrieval;medical signal processing;wearable computers cameras content based retrieval medical signal processing;cameras	In this paper we describe our work on creating a multimodal sensing platform for providing feedback to tennis coaches and players. The platform includes a fixed installation around a tennis court consisting of a video camera network and a localisation system as well as wearable sensing technology deployed to individual athletes. We describe the various components of this platform and explain how we can capture synchronised multi-modal sensor data streams for games or training sessions. We then describe the content-based retrieval system we are building to facilitate the development of novel coaching tools. We provide some examples of the queries that the system can support, where these queries are chosen to be suitably expressive so as to reflect a coach’s complex information needs regarding tennis-related performance factors.	feedback;information needs;modal logic;motion capture;multimodal interaction;radio frequency;wearable computer;while	Damien Connaghan;Sarah Hughes;Gregory C. May;Philip Kelly;Ciarán Ó Conaire;Noel E. O'Connor;Donal O'Gorman;Alan F. Smeaton;Niall Moyna	2009	2009 Sixth International Workshop on Wearable and Implantable Body Sensor Networks	10.1109/BSN.2009.63	frequency modulation;games;information needs;computer vision;simulation;wearable computer;telecommunications;image processing;computer science;sensor;feedback;multimedia	HCI	-8.907518907813426	-63.834918272795186	14900
25babf5ef0d4eb3f09f933c9a7f46d2f8e296306	processing of sentences with intra-sentential code-switching	intra-sentential code-switching;closed class item;language systems systematically interact;computational framework;certain bilingual community;grammatical system	Speakers of certain bilingual communities systematically produce utterances in whichthey switch from one language to another, suggesting that the two language systems syst~matically interact with each other in the production (and reoognitlon) of these sentences. We have investigated this phenomenon in a formal or computational framework which consists of two gramnatical systems and a mechanism for switching between the two systems. A variety of constraints apparent in these sentences are then explained in terms of constraints on the switching mechanism, especially, those on closed class items.	computation;mathematical model	Aravind K. Joshi	1982			natural language processing;computer science;linguistics	AI	-32.454399645452284	-81.9857378141623	14964
a46ad94ea704141398f6b9b4c3e486c4b3ff68da	auditory cortical representations of speech signals for phoneme classification	front end;primary auditory cortex;signal analysis;independent component analysis;mel frequency cepstral coefficient;feature extraction;signal representation;time use;receptive field;sparse representation	The use of biologically inspired, feature extraction methods has improved the performance of artificial systems that try to emulate some aspect of human communication. Recent techniques, such as independent component analysis and sparse representations, have made it possible to undertake speech signal analysis using features similar to the ones found experimentally at the primary auditory cortex level. In this work, a new type of speech signal representation, based on the spectrotemporal receptive fields, is presented, and a problem of phoneme classification is tackled for the first time using this representation. The results obtained are compared, and found to greatly improve both an early auditory representation and the classical front-end based on Mel frequency cepstral coefficients.	additive white gaussian noise;ar (unix);architecture of windows nt;bibliothèque de l'école des chartes;coefficient;dictionary;digraphs and trigraphs;experiment;feature extraction;independent component analysis;mel-frequency cepstrum;multilayer perceptron;nl (complexity);neuro-fuzzy;numerical aperture;pict;sensor;signal processing;sparse matrix;spectrogram;stationary process;symantec endpoint protection;utility functions on indivisible goods	Hugo Leonardo Rufiner;César E. Martínez;Diego H. Milone;John C. Goddard	2007		10.1007/978-3-540-76631-5_96	independent component analysis;speech recognition;feature extraction;computer science;front and back ends;machine learning;signal processing;pattern recognition;sparse approximation;receptive field	ML	-10.96542350710553	-88.35978048529674	14965
5d18925187b8291246e991121bee266a946f70a6	the ehu systems for the nist 2011 language recognition evaluation		This paper describes the systems developed by the Software Technologies Working Group of the University of the Basque Country (EHU) for the NIST 2011 Language Recognition Evaluation (LRE). One primary and three contrastive systems were submitted, all of them fusing five component subsystems: a Linearized Eigenchannel GMM (LE-GMM) subsystem, an iVector subsystem and three phone-lattice-SVM subsystems based on the publicly available BUT decoders for Czech, Hungarian an Russian. The four submitted systems were identical except for the backend approach and the development dataset used to estimate the backend and fusion parameters. Multiclass discriminative fusion was performed separately for each nominal duration. A development set was defined, including the evaluation sets of LRE07 and LRE09 and the development data provided by NIST for 9 additional languages in the 2011 campaign. The official results, which were among the best submitted to the evaluation, are presented and briefly discussed. Post-key analyses are also addressed in the paper, including the performance attained by component subsystems and a study of their contribution to fusion performance by means of a greedy selection procedure.	cross-validation (statistics);google map maker;greedy algorithm;language identification;oracle fusion architecture	Mikel Peñagarikano;Amparo Varona;Luis Javier Rodríguez-Fuentes;Mireia Díez;Germán Bordel	2012			discriminative model;speech recognition;software;nist;computer science;czech	SE	-19.261162079013207	-79.58000486758426	14968
b8f1608a6dfe5267f19ef89ac7929dc23a8c9849	portraits of individuals with dementia: views of care managers	multimedia;care aid;dementia;usability	Dementia is a growing concern worldwide. In Scotland alone 1.32% of the population have dementia and about 40% of these people live in a care home or hospital (Care Commission and Mental Welfare Commission, 2009). Dementia is an umbrella term for a persistent impairment in multiple areas of intellectual function including attention, orientation, memory, judgment, language, motor and spatial skills (Mendez and Cummings, 2003). Dementia, although typically associated with aging, can occur to anyone at any age. The most common form of dementia is Alzheimer‘s disease but there are over 100 different types of dementia each with unique causes and symptoms. No two people with dementia will have the same experience of the disease with life history, relationships, personality and environment a stronger factor than the dementia (Chapman, et al., 2001).	population;umbrella term	Gemma Webster;Deborah I. Fels;Gary Gowans;Vicki L. Hanson	2011			medicine;nursing;social psychology;clinical psychology	HCI	-57.25390943851556	-56.01536120826657	14973
ba804aff340943dce821b3e0f7168f55e45eb2e4	s4d: speaker diarization toolkit in python		In this paper, we present S4D, a new open-source Python toolkit dedicated to speaker diarization. S4D provides various state-ofthe-art components and the possibility to easily develop end-toend diarization prototype systems. S4D offers a large panel of clustering, segmentation, scoring and visualization algorithms. S4D has been thought to be easily understood, installed, modified and used in order to allow fast transfers of diarization technologies to industry and facilitate development of new approaches. Examples, benchmarks on standard tasks and tutorials are provided in this paper. S4D is an extension of the opensource toolkit for speaker recognition: SIDEKIT.	algorithm;benchmark (computing);cluster analysis;open-source software;pc speaker;prototype;python;speaker diarisation;speaker recognition	Pierre-Alexandre Broux;Florent Desnous;Anthony Larcher;Simon Petit-Renaud;Jean Carrive;Sylvain Meignier	2018		10.21437/Interspeech.2018-1232	speech recognition;visualization;cluster analysis;speaker recognition;speaker diarisation;python (programming language);computer science;segmentation	HCI	-24.06109160430436	-86.14167280360468	14977
41e1156b71a42d0fa3dc4f2555ce0108874a6539	the relevance of text and speech features in automatic non-native english accent identification		This paper describes our experiments with automatically identifying native accents from speech samples of non-native English speakers using low level audio features, and n-gram features from manual transcriptions. Using a publicly available non-native speech corpus and simple audio feature representations that do not perform word/phoneme recognition, we show that it is possible to achieve close to 90% classification accuracy for this task. While character n-grams perform similar to speech features, we show that speech features are not affected by prompt variation, whereas ngrams are. Since the approach followed can be easily adapted to any language provided we have enough training data, we believe these results will provide useful insights for the development of accent recognition systems and for the study of accents in the context of language learning.		Sowmya Vajjala;Ziwei Zhou	2018	CoRR		artificial intelligence;language acquisition;natural language processing;speech corpus;transcription (linguistics);computer science;training set	NLP	-18.82123567545096	-82.89937564105018	14995
ad3e3b92f88607a62d6d8b2491ef41ceb8f018ff	how practical is it? machine learning for identifying conceptual interoperability constraints in api documents		Building meaningful interoperation with external software units requires performing the conceptual interoperability analysis that starts with identifying the conceptual interoperability constraints of each software unit, then it compares the systems’ constraints to detect their conceptual mismatch. We call the conceptual interoperability constraints (the COINs) that can be of different types including structure, dynamic, and quality. Missing such constraints may lead to unexpected mismatches, expensive resolution, and running-late projects. However, it is a challenging task for software architects and analysts to manually analyze the unstructured text in API documents to identify the COINs. Not only it is a tedious and time-consuming task, but also it needs knowledge about the constraint types. In this article, we present and evaluate our idea of utilizing machine learning techniques in automating the COIN identification, which is the first step of conceptual interoperability analysis, from human text in API documents. Our empirical research started with a multiple-case study to build the ground truth dataset, on which we contributed our machine learning COINClassification Model. We show the model’s robustness through experiments using different machine learning text-classification algorithms. The experiments’ results revealed that our model can achieve up to 87% accuracy in automatically identifying the COINs in text. Thus, we implemented a tool that embeds our model to demonstrate its practical value in industrial context. Then, we evaluated the practitioners’ acceptance for the tool and found that they significantly agreed on its usefulness and ease of use.		Hadil Abukwaik;Mohammed Abufouda;T. R. Gopalakrishnan Nair;H. Dieter Rombach	2018	CoRR			SE	-31.120701161612047	-71.53142481481625	15011
c03e39354020ff0b1d773c9c22c5b9829fbd3a94	refinement by filtering translation candidates and similarity based approach to expand emotion tagged corpus		Researches on emotion estimation from text mostly use machine learning method. Because machine learning requires a large amount of example corpora, how to acquire high quality training data has been discussed as one of its major problems. The existing language resources include emotion corpora; however, they are not available if the language is different. Constructing bilingual corpus manually is also financially difficult. We propose a method to convert a training data into different language using an existing Japanese-English parallel emotion corpus. With a bilingual dictionary, the translation candidates are extracted against every word of each sentence included in the corpus. Then the extracted translation candidates are narrowed down into a set of words that highly contribute to emotion estimation and we used the set of words as training data. Moreover, when one language’s unannotated linguistic resources can be obtained, the words can be expanded based on the word distributed expression. By using this expressions, we can improve accuracy without decreasing information volume of one sentence. Then, we tried the corpus expansion without translating target linguistic resource. As the result of the evaluation experiment using the machine learning algorithm, we could clear the effectiveness of the emotion corpus which expanded based on the original language’s unannotated sentences and based on similar sentence. Moreover, when large amount of linguistic resources without annotation can be obtained in one language, their words can be expanded based on distributed expressions of the words. By using distributed expressions, we can improve accuracy without decreasing information volume of one sentence. Then, we attempted to expand corpus without translating target linguistic resource. The result of the evaluation experiment using the machine learning algorithm showed the effectiveness of the expanded emotion corpus based on the original language’s unannotated sentences and their similar sentences.	refinement (computing)	Kazuyuki Matsumoto;Fuji Ren;Minoru Yoshida;Kenji Kita	2015		10.1007/978-3-319-52758-1_15	natural language processing;speech recognition;computer science;information retrieval	NLP	-24.404070984522857	-75.95035328314434	15024
64dcc5eb4acb8acba68b2eed36a94f5b5c5141da	in-picture search algorithm for content-based image retrieval	database indexing;database indexing content based retrieval image retrieval visual databases database theory;search algorithm;image database;multimedia data;image retrieval content based retrieval image databases multimedia databases indexes data engineering information retrieval filters indexing computer science;retrieval model;content based image retrieval;search quality content based image retrieval in picture search algorithm fragments generalized virtual node search algorithm data fragments image database picture object hierarchical multidimensional structure k tree unified retrieval model multimedia data;content based retrieval;database theory;visual databases;image retrieval	Researchers are currently more interested in searching for fragments that are similar to a query, than a total data item that is similar to a query; the search interest is for “contains”, not “is”. This paper presents an ) (logn O algorithm, called the “generalized virtual node (GVN)” algorithm; the GVN algorithm is a search algorithm for data fragments that have similar contents to that of a query. An example of the use of the GVN algorithm is in the search of an image database for a certain picture object regardless to what their picture backgrounds are. Each image is transformed into characteristic features and these features are stored in a hierarchical multidimensional structure, called a “k-tree.” The k-tree is exploited to build a unified retrieval model for any types of multimedia data. The experimental results of this “in-picture” search algorithm on an image database demonstrate a search quality is qualitatively and quantitatively acceptable, with a retrieval time is faster than other algorithms, such as brute-force and Partial Matching.	content-based image retrieval;data item;global value numbering;information retrieval;k-tree;search algorithm	Punpiti Piamsa-nga;Nikitas A. Alexandridis;Sanan Srakaew;George C. Blankenship;S. R. Subramanya	1999		10.1109/ICIP.1999.821580	document retrieval;database index;database theory;query expansion;visual word;image retrieval;computer science;concept search;data mining;database;automatic image annotation;data retrieval;information retrieval;search algorithm;human–computer information retrieval	DB	-11.727169933359223	-57.99734755429781	15034
a52ec435c1dbf00067572b9c08c57744ee09e07d	a recommendation method based on semantic similarity and complementarity using weighted taxonomy: a case on construction materials dataset		Products and web pages are the main components of the e-commerce data knowledge and the relationship among them is an important issue to be highly considered in recommender systems. This study aims to focus on the similarity and complementarity relationships among the products that have wide applications in the recommender systems. In the previously proposed methods, products and their relationships were revealed using taxonomy and “IS-A” relationship. In addition, the similarity and complementarity calculations were conducted based on edge computation by assigning a similar degree to any edge. More specifically, the children of a concept in the taxonomy was supported by a similar father’s “IS-A” degree. In contrast, this study provides a new approach based on ontology, data mining, and automatic discovering algorithms for the relationships with different degrees for the edges among the concepts. Accordingly, these relationships are initialised according to the “IS-A” degree. With regard to this weighted ...	complementarity theory;semantic similarity	Karamollah Bagherifard;Mohsen Rahmani;Vahid Rafe;Mehrbakhsh Nilashi	2018	JIKM	10.1142/S0219649218500107	recommender system;web page;complementarity (molecular biology);taxonomy (biology);management science;ontology;data mining;semantic similarity;computation;computer science	NLP	-28.269918427251024	-58.93113357837991	15044
42bbcd6b890965a00f337fa805c98be0a5c83550	syntagmatic, paradigmatic, and automatic n-gram approaches to assessing essay quality	collocations;n grams;lexical bundles;writing quality	Computational indices related to n-gram production were developed in order to assess the potential for n-gram indices to predict human scores of essay quality. A regression analyses was conducted on a corpus of 313 argumentative essays. The analyses demonstrated that a variety of n-gram indices were highly correlated to essay quality, but were also highly correlated to the number of words in the text (although many of the n-gram indices were stronger predictors of writing quality than the number of words in a text). A second regression analysis was conducted on a corpus of 88 argumentative essays that were controlled for text length differences. This analysis demonstrated that n-gram indices were still strong predictors of essay quality when text length was not a factor.	n-gram	Scott A. Crossley;Zhiqiang Cai;Danielle S. McNamara	2012			n-gram;natural language processing;speech recognition;computer science	NLP	-16.96246523980543	-80.63835071751774	15069
d495c81c7a09c5948035d905969e636cc63b535c	analyzing genre of organizational communication in clinical information systems	interpersonal communication;information systems;electronic patient record;social interaction;organizational communication;clinical practice;clinical information system;information system;medical informatic;record keeping;health care	Proposes using the analytic concept of genre of organizational communication to study the organizational consequences of implementing clinical information systems and shifting from paper‐based to electronic patient records in clinical practices. By focusing research attention on interpersonal communication and social interaction issues not addressed in medical informatics research, this approach contributes to the understanding of organizational and institutional issues that implementing such systems may entail. The paper develops an example drawn from an in‐depth case study of a computerized order entry system to illustrate the insights this approach may provide.	information system	Elizabeth J. Davidson	2000	IT & People	10.1108/09593840010377635	knowledge management;organizational communication;management science;information system;interpersonal communication	HCI	-61.33541898653656	-61.64739719783655	15080
feca1d04f3c0a58e383e239548adf12a8a402299	unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition		This paper describes a novel character tagging approach to Chinese word segmentation and named entity recognition (NER) for our participation in Bakeoff-4.1 It integrates unsupervised segmentation and conditional random fields (CRFs) learning successfully, using similar character tags and feature templates for both word segmentation and NER. It ranks at the top in all closed tests of word segmentation and gives promising results for all closed and open NER tasks in the Bakeoff. Tag set selection and unsupervised segmentation play a critical role in this success.	conditional random field;machine learning;named-entity recognition;preprocessor;supervised learning;text segmentation;unsupervised learning	Hai Zhao;Chunyu Kit	2008			natural language processing;supervised learning;artificial intelligence;crfs;pattern recognition;computer science;unsupervised learning;segmentation-based object categorization;machine learning;named-entity recognition;conditional random field;text segmentation;segmentation	NLP	-22.964464936620374	-71.20927645064975	15084
61aef52a6a099983e982dd8adb6573454a89ce9a	a web application for reading and attentional assessments		This work aims to design and develop a web application for reading and attentional evaluation in schooling children using a standardized tests battery. The application allows the automatic and immediate calculation of child's scores by inserting times and errors committed by the subject. The obtained scores are needed to establish whether the participant's performance is in deficit or not (i.e., Z-score).		Sara Bertoni;Andrea Facoetti;Sandro Franceschini;Claudio E. Palazzi;Daniele Ronzani	2018		10.1145/3284869.3284896	standardized test;battery (electricity);web application;applied psychology;computer science	HCI	-58.00332242400783	-55.844554639840446	15092
161a470c1eb3326ab8c03d0d7311e44e92a7cf16	named entity mining from click-through data using weakly supervised latent dirichlet allocation	data collection;log mining;search log mining;latent dirichlet allocation;web search engine;large scale;recommender system;named entity recognition;keyword search;online advertising;web mining;web search;named entity;topic model	This paper addresses Named Entity Mining (NEM), in which we mine knowledge about named entities such as movies, games, and books from a huge amount of data. NEM is potentially useful in many applications including web search, online advertisement, and recommender system. There are three challenges for the task: finding suitable data source, coping with the ambiguities of named entity classes, and incorporating necessary human supervision into the mining process. This paper proposes conducting NEM by using click-through data collected at a web search engine, employing a topic model that generates the click-through data, and learning the topic model by weak supervision from humans. Specifically, it characterizes each named entity by its associated queries and URLs in the click-through data. It uses the topic model to resolve ambiguities of named entity classes by representing the classes as topics. It employs a method, referred to as Weakly Supervised Latent Dirichlet Allocation (WS-LDA), to accurately learn the topic model with partially labeled named entities. Experiments on a large scale click-through data containing over 1.5 billion query-URL pairs show that the proposed approach can conduct very accurate NEM and significantly outperforms the baseline.	baseline (configuration management);book;latent dirichlet allocation;nem (cryptocurrency);named entity;online advertising;recommender system;topic model;ws-security;web search engine	Gu Xu;Shuang-Hong Yang;Hang Li	2009		10.1145/1557019.1557165	latent dirichlet allocation;web mining;online advertising;web search engine;computer science;machine learning;data mining;topic model;world wide web;information retrieval;recommender system;data collection	ML	-23.45643098073147	-53.04255830255028	15093
2555b4598aa6e5153df5caf4a13a05274bf583a1	unit selection for speech synthesis using splicing costs with weighted finite state transducers	speech synthesis	In this paper we describe how unit selection for concatenative speech synthesis can be implemented efficiently for sub-phonetic units using weighted finite state transducers (WFST). We also introduce splicing costs as a measure to indicate which unit boundaries are particularly good or poor joint points. Splicing costs extend the flexibility offered by the unit selection paradigm. Through a perceptual experiment we demonstrate an improvement in speech quality achieved by using splicing costs during unit selection.	finite-state transducer;programming paradigm;speech synthesis	Ivan Bulyko;Mari Ostendorf	2001			artificial intelligence;speech recognition;rna splicing;pattern recognition;computer science;speech synthesis;transducer	NLP	-18.167966993775796	-86.52954707543749	15097
e40a83b8df8ff08d816eb392ebd67acde5df1d27	biomedical informatics research network: integrating multi-site neuroimaging data acquisition, data sharing and brain morphometric processing	clinical data;data sharing;biomedical informatics neuroimaging data acquisition software testing biomedical imaging data analysis collaboration information technology technological innovation image analysis;brain;image processing;software integration;information technology;national institute of health;diseases medical computing data acquisition brain neurophysiology cognition;medical computing;large scale;biomedical informatics research network;cognition;diseases;neurophysiology;mild cognitive impairment;distributed collaboration;data acquisition;population data;multisite image processing biomedical informatics research network multisite neuroimaging data acquisition data sharing brain morphometric processing national institutes of health structural imaging data alzheimer s disease depression mild cognitive impairment multisite image calibration software integration	The Biomedical Informatics Research Network (BIRN) is a National Institutes of Health (USA) initiative that fosters distributed collaborations in biomedical science by utilizing information technology innovations. Morphometry BIRN is one of its testbeds and has the goal to develop the ability to conduct clinical imaging studies across multiple sites, to analyze structural imaging data with the most powerful software regardless of development site, and to test new hypotheses on large collections of subjects with well-characterized image and clinical data. Through large-scale analyses of patient population data acquired and pooled across sites, we are investigating neuroanatomic correlates of Alzheimer's Disease Depression and Mild Cognitive Impairment subjects. This paper describes progress in multi-site image calibration and in software integration for multi-site image processing.	biomedical informatics research network;data acquisition;medical imaging;morphometrics;system integration	Jorge Jovicich;Mirza Faisal Beg;Steven D. Pieper;Carey E. Priebe;Michael I. Miller;Randy L. Buckner;Bruce R. Rosen	2005	18th IEEE Symposium on Computer-Based Medical Systems (CBMS'05)	10.1109/CBMS.2005.38	cognition;image processing;computer science;data science;data mining;data acquisition;law;information technology;neurophysiology;system integration	ML	-51.63064146887397	-63.37058830332254	15098
070be29629c7158ed5fbdf337c4a7702efcf3d67	learning an artificial f0-contour for alt speech.		The Artificial Larynx Transducer (ALT) as a possibility to re-obtain audible speech for people who had to undergo a total laryngectomy has been known for decades. Not only the design and underlying technique but also the poor speech quality and intelligibility have not improved until now. In a world where technology rules everyday life, it is necessary to use the known technology to improve the quality of life for handicapped people. One reason for the lack of naturalness is the constant vibration of the ALT. A method to substantially improve ALT speech is to introduce a varying fundamental frequency ( F0) contour. In this paper we present a new method to automatically learn an artificial F0-contour. The model used is a Gaussian mixture model (GMM) which is trained with a database containing speech of ALT users as well as healthy people. Informal listening tests suggest that this approach is a first step for a subsequent overall enhancement technique for speech produced by an ALT.	google map maker;intelligibility (philosophy);mixture model;the quality of life;transducer	Anna Katharina Fuchs;Martin Hagmüller	2012			speech recognition	HCI	-10.921213256674738	-85.45817791660967	15111
346feec092a53f96877bb7d15f1be243a6d29c3d	computing inter-document similarity with context semantic analysis		Abstract We propose a novel knowledge-based technique for inter-document similarity computation, called Context Semantic Analysis (CSA). Several specialized approaches built on top of specific knowledge base (e.g. Wikipedia) exist in literature, but CSA differs from them because it is designed to be portable to any RDF knowledge base. In fact, our technique relies on a generic RDF knowledge base (e.g. DBpedia and Wikidata) to extract from it a Semantic Context Vector , a novel model for representing the context of a document, which is exploited by CSA to compute inter-document similarity effectively. Moreover, we show how CSA can be effectively applied in the Information Retrieval domain. Experimental results show that: (i) for the general task of inter-document similarity, CSA outperforms baselines built on top of traditional methods, and achieves a performance similar to the ones built on top of specific knowledge bases; (ii) for Information Retrieval tasks, enriching documents with context (i.e., employing the Semantic Context Vector model) improves the results quality of the state-of-the-art technique that employs such similar semantic enrichment .		Fabio Benedetti;Domenico Beneventano;Sonia Bergamaschi;Giovanni Simonini	2019	Inf. Syst.	10.1016/j.is.2018.02.009	data mining;rdf;computer science;computation;knowledge base	DB	-31.59343864485141	-66.0909645209732	15116
01954729c5305cdd7cda94c30c0ab470482b5b7f	role of verbs in document analysis	document analysis;noun;information technology;computer science;document classification	We present results of two methods for assessing the event profile of news articles as a function of verb type. The unique contribution of this research is the focus on the role of verbs, rather than nouns. Two algorithms are presented and evaluated, one of which is shown to accurately discriminate documents by type and semantic properties, i.e. the event profile. The initial method, using WordNet (Miller et al. 1990), produced multiple cross-classification of articles, primarily due to the bushy nature of the verb tree coupled with the sense disambiguation problem. Our second approach using English Verb Classes and Alternations (EVCA) Levin (1993) showed that monosemous categorization of the frequent verbs in WSJ made it possible to usefully discriminate documents. For example, our results show that articles in which communication verbs predominate tend to be opinion pieces, whereas articles with a high percentage of agreement verbs tend to be about mergers or legal cases. An evaluation is performed on the results using Kendall’s τ . We present convincing evidence for using verb semantic classes as a discriminant in document classification.	algorithm;automatic summarization;categorization;contingency table;discriminant;document classification;information extraction;the wall street journal;word-sense disambiguation;wordnet	Judith L. Klavans;Min-Yen Kan	1998		10.3115/980845.980959	natural language processing;noun;speech recognition;modal verb;computer science;linguistics;information technology	NLP	-27.344412043542643	-74.55427805278454	15120
c3bcb76d6384854bc37672909d068a4d20acf359	an examination of the impact of stimuli type and gss structure on creativity: brainstorming versus non-brainstorming techniques in a gss environment	analogy assumption reversal brainstorming creativity group support system idea generation idea quality idea quantity laboratory experiment;creativity;group support system;gss;reversal brainstorming;brainstorming;endnotes;idea quantity;support system;creativity group;analogy;electronic meeting systems;idea quality;laboratory experiment;pubications;assumption reversal;analogy assumption;idea generation;creative methods	Of the techniques available for idea generation with group support systems (GSS), little research attention has been given to techniques that challenge problem assumptions or that use unrelated stimuli to promote creativity. When implementing such techniques with GSS, choices must be made regarding how to configure the GSS to deploy the initial creative stimuli and to present the pool of emerging ideas that act as additional stimuli. This paper reports the results of an experiment that compares Electronic Brainstorming (few unnamed rotating dialogues) with Assumption Reversals (many related stimuli, many named dialogues, free movement among dialogues) and Analogies (many unrelated stimuli, many named dialogues, free movement among dialogues). Analogies produced creative, but fewer, ideas, due to the use of unrelated stimuli. Assumption Reversals produced the most, but less creative, ideas, possibly due to fragmentation of the group memory and cognitive inertia caused by lack of forced movement among dialogues.		Jillian M. Hender;Douglas L. Dean;Thomas Lee Rodgers;Jay F. Nunamaker	2002	J. of Management Information Systems		analogy;brainstorming;ideation;computer science;artificial intelligence;creativity;management;social psychology	DB	-51.56573593967259	-53.73784009407744	15128
0cf5934b406e7ae56b0cbf07ea08d8eac3d904c5	a two-level schema for detecting recognition errors	language model;support vector machine;error correction;speech recognition	This paper proposes a two-level schema for the automatic detection of possible errors in speech recognition hypotheses. Given the recognition hypothesis of an utterance, the first level in our schema applies an uttera nc classifier (UC) to decide if the hypothesis is error-fre e or erroneous. In the latter case, the utterance is passed on to the second level in our schema for further processing. A word classifier (WC) is applied to each of the word hypotheses in the utterance to decide whether or not it is a misrecognition. Hence the two-level schema can locate error-containing regions in the recognition hypotheses. These are the target regions to which we can apply more sophisticated and expensive language models for error correction as a next step. We have developed UC and WC based on Support Vector Machines (SVM). Experiments on Mandarin Chinese speech recognition using the Speech-Lab-In-A-Box corpora showed that the UC has a detection error rate of 16.5% for misrecognized utterances; the WC has a detection error rate of 19.8% for erroneous word hypotheses; and the overall two-level schema can catch 44.5% of the erroneous word hypotheses.	database schema;error detection and correction;language model;microsoft word for mac;sensor;speech recognition;super robot monkey team hyperforce go!;support vector machine;tag (game);text corpus;uc browser;writing commons	Zhengyu Zhou;Helen M. Meng	2004			word error rate;artificial intelligence;speech recognition;error detection and correction;support vector machine;pattern recognition;computer science;language model;utterance;classifier (linguistics);schema (psychology)	NLP	-19.392316212654958	-81.53129812604605	15135
0b07f533482acfcdef6859e95ab3eb796ee48ff0	latent information mining for semi-supervised sentiment classification in catering reviews	support vector machines;training;data mining;feature extraction;robustness;semisupervised learning;labeling	In this paper, we aim to address the issue that semi-supervised learning is prone to be influenced by the quality and quantity of initial seeds. In order to expand the initial labeled data, we select credible samples from unlabeled data by a proposed bilateral latent information miner. The miner can extract information from unlabeled data for both positive and negative class respectively. Then we train a semi-supervised sentiment learner with the expanded labeled data. Experiments show that the proposed method can achieve a better and more robust performance than other methods, especially when initial labeled data is randomly selected in a small size.	algorithm;bilateral filter;data mining;experiment;randomness;semi-supervised learning;semiconductor industry;sentiment analysis;statistical classification;supervised learning	Jia Feng;Peng Tang;Bin Feng;Wenyu Liu	2016	2016 8th International Conference on Wireless Communications & Signal Processing (WCSP)	10.1109/WCSP.2016.7752604	semi-supervised learning;computer science;machine learning;pattern recognition;data mining	Robotics	-18.798293012458654	-65.14752962484269	15150
c2b6be54703c919a37a159ae272d56d581d4e278	automatic text generation via text extraction based on submodular		Automatic text generation is the generation of natural language texts by computer. It has many applications, including automatic report generation, online promotion, etc. However, the problem is still a challenged task due to the lack of readability and coherence even there are many existing works studied it. In this paper, we propose a two-phase algorithm, which consists of text cleanup and text extraction, to automatically generate text from multiple texts. In the first phase, we generate paragraphs based on the topic modeling and clustering analysis. In the second phase, we model the text extraction as a set covering problem after we find the keywords in terms of the scores of TF-IDF, and solve the problem via employing the tool of submodular. We conduct a set of experiments to evaluate our proposed method and experimental results demonstrate the effectiveness of our proposed method by comparing with some comparable baselines.	submodular set function	Lisi Ai;Na Li;Jianbing Zheng;Ming Gao	2017		10.1007/978-3-319-69781-9_23	submodular set function;machine learning;topic model;readability;artificial intelligence;computer science;cluster analysis;natural language;k-means clustering;set cover problem	NLP	-24.83256296076621	-66.1776466868289	15173
d94e70037e62ac941565de4f85c9139ec4e08716	igbo diacritic restoration using embedding models		Igbo is a low-resource language spoken by approximately 30 million people world-wide. It is the native language of the Igbo people of south-eastern Nigeria. In Igbo language, diacritics orthographic and tonal play a huge role in the distinction of the meaning and pronunciation of words. Omitting diacritics in texts often leads to lexical ambiguity. Diacritic restoration is a pre-processing task that replaces missing diacritics on words from which they have been removed. In this work, we applied embedding models to the diacritic restoration task and compared their performances to those of n-gram models. Although word embedding models have been successfully applied to various NLP tasks, it has not been used, to our knowledge, for diacritic restoration. Two classes of word embeddings models were used: those projected from the English embedding space; and those trained with Igbo bible corpus (≈ 1m). Our best result, 82.49%, is an improvement on the baseline n-gram models.	baseline (configuration management);circuit restoration;image restoration;lexicon;n-gram;natural language processing;orthographic projection;performance;preprocessor;real life;word embedding;word sense;word-sense disambiguation	Ignatius Ezeani;Mark Hepple;Ikechukwu E. Onyenwe;Enemouh Chioma	2018			natural language processing;diacritic;embedding;artificial intelligence;computer science;igbo	NLP	-24.92384305059397	-81.10130005566232	15192
675040873cce0ec18afa95d22583b95951997ade	tool for metabolic and regulatory pathways visual analysis	via metabolica;navegacion;dessin semi automatise;visualizacion;voie metabolique;knowledge extraction;bioinformatique;information visualization;biology;gene expression;semantic zooming;expression genique;navigation;visualization;level of detail;visualisation;visual analysis;bioinformatica;metabolic pathway;expresion genetica;biological process;bioinformatics	The research activity in bio-informatics has now reached a new phase, called post-genomics. It aims at the description of gene products as part of global processes in cells. In this research area, the various tasks to be conducted by biologists call for methods inspired from knowledge extraction and representation, and from information visualization. We describe a system devoted to the visualization of metabolic pathways. This set of biological reactions describe product transformations in the cell. The analysis of various pathway visualization tools led us to qualitative assertions. First, it is essential that the visualization environment preserves the drawing conventions borrowed from biology. Second, it seems important to offer an environment in which the user can navigate while preserving cognitive continuity. Our system focuses on these interactive and navigational issues. It offers mechanisms such as interactive color mapping and semantic zooming of pathways through various levels of details. Our tool also aims at helping biologists in the analysis of experimental results measuring gene expression in various biological processes. Although our efforts have focused on the visualization of metabolic pathways, our system should help to visualize, analyze and discover other types of biological pathways (e.g. regulatory pathways).© (2003) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.		Fabien Jourdan;Guy Melançon	2003		10.1117/12.477524	computer science;bioinformatics;artificial intelligence;data mining	HCI	-7.213125052321237	-60.60328109397409	15216
1e618b962085809e52754ed757d29ef780076d5e	disambiguation of textual data typification for the purpose of categorial analysis	categorial grammars;typification;categorial typification	The operation of categorial type assignment is prior to a categorial analysis. The lexical units, which are entries in a dictionary, are commonly associated to one or more categorial types. Therefore, we need to determine for each unit in a sentence the correct categorial type to be assigned. Current research on Categorial Grammars is not paying attention enough to this issue. The assignment of categorial types is often done either manually, or with ad hoc heuristics. In this paper we present a method based on conditional probabilities.	categorial grammar;dictionary;heuristic (computer science);hoc (programming language);text corpus;word-sense disambiguation	Adam Joly;Ismaïl Biskri;Boubaker Hamrouni	2010			natural language processing;categorial grammar;combinatory categorial grammar;algorithm	NLP	-26.077810153541964	-76.12768158889607	15224
322093d5f6d0be1f2d8ac92af2a818e942f0472f	a multipitch tracking algorithm for noisy and reverberant speech	background noise;multipitch tracking;front end;reverberation;detection algorithms;hidden markov model;speech processing;working environment noise;time frequency;speech analysis;channel selection method;speech;pitch detection algorithm;time frequency space;noise measurement;speech processing hidden markov models reverberation;personal digital assistants;hidden markov models;hidden markov models reverberation acoustic noise speech analysis personal digital assistants background noise working environment noise detection algorithms robustness algorithm design and analysis;acoustic noise;noisy speech;detection algorithm;reverberant speech;multipitch tracking algorithm;hmm tracking;robustness;room reverberation;hidden markov model multipitch tracking algorithm reverberant speech noisy speech channel selection method time frequency space;conditional probability;algorithm design and analysis;hmm tracking multipitch tracking pitch detection algorithm room reverberation;quantitative evaluation;noise;harmonic analysis	Determining multiple pitches in noisy and reverberant speech is an important and challenging task. We propose a robust multipitch tracking algorithm in the presence of both background noise and room reverberation. A new channel selection method is utilized in conjunction with an auditory front-end to extract periodicity features in the time-frequency space. These features are combined to formulate frame level conditional probabilities given each pitch state. A hidden Markov model is then applied to integrate these probabilities and search for the most likely pitch state sequences. The proposed approach can reliably detect up to two simultaneous pitch contours in noisy and reverberant conditions. Quantitative evaluations show that our system significantly outperforms existing ones, particularly in reverberant environments.	algorithm;hidden markov model;markov chain;quasiperiodicity	Zhaozhang Jin;DeLiang Wang	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495702	algorithm design;speech recognition;time–frequency analysis;conditional probability;reverberation;computer science;noise measurement;noise;speech;front and back ends;pitch detection algorithm;noise;pattern recognition;background noise;hidden markov model;robustness	Vision	-12.120717244147613	-92.78934319372509	15225
ee6b77e2f574536c96a544c3ab3d66c897872d03	n-gram-based tense models for statistical machine translation	state-of-the-art phrase-based smt system;statistical machine translation;proposed tense model;language model;translation model;tense information;n-gram-based tense model;blue point;tense model;natural language processing application;additional feature;nist chinese-english translation task	Tense is a small element to a sentence, however, error tense can raise odd grammars and result in misunderstanding. Recently, tense has drawn attention in many natural language processing applications. However, most of current Statistical Machine Translation (SMT) systems mainly depend on translation model and language model. They never consider and make full use of tense information. In this paper, we propose n-gram-based tense models for SMT and successfully integrate them into a state-of-the-art phrase-based SMT system via two additional features. Experimental results on the NIST Chinese-English translation task show that our proposed tense models are very effective, contributing performance improvement by 0.62 BLUE points over a strong baseline.	baseline (configuration management);language model;n-gram;natural language processing;statistical machine translation	Zhengxian Gong;Min Zhang;Chew Lim Tan;Guodong Zhou	2012			natural language processing;speech recognition;computer science;linguistics	NLP	-22.00078504301942	-77.05647529991627	15226
5629efbf95298d2ee5a6fe697491445940f332f2	input specification in the wag sentence generation system		This paper describes the input specification language of the WAG Sentence Generation system. The input is described in terms of Halliday's (1978) three meaning components, ideational meaning (the propositional content to be expressed), interactional meaning (what the speaker intends the listener to do in making the utterance), and textual meaning (how the content is structured as a message, in terms of theme, reference, etc.). 1 I n t r o d u c t i o n This paper describes the input specification language of the WAG Sentence Generation system. The input is described in terms of Halliday's (1978) three meaning components, ideational meaning (the propositional content to be expressed), interactional meaning (what the speaker intends the listener to do in making the utterance), and textual meaning (how the ideational content is structured as a message, in terms of theme, reference, etc.). One motivation for this paper is the lack of descriptions of input-specifications for sentence generators. I have been asked at various times to fill this gap. Perhaps a better motivation is the need to argue for a more abstract level of input. Many of the available sentence generators require specification of syntactic information within the input specification. This means that any text-planner which uses this system as its realisation module needs to concern itself with these fiddling details. One of the aims in the WAG system has been to lift the abstractness of sentence specification to a semantic level. This paper discusses this representation. The WAG Sentence Generation System is one component of the Workbench for Analysis and Generation (WAG), a system which offers various tools for developing Systemic resources (grammars, semantics, lexicons, etc.), maintaining these resources (lexical acquisition tools, network graphers, hypertext browsers, etc.), and processing (sentence analysis O'Donnell 1993, 1994; sentence generation O'Donnell 1995b; knowledge representation O'Donnell 1994; corpus tagging and explorat i o n O'Donnell 1995a). The Sentence Generation component of this system generates single sentences from a semantic input. This semantic input could be supplied by a human user. Alternatively, the semantic input can be generated as the output of a multi-sentential text generation system, allowing such a system to use the WAG system as its realisation component. The sentence generator can thus be treated a blackbox unit. Taking this approach, the designer of the multi-sentential generation system can focus on multi-sentential concerns without worrying about sentential issues. WAG improves on earlier sentence generators in various ways. Firstly, it provides a more abstract level of input than many other systems (Mumble: McDonald 1980; Meteer et al. 1987; FUF: Elhadad 1991), as will be demonstrated throughout this paper. The abstractness improves even over the nearest comparable system, Penman (Mann 1983; Mann 8z Matthiessen 1985), in its treatment of textual information (see below). Other sentence generators, while working from abstract semantic specifications, do not represent a generalised realiser, but are somewhat domain specific in implementation, e.g., Proteus (Davey 1974/1978); Slang (Patten 1988). Other systems do not allow generation independent from user interaction, for instance, Genesys (Faw-	genesys (website);hypertext;knowledge representation and reasoning;lexicon;natural language generation;proteus;specification language;the sentence;workbench	Michael O'Donnell	1996			natural language processing;speech recognition	NLP	-31.694838340471808	-83.45175145573307	15228
399f75d9f4482af5596af1ba92af7150350105e2	evaluation of a robust parser for spoken japanese	analyse de corpus;correction discursive;traitement automatique de la parole;japonais;etude experimentale;speech processing;linguistique appliquee;methode;corpus analysis;evaluative study;speech disorder;speech correction;parser;computational linguistics;linguistique informatique;method;analyseur;trouble de la phonation;etude evaluative;applied linguistics;experience design	We implemented a parser designed to handle ill-formedness in Japanese speech. The parser was evaluated by utilizing newly collected speech data, which was obtained from an experiment designed to produce ill-formed data effectively. Introducing the proposed method increased the number of correctly analyzed utterances from 171 to 322, from among 532 utterances in the corpus.	parsing;text corpus	Kotaro Funakoshi;Takenobu Tokunaga	2003			natural language processing;speech recognition;computer science;linguistics	NLP	-27.03946175536893	-78.28135693878428	15237
07106b6f2a7d70ce3fcee2bda11246595c28d964	robust speaker identification using greedy kernel pca	kernel principal component analysis;kernel;speaker identification;high dimensionality;gaussian processes;gkpca;training;speech;speaker recognition gaussian processes principal component analysis;speaker recognition;error analysis;gaussian mixture model;gaussian mixture models robust speaker identification greedy kernel pca principal component analysis;greedy kernel principal component analysis speaker recognition speaker identification gkpca;feature extraction;gaussian mixture models;principal component analysis;greedy kernel principal component analysis;kernel pca;robustness;greedy kernel pca;robustness kernel principal component analysis speaker recognition working environment noise speech computational efficiency feature extraction matrix decomposition artificial intelligence;robust speaker identification	We propose a robust speaker identification system in noisy environments using greedy kernel principal component analysis. We expect that kernel PCA can project important information to some axes and the noise to some other axes in the arbitrary high dimensional space resulting in denoising of the input features. However, it is not easy to use kernel PCA for speaker identification because the storage required for the kernel matrix grows quadratically, and the computational cost grows linearly with the number of training vectors. Therefore, we use greedy kernel PCA which can approximate kernel PCA with small representation error. In the experiments, we compare the accuracy of the greedy kernel PCA with that of the baseline Gaussian mixture models using MFCCs and PCA in noisy environment. As the results, the greedy kernel PCA outperforms conventional methods.	approximation algorithm;baseline (configuration management);computation;computational complexity theory;experiment;greedy algorithm;kernel principal component analysis;mixture model;noise reduction;speaker recognition	Min-Seok Kim;Il-Ho Yang;Ha-Jin Yu	2008	2008 20th IEEE International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2008.105	kernel;principal component regression;speaker recognition;string kernel;speech recognition;kernel embedding of distributions;radial basis function kernel;kernel principal component analysis;computer science;machine learning;pattern recognition;mixture model;mathematics;tree kernel;variable kernel density estimation;polynomial kernel;kernel smoother	ML	-17.30591594207174	-92.43525081944671	15306
c45af1cd939da95d435bf426c907a692f13b606b	characterizing diseases from unstructured text: a vocabulary driven word2vec approach	rare diseases;disease characterization;healthmap;emerging diseases;application specific word embeddings	Traditional disease surveillance can be augmented with a wide variety of real-time sources such as, news and social media. However, these sources are in general unstructured and, construction of surveillance tools such as taxonomical correlations and trace mapping involves considerable human supervision. In this paper, we motivate a disease vocabulary driven word2vec model (Dis2Vec) to model diseases and constituent attributes as word embeddings from the HealthMap news corpus. We use these word embeddings to automatically create disease taxonomies and evaluate our model against corresponding human annotated taxonomies. We compare our model accuracies against several state-of-the art word2vec methods. Our results demonstrate that Dis2Vec outperforms traditional distributed vector representations in its ability to faithfully capture taxonomical attributes across different class of diseases such as endemic, emerging and rare.	real-time locating system;social media;taxonomy (general);vocabulary;word embedding;word2vec	Saurav Ghosh;Prithwish Chakraborty;Emily Cohn;John S. Brownstein;Naren Ramakrishnan	2016		10.1145/2983323.2983362	natural language processing;computer science;machine learning;data mining	NLP	-21.65217758777052	-67.56086853979498	15326
c794c0fd79490ec4e2f842e4e023ff1572f41c9f	idocument: using ontologies for extracting and annotating information from unstructured text	information extraction;domain knowledge;semantic web;domain ontology	Due to the huge amount of text data in the WWW, annotating unstructured text with semantic markup is a crucial topic in Semantic Web research. This work formally analyzes the incorporation of domain ontologies into information extraction tasks in iDocument. Ontologybased information extraction exploits domain ontologies with formalized and structured domain knowledge for extracting domain-relevant information from un-annotated and unstructured text. iDocument provides a pipeline architecture, an extraction template interface and the ability of exchanging domain ontologies for performing information extraction tasks. This work outlines iDocument’s ontology-based architecture, the use of SPARQL queries as extraction templates and an evaluation of iDocument in an automatic document annotation scenario.	information extraction;markup language;microsoft outlook for mac;ontology (information science);pipeline (computing);resource description framework;sparql;semantic html;semantic web;text corpus;www	Benjamin Adrian;Jörn Hees;Ludger van Elst;Andreas Dengel	2009		10.1007/978-3-642-04617-9_32	idef5;relationship extraction;computer science;data mining;database;knowledge extraction;information extraction;information retrieval	NLP	-32.58635145592556	-68.1692343811758	15331
f02067277756d2a06b1b4eb5659f39fd807c917d	exploiting semantic connectivity in redefined data representation for image retrieval	image dataset semantic connectivity redefined data representation automatic image annotation research visual features text features rdf based annotation dbpedia ontology based image annotation tool image retrieval tool osia;semantics;resource description framework;semantics resource description framework feature extraction hidden markov models ontologies image retrieval probabilistic logic;hidden markov models;semantic image retrieval image annotation ontology dataset;feature extraction;ontologies;probabilistic logic;visual databases content based retrieval data structures feature extraction image classification image retrieval ontologies artificial intelligence;image retrieval	The state-of-the-art in automatic image annotation research includes many methods of combining the visual features and text to deal with the semantic gap between low-level visual feature and high-level concept. However, the text combined with images have not been clearly defined, which people disable to retrieve the desired resources by describing the queried target. Consequently, we propose a RDF-based annotation linked with ontology as DBpedia to have more semantic meanings. Although there are few image annotation researches based on ontology, no public datasets are released. In this paper, we develop an ontology-based image annotation and retrieval tool, namely OSIA. In the proposed scheme, the image dataset containing RDF annotation is announced.	automatic image annotation;dbpedia;definition;high- and low-level;image retrieval;resource description framework;sparql	Yi-Hui Chen;Eric Jui-Lin Lu;Sheng-Jia Lin	2016	2016 IEEE Second International Conference on Multimedia Big Data (BigMM)	10.1109/BigMM.2016.15	visual word;image retrieval;computer science;pattern recognition;data mining;automatic image annotation;information retrieval	Vision	-15.406822920263377	-59.50486869101717	15375
6fc324ab1108a6461adae44b093f804c166019e9	combining automatic and manual index representations in probabilistic retrieval	probabilistic retrieval model;manual index representation;combining automatic;manual index representations;multiple index representation;query strategy;retrieval effectiveness;multiple source;combined representation;information retrieval;probabilistic retrieval;query content;evidential reasoning process;databases;relevance information retrieval;documentation;indexing;indexation;statistical inference	Results from research in information retrieval have suggested that signiicant improvements in retrieval eeectiveness can be obtained by combining results from multiple index representations, query formulations, and search strategies. The inference net model of retrieval, which was designed from this point of view, treats information retrieval as an evidential reasoning process where multiple sources of evidence about document and query content are combined to estimate relevance probabilities. In this paper, we use a system based on this model to study the retrieval eeectiveness beneets of combining the types of document and query information that are found in typical commercial databases and information services. The results indicate that substantial real beneets are possible.	database;information retrieval;relevance	T. B. Rajashekar;W. Bruce Croft	1995	JASIS	10.1002/(SICI)1097-4571(199505)46:4%3C272::AID-ASI4%3E3.0.CO;2-T	document retrieval;judgment;statistical inference;query expansion;ranking;relevance;cognitive models of information retrieval;documentation;computer science;artificial intelligence;concept search;data mining;okapi bm25;natural language;term discrimination;vector space model;data retrieval;information retrieval;algorithm;multiple;statistics;human–computer information retrieval;divergence-from-randomness model	Web+IR	-34.2626851197046	-61.41775316375555	15415
00b9515b2fed1d2da5ddb74994f29bc338e3d99c	extraction of tv highlights using multimedia features	extracted descriptions;multimedia features;audio signal processing;video signal processing;television applications;digital camera;image classification;text analysis;consumer electronics;tv highlights extraction;sports programs tv highlights extraction multimedia features digital multimedia content digital capturing devices content based video browsing extracted descriptions annotations audio features text features visual features sports videos;multimedia systems;sports videos;annotations;sports video;digital cameras;text analysis multimedia communication television applications video signal processing image classification image retrieval content based retrieval feature extraction audio signal processing;indexing;video equipment;feature extraction;multimedia communication;energy states;digital multimedia content;visual features;digital capturing devices;humans;tv;video browsing;content based video browsing;text features;audio features;content based retrieval;sports programs;tv multimedia systems indexing energy states digital cameras video equipment multimedia communication automation humans consumer electronics;image retrieval;automation	The amount of digital multimedia content available to consumers is growing due to the digital capturing devices such as digital cameras and camcorders. With this increase in content, it becomes important for people to be able to browse and search for content in a timely manner. To enable efficient content-based video browsing, extracted descriptions and annotations are needed to represent the content. We present algorithms for the automatic extraction of highlights in video using audio, text and visual features. These extracted descriptions can be used for selective browsing of sports videos. We also present the experimental results for the proposed algorithms on several hours of sports programs.		Serhan Dagtas;Mohamed Abdel-Mottaleb	2001		10.1109/MMSP.2001.962717	computer vision;search engine indexing;contextual image classification;audio signal processing;feature extraction;image retrieval;computer science;energy level;automation;machine learning;multimedia;world wide web	NLP	-14.249195055558689	-55.251005699386425	15448
80c2d8c691b09f8b4e53f512b9d2641b49fda935	large-scale semantic parsing via schema matching and lexicon extension		Supervised training procedures for semantic parsers produce high-quality semantic parsers, but they have difficulty scaling to large databases because of the sheer number of logical constants for which they must see labeled training data. We present a technique for developing semantic parsers for large databases based on a reduction to standard supervised training algorithms, schema matching, and pattern learning. Leveraging techniques from each of these areas, we develop a semantic parser for Freebase that is capable of parsing questions with an F1 that improves by 0.42 over a purely-supervised learning algorithm.	algorithm;database;freebase;image scaling;lexicon;logical constant;map;natural language;parsing;supervised learning	Qingqing Cai;Alexander Yates	2013			natural language processing;semantic computing;computer science;database;programming language;lr parser	NLP	-21.814644881875243	-74.33795963898648	15471
9c40b631de66f7ce53b848d3dc113112442da600	lexical processing in the clare system	clare system;lexical processing	"""1 Introduction In many language processing systems, uncertainty in the boundaries of linguistic units means that data are represented not as a well-defined sequence of units but as a lattice of possibilities. This is often the case in speech recognition, syntactic parsing and Japanese kana-kanji conversion. In contrast, however, it is often assumed that, for languages written with interword spaces, it is sufficient to prepare an input character stream for parsing by grouping it deterministically into a sequence of words, punctuation symbols and perhaps other items. But for typed input, spaces do not necessarily correspond to boundaries between lexical items, because of errors and other, linguistic, phenomena. This means that a lattice representation, not a simple sequence, should be used throughout front end (pre-parsing) analysis. The CLARE system under development at SRI Cambridge uses such a representation, allowing it to deal straightforwardly with combinations or multiple occurrences of phenomena that would be difficult or impossible to process correctly under a sequence representation. This paper concentrates on CLARE's ability to deal with typing and spelling errors, which are especially common in interactive use, for which CLARE is designed. The word identity and word boundary ambiguities encountered in the interpretation of errorful input often require the application of syntactic and semantic knowledge on a phrasal or even sentential scale. Such knowledge may be applied as soon as the problem is encountered ; however, this brings major problems with it, such as the need for adequate lookahead, and the difficulties of engineering large systems where the processing levels are tightly coupled. To avoid such problems, CLARE adopts a staged architecture, in which indeterminacy is preserved until the knowledge needed to resolve it is ready to be applied. An appropriate representation is of course the key to doing this efficiently. In general, typing errors are not just a matter of one intended input token being miskeyed as another one. Spaces between tokens may be deleted or inserted. Multiple errors, involving both spaces and other characters, may be combined in the same intended or actual token. A reliable spelling corrector must allow for all these possibilities. However, even in the absence of """"noise"""" of this kind, spaces do not always correspond to lexical item boundaries , at least if lexical items are defined in a way that is most convenient for grammatical purposes. For example, """"special"""" forms such as telephone numbers or e-mail addresses , …"""	email;japanese input methods;nondeterministic algorithm;parsing;spaces;speech recognition;telephone number	David M. Carter	1992			lexical decision task;lexical functional grammar	NLP	-30.180062151231784	-82.70502579278254	15486
8832614c8562b639de95132a8a5dd8635614f4c1	scalable i-vector concatenation for plda based language identification system	nist feature extraction mel frequency cepstral coefficient speech principal component analysis australia;nist;nist 2007 lre database scalable i vector concatenation plda based language identification system acoustic feature spaces mfcc front ends plp front ends phonotactic front ends;speech;mel frequency cepstral coefficient;feature extraction;principal component analysis;australia;vectors acoustic signal processing cepstral analysis speech processing	Language identification systems combining i-vectors estimated from different acoustic feature spaces have recently been shown to be superior to i-vector systems based on a single acoustic feature space. Specifically, i-vectors estimated using MFCC and PLP front-ends were concatenated prior to using LDA to obtain a combined i-vector. In this work, we investigate the scalability of this i-vector concatenation based framework to incorporate a larger number of front-ends, in particular, phonotactic front-ends. A modification to the framework is also proposed in order to improve this scalability. The proposed framework is evaluated on the 30, 10 and 3 seconds test set of NIST 2007 LRE database.	acoustic cryptanalysis;concatenation;dimensionality reduction;feature vector;front and back ends;language identification;local-density approximation;pl/p;scalability;test set	Saad Irtza;Haris Bavattichalil;Vidhyasaharan Sethu;Eliathamby Ambikairajah	2015	2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)	10.1109/APSIPA.2015.7415458	speech recognition;computer science;pattern recognition;communication	Robotics	-16.194569891387236	-91.50828854718414	15497
9011e32169118dee27454cbd65d00009278e98cf	supporting semantic annotations in flickr	pragmatics;semantic annotation;electronic mail;semantics;natural languages;resource description framework;user interfaces feature semantic annotation flickr tripartite folksonomy model semantic service knowledge organisation system;web sites feature extraction semantic web user interfaces;feature extraction;natural language;web sites;semantics ontologies tagging pragmatics natural languages electronic mail resource description framework;semantic web;ontologies;user interfaces;tagging	In this paper we propose an extension to the tripartite folksonomy model to explicitly encode the semantics of tags. This enables stronger semantic services for the user such as search taking into account synonymy and hypernymy in a knowledge organisation system. However, automatic disambiguation is not yet possible and inputting the semantics of tags manually should not be a chore for the users. We thus propose a set of user interfaces features, illustrated in a working uploader for Flickr, that simplify the semantic annotation of photos before their publication. Finally, we discuss the enabling services for such interfaces, thus providing a complete description of the theoretical and practical issues of semantic annotations on Flickr.	automatic control;encode;flickr;folksonomy;homography (computer vision);semiconductor industry;upload;usability;user interface;word-sense disambiguation	Pierre Andrews;Juan Pane;Ilya Zaihrayeu;Aliaksandr Autayeu	2011	2011 IEEE 7th International Conference on Intelligent Computer Communication and Processing	10.1109/ICCP.2011.6047867	computer science;database;world wide web;information retrieval	Robotics	-29.884002769267838	-57.29399435861657	15516
df0c1af82a5725145d0cc581acdf73be40bebec9	improvements on deep bottleneck network based i-vector representation for spoken language identification.	t technology	Recently, the i-vector representation based on deep bottleneck networks (DBN) pre-trained for automatic speech recognition has received significant interest for both speaker verification (SV) and language identification (LID). In particular, a recent unified DBN based i-vector framework, referred to as DBNpGMM i-vector, has performed well. In this paper, we replace the pGMM with a phonetic mixture of factor analyzers (pMFA), and propose a new DBN-pMFA i-vector. The DBN-pMFA i-vector includes the following improvements: (i) a pMFA model is derived from the DBN, which can jointly perform feature dimension reduction and de-correlation in a single linear transformation, (ii) a shifted DBF, termed SDBF, is proposed to exploit the temporal contextual information, (iii) a senone selection scheme is proposed to improve the i-vector extraction efficienvy. We evaluate the proposed DBN-pMFA i-vector on the most confused six languages selected from NIST LRE 2009. The experimental results demonstrate that DBN-pMFA can consistently outperform the previous DBN based framework [1]. The computational complexity can be significantly reduced by applying a simple senone selection scheme.	computational complexity theory;deep belief network;dimensionality reduction;language identification;speaker recognition;speech recognition;systemverilog	Yan Song;Ruilian Cui;Ian Vince McLoughlin;Li-Rong Dai	2016		10.21437/Odyssey.2016-20	natural language processing;speech recognition;computer science;communication	AI	-16.993946203129756	-90.58943555467576	15524
8dc68795d74efdb52688395a3eec435873af89e0	a knowledge-driven approach to biomedical document conceptualization	conceptual knowledge;conceptualization;domain knowledge;medical subject heading;clustering method;knowledge driven approach;subject headings;latent semantic analysis;ontology;gene ontology	OBJECTIVE Biomedical document conceptualization is the process of clustering biomedical documents based on ontology-represented domain knowledge. The result of this process is the representation of the biomedical documents by a set of key concepts and their relationships. Most of clustering methods cluster documents based on invariant domain knowledge. The objective of this work is to develop an effective method to cluster biomedical documents based on various user-specified ontologies, so that users can exploit the concept structures of documents more effectively.   METHODS We develop a flexible framework to allow users to specify the knowledge bases, in the form of ontologies. Based on the user-specified ontologies, we develop a key concept induction algorithm, which uses latent semantic analysis to identify key concepts and cluster documents. A corpus-related ontology generation algorithm is developed to generate the concept structures of documents.   RESULTS Based on two biomedical datasets, we evaluate the proposed method and five other clustering algorithms. The clustering results of the proposed method outperform the five other algorithms, in terms of key concept identification. With respect to the first biomedical dataset, our method has the F-measure values 0.7294 and 0.5294 based on the MeSH ontology and gene ontology (GO), respectively. With respect to the second biomedical dataset, our method has the F-measure values 0.6751 and 0.6746 based on the MeSH ontology and GO, respectively. Both results outperforms the five other algorithms in terms of F-measure. Based on the MeSH ontology and GO, the generated corpus-related ontologies show informative conceptual structures.   CONCLUSIONS The proposed method enables users to specify the domain knowledge to exploit the conceptual structures of biomedical document collections. In addition, the proposed method is able to extract the key concepts and cluster the documents with a relatively high precision.		Hai-Tao Zheng;Charles Borchert;Yong Jiang	2010	Artificial intelligence in medicine	10.1016/j.artmed.2010.02.005	upper ontology;conceptualization;latent semantic analysis;computer science;ontology;artificial intelligence;data science;ontology;data mining;information retrieval;process ontology;domain knowledge	Web+IR	-31.505672374042035	-67.80305958344552	15525
d2fa04dbfa54cf5f1e7b6103341032330be008c2	noise-free similarity model for image retrieval systems	databases;query processing;data storage;image retrieval;image similarity	Reducing noise (i.e., irrelevant regions) in image query processing is no doubt one of the key elements to achieve high retrieval e ectiveness. However, existing techniques are not able to eliminate noise from similarity matching since they capture the features of the entire image area or pre-perceived objects at the database build time. In this paper, we address this outstanding issue by proposing a similarity model for noise-free queries. In our approach, users formulate their queries by specifying objects of interest, and image similarity is based only on these relevant objects. We discuss how our approach can handle translation and scaling matching as well as how space overhead can be minimized. Our experiments show that this approach, with 1/16 the storage overhead, outperforms techniques for rectangular queries and a related technique by a signi cant margin.	database;experiment;image retrieval;image scaling;overhead (computing);relevance	Khanh Vu;Kien A. Hua;Jung Hwan Oh	2001		10.1117/12.410917	image retrieval;computer science;computer data storage;data mining;database;information retrieval	DB	-11.382714661168588	-58.4726626692193	15532
0021f38840e59a2a9c404a7bf46a5432cf1ee795	an effective approach for chinese news headline classification based on multi-representation mixed model with attention and ensemble learning		In NLPCC 2017 shared task two, we propose an efficient approach for Chinese news headline classification based on multi-representation mixed model with attention and ensemble learning. Firstly, we model the headline semantic both on character and word level via Bi-directional Long Short-Term Memory (BiLSTM), with the concatenation of output states from hidden layer as the semantic representation. Meanwhile, we adopt attention mechanism to highlight the key characters or words related to the classification decision, and we get a preliminary test result. Then, for samples with lower confidence level in the preliminary test result, we utilizing ensemble learning to determine the final category of the whole test samples by sub-models voting. Testing on the NLPCC 2017 official test set, the overall F1 score of our model eventually reached 0.8176, which can be ranked No. 3.	concatenation;ensemble learning;f1 score;long short-term memory;mixed model;named entity;test set;text segmentation	Zhonglei Lu;Wenfen Liu;Yanfang Zhou;Xuexian Hu;Binyu Wang	2017		10.1007/978-3-319-73618-1_29	ensemble learning;mixed model;concatenation;f1 score;headline;confidence interval;ranking;computer science;pattern recognition;artificial intelligence;test set	ML	-20.416925607577884	-70.11091333619862	15578
580f86f1ace1feed16b592d05c2b07f26c429b4b	dense-captioning events in videos		Most natural videos contain numerous events. For example, in a video of a “man playing a piano”, the video might also contain “another man dancing” or “a crowd clapping”. We introduce the task of dense-captioning events, which involves both detecting and describing events in a video. We propose a new model that is able to identify all events in a single pass of the video while simultaneously describing the detected events with natural language. Our model introduces a variant of an existing proposal module that is designed to capture both short as well as long events that span minutes. To capture the dependencies between the events in a video, our model introduces a new captioning module that uses contextual information from past and future events to jointly describe all events. We also introduce ActivityNet Captions, a large-scale benchmark for dense-captioning events. ActivityNet Captions contains 20k videos amounting to 849 video hours with 100k total descriptions, each with its unique start and end time. Finally, we report performances of our model for dense-captioning events, video retrieval and localization.	benchmark (computing);internationalization and localization;natural language;performance;sensor	Ranjay Krishna;Kenji Hata;Frederic Ren;Li Fei-Fei;Juan Carlos Niebles	2017	2017 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2017.83	artificial intelligence;speech recognition;machine learning;natural language;feature extraction;computer science;context model;closed captioning	Vision	-14.5949940529589	-70.68224259277731	15587
8a1cac3eceaafd97cceb3abf4341b272b8ca5d4a	efficient index for retrieving top-k most frequent documents	search engine;retrieving top k;building block;information retrieval;natural extension;suffix tree;indexation;efficient index;total length;document retrieval;point of view	In the  document retrieval problem  [9], we are given a collection of documents (strings) of total length  D  in advance, and our target is to create an index for these documents such that for any subsequent input pattern  P  , we can identify which documents in the collection contain  P  . In this paper, we study a natural extension to the above document retrieval problem. We call this  top-k    frequent document retrieval  , where instead of listing all documents containing  P  , our focus is to identify the top  k  documents having most occurrences of  P  . This problem forms a basis for search engine tasks of retrieving documents ranked with TFIDF metric.#R##N##R##N#A related problem was studied by [9] where the emphasis was on retrieving all the documents whose number of occurrences of the pattern  P  exceeds some frequency threshold  f  . However, from the information retrieval point of view, it is hard for a user to specify such a threshold value  f  and have a sense of how many documents will be outputted. We develop some additional building blocks which help the user overcome this limitation. These are used to derive an efficient index for top- k  frequent document retrieval problem, answering queries in  O  ( P  + log D  loglog D  +  k  ) time and taking  O  ( D  log D  ) space. Our approach is based on novel use of the suffix tree called  induced generalized suffix tree  (IGST).		Wing-Kai Hon;Rahul Shah;Shih-Bin Wu	2009		10.1007/978-3-642-03784-9_18	document retrieval;computer science;data mining;database;information retrieval;search engine	DB	-32.809785116887745	-59.14192339884809	15593
86ec4f0923cf89c44a371d281fee4b483c67a92e	ontological content-based filtering for personalised newspapers: a method and its evaluation	busqueda informacion;electronic media;content based filtering;newspapers;information retrieval;diario;journal;user profile;recherche information;newspaper;evaluation;ressource electronique;it evaluation;information system;evaluacion;recursos electronicos;systeme information;design methodology;sistema informacion;electronic resource	Purpose – The purpose of this paper is to describe a new ontological content-based filtering method for ranking the relevance of items for readers of news items, and its evaluation. The method has been implemented in ePaper, a personalised electronic newspaper prototype system. The method utilises a hierarchical ontology of news; it considers common and related concepts appearing in a user’s profile on the one hand, and in a news item’s profile on the other hand, and measures the “hierarchical distances” between these concepts. On that basis it computes the similarity between item and user profiles and rank-orders the news items according to their relevance to each user. Design/methodology/approach – The paper evaluates the performance of the filtering method in an experimental setting. Each participant read news items obtained from an electronic newspaper and rated their relevance. Independently, the filtering method is applied to the same items and generated, for each participant, a list of news items ranked according to relevance. Findings – The results of the evaluations revealed that the filtering algorithm, which takes into consideration hierarchically related concepts, yielded significantly better results than a filtering method that takes only common concepts into consideration. The paper determined a best set of values (weights) of the hierarchical similarity parameters. It also found out that the quality of filtering improves as the number of items used for implicit updates of the profile increases, and that even with implicitly updated profiles, it is better to start with user-defined profiles. Originality/value – The proposed content-based filtering method can be used for filtering not only news items but items from any domain, and not only with a three-level hierarchical ontology but any-level ontology, in any language.	algorithm;content-control software;electronic paper;prototype;recommender system;relevance;user profile	Veronica Maidel;Peretz Shoval;Bracha Shapira;Meirav Taieb-Maimon	2010	Online Information Review	10.1108/14684521011084591	newspaper;computer science;data mining;database;multimedia;law;world wide web;information retrieval	Web+IR	-34.74224695282596	-58.137830652305155	15624
a311935e36d9e27c906a69108cfc54160c011a43	hit and mis: implications of health information technology and medical information systems	medical information system;electronic health care record;potential advantage;health information technology	Evaluating the potential advantages and considering the risks associated with electronic health care records.	information system	Peter G. Goldschmidt	2005	Commun. ACM	10.1145/1089107.1089141	human–computer interaction;data mining;world wide web	HCI	-58.45540369423712	-63.44678201685315	15640
151f97b2799ecd274f2f0e3b78a7092e27147d04	authorship disambiguation in a collaborative editing environment		Abstract “Wikipedia”, known as the worldu0027s largest online free encyclopedia, is one of the remarkable examples of crowdsourcing, where millions of articles have been produced by volunteers from all over the world. Wikipedia allows anyone to edit articles without being pre-screened or authorized. A user can edit articles using either a valid ID or an IP address. The freedom of editing using ID or IP makes editorial identities ambiguous. This may affect the integrity of the research process. It also facilitates malicious users to vandalize Wikipedia content. Disambiguation of usersu0027 identity in Wikipedia can assist in distinguishing between trusted and mischievous users and more important in defining authorship in a less ambiguous manner. The present paper introduces a new methodology to ascertain Wikipedia authorship and to reduce ambiguity of user IDs. Our methodology uses the editing activity of users as a distinguishing feature for identifying non-ambiguous profiles. Reducing ambiguity of authorship can facilitate understanding of human behavior in collaborative editing, predicting sock puppetry (duplicate accounts), detecting anomaly, identifying trustworthy as well as offensive authors, and in improving security procedures and research in online social media. Our experimental results indicate that it is possible to disambiguate with high degree of certainty the editorial activity by 75% at rank 1.	word-sense disambiguation	Padma Polash Paul;Madeena Sultana;Sorin Adam Matei;Marina L. Gavrilova	2018	Computers & Security	10.1016/j.cose.2018.01.010	encyclopedia;collaborative editing;trustworthiness;internet privacy;certainty;ambiguity;computer science;social media;crowdsourcing	HCI	-23.512994305758944	-59.41506954876494	15648
283efc76fed2a07ccceb281e72d658576d0c25ae	an intermediate representation for the interpretation of temporal expressions	question answering;natural language processing;intermediate representation;information extraction	The interpretation of temporal expressions in text is an important constituent task for many practical natural language processing tasks, including question-answering, information extraction and text summarisation. Although temporal expressions have long been studied in the research literature, it is only more recently, with the impetus provided by exercises like the ACE Program, that attention has been directed to broad-coverage, implemented systems. In this paper, we describe our approach to intermediate semantic representations in the interpretation of temporal expressions.	ace;information extraction;intermediate representation;natural language processing;question answering;scientific literature;temporal expressions;the australian	Pawel P. Mazur;Robert Dale	2006			natural language processing;question answering;computer science;data mining;database;intermediate language;information extraction	NLP	-30.36976950824689	-75.46056556622239	15658
a96144035b4a55e2e5d03ec58d2325cc3ef820f6	on-line handwritten digit recognition based on trajectory and velocity modeling	modelizacion;evaluation performance;digit recognition;handwriting recognition;pen based interface;performance evaluation;learning;caracter manuscrito;model system;implementation;digitizing;evaluacion prestacion;manuscript character;handwritten digit recognition;conversion an;numerisation;algorithme;aprendizaje;beta velocity modeling;modelisation;algorithm;vecino mas cercano;apprentissage;reconnaissance ecriture;automatic recognition;arabic;signal classification;pattern recognition;classification signal;autoorganizacion;numerizacion;plus proche voisin;self organization;nearest neighbour;k nearest neighbor;self organized map;handwriting modeling;arabe;elliptic trajectory modeling;reconnaissance forme;classification automatique;reseau neuronal;stroke overlapping;reconocimiento patron;implementacion;automatic classification;modeling;caractere manuscrit;clasificacion automatica;red neuronal;autoorganisation;handwritten character recognition;ad conversion;reconocimiento automatico;reconnaissance caractere manuscrit;reconnaissance automatique;neural network;algoritmo	The handwriting is one of the most familiar communication media. Pen based interface combined with automatic handwriting recognition offers a very easy and natural input method. The handwritten signal is on-line collected via a digitizing device, and it is classified as one pre-specified set of characters. The main techniques applied in our work include two fields of research. The first one consists of the modeling system of handwriting. In this area, we developed a novel method of the handwritten trajectory modeling based on elliptic and Beta representation. The second part of our work shows the implementation of a classifier consisting of the Multi-Layers Perception of Neural Networks (MLPNN) developed in a fuzzy concept. The training process of the recognition system is based on an association of the Self Organization Maps (SOM) with Fuzzy K-Nearest Neighbor Algorithms (FKNNA). To test the performance of our system we build 30,000 Arabic digits. The global recognition rate obtained by our recognition system is about 95.08%. 2007 Elsevier B.V. All rights reserved.	artificial neural network;complementarity theory;feature vector;fuzzy concept;fuzzy logic;handwriting recognition;input method;k-nearest neighbors algorithm;online and offline;self-organization;velocity (software development)	Monji Kherallah;Lobna Haddad;Adel M. Alimi;Amar Mitiche	2008	Pattern Recognition Letters	10.1016/j.patrec.2007.11.011	computer vision;speech recognition;intelligent character recognition;computer science;artificial intelligence;machine learning;arabic;pattern recognition;handwriting recognition;implementation	AI	-13.017633094373045	-97.26565977564657	15659
c18c72ff8dabc5899f06a0bdad4b096043d3891b	application of data mining software to predict the alum dosage in coagulation process: a case study of vientaine, lao pdr			data mining;design review (u.s. government)	Khoumkham Ladsavong;Petchporn Chawakitchareon;Yasushi Kiyoki	2017		10.3233/978-1-61499-834-1-110	mathematics;data mining;software;alum	ML	-55.801551431400064	-65.08343418196428	15665
79b2125873a59f85d01da3cb8540eb9ecab26758	discourse relations in the prague dependency treebank 3.0		The aim of the demo is threefold. First, it introduces the current version of the annotation tool for discourse relations in the Prague Dependency Treebank 3.0. Second, it presents the discourse relations in the treebank themselves, including new additions in comparison with the previous release. And third, it shows how to search in the treebank, with focus on the discourse relations.	php development tools;treebank	Jirí Mírovský;Pavlína Jínová;Lucie Poláková	2014			natural language processing;computer science;treebank;linguistics	NLP	-30.900326652230262	-76.10684219891897	15669
2a4067211dc6b2f9678e37f210d6976e704075e9	parichaya - a low-cost device to increase adherence among tuberculosis patients in rural assam	contextual enquiry;low cost;medical kit;assam;ict4d;india;tuberculosis	Limited access to information, lack of motivation and unawareness of cause, precaution and prevention of Tuberculosis (TB) are some of the leading causes for in-adherence towards TB therapy. We present key findings and insights through a contextual enquiry conducted with patients and healthcare workers, in order to develop new Information Communication Technology (ICT) interventions for empowerment of TB patients in rural parts of Assam. We also propose Parichaya, a low-cost device that helps patients in utilizing their idle time during medication to learn about the disease and importance of DOTS therapy. The device aims to motivate the patients to adhere to DOTS therapy and to make informed health decisions. Using a basic recording module and push buttons, the system makes use of illustrations with embedded audio in Assamese language to spread awareness through narratives. The device, in the form of a medical kit, replaces the traditional medication blisters for TB patients.	electroconvulsive therapy;embedded system;freedom of information laws by country;terabyte	Himanshu Seth;Keyur Sorathia	2013		10.1145/2525194.2525204	low-cost carrier	HCI	-61.32880558720536	-56.58815623911061	15682
7e2825a0e1ed5a94c249cd16d1d0d5a9f3147cc1	the indiana public health emergency surveillance system: ongoing progress, early findings, and future directions	computer communication networks;medical records systems computerized;gastroenteritis;population surveillance;indiana;regional medical programs;carbon monoxide poisoning;emergency service hospital;humans;disasters;disease outbreaks	Beginning in 2004, the Indiana State Department of Health (ISDH) partnered with the Regenstrief Institute on a 4-year project to implement a statewide biosurveillance system incorporating more than 110 hospitals. This paper describes our evolving experience with the system including ongoing implementation challenges, how the system has helped to uncover events of public health significance, and future directions. The system currently receives ED visit data from 50 hospitals totaling nearly 5,000 visits per day, and is projected to have 65 hospitals connected by August 2006.	biosurveillance;delivery of health care;natural disasters;projections and predictions;systems engineering;childhood mixed cellularity hodgkin's lymphoma	Shaun J. Grannis;Michael Wade;P. Joseph Gibson;J. Marc Overhage	2006	AMIA ... Annual Symposium proceedings. AMIA Symposium		medicine;environmental health;medical emergency	Embedded	-60.40375082119323	-68.02889505400915	15700
ea9d261a8f53c08d5c4e912bd7e932a951d283be	the ibm systems for trilingual entity discovery and linking at tac 2016		This paper describes the IBM systems for the Trilingual Entity Discovery and Linking (EDL) for the TAC 2016 KnowledgeBase Population track. The entity discovery or mention detection (MD) system is based on system combination of deep neural networks and conditional random fields. The entity linking (EL) system is based on a language independent probabilistic disambiguation model described in (Sil and Florian, 2016). However, the system is different than TAC 2015 as it is trained using more training data from previous TAC evaluations and a significant portion of the Wikipedia. The same EL model was applied across all 3 languages: English, Spanish and Chinese. We submitted 3 runs for the first EDL evaluation window and 5 for the next one. 1 System Description 1.1 Mention Detection The IBM mention detection system was a combination of two mention detection systems one being a Neural Net-based (NN) system and one being a Conditional Random Fields (CRF) system, both trained to predict the standard IOB mention detection encoding (for English, the tag also has a bit specifying whether the mention is named or nominal). The Chinese model was a characterbased model, while the English and Spanish models are more standard token-based models. All models were trained and applied using the IBM Statistical Information Relation and Extraction toolkit (SIRE). The CRF model is a linear-chain CRF model of size 1 (the previous tag is used in features), using a multitude of features including words in context, capitalization flags, various entity dictionaries, both supervised (lists extracted from the ACE’05 data, the CoNLL’03 data, etc) and unsupervised (the system output on Gigaword), word clustering (Brown clusters), cache features, word length and IDF. In addition, the output of a KLUE model (an information extraction system with 50 mention types and relation types) was used as an additional input (for a minor improvement in performance). All parameters of the model were estimated by 5-fold cross-validation on the training data. We also used part of the test data from 2015 for training we separated about 20 documents in each language for development, and added the rest to the training set. On this small training set, the performance improved about 5F for English by using this additional data. The NN system uses a feed-forward neural net to predict entity labels. The network architecture (Figure 1) is similar to that proposed in (Collobert et al., 2011) and uses as input the concatenation of the target and context words (symmetric window of size 4) to which we add vectors for two of the features used in the CRF model: dictionaries and capitalization flags. For these additional features, when multiple values fire, their vectors are averaged (e.g. the capitalization vector for CEO is the mean of allcap, initcap and 3upper vectors.) We attach scalar weights to each of the features (λi), allowing the model to more easily learn the relative importance of each word/feature used in the input representations. (Learning for example that the target word has the highest weight and context word weights decay with increasing distance to the target). We use one hidden layer of size 1000 and sigmoid as its activation function. The cost function is the word-level log-likelihood described in (Collobert et al., 2011) in which the probability of the correct label is normalized w.r.t. the other labels using a softmax function. We additionally incorporate character-level representations by concatenating the output of a forward and a backward LSTM on the sequence of characters of the current token (Lample et al., 2016), where the character embeddings are randomly initialized for English and Spanish and pretrained for Chinese. For English, we utilize an additional feature consisting of the label assigned by a mention detection model assigning one of 50 predefined labels and trained on additional data. The word vectors are initialized with 300dimensional pre-trained embeddings build on a concatenation of Gigaword, Bolt and Wikipedia, (totaling≈ 6 billion tokens). Embeddings are built using a variant of the word2vec CBOW architecture, which predicts a target word from the concatenation of its context words, rather than the average. This variant outperforms CBOW both on standard word similarity benchmarks as well as in mention detection experiments. Both the additional feature vectors as well as word vectors are fine-tuned during training (i.e. error is backpropagated to the input representation). For Chinese we also use positional character embeddings (Peng and Dredze, 2015) with each character being concatenated with its position in the word, leading from a vocabulary of 8K characters to 18K positional characters. The target word is represented as the mean of the positional character embeddings. For Chinese, both word and character embeddings are 300-dimensional and learned with word2vec on GigaWord. Figure 1: Architecture of the neural network used for mention detection The two systems were combined in a simple scheme described below (same combination we have performed in 2015). We noticed that all models were slanted towards precision (meaning, precision was 5-6 points higher than recall), and we combined them as follows: • The initial system output is the best performing system (NNs for English and CRF for Spanish) • Considering the remaining systems in the order of performance, add any mentions that do not overlap with the combined system The combination resulted in improvements of 0.51F on the small development data, as can be seen in Table 1. We have also built a coreference model directly from the training data. To be able to produce nominal mentions for the types for which such information was not provided (Spanish and Chinese data, and all types besides PER for English), we have done the following: • Ran KLUE model described above on the test data • Aligned the coreference chains from the TAC output and the KLUE output, by the maximum mention overlap • Added any nominal mentions found in a chain of a proper type (i.e. PER, ORG, etc) to the corresponding TAC chain (if one existed); the mentions that belong to a chain that does not align are thrown out.	8k resolution;activation function;align (company);artificial neural network;chinese room;cluster analysis;concatenation;conditional random field;cross-validation (statistics);deep learning;dictionary;entity linking;experiment;feedforward neural network;information extraction;inside outside beginning;long short-term memory;network architecture;null character;randomness;sigmoid function;softmax function;test data;test set;tf–idf;vocabulary;wikipedia;window function;word embedding;word lists by frequency;word-sense disambiguation;word2vec	Avirup Sil;Georgiana Dinu;Radu Florian	2016				NLP	-19.723969868736912	-73.62639961679282	15730
403f4401e449c5aa751ad14a4eeb775cb3c264a6	head/modifier pairs for everyone	information retrieval;linguistic resources;linguistic phrases;head modifier pairs	The “English Phrases for IR” (EP4IR) grammar is a grammar of English concentrating on the description of the noun phrase and the verb phrase. The grammar is provided with a large lexicon, providing detailed Part-Of-Speech information. It is quite robust against badly formed input and unknown words and generates only the most probable analysis. The EP4IR grammar and lexicon are released along with the AGFL system [1], which is the first parser-generator for linguistic applications available under the GNU Public License. The parsers generated from it fall under the LGPL, so that they can be used for scientific and even commercial applications. From the EP4IR grammar and lexicon, an English parser can be generated automatically using the AGFL system, which produces as its output not parse trees but Head/ Modifier trees, binary dependency trees that can be unnested to Head/Modifier pairs. In this transduction process, the HM trees are syntactically normalized. The pairs generated when parsing some text represent only the major relations expressed in the text [2]:	affix grammar over a finite lattice;compiler-compiler;gnu;lexicon;modifier key;parsing;transduction (machine learning)	Cornelis H. A. Koster	2003		10.1145/860435.860557	computer science;information retrieval	NLP	-25.629687892776506	-77.53490397535205	15742
f5b128925dbf649d6d97d383075861f6d8ca582f	intonational phonology and prosodic hierarchy in malay	drntu humanities linguistics phonology;conference paper	This paper presents original data in support of a new model of intonational phonology for Malay as spoken in Singapore. Building on the Autosegmental-Metrical approach (Beckman & Pierrehumbert, 1986), we propose that intonational variation in Malay can be explained in terms of underlying sequences of abstract tonal units (H and L), which are aligned to the edges and internal syllables of prosodic phrases organized in a hierarchy. Data was drawn from a production experiment (Hamzah, 2012) involving declarative utterances under different focus patterns in a question-answer context, as well as from story-telling interviews and TV interviews. We find evidence for at least three levels of prosodic organization: (i) an accentual phrase which comprises one or more words and bears an L and H tone at its left and right edges, respectively, (ii) an intermediate phrase, which serves as the domain of catathesis, and (iii) an intonational phrase, which may span the entire utterance and bears an additional H or L tone at its right edge. Differences in F0 peak alignment for focused words support the presence of a focus pitch accent. We outline a series of follow-up studies for extending the model further.	articulatory phonology	Diyana Hamzah;James Sneed German	2014			natural language processing;theoretical linguistics;speech recognition;computer science;linguistics;phonology	NLP	-11.866736056158764	-80.48502826260814	15788
147154a3719197d921eda6d6b533e44b3f8c28f3	answering complex questions using open information extraction		While there has been substantial progress in factoid question-answering (QA), answering complex questions remains challenging, typically requiring both a large body of knowledge and inference techniques. Open Information Extraction (Open IE) provides a way to generate semi-structured knowledge for QA, but to date such knowledge has only been used to answer simple questions with retrievalbased methods. We overcome this limitation by presenting a method for reasoning with Open IE knowledge, allowing more complex questions to be handled. Using a recently proposed support graph optimization framework for QA, we develop a new inference model for Open IE, in particular one that can work effectively with multiple short facts, noise, and the relational structure of tuples. Our model significantly outperforms a state-of-the-art structured solver on complex questions of varying difficulty, while also removing the reliance on manually curated knowledge.	error analysis (mathematics);information extraction;knowledge base;lossy compression;mathematical optimization;question answering;semiconductor industry;similarity learning;software quality assurance;solver	Tushar Khot;Ashish Sabharwal;Peter Clark	2017		10.18653/v1/P17-2049	information retrieval	NLP	-17.463194048239483	-67.5996785697654	15816
937ec6a3ce35b04dfdfd63857a040be8e694b219	improving the phonetic annotation by means of prosodie phrasing	technology and engineering	It was established that the performance of our annotation system [8] is affected by the length of the utterances: the error rate, the CPU-load and the memory requirements tend to increase as the utterances get longer. In this contribution the speech signal is first segmented into speech, pauzes and noise (breaths, clicks, : : : ) and subsequently split in signal phrases prior to the annotation. Experiments on 3 different databases (3 languages) demonstrate that this stategy yields a significant improvement of the annotation accuracy.	central processing unit;database;requirement	Halewijn Vereecken;Annemie Vorstermans;Jean-Pierre Martens;Bert Van Coile	1997			natural language processing;speech recognition;computer science;information retrieval	NLP	-20.70425800080057	-82.97705603707955	15850
907fd9142d665003a538ad81db1ffd2c5379e5dc	an adaptive domain knowledge manager for dialogue systems	domain model;dialogue system;human computer interaction;ambient intelligence;mixed initiative;spoken dialogue system;domain knowledge;large scale;machine learning;state space;dialogue manager	This paper describes the recent effort to improve our Domain Knowledge Manager (DKM) that is part of a mixed-initiative task based Spoken Dialogue System (SDS) architecture, namely to interact within an ambient intelligence scenario. Machine-learning applied to SDS dialogue management strategy design is a growing research area. Training of such strategies can be done using human users or using corpora of human computer dialogue. However, the size of the state space grows exponentially according to the state variables taken into account, making the task of learning dialogue strategies for large-scale SDS very difficult. To address that problem, we propose a divide to conquer approach, assuming that practical dialogue and domain-independent hypothesis are true. In this context, we have considered a clear separation between linguistic dependent and domain dependent knowledge, which allows reducing the complexity of SDS typical components, specially the Dialoguer Manager (DM). Our contribution enables domain portability issues, proposing an adaptive DKM to simplify DM’s clarification dialogs. DKM learns, through trial-and-error, from the interaction with DM suggesting a set of best task-device pairs to accomplish a request and watching the user’s confirmation. This adaptive DKM has been tested in our domain simulator.	ambient intelligence;dialog system;human computer;machine learning;simulation;software portability;spoken dialog systems;state space;text corpus	Porfírio P. Filipe;Luís Morgado;Nuno J. Mamede	2007			natural language processing;ambient intelligence;computer science;state space;knowledge management;artificial intelligence;domain model;domain knowledge	NLP	-27.214796693895945	-86.81414173378779	15851
38f66ea1fb0fbadcd8b3b4c29e219d83bc024d1b	automatic query adjustment in document retrieval	information retrieval;online systems;search strategies;document retrieval;relevance information retrieval;automation	Abstract   A system is described for the automatic adjustment of queries addressed to information retrieval systems employing a structurised thesaurus for the coordinate indexing of an average of at least five or six descriptors per document. Starting with at least two documents considered by the user as relevant to his inquiry, the system formulates different queries using descriptors occuring in the relevant documents. Results from these queries are presented to the user for relevance assessment as a result of which the most efficient queries are automatically selected and loosened (broadened). The new documents retrieved are again checked for relevance by the user; and with new relevant documents the loop starts again.  The result of the automatic procedure is independent of the point of departure. The automatic procedure is superior to traditional searching procedures in terms of both recall and precision. The automatic procedure requires more computing, but probably for more than 80% of the inquiries the need for a documentalist as an intermediary between the user and the system can be avoided.	document retrieval	Carlo Vernimb	1977	Inf. Process. Manage.	10.1016/0306-4573(77)90054-1	document retrieval;ranking;relevance;computer science;automation;data mining;database;world wide web;information retrieval	DB	-33.722199943149725	-60.033650760329344	15856
cdd4391a77a17c5284b57dee4bc370da43b9d11f	suspect and popular tag detection model for social media	twitter computer hacking computational modeling media facebook internet;social networking online computer crime computer network security;tag detection criminal network suspect detection;computer hacking;media;computational modeling;internet;feature space popular tag detection model suspect detection model social media suspicious group hacker twitter social network term frequency inverse document frequency tf idf;facebook;twitter	In this work, a model has been developed for detecting popular tags belonging to suspicious group using shares of active hackers and followers on Twitter social network. Term frequency-inverse document frequency (tf-idf) is reinterpreted with the number of favorite and re-tweet to detect popular tags belonging to suspicious group. The obtained feature space is used for detecting the most strongly suspected which are similar to the target hackers. The results show that suspected profiles, which are detected by our model, have been closed by Twitter with course decision.	feature vector;sensor;social media;social network;tf–idf	Arzu Kakisim;Yavuz Oguz Ipek;Ibrahim Sogukpinar	2016	2016 24th Signal Processing and Communication Application Conference (SIU)	10.1109/SIU.2016.7495884	computer science;internet privacy;world wide web;computer security	Metrics	-20.079788080208658	-55.72956596860863	15861
2ab0f5ec27d4bed403806e0ae4000b23b92ff2bb	integrating multiple data sources in a cardiology imaging laboratory	ris;pacs;structured reports;dicom;echocardiography	Nowadays, medical imaging laboratories are supported by heterogeneous systems, including image repositories, acquisition devices, viewer workstations and other administrative information systems. They hold tremendous amounts of data resulting not only from imaging modalities, but also from patient diagnosis, treatment, and services management. Unfortunately, the interoperability between the different medical information systems is still a major limitation. Despite the existence of standards to support the distinct RIS and PACS applications, such as DICOM and HL7, the interoperability between them is, in most cases, limited to a few sets of information elements. As a result, the establishment of cooperative workflows or the integrated visualization of patient data is still compromised. Moreover, this scenario severely constraints the usage of these data for research and business analytics purposes, commonly referred as secondary uses of data. In this document, we propose a method for transforming echocardiography reports held by proprietary information systems into DICOM Structured Reports (SR), the gold standard for interoperability in medical imaging. As a result, reports, images, and associated metadata can be accessed and shared by all PACS applications in an integrated and structured manner. Furthermore, the large-scale federation of those elements has a tremendous interest for data analytics and secondary uses of data.	business analytics;dicom;health level 7;information system;interoperability;medical imaging;modal logic;picture archiving and communication system;relational interface system;workstation	Tiago Marques Godinho;Eduardo Almeida;Luís Bastião;Carlos Costa	2016	2016 IEEE 18th International Conference on e-Health Networking, Applications and Services (Healthcom)	10.1109/HealthCom.2016.7749524	computer science;data mining;database;picture archiving and communication system;world wide web	Visualization	-53.01520157159702	-63.315978838171205	15866
22da6989eeca468426aca58f39ca0188616d9a5b	diagnosing multiple interacting defects with combination descriptions	congenital heart disease;artificial intelligence	Cases with multiple defects can be difficult to diagnose because the defects can interact, meaning that the observable cues are not a sum of the cues for the component defects. Diagnostic methods that use cueto-defect relationships fail when interactions between defects change the observable cues. The primary alternative, model-based methods, are limited to domains with accurate and complete models, along with initialization data. Using these traditional methods, when defects interact and models aren’t available, each possible defect combination must be included in the knowledge base. This results in an explosion of possible alternatives, greatly increased knowledge acquisition effort, slower processing, and increased maintenance effort. This research develops a computational diagnostic model that can diagnose multiple defects, even when cues are altered or missing, by using descriptions of cue combinations. We develop a description and classification of the ways cues change when defects interact. Each type of cue may combine in a different way, so each type has a separate description. The combination methods use the expectations of component defects to diagnose multiple interacting defects, instead of requiring a description of each possible defect combination. In a medical domain (diagnosis of congenital heart defects), we found that cues combine with one another in a small number of ways: all cues may appear (union), the values of the cues may be added (quantitatively or qualitatively), or dominant cues may mask other cues present. Cues of each type combine in one of these basic ways, or use a combination of a few of the basic ways, depending on characteristics of the cues, case, or domain. The cues are different than expected because the defects interact. Two murmurs are expected, a loud systolic ejection murmur in the pulmonary area for ASD, and a moderate systolic ejection murmur in the aortic area for AS. Only one murmur is observed. Loud murmurs mask softer ones occurring at the same time, so the absence of the (softer) expected murmur for AS is explained. The observed murmur supports ASD alone or ASD+AS. The observed heart sound S2 is normal, while both defect expectations are abnormal. ASD produces a wide, fixed split S2 while AS produces a narrow, variably split S2. The wide and narrow widths combine additively to explain a normal width, while the variable and fixed expectations combine additively to a variable split, which is a normal S2. Neither ASD nor AS is supported alone, but ASD+AS is supported by the normal S2 cue. If we had used cue-to-defect relationships and matching on this case, we would have explained only the observed murmur (with one of the component defects), leaving three missing abnormal expectations unexplained. The diagnostic model is tested by constructing a program with a knowledge base in pediatric cardiology (Fallot) and testing it on cases of single and multiple defects from hospital files. Fallot uses a combination of recognition-based reasoning (Thompson et al. 1983) and the cue combination descriptions. This program correctly diagnoses cases with multiple interacting defects for which conventional methods fail.	computation;cue tone;interaction;knowledge acquisition;knowledge base;list of english terms of venery, by animal;murmurhash;observable;software bug;software defect indicator	Nancy E. Reed	1994			computer science;artificial intelligence;machine learning	AI	-5.812331684367423	-77.55700192624695	15874
f20823302117b27e68ac94efc3be97451692b6cd	why do readers answer questions wrongly after reading garden-path sentences?				Zhiying Qian;Susan M. Garnsey	2015			natural language processing;linguistics	NLP	-29.94308357344685	-77.64296077110045	15897
7fa412a47d76cdd34aadb8ce8a9778e8813a9f15	blog tells what kind of personality you have: egogram estimation from japanese weblog	personality;personality type;computational linguistic;egogram;bayes classifier;automatic classification;information gain	In this paper, we investigate personality estimation from Japanese weblog text. Among various personality types, we focus on Egogram, which has been used in Transactional Analysis and is strongly related to the communicative behavior of individuals. Estimation is performed using the Multinomial Naïve Bayes classifier with some feature words that are selected based on the information gain. The validity of this approach was evaluated with real weblog text of 551 subjects. The results show that our approach achieved 12-25% improvement from baseline. The feature words selected for the estimation are strongly correlated with the characteristics of Egogram.	baseline (configuration management);blog;information gain in decision trees;kullback–leibler divergence;multinomial logistic regression;naive bayes classifier;strongly correlated material	Atsunori Minamikawa;Hiroyuki Yokoyama	2011		10.1145/1958824.1958856	psychology;bayes classifier;computer science;machine learning;pattern recognition;personality;kullback–leibler divergence;communication;social psychology	NLP	-22.18258944222034	-65.27242445429899	15898
f675091725a2ad577de3c95e06674cf60a6ed431	privacy in online review sites	online data privacy semantic name entity recognition;history;motion pictures;text analysis;web sites data privacy text analysis;html;named entity recognition;online data;data privacy;business;web sites;semantic;name entity recognition;twitter;private information;sensitive review text online review sites user privacy private information structured information unstructured information privacy check tool keyword matching named entity recognition;privacy;privacy html business motion pictures history real time systems twitter;real time systems	The increasing use of online review sites is creating new challenges for user privacy. Although reviews are public, many users inadvertently disclose private information about relationship, location, and temporal attributes to the world. This research protects users of online review sites from the inadvertent disclosure of private information in three ways. First, the types of unstructured and structured information made public by online review sites are characterized and used to grade those sites on their attention to privacy. Second, a privacy-check tool that uses keyword matching and named-entity recognition to annotate potentially sensitive review text is presented. Third, we raise awareness of the privacy threat in online review sites through examples and statistics derived from the privacy-check tool.	client-side;google sites;named-entity recognition;personally identifiable information;privacy	Matthew Burkholder;Rachel Greenstadt	2012	2012 IEEE Symposium on Security and Privacy Workshops	10.1109/SPW.2012.23	text mining;private information retrieval;html;information privacy;privacy by design;computer science;data mining;internet privacy;privacy;world wide web;computer security	Security	-21.233420405456243	-55.79653477277273	15900
058086f9d4cb785fccc1f8863f2a5cf0f284d39d	sledded: a proposed dataset of event descriptions for evaluating phrase representations		Measuring the semantic relatedness of phrase pairs is important for evaluating compositional distributional semantic representations. Many existing phrase relatedness datasets are limited to either lexical or syntactic alternations between phrase pairs, which limits the power of the evaluation. We propose SLEDDED (Syntactically and LExically Divergent Dataset of Event Descriptions), a dataset of event descriptions in which related phrase pairs are designed to exhibit minimal lexical and syntactic overlap; for example, a decisive victory — won the match clearly. We also propose a subset of the data aimed at distinguishing event descriptions from related but dissimilar phrases; for example, vowing to fight to the death — a new training regime for soldiers, which serves as a proxy for the tasks of narrative generation, event sequencing, and summarization. We describe a method for extracting candidate pairs from a corpus based on occurrences of event nouns (e.g. war) and a two-step annotation process consisting of expert annotation followed by crowdsourcing. We present examples from a pilot of the expert annotation step.	crowdsourcing;machine translation;semantic similarity;textual entailment	Laura Rimell;Eva Maria Vecchi	2016		10.18653/v1/W16-2525	natural language processing;computer science;pattern recognition;information retrieval	NLP	-25.94269381783027	-71.93584228980107	15907
4cf090824b89b4fddebc659f09ddca4aafafcdd7	ranking entities for web queries through text and knowledge	knowledge bases;entities;information retrieval;entity ranking;004 informatik	When humans explain complex topics, they naturally talk about involved entities, such as people, locations, or events. In this paper, we aim at automating this process by retrieving and ranking entities that are relevant to understand free-text web-style queries like Argentine British relations, which typically demand a set of heterogeneous entities with no specific target type like, for instance, Falklands_-War} or Margaret-_Thatcher, as answer. Standard approaches to entity retrieval rely purely on features from the knowledge base. We approach the problem from the opposite direction, namely by analyzing web documents that are found to be query-relevant. Our approach hinges on entity linking technology that identifies entity mentions and links them to a knowledge base like Wikipedia. We use a learning-to-rank approach and study different features that use documents, entity mentions, and knowledge base entities -- thus bridging document and entity retrieval. Since established benchmarks for this problem do not exist, we use TREC test collections for document ranking and collect custom relevance judgments for entities. Experiments on TREC Robust04 and TREC Web13/14 data show that: i) single entity features, like the frequency of occurrence within the top-ranke documents, or the query retrieval score against a knowledge base, perform generally well; ii) the best overall performance is achieved when combining different features that relate an entity to the query, its document mentions, and its knowledge base representation.	bridging (networking);document;entity linking;knowledge base;learning to rank;ranking (information retrieval);relevance;text retrieval conference;web page;wikipedia	Michael Schuhmacher;Laura Dietz;Simone Paolo Ponzetto	2015		10.1145/2806416.2806480	computer science;data mining;entity linking;entity;database;weak entity;information retrieval	Web+IR	-29.491636494326354	-65.44591811671285	15915
2b6f78b95a518f16dbd8cdaf4715c0e40b8b453b	evaluation of string normalisation modules for string-based biomedical vocabularies alignment with anagram		Biomedical vocabularies have specific characteristics that make their lexical alignment challenging. We have built a string-based vocabulary alignment tool, AnAGram, dedicated to efficiently compare terms in the biomedical domain, and evaluate this tool’s results against an algorithm based on Jaro-Winkler’s edit-distance. AnAGram is modular, enabling us to evaluate the precision and recall of different normalization procedures. Globally, our normalization and replacement strategy improves the F-measure score from the edit-distance experiment by more than 100%. Most of this increase can be explained by targeted transformations of the strings with the use of a dictionary of adjective/noun correspondences yielding useful results. However, we found that the classic Porter stemming algorithm needs to be adapted to the biomedical domain to give good quality results in this area.	algorithm;dictionary;graph edit distance;jaro–winkler distance;precision and recall;stemming;vocabulary	Anique van Berne;Véronique Malaisé	2014			natural language processing;computer science;data mining;database;world wide web;algorithm	Comp.	-29.802200821021923	-71.04879009835086	15939
3916d62eb4c7abc9ed2f903697e1d272a938bd91	target speech detection and separation for communication with humanoid robots in noisy home environments	humanoid robot;robot audition;real time;spectrum;human robot interaction;automatic speech recognition;success rate;sound source localization;sound source separation;voice activity detection;signal to noise ratio;source separation;face to face;speech detection	People usually talk face to face when they communicate with their partner. Therefore, in robot audition, the recognition of the front talker is critical for smooth interactions. This paper presents an enhanced speech detection method for a humanoid robot that can separate and recognize speech signals originating from the front even in noisy home environments. The robot audition system consists of a new type of voice activity detection (VAD) based on the complex spectrum circle centroid (CSCC) method and a maximum signal-to-noise ratio (SNR) beamformer. This VAD based on CSCC can classify speech signals that are retrieved at the frontal region of two microphones embedded on the robot. The system works in real-time without needing training filter coefficients given in advance even in a noisy environment (SNR > 0 dB). It can cope with speech noise generated from televisions and audio devices that does not originate from the center. Experiments using a humanoid robot, SIG2, with two microphones showed that ou...	robot	Hyun-Don Kim;Jinsung Kim;Kazunori Komatani;Tetsuya Ogata;Hiroshi G. Okuno	2009	Advanced Robotics	10.1163/016918609X12529300552105	voice activity detection;human–robot interaction;speech recognition;computer science;engineering;artificial intelligence;speech processing	Robotics	-11.63720005707302	-90.17044359615504	15940
c4bae7ab8754f70caa4214980c58f2fee89ccbf4	a brain informatics research recommendation system		Finding and learning related research is a necessary work in Brain Informatics studies. However, the keyword-based search on brain and mental big data center often brings a large amount of unnecessary results. It is very difficult to find needed research from those results for researchers. This paper proposes a Brain Informatics research recommendation system based on the Data-Brain and BI provenances. By choosing interest aspects from the Data-Brain and applying the unification of search and reasoning based on Data-Brain interests, the more accurate search can be realized to find really related literatures for supporting systematic Brain Informatics studies.	informatics;recommender system	Jian Han;Jianhui Chen;Han Zhong;Ning Zhong	2014		10.1007/978-3-319-09891-3_20	human–computer interaction	ECom	-47.7088019201752	-63.71740067048525	15955
1eee7cc7c02acc3281ff75053e54707ce038d6b2	a corpus-based approach for robust asr in reverberant environments	automatic speech recognition	In this paper , we discussthe use of artificial room reverberation to increasetheperformanceof automaticspeechrecognition (ASR) systemsin reverberantenclosures.Our approachconsists in trainingacousticmodelsonartificially reverberatedspeechmaterial. In orderto obtainthedesiredreverberatedspeechtraining database, we proposeto usea reverberatingfilter whoseimpulse responseis designedto matchtwo high-level acousticproperties of thetargetreverberantoperatingenvironment,namelytheearlyto-late energy ratio and the reverberationtime. Speechrecognition experimentsin simulatedreverberantenvironmentsshow thatrecognizerstrainedonspeechreverberatedwith theproposed methodoutperformsystemstrainedon cleanspeech,even when channelnormalizationmethodslike CMS and logRASTA-PLP areused. The extensionof our approachto multi-style training is alsoconsidered.	automated system recovery;high- and low-level;pl/p;text corpus	Laurent Couvreur;Christophe Couvreur;Christophe Ris	2000			speech recognition;voxforge;natural language processing;computer science;artificial intelligence	AI	-15.95568657293791	-86.26149050307772	15968
1ae50f5b31c25fe766991cd8b606330df5b9cb5a	parconnect: results from the student cluster competition at sc16		Abstract In this paper, we present results of our reproducibility study conducted as part of the SC16 Student Cluster Competition. The challenge application and its scalability study were described in a SC15 conference paper by Flick et al. [1]. The goal of the challenge was to validate the results obtained by the authors after performing the same algorithms on different data sets on our machine. We identify nine qualitative conclusions made in the performance analysis of the original work and use them to validate the results and judge reproducibility. We discuss the four tasks upon which our reproducibility study was evaluated at the competition and show that our work supports eight conclusions while one is refuted. Detailed analysis is provided in this paper.		Edward Hutter;Chung-Ting Huang	2017	Parallel Computing	10.1016/j.parco.2017.08.002	parallel computing;reproducibility;data mining;computer science;scalability;data set	HPC	-42.77661863108887	-64.33156135615596	15978
13d1f31b0ba3478f6e9f70e2da4ad1800cdd8212	dialog-based online argumentation		• Forum? → Does not scale well for many participants! • Pro-Contra List? → Only for one proposal! • Argumentmaps? → Expert knowledge necessary! ⇒ Current approaches do not scale, are unstructured or too complex! ⇒ Novel approach: Simulation of dialog between participants, whereby the system recycles arguments that are given by users! ⇒ A Dialog-Based Argumentation System for Online Participation ⇒ D-BAS	broadcast auxiliary service;simulation;dialog	Tobias Krauthoff	2016		10.3233/978-1-61499-686-6-33	knowledge management;argumentation theory;natural language processing;artificial intelligence;dialog box;computer science	HCI	-41.300677799621184	-60.40826239168046	15980
7a595800b490ff437ab06fe7612a678d5fe2b57d	improved concept similarity measuring in the visual domain	semantic similarity;human cognition;modeling technique;multimedia retrieval;sampling concept models;image processing;gaussian processes;human cognition semantic similarity visual domain natural language processing multimedia retrieval concept modeling technique data pruning process sampling concept models web images;web images;trees mathematics;noise measurement;visualization;roads;airplanes;lead;data pruning process;trees mathematics data handling gaussian processes image processing;concept hierarchy;image sampling natural language processing information retrieval humans cognition search engines noise measurement information technology asia image retrieval;face;data handling;similarity measure;natural language processing;concept modeling technique;visual domain;noise	Exploring semantic similarity between concepts in visual domain has a wide range of applications such as natural language processing and multimedia retrieval, which in general requires both a large pool of sample images for each concept and a model to capture its visual characteristics. Instead of relying on high quality and large quantity sample data which is very difficult to obtain, in this paper, a novel method is proposed to achieve improvement in measuring concept similarity by incorporating concept modeling technique into data pruning process. At first, a number of sampling concept models are obtained by sampling a subset from the sample dataset of each concept. Then noisy samples are discarded in terms of their probabilities to the sampling concept models. Experimental results on 31,275 web images of 38 concepts defined in LSCOM indicate that the concept similarity obtained through our proposed approach is more coherent to human cognition. A concept hierarchy tree built from the 38 concepts with their similarity further demonstrates the effectiveness of our proposed method.	approximation algorithm;cognition;coherence (physics);display resolution;google map maker;mixture model;natural language processing;sampling (signal processing);semantic similarity	Genliang Guan;Zhiyong Wang;Qi Tian;David Dagan Feng	2009	2009 IEEE International Workshop on Multimedia Signal Processing	10.1109/MMSP.2009.5293285	face;computer vision;lead;semantic similarity;cognition;visualization;image processing;computer science;noise measurement;noise;machine learning;group method of data handling;gaussian process;information retrieval	Vision	-15.285778951914757	-61.85330267434134	15994
880a018f1e0a587b39a4ca9a5c9f6ba4029e2ea1	end-to-end driving via conditional imitation learning		Deep networks trained on demonstrations of human driving have learned to follow roads and avoid obstacles. However, driving policies trained via imitation learning cannot be controlled at test time. A vehicle trained end-to-end to imitate an expert cannot be guided to take a specific turn at an upcoming intersection. This limits the utility of such systems. We propose to condition imitation learning on high-level command input. At test time, the learned driving policy functions as a chauffeur that handles sensorimotor coordination but continues to respond to navigational commands. We evaluate different architectures for conditional imitation learning in vision-based driving. We conduct experiments in realistic three-dimensional simulations of urban driving and on a 1/5 scale robotic truck that is trained to drive in a residential area. Both systems drive based on visual input yet remain responsive to high-level navigational commands.		Felipe Codevilla;Matthias Miiller;Antonio López;Vladlen Koltun;Alexey Dosovitskiy	2018	2018 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2018.8460487	task analysis;engineering;simulation;imitation;cognitive imitation;end-to-end principle	Robotics	-47.612695000902114	-52.96226061262257	15995
b6a04ad9ff4b26b1155981438aa30716ff851d07	practical approach to semantics	semantic web data structures knowledge representation learning artificial intelligence natural language processing;context semantics natural languages resource description framework vocabulary knowledge representation;machine learning knowledge representation semantics natural languages semantic web linked open data;data structures;semantic web;supervised learning practical semantics approach knowledge representation techniques semantic problem implicit context approach explicit context approach linked data concept structured data representation natural language structures semantic categories semantically relevant word representation semantically relevant phrase representation;learning artificial intelligence;knowledge representation;natural language processing	All knowledge representation techniques have to solve the problem of semantics, i.e. how to represent the meaning of the represented knowledge. Basically, there are two possible approaches: explicit and implicit context approaches. Actually, all known knowledge representation techniques are based on the explicit context representation where the meanings of the represented concepts, properties and relations are defined by names. The explicit context approach is illustrated on the example of Linked Data concept for the structured representation of data on the Web. The implicit context representation is based on contexts defined by natural language structures (words, phrases, paragraphs, documents) and the meaning is defined using semantic categories representing semantically relevant words and phrases in different contexts. Unlike the explicit context approach, which requires an effort of specialized experts, the implicit context approach is based on various kinds of learning, where human support is only needed in supervised learning.	knowledge representation and reasoning;linked data;natural language;supervised learning;world wide web	Mladen Stanojevic	2013	Eurocon 2013	10.1109/EUROCON.2013.6625002	natural language processing;knowledge representation and reasoning;multinet;representation term;data structure;computer science;artificial intelligence;machine learning;semantic web;information retrieval	AI	-32.16227762688167	-69.19669181877613	16013
bdde3ef6f07ce4447a567bd7be08f732d5592288	evidence-based management of ambulatory electronic health record system implementation: an assessment of conceptual support and qualitative evidence	medical informatics;information systems;ehr emr implementation;information technology;quality improvement;best practices;ambulatory physician office health;information management systems computerization ambulatory physician office health information technology;information management systems computerization;electronic health records	OBJECTIVES While electronic health record (EHR) systems have potential to drive improvements in healthcare, a majority of EHR implementations fall short of expectations. Shortcomings in implementations are often due to organizational issues around the implementation process rather than technological problems. Evidence from both the information technology and healthcare management literature can be applied to improve the likelihood of implementation success, but the translation of this evidence into practice has not been widespread. Our objective was to comprehensively study and synthesize best practices for managing ambulatory EHR system implementation in healthcare organizations, highlighting applicable management theories and successful strategies.   METHODS We held 45 interviews with key informants in six U.S. healthcare organizations purposively selected based on reported success with ambulatory EHR implementation. We also conducted six focus groups comprised of 37 physicians. Interview and focus group transcripts were analyzed using both deductive and inductive methods to answer research questions and explore emergent themes.   RESULTS We suggest that successful management of ambulatory EHR implementation can be guided by the Plan-Do-Study-Act (PDSA) quality improvement (QI) model. While participants did not acknowledge nor emphasize use of this model, we found evidence that successful implementation practices could be framed using the PDSA model. Additionally, successful sites had three strategies in common: 1) use of evidence from published health information technology (HIT) literature emphasizing implementation facilitators; 2) focusing on workflow; and 3) incorporating critical management factors that facilitate implementation.   CONCLUSIONS Organizations seeking to improve ambulatory EHR implementation processes can use frameworks such as the PDSA QI model to guide efforts and provide a means to formally accommodate new evidence over time. Implementing formal management strategies and incorporating new evidence through the PDSA model is a key element of evidence-based management and a crucial way for organizations to position themselves to proactively address implementation and use challenges before they are exacerbated.		Ann Scheck McAlearney;Jennifer L. Hefner;Cynthia J Sieck;Milisa K Rizer;Timothy R. Huerta	2014	International journal of medical informatics	10.1016/j.ijmedinf.2014.04.002	health informatics;medicine;knowledge management;nursing;data mining;management science;management;law;information technology;information system;best practice	HCI	-60.58344658486781	-62.4519970901707	16022
bf8690451716797282ecbbe21235b0244e41bddd	exploiting a comparability mapping to improve bi-lingual data categorization: a three-mode data analysis perspective		We address in this paper the co-clustering and co-classification of bilingual data laying in two linguistic similarity spaces when a comparability measure defining a mapping between these two spaces is available. A new approach that we can characterized as a three-mode data analysis scheme, is proposed to mix the comparability measure with the two similarity measures. Our aim is to improve jointly the accuracy of classification and clustering tasks performed in each of the two linguistic spaces, as well as the quality of the final alignment of comparable clusters that can be obtained. We used first some purely synthetic random data sets to assess our formal similarity-comparability mixing model. We then propose two variants of the comparability measure that has been defined by (Li & Gaussier, 2010) in the context of bilingual lexicon extraction to adapt it to clustering or categorizing tasks. These two variant measures are subsequently used to evaluate our similarity-comparability mixing model in the context of the co-classification and co-clustering of comparable textual data sets collected from Wikipedia categories for the English and French languages. Our experiments show clear improvements in clustering and classification accuracies when mixing comparability with similarity measures, with, as expected, a higher robustness obtained when the two comparability variant measures that we propose are used. We believe that this approach is particularly well suited for the construction of thematic comparable corpora of controllable quality.	biclustering;bilingual dictionary;categorization;cluster analysis;experiment;lexicon;randomness;synthetic intelligence;text corpus;wikipedia	Pierre-François Marteau;Guiyao Ke	2013	CoRR		data science;machine learning;data mining;information retrieval	NLP	-27.651044072552885	-67.6543054741743	16025
21a305155c43ae6930f2fe0486259b0643f99f0a	neural network based regression for robust overlapping speech recognition using microphone arrays	microphone array;speech recognition;neural network	This paper investigates a neural network based acoustic feature mapping to extract robust features for automatic speech recognition (ASR) of overlapping speech. In our preliminary studies, we trained neural networks to learn the mapping from log mel filter bank energies (MFBEs) extracted from the distant microphone recordings, including multiple overlapping speakers, to log MFBEs extracted from the clean speech signal. In this paper, we explore the mapping of higher order mel-filterbank cepstral coefficients (MFCC) to lower order coefficients. We also investigate the mapping of features from both target and interfering distant sound sources to the clean target features. This is achieved by using the microphone array to extract features from both the direction of the target and interfering sound sources. We demonstrate the effectiveness of the proposed approach through extensive evaluations on the MONC corpus, which includes both non-overlapping single speaker and overlapping multi-speaker conditions.	acoustic cryptanalysis;artificial neural network;beamforming;coefficient;fast fourier transform;filter bank;information management;mel-frequency cepstrum;microphone;modal logic;speech recognition;switzerland	Weifeng Li;John Dines;Mathew Magimai-Doss;Hervé Bourlard	2008			speech recognition;computer science;time delay neural network;artificial neural network	ML	-14.31582996401256	-90.08068771266885	16026
9b0e773b86a0fd465711502d5582dcfe47abf300	enhanced likelihood computation using regression		In a rank based large vocabulary continuous speech recognition system [1], the correct leaf is expected to occupy the top rank positions. An increase in the number of times the correct leaf occurs in the top rank positions translates to an increase in word accuracy. In order to achieve low error rates, we need to discriminate the most confusable incorrect leaves from the correct leaf by lowering their ranks. Therefore, the goal here is to increase the likelihood of the correct leaf of a frame, while decreasing the likelihoods of the confusable leaves. In order to do this, we use the auxiliary information from the prediction of the neighboring frames to augment the likelihood computation of the current frame. We then use the residual errors in the predictions of neighboring frames to discriminate between the correct (best) and incorrect leaves of a given frame. In this paper, we present a new algorithm that incorporates prediction error likelihoods into the overall likelihood computation to improve the rank position of the correct leaf. Experimental results on the Wall Street Journal task and an in-house large vocabulary continuous speech recognition task show a relative accuracy improvements in speaker-independent performance of 10%.	algorithm;cepstrum;computation;naruto shippuden: clash of ninja revolution 3;speech recognition;the wall street journal;unsupervised learning;vector quantization;vocabulary	Peter V. de Souza;Bhuvana Ramabhadran;Yuqing Gao;Michael Picheny	1999			pattern recognition;computation;machine learning;computer science;artificial intelligence;restricted maximum likelihood	NLP	-20.26974453320533	-88.06242848810396	16033
032c0f4f98d0bbe70cd16361027b1606a0ab0680	faceted visualization of three dimensional neuroanatomy by combining ontology with faceted search	software;search engine;brain;imaging three dimensional;biological ontologies;humans;databases factual	In this work, we present a faceted-search based approach for visualization of anatomy by combining a three dimensional digital atlas with an anatomy ontology. Specifically, our approach provides a drill-down search interface that exposes the relevant pieces of information (obtained by searching the ontology) for a user query. Hence, the user can produce visualizations starting with minimally specified queries. Furthermore, by automatically translating the user queries into the controlled terminology our approach eliminates the need for the user to use controlled terminology. We demonstrate the scalability of our approach using an abdominal atlas and the same ontology. We implemented our visualization tool on the opensource 3D Slicer software. We present results of our visualization approach by combining a modified Foundational Model of Anatomy (FMA) ontology with the Surgical Planning Laboratory (SPL) Brain 3D digital atlas, and geometric models specific to patients computed using the SPL brain tumor dataset.	3dslicer;anatomic structures;atlases;bsd;brain neoplasms;brain atlas;cervical atlas;data drilling;documentation;download;fma instruction set;faceted classification;forecast of outcome;foundational model of anatomy ontology;image analysis;imagery;interface device component;medical image computing;modality (human–computer interaction);neuroanatomy;nomenclature;open-source license;open-source software;patients;sql;scalability;scientific visualization;silo (dataset);structured query language	Harini Veeraraghavan;James V. Miller	2013	Neuroinformatics	10.1007/s12021-013-9202-5	computer science;data science;data mining;information retrieval;search engine	Visualization	-50.447903987145935	-63.74364653371289	16102
4b666f401f01521689bc1bb5cb9dea0ae3e13176	negative efficacy of fixed gain error reducing shared control for training in virtual environments	motor skills;virtual training;haptic assistance;virtual reality;motor skill training;shared control;haptic feedback;virtual environment;manual control	Virtual reality with haptic feedback provides a safe and versatile practice medium for many manual control tasks. Haptic guidance has been shown to improve performance of manual control tasks in virtual environments; however, the efficacy of haptic guidance for training in virtual environments has not been studied conclusively. This article presents experimental results that show negative efficacy of haptic guidance during training in virtual environments. The haptic guidance in this study is a fixed-gain error-reducing shared controller, with the control effort overlaid on the dynamics of the manual control task during training. Performance of the target-hitting manual control task in the absence of guidance is compared for three training protocols. One protocol contained no haptic guidance and represented virtual practice. Two protocols utilized haptic guidance, varying the duration of exposure to guidance during the training sessions. Exposure to the fixed-gain error-reducing shared controller had a detrimental effect on performance of the target-hitting task at the conclusion of a month-long training protocol, regardless of duration of exposure. While the shared controller was designed with knowledge of the task and an intuitive sense of the motions required to achieve good performance, the results indicate that the acquisition of motor skill is a complex phenomenon that is not aided with haptic guidance during training as implemented in this experiment.	haptic technology;virtual reality	Yanfang Li;Volkan Patoglu;Marcia Kilchenman O'Malley	2009	TAP	10.1145/1462055.1462058	computer vision;simulation;motor skill;computer science;virtual machine;artificial intelligence;virtual reality;multimedia;haptic technology	Visualization	-44.55157096228528	-52.133109933751314	16109
a9f6fde3d1f58ae899f2ff16488ae73f5a2727da	sentence-level subjectivity detection using neuro-fuzzy models		In this work, we attempt to detect sentencelevel subjectivity by means of two supervised machine learning approaches: a Fuzzy Control System and Adaptive Neuro-Fuzzy Inference System. Even though these methods are popular in pattern recognition, they have not been thoroughly investigated for subjectivity analysis. We present a novel “Pruned ICF Weighting Coefficient,” which improves the accuracy for subjectivity detection. Our feature extraction algorithm calculates a feature vector based on the statistical occurrences of words in a corpus without any lexical knowledge. For this reason, these machine learning models can be applied to any language; i.e., there is no lexical, grammatical, syntactical analysis used in the classification process.	adaptive neuro fuzzy inference system;algorithm;coefficient;feature extraction;feature vector;fuzzy control system;inference engine;machine learning;neuro-fuzzy;parsing;pattern recognition;supervised learning;windows firewall	Samir Rustamov;Elshan Mustafayev;Mark A. Clements	2013			natural language processing;speech recognition;computer science;machine learning;pattern recognition	NLP	-20.898292145216654	-68.88837517443197	16119
5db265e519a01bb07289c9ef98cf9ee1e941f37c	low-cost customized speech corpus creation for speech technology applications		Speech technology applications, such as speech recognition, speech synthesis, and speech dialog systems, often require corpora based on highly customized specifications. Existing corpora available to the community, such as TIMIT and other corpora distributed by LDC and ELDA, do not always meet the requirements of such applications. In such cases, the developers need to create their own corpora. The creation of a highly customized speech corpus, however, could be a very expensive and time-consuming task, especially for small organizations. It requires multidisciplinary expertise in linguistics, management and engineering as it involves subtasks such as the corpus design, human subject recruitment, recording, quality assurance, and in some cases, segmentation, transcription and annotation. This paper describes LDC’s recent involvement in the creation of a low-cost yet highly-customized speech corpus for a commercial organization under a novel data creation and licensing model, which benefits both the particular data requester and the general linguistic data user community.	dialog system;linguistic data consortium;requirement;speech corpus;speech recognition;speech synthesis;speech technology;timit;text corpus;transcription (software);virtual community	Kazuaki Maeda;Christopher Cieri;Kevin Walker	2006			artificial intelligence;timit;speech analytics;natural language processing;computer science;speech corpus;speech technology;segmentation;dialog box;annotation;speech synthesis	NLP	-22.776604600044283	-84.54004758189396	16122
a0a216d4e2487fe5bbc50077c0cfe115d60ad342	effects of speaker variability on processing spoken word form and meaning in short-term priming		Processing of spoken word form and meaning is separately evaluated from short-term repetition and semantic/associative priming experiments to investigate the role of speaker variability in spoken word recognition. The assumption that lexical representation and processing only involve abstract component devoid of stimulus variability is evaluated. The results from the repetition priming experiment show a robust attenuation of phonological form priming by speaker variability. However, the same effect is absent from the semantic priming experiment. These results suggest that the effect of speaker variability on processing spoken language may depend on the depth or level of processing. The time course for speaker variability cannot be confirmed from these two experiments. Different patterns arise from lexical decision and voice discrimination tasks, suggesting the influence of attention factors on speaker variability effect.	experiment;heart rate variability;spatial variability;speaker recognition;speech recognition	Yu Zhang;Chao-Yang Lee	2015			priming (psychology);spoken word;communication;natural language processing;computer science;artificial intelligence	NLP	-10.174020966254764	-81.81584855337695	16128
63c58d8dff70eb8c68258faad13caa9ee8357151	a gpu-based wfst decoder with exact lattice generation		We describe initial work on an extension of the Kaldi toolkit that supports weighted finite-state transducer (WFST) decoding on Graphics Processing Units (GPUs). We implement token recombination as an atomic GPU operation in order to fully parallelize the Viterbi beam search, and propose a dynamic load balancing strategy for more efficient token passing scheduling among GPU threads. We also redesign the exact lattice generation and lattice pruning algorithms for better utilization of the GPUs. Experiments on the Switchboard corpus show that the proposed method achieves identical 1-best results and lattice quality in recognition and confidence measure tasks, while running 3 to 15 times faster than the single process Kaldi decoder. The above results are reported on different GPU architectures. Additionally we obtain a 46-fold speedup with sequence parallelism and multi-process service (MPS) in GPU.	algorithm;beam search;finite-state transducer;graphics processing unit;kaldi;load balancing (computing);parallel computing;pruning (morphology);scheduling (computing);speedup;telephone switchboard	Zhehuai Chen;Justin Luitjens;Hainan Xu;Yiming Wang;Daniel Povey;Sanjeev Khudanpur	2018		10.21437/Interspeech.2018-1339	machine learning;parallel computing;viterbi algorithm;artificial intelligence;speedup;scheduling (computing);decoding methods;security token;computer science;thread (computing);token passing;beam search	NLP	-22.3658178546252	-87.91553855480937	16140
124c52afa4ff178e57368a3ff61b7f64f5d76bcd	serious games for assessment and training in post-stroke robotic upper-limb telerehabilitation		Research shows that better results in post-stroke rehabilitation are obtained when patients receive more intensive therapy. However, the increasing affected population and the limited healthcare resources prevent the provision of intense rehabilitation care. Thus, there is a need for a more autonomous and scalable care provision methods that can be transferred out of the clinic and into home environments. Serious games in combination with robotic rehabilitation can provide an affordable, engaging, and effective way to intensify treatment, both at the clinic and at home. Furthermore, they can offer quantitative assessment of motor performance, allowing individualized treatments and to keep the patient and their therapists informed about therapy progress. Towards this end, a set of games for assessment and training of upper-limb motor impairment after stroke with the ArmAssist is presented. A special effort has been made to design the assessment games in order to be able, not only to measure the effectiveness of the training, but also to compare the assessment results with the standard assessment scales used in the clinic. Feedback from usability testing of previous versions of the system has also been crucial for the final design. Preliminary results of an ongoing clinical testing are presented.	autonomous robot;feedback;rehabilitation robotics;scalability;usability testing;video post-processing	Cristina Rodriguez-de-Pablo;Joel C. Perry;Sivakumar Balasubramanian;Aitor Belloso;Andrej M Savić;Tijana D. Tomic;Thierry Keller	2014		10.5220/0005168601260134	simulation;physical medicine and rehabilitation;physical therapy	HCI	-56.36213379881626	-55.362184045366185	16142
e9587895d1f8c5b8a75eb42a909fb0f330049725	the multisensory effects of atmospheric cues on online shopping satisfaction		This study investigates the way how consumers react to colors and scents as two independent atmospheric cues in stores, given that the two independent variables, were classified into two different levels of cool (blue or citrus-mint) and warm (red or citrus-vanilla) depending on the properties of those. In this study, a 2 (color: blue vs. red) × 3 (scent: no scent vs. cool vs. warm) factorial design was conducted. The results show that ambient cues have an impact on customer emotions such as pleasure and arousal, leading to better shopping satisfaction when they interact together. The results of these sensory interactions indicated that cool visual and olfactory cues received higher ratings than warm cues did.	online shopping	So-Jeong Kim;Dong-Hee Shin	2016		10.1007/978-3-319-39396-4_37	variables;pleasure;social psychology;atmospherics;sensory system;arousal;psychology;factorial experiment	HCI	-52.54534529850297	-52.311358393331695	16149
5b811f91cde62aa84fd6ddeded2c6eae7adb135e	detecting sentiment change in twitter streaming data	conference report	MOA-TweetReader is a real-time system to read tweets in real time, to detect changes, and to find the terms whose frequency changed. Twitter is a micro-blogging service built to discover what is happening at any moment in time, anywhere in the world. Twitter messages are short, and generated constantly, and well suited for knowledge discovery using data stream mining. MOA-TweetReader is a software extension to the MOA framework. Massive Online Analysis (MOA) is a software environment for implementing algorithms and running experiments for online learning from evolving data streams.	algorithm;blog;data stream mining;experiment;moa;plug-in (computing);real-time computing;real-time web;stream (computing);streaming media	Albert Bifet;Geoff Holmes;Bernhard Pfahringer;Ricard Gavaldà	2011			computer science;data mining;internet privacy;world wide web	ML	-23.8004859097979	-54.833518163519585	16158
01b9eb22c628b08d4c0ec70c18342b57fe20bd44	towards perfect text classification with wikipedia-based semantic naïve bayes learning		Abstract This paper suggests a novel way of dramatically improving the Naive Bayes text classifier with our semantic tensor space model for document representation. In our work, we intend to achieve a perfect text classification with the semantic Naive Bayes learning that incorporates the semantic concept features into term feature statistics; for this, the Naive Bayes learning is semantically augmented under the tensor space model where the ‘concept’ space is regarded as an independent space equated with the ‘term’ and ‘document’ spaces, and it is produced with concept-level informative Wikipedia pages associated with a given document corpus. Through extensive experiments using three popular document corpora including Reuters-21578, 20Newsgroups , and OHSUMED corpora, we prove that the proposed method not only has superiority over the recent deep learning-based classification methods but also shows nearly perfect classification performance.	document classification;naive bayes classifier;wikipedia	Han-joon Kim;Jiyun Kim;Jinseog Kim;Pureum Lim	2018	Neurocomputing	10.1016/j.neucom.2018.07.002	naive bayes classifier;machine learning;tensor;artificial intelligence;deep learning;mathematics;pattern recognition;classifier (linguistics)	NLP	-20.0401695170036	-65.67818431849575	16201
d4fa597ce390cf53d76eae2e097371354fa35aa2	it's maybe somewhat difficult but i understand it!		Most studies of implicatures focused on conversational implicatures. This study, however, examined the conventional implicature induced by but. According to the literature, one can assume that the second argument in a ‘p but q’ construction is the argument with the most weight. This is, however, never experimentally tested with a direct distancingcontrastive but. We presented participants with stories which ended with a direct distancing but construction, in which one of the arguments expressed a feeling of understanding towards the behavior of the main character in the story. The results indicated that indeed the q-argument has most weight. There was, however, also an effect of the specific content of the stories. These results are discussed in light of the hypotheses generated on the basis of previous research with an indirect distancing-contrastive but, but also in the light of the effect of content of the stories in conventional implicatures research and specific task characteristics.	experiment	Leen Janssens;Kim Delombaerde;Walter Schaeken	2015			implicature;somewhat difficult;cognitive psychology;distancing;psychology;feeling	HCI	-51.1577796033819	-53.027593063356754	16266
0c9f8491df956b8030f2a81e3a9e6f85af211eac	maintaining the consistency of electronic health record's medication list		An electronic health record (EHR) is a systematic collection of health information about an individual patient. It includes a variety of types of observations entered over time by health care professionals, recording observations and administrations of drugs and therapies, orders for the administration of drugs and therapies, and test results. A well known problem is the consistency of EHR’s medication list: often some of the prescribed drugs are missing, or some information is no more valid, e.g., some prescribed drugs may be replaced by new drugs, or the dosage may be changed. In this paper we have focused on this problem. We have restricted on EHRs and on prescriptions that are applications of the HL7 Reference Information Model (RIM). Further, we have used the Refined Message Information Model (RMIM) to specify the components that are extracted from prescriptions and transmitted into EHR. We have also specified the criteria for medication list’s consistency, and the way it can be maintained. In addition, we have studied the ways the RIM can be used in linking patient’s prescriptions among themselves such that the name of the link indicates its semantics. By viewing patient’s prescriptions as a linked data structure we can improve the effectiveness of prescriptions’ retrieval as well as provide expressive queries on patient’s health documentation.	documentation;health level 7;information model;linked data structure	Juha Puustjärvi;Leena Puustjärvi	2015				Web+IR	-52.69929912321414	-66.86621490160826	16267
caf79f30725cf800ec6ee824fc6efe4092ad5147	power to the patients: the healthnetsocial network		Abstract HealthNet (HN) is a social network that brings together patients with similar health conditions. HN helps users in finding a solution to their health problems by suggesting doctors and health facilities that best fit the patient profile. Indeed, the core component of HN is a recommender system that suggests patients similar to the target user and supports the choice of the doctor and the hospital for a specific condition. The recommendation algorithm first computes similarities among patients, and then generates a ranked list of doctors and hospitals for a given patient profile by exploiting health data shared by the community. The HN typical user can find the most similar patients, can look how they treated their diseases, and can receive suggestions for solving her condition. In order to facilitate the interaction with the system and improve the recommendation step, the patient can express her health status by a natural-language sentence. The system analyzes the sentence and identifies the most relevant medical area (e.g., orthopedics, neurology, allergology, etc.) for that specific case, and uses this information for the recommendation task. Currently HN is in alpha version and only for Italian users, but in the future we want to extend the platform to other languages. We carried out both an in-vitro experimental evaluation to assess the effectiveness of the module for analyzing natural language descriptions provided by users as well as the recommender system to suggest the right doctors for a specific health problem, and an in-vivo evaluation performed by real doctors. Results are really encouraging.		Fedelucio Narducci;Pasquale Lops;Giovanni Semeraro	2017	Inf. Syst.	10.1016/j.is.2017.07.005	recommender system;data mining;database;computer science;patient profile;natural language;social network;ranking;sentence	DB	-54.14510458285475	-67.7373395846856	16271
307e505d3d3be5ca18e616d6f5ad3b5b1a84eb3c	state of the art on methodologies for the development of a metadata application profile	semantic web;interoperability;metadata;semantics;methodologies	This article presents the state of the art on methodologies for the development of a metadata application profile. For this purpose we have performed searches in scientific on-line databases and made other efforts such as global searches on the Web and calls on the mailing lists of the metadata communities to find articles and Web pages about metadata application profiles development and metadata best practices or methodologies. These searches produced 21 items of which 9 have information on how the metadata application profiles were developed. As a result of this analysis we have found small formulas or private recipes for very particular phases of the process, but none is described in detail. We have also found guidelines that were too global and not sufficiently detailed for the metadata application profile development. As far as we could determine, there is no comprehensive methodological support for the metadata application profile development.		Mariana Curado Malta;Ana Alice Baptista	2013	IJMSO	10.1504/IJMSO.2013.058416	interoperability;computer science;semantic web;methodology;data mining;database;semantics;linguistics;metadata;world wide web;metadata repository	NLP	-34.028037899989855	-66.91545354461971	16290
8687992c80a65b6562fe9e7e42eb21102d61a038	a cascaded machine learning approach to interpreting temporal expressions	universiteitsbibliotheek;system performance;machine learning	A new architecture for identifying and interpreting temporal expressions is introduced, in which the large set of complex hand-crafted rules standard in systems for this task is replaced by a series of machine learned classifiers and a much smaller set of context-independent semantic composition rules. Experiments with the TERN 2004 data set demonstrate that overall system performance is comparable to the state-of-the-art, and that normalization performance is particularly good.	context-sensitive language;experiment;machine learning;temporal expressions;text corpus;timex sinclair;word-sense disambiguation	David Ahn;Joris van Rantwijk;Maarten de Rijke	2007			natural language processing;computer science;machine learning;pattern recognition;data mining;computer performance	NLP	-20.499154844444643	-71.75671903565389	16360
09aa0759fbcd362241302d09ff47c09cdd78bbd6	transcribing code-switched bilingual lectures using deep neural networks with unit merging in acoustic modeling	hidden markov models merging speech recognition acoustics speech accuracy neural networks;data imbalance problem code switched bilingual lectures deep neural networks unit merging acoustic modeling bilingual code switched speech guest language host language bf hmm gmm context dependent hmm senones;unit merging code switching deep neural networks bilingual speech recognition;speech recognition hidden markov models	This paper considers the transcription of the widely observed yet less investigated bilingual code-switched speech: the words or phrases of the guest language are inserted within the utterances of the host language, so the languages are switched back and forth within an utterance, and much less data are available for the guest language. Two approaches utilizing the deep neural network (DNN) were tested and analyzed, including using DNN bottleneck features in HMM/GMM (BF-HMM/GMM) and modeling context-dependent HMM senones by DNN (CD-DNN-HMM). In both cases the unit merging (and recovery) techniques in acoustic modeling were used to handle the data imbalance problem. Improved recognition accuracies were observed with unit merging (and recovery) for the two approaches under different conditions.	acoustic cryptanalysis;acoustic model;artificial neural network;brainfuck;context-sensitive language;dos;deep learning;google map maker;hidden markov model;transcription (software)	Ching-feng Yeh;Lin-Shan Lee	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6853590	natural language processing;speech recognition;computer science	Robotics	-18.960827811603767	-87.66067990319179	16395
99cb4539898dadadd0cdc959ce8acc5e5dd3212d	structured log linear models for noise robust speech recognition	large margin training;support vector machines speech recognition;noise robust speech recognition;kernel;model combination;generic model;support vector machines;training;joints;noise robustness;structured log linear models;aurora 2 0 structured log linear models noise robust speech recognition state of the art model based compensation automatic speech recognition support vector machines svm lattice based training;automatic speech recognition;adaptation model;hidden markov models;state of the art model based compensation;speech recognition;aurora 2 0;structured svm discriminative models large margin training speech recognition;svm;support vector machine;structured svm;discriminative model;discriminative models;log linear model;hidden markov models training joints speech recognition noise robustness kernel adaptation model;lattice based training	The use of discriminative models for structured classification tasks, such as speech recognition is becoming increasingly popular. This letter examines the use of structured log-linear models for noise robust speech recognition. An important aspect of log-linear models is the form of the features. By using generative models to derive the features, state-of-the-art model-based compensation schemes can be used to make the system robust to noise. Previous work in this area is extended in two important directions. First, a large margin training of sentence-level log linear models is proposed for automatic speech recognition (ASR). This form of model is shown to be similar to the recently proposed structured Support Vector Machines (SVM). Second, based on the designed joint features, efficient lattice-based training and decoding are performed. This novel model combines generative kernels, discriminative models, efficient lattice-based large margin training and model-based noise compensation. It is evaluated on a noise corrupted continuous digit task: AURORA 2.0.	automated system recovery;discriminative model;feature vector;generative model;linear model;log-linear model;multiclass classification;robustification;speech recognition;structured support vector machine;vocabulary	Shi-Xiong Zhang;Anton Ragni;Mark J. F. Gales	2010	IEEE Signal Processing Letters	10.1109/LSP.2010.2077626	support vector machine;speech recognition;computer science;machine learning;pattern recognition;hidden markov model	ML	-18.671006828335027	-90.91653432153814	16408
f438d0ced65407551d1c44039eb572749a87e281	an eye-tracking evaluation of some parser complexity metrics		Information theoretic measures of incremental parser load were generated from a phrase structure parser and a dependency parser and then compared with incremental eye movement metrics collected for the same temporarily syntactically ambiguous sentences, focussing on the disambiguating word. The findings show that the surprisal and entropy reduction metrics computed over a phrase structure grammar make good candidates for predictors of text readability for human comprehenders. This leads to a suggestion for the use of such metrics in Natural Language Generation (NLG).	eye tracking;natural language generation;parser combinator;parsing;phrase structure grammar;phrase structure rules;self-information;theory	Matthew J. Green	2014		10.3115/v1/W14-1205	natural language processing;parser combinator;speech recognition;computer science;glr parser;programming language;simple lr parser	NLP	-23.04946144368152	-79.33053691979411	16409
44d23d220234b2575d10ca315adb917e712d39be	long-range order in canary song	animals;syntax;syllables;acoustics;vocalization animal;models biological;bird song;birds;reproducibility of results;canaries;models statistical;entropy;markov processes;computational biology;computer simulation	Bird songs range in form from the simple notes of a Chipping Sparrow to the rich performance of the nightingale. Non-adjacent correlations can be found in the syntax of some birdsongs, indicating that the choice of what to sing next is determined not only by the current syllable, but also by previous syllables sung. Here we examine the song of the domesticated canary, a complex singer whose song consists of syllables, grouped into phrases that are arranged in flexible sequences. Phrases are defined by a fundamental time-scale that is independent of the underlying syllable duration. We show that the ordering of phrases is governed by long-range rules: the choice of what phrase to sing next in a given context depends on the history of the song, and for some syllables, highly specific rules produce correlations in song over timescales of up to ten seconds. The neural basis of these long-range correlations may provide insight into how complex behaviors are assembled from more elementary, stereotyped modules.	behavior;buffer overflow protection;nightingale;note (document);passeridae;phrases;rule (guideline);stereotyping;syllable	Jeffrey E. Markowitz;Elizabeth Ivie;Laura Kligler;Timothy J. Gardner	2013		10.1371/journal.pcbi.1003052	computer simulation;computational biology;entropy;speech recognition;syntax;markov process	NLP	-10.87841373913247	-86.25644270672457	16417
6f944100f3de7d043a8dbb5e96b65071a1e59a32	modeling general and specific aspects of documents with a probabilistic topic model	count data;latent variable;dimension reduction;information retrieval;model generation;latent semantic indexing;indexation;mixture distribution	Techniques such as probabilistic topic models and latent-s mantic indexing have been shown to be broadly useful at automatically extracting the topical or semantic content of documents, or more generally for dimension-r eduction of sparse count data. These types of models and algorithms can be viewe d as generating an abstraction from the words in a document to a lower-dimensio nal latent variable representation that captures what the document is generall y about beyond the specific words it contains. In this paper we propose a new probabi listic model that tempers this approach by representing each document as a com bination of (a) a background distribution over common words, (b) a mixture di stribution over general topics, and (c) a distribution over words that are treat ed as being specific to that document. We illustrate how this model can be used for in mation retrieval by matching documents both at a general topic level and at a sp ecific word level, providing an advantage over techniques that only match docu ments at a general level (such as topic models or latent-sematic indexing) or t hat only match documents at the specific word level (such as TF-IDF).	algorithm;count data;emoticon;latent variable;sparse matrix;tf–idf;topic model	Chaitanya Chemudugunta;Padhraic Smyth;Mark Steyvers	2006			latent dirichlet allocation;latent variable;latent semantic indexing;document clustering;mixture distribution;computer science;machine learning;count data;pattern recognition;data mining;topic model;probabilistic latent semantic analysis;information retrieval;statistics;dimensionality reduction	ML	-16.613467754998936	-63.73716068481568	16421
6fe0c65114b3d4ad8c996eb8b815b36ae1b6009f	better evaluation metrics lead to better machine translation	machine translation community;human-judged translation quality;automatic machine translation;bleu-tuned baseline;better machine translation;state-of-the-art machine translation system;translation system;machine translation evaluation metrics;tuning machine translation system;automatic machine translation evaluation;new generation metrics;better evaluation metrics	Many machine translation evaluation metrics have been proposed after the seminal BLEU metric, and many among them have been found to consistently outperform BLEU, demonstrated by their better correlations with human judgment. It has long been the hope that by tuning machine translation systems against these new generation metrics, advances in automatic machine translation evaluation can lead directly to advances in automatic machine translation. However, to date there has been no unambiguous report that these new metrics can improve a state-of-theart machine translation system over its BLEUtuned baseline. In this paper, we demonstrate that tuning Joshua, a hierarchical phrase-based statistical machine translation system, with the TESLA metrics results in significantly better humanjudged translation quality than the BLEUtuned baseline. TESLA-M in particular is simple and performs well in practice on large datasets. We release all our implementation under an open source license. It is our hope that this work will encourage the machine translation community to finally move away from BLEU as the unquestioned default and to consider the new generation metrics when tuning their systems.	bleu;baseline (configuration management);nvidia tesla personal supercomputer;open-source license;open-source software;statistical machine translation	Chang Liu;Daniel Dahlmeier;Hwee Tou Ng	2011			bleu;computer science;artificial intelligence;data science;machine learning;data mining	NLP	-20.3072907671177	-75.70410045931274	16425
1690710333898be12af7c885b96925da850d3d1c	thai monosyllabic words recognition using ant-miner algorithm		In this paper, Ant-Miner software is used to develop classification rules for Thai monosyllabic words. The hypothetical words used in this paper are composed of 65 command monosyllabic Thai words. The binary desired outputs were used during training 520 Thai words consist of 10 numerals and single-syllable, 65 words in each group were used for system evaluation. In order to improve recognition accuracy, initial consonants, vowels, final consonants and tonal level detected were conducted for speech preclassification. The parameters used in the metaheuritstic algorithms are optimized using pruning algorithm with the aim of improving the accuracy by generating minimum number of rule in order to cover more patterns. Thai monosyllabic words recognition using Ant-Miner yielded Thai monosysllabic words accuracy of recognition on test set of 88.65%, 87.69% and 91.54% for 50, 100 and 250 number of ants respectively.	algorithm;algorithmic efficiency;ant colony;database;experiment;feature selection;selection algorithm;statistical classification;syllable;test set	Saritchai Predawan;Chom Kimpan;Chai Wutiwiwatchai	2013	Int. Arab J. Inf. Technol.		speech recognition;computer science;pattern recognition	NLP	-18.51707383257419	-85.83583305000646	16430
79f255b17484ed14a8580d2454f0ce2b14927b7f	mining preorder relation between knowledge units from text	knowledge unit;text;preorder relation;locality;computational complexity	Preorder relation between Knowledge Units (KU) is the precondition for navigation learning. Although possible solutions, existing link mining methods lack the ability of mining preorder relation between knowledge units which are linearly arranged in text. Through the analysis of sample data, we discovered and studied two characteristics of knowledge units: the locality of preorder relation and the distribution asymmetry of domain terms. Based on these two characteristics, a method is presented for mining preorder relation between knowledge units from text documents, which proceeds in three stages. Firstly, the associations between text documents are established according to the distribution asymmetry of domain terms. Secondly, candidate KU-pairs are generated according to the locality of preorder relation. Finally, the preorder relations between KU-pairs are identified by using classification methods. The experimental results show the method can efficiently extract the preorder relation, and reduce the computational complexity caused by the quadratic problem of link mining.	computational complexity theory;locality of reference;precondition;quadratic equation	Jun Liu;Lu Jiang;Zhaohui Wu;Qinghua Zheng;Ya-nan Qian	2010		10.1145/1774088.1774307	computer science;data mining;computational complexity theory;algorithm	NLP	-27.552601087247364	-66.02200657453788	16441
8eeb45f5cd2aebf1ef44dcdeea6a3f5b4caf4965	integrating world knowledge with cognitive parsing: a fine grained, weakly interactive computational approach			commonsense knowledge (artificial intelligence);parsing	Harold Paredes-Frigolett	1996				NLP	-31.904187871016415	-78.86393783079542	16464
bbeb85940cc0e228972244f7a3cf3bfa7f29eb06	a method of automatic hypertext construction from an encyclopedic dictionary of a specific field	specific field;automatic hypertext construction;encyclopedic dictionary	1 Introduction Nowadays, very large volume of texts are created and stored in computer, and as a result the retrieval of texts which fits to a user's demand has become a difficult problem. Hypertext is a typical system to answer this problem , whose primary objective is to establish flexible as-sociative links between relevant text parts and to allow users to select and trace links to see relevant text contents which are connected by links. A difficult problem here is how to construct automatically a network structure in a given set of text data. This paper is concerned with (1) automatic conversion of a plain text set into a hypertext structure, and (2) construction of flexible human interface for the hypertext system. We applied natural language processing methods to locate important conceptual terms in a text corpus and to establish varieties of links between these terms and appropriate text portions. 2 Extraction of thesaurus information The text corpus we handled as a concrete example was the Encyclopedic Dictionary of Computer Science (hereafter abbreviated as EDCS. Iwanami Publ. 1990. En-glish translation will appear soon from Academic Press). It includes 4500 terms and has the text volume of two million Japanese characters (4 Mega bytes). The first part of the term description of EDCS is devoted to synonyms, antonyms, abbreviations and broader concept words. This part has typical sentential styles such as, (ii) A is abbreviated as B. (iv) A stands for B. (v) We call A S {for short}. By finding these sentential patterns the relation between the words A and B is established as follows. (i) p-link is set up from a synonym word to a sentence which defines the synonym relation. (ii) s-link (by synonym) is set up from a defined word to defining words by synonym relation. Typical sentential styles of intensional definition are: (i) A is defined as B. A is regarded as B. (ii) A means B. A connotes B. A is B. By identifying these patterns in a term description part, the relation between the defined word (A) and the definition sentences is established as: (i) p-link is set up from the defined word to the definition sentence when the defined word is not the headword of the term description. This is the case when the defined word is not so important as a headword of the dictionary, and so a rather simple definition description …	byte;computer science;dictionary;emoticon;extensional and intensional definitions;fits;headword;hypertext;natural language processing;text corpus;thesaurus;user interface	Sadao Kurohashi;Makoto Nagao;Satoshi Sato;Masahiko Murakami	1992			natural language processing;speech recognition;machine-readable dictionary;computer science;information retrieval	NLP	-33.83068549162282	-68.51297888076007	16468
bf2ec11d54ebad5f68d2ba4044268f6307f9a7b7	adaptive contour classification of comics speech balloons		Comic books digitization combined with subsequent comic book understanding give rise to a variety of new applications, including content reflowing, mobile reading and multi-modal search. Document understanding in this domain is challenging as comics are semi-structured documents, with semantic information shared between the graphical and textual parts. Speech balloon contour analysis reveals the speech tone which is an essential step towards a fully automatic comics understanding. In this paper we present the first approach for classifying speech balloon in scanned comic books where we separate and analyze their contour variations to classify them as “smooth” (normal speech), “wavy” (thought) or “zigzag” (exclamation). The experiments show a global accuracy classification of 85.2 % on a wide variety of balloons from the eBDtheque dataset.	contour line	Christophe Rigaud;Dimosthenis Karatzas;Jean-Christophe Burie;Jean-Marc Ogier	2013		10.1007/978-3-662-44854-0_5	speech recognition	NLP	-17.768909275830445	-82.2243302856311	16471
f68a3f4d921286d5267a948e387c5057217a0639	integrating feature analysis and background knowledge to recommend similarity functions	recommendation;feature analysis;similarity function	Existing approaches in similarity analysis is little concerned with the right choice of similarity functions. We present an approach for suggesting which similarity functions (e.g., edit distance) are most appropriate for a given similarity search task. We identify data features (e.g., misspellings) that are considerable when choosing similarity functions. We also introduce the concept of similarity function background knowledge that associates data features with similarity functions, and apply the knowledge to recommend suitable similarity functions.		Seung Hwan Ryu;Boualem Benatallah	2012		10.1007/978-3-642-35063-4_52	pattern recognition;semantic similarity;pattern recognition;normalized compression distance;data mining;information retrieval;similarity heuristic	NLP	-32.44987088651676	-59.163920507370996	16482
49fe6138c61a97184c8588fe726b546e72ac2872	bridje over a language barrier: cross-language information access by integrating translation and retrieval	machine translation approach;evaluation result;information distiller;semantic role analysis;language barrier;cross-language information access;new feature;search request translation;bi-directional retriever;bridje system;cross-language ir;machine translation	This paper describes two new features of the BRIDJE system for cross-language information access. The first feature is the partial disambiguation function of the Bi-directional Retriever, which can be used for search request translation in cross-language IR. Its advantage over a “black-box” machine translation approach is consistent across five test collections and across two language permutations: English-Japanese and Japanese-English. The second new feature is the Information Distiller, which performs interactive summarisation of retrieved documents based on Semantic Role Analysis. Our examples illustrate the usefulness of this feature, and our evaluation results show that the precision of Semantic Role Analysis is very high.	black box;cross-language information retrieval;information access;machine translation;query expansion;question answering;word-sense disambiguation	Tetsuya Sakai;Makoto Koyama;Masaru Suzuki;Akira Kumano;Toshihiko Manabe	2003		10.1145/1118935.1118944	computer-assisted translation;natural language processing;transfer-based machine translation;example-based machine translation;computer science;database;rule-based machine translation;information retrieval	NLP	-33.303266295231836	-63.9575085529189	16508
f9ae49807f4ce7a8e966f06a7ef171aa278f6278	automatic determination of chord roots		Even though chord roots constitute a fundamental concept in music theory, existing models do not explain and determine them to full satisfaction. We present a new method which takes sequential context into account to resolve ambiguities and detect nonharmonic tones. We extract features from chord pairs and use a decision tree to determine chord roots. This leads to a quantitative improvement in correctness of the predicted roots in comparison to other models. All this raises the question how much harmonic and nonharmonic tones actually contribute to the perception of chord roots.	correctness (computer science);decision tree;roots	Samuel Rupprechter	2016	CoRR		mathematics;algorithm	NLP	-6.650712221936309	-80.19583272819781	16524
f0043bc3464b17d2e6f2c1c0ade9404ecf36f18f	bayesian network modeling of strokes and their relationships for on-line handwriting recognition	bayesian network;on line handwriting recognition;on line handwritten character recognition;dependency modeling;speech recognition;stroke relationships;stroke model;article;handwritten character recognition;characters;bayesian networks	In this paper, we propose a Bayesian network framework for explicitly modeling strokes and their relationships of characters. A character is modeled as a composition of stroke models, and a stroke as a composition of point models. A point is modeled with 2-D Gaussian distribution for its  X – Y  position. Relationships between points and strokes are modeled as their positional dependencies. All the models and relationships are represented probabilistically in Bayesian networks. The recognition experiment with on-line handwritten digits showed promising results; the recognition errors of the proposed system were greatly reduced by dependency modeling, and its recognition rates were higher than those of previous methods.	bayesian network;handwriting recognition;online and offline	Sung-Jung Cho;Jin H. Kim	2004	Pattern Recognition	10.1016/j.patcog.2003.01.001	speech recognition;computer science;machine learning;pattern recognition;bayesian network	Vision	-21.194584780131102	-89.65148531860221	16561
bbeef848aaaa4277a38c43dc907062078abf2dc3	the pars family of machine translation systems for dutch system description/demonstration	grammar;interfase usuario;linguistique;sistema experto;traduccion automatica;user interface;rule based;base connaissance;dictionnaire;analyse syntaxique;linguistica;traduction automatique;internet;analisis sintaxico;grammaire;syntactic analysis;dictionaries;base conocimiento;interface utilisateur;systeme expert;diccionario;gramatica;machine translation;knowledge base;expert system;automatic translation;linguistics	Lingvistica is developing a family of MT systems for Dutch to and from English, German, and French. PARS/H, a Dutch⇔English system, is a fully commercial product, while PARS/HD, for Dutch⇔German MT, and PARS/HF, for Dutch⇔French, are under way. The PARS/Dutch family of MT systems is based on the rule-based Lingvistica's Dutch morphological-syntactic analyzer and synthesizer dealing with vowel and consonant alterations in Dutch words, as well as Dutch syntactic analysis and synthesis. Besides, a German analyzer and synthesizer have been developed, and a similar French one is being constructed. Representative Dutch and German grammatical dictionaries have been created, comprising Dutch and German words and their complete morphological descriptions: class and subclass characteristics, alteration features, and morphological declension/conjugation paradigms. The PARS/H dictionary editor provides simple dictionary updating. Numerous specialist dictionaries are being and have been created. The user interface integrates PARS/H with MS Word and MS Internet Explorer, fully preserving the corresponding formats. Integrating with MS Excel and many other applications is under way.	machine translation	Edward A. Kool;Michael Blekhman;Andrei Kursin;Alla Rakova	2004		10.1007/978-3-540-30194-3_14	natural language processing;knowledge base;the internet;speech recognition;computer science;artificial intelligence;parsing;grammar;database;linguistics;machine translation;user interface;expert system;algorithm	NLP	-28.591471162927284	-80.83850970803627	16569
14d888ae9c9c1d5bfddb5ce1ddae2405b5c93871	dialogue homme-machine multimodal : de la pragmatique linguistique à la conception de systèmes. (multimodal human-machine dialogue: from pragmatic linguistics to system design)		Human-machine dialogue aims at providing natural dialogue in natural language, i.e., allowing the user to speak using his own language (natural language), following a structure of moves and exchanges that is similar to human dialogue structure (natural dialogue). Related research works feed on linguistics, which analyses language markers, and pragmatics, which analyses language use in context. Two important sectors of linguistic pragmatics focus on referring phenomena, for instance referring to the objects that are accessible in the dialogue context, and speech acts, or dialogue acts, i.e., communicative actions carried out by utterances. We present our modelling and formalizing works dealing with these two aspects and with their application to dialogue systems, particularly when a visual scene is implied or when co-verbal gestures are processed together with speech (multimodal dialogue). Human-machine dialogue also aims at facilitating the design and development of effective systems, with methodologies and means like software architectures. We present our theoretical and practical works in this way, illustrated by our participation to several European projects. We then propose some future works that focus on the integration of linguistic and pragmatic phenomena like salience and ambiguity to human-machine dialogue.		Frédéric Landragin	2013				NLP	-13.67608169934238	-77.19002386773234	16579
01b93c8ef3c2b33a9f6decca882b6f21ed8908e5	degeneracy results in canalisation of language structure: a computational model of word learning		There is substantial variation in language experience between learners, yet there is surprising similarity in the language structure they eventually acquire. While it is possible that this canalisation of language structure may be due to constraints imposed by modulators, such as an innate language system, it may instead derive from the broader, communicative environment in which language is acquired. In this paper, the latter perspective is tested for its adequacy in explaining the robustness of language learning to environmental variation. A computational model of word learning from cross-situational, multimodal information was constructed and tested. Key to the model’s robustness was the presence of multiple, individually unreliable information sources that could support learning when combined. This “degeneracy” in the language system had a detrimental effect on learning when compared to a noise-free environment, but was critically important for acquiring a canalised system that is resistant to environmental noise in communication.	computational model;degeneracy (graph theory);multimodal interaction	Padraic Monaghan	2016			psychology;cognitive psychology;canalisation;degeneracy (mathematics)	NLP	-7.310877207364106	-78.87380649781262	16603
296a3d834e7c2c5d77f708813eda781b9d052ac6	using early quizzes to predict student outcomes in online introductory biomedical informatics courses		Identifying students at risk for poor performance in large online classes can be challenging. We determined whether the first few quiz scores can be used to identify students who will have poor course outcomes in an introductory informatics class. Mean scores on the first four quizzes can identify students at risk for failure. Even the first quiz score significantly predicted introductory informatics course outcome. Automating early identification of students likely to fail may allow instructors to create targeted interventions. Introduction: Distance learning enabled by the Web is one approach to extend the geographic reach of existing informatics training. Regardless of their career goals, many students start with an introductory informatics course. As a result, more introductory informatics courses are offered online to ever increasing numbers of students with diverse backgrounds and career goals. As the number of students enrolled in each course increases it becomes more difficult to offer personal assistance to every student. If “at risk” students can be automatically identified early, instructors may be able to intervene and rescue students who otherwise would earn a failing grade or drop the course. We determined that the first four weekly quizzes could be used to identify students at risk for failing or dropping an introductory informatics course within the first month of the semester. Methods: We reviewed data from two online introductory informatics courses presented at the University of Texas School of Biomedical Informatics (UT) at Houston and the Medical Informatics program at the University of West Florida (UWF). For UT we used all students beginning with Spring 2007 and ending with Spring 2010, for a total of 205 graduate students. For UWF we used the Fall 2009 semester with a total of 42 students, 28 undergraduates and 14 graduates. We built the prediction model based on UT data and then validated the model using UWF data. We compared automated identification of students at risk for failure to manual identification by a faculty member familiar with the course design and content (TRJ), but not aware of the individual student outcomes. Results: We considered students who scored an average of 75% or below on the weekly quizzes to be at risk of NSC. We chose 75% as the quiz score threshold because this was the highest score where the predictor’s false positives were equal to its false negatives. Even the first quiz score is a significant predictor of course outcome. Each successive quiz added to the model improved the model’s performance. ROC = Receiver Operator Characteristic, PPV/NPV = Positive/Negative Predictive Value. Discussion: While representing only 6% to 8% of the total grade, the first four quizzes are highly predictive for course outcome. Using only the first two quizzes available by the UT add/drop deadline still allows prediction, but with a lower PPV. Conclusion: A simple threshold model developed at UT predicted non-successful completion at another institution offering an introductory course, UWF. Automated prediction compares favorably to human instructor prediction. Acknowledgements: The authors thank all instructors and students who contributed data to this study, including Stephanie Reedy at UWF for help with data collection. This work was funded in part by NCATS Grant UL1 TR000371 establishing the Center for Clinical and Translational Sciences at the University of Texas at Houston. Week(s) Area under ROC Curve (AUC) PPV	failure;informatics;kerrison predictor;national supercomputer centre in sweden;receiver operating characteristic;threshold model;world wide web	Irmgard Willcockson;Jorge R. Herskovic;Melanie A. Sutton;Robert E. Hoyt;Craig W. Johnson;Todd R. Johnson;Elmer V. Bernstam	2013			data collection;medical education;informatics;multimedia;distance education;false positive paradox;health informatics;psychological intervention;computer science	ML	-60.707622949524804	-66.52720296700313	16605
d1b73b142d7aaddfb3f03ab718167c78d06275de	characterising emergent semantics in twitter lists	informatica	synsets appear in the top of the hierarchy, while more specific ones are placed at the bot tom. Thus, Wu and Palmer [26] propose a similarity measure which includes the depth of the synsets and of the least common subsumer (see equation 1). The least common subsumer les is the deepest hypernym tha t subsumes both synsets, and depth is the length of the pa th from the root to the synset. This similarity range between 0 and 1, the larger the value the greater the similarity between the terms. For terms measure and communication, both synsets have depth 4, and the depth of the les abstraction is 3; therefore, their similarity is 0.75. wp(synseti, synset^) = 2 * depth(lcs)/(depth(synseti) + depth(synset2) (1) Jiang and Conrath [16] propose a distance measure tha t combines hierarchical and distributional information. Their formula includes features such as local network density (i.e., children per synset), synset depth, weight according to the link type, and information content IC of synsets and of the least common subsumer. The information content of a synset is calculated as the inverse log of its probability of occurrence in the WordNet hierarchy. This probability is based on the frequency of words subsumed by the synset. As the probability of a synset increases, its information content decreases. Jiang and Conrath distance can be computed using equation 2 when only the information content is used. A shorter distance means a stronger semantic relation. The IC of measure and communication is 2.95 and 3.07 respectively while abstraction has a IC of 0.78, thus their semantic distance is 4.46. jc(synseti, synset^) = IC(synseti) + ICi^synset^) — 2 * IC(lcs) (2) We use, in section 4, the pa th length, Wu and Palmer similarity, and Jiang and Conrath distance to study the semantics of the relations extracted from Twitter lists using the vector space model and LDA. 3.2 Linked D a t a to Ident i fy R e l a t i o n T y p e s WordNet-based analysis is rather limited, since WordNet contains a small number of relations between synsets. To overcome this limitation and improve the detection of relationships, we use general purpose knowledge bases such as DBpedia [4], OpenCyc, and UMBEL 3 , which provide a wealth of well-defined relations between concepts and instances. DBpedia contains knowledge from Wikipedia for close to 3.5 million resources and more than 600 relations. OpenCyc is a general purpose knowledge base with nearly 500K concepts around 15K types of relations. UMBEL is an ontology with 28,000 concepts and 38 relations. These knowledge bases are published as linked data [3] in R D F and with links between them: DBpedia resources, and classes are connected to OpenCyc concepts using owhsameAs, and to UMBEL concepts using umbel#correspondsTo. Our aim is to bind keywords extracted from list names to semantic resources in these knowledge bases so tha t we can identify which kind of relations appear between them. To do so we harness the high degree of interconnection in the linked data cloud offered by DBpedia. We first ground keywords to DBpedia [12], and then we browse the linked data set for relations connecting the keywords. After connecting keywords to DBpedia resources we query the linked data set to search for relations between pairs of resources. We use a similar approach to [14] where SPARQL queries are used to search for relations linking two resources rs and rt. We define the pa th length L as the number of objects found in the pa th linking rs with rt. For L = 2 we look for a relation^ linking rs with rt. As we do not know the direction of relation^, we search in both directions: 1) rs relationi rt, and 2) rt relation^ rs. For L = 3 we look for a pa th containing two relationships and an intermediate resource node such as: rs relationi node, and node relationj rt. Note tha t each relationship may have two directions and hence the number of possible paths is 2 = 4. For L = 4 we have three relationship placeholders and the number of possible paths is 2 3 = 8. In general, for a pa th length L we have n = ^4=2 2('~) possible paths tha t can be traversed by issuing the same number of SPARQL queries on the linked data set. For instance, let us find the relation between the keywords Anthropology and Sociology. First both keywords are grounded to the respective DBpedia resources, in this case dbpr:Anthropology and dbpr:Sociology. Figure 2 shows linked data relating these DBpedia resources. To retrieve this information, we pose the query shown in Listing l . l . 5 The result is the triples making up the pa th between 2 OpenCyc home page: http://sw.opencyc.org/ 3 UMBEL home page: http://www.umbel.org/ 4 Note that for large L values the queries can last long time in large data sets. 5 Property paths, in SPARQL 1.1 specification, allow simplifying these queries. the resources. In our case we discard the initial owhsameAs relation between DBpedia and OpenCyc resources, and keep the assertion tha t Anthropology and Sociology are Social Sciences. Keyword f ~\ Keyword anthropology ^w v _ y sociology grounding rd f : type / opencyc: \rdf:type grounding , . / social science \ , . f owl:sameAs / \ owl:sameAs T dbpr:Anthropology opencyc :anthropology opencyc :sociology dbpr: Sociology Fig. 2. Linked data showing the relation between the anthropology and sociology SELECT * WHERE{<dbpr:Anthropology> ? r e l a t i o n l ?nodel. ?nodel ?re la t ion2 ?node2. <dbpr:Sociology> ?re la t ion4 ?node3. ?node3 ?re la t ion3 ?node2.} Listing 1.1. SPARQL query for finding relations between two DBpedia resources 4 Experiment Description D a t a Set: Twitter offers an Application Programming Interface (API) for data collection. We collected a snowball sample of users and lists as follows. Starting with two initial seed users, we collected all the lists they subscribed to or are members of. There were 260 such lists. Next, we expanded the user layer based on current lists by collecting all other users who are members of or subscribers to these lists. This yielded an additional set of 2573 users. In the next iteration, we expanded the list layers by collecting all lists tha t these users subscribe to or are members of. In the last step, we collected 297,521 lists under which 2,171,140 users were classified. The lists were created by 215,599 distinct curators, and 616,662 users subscribe to them 6 . From list names we extracted, by approximate matching of the names with dictionary entries, 5932 unique keywords; 55% of them were found in WordNet. The dictionary was created from article titles and redirection pages in Wikipedia. O b t a i n i n g R e l a t i o n s from Lists: For each keyword we created the vectors and the bags of words for each of the three user-based representations defined in section 2. We calculated cosine similarity in the corresponding user-based vector space. We also run the LDA algorithm over the bags of words and calculated the cosine similarity between the topic distribution produced for each document. We kept the 5 most similar terms for each keyword according to the Vector-space and LDA-based similarities. The data set can be found here: http://goo.gl/vCYyD	application programming interface;approximation algorithm;artificial intelligence;biocurator;browsing;cosine similarity;cyc;dbpedia;dictionary;emergent;home page;ident protocol;integrated circuit;interconnection;iteration;knowledge base;linear algebra;linear discriminant analysis;link relation;linked data;local-density approximation;lowest common ancestor;ontology (information science);ontology components;sparql;self-information;similarity measure;synonym ring;tag cloud;tom;umbel;wikipedia;wordnet	Andrés García-Silva;Jeon-Hyung Kang;Kristina Lerman;Óscar Corcho	2012		10.1007/978-3-642-30284-8_42	computer science;data mining;database;information retrieval	AI	-28.380079964241215	-63.30404272223638	16615
42c0088ca646c9908f51a2800bfcfde101ab8a7d	feature selection for support vector machines in text categorization.	feature selection;support vector machine;text categorization		categorization;document classification;feature selection;support vector machine	Yi Liu;Haiming Lu;Zengxiang Lu;Pu Wang	2003			feature vector;boosting methods for object categorization;pattern recognition;relevance vector machine	ML	-21.101683254035994	-64.75128056329324	16651
c52d9bf3a616bcb5ffd30dfb8b79906c5017c8f1	capturing behavioral changes of elderly people through unobtruisive sensing technologies	senior citizens;sensors;smart phones;urban areas;statistics;diseases	The behavioral analysis of individuals is an important science, especially if it is conducted on the elderly population, aiming to prevent Mild Cognitive Impairment (MCI) and frailty problems. A fundamental aspect in this context is to explore the use of innovative technologies enabling the Internet of Things (IoT), above all sensors, to unobtrusively capture personal data for automatically recognizing behavioral changes in elderly people. This is done with the aim to timely identify risks of MCI and frailty before they escalate into more serious conditions such as Alzheimer Disease. This paper aims to briefly describe the overall goal of the City4Age project, funded by the Horizon 2020 Programme of the European Commission, in particular focusing on the IoT-based personal data capturing system.	algorithm;automatic identification and data capture;cognitive science;internet of things;personally identifiable information;quorum sensing;sensor	Luca Mainetti;Luigi Patrono;Piercosimo Rametta	2016	2016 24th International Conference on Software, Telecommunications and Computer Networks (SoftCOM)	10.1109/SOFTCOM.2016.7772126	simulation;engineering;multimedia;computer security	SE	-61.05655591679907	-54.7175397085268	16662
1e4d1157ddd9af8dcc7ebbff526493dcd314d396	the merl spokenquery information retrieval system a system for retrieving pertinent documents from a spoken query	keyboards;spoken query based information retrieval system;technological innovation;information retrieval system;uncertainty;information retrieval;uncertainty vectors;vocabulary;information retrieval speech recognition;automatic signature inclusion;indexing method;personal digital assistants;engines;indexing;mitsubishi electric research labs;indexation;information retrieval systems;merl spokenquery information retrieval system;speech recognition;document retrieval;speech recognizer vocabulary;information retrieval systems merl spokenquery information retrieval system document retrieval spoken query based information retrieval system mitsubishi electric research labs automatic signature inclusion speech recognizer vocabulary uncertainty vectors indexing method;information retrieval speech recognition vocabulary engines uncertainty technological innovation indexing keyboards personal digital assistants cellular phones;cellular phones	This papers describes some key concepts developed and used in the design of a spoken-query based information retrieval system developed at the Mitsubishi Electric Research Labs (MERL). Innovations in the system include automatic inclusion of signature terms of documents in the recognizer’s vocabulary, the use of uncertainty vectors to represent spoken queries, and a method of indexing that accommodates the usage of uncertainty vectors. This paper describes these techniques and includes experimental results that demonstrate their effectiveness.	ccir system a;finite-state machine;information retrieval;relevance;vocabulary	Peter Wolf;Bhiksha Raj	2002		10.1109/ICME.2002.1035591	natural language processing;document retrieval;search engine indexing;speech recognition;uncertainty;computer science;information retrieval	Web+IR	-22.65018859499609	-83.80421234977743	16663
71fee4ec097f211ac81cc2f813abcf6e9087864b	a signature-based bag of visual words method for image indexing and search	image search;content based image retrieval;image retrieval	We formalize the concept of Signature-based Bag of Visual Words (SBoVW) methods.We present a detailed study of parameters required by SDLC, a SBoVW method.We analyzed the impact of distinct weighting schemes on SDLC results.We compare SDLC to well-known Cluster-Based Bag of Visual Words methods.With a proper configuration, we improve the SDLC in terms of quality and performance. In this paper, we revisit SDLC, an image retrieval method that adopts a signature-based approach to identify visual words, instead of the more conventional approach that identifies them by using clustering techniques. We start by providing a formal and generalized definition of the approach adopted in SDLC, which we call Signature-Based Bag of Visual Words. After that, we present a detailed study of SDLC parameters and experiments with distinct weighting schemes used to compute the ranking of results, comparing the method to well-known cluster-based bag of visual words approaches. When compared to the initial proposal of SDLC, the choice of different parameters and a new weighting scheme allowed us to considerably reduce the size of the textual representation generated by the method, reducing also the indexing times and the query processing times in all collections adopted in the experiments. Further, the SDLC outperforms the baselines in most of these collections.	bag-of-words model in computer vision	Joyce Miranda dos Santos;Edleno Silva de Moura;Altigran Soares da Silva;João M. B. Cavalcanti;Ricardo da Silva Torres;Márcio L. A. Vidal	2015	Pattern Recognition Letters	10.1016/j.patrec.2015.06.023	image retrieval;computer science;pattern recognition;data mining;world wide web;information retrieval	Vision	-28.879025757150597	-61.35165891738377	16669
63ac777af60e6fcd1a105f7001fce499bb3a3d2e	low-resource semantic role labeling		We explore the extent to which highresource manual annotations such as treebanks are necessary for the task of semantic role labeling (SRL). We examine how performance changes without syntactic supervision, comparing both joint and pipelined methods to induce latent syntax. This work highlights a new application of unsupervised grammar induction and demonstrates several approaches to SRL in the absence of supervised syntax. Our best models obtain competitive results in the high-resource setting and state-ofthe-art results in the low resource setting, reaching 72.48% F1 averaged across languages. We release our code for this work along with a larger toolkit for specifying arbitrary graphical structure.1	grammar induction;graphical user interface;pipeline (computing);semantic role labeling;treebank	Matthew R. Gormley;Margaret Mitchell;Benjamin Van Durme;Mark Dredze	2014			natural language processing;computer science;machine learning;programming language	NLP	-22.35290321884095	-74.91241268663352	16680
059274bf890d79970fab17ad570103dad9507dc6	speech intelligibility prediction using a neurogram similarity index measure	speech intelligibility;simulated performance intensity function;journal article;medical engineering;ssim;nsim;auditory periphery model	Discharge patterns produced by fibres from normal and impaired auditory nerves in response to speech and other complex sounds can be discriminated subjectively through visual inspection. Similarly, responses from auditory nerves where speech is presented at diminishing sound levels progressively deteriorate from those at normal listening levels. This paper presents a Neurogram Similarity Index Measure (NSIM) that automates this inspection process, and translates the response pattern differences into a bounded discrimination metric. Performance Intensity functions can be used to provide additional information over measurement of speech reception threshold and maximum phoneme recognition by plotting a test subject’s recognition probability over a range of sound intensities. A computational model of the auditory periphery was used to replace the human subject and develop a methodology that simulates a real listener test. The newly developed NSIM is used to evaluate the model outputs in response to Consonant-Vowel-Consonant (CVC) word lists and produce phoneme discrimination scores. The simulated results are rigorously compared to those from normal hearing subjects in both quiet and noise conditions. The accuracy of the tests and the minimum number of word lists necessary for repeatable results is established and the results are compared to predictions using the speech intelligibility index (SII). The experiments demonstrate that the proposed Simulated Performance Intensity Function (SPIF) produces results with confidence intervals within the human error bounds expected with real listener tests. This work represents Email address: hinesa@tcd.ie (Andrew Hines) Preprint submitted to Speech Communication September 12, 2011 an important step in validating the use of auditory nerve models to predict speech intelligibility.	algorithm;allen boothroyd;dictionary attack;diff utility;dual ec drbg;epub;enea ose;execution unit;human error;intelligibility (philosophy);lars arge;lex (software);list of tor hidden services;non-functional requirement;ork;re-order buffer;sed;simulation;sly 3: honor among thieves;tempest (codename);virtual reality	Andrew Hines;Naomi Harte	2012	Speech Communication	10.1016/j.specom.2011.09.004	speech recognition;computer science;structural similarity;linguistics;intelligibility	ML	-12.214132701883724	-83.89832970822873	16702
0843ad40acf836b4b2a992a445bb0a904d253c00	on mispronunciation lexicon generation using joint-sequence multigrams in computer-aided pronunciation training (capt)	joint-sequence multigrams;lexicon extension;mispronunciation detection and diagnosis	We investigate the use of joint-sequence multigrams to generate L2 mispronunciation lexicons for mispronunciation detection and diagnosis. In the joint-sequence framework, a pair of parallel strings (namely, the input string of either graphemes or phonemes of the canonical pronunciation and the phonetic string of the mispronunciation) are aligned to form joint units for probabilistic estimation. We compare results on lexicons produced by phoneme-to-mispronunciation conversion and those by grapheme-to-mispronunciation conversion. Results reflect the hypothesized advantage (1.1% reduction in expected miss rate) in unifying phonetic confusion due to L1 negative transfer with those due to grapheme-to-phoneme errors. The impact of mispronunciation by mis-use of analogy is also studied. Recognition results show the benefit of a lexicon with proper priors.	lexicon	Xiaojun Qian;Helen M. Meng;Frank K. Soong	2011			confusion;computer-aided;speech recognition;lexicon;probabilistic logic;natural language processing;prior probability;analogy;negative transfer;pronunciation;computer science;artificial intelligence	NLP	-19.375114591838877	-84.74221445792304	16716
b647322e057fb59d1c80810c5023cb674f180afe	refining image annotation based on object-based semantic concept capturing and wordnet ontology	semantic similarity;cluster algorithm;pattern clustering;block based structure clustering;object based semantic concept capturing;image annotation refinement;wordnet ontology;image annotation;feature space;corel dataset image annotation refinement object based semantic concept capturing wordnet ontology feature extraction block based structure clustering semantic similarity content based image retrieval;noise measurement;distance measurement;multi feature space;feature extraction;ontologies clustering algorithms humans computer science image retrieval content based retrieval image analysis fuzzy systems feature extraction information retrieval;corel dataset content based image retrieval image annotation multi feature space wordnet;pattern clustering content based retrieval feature extraction image retrieval natural language processing;clustering algorithms;corel dataset;ontologies;wordnet;refining;content based image retrieval;content based retrieval;titanium;natural language processing;image retrieval	This paper presents a novel approach to automatically refining the original annotations of images. An existing image annotation method is used to obtain the candidate annotations for an image in advance. Then, low-level features are extracted automatically from all blocks in the image to construct a suitable multi-feature space. Next, the image is divided into nonoverlapping block-based structures and a block-based structure clustering algorithm to capture the semantic concept of object as accepted annotations is proposed. Based on these accepted annotations, the irrelevant annotations are pruned according to the semantic similarity in WordNet. Experimental results on the typical Corel dataset show that the approach outperforms the existing image annotation refining techniques.	algorithm;automatic image annotation;cluster analysis;feature vector;high- and low-level;object-based language;relevance;semantic similarity;wordnet	Liu Zheng;Ma Jun	2008	2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2008.242	titanium;wordnet;semantic similarity;refining;feature vector;feature extraction;image retrieval;computer science;noise measurement;ontology;machine learning;pattern recognition;data mining;cluster analysis;information retrieval	Vision	-14.517440190302217	-59.693425955276396	16718
c4d45060f45bbabef4e158b553cf71a7db76ce23	a tool for generating synthetic authorship records for evaluating author name disambiguation methods	digital library;author name ambiguity;artigo publicado em periodico;author name disambiguation;bibliographic citation;synthetic generator	0020-0255/$ see front matter 2012 Elsevier Inc http://dx.doi.org/10.1016/j.ins.2012.04.022 ⇑ Corresponding author at: Departamento de Ciên E-mail addresses: ferreira@dcc.ufmg.br (A.A. F dcc.ufmg.br (A.H.F. Laender), adrianov@dcc.ufmg.br 1 Here regarded as a set of bibliographic informati particular article. The author name disambiguation task has to deal with uncertainties related to the possible many-to-many correspondences between ambiguous names and unique authors. Despite the variety of name disambiguation methods available in the literature to solve the problem, most of them are rarely compared against each other. Moreover, they are often evaluated without considering a time evolving digital library, susceptible to dynamic (and therefore challenging) patterns such as the introduction of new authors and the change of researchers’ interests over time. In order to facilitate the evaluation of name disambiguation methods in various realistic scenarios and under controlled conditions, in this article we propose SyGAR, a new Synthetic Generator of Authorship Records that generates citation records based on author profiles. SyGAR can be used to generate successive loads of citation records simulating a living digital library that evolves according to various publication patterns. We validate SyGAR by comparing the results produced by three representative name disambiguation methods on real as well as synthetically generated collections of citation records. We also demonstrate its applicability by evaluating those methods on a time evolving digital library collection generated with the tool, considering several dynamic and realistic scenarios. 2012 Elsevier Inc. All rights reserved.	bibliographic record;digital library;internationalized domain name;many-to-many;simulation;synthetic intelligence;word-sense disambiguation	Anderson A. Ferreira;Marcos André Gonçalves;Jussara M. Almeida;Alberto H. F. Laender;Adriano Veloso	2012	Inf. Sci.	10.1016/j.ins.2012.04.022	digital library;computer science;artificial intelligence;data mining;world wide web;information retrieval	AI	-26.276594045631082	-58.92512606617094	16736
1ec5f8166c6f8f5af178b50d0ea81bbbde975df4	a comparison of linear and non-linear calibrations for speaker recognition	article in monograph or in proceedings	In recent work on both generative and discriminative score to log-likelihood-ratio calibration, it was shown that linear transforms give good accuracy only for a limited range of operating points. Moreover, these methods required tailoring of the calibration training objective functions in order to target the desired region of best accuracy. Here, we generalize the linear recipes to non-linear ones. We experiment with a nonlinear, non-parametric, discriminative PAV solution, as well as parametric, generative, maximum-likelihood solutions that use Gaussian, Student’s T and normal-inverse-Gaussian score distributions. Experiments on NIST SRE’12 scores suggest that the non-linear methods provide wider ranges of optimal accuracy and can be trained without having to resort to objective function tailoring.	discriminative model;loss function;nist hash function competition;nonlinear system;optimization problem;speaker recognition	Niko Brümmer;Albert Swart;David A. van Leeuwen	2014	CoRR		speech recognition;computer science;machine learning;mathematics;statistics	ML	-16.534215176189456	-95.29435810586668	16752
5af268b755e4987b5412ce6344876d387c98f572	exploring flow, factors, and outcomes of temporal event sequences with the outflow visualization	outflow visualization technique;outflow;data visualization information analysis sequential analysis layout image color analysis;models theoretical;computer graphics;soccer;temporal event sequences;aggregate pathway;sequential analysis;information visualization;event progression pathway;surveys and questionnaires;layout;event sequence data;image processing computer assisted;event state;data visualisation;time factors;heart failure;image color analysis;state transition outflow information visualization temporal event sequences state diagram;pathway state transition;data visualization;state diagram;emr;pathway state transition temporal event sequence event sequence data electronic medical record emr sports event event progression pathway outflow visualization technique aggregate pathway event state cardinality;humans;temporal event sequence;user computer interface;electronic medical record;cardinality;sports event;information analysis;state transition	Event sequence data is common in many domains, ranging from electronic medical records (EMRs) to sports events. Moreover, such sequences often result in measurable outcomes (e.g., life or death, win or loss). Collections of event sequences can be aggregated together to form event progression pathways. These pathways can then be connected with outcomes to model how alternative chains of events may lead to different results. This paper describes the Outflow visualization technique, designed to (1) aggregate multiple event sequences, (2) display the aggregate pathways through different event states with timing and cardinality, (3) summarize the pathways' corresponding outcomes, and (4) allow users to explore external factors that correlate with specific pathway state transitions. Results from a user study with twelve participants show that users were able to learn how to use Outflow easily with limited training and perform a range of tasks both accurately and rapidly.	aggregate data;cardinality;cessation of life;color gradient;electronic health records;electronics, medical;gene regulatory network;imagery;scientific visualization;sports;usability testing	Krist Wongsuphasawat;David Gotz	2012	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2012.225	layout;cardinality;real-time computing;state diagram;simulation;computer science;sequential analysis;data mining;mathematics;data analysis;computer graphics;data visualization;outflow;statistics	Visualization	-6.7011552979385085	-60.2342542168879	16787
b9c4f2d0315a506fa958b3abcf09cafb0a8419a7	platform independent visualization of dicom-datasets in 3-d visualmedija		VisualMediJa is a tool for the administration, 2-D and 3- D visualization and diagnostics of DICOM-conform images. It offers a Graphical User Interface (GUI), which allows multiple interactions with images, e.g. choosing of a point of view on a 3-D object or laying virtual image slices through the images. The GUI is freely configurable. By the positioning of acquired or even virtual image slices a differentiated anatomical analysis becomes feasible. Topographic details may be viewed in independent 2-D or 3-D windows and may be optimally arranged for the finding. Views, pan-shots and documentations can be stored.	dicom	Stefan Maas;Heinrich M. Overhoff	2004			simulation;computer science;world wide web;computer graphics (images)	Visualization	-9.944450966863352	-53.73665523646498	16795
1f802eb184cb7f7659c89f87b55119b20e851311	speeding up reinforcement learning-based information extraction training using asynchronous methods		RLIE-DQN is a recently proposed Reinforcement Learning-based Information Extraction (IE) technique which is able to incorporate external evidence during the extraction process. RLIE-DQN trains a single agent sequentially, training on one instance at a time. This results in significant training slowdown which is undesirable. We leverage recent advances in parallel RL training using asynchronous methods and propose RLIE-A3C. RLIEA3C trains multiple agents in parallel and is able to achieve upto 6x training speedup over RLIE-DQN, while suffering no loss in average accuracy.	algorithm;information extraction;microsoft research;natural language processing;reinforcement learning;speedup	Aditya Sharma;Zarana Parekh;Partha P. Talukdar	2017			artificial intelligence;natural language processing;computer science;machine learning;information extraction;reinforcement learning;asynchronous communication	NLP	-15.547876264118772	-75.33924724764715	16836
a9ba2ce375f39a8b8768848fda2bca1b16a7eb4e	prognosis essay scoring and article relevancy using multi-text features and machine learning	computer assisted assessment;text mining;data mining;article relevancy;prediction model	This study develops a model for essay scoring and article relevancy. Essay scoring is a costly process when we consider the time spent by an evaluator. It may lead to inequalities of the effort by various evaluators to apply the same evaluation criteria. Bibliometric research uses the evaluation criteria to find relevancy of articles instead. Researchers mostly face relevancy issues while searching articles. Therefore, they classify the articles manually. However, manual classification is burdensome due to time needed for evaluation. The proposed model performs automatic essay evaluation using multi-text features and ensemble machine learning. The proposed method is implemented in two data sets: a Kaggle short answer data set for essay scoring that includes four ranges of disciplines (Science, Biology, English, and English language Arts), and a bibliometric data set having IoT (Internet of Things) and non-IoT classes. The efficacy of the model is measured against the Tandalla and AutoP approach using Cohen’s kappa. The model achieves kappa values of 0.80 and 0.83 for the first and second data sets, respectively. Kappa values show that the proposed model has better performance than those of earlier approaches.	attribute–value pair;automated essay scoring;bibliometrics;binary file;common criteria;internet of things;interpreter (computing);item unique identification;machine learning;nonlinear gameplay;relevance;text mining	Arif Mehmood;Byung-Won On;Ingyu Lee;Gyu Sang Choi	2017	Symmetry	10.3390/sym9010011	text mining;computer science;artificial intelligence;data science;data mining;mathematics;predictive modelling	AI	-45.28155583861156	-69.87997095007277	16839
98582dad19f8ca6ee45d447ba4e9941cde676fd0	data-driven foot-based intonation generator for text-to-speech synthesis	intonation modeling;text to speech synthesis;prosody	We propose a method for generating F0 contours for text-tospeech synthesis. Training speech is automatically annotated in terms of feet, with features indicating start and end times of syllables, foot position, and foot length. During training, we fit a foot-based superpositional intonation model comprising accent curves and phrase curves. During synthesis, the method searches for stored, fitted accent curves associated with feet that optimally match to-be-synthesized feet in the feature space, while minimizing differences between successive accent curve heights. We tested the proposed method against the HMMbased Speech Synthesis System (HTS) by imposing contours generated by these two methods onto natural speech, and obtaining quality ratings. Test sets varied in how well they were covered by the training data. Contours generated by the proposed method were preferred over HTS-generated contours, especially for poorly-covered test items. To test the new method’s usefulness for processing marked-up text input, we compared its ability to convey contrastive stress with that of natural speech recordings, and found no difference. We conclude that the new method holds promise for generating comparatively highquality F0 contours, especially when training data are sparse and when mark-up is required.	contour line;feature vector;high-throughput satellite;natural language;sparse matrix;speech synthesis;test set	Mahsa Sadat Elyasi Langarani;Jan P. H. van Santen;Seyed Hamidreza Mohammadi;Alexander Kain	2015			speech recognition;linguistics;prosody	ML	-18.713783961044733	-83.95433096766875	16878
85025a3f76b3bf6bc9eec247d1f97c5c3f93a9a2	segmental intelligibility of synthetic and natural speech in real and nonsense words			intelligibility (philosophy);natural language;synthetic intelligence	Rolf Carlson;Björn Granström;Lennart Nord	1990			nonsense;speech recognition;computer science;intelligibility (communication)	ML	-13.44939083309012	-84.5446126531503	16883
b7422518e928ea678e7d828609a84235953248a2	realityconvert: a tool for preparing 3d models of biochemical structures for augmented and virtual reality		Motivation There is a growing interest for the broad use of Augmented Reality (AR) and Virtual Reality (VR) in the fields of bioinformatics and cheminformatics to visualize complex biological and chemical structures. AR and VR technologies allow for stunning and immersive experiences, offering untapped opportunities for both research and education purposes. However, preparing 3D models ready to use for AR and VR is time-consuming and requires a technical expertise that severely limits the development of new contents of potential interest for structural biologists, medicinal chemists, molecular modellers and teachers.   Results Herein we present the RealityConvert software tool and associated website, which allow users to easily convert molecular objects to high quality 3D models directly compatible for AR and VR applications. For chemical structures, in addition to the 3D model generation, RealityConvert also generates image trackers, useful to universally call and anchor that particular 3D model when used in AR applications. The ultimate goal of RealityConvert is to facilitate and boost the development and accessibility of AR and VR contents for bioinformatics and cheminformatics applications.   Availability and implementation http://www.realityconvert.com.   Contact dfourch@ncsu.edu.   Supplementary information Supplementary data are available at Bioinformatics online.	3d modeling;accessibility;augmented reality;bioinformatics;body dysmorphic disorders;cheminformatics;display resolution;experience;geographic information systems;physical object;preparation;programming tool;vr - veterans rand health survey;virtual reality;web site;contents - htmllinktype	Alexandre Borrel;Denis Fourches	2017	Bioinformatics	10.1093/bioinformatics/btx485	computer science;human–computer interaction;virtual reality	Visualization	-8.142125574997603	-56.75038300869471	16913
08093dcd6bf8554e490e8323077a0722903c5b00	templates from syntax to morphology: affix ordering in qafar		The functional head suggests that verbs acquire their inflectional properties by moving from one head position to the next in the syntactic derivation. A problem arises as affixes’ ordering is not sensitive to syntactic properties, as it is the case in Qafar. This Cushitic language exhibits two verbal classes depending on whether verbs can have prefixes. I argue that the hierarchical structure of template corresponds to the syntactic structure. Phonological constraints on templates formation activate adequate syntactic operations. If we assume that templatic domains lie at the interface between syntax and phonology, we account for some issues of affix ordering, that involve no syntactic property.	galaxy morphological classification	Pierre Rucart	2006			natural language processing;computer science;linguistics;communication	NLP	-10.56267731971563	-78.6240138230976	16919
d5455a76b69803a4cf25d2e0a03ac9aeba43192c	identifying the subject of documents in digital libraries automatically using frequently occurring words - study and findings	frequently-occurring words;catalog;identify topics of documents;web documents classification;search results list;cataloging;digital library	Contemporary information databases contain millions of electronic documents. The immense number of documents makes it difficult to conduct efficient searches on the Internet. Several studies have found that associating documents with a subject or list of topics can make them easier to locate online [5] [6] [7]. Effective cataloging of information is performed manually, requiring extensive resources. Consequently, at present most information is not cataloged. This paper will present the findings of a study based on a software tool (TextAnalysis) that automatically identifies the subject of a document. We tested documents in two subject categories: geography and family studies. The present study follows an earlier one that examined the subject categories of industrial management and general management.	boolean algebra;database;digital library;document;information retrieval;internet;programming tool	Offer Drori	2003			signal;dial;database;digital library;electrical engineering;electronic circuit;computer science	Web+IR	-35.609530217349345	-65.58797668322404	16938
994bd979bb800be3fe661dbcdbb97b2becb33b85	a personalized method of literature recommendation based on brain informatics provenances		Systematic Brain Informatics (BI) depends on a lot of prior knowledge, from experimental design to result interpretation. Scientific literatures are a kind of important knowledge source. However, it is difficult for researchers to find really useful references from a large number of literatures. This paper proposes a personalized method of literature recommendation based on BI provenances. By adopting the interest retention model, user models can be built based on the Data-Brain and BI provenances. Furthermore, semantic similarity is added into traditional literature vector modeling for obtaining literature models. By measuring similarity between the user models and literature models, the really needed literatures can be obtained. Results of experiments show that the proposed method can effectively realize a personalized literature recommendation according to BI researchers’ interests.	informatics	Ningning Wang;Ning Zhong;Jian Han;Jianhui Chen;Han Zhong;Taihei Kotake;Dongsheng Wang;Jianzhuo Yan	2015		10.1007/978-3-319-23344-4_17	alternative medicine;medicine;data mining;world wide web	NLP	-47.68789167049832	-63.700571591290604	16952
88dec013723e32a43d2e642795cd4b629016534a	body-part nouns and whole-part relations in portuguese		In this paper, we target the extraction of whole-part relations involving human entities and body-part nouns occurrences in texts using STRING, a hybrid statistical and rule-based Natural Language Processing chain for Portuguese. Whole-part relation is a semantic relation between an entity that is perceived as a constituent part of another entity, or a member of a set.	entity;f1 score;logic programming;natural language processing;ontology components;stratified sampling;text corpus	Ilia Markov;Nuno J. Mamede;Jorge Baptista	2014		10.1007/978-3-319-09761-9_13	natural language processing;speech recognition;art;linguistics	NLP	-28.169934203541988	-76.94378002754904	17021
5c40bea83ac620a5b73825061d4c0d373564136a	beyond a joke: types of conversational humour		Abstract#R##N##R##N#The main objective of this article is to list and briefly characterise several semantic and pragmatic types of verbal humour, primarily those which cannot be reduced to (canned) jokes. First of all, a distinction is drawn between jokes and conversational humour, an umbrella term covering a variety of semantic and pragmatic types of humour, which recur in interpersonal communication, whether real-life (everyday conversations or TV programmes) or fictional (film and book dialogues). On a different axis representing formal structure, stylistic figures are distinguished, such as irony, puns and allusions.		Marta Dynel	2009	Language and Linguistics Compass	10.1111/j.1749-818X.2009.00152.x	psychology;linguistics;communication;social psychology;literature	NLP	-34.05510215741766	-79.7989724468574	17023
86d9ce98cccb6e6b348a4ed3d53b4d99f669a933	the effect of frame rate and video information redundancy on the perceptual learning of american sign language gestures	american sign language;sign language;rule of thumb;mental representation;biological motion;signal detection theory;perceptual learning;gesture recognition	AN experiment lS reported that addressed whether reductions of frame rate and information redundancy attectec{ the recognition ot American Sign Lang~~age (AS L ) geStLlt’e S that were presented in a multimedia tormat. Frame rate (30, 15,5, & 1 frames-per-second or tps ) prim~wily affected tmle ncedeci to learn the geshlres to criterion while point light presentation of gestures (versus conveotio nal video) affected recognition rates in a transfer testing condition. Contrary to conventional frame rate rLIles ot’ thumb (e.:., 10-20 fps), once trained participants were exceptional at recognizing ASL geshwes even at rates as (C>W as 5 and 1 fps. Results are discussed as they conrri bate to computer mediated learning of sign langLlage and t’rcmle rate guicielines,	redundancy (information theory);sculpted prim;wily	B. F. Johnson;J. K. Caird	1996		10.1145/257089.257200	natural language processing;speech recognition;biological motion;sign language;computer science;mental representation;gesture recognition;perceptual learning;rule of thumb;gesture;detection theory	HCI	-6.52527687145315	-80.50349723899768	17060
2c0e1b5db1b6851d95a765a2264bb77f19ee04e1	topic aware neural response generation.		We consider incorporating topic information into a sequenceto-sequence framework to generate informative and interesting responses for chatbots. To this end, we propose a topic aware sequence-to-sequence (TA-Seq2Seq) model. The model utilizes topics to simulate prior human knowledge that guides them to form informative and interesting responses in conversation, and leverages topic information in generation by a joint attention mechanism and a biased generation probability. The joint attention mechanism summarizes the hidden vectors of an input message as context vectors by message attention and synthesizes topic vectors by topic attention from the topic words of the message obtained from a pre-trained LDA model, with these vectors jointly affecting the generation of words in decoding. To increase the possibility of topic words appearing in responses, the model modifies the generation probability of topic words by adding an extra probability item to bias the overall distribution. Empirical studies on both automatic evaluation metrics and human annotations show that TA-Seq2Seq can generate more informative and interesting responses, significantly outperforming state-of-theart response generation models.	evaluation function;evaluation of machine translation;information;simulation	Chen Xing;Wei Chung Wu;Yu Ping Wu;Jie Liu;Yalou Huang;Ming Zhou;Wei-Ying Ma	2017			natural language processing;machine learning;empirical research;artificial intelligence;conversation;computer science;joint attention;decoding methods	NLP	-15.46878736394949	-72.53340392938891	17061
7e2e50fb2258ef5750ed6ee3586a55ea0d1d840b	phonetic properties of dutch accent lending pitch movements under time pressure				J. Caspars;Vincent J. van Heuven	1992			artificial intelligence;speech recognition;pattern recognition;computer science	HCI	-13.965669426654388	-85.07866027348581	17068
8a5b7bba4fa1ce57009fadacd77f9b8656b35bab	lost in discussion? tracking opinion groups in complex political discussions by the example of the fomc meeting transcriptions		The Federal Open Market Committee (FOMC) is a committee within the central banking system of the US and decides on the target rate. Analyzing the positions of its members is a challenge even for experts with a deep knowledge of the financial domain. In our work, we aim at automatically determining opinion groups in transcriptions of the FOMC discussions. We face two main challenges: first, the positions of the members are more complex as in common opinion mining tasks because they have more dimensions than pro or contra. Second, they cannot be learned as there is no labeled data available. We address the challenge using graph clustering methods to group the members, including the similarity of their speeches as well as agreement and disagreement they show towards each other in discussions. We show that our approach produces stable opinion clusters throughout successive meetings and correlates with positions of speakers on a dove-hawk scale estimated by experts.	cluster analysis;unsupervised learning	Cäcilia Zirn;Robert Meusel;Heiner Stuckenschmidt	2015			labeled data;sentiment analysis;politics;data mining;transcription (linguistics);clustering coefficient;open market operation;computer science	ML	-21.764486164984092	-68.96398211646054	17095
5fb6fae9b05fe46760e535156fe8872c322eed9b	human reading based strategies for off-line arabic word recognition	specific paper;paper summarizes technique;off-line arabic word recognition;local verification sim;global memorization;human reading;complex script;interactive mechanism;view concern	This paper summarizes some techniques proposed for off-line Arabic word recognition. The point of view developed here concerns the human reading favoring an interactive mechanism between global memorization and local checking making easier the recognition of complex scripts as Arabic. According to this consideration, some specific papers are analyzed and their strategies commented.	complex text layout;hidden markov model;high- and low-level;human-based computation;online and offline;relevance;semantics (computer science);text segmentation;vocabulary	Abdel Belaïd;Christophe Choisy	2006		10.1007/978-3-540-78199-8_3	natural language processing;speech recognition;computer science;linguistics	HCI	-32.24784780749357	-79.86828834642914	17099
b6ebb55d227719438bca2d97435731714cbd6a0a	decisional dna for modeling and reuse of experiential clinical assessments in breast cancer diagnosis and treatment	nova;clinical decision support systems;semantic technologies;research repository;university of newcastle;decisional dna;breast cancer diagnosis and treatment;set of experience knowledge structure;institutional repository;research online	Clinical Decision Support Systems (CDSS) are active knowledge resources that use patient data to generate case specific advice. The fast pace of change of clinical knowledge imposes to CDSS the continuous update of the domain knowledge and decision criteria. Traditional approaches require costly tedious manual maintenance of the CDSS knowledge bases and repositories. Often, such an effort cannot be assumed by medical teams, hence maintenance is often faulty. In this paper, we propose a (semi-)automatic update process of the underlying knowledge bases and decision criteria of CDSS, following a learning paradigm based on previous experiences, such as the continuous learning that clinicians carry out in real life. In this process clinical decisional events are acquired and formalized inside the system by the use of the SOEKS and Decisional DNA experiential knowledge representation techniques. We propose three algorithms processing clinical experience to: (a) provide a weighting of the different decision criteria, (b) obtain their fine-tuning, and (c) achieve the formalization of new decision criteria. Finally, we present an implementation instance of a CDSS for the domain of breast cancer diagnosis and treatment.	dna barcoding	Eider Sanchez;Peng Wang;Carlos Toro;Cesar Sanín;Manuel Graña;Edward Szczerbicki;Eduardo Carrasco;Frank Guijarro;Luis Brualla	2014	Neurocomputing	10.1016/j.neucom.2014.06.032	clinical decision support system;computer science;knowledge management;data mining;management science;semantic technology;nova	Comp.	-52.94708210596369	-67.64705854675468	17122
0f25eae22109007b09ea8f6c3d279a0f19d775d6	calibration and weight of the evidence by human listeners. the atvs-uam submission to nist human-aided speaker recognition 2010	likelihood ratio;speaker recognition maximum likelihood estimation;human listeners;forensic speaker recognition;maximum likelihood estimation;speaker recognition;hasr atvs uam submission nist human aided speaker recognition calibration human lay listeners speech evidence speaker recognition process;conferenceobject;humans;bookpart;calibration;nist hasr forensic speaker recognition likelihood ratio calibration human listeners;nist hasr	This work analyzes the performance of speaker recognition when carried out by human lay listeners. In forensics, judges and jurors usually manifest intuition that people is proficient to distinguish other people from their voices, and therefore opinions are easily elicited about speech evidence just by listening to it, or by means of panels of listeners. There is a danger, however, since little attention has been paid to scientifically measure the performance of human listeners, as well as to the strength with which they should elicit their opinions. In this work we perform such a rigorous analysis in the context of NIST Human-Aided Speaker Recognition 2010 (HASR). We have recruited a panel of listeners who have elicited opinions in the form of scores. Then, we have calibrated such scores using a development set, in order to generate calibrated likelihood ratios. Thus, the discriminating power and the strength with which human lay listeners should express their opinions about the speech evidence can be assessed, giving a measure of the amount of information given by human listeners to the speaker recognition process.	computer forensics;manifest (transportation);speaker recognition	Daniel Ramos-Castro;Javier Franco-Pedroso;Joaquín González-Rodríguez	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5947706	speaker recognition;speaker diarisation;calibration;speech recognition;likelihood-ratio test;computer science;pattern recognition;mathematics;maximum likelihood;statistics	Vision	-11.334979366931272	-84.72492421253466	17130
f28e85dad4f162afeed6aa04f91395bb04f62b19	exploring the query halo effect in site search: leading people to longer queries		People tend to type short queries, however, the belief is that longer queries are more effective. Consequently, a number of attempts have been made to encourage and motivate people to enter longer queries. While most have failed, a recent attempt - conducted in a laboratory setup - in which the query box has a halo or glow effect, that changes as the query becomes longer, has been shown to increase query length by one term, on average. In this paper, we test whether a similar increase is observed when the same component is deployed in a production system for site search and used by real end users. To this end, we conducted two separate experiments, where the rate at which the color changes in the halo were varied. In both experiments users were assigned to one of two conditions: halo and no-halo. The experiments were ran over a fifty day period with 3,506 unique users submitting over six thousand queries. In both experiments, however, we observed no significant difference in query length. We also did not find longer queries to result in greater retrieval performance. While, we did not reproduce the previous findings, our results indicate that the query halo effect appears to be sensitive to performance and task, limiting its applicability to other contexts.	color;experiment;glow;information needs;production system (computer science);unique user;web search query	Djoerd Hiemstra;Claudia Hauff;Leif Azzopardi	2017		10.1145/3077136.3080696	data mining;end user;a/b testing;computer science;information retrieval;halo;range query (database);limiting	Web+IR	-35.06164836080046	-53.371554713570816	17136
8dbfdc3dc9be2ad9a57275332494283402e61549	learning from noisy label proportions for classifying online social data	social networks;text classification;machine learning	Inferring latent attributes (e.g., demographics) of social media users is important to improve the accuracy and validity of social media analysis methods. While most existing approaches use either heuristics or supervised classification, recent work has shown that accurate classification models can be trained using supervision from population statistics. These learning with label proportion (LLP) models are fit on bags of instances and then applied to individual accounts. However, it is well known that many social media sites such as Twitter are not a representative sample of the population; thus, there are many sources of noise in these label proportions (e.g., sampling bias). This can in turn degrade the quality of the resulting model. In this paper, we investigate classification algorithms that use population statistical constraints such as demographics, names, and social network followers to fit classifiers to predict individual user attributes. We propose LLP methods that explicitly model the noise inherent in these label proportions. On several real and synthetic datasets, we find that combining these enhancements together can significantly reduce averaged classification error by 7%, resulting in methods that are robust to noise in the provided label proportions.		Ehsan Mohammady Ardehaly;Aron Culotta	2017	Social Network Analysis and Mining	10.1007/s13278-017-0478-6	data mining;sampling (statistics);population;heuristics;social network;social media;statistical classification;machine learning;sampling bias;computer science;population statistics;artificial intelligence	ML	-18.4654955922801	-64.7278236695391	17152
cc4a87aaeffb12f868e3894720882660f04140a1	remedy: supporting consumer-centered medication information search	text analysis consumer behaviour information retrieval medical information systems search engines;health sensemaking;popular online web search interfaces remedy consumer centered medication information search consumer centered medication information search system text processing techniques probabilistic generative topic modeling approaches consumer recognition quality indicators relevance indicators medication related search results;search engines;information retrieval;text analysis;navigation;health sensemaking www health information search;medical information systems;consumer behaviour;health information search;www	Remedy is a prototype consumer-centered medication-information search system, developed by applying text-processing techniques and probabilistic generative topic-modeling approaches. Our prototype system includes tools to increase consumer recognition of quality and relevance indicators in medication-related search results. Attendees will be able to use Remedy to search for a range of over-the-counter and prescription medications, and can compare the search experience with that of popular online web search interfaces.	prototype;relevance;topic model;web search engine	Lauren Wilcox;Steven K. Feiner;Noémie Elhadad;David K. Vawdrey;Tran H. Tran	2013	2013 7th International Conference on Pervasive Computing Technologies for Healthcare and Workshops	10.4108/icst.pervasivehealth.2013.252102	search engine indexing;navigation;text mining;semantic search;computer science;data mining;multimedia;search analytics;information retrieval;consumer behaviour;search engine;human–computer information retrieval	SE	-33.32691022429021	-52.8390996061938	17157
1a9a5aabc010b59d80af678034c9e206ca0a8d91	using snomed ct to identify a crossmap between two classification systems: a comparison with an expert-baseda data-driven strategy		A crossmap between successive versions of classification systems is necessary to maintain the continuity of health care documentation. A reference terminology can serve as an intermediary to support this task. Within this study we evaluated the use of SNOMED CT to create a crossmap between two versions of an intensive care classification system. Firstly, the SNOMED CT crossmap was compared with an expert-based and a data-driven crossmap. Next, the influence of these crossmap strategies on the health care outcome was evaluated. For 50% of the analyzed cases, the three mapping strategies resulted in the same crossmaps. In other cases, there was an overlap between the SNOMED CT crossmaps and the crossmaps provided by one of the two other strategies. Differences in the crossmap results had however no significant influence on the health care outcomes. SNOMED CT can be used as an intermediary to solve the problem of crossmapping between versions of classification systems.		Ferishta Bakhshi-Raiez;Ronald Cornet;Robert-Jan Bosman;Hans Joore;Nicolette F. de Keizer	2010	Studies in health technology and informatics	10.3233/978-1-60750-588-4-1035		HCI	-55.96098378213131	-67.10212120466466	17164
c30798aa981071b5474dd7ac7f7c15533ecd3dda	using a trie-based structure for question analysis	perforation;look ahead	This paper presents an approach for question analysis that defines the question subject and its required answer type by building a triebased structure from a set of question patterns. The question analysis consists of comparing the question tokens with the path of nodes in the trie. A look-ahead process solve the mismatches of unknown words by assigning a entity-type or semantically linking them with other question words. The developed approach is evaluated using different datasets showing that its performance is comparable with state-of-the-art systems.	trie	Luiz Augusto Sangoi Pizzato	2004			algorithm	NLP	-27.187352940218716	-70.36458473466209	17171
4025fead6cd707e8159dc052532ed0ed3e0f245d	a pipeline approach to supervised error correction for the qalb-2014 shared task		This paper describes our submission to the ANLP-2014 shared task on automatic Arabic error correction. We present a pipeline approach integrating an error detection model, a combination of characterand word-level translation models, a reranking model and a punctuation insertion model. We achieve an F1 score of 62.8% on the development set of the QALB corpus, and 58.6% on the official test set.	error detection and correction;f1 score;pipeline (computing);test set	Nadi Tomeh;Nizar Habash;Ramy Eskander;Joseph Le Roux	2014		10.3115/v1/W14-3614	natural language processing;speech recognition;computer science;machine learning;pattern recognition;data mining	NLP	-23.041887433345032	-76.067740416155	17187
d958cf056531dde2a591d7477303aa67440b1c2e	siabo - semantic information access through biomedical ontologies	noun phrase;data mining;semantic information;domain modelling;natural language;ontology engineering;domain ontology;natural language processing;formal ontology	The scientific aim of the project presented in this paper is to provide an approach to representing, organizing, and accessing conceptual content of biomedical texts using a formal ontology. The ontology is based on UMLS resources supplemented with domain ontologies developed in the project. The approach introduces the notion of ‘generative ontologies’, i.e., ontologies providing increasingly specialized concepts reflecting the phrase structure of natural language. Furthermore, we propose a novel so-called ‘ontological semantics’ which maps noun phrases from texts and queries into nodes in the generative ontology. This enables an advanced form of data mining of texts identifying paraphrases and concept relations and measuring distances between key concepts in texts. Thus, the project gains its identity in its attempt to provide a formal underpinning of conceptual similarity or relatedness of meaning.	data mining;formal ontology;gödel's ontological proof;information access;map;natural language;ontology (information science);organizing (structure);phrase structure rules;search algorithm;software prototyping	Troels Andreasen;Henrik Bulskov;Tine Lassen;Sine Zambach;Per Anker Jensen;Bodil Nistrup Madsen;Hanne Erdman Thomsen;Jørgen Fischer Nilsson;Bartlomiej Antoni Szymczak	2009			natural language processing;upper ontology;open biomedical ontologies;ontology alignment;semantic similarity;semantic computing;noun phrase;ontology components;bibliographic ontology;ontology inference layer;computer science;ontology;linguistics;ontology-based data integration;natural language;owl-s;information retrieval;process ontology;suggested upper merged ontology	AI	-32.59401189887219	-70.23140008057335	17197
dc0eda916ef3dec129ee49b0461ce86601fc5367	investigate into the cognitive motivation of the derivative meaning of words: the case of luojuan		Taking the “裸捐(Luojuan; nakedly donate)” as an example and insisting on CCMO(Cognition-Combinatory Meaning Outlook, CCMO), this paper has completed the new segmentation and description on the structure of word meaning, and holds that the structure of word meaning constituted by the three elements, that is “referent meaning(RM), attribute meaning(AM) and feature-value meaning(FVM)”, and this paper also hold that the strong expressive power of word meaning dues to the varied possibilities of the combinations among the elements of meaning’s structure. The different assignments and combinations of the elements of meaning’s structure have brought about the new derivations of word meaning. And the psychology of cognitive metaphor plays an important and basic role in the different assignments and combinations of three elements of the meaning’s structure. Taking three elements of the meaning’s structure of “裸捐(Luojuan; nakedly donate)”as an example, its feature-value meanings are different each other, and based on the figurative abstraction and the cognitive metaphor, those different feature-value meanings all have been unified and symbolized as a symbol of “裸(Luo; nakedly)”, which led to the richness and magic beauty of the meaning of “裸捐(Luojuan; nakedly donate)”.		Qingshan Qiu	2014		10.1007/978-3-319-14331-6_21	psychology;cognitive psychology;developmental psychology;social psychology	NLP	-8.526735199018582	-76.26628855351831	17203
58ef1c1a9b5d781490c7fff6bc108544bc90d463	reordering rules for english-hindi smt		Reordering is a preprocessing stage for Statistical Machine Translation (SMT) system where the words of the source sentence are reordered as per the syntax of the target language. We are proposing a rich set of rules for better reordering. The idea is to facilitate the training process by better alignments and parallel phrase extraction for a phrase-based SMT system. Reordering also helps the decoding process and hence improving the machine translation quality. We have observed significant improvements in the translation quality by using our approach over the baseline SMT. We have used BLEU, NIST, multireference word error rate, multi-reference position independent error rate for judging the improvements. We have exploited open source SMT toolkit MOSES to develop the system.	bleu;baseline (configuration management);bayesian network;compiler;decoding methods;moses;open-source software;position-independent code;preprocessor;statistical machine translation;word error rate	Raj Nath Patel;Rohit Gupta;Prakash B. Pimpale;M Sasikumar	2013			natural language processing;speech recognition;computer science;programming language	NLP	-22.015129743238187	-77.31830694341129	17234
56e6496a2042214ddac83b3ee984432844bcdd46	generating news headlines with recurrent neural networks		We describe an application of an encoder-decoder recurrent neural network with LSTM units and attention to generating headlines from the text of news articles. We find that the model is quite effective at concisely paraphrasing news articles. Furthermore, we study how the neural network decides which input words to pay attention to, and specifically we identify the function of the different neurons in a simplified attention mechanism. Interestingly, our simplified attention mechanism performs better that the more complex attention mechanism on a held out set of articles.	artificial neural network;encoder;long short-term memory;machine translation;recurrent neural network;samy (computer worm)	Konstantin Lopyrev	2015	CoRR		speech recognition;artificial intelligence;machine learning	ML	-18.11502166195413	-71.9514271716959	17251
949d80351bb649ab85c8c9d7a3c936e7d6d23bdc	turksent: a sentiment annotation tool for social media		In this paper, we present an annotation tool developed specifically for manual sentiment analysis of social media posts. The tool provides facilities for general and target based opinion marking on different type of posts (i.e. comparative, ironic, conditional) with a web based UI which supports synchronous annotation. It is also designed as a SaaS (Software as a Service). The tool’s outstanding features are easy and fast annotation interface, detailed sentiment levels, multi-client support, easy to manage administrative modules and linguistic annotation capabilities.	conditional (computer programming);item unique identification;sentiment analysis;social media;software as a service;user interface	Gülsen Eryigit;Fatih Samet Çetin;Meltem Yanik;Tanel Temel;Ilyas Çiçekli	2013			computer science;data mining;world wide web;information retrieval	NLP	-35.9828802214324	-74.33811903954752	17281
b81c9a3f774f839a79c0c6e40449ae5d5f49966a	talla at semeval-2017 task 3: identifying similar questions through paraphrase detection		This paper describes our approach to the SemEval-2017 shared task of determining question-question similarity in a community question-answering setting (Task 3B). We extracted both syntactic and semantic similarity features between candidate questions, performed pairwise-preference learning to optimize for ranking order, and then trained a random forest classifier to predict whether the candidate questions were paraphrases of each other. This approach achieved a MAP of 45.7% out of max achievable 67.0% on the test set.	bitext word alignment;kernel method;map;microsoft word for mac;preference learning;question answering;random forest;relevance;semantic similarity;synapomorphy;test set;unbalanced circuit	Byron Galbraith;Bhanu Pratap;Daniel Shank	2017		10.18653/v1/S17-2062	computer science;artificial intelligence;natural language processing;semeval;paraphrase	NLP	-22.615323738117205	-71.80945062897682	17343
89aeff769fc1c0697a02f9ba021b1403379b833c	feature-driven recognition of music styles	base donnee;automatic system;numerical method;information retrieval;musica;database;base dato;armonica;harmonic;multimedia application;separability;classification;musique;separabilidad;sistema automatico;harmonique;metodo numerico;indexing;computer music;indexation;indizacion;systeme automatique;pattern recognition;autoorganizacion;self organization;separabilite;feature selection;reconnaissance forme;reconocimiento patron;music;clasificacion;autoorganisation;methode numerique;self organising map	In this paper the capability of using self-organising neural maps (SOM) as music style classifiers of musical fragments is studied. From MIDI files, the monophonic melody track is extracted and cut into fragments of equal length. From these sequences, melodic, harmonic, and rhythmic numerical descriptors are computed and presented to the SOM. Their performance is analysed in terms of separability in different music classes from the activations of the map, obtaining different degrees of success for classical and jazz music. This scheme has a number of applications like indexing and selecting musical databases or the evaluation of style-specific automatic composition systems.	compiler;data descriptor;database;linear separability;list of online music databases;midi;map;numerical analysis;pattern recognition;self-organization	Pedro J. Ponce de León;José Manuel Iñesta Quereda	2003		10.1007/978-3-540-44871-6_90	speech recognition;computer science;artificial intelligence;machine learning;harmonic;pattern recognition;music;database;computer music;programming language;feature selection	ML	-5.1901435494873995	-92.47786569423317	17344
e6331d81a0ccfc93c6d8a98fb2b2f6f5af0b6ecc	ranked-listed or categorized results in ir: 2 is better than 1	ucl;computer science and information systems;discovery;search strategy;theses;conference proceedings;digital web resources;ucl discovery;open access;error rate;ucl library;book chapters;open access repository;user model;ucl research	In this paper we examine the performance of both rankedlisted and categorized results in the context of known-item search (target testing). Performance of known-item search is easy to quantify based on the number of examined documents and class descriptions. Results are reported on a subset of the Open Directory classification hierarchy, which enable us to control the error rate and investigate how performance degrades with error. Three types of simulated user model are identified together with the two operating scenarios of correct and incorrect classification. Extensive empirical testing reveals that in the ideal scenario, i.e. perfect classification by both human and machine, a category-based system significantly outperforms a ranked list for all but the best queries, i.e. queries for which the target document was initially retrieved in the top-5. When either human or machine error occurs, and the user performs a search strategy that is exclusively category based, then performance is much worse than for a ranked list. However, most interestingly, if the user follows a hybrid strategy of first looking in the expected category and then reverting to a ranked list if the target is absent, then performance can remain significantly better than for a ranked list, even with misclassification rates as high as 30%. We also observe that this hybrid strategy results in performance degradations that degrade gracefully with error rate.	apple open directory;categorization;fault tolerance	Zheng Zhu;Ingemar J. Cox;Mark Levene	2008		10.1007/978-3-540-69858-6_12	natural language processing;user modeling;word error rate;computer science;data science;data mining;world wide web	Web+IR	-38.426597118932314	-61.45883206029924	17345
4ac8b6edb0eab377d81deb9db9789bd4690eba2f	robotic training for hand movements during manual welding with real-time alarm feedback	manual welding;training for welding;robotics;welding skills;man machine interface mmi;assistive robotics;physical human robot interaction	Purpose – The purpose of this study is to develop a robotic training system for the hand movements during manual welding. The system provides real-time notice-feedback with sound or light alarms, whenever the welding hand vibrates beyond the nominal level observed with professional welders. Design/methodology/approach – The large variations of hand movements are detected by monitoring the deviation of the tool position from a smooth curve estimated in real time by a Kalman filter. An alarm is generated in the form of a flashing light or beep sound whenever the deviations exceed a predetermined threshold. The performance of hand movements is measured in terms of the variations of the position data. Twelve novice and five professional welders took part in the experiments and answered a questionnaire that assessed the usability and work load of the system. Findings – Compared to the sound alarms, the light alarms resulted in a larger and statistically significant decrease in the variation of hand movements of the novice welders and brought the level of variation close to that of the professional welders. The alarms did not result in a significant decrease in the variation of hand movements of the professional welders. The responses to the questionnaire indicated that both professional and novice welders found the system useful and they did not experience any significant work load. Social implications – The system developed in this study can ease the training of novice welders, by speeding up the learning and reducing the need for human tutors. Originality/value – This study is first to provide real-time notice-feedback for training while manual welding, based on a comparison of the performances of novice and professional welders.	beep;bios;experiment;kalman filter;nominal level;performance;real-time clock;real-time computing;real-time locating system;robot;usability	Mustafa Suphi Erden;Aude Billard	2015	Industrial Robot	10.1108/IR-04-2015-0083	simulation;computer science;engineering;artificial intelligence;robotics;engineering drawing;mechanical engineering	HCI	-46.98068318988623	-57.626670211800835	17364
883564536e20ad0831d1efd4a2383f5263ba4791	patient-centered e-health record over the cloud		The purpose of this paper is to introduce the Patient-Centered e-Health (PCEH) conceptual aspects alongside a multidisciplinary project that combines state-of-the-art technologies like cloud computing. The project, by combining several aspects of PCEH, such as: (a) electronic Personal Healthcare Record (e-PHR), (b) homecare telemedicine technologies, (c) e-prescribing, e-referral, e-learning, with advanced technologies like cloud computing and Service Oriented Architecture (SOA), will lead to an innovative integrated e-health platform of many benefits to the society, the economy, the industry, and the research community. To achieve this, a consortium of experts, both from industry (two companies, one hospital and one healthcare organization) and academia (three universities), was set to investigate, analyse, design, build and test the new platform. This paper provides insights to the PCEH concept and to the current stage of the project. In doing so, we aim at increasing the awareness of this important endeavor and sharing the lessons learned so far throughout our work.	academia (organization);cloud computing;computation (action);electronic health records;electronic prescribing;emoticon;health care organization;intention - mental process;interoperability;patient referral;research and development;service-oriented architecture;service-oriented device architecture;telemedicine;united states public health service;benefit;sensor (device)	Konstantinos Koumaditis;Marinos Themistocleous;George Vassilacopoulos;Andriana Prentza;Dimosthenis Kyriazis;Flora Malamateniou;Nicos Maglaveras;Ioanna Chouvarda;Alexandros Mourouzis	2014	Studies in health technology and informatics	10.3233/978-1-61499-423-7-193	cloud computing;medical record;data mining;medicine	DB	-56.41118551807119	-62.20935377896243	17370
b6ed1400cf1600e2f85f4dfa2ea73a698dba5937	content-based video retrieval integrating human perception	human vision;multimedia;video retrieval;human vision and color perception;feedback;content based video retrieval;video;learning strategies;human perception	Content-based video retrieval system is one of the important design issues of multimedia, mainly depending on its visual and spatio-temporal characteristic. But till now, well-defined model for video retrieval is still at rudimentary stage. We propose a unified video retrieval model to simulate human perception. Given an arbitrary video, considering all the factors existing in human vision perception, we can find similar ones from large video repository within time limitation. This kind of measurement simulates the rules in human being’s judgement, so it can be close to the real need. Furthermore, integrating with feedback, the results can be adjusted according to user’s preference. This learning strategy can emphasize the aspect user cares about, and embodies it in the next iteration of similarity computing. In this way, retrieval result can be optimized greatly. Keyword: content-based video retrieval system, multimedia, video retrieval model, human perception, feedback, learning strategy, similarity computing, retrieval result 1.INTRODUCTION Developing content-based video retrieval system is a promising topic in multimedia application domain. Video contains too rich content to be described in plain text. Furthermore, the drawbacks of manual labor and subjectivity in text annotation make researchers realize that in video retrieval domain it is impractical to index only based on text information. Therefore, although video database still preserves the index of text annotation, it is not used for describing what’s the content of video but defining some doubtless features within limited scope, such as director, produce date and etc. Content-based video retrieval system mainly depends on its visual and spatio-temporal characteristic. The commonly used retrieval approach is querying by example. Integrating text annotation and other retrieval method into large video database, it can ensure (1)Efficiency: Get the needed video quickly (in large database); (2)Simplicity: User’s operation is convenient which requires no special knowledge; (3)Correctness: the result must be as close to user’s demand as possible. Although content-based video retrieval system has predominant advantage over text-based retrieval system, the difficulty in designing is accordingly increasing. Till now, there is still no decided standard about the definition of video similarity. Dimitiova Abdel-mottaleb[1] regards the average distance of all the corresponding frames as two videos’ similarity and defines that video frame sequence must obey temporal order. A.Prasad Sistla[2] defines a Hierarchical Temporal Language(HTL) to specify temporal order of video sequences. They all try to find the factors influencing adjudement of video similarity, but the measurement factors they consider are too limited. From the experiment, we notice that people use varied standards in judging visual similarity, so it is sensible only if the measurement can consider the intervention of human being. In this paper, we propose a content-based video similarity model, which simulates human perception. However, whether two videos are similar is a complicated problem for distinct realization. For two persons, same video may produce different impression. So the strategy of learning should be taken into * Correspondence: Email: wu_yi_77@yahoo.com, Telephone: 086-571-5131127 Storage and Retrieval for Media Databases 2001, Minerva M. Yeung, Chung-Sheng Li, Rainer W. Lienhart, Editors, Proceedings of SPIE Vol. 4315 (2001) © 2001 SPIE · 0277-786X/01/$15.00 562 Downloaded From: http://proceedings.spiedigitallibrary.org/ on 10/05/2013 Terms of Use: http://spiedl.org/terms account in order to make clear what’s the user preference. We need design not only a video similarity model, but also a reasonable feedback algorithm. Section 2 presents the whole system framework. Section 3 proposes out video retrieval model for query processing. Section 4 introduces progressive learning feedback advice in video retrieval. Section 5 shows the experimental result and evaluation. Section 6 is the conclusion and prospect. 2.SYSTEM FRAMEWORK Video data management poses great challenge which require the inter-disciplinary research effort including Video processing, Computer Vision, Database management, etc. The representation form is important for facilitating retrieval and interaction. Here, we take the video hierarchical form of keyframe->shot->video, on the base of which, we implement the video similarity measurement. System framework is composed by several components, shown in Fig.1.	algorithm;application domain;computer vision;database;email;feedback;information retrieval;interaction;iteration;simulation;text-based (computing);video processing	Yi Wu;Yueting Zhuang;Yunhe Pan	2001		10.1117/12.410968	subjective video quality;computer vision;visual word;simulation;video;computer science;video tracking;feedback;multimedia;perception	Vision	-14.411386238027175	-54.7768829326592	17404
c409740f975040ccc784f2f45f0cd6d0a1764318	generating extensional definitions of concepts from ostensive definitions by using web	web pages;proof of concept	We present GEO (Generating an Extensional definition from an Ostensive definition), a method to exhaustively gather items falling under an ostensively defined concept from the Web. By utilizing structural information about HTML documents, GEO automatically and efficiently gathers thousands of items from Web pages taking only 2 or 3 items as input. GEO also yields high precision (0.99 at maximum, 0.97 in average over a set of inputs). We also introduce a new style of searching information, called Item Search, in which GEO plays an essential role. Item Search can look for items in a targeted category that are the best matches against a given query. Some examples of Item Search are presented as the proof-of-concept of the idea.		Shin-ya Sato;Kensuke Fukuda;Satoshi Kurihara;Toshio Hirotsu;Toshiharu Sugawara	2007		10.1007/978-3-540-76993-4_50	computer science;artificial intelligence;web page;data mining;database;proof of concept;world wide web;information retrieval	Web+IR	-30.95276256630984	-54.415309776063054	17414
f7f6160d4e9e3bf7f36bacbc9f15e916a6f226de	does speech enhancement work with end-to-end asr objectives?: experimental analysis of multichannel end-to-end asr		Recently we proposed a novel multichannel end-to-end speech recognition architecture that integrates the components of multichannel speech enhancement and speech recognition into a single neural-network-based architecture and demonstrated its fundamental utility for automatic speech recognition (ASR). However, the behavior of the proposed integrated system remains insufficiently clarified. An open question is whether the speech enhancement component really gains speech enhancement (noise suppression) ability, because it is optimized based on end-to-end ASR objectives instead of speech enhancement objectives. In this paper, we solve this question by conducting systematic evaluation experiments using the CHiME-4 corpus. We first show that the integrated end-to-end architecture successfully obtains adequate speech enhancement ability that is superior to that of a conventional alternative (a delay-and-sum beamformer) by observing two signal-level measures: the signal-todistortion ratio and the perceptual evaluation of speech quality. Our findings suggest that to further increase the performances of an integrated system, we must boost the power of the latter-stage speech recognition component. However, an insufficient amount of multichannel noisy speech data is available. Based on these situations, we next investigate the effect of using a large amount of single-channel clean speech data, e.g., the WSJ corpus, for additional training of the speech recognition component. We also show that our approach with clean speech significantly improves the total performance of multichannel end-to-end architecture in the multichannel noisy ASR tasks.	artificial neural network;automated system recovery;beamforming;end-to-end principle;experiment;pesq;performance;speech corpus;speech enhancement;speech recognition;the wall street journal;zero suppression	Tsubasa Ochiai;Shinji Watanabe;Shigeru Katagiri	2017	2017 IEEE 27th International Workshop on Machine Learning for Signal Processing (MLSP)	10.1109/MLSP.2017.8168188	architecture;artificial intelligence;feature extraction;pattern recognition;noise measurement;computer science;speech enhancement;end-to-end principle	NLP	-14.769524911858626	-90.41376825225997	17421
45ee15d6fc7b465f84dd3e4f2595fc8e79aea962	three mechanisms of parser driving for structure disambiguation	traitement automatique des langues naturelles;syntactic parsing;structure syntaxique;text analysis;grammaire hors contexte;linguistique appliquee;methode;analyse syntaxique automatique;development tool;contexte free grammar;disambiguation;computational linguistics;syntactic structure;desambiguisation;linguistique informatique;method;natural language processing;applied linguistics	Structural ambiguity is one of the most difficult problems in natural language processing. Two disambiguation mechanisms for unrestricted text analysis are commonly used: lexical knowledge and context considerations. Our parsing method includes three different mechanisms to reveal syntactic structures and an additional voting module to obtain the most probable structures for a sentence. The developed tools do not require any tagging or syntactic marking of texts.	item unique identification;natural language processing;parser;tag (metadata);word-sense disambiguation	Sofía N. Galicia-Haro;Alexander F. Gelbukh;Igor A. Bolshakov	2001		10.1007/3-540-44686-9_19	natural language processing;method;speech recognition;computer science;computational linguistics;applied linguistics;linguistics	NLP	-27.62255926958823	-77.28202685052347	17427
fb5ad6cdf716f6d84138f928f80d0dd8e91bd4f0	learning sentiment-inherent word embedding for word-level and sentence-level sentiment analysis	semantics;computational modeling;sentiment analysis;semantics computational modeling;english corpora sentiment inherent word embedding learning sentence level sentiment analysis word level sentiment analysis vector based word representations natural language processing sentiment information continuous skip gram model sentiment information integration semantic word representations chinese corpora	Vector-based word representations have made great progress on many Natural Language Processing tasks. However, due to the lack of sentiment information, the traditional word vectors are insufficient to settle sentiment analysis tasks. In order to capture the sentiment information, we extended Continuous Skip-gram model (Skip-gram) and presented two sentiment word embedding models by integrating sentiment information into semantic word representations. Experimental results showed that the sentiment word embeddings learned by two models indeed capture sentiment and semantic information as well. Moreover, the proposed sentiment word embedding models outperform traditional word vectors on both Chinese and English corpora.	microsoft word for mac;natural language processing;sentiment analysis;text corpus;word embedding	Zhihua Zhang;Man Lan	2015	2015 International Conference on Asian Language Processing (IALP)	10.1109/IALP.2015.7451540	natural language processing;speech recognition;computer science;semantics;linguistics;computational model;sentiment analysis	AI	-19.043825715984365	-74.63443691737812	17436
dd596f9da673fd7b8af9a8bfaac7a1f617086fe6	bigrams of syntactic labels for authorship discrimination of short texts	authorship;quantitative linguistics;linguistique quantitative;stylometrie;stylometry;paternite textuelle	We present a method for authorship discrimination that is based on the frequency of bigrams of syntactic labels that arise from partial parsing of the text. We show that this method, alone or combined with other classification features, achieves a high accuracy on discrimination of the work of Anne and Charlotte Brontë, which is very difficult to do by traditional methods. Moreover, high accuracies are achieved even on fragments of text little more than 200 words long. .................................................................................................................................................................................	bigram;parsing	Graeme Hirst;Olga Feiguina	2007	LLC	10.1093/llc/fqm023	natural language processing;speech recognition;philosophy;quantitative linguistics;computer science;linguistics;stylometry	NLP	-27.078476967001905	-76.43142396860164	17447
8f9e23d1c34c1c4f90ab905443038d34e5dfe038	integrating pharmacogenetic information and clinical decision support into the electronic health record	drug therapy computer assisted;medical order entry systems;hla antigens;individualized medicine;genotype;decision support systems clinical;algorithms;humans;pharmacogenetics;user computer interface;systems integration;electronic health records	Pharmacogenetics (PG) examines gene variations for drug disposition, response, or toxicity. At the National Institutes of Health Clinical Center (NIH CC), a multidepartment Pharmacogenetics Testing Implementation Committee (PGTIC) was established to develop clinical decision support (CDS) algorithms for abacavir, carbamazepine, and allopurinol, medications for which human leukocyte antigen (HLA) variants predict severe hypersensitivity reactions. Providing PG CDS in the electronic health record (EHR) during order entry could prevent adverse drug events. Medical Logic Module (MLM) programming was used to implement PG CDS in our EHR. The MLM checks to see if an HLA sequence-based gene test is ordered. A message regarding test status (result present, absent, pending, or test not ordered) is displayed on the order form, and the MLM determines if the prescriber can place the order, place it but require an over-ride reason, or be blocked from placing the order. Since implementation, more than 725 medication orders have been placed for over 230 patients by 154 different prescribers for the three drugs included in our PG program. Prescribers commonly used an over-ride reason when placing the order mainly because patients had been receiving the drug without reaction before implementation of the CDS program. Successful incorporation of PG CDS into the NIH CC EHR required a coordinated, interdisciplinary effort to ensure smooth activation and a positive effect on patient care. Prescribers have adapted to using the CDS and have ordered PG testing as a direct result of the implementation.	am broadcasting;adverse reaction to drug;advisory committees;allopurinol;blood transfusion, intrauterine;cds isis;carbamazepine;clinical decision support system;clomiphene;drug allergy;electronic health records;gd-rom;gadolinium;greater than;hla antigens;igfbp7 protein, human;informatics (discipline);leukocytes;neoplasms;order (action);order management system;override;patients;peer review;pharmacogenetics;strand displacement amplification;test automation;united states national institutes of health;waf;wigner distribution function;abacavir;algorithm;interest	Barry R. Goldspiel;Willy Albert Flegel;Gary DiPatrizio;Tristan Sissung;Sharon D. Adams;Scott R. Penzak;Leslie G. Biesecker;Thomas A. Fleisher;Jharana J. Patel;David Herion;William D. Figg;Juan J. L. Lertora;Jon W. McKeeby	2014	Journal of the American Medical Informatics Association : JAMIA	10.1136/amiajnl-2013-001873	personalized medicine;pharmacogenetics;medicine;genotype;human leukocyte antigen;system integration	HCI	-55.66504930213011	-66.40581281218138	17452
9348fa5910a301696a351641ccb607bb1f9e22a6	optimization of association word knowledge base through genetic algorithm	web documents;red www;noun;information retrieval system;information retrieval;relation semantique;reseau web;relacion semantica;semantics;base connaissance;user preferences;algoritmo genetico;semantica;semantique;regle association;internet;association rule;recherche information;preferencia;algorithme genetique;world wide web;base conocimiento;genetic algorithm;semantic relation;preference;recuperacion informacion;information system;semantic relations;query expansion;systeme information;sistema informacion;knowledge base	Query expansion in knowledge based on information retrieval system requires knowledge base being considered semantic relations between words. Since Apriori algorithm extracts association word without taking user preference into account, recall is improved but accuracy is reduced. This paper shows how to establish optimized association word knowledge base with improved accuracy only including association word that users prefer among association words being considered semantic relations between words. Toward this end, web documents related to computer are classified into eight classes, and nouns are extracted from web document of each class. Association word is extracted from nouns through Apriori algorithm, and association word that users do not favor is excluded from knowledge base through genetic algorithm.	genetic algorithm;knowledge base	Su-Jeong Ko;Jung-Hyun Lee	2002		10.1007/3-540-46145-0_21	noun;knowledge base;query expansion;the internet;genetic algorithm;association rule learning;computer science;data mining;database;semantics;information retrieval;information system	AI	-35.355494363518034	-60.24209551155852	17471
1d269aa3bae9adb4d5088ef62e34e0e0c19ed7cb	national strategy development for telehealth: learnings from ehealth				Anthony J. Maeder;Maurice Mars;Richard Scott;Ramesh Krishnamurthy	2013			engineering management;ehealth;telehealth;nursing;medicine	Robotics	-57.14265791639935	-63.3998491183633	17476
03fc2198e8ceb995992552e91566d65a70a233fd	adoption of a clinical decision support system to promote judicious use of antibiotics for acute respiratory infections in primary care	clinical decision support systems;quality improvement;respiratory tract infections;decision support systems;electronic health records;anti bacterial agents;primary care	"""PURPOSE Overuse of antibiotics for acute respiratory infections (ARIs) in primary care is an established risk factor for worsening antimicrobial resistance. The """"Reducing Inappropriate Prescribing of Antibiotics by Primary Care Clinicians"""" study is assessing the impact of a clinical decision support system (CDSS) on antibiotic prescribing for ARIs using a multimethod intervention to facilitate CDSS adoption. The purpose of this report is to describe use of the CDSS, as well as facilitators and barriers to its adoption, during the first year of the 15-month intervention.   METHODS Between January 1, 2010 and December 31, 2010, 39 providers in 9 practices in US states participated in this study. Quarterly EHR based audit and feedback, practice site visits for academic detailing, performance review and CDSS training, and """"best-practice"""" dissemination during two meetings of study participants were used to facilitate CDSS adoption. Mixed methods were used to evaluate adoption of the CDSS. Using data extracted from the EHR, CDSS use for ARI was calculated. To determine facilitators and barriers of CDSS adoption, semi-structured group interviews were conducted with providers and staff at each practice.   RESULTS During the first year of implementation, the ABX-TRIP CDSS was used 14,086 times for ARI encounters. Overall, practice use of the CDSS during ARI encounters ranged from 39.4% to 77.2%. Median use of the CDSS for adult patients was 58.2% and 68.6% for pediatric patients. Key factors associated with CDSS adoption include the perception by providers that it assists with decision making and stimulates patient discussions, engagement of non-physician staff and an iterative CDSS development process.   CONCLUSIONS Adoption of a custom designed CDSS in the first year of implementation is promising. Successful implementation of such technology requires a focus not only on the technological solution itself, but on its integration with the entire clinical workplace."""		Cara B. Litvin;Steven M. Ornstein;Andrea M. Wessell;Lynne S. Nemeth;Paul J. Nietert	2012	International journal of medical informatics	10.1016/j.ijmedinf.2012.03.002	clinical decision support system;quality management;intensive care medicine;medicine;decision support system;computer science;artificial intelligence;nursing;pediatrics	HCI	-60.552396421533516	-63.46862501926247	17496
8e3fcc2713a5d3dcdf8128552808a024af3c5fec	prominence patterns of focus in standard chinese		The present study mainly investigates the prominence patterns of focus in Standard Chinese (Hereinafter, SC). The focus is classified into three types, i.e., single focus with various kinds (whquestion induced focus, shi-marked focus and lianmarked focus); double focus with various placements (adjacent and non-adjacent constituents); multiple focus with various distributions (adjacent and non-adjacent constituents). Results of the acoustic analysis revealed that the distribution of the prominence shows symmetric relation with the focus in the single and double focus conditions. All the syllables in the focus domain contribute to the F0 prominence, e.g., all the H and L tones being raised significantly. However, under multiple focus conditions, the prominence does not associates with the focus in the way that only the rightmost item serves as the anchor to realize prominence.	acoustic cryptanalysis;graph (discrete mathematics)	Yuan Jia;Aijun Li	2011			social science;computer science;standard chinese	AI	-11.252037581695685	-80.1755174043763	17525
0baf7098406fcf9c9a8e21196aa13704ee8e7e92	large-scale information retrieval experimentation with terrier	query language;information retrieval;terrier;large scale;indexation;online discussion forum;retrieval model;data structure;use case;large scale experimentation;open source	This tutorial aims to provide a practical introduction to conducting large-scale information retrieval (IR) experiments, using Terrier (http://terrier.org) as an experimentation platform. Written in Java, Terrier provides an open-source, feature-rich, flexible, and robust environment for large-scale IR experimentation. This tutorial will cover the experimentation process end-to-end, from configuring Terrier to a particular experimental setting, to efficiently indexing a document corpus and retrieving from it, and to evaluating the outcome. Moreover, it will describe how to use and extend the platform to one's own needs, and will be illustrated by practical research-driven examples. As a half-day tutorial, it will be split into two major sessions, with each session comprising both background information and practical demonstrations. In the first session, we will provide an overview of several aspects of large-scale IR experimentation, spanning areas such as indexing, data structures, query languages, and advanced retrieval models, and how these are implemented within Terrier. In the second session, we will discuss how to extend Terrier to conduct one's own experiments in a large-scale setting, including how to facilitate the evaluation of non-standard IR tasks through crowdsourcing. The practical demonstrations will cover recent use cases identified from Terrier's online discussion forum, so as to provide attendees with concrete examples of what can be done within Terrier.	crowdsourcing;data structure;end-to-end principle;experiment;file spanning;information retrieval;open-source software;query language;software feature;text corpus	Rodrygo L. T. Santos;Richard McCreadie;Vassilis Plachouras	2011		10.1145/2063576.2064032	use case;data structure;computer science;artificial intelligence;data science;data mining;database;programming language;world wide web;information retrieval;query language	Web+IR	-33.60311279427875	-66.10423291269859	17532
ecd1db931c18ec522fdcc96a9356e9f7b4331741	a bandwidth friendly search engine	search engine;multimedia on the internet;search engines;digital libraries;qa75 electronic computers computer science;internet;search engines internet;categories bandwidth friendly search engine internet web content cache hierarchy queries;bandwidth search engines internet databases telecommunication traffic filters electronic mail navigation information analysis;content based indexing retrieval	The Internet plays host to many millions of documents and images and is increasing in size all the time. As a result locating web content is becoming increasingly difficult for users, and search traffic from users and spiders is increasing rapidly. A directory of the contents of the emerging cache hierarchy would be more complete than existing tools and avoid the need for spider traffic. However it is also essential to minimise the user traffic. A promising approach is to encourage users to refine their queries through categories. Automatic categorisation of a cache directory is demonstrated, and an adaptive categorisation scheme is proposed.	cpu cache;categorization;directory (computing);internet;web cache;web content;web search engine	Clare Bradford;Ian W. Marshall	1999		10.1109/MMCS.1999.778573	database search engine;digital library;the internet;internet traffic;metasearch engine;computer science;internet privacy;search analytics;world wide web;information retrieval;search engine	Web+IR	-31.550823974554717	-53.84550771043481	17555
777df3c3e06ee78a58ed5c715edb0d3248344ec2	who wrote shamela? verifying the authorship of a parodic text	authorship;computacion informatica;historia y critica literaria;filologias;grupo de excelencia;lexical statistic;linguistica;ciencias basicas y experimentales;linguistique mathematique;paternite textuelle;grupo a;statistique lexicale;mathematical linguistics	Imitative texts of high quality are of some importance to students of attribution, especially those who use computational methods. The authorship of such texts is always likely to be difficult to demonstrate. In some cases, the identity of the author is a question of interest to literary scholars. Even when that is not so, students of attribution face a challenge. If we cannot distinguish between original and imitation in such cases, we must always concede that an imitator may have been at work. Shamela (1741) has always been regarded as a brilliant parody. When it is subjected to our standard common-words tests of authorship, it yields mixed results. A new procedure, in which special word-lists are established according to a predetermined set of rules, proves more effective. It needs, however, to be tried in other cases. ..................................................................................................................................	display resolution	John Burrows	2005	LLC	10.1093/llc/fqi049	philosophy;computer science;linguistics;sociology;literature	Web+IR	-37.87279158531701	-73.35038815295373	17571
62005f604e027697cb4dd2993d06ab98506e798a	confidence penalty, annealing gaussian noise and zoneout for bilstm-crf networks for named entity recognition		Named entity recognition (NER) is used to identify relevant entities in text. A bidirectional LSTM (long short term memory) encoder with a neural conditional random fields (CRF) decoder (biLSTM-CRF) is the state of the art methodology. In this work, we have done an analysis of several methods that intend to optimize the performance of networks based on this architecture, which in some cases encourage overfitting avoidance. These methods target exploration of parameter space, regularization of LSTMs and penalization of confident output distributions. Results show that the optimization methods improve the performance of the biLSTM-CRF NER baseline system, setting a new state of the art performance for the CoNLL-2003 Spanish set with an F1 of 87.18.	conditional random field;named-entity recognition;simulated annealing	Antonio Jimeno-Yepes	2018	CoRR		encoder;machine learning;overfitting;parameter space;artificial intelligence;architecture;gaussian noise;regularization (mathematics);named-entity recognition;conditional random field;computer science	NLP	-18.134660219419143	-75.46563833097362	17589
17028270221d276e94b0313d3416566f74ffafe2	syntax-semantics mapping of locative arguments	conference paper	This paper proposes a syntax-semantics correspondence of locative expressions: This proposal is based on the syntactic hierarchy among three locative structures (PPs, VPs, and verbal affixes) and the semantic hierarchy among four locative arguments (Goal, Source, Symmetric Path, Stative Location). As for the syntactic hierarchy, the verbal affixes are closer to the head verb than the locative/path verbs are, and the locative/path verbs than the locative PPs. As for the semantic hierarchy, the following four arguments form a hierarchy due to their semantic closeness to the motion event: Goal > S-Path > Source > St-Location. (cf. Nam 1995, 2004) We argue for this correspondence claim by identifying some crucial typological implications holding between the syntactic/semantic hierarchies.	centrality;nam	Seungho Nam	2012			locative case;syntax;linguistics;semantics;closeness;verb;expression (mathematics);hierarchy;mathematics	NLP	-8.647303850603866	-75.65193144511204	17590
be4ce9e0298ada666045e75e7664f6fc97470d9b	a note based query by humming system using convolutional neural network			convolutional neural network;query by humming	Naziba Mostafa;Pascale Fung	2017			speech recognition;convolutional neural network;pattern recognition;artificial intelligence;computer science;query by humming	Robotics	-15.258765090108513	-87.35790671530022	17605
acaffc76191674841412303f686071aba1b23000	a novel video annotation framework based on video object	video object;video signal processing;active learning;video analysis;video retrieval;data mining;active learning video annotation video object video analysis;system design;automatic annotation;video annotation;web mining;relevance feedback video management object based video annotation web mining method active learning model video retrieval;video signal processing data mining relevance feedback video retrieval;video database;relevance feedback;internet videoconference vocabulary humans object detection learning systems databases digital cameras motion pictures libraries	Video annotation is very important for video management, such as video retrieval. Despite continuous efforts in inventing new annotation algorithms, the annotation performance is usually unsatisfactory, and the annotation vocabulary is still limited due to the use of a small scale training set. In this paper, a novel video annotation framework based on the video object is presented, named Object-Based Video Annotation. By dividing video into three types, we deal with different kind of video in different way. The first kind of video was annotated by human base on the e-Annotation architecture. The second kind of video was automatically annotated by the web mining methods. The third kind of video annotated by video analysis model which detect the video object and label them at the same time. Then active learning model implement active learning method in the video database, which can add new labels and video in the database. We also present an application system based on annotations: video retrieval. At the same time we add relevance feedback in our framework to optimize the result. The system designed base on a real-world situation by including video gathered from the Internet and is designed for exploratory video retrieval system based on the internet.	active learning (machine learning);algorithm;internet;object-based language;relevance feedback;test set;video content analysis;vocabulary;web mining	Yang Li;Jianjiang Lu;Yafei Zhang;Ran Li;Bo Zhou	2009	2009 International Joint Conference on Artificial Intelligence	10.1109/JCAI.2009.32	video compression picture types;computer vision;web mining;computer science;video tracking;multimedia;active learning;smacker video;information retrieval;systems design	Vision	-14.846905752743458	-57.00795966259927	17614
dbb35fc25270c183a10cf5d463487531529a599b	centrality based document ranking	graphs;ranking;documents;india	In this paper, we address the problem of ranking clinical documents using centrality based approach. We model the documents to be ranked as nodes in a graph and place edges between documents based on their similarity. Given a query, we compute similarity of the query with respect to every document in the graph. Based on these similarity values, documents are ranked for a given query. Initially, Lucene is used to retrieve top fifty documents that are relevant to the query and then our proposed approach is applied on these retrieved documents to rerank them. Experimental results show that our approach did not perform well as the documents retrieved by Lucene are not among the top 50 documents in the Gold Standard.	centrality;graph (discrete mathematics);ranking (information retrieval)	Anil Kumar Singh;C. Ravindranath Chowdary	2014			ranking;ranking;data mining;graph;world wide web;information retrieval	Web+IR	-28.596759375883032	-62.529622174559144	17617
308c101edf9d92c1e0d92af8e0ca5e10c05c4075	querying complex federated clinical data using ontological mapping and subsumption reasoning	secondary use of clinical data ontology driven query interface;query processing;user interfaces database management systems health care medical computing medical disorders medical information systems ontologies artificial intelligence query processing;database management systems;user interface features querying complex federated clinical data ontological mapping subsumption reasoning translational effectiveness research comparative effectiveness research user interface design clinical data querying disease classification codes medication history clinical care ontological system terminological system plug and play components system architecture data exploration user interface support ontology driven query interface framework visual aggregator and explorer plus database columns multidimensional data concept attributes ontology class hierarchy visage instances multicenter projects sleep medicine epilepsy medicare medicaid data visage framework large scale data federation;ontologies artificial intelligence;medical disorders;medical computing;ontology driven query interface;ontologies databases medical diagnostic imaging cognition drugs educational institutions;medical information systems;secondary use of clinical data;user interfaces;health care	The secondary use of clinical data is an essential part of translational and comparative effectiveness research. In designing user interfaces for querying clinical data, we found challenges in managing disease classification codes, medication history, and computed indices, which were amplified by the longitudinal nature of clinical care. In order to address these challenges, we integrate domain-specific and standardized ontological/terminological systems as plug-and-play components in the system architecture for data exploration and user interface support. We present an ontology-driven query interface framework called VISAGE+ (Visual Aggregator and Explorer Plus), which is capable of: (a) mapping multiple data sources and database columns to single concepts in an ontological system, (b) querying multi-dimensional data using concept attributes, and (c) supporting queries that require transitive closure over the ontology class hierarchy (a.k.a. subsumption reasoning). Three VISAGE+ instances have been deployed for large-scale multi-center projects in Sleep Medicine, Epilepsy, and for exploring Medicare and Medicaid data. These instances demonstrate the generality and flexibility of the VISAGE+ framework in supporting large-scale data federation, novel user-interface features, and the usability of the interfaces and systems for clinical research.	class hierarchy;code;column (database);emoticon;ontology (information science);plug and play;subsumption architecture;systems architecture;transitive closure;usability;user interface;visage	Licong Cui;Remo Mueller;Satya Sanket Sahoo;Guo-Qiang Zhang	2013	2013 IEEE International Conference on Healthcare Informatics	10.1109/ICHI.2013.49	computer science;data science;data mining;database	DB	-52.89413708939147	-64.30017655867658	17625
c073b720b8c23ddd952776d2d0002b991bbeea1d	advanced querying architecture for dicom systems	radiology;visual databases advanced querying architecture radiology digital image communication in medicine information technology picture archiving communication systems client server architecture;digital image communication in medicine;pacs;client server architecture;query processing;information technology;picture archiving and communication system;medical computing;advanced querying architecture;medical image;digital media;complex system;relational model;dicom biomedical imaging picture archiving and communication systems medical diagnostic imaging image coding application software computer architecture radiology hospitals image storage;picture archiving communication systems;digital image;visual databases medical computing pacs query processing radiology;visual databases	Radiology, which deals with medical images for diagnosis purposes, has seen a lot of improvements during the last decades thanks to information technology. The migration from studies based on physical media to studies based on digital media brought new possibilities and new challenges. One of the most important marks occurred in 1993 with Digital Image Communication in Medicine (DICOM), a standard used to produce, store, display, process and print medical images. DICOM enabled the development of Picture Archiving and Communication Systems (PACS), complex systems that allow all the major operations needed in medical imaging. In this paper we describe an add-on that brings advanced search possibilities to basic DICOM systems, which usually don't implement anything similar, since the standard doesn't require them to. The system we present here is based on a client-server architecture and uses the relational model.	add-ons for firefox;client–server model;complex systems;dicom;data model;digital image;digital media;file archiver;medical imaging;picture archiving and communication system;radiology;relational model;server (computing)	Rui Agostinho;Manuela Pereira;Mário M. Freire	2007	2007 Second International Conference on Systems and Networks Communications (ICSNC 2007)	10.1109/ICSNC.2007.12	computer science;vendor neutral archive;database;multimedia;dicom;picture archiving and communication system;information technology;world wide web	EDA	-51.02961107554742	-61.60309512055069	17639
f0afdccf2903039d202085a771953a171dfd57b1	monotonic chunkwise attention		Sequence-to-sequence models with soft attention have been successfully applied to a wide variety of problems, but their decoding process incurs a quadratic time and space cost and is inapplicable to real-time sequence transduction. To address these issues, we propose Monotonic Chunkwise Attention (MoChA), which adaptively splits the input sequence into small chunks over which soft attention is computed. We show that models utilizing MoChA can be trained efficiently with standard backpropagation while allowing online and linear-time decoding at test time. When applied to online speech recognition, we obtain state-of-theart results and match the performance of a model using an offline soft attention mechanism. In document summarization experiments where we do not expect monotonic alignments, we show significantly improved performance compared to a baseline monotonic attention-based model.		Chung-Cheng Chiu;Colin Raffel	2017	CoRR		automatic summarization;machine learning;artificial intelligence;pattern recognition;time complexity;computer science;backpropagation;decoding methods;monotonic function	NLP	-18.180331345054658	-76.11432132352296	17642
fcc59d909a248703c937994a504608149ca4b818	a supervised korean verb sense disambiguation algorithm based on decision lists of syntactic features	word sense disambiguation	  We present a new approach for resolving sense ambiguity using the decision lists of syntactic features. This approach exploits  the 25 syntactic features including the basic lexical features in the target verb and surrounding words. Our word sense disambiguation  algorithm selects the correct sense by utilizing the strongest evidence on the decision lists when the evidence is ranked  at the higher level of the decision lists. If the strongest one is not available the contributions of all features that provide  weak evidence are summed up and taken into account for the selection. The experiments with ten Korean ambiguous verbs show  significant improvement of performance than the decision lists algorithm. In addition, results of experiments show that the  syntactic features provide more significant evidences than unordered surrounding words for resolving sense ambiguity.    	algorithm;decision list;word-sense disambiguation	Kweon Yang Kim;Byong Gul Lee;Dong Kwon Hong	2004		10.1007/978-3-540-24707-4_17	semeval;computer science	NLP	-24.768158000006995	-73.77438353178152	17646
ddd12c5ac7ad0ffc4bdda9ef049e9d35c96aaa97	discrete reconstruction techniques	discrete tomography;program system;software development	DIscrete REConstruction Techniques (DIRECT) is a program system for generating test images, for computing their projections, for performing reconstructions, for comparing reconstructed images, and for visualizing the results. Our framework supplies a solid workflow to be performed during testing DT methods. A collection of DT reconstruction methods is planned to be implemented in this system to help further comparisions and software developments in DT.	email;user interface	Attila Kuba;László Ruskó;Zoltán Kiss;Antal Nagy	2005	Electronic Notes in Discrete Mathematics	10.1016/j.endm.2005.04.005	computer vision;simulation;computer science;software development;computer graphics (images)	Graphics	-9.939246599839365	-53.721991024697616	17669
b2a2428fb14fab3ee4a6576eb515b6566584594b	deterministic parsing using pcfgs		We propose the design of deterministic constituent parsers that choose parser actions according to the probabilities of parses of a given probabilistic context-free grammar. Several variants are presented. One of these deterministically constructs a parse structure while postponing commitment to labels. We investigate theoretical time complexities and report experiments.	context-free language;deterministic parsing;experiment;stochastic context-free grammar	Mark-Jan Nederhof;Martin McCaffery	2014			natural language processing;parser combinator;computer science;parsing;programming language;top-down parsing;algorithm;lr parser	NLP	-21.386445591160452	-78.52865100340316	17678
5945da974a1373eb2cbfd0a37c21392351904218	using plans to understand natural language	natural language understanding;natural language	Our ability to build a natural language understanding system is limited by the degree to which we can organize and apply world knowledge. This paper describes a program, called PAM, that has knowledge about people's intentions. PAM uses its knowledge to infer the relationships between sentences in a text. A sample run of the program is presented and is described in detail. The inference mechanisms of PAM are compared to those of other knowledge-application programs.	commonsense knowledge (artificial intelligence);natural language understanding	Robert Wilensky	1976		10.1145/800191.805524	natural language processing;language identification;natural language programming;universal networking language;question answering;computer science;linguistics;communication	AI	-31.533567419677677	-80.62850937298897	17730
8ad8e98574a275930bf04a477ce3532fd13c503c	generalized probabilistic lr parsing of natural language (corpora) with unification-based grammars	grammar;langue de specialite;traitement automatique des langues naturelles;pilot study;syntax;noun;regles;regles syntaxiques;syntaxe;grammaire context free;probabilistic context free grammar;probabilistic model;frequency of occurrence;dictionnaire;automatic analysis;specialist language;anglais;corpus;interactive system;grammaire;natural language;statistique linguistique;modele probabiliste;dictionary;analyse automatique;context dependent;langue naturelle;computational linguistics;analyzer;syntactic rules;rule;linguistique informatique;analyseur;natural language processing;language model;longman dictionary of contemporary english	We describe work toward the construction of a very wide-coverage probabilistic parsing system for natural language (NL), based on LR parsing techniques. The system is intended to rank the large number of syntactic analyses produced by NL grammars according to the frequency of occurrence of the individual rules deployed in each analysis. We discuss a fully automatic procedure for constructing an LR parse table from a unification-based grammar formalism, and consider the suitability of alternative LALR(1) parse table construction methods for large grammars. The parse table is used as the basis for two parsers; a user-driven interactive system that provides a computationally tractable and labor-efficient method of supervised training of the statistical information required to drive the probabilistic parser. The latter is constructed by associating probabilities with the LR parse table directly. This technique is superior to parsers based on probabilistic lexical tagging or probabilistic context-free grammar because it allows for a more context-dependent probabilistic language model, as well as use of a more linguistically adequate grammar formalism. We compare the performance of an optimized variant of Tomita's (1987) generalized LR parsing algorithm to an (efficiently indexed and optimized) chart parser. We report promising results of a pilot study training on 150 noun definitions from the Longman Dictionary of Contemporary English (LDOCE) and retesting on these plus a further 55 definitions. Finally, we discuss limitations of the current system and possible extensions to deal with lexical (syntactic and semantic)frequency of occurrence.	algorithm;chart parser;cobham's thesis;context-free language;context-sensitive language;dictionary;formal grammar;formal system;glr parser;han unification;interactivity;lalr parser;lr parser;language model;lexicon;nl (complexity);natural language;parsing;stochastic context-free grammar;text corpus;unification (computer science)	Ted Briscoe;John A. Carroll	1993	Computational Linguistics		natural language processing;noun;statistical model;parser combinator;spectrum analyzer;speech recognition;lalr parser;canonical lr parser;syntax;parsing expression grammar;top-down parsing language;computer science;computational linguistics;context-dependent memory;parsing;glr parser;s-attributed grammar;grammar;linguistics;natural language;context-free grammar;top-down parsing;lr parser;language model;simple lr parser	NLP	-25.896905385977742	-77.34633606685802	17735
05919cfb9b6615281b256bad4aa531d11c126cd6	mychildren's: integration of a personally controlled health record with a tethered patient portal for a pediatric and adolescent population	internet;confidentiality	Personally controlled health records (PCHRs) and patient portals are increasingly being offered by healthcare institutions, employers, insurance companies and commercial entities to allow patients access to their health information. Both applications offer unique services to provide patients with tools to manage their health. While PCHRs allow users ubiquitous, portable, patient controlled access to their health information, traditional patient portals provide provider-tethered applications allowing patients access, but not control of, certain healthcare information, as well as communication and administrative functions, such as secure messaging, appointment management and prescription refill requests, facilitating care at a specific healthcare facility.We describe our approach for the design, content creation, policy development, and implementation of MyChildren's, a unique web-based application leveraging the advantages of both a provider-tethered patient portal and a PCHR to allow patients and their guardians access to the functionality and convenience of a traditional patient portal, as well as the portability and flexibility of a PCHR.	electronic health records;entity;health care facility;patients;portals;secure messaging;software portability;web application	Fabienne C. Bourgeois;Kenneth D. Mandl;Danny Shaw;Daisy Flemming;Daniel J. Nigrin	2009	AMIA ... Annual Symposium proceedings. AMIA Symposium		secure messaging;world wide web;masking (electronic health record);confidentiality;patient portal;health care;the internet;population;content creation;internet privacy;medicine	HPC	-53.281887720546294	-62.10730838064188	17738
c6e05da3da7bf03145701d8a7cb826aec8b7f24e	a maximum-likelihood approach to symbolic indirect correlation	nonparametric method;ordered unsegmented signal recognition;maximum likelihood;noisy input;online handwriting recognition;isomorphism;maximum likelihood detection isomorphism;noisy input symbolic indirect correlation nonparametric method ordered unsegmented signal recognition subgraph isomorphism maximum likelihood problem online handwriting recognition;silicon carbide hidden markov models pattern matching handwriting recognition ink maximum likelihood estimation pattern recognition noise robustness speech recognition engines;maximum likelihood detection;non parametric method;symbolic indirect correlation;maximum likelihood problem;subgraph isomorphism	Symbolic indirect correlation (SIC) is a non-parametric method that offers significant advantages for recognition of ordered unsegmented signals. A previously introduced formulation of SIC based on subgraph-isomorphism requires very large reference sets in the presence of noise. In this paper, we seek to address this issue by formulating SIC classification as a maximum likelihood problem. We present experimental evidence that demonstrates that this new approach is more robust for the problem of online handwriting recognition using noisy input	handwriting recognition;lexicon;simple features;simplified instructional computer;simulation;subgraph isomorphism problem;tracing (software)	Ashutosh Joshi;George Nagy;Daniel P. Lopresti;Sharad C. Seth	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.97	machine learning;pattern recognition;subgraph isomorphism problem;mathematics;maximum likelihood;isomorphism;statistics	Vision	-21.96862454503389	-93.19513612017091	17740
d19452a99859b4493e57b3078870141025d7b6ae	implementation of accent recognition methods subsystem for elearning systems		The results of the implementation of an external accent recognition system and its integration into massive open online courses platform Moodle are reported. Accent recognition becomes important in foreign languages learning to provide a feedback to a student on a presence of a certain unwanted accent in a foreign language pronunciation. Implementation of several accent recognition methods and their comparison is provided. It is shown that neural networks provide the most reliable recognition given the accented utterances from Wildcat Corpus of Native- and Foreign-Accented English.	artificial neural network;coefficient;massive open online course;mel-frequency cepstrum	Eugen Tverdokhleb;Hennadii Dobrovolskyi;Nataliya Keberle;Natalia Myronova	2017	2017 9th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)	10.1109/IDAACS.2017.8095243	artificial neural network;feature extraction;hidden markov model;natural language processing;computer science;pronunciation;mel-frequency cepstrum;speech processing;speech recognition;foreign language;artificial intelligence	Robotics	-16.302257978091838	-86.20779242542297	17744
2ab9183023f351f1c5b3dd59d755e591f581ac6d	k-nearest neighbors relevance annotation model for distance education	distance education;probability;metadata;information retrieval;rank weighting;image understanding;internet;computational complexity;children;k nearest neighbor;image automatic annotation;visual aids;knnr;article;distance weighting;models;documentation;image retrieval;automation	With the rapid development of Internet technologies, distance education has become a popular educational mode. In this paper, the authors propose an online image automatic annotation distance education system, which could effectively help children learn interrelations between image content and corresponding keywords. Image automatic annotation is a significant problem in image retrieval and image understanding. The authors propose a K-Nearest Neighbors Relevance model, which combines KNN method with relevance models. The model solves the problems of high computational complexity and annotation results affected by irrelevant training images when joint generation probabilities between visual areas and keywords are calculated. The authors also propose a multi-scale windows method and nearest-neighbors weighting method based on rank-weighting and distance-weighting. Experiments conducted on Corel datasets verify that the K-Nearest Neighbors Relevance model is quite effective.	k-nearest neighbors algorithm;relevance	Xiao Ke;Shaozi Li;Donglin Cao	2011	IJDET	10.4018/jdet.2011010106	distance education;image retrieval;documentation;computer science;automation;machine learning;probability;data mining;metadata;automatic image annotation;world wide web;information retrieval;statistics	Vision	-14.057261872379083	-61.178816466439784	17791
0bb72e740d799752ac960896a9964942e5f60ed8	from propbank to engvallex: adapting the propbank-lexicon to the valency theory of the functional generative description		EngValLex is the name of an FGD-compliant valency lexicon of English verbs, built from the PropBank-Lexicon and following the structure of Vallex, the FGD-based lexicon of Czech verbs. EngValLex is interlinked with the PropBank-Lexicon, thus preserving the original links between the PropBank-Lexicon and the PropBank-Corpus. Therefore it is also supposed to be part of corpus annotation. This paper describes the automatic conversion of the PropBank-Lexicon into Pre-EngValLex, as well as the progress of its subsequent manual refinement (EngValLex). At the start, the Propbank-arguments were automatically re-labeled with functors (semantic labels of FGD) and the PropBank-rolesets were split into the respective example sentences, which became FGD-valency frames of PreEngValLex. Human annotators check and correct the labels and make the preliminary valency frames FGD-compliant. The most essential theoretical difference between the original and EngValLex is the syntactic alternations used by the PropBank-Lexicon, not yet employed within the Czech framework. The alternation-based approach substantially affects the conception of the frame, making in very different from the one applied within the FGD-framework. Preserving the valuable alternation information required special linguistic rules for keeping, altering and re-merging the automatically generated preliminary valency frames.	hands-on computing;lexicon;propbank;refinement (computing);software release life cycle;treebank	Silvie Cinková	2006			propbank;artificial intelligence;syntax;natural language processing;generative grammar;alternation (linguistics);computer science;lexicon;czech;valency;english verbs	NLP	-27.68470654902729	-75.43323398157669	17851
00cf0b43e0042f8a9badb47e9dd32fa274355ebf	assessing conceptual similarity to support concept mapping	analogical reasoning;in formation retrieval;concept map;knowledge modeling	Conceptmapscaptureknowledgeabout the conceptsand conceptrelationshipsin a domain,usinga two-dimensional visually-basedrepresentation.Computertools for concept mappingempower experts to directly construct,navigate, share,andcriticize rich knowledgemodels.This paperdescribesongoingresearchon augmentingconceptmapping tools with systemsto supportthe userby proacti vely suggestingrelevantconceptsandassociatedresources(e.g.,images,video, and text pages)during conceptmapcreation. Providing suchsupportrequiresefficientandeffectivealgorithms for judging conceptsimilarity and the relevanceof prior conceptsto new conceptmaps.We discusskey issues for suchalgorithmsandpresentfour new approachesdevelopedfor assessingconceptual similarity for conceptsin conceptmaps. Two useprecomputedsummariesof structural andcorrelationalinformationto determinetherelevanceof storedconceptsto selectedconceptsin a new conceptmap, andtwo useinformationaboutthecontext in which theselectedconceptappears.We closeby discussingtheir tradeoffs andtheir relationshipsto researchin areassuchasinformationretrieval andanalogicalreasoning.	concept map	David B. Leake;Ana Gabriela Maguitman;Alberto J. Cañas	2002			concept map;computer science;knowledge management;artificial intelligence;data mining	HCI	-40.16359495290611	-57.43322600421007	17865
34a7840862593603b2dbea07e42b7d5eea786b6d	feasibility of using automatic speech recognition with voices of deaf and hard-of-hearing individuals		Many personal devices have transitioned from visual-controlled interfaces to speech-controlled interfaces to reduce costs and interactive friction, supported by the rapid growth in capabilities of speech-controlled interfaces, e.g., Amazon Echo or Apple's Siri. A consequence is that people who are deaf or hard of hearing (DHH) may be unable to use these speech-controlled devices. We show that deaf speech has a high error rate compared to hearing speech, in commercial speech-controlled interfaces. Deaf speech had approximately a 78% word error rate (WER) compared to a hearing speech 18% WER. Our findings show that current speech-controlled interfaces are not usable by DHH people.	siri;speech recognition;word error rate	Abraham T. Glasser;Kesavan R. Kushalnagar;Raja S. Kushalnagar	2017		10.1145/3132525.3134819	hearing speech;cued speech;word error rate;computer science;usable;speech technology;speech recognition	HCI	-24.927455856506807	-87.21456097703557	17867
4354a77bae44367af43b8fa809ea8f3be2e88dfa	utilising wikipedia for text mining applications	interactive information retrieval;personal information management;settore inf 01 informatica;wikipedia;text mining;information technology;user studies;category article structure;thesis;engineering informatics;user interfaces	"""The process whereby inferences are made from textual data is broadly referred to as text mining. In order to ensure the quality and effectiveness of the derived inferences, several approaches have been proposed for different text mining applications. Among these applications, classifying a piece of text into pre-defined classes through the utilisation of training data falls into supervised approaches while arranging related documents or terms into clusters falls into unsupervised approaches. In both these approaches, processing is undertaken at the level of documents to make sense of text within those documents. Recent research efforts have begun exploring the role of knowledge bases in solving the various problems that arise in the domain of text mining. Of all the knowledge bases, Wikipedia on account of being one of the largest human-curated, online encyclopaedia has proven to be one of the most valuable resources in dealing with various problems in the domain of text mining. However, previous Wikipedia-based research efforts have not taken both Wikipedia categories and Wikipedia articles together as a source of information.  This thesis serves as a first step in eliminating this gap and throughout the contributions made in this thesis, we have shown the effectiveness of Wikipedia category-article structure for various text mining tasks. Wikipedia categories are organized in a taxonomical manner serving as semantic tags for Wikipedia articles and this provides a strong abstraction and expressive mode of knowledge representation. In this thesis, we explore the effectiveness of this mode of Wikipedia's expression (i.e., the category-article structure) via its application in the domains of text classification, subjectivity analysis (via a notion of """"perspective"""" in news search), and keyword extraction.  First, we show the effectiveness of exploiting Wikipedia for two classification tasks i.e., 1- classifying the tweets1 being relevant/irrelevant to an entity or brand, 2- classifying the tweets into different topical dimensions such as tweets related with workplace, innovation, etc. To do so, we define the notion of relatedness between the text in tweet and the information embedded within the Wikipedia category-article structure. Then, we present an application in the area of news search by using the same notion of relatedness to show more information related to each search result highlighting the amount perspective or subjective bias in each returned result towards a certain opinion, topical drift, etc. Finally, we present a keyword extraction strategy using community detection over the Wikipedia categories to discover related keywords arranged in different communities.  The relationship between Wikipedia categories and articles is explored via a textual phrase matching framework whereby the starting point is textual phrases that match Wikipedia articles' titles/redirects. The Wikipedia articles for which a match occurs are then utilised by extraction of their associated categories, and these Wikipedia categories are used to derive various structural measures such as those relating to taxonomical depth and Wikipedia articles they contain. These measures are utilised in our proposed text classification, subjectivity analysis, and keyword extraction framework and the performance is analysed via extensive experimental evaluations. These experimental evaluations undertake comparisons with standard text mining approaches in the literature and our Wikipedia framework based on its category-article structure outperforms the standard text mining techniques."""	document classification;embedded system;information source;keyword extraction;knowledge base;knowledge representation and reasoning;relevance;supervised learning;taxonomy (general);text corpus;text mining;wikipedia	Muhammad Atif Qureshi	2015	SIGIR Forum	10.1145/2888422.2888449	text mining;engineering informatics;computer science;personal information management;data mining;brand;user interface;information technology;world wide web;information retrieval	Web+IR	-24.547155183586668	-59.45369038281367	17876
f197b646598c490dc3b98eb618393045d1269a17	multiresolution video indexing for subband coded video databases	databases;spatial frequencies;search algorithm;feature matching;video indexing;indexation;scene change detection;query by example;video;multi resolution;video database;computational efficiency;subband coding;spatial frequency	In this paper, we present a multiresolution approach for video indexing and feature matching of subband coded video databases. Subband coding refers to a coding technique where the input images are quantized after being decomposed into several narrow spatial frequency bands by filtering and decimation. Five different approaches were tested for scene change detection which is applied only on the lowest subband for computational efficiency. Two kinds of scene changes, abrupt and smoothly accumulated scene changes, mark the beginning of new scene segments. An index for each scene segment is the histograms of two representative frames, which we take to be the first and the last frame of the scene for simplicity. Using the approach of query by example, the index matching algorithm takes a multi-resolution approach by hierarchically comparing histograms at different resolutions. The search algorithm for the match between example query and its target scene segment starts from the coarsest resolution, and moves to the next finer resolution until a single match is obtained or the finest resolution is reached. Experimental results are presented, and the proposed indexing technique appears to be promising for its computational efficiency and its inherent hierarchical search procedure.	computation;database;decimation (signal processing);frequency band;multiresolution analysis;query by example;search algorithm;smoothing;sub-band coding	Jungwoo Lee;Bradley W. Dickinson	1994		10.1117/12.171773	computer vision;speech recognition;computer science;data mining	Vision	-6.660675477065853	-94.26520109315666	17961
a467dcc9c991890d4e2d8d6dd07063fc438736db	the role of large memories in scientific communications	scientific communication	Large memories provide automatic reference to millions of words of machine-readable coded information or to millions of images of document pages. Higher densities of storage will make possible low-cost memories of billions of words with access to any part in a few seconds or complete searches in minutes. These memories will serve as indexes to the deluge of technical literature when the problems of input and of the automatic generation of classification information are solved. Document files will make the indexed literature rapidly available to the searcher. However, memory capacity is currently well ahead of our ability to use it, and much work remains in this area. Machine translation of languages and recognition of spoken information are two other areas which will require fast, large memories.		Morton M. Astrahan	1958	IBM Journal of Research and Development	10.1147/rd.24.0310	computer science;artificial intelligence;data mining;information retrieval	HPC	-35.9383405401308	-73.27392835011766	17974
b0e044448813b02eaf17dedf3b6c941f7f3a94f8	explanations as mechanisms for supporting algorithmic transparency		Transparency can empower users to make informed choices about how they use an algorithmic decision-making system and judge its potential consequences. However, transparency is often conceptualized by the outcomes it is intended to bring about, not the specifics of mechanisms to achieve those outcomes. We conducted an online experiment focusing on how different ways of explaining Facebooku0027s News Feed algorithm might affect participantsu0027 beliefs and judgments about the News Feed. We found that all explanations caused participants to become more aware of how the system works, and helped them to determine whether the system is biased and if they can control what they see. The explanations were less effective for helping participants evaluate the correctness of the systemu0027s output, and form opinions about how sensible and consistent its behavior is. We present implications for the design of transparency mechanisms in algorithmic decision-making systems based on these results.	algorithm;correctness (computer science)	Emilee J. Rader;Kelley Cotter;Janghee Cho	2018		10.1145/3173574.3173677	management science;human–computer interaction;correctness;computer science;transparency (graphic)	HCI	-51.69275464603415	-54.40125662225921	18001
2147efe76c8bd047a772a4d1a6793e6f8a6cad9f	mmid: multimodal multi-view integrated database for human behavior understanding	human behavior understanding;multimodal multi-view integrated database;information retrieval;motion estimation;databases;video;database design;intelligent systems;human body;image recognition;human behavior;anthropometry;user interfaces;human computer interaction;statistical analysis;speech;face	This paper introduces the Multimodal Multi-view Integrated Database (MMID), which holds human activities in presentation situations. MMID contains audio, video, human body motions, and transcripts, which are related to each other by their occurrence time. MMID accepts basic queries for the stored data. We can examine, by referring the retrieved data, how the different modalities are cooperatively and complementarily used in real situations. This examination over different situations is essential for understanding human behaviors, since they are heavily dependent on their contexts and personal characteristics. In this sense, MMID can serve as a basis for systematic or statistical analysis of those modalities, and it can be a good tool when we design an intelligent user interface system or a multimedia contents handling system. In this paper, we will present the database design and its possible applications.	database design;image analysis;intelligent user interface;modality (human–computer interaction);multimodal interaction	Yuichi Nakamura;Yoshifumi Kimura;Y. Yu;Yuichi Ohta	1998			computer vision;human body;human–computer interaction;computer science;motion estimation;multimedia;user interface;human behavior;database design	DB	-14.217190255246091	-54.08779760563997	18006
f1d118de71264be51d18b0c1c234ce28c59bff44	dealing with spurious ambiguity in learning itg-based word alignment	detailed study;discriminative learning;exponentially large search space;previous grammar;inversion transduction grammar;search efficiency;synchronous parsing algorithm;synchronous parsing;itg-based word alignment;word alignment;spurious ambiguity	Word alignment has an exponentially large search space, which often makes exact inference infeasible. Recent studies have shown that inversion transduction grammars are reasonable constraints for word alignment, and that the constrained space could be efficiently searched using synchronous parsing algorithms. However, spurious ambiguity may occur in synchronous parsing and cause problems in both search efficiency and accuracy. In this paper, we conduct a detailed study of the causes of spurious ambiguity and how it effects parsing and discriminative learning. We also propose a variant of the grammar which eliminates those ambiguities. Our grammar shows advantages over previous grammars in both synthetic and real-world experiments.	algorithm;attachments;bitext word alignment;combinatory categorial grammar;data structure alignment;experiment;shallow parsing;synthetic intelligence;transduction (machine learning)	Shujian Huang;Stephan Vogel;Jiajun Chen	2011			natural language processing;speech recognition;computer science;machine learning;algorithm	NLP	-20.327803608039485	-76.82245310737581	18020
6c0715562d3580b09b364f3989638c4618e8ac17	convolutional neural networks and multitask strategies for semantic mapping of natural language input to a structured database		In this work, we investigate mapping both natural language food and quantity descriptions to matching USDA database entries. We demonstrate that a convolutional neural network (CNN) model with a softmax layer on top to directly predict the most likely database matches outperforms our previous state-of-the-art approach of learning binary classification and subsequently ranking database entries using similarity scores with the learned embeddings. The softmax model achieves 97.3% top-5 USDA quantity and 91.1 % food recall over the full database, compared to only 70.0% quantity and 46.4% food recall with a sigmoid model, where top-5 recall indicates the percentage of test cases in which the correct quantity or food is in the top-5 hits. Evaluated on 9,600 spoken meals over all foods, the softmax model achieves 91.6% top-5 quantity and 80.1 % food recall. We also explore jointly learning both mappings with a single CNN to boost quantity mapping, and improve food mapping by reranking the food database entries using the predicted quantity matches.	artificial neural network;binary classification;computer multitasking;convolutional neural network;natural language;sigmoid function;softmax function;test case	Mandy Korpusik;James R. Glass	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8461769	task analysis;convolutional neural network;semantic mapping;semantics;database;artificial intelligence;computer science;recall;binary classification;pattern recognition;ranking;softmax function	DB	-16.911261391162377	-71.86708923107128	18021
80a9e545d6bd6ab4dc59635eae32b6c704af1bfa	quantum-like uncertain conditionals for text analysis	quantum measurement;information retrieval;text analysis;probabilistic approach;quantum physics;natural language;quantum like;natural language processing	Simple representations of documents based on the occurrences of terms are ubiquitous in areas like Information Retrieval, and also frequent in Natural Language Processing. In this work we propose a logical-probabilistic approach to the analysis of natural language text based in the concept of Uncertain Conditional, on top of a formulation of lexical measurements inspired in the theoretical concept of ideal quantum measurements. The proposed concept can be used for generating topic-specific representations of text, aiming to match in a simple way the perception of a user with a pre-established idea of what the usage of terms in the text should be. A simple example is developed with two versions of a text in two languages, showing how regularities in the use of terms are detected and easily represented.	information retrieval;natural language processing;quantum;text-based (computing);theoretical definition	Alvaro Francisco Huertas-Rosero;C. J. van Rijsbergen	2011		10.1007/978-3-642-24971-6_14	natural language processing;text graph;language identification;text simplification;text mining;computer science;artificial intelligence;theoretical computer science;machine learning;concept search;linguistics;natural language;information retrieval	NLP	-28.040015026205015	-66.85870146334952	18024
455c2199be6cb82e18f62fd72bf8268fb801209e	query expansion strategy based on pseudo relevance feedback and term weight scheme for monolingual retrieval	international journal of computer applications ijca	Query Expansion using Pseudo Relevance Feedback is a useful and a popular technique for reformulating the query. In our proposed query expansion method, we assume that relevant information can be found within a document near the central idea. The document is normally divided into sections, paragraphs and lines. The proposed method tries to extract keywords that are closer to the central theme of the document. The expansion terms are obtained by equi-frequency partition of the documents obtained from pseudo relevance feedback and by using tf-idf scores. The idf factor is calculated for number of partitions in documents. The group of words for query expansion is selected using the following approaches: the highest score, average score and a group of words that has maximum number of keywords. As each query behaved differently for different methods, the effect of these methods in selecting the words for query expansion is investigated. From this initial study, we extend the experiment to develop a rule-based statistical model that automatically selects the best group of words incorporating the tf-idf scoring and the 3 approaches explained here, in the future. The experiments were performed on FIRE 2011 Adhoc Hindi and English test collections on 50 queries each, using Terrier as retrieval engine.	experiment;frequency partition of a graph;logic programming;query expansion;relevance feedback;statistical model;tf–idf	Rekha Vaidyanathan;Sujoy Das;Namita Srivastava	2012	CoRR	10.5120/18394-9646	natural language processing;sargable;query optimization;query expansion;web query classification;ranking;boolean conjunctive query;computer science;artificial intelligence;machine learning;data mining;information retrieval;algorithm	Web+IR	-30.188607719614467	-63.86295322472982	18038
26567aeb860209d1df93f7fed2f2a07b5cba0f58	towards spanish verbs' selectional preferences automatic acquisition: semantic annotation of the sensem corpus	noun	We present the results of an agreement task carried out in the framework of the KNOW Project and consisting in manually annotating an agreement sample totaling 50 sentences extracted from the SenSem corpus. Diambiguation was carried out for all nouns, proper nouns and adjectives in the sample, all of which were assigned EuroWordNet (EWN) synsets. As a result of the task, Spanish WN has been shown to exhibit 1) lack of explanatory clarity (it does not define word meanings, but glosses and examplifies them instead; it does not systematically encode metaphoric meanings, either); 2) structural inadequacy (some words appear as hyponyms of another sense of the same word; sometimes there even coexist in Spanish WN a general sense and a specific one related to the same concept, but with no structural link in between; hyperonymy relationships have been detected that are likely to raise doubts to human annotators; there can even be found cases of auto-hyponymy); 3) cross-linguistic inconsistency (there exist in English EWN concepts whose lexical equivalent is missing in Spanish WN; glosses in one language more often than not contradict or diverge from glosses in another language).	cluster analysis;coexist (image);encode;eurowordnet;existential quantification;gloss (annotation);lexical definition;sensor;synonym ring;web services for devices;word-sense disambiguation;wordnet	Jordi Carrera;Irene Castellón;Salvador Climent;Marta Coll-Florit	2008			natural language processing;noun;eurowordnet;artificial intelligence;computer science;proper noun;spanish verbs;sentence extraction;annotation	NLP	-27.747290976505173	-72.43061196975327	18039
e55b10878100863120b7d078f334fa87c8551bb5	combating obesity trends in teenagers through persuasive mobile technology	theory of planned behavior;mobile device;technology acceptance model;physical activity;health risk;cardiovascular disease;mobile technology;health care system	Throughout the last decade, there has been an alarming increase in obesity prevalence among adults and teens throughout the world. Obesity has been found to increase the risk of developing diabetes, cardiovascular diseases, and some cancers. Due to the many health risks associated with obesity, an increase in prevalence has also pressured health care systems and the finances of the individual. Our research proposes to decrease obesity prevalence in adults by motivating teens to become or continue being physically active so that they can continue these healthy lifestyles as adults. Our goal is to encourage long term adoption of physically active behaviors by introducing a motivating application running on a mobile device. We use the Technology Acceptance Model, the Theory of Planned Behavior, the Theory of Meaning Behavior, and the Big 5 Personality Theory to guide our design.	mobile device	Sonia M. Arteaga;Mo Kudeki;Adrienne Woodworth	2009	ACM SIGACCESS	10.1145/1595061.1595064	simulation;computer science;operating system;mobile technology;mobile device;physical fitness;theory of planned behavior	HCI	-59.09006916853902	-56.227014214368005	18044
37b6d05eaee5234c8e52bf191ad4147703a71d88	spoken language identification with phonotactics methods on minangkabau, sundanese, and javanese languages		Abstract   Research in the field of spoken language identification (spoken LID) on local languages helps to extend the outreach of technology to local language speakers. This research also contributes to the preservation of local languages. In this paper, we report our work on identifying spoken data in three local Indonesian languages: Minangkabau, Sundanese and Javanese. Statistical phonotactics models are created to map the speech signals into the language used by the speaker. We use two phonotactics methods, namely Phone Recognition followed by Language Modelling (PRLM) and Parallel Phone Recognition followed by Language Modelling (PPRLM). PRLM method shows the highest accuracy using the phone recognizer trained for English and Russian with the average of 77.42% and 75.94% respectively.	language identification	Nur Endah Safitri;Amalia Zahra;Mirna Adriani	2016		10.1016/j.procs.2016.04.047	natural language processing;speech recognition;computer science	NLP	-23.61501758552236	-84.56324245619197	18055
01e50c1dea87fa5f871fd0bd2eb7ef737697f93d	i-care - a health promotion system for active ageing		I-Care is a health promotion system designed to: a) monitor vital signs of patients in real time; b) improve the patients’ knowledge of the disease, aiming to increase their motivation to develop healthy habits; c) motivate the patients through their interaction with doctors, friends and other patients with the same disease. The system works through the web, being suitable for many platforms. A group of three people was monitored during four weeks, generating some interesting conclusions about different ways to motivate people to get older in a more active and healthy way. The test subjects were from 56 to 67 years, both male and female, with hypertension, overweight and diabetes. The adopted methodology was both quantitative and qualitative. Results showed that the proposed system can improve significantly the health of the monitored patients, developing in the participants a better understanding about their health situation.	care-o-bot;emoticon;world wide web	Maria Lúcia Kroeff Barbosa;Valter Roesler;Alexandro Bordignon;Johannes Doll;Eliseo Reategui	2012			knowledge management;environmental health;active ageing;health promotion;computer science	HCI	-58.93045757092134	-56.5229454630069	18059
d7c9b08d83b0ececf64325fce68756671d7a2db6	graphics software for remote terminals and their use in radiation treatment planning	treatment planning;clinical data;life sciences;cost effectiveness;patient monitoring;interactive graphics;human perception;molecular structure	Interactive graphics' ability to provide a meaningful interface to investigators in a variety of applied fields has been recognized for many years. Nowhere is this promise greater than in medicine and the life sciences. Levinthal's interactive program for constructing and displaying complex molecular structures is well-known. Neurath and Frey have analyzed chromosome spreads by delegating to human perception the light-pen dissection of individual chromosomes from overlapping clusters while leaving subsequent measurements and final classification to the computer. Cox and Clark's Programmed Console has introduced many radiotherapists to the use of computers in treatment planning. Other applications at this facility and elsewhere illustrate the great value of interactive graphics support in deriving insight from large clinical data bases, exploring complex biological models in the hope of improving treatment strategies, and developing cost-effective algorithms or special hardware for patient monitoring.	algorithm;computer;database;graphics software;interactive computing;light pen	Karl H. Ryden;Carol M. Newton	1971		10.1145/1478873.1479024	simulation;computer science;computer graphics (images)	Graphics	-9.290694887520594	-54.479168303303496	18072
f79523b5496457315382e11b9303a212ddb1ebde	automatic summarization based on sentence morpho-syntactic structure: narrative sentences compression	automatic summarization	We propose an automated text summarization through sentence compression. Our approach uses constituent syntactic function and position in the sentence syntactic tree. We first define the idea of a constituent as well as its role as an information provider, before analyzing contents and discourse consistency losses caused by deleting such a constituent. We explain why our method works best with narrative texts. With a rule-based system using SYGFRAN’s morpho-syntactic analysis for French [Cha84], we select removable constituents. Our results are satisfactory at the sentence level but less effective at the whole text level, a situation we explain by describing the difference of impact between constituents and relations.	automatic summarization;parsing;removable media;rule-based system	Mehdi Yousfi Monod;Violaine Prince	2005			natural language processing;speech recognition;linguistics	NLP	-27.391239679246347	-75.41248906365848	18088
5f6bfb6d53c58b65afab94f09dd7a55dcb23462c	toward a comparable corpus of latvian, russian and english tweets		Twitter has become a rich source for linguistic data. Here, a possibility of building a trilingual Latvian-Russian-English corpus of tweets from Riga, Latvia is investigated. Such a corpus, once constructed, might be of great use for multiple purposes including training machine translation models, examining cross-lingual phenomena and studying the population of Riga. This pilot study shows that it is feasible to build such a resource by collecting and analysing a pilot corpus, which is made publicly available and can be used to construct a large comparable corpus.	machine translation	Dmitrijs Milajevs	2017			computer science;natural language processing;artificial intelligence;latvian	NLP	-29.53627538801452	-74.42619539911972	18089
538fbb4716917085fdd7b72547e5bb93902b102b	domain-specific entity and relationship extraction from query logs	generality;entity extraction;query logs;ontology	Extracting domain-specific entity-relationships is useful in a wide variety of applications. For example, knowledge of camera companies and their product hierarchies can help photography-related search engines greatly in improving search interfaces. In this paper we describe an unsupervised approach for extracting prominent domain specific entityrelationships from query logs. Our approach is complementary to other entity extraction methods. It first constructs a weighted directed graph with query keywords as nodes and then prunes out edges not likely to represent useful relations. Experiments with multiple domains show promising results with over 80% precision.	directed graph;experiment;named-entity recognition;relationship extraction;unsupervised learning;web search engine	Parikshit Sondhi;Raman Chandrasekar	2010		10.1002/meet.14504701458	query optimization;query expansion;computer science;ontology;data mining;database;web search query;information retrieval	Web+IR	-27.57364276104639	-64.00445734982179	18104
a0bf08e0e4bfe4a334ef388a8521fa9d36f3d547	towards human-machine collaboration in creating an evaluation corpus for adverse drug events in discharge summaries of electronic medical records	text mining;discharge summaries;narrative medical reports;adverse drug events;natural language processing	Adverse drug events (ADEs) contribute significantly to morbidity and mortality in the healthcare system. The availability of digitalised hospitalsu0027 narrative clinical data offers a potentially rich resource to enhance pharmacovigilance efforts to manage potential safety issues arising from real-world use of drugs. The goal of this paper was to establish a foundation for creating an evaluation corpus by developing a set of annotation guidelines to achieve high inter-annotator agreement (IAA) and to evaluate the performance of basic entity identification tools for drugs, adverse events (AEs) and drug-AE relationships from 100 discharge summaries of a tertiary hospital in Singapore. Two teams of three annotators worked independently on text annotation using Knowtator. Three-way IAA of 86%, 70% and 49% were achieved for drugs, AEs and drug-AE relationships respectively. The performance of the machine algorithm was evaluated against annotations made by at least two annotators, with a recall of 84% and precision of 73% for drugs and a recall of 67% and precision of 53% for AEs. The high recall and precision for drug entity extraction suggests that machine pre-annotation of drugs followed by human annotation of AEs and drug-AE relationships could be a feasible approach in expediting the process of creating a larger evaluation corpus. Non-matches between machine and human annotations were examined to identify ways to further refine the algorithm. When successfully implemented, the identification of ADEs could greatly support pharmacovigilance work in characterising the magnitude and scope of ADEs and prioritising interventions to improve the drug safety.	discharger	Pei San Ang;Liza Y. P. Fan;Mun Yee Tham;Siew Har Tan;Sally B. L. Soh;Belinda P. Q. Foo;Celine W. P. Loke;Shangfeng Hu;Cynthia Sung	2016	Big Data Research	10.1016/j.bdr.2016.04.001	text mining;computer science;data science;data mining	ML	-49.8735859160054	-68.83660126125028	18114
f1e8f81a776ddefb621007370c56454f47c0cdce	automated grammatical error correction for language learners		A fast growing area in Natural Language Processing is the use of automated tools for identifying and correcting grammatical errors made by language learners. This growth, in part, has been fueled by the needs of a large number of people in the world who are learning and using a second or foreign language. For example, it is estimated that there are currently over one billion people who are non-native writers of English. These numbers drive the demand for accurate tools that can help learners to write and speak proficiently in another language. Such demand also makes this an exciting time for those in the NLP community who are developing automated methods for grammatical error correction (GEC). Our motivation for the COLING tutorial is to make others more aware of this field and its particular set of challenges. For these reasons, we believe that the tutorial will potentially benefit a broad range of conference attendees. In general, there has been a surge in interest in using NLP to address educational needs, which in turn, has spawned the recurring ACL/NAACL workshop “Innovative Use of Natural Language Processing for Building Educational Applications” that had its 9th edition at ACL 2014. The last three years, in particular, have been pivotal for GEC. Papers on the topic have become more commonplace at main conferences such as ACL, NAACL and EMNLP, as well as two editions of a Morgan Claypool Synthesis Series book on the topic (Leacock et al., 2010; Leacock et al., 2014). In 2011 and 2012, the first shared tasks in GEC (Dale and Kilgarriff, 2011; Dale et al., 2012) were created, and dozens of teams from all over the world participated. This was followed by two successful CoNLL Shared Tasks on the topic in 2013 and 2014 (Ng et al., 2013; Ng et al., 2014). While there have been many exciting developments in GEC over the last few years, there is still considerable room for improvement as state-of-the-art performance in detecting and correcting several important error types is still inadequate for real world applications. We hope to engage researchers from other NLP fields to develop novel and effective approaches to these problems. Our tutorial is specifically designed to:	empirical methods in natural language processing;error detection and correction;morgan;sensor;word lists by frequency	Joel R. Tetreault;Claudia Leacock	2014			natural language processing;error detection and correction;speech recognition;computer science;machine learning;linguistics	NLP	-38.78425057519983	-78.35716058024339	18122
78131b8e84df30df4094907ceb8301be6b6e9308	learning to recommend questions based on public interest	public interest;cqa;question recommendation;language model;question answering	This paper is concerned with the problem of question recommendation in the setting of Community Question Answering (CQA). Given a question as query, our goal is to rank all of the retrieved questions according to their likelihood of being good recommendations for the query. In this paper, we propose a notion of public interest, and show how public interest can boost the performance of question recommendation. In particular, to model public interest in question recommendation, we build a language model to combine relevance score to the query and popularity score regarding question popularity. Experimental results on Yahoo!Answers dataset demonstrate the performance of question recommendation can be greatly improved with considering the public interest.	language model;question answering;relevance	Jun Wang;Xia Hu;Zhoujun Li;Wen-Han Chao;Biyun Hu	2011		10.1145/2063576.2063882	natural language processing;question answering;computer science;data mining;world wide web;information retrieval;language model	Web+IR	-27.649994477194152	-53.60096787839024	18131
38d95819043237bf78eca0e17891885b76cc553c	facilitating physicians' access to information via tailored text summarization	questionnaires;information technology;analysis of variance;information services;physicians;computer science	We have developed a summarization system, TAS (Technical Article Summarizer), which, when provided with a patient record and journal articles returned by a search, automatically generates a summary that is tailored to the patient characteristics. We hypothesize that a personalized summary will allow a physician to more quickly find information relevant to patient care. In this paper, we present a user study in which subjects carried out a task under three different conditions: using search results only, using a generic summary and search results, and using a personalized summary with search results. Our study demonstrates that subjects do a better job on task completion with the personalized summary, and show a higher level of satisfaction, than under other conditions.	automatic summarization;generic drugs;pentalogy of cantrell;personalization;thermal-assisted switching;usability testing	Noémie Elhadad;Kathleen McKeown;David R. Kaufman;Desmond A. Jordan	2005	AMIA ... Annual Symposium proceedings. AMIA Symposium		automatic summarization;information retrieval;information system;information technology;data mining;computer science	NLP	-51.422189221869395	-68.62772741312803	18132
de33867d734570d64d993fcb02c16308cbd5411a	an experiment on human face recognition performance for access control	false reject rate;individual variation;face recognition;human factors;access control;experimental methodology;security	An experiment was conducted on human face recognition performance in an access control scenario. Ten judges compared fifty individuals to security ID style photos where 20% of the photos were of different people, assessed to look similar to the individual presenting the photo. Performance was better than that observed in the only other comparable live-to-photo experiment [1] with a false match rate of 9% [CI95%: 2%, 16%] in this study compared to 66% [CI95%: 50%, 82%] and a false reject rate of 5% [CI95%: 0%, 11%] compared to 14% [CI95%: 0.3%, 28%]. These differences were attributed to divergences in experimental methodology, especially with regards to the distractor tasks used. It is concluded that the figures provided in the current study are more appropriate estimates of performance in access control scenarios. Substantial individual variation in face matching abilities, response time and confidence ratings was observed.		Marcus A. Butavicius;Chloë Mount;Veneta MacLeod;Robyn Vast;Ian Graves;Jadranka Sunde	2008		10.1007/978-3-540-85563-7_23	psychology;artificial intelligence;communication;social psychology	Vision	-48.70869211008054	-54.25452710861451	18154
0ad1fd188974bb2079bd17ccf3fd3c6e5cc4f5f0	disseminating ambient assisted living in rural areas	software;smart home;assisted living facilities;rural population;ambient assisted living;real world application;rural area;humans;austria;field study;aged	The smart home, ambient intelligence and ambient assisted living have been intensively researched for decades. Although rural areas are an important potential market, because they represent about 80% of the territory of the EU countries and around 125 million inhabitants, there is currently a lack of applicable AAL solutions. This paper discusses the theoretical foundations of AAL in rural areas. This discussion is underlined by the achievements of the empirical field study, Casa Vecchia, which has been carried out over a four-year period in a rural area in Austria. The major goal of Casa Vecchia was to evaluate the feasibility of a specific form of AAL for rural areas: bringing AAL technology to the homes of the elderly, rather than moving seniors to special-equipped care facilities. The Casa Vecchia project thoroughly investigated the possibilities, challenges and drawbacks of AAL related to this specific approach. The findings are promising and somewhat surprising and indicate that further technical, interactional and socio-psychological research is required to make AAL in rural areas reasonable in the future.	atm adaptation layer;ambient intelligence;anthropology;assisted living;athysanella casa;capability maturity model;computational auditory scene analysis;eighty;electronic component;european union;field research;foundations;home automation;mind;requirement;seniors' health;simulation;sociology;solutions;usability	Gerhard Leitner;Alexander Felfernig;Anton Josef Fercher;Martin Hitz	2014		10.3390/s140813496	rural area;field research	HCI	-61.818124534500036	-55.304405371890056	18158
094497a46a8ffa0ef127ed909104bd3a83305e90	idiap at mediaeval 2013: search and hyperlinking task	video hyperlinking;video search;topic segmentation	The Idiap system for Search and Hyperlinking Task uses topic-based segmentation, content-based recommendation algorithms, and multimodal re-ranking. For both sub-tasks, our system performs better with automatic speech recognition output than with manual subtitles. For linking, the results benefit from the fusion of text and visual concepts detected in the anchors.	algorithm;html element;hyperlink;multimodal interaction;speech recognition	Chidansh Amitkumar Bhatt;Nikolaos Pappas;Maryam Habibi;Andrei Popescu-Belis	2013			computer vision;speech recognition;computer science;multimedia	NLP	-31.09664453768516	-62.546129868483554	18204
0c118562f9339bff3b9a5c2878d7c0a849bba47a	characteristics of successful technological interventions in mental resilience training	vulnerable population;human computer interaction;health program;qualitative research;systemdesign;interview;technology;pcs perceptual and cognitive systems;training;virtual reality;qualitative analysis;posttraumatic stress disorder;universiteitsbibliotheek;elss earth life and social sciences;vignette;war;human performances;system design;emotion;post traumatic stress disorder;human;information society;breathing exercise;emotional stress;informatics;feedback system;practice guideline;army;mental resilience;review;interaction design;psychotherapy;coping behavior	In the last two decades, several effective virtual reality-based interventions for anxiety disorders have been developed. Virtual reality interventions can also be used to build resilience to psychopathology for populations at risk of exposure to traumatic experiences and developing mental disorders as a result, such as for people working in vulnerable professions. Despite the interest among mental health professionals and researchers in applying new technology-supported interventions for pre-trauma mental resilience training, there is a lack of recommendations about what constitutes potentially effective technology-supported resilience training. This article analyses the role of technology in the field of stress-resilience training. It presents lessons learned from technology developers currently working in the area, and it identifies some key clinical requirements for the supported resilience interventions. Two processes made up this research: 1) developers of technology-assisted resilience programs were interviewed regarding human-computer interaction and system development; 2) discussions with clinicians were prompted using technology-centered concept storyboards to elicit feedback, and to refine, validate and extend the initial concepts. A qualitative analysis of the interviews produced a set of development guidelines that engineers should follow and a list of intervention requirements that the technology should fulfill. These recommendations can help bridge the gap between engineers and clinicians when generating novel resilience interventions for people in vulnerable professions.	anxiety disorders;engineering;experience;human–computer interaction;mental disorders;occupations;population;post-traumatic stress disorder;psychopathology;requirement;storyboard;virtual reality;wounds and injuries;mental health	Vanessa Vakili;Willem-Paul Brinkman;Nexhmedin Morina;Mark A. Neerincx	2014	Journal of Medical Systems	10.1007/s10916-014-0113-2	psychiatry;medicine;computer science;qualitative research;nursing;virtual reality;management	HCI	-56.93554997843766	-55.13870925927195	18208
fec3d10cb524235578535346ef8ab8e96d4339cc	development and implementation of a floor admit reevaluation alert (fara) in a large academic emergency department			3d floor plan	James Booth;Eta S. Berner;Jorge A. Alsip	2015			emergency department;emergency medicine;engineering	NLP	-57.37903211328633	-64.2736594374089	18219
42e31c3bdcf5eea3801a035203b0c05bd2bf9328	review: word sense disambiguation: the case for combinations of knowledge sources, by mark stevenson			word sense;word-sense disambiguation	Zdenek Zabokrtský	2003	Prague Bull. Math. Linguistics		natural language processing;artificial intelligence;linguistics;semeval;computer science;speech recognition;word-sense disambiguation	NLP	-30.182731237030467	-77.36424015959358	18224
3af1c14dffd8d7da18ba443e58a98297f01f5310	tools for enabling digital access to multi-lingual indic documents	language use;document image analysis;image segmentation;software libraries;virtual keyboard;automatic segmentation;optical character recognition;multilingual indian document image;transliterated english text;text analysis;natural languages;data representation;devanagari document image;multilingual indic document;data structures;xml;ocr;document image processing;xml document;meta data;image analysis;ground truth;conferences text analysis image analysis software libraries;user interfaces;multilingual indic document multilingual indian document image document image analysis devanagari document image ocr indian language transliterated english text virtual keyboard unicode data representation xml document meta data;unicode data representation;text analysis document image processing optical character recognition image segmentation user interfaces natural languages xml data structures meta data;conferences;indian language	We present methodologies for three important tasks that will eventually enable digital access of multilingual Indian document images. First, we describe several document image analysis techniques necessary to prepare Devanagari document images for OCR. The second task is OCR for machine printed Devanagari words without the help of a lexicon. We describe the OCR methodology and show how it is being extended to other Indian languages. Finally, we describe a versatile platform that facilitates automatic segmentation of document images in multiple Indian languages and an interface to capture the ground truth corresponding to the text. We use transliterated English text and virtual keyboards in a range of Indian languages for this purpose. The multilingual data entry capabilities of the tool and its underlying UNICODE data representation within a structured XML document also allow users to annotate passages of text in one language in other languages using a markup scheme to switch between scripts. Text and annotations are rendered in the appropriate scripts as the text is being annotated, thus providing users prompt and natural feedback. The XML back-end allows meta-data to be recorded describing the annotated document.	data (computing);ground truth;image analysis;lexicon;markup language;optical character recognition;printing;unicode;virtual keyboard;xml	Venu Govindaraju;Swapnil Khedekar;Suryaprakash Kompalli;Faisal Farooq;Srirangaraj Setlur;Vemulapati Ramanaprasad	2004	First International Workshop on Document Image Analysis for Libraries, 2004. Proceedings.	10.1109/DIAL.2004.1263244	well-formed document;natural language processing;speech recognition;computer science;document schema definition languages;information retrieval	Web+IR	-35.49651118875837	-74.8665718113463	18258
204a4962f67b11a8850a6d212007fc3b01080644	semeval-2012 task 4: evaluating chinese word similarity	semeval-2012 task;tau value;chinese word similarity;twenty undergraduate;gold standard;word similarity computation;gold standard data;word pair;average value;similarity score;twenty annotators;chinese linguistics	This task focuses on evaluating word similarity computation in Chinese. We follow the way of Finkelstein et al. (2002) to select word pairs. Then we organize twenty undergraduates who are major in Chinese linguistics to annotate the data. Each pair is assigned a similarity score by each annotator. We rank the word pairs by the average value of similar scores among the twenty annotators. This data is used as gold standard. Four systems participating in this task return their results. We evaluate their results on gold standard data in term of Kendall's tau value, and the results show three of them have a positive correlation with the rank manually created while the taus' value is very small.	computation;kendall tau distance;microsoft word for mac;semeval	Peng Jin;Yunfang Wu	2012			speech recognition;computer science;data mining;information retrieval	NLP	-26.442388811684268	-68.23763800043959	18318
ff282636d728dc8672f61da5e86b077cc3749b98	anaphora as an indicator of elaboration: a corpus study		entity anaphora abstrProp abstrCluster abstrEvType anaphora poss meronym holonym hasMember setMember bridging Figure 1: Sekimo hierarchy of anaphoric relations For cospecLink two sets of secondary relations exist: one set for relations with antecedents of nominal type and one set for abstract entity anaphora. The subtypes of abstract entity anaphora are characterised as follows: abstrProp describes anaphoric relations with an antecedent of propositional type, abstEvType describes anaphoric relations with an event type antecedent, and abstrCluster describes anaphoric relations where the anaphor refers to a cluster of propositions. For nominal antecedents, we annotate eight secondary relation types: The relation ident is chosen for pronominal anaphors or anaphor-antecedent pairs with identical head noun. The value propName is chosen if the anaphoric element is a proper name that refers to an NP antecedent.	anaphora (linguistics);bridging (networking);nominal type system	Maja Bärenfänger;Daniela Goecke;Mirco Hilbert;Harald Lüngen;Maik Stührenberg	2008	JLCL		natural language processing;linguistics	NLP	-33.14679859831692	-81.96512361438657	18351
2443ead47f5f6ad068a3bfefed017a69e39371d9	semantic addressable encoding	tratamiento automatico;categorisation;authorship;linguistique;word perception;analyse linguistique;semantics;indexing terms;proceso adquisicion;semantica;semantique;acquisition process;linguistic analysis;categorizacion;linguistica;automatic processing;analisis linguistico;content addressable memory;semantic search;reseau neuronal;traitement automatique;red neuronal;elman network;processus acquisition;personalized code;categorization;neural network;linguistics	This paper presents an automatic acquisition process to acquire the semantic meaning for the words. This process obtains the representation vectors for stemmed words by iteratively improving the vectors, using a trained Elman network [4]. Experiments performed on a corpus composed of Shakespeare’s writings show its linguistic analysis and categorization abilities.	categorization;experiment;national supercomputer centre in sweden;personalization;qr code;recurrent neural network;text corpus	Cheng-Yuan Liou;Jau-Chi Huang;Wen-Chie Yang	2006		10.1007/11893028_21	natural language processing;speech recognition;index term;semantic search;computer science;artificial intelligence;machine learning;content-addressable memory;semantics;artificial neural network;categorization	NLP	-26.70727375154329	-79.11624493694872	18366
23d1295c3b5fee0bee053a81a1944990aee81238	application of technology: design and implementation of a national clinical trials registry	search method;national institute of health;clinical trial;unified medical language system;food and drug administration;design and implementation;system design;web based system;system development	"""The authors have developed a Web-based system that provides summary information about clinical trials being conducted throughout the United States. The first version of the system, publicly available in February 2000, contains more than 4,000 records representing primarily trials sponsored by the National Institutes of Health. The impetus for this system has come from the Food and Drug Administration (FDA) Modernization Act of 1997, which mandated a registry of both federally and privately funded clinical trials """"of experimental treatments for serious or life-threatening diseases or conditions."""" The system design and implementation have been guided by several principles. First, all stages of system development were guided by the needs of the primary intended audience, patients and other members of the public. Second, broad agreement on a common set of data elements was obtained. Third, the system was designed in a modular and extensible way, and search methods that take extensive advantage of the National Library of Medicine's Unified Medical Language System (UMLS) were developed. Finally, since this will be a long-term effort involving many individuals and organizations, the project is being implemented in several phases."""	data element;patients;registries;systems design;unified medical language system;united states food and drug administration;web	Alexa T. McCray;Nicholas C. Ide	2000	Journal of the American Medical Informatics Association : JAMIA	10.1136/jamia.2000.0070313	medicine;nursing;clinical trial;data mining;database;unified medical language system;systems design	Graphics	-52.52416444317159	-63.70344790578181	18372
357af65dc1b09248b72e3c9604a887edb1cbb4ac	g-roi: automatic region-of-interest detection driven by geotagged social media data		Geotagged data gathered from social media can be used to discover interesting locations visited by users called Places-of-Interest (PoIs). Since a PoI is generally identified by the geographical coordinates of a single point, it is hard to match it with user trajectories. Therefore, it is useful to define an area, called Region-of-Interest (RoI), to represent the boundaries of the PoI’s area. RoI mining techniques are aimed at discovering ROIs from PoIs and other data. Existing RoI mining techniques are based on three main approaches: predefined shapes, density-based clustering, and grid-based aggregation. This article proposes G-RoI, a novel RoI mining technique that exploits the indications contained in geotagged social media items to discover RoIs with a high accuracy. Experiments performed over a set of PoIs in Rome and Paris using social media geotagged data, demonstrate that G-RoI in most cases achieves better results than existing techniques. In particular, the mean F1 score is 0.34 higher than that obtained with the well-known DBSCAN algorithm in Rome RoIs and 0.23 higher in Paris RoIs.	algorithm;cluster analysis;dbscan;experiment;f1 score;flickr;geographic coordinate system;geotagging;open-source software;sensor;social media;whole earth 'lectronic link	Loris Belcastro;Fabrizio Marozzo;Domenico Talia;Paolo Trunfio	2018	TKDD	10.1145/3154411	social network analysis;data mining;cluster analysis;region of interest;f1 score;social media;computer science;dbscan	ML	-24.651347826109717	-52.62814975778573	18400
dfdfe4153307be6304f37fde24439bec2ae3d1ab	rough set-based application to recognition of emotionally-charged animated character's gestures	spacing;espacement;key frame;imagen clave;espaciamiento;melangeage;gesture;rough set theory;localization;localizacion;identificacion sistema;feature vector;vie artificielle;evaluation subjective;localisation;system identification;emotion emotionality;theorie ensemble approximatif;mixed method;controle qualite;emotion emotivite;emocion emotividad;information system;mixing;rough set;quality control;subjective evaluation;mezclado;geste;systeme information;identification systeme;artificial life;image cle;animated character;control calidad;gesto;evaluacion subjetiva;sistema informacion	This research study is intended to analyze emotionally-charged animated character's gestures. Animation methods and rules are first shortly reviewed in this paper. Then the experiment layout is presented. For the purpose of the experiment, the keyframe method is used to create animated objects characterized by differentiating emotions. The method comprised the creation of an animation achieved by changing the properties of a temporal structure of an animated sequence. The sequence is then analyzed in terms of identifying the locations and spacing of keyframes, as well as the features that could be related to emotions present in the animation. On the basis of this analysis several parameters contained in feature vectors describing each object emotions at key moments are derived. The labels are assigned to particular sequences by viewers participating in subjective tests. This served as a decision attribute. The rough set system is used to process the data. Rules related to various categories of emotions are derived. They are then compared with the ones used in traditional animation. Also, the most significant parameters are identified. The second part of the experiment is aimed at checking the viewers' ability to discern less dominant emotional charge in gestures. A time-mixing method is proposed and utilized for the generation of new gestures emotionally-charged with differentiated intensity. Viewers' assessment of the animations quality is presented and analyzed. Conclusions and future experiments are shortly outlined.	rough set	Bozena Kostek;Piotr Szczuko	2006		10.1007/11847465_7	computer vision;rough set;computer science;artificial intelligence;machine learning	HCI	-8.685511879030132	-68.31348501670591	18413
76e3c9965b15ccfa95956b8c77de5decc37a7752	singular value decomposition based low-footprint speaker adaptation and personalization for deep neural network	kld singular value decomposition svd low footprint speaker adaptation deep neural network dnn adaptation automatic speech recognition asr speaker personalization storage cost weight matrix speaker independent dnn low rank matrices square matrix delta matrix short message dictation task kullback leibler divergence;singular value decomposition deep neural network speaker adaptation speaker personalization;speech recognition matrix algebra neural nets singular value decomposition;adaptation models silicon hidden markov models matrix decomposition data models neural networks accuracy	The large number of parameters in deep neural networks (DNN) for automatic speech recognition (ASR) makes speaker adaptation very challenging. It also limits the use of speaker personalization due to the huge storage cost in large-scale deployments. In this paper we address DNN adaptation and personalization issues by presenting two methods based on the singular value decomposition (SVD). The first method uses an SVD to replace the weight matrix of a speaker independent DNN by the product of two low rank matrices. Adaptation is then performed by updating a square matrix inserted between the two low-rank matrices. In the second method, we adapt the full weight matrix but only store the delta matrix - the difference between the original and adapted weight matrices. We decrease the footprint of the adapted model by storing a reduced rank version of the delta matrix via an SVD. The proposed methods were evaluated on short message dictation task. Experimental results show that we can obtain similar accuracy improvements as the previously proposed Kullback-Leibler divergence (KLD) regularized method with far fewer parameters, which only requires 0.89% of the original model storage.	artificial neural network;deep learning;kullback–leibler divergence;personalization;singular value decomposition;speech recognition	Jian Xue;Jinyu Li;Dong Yu;Mike Seltzer;Yifan Gong	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6854828	speech recognition;computer science;machine learning;pattern recognition	Robotics	-17.58564233626777	-90.12213165276918	18443
4218fb024b2ab1837dfebc81e5d814de5de9b811	model based binaural enhancement of voiced and unvoiced speech		This paper deals with the enhancement of speech in presence of non-stationary babble noise. A binaural speech enhancement framework is proposed which takes into account both the voiced and unvoiced speech production model. The usage of this model in enhancement requires the Short term predictor (STP) parameters and the pitch information to be estimated. This paper uses a codebook based approach for estimating the STP parameters and a parametric binaural method is proposed for estimating the pitch parameters. Improvements in objective score are shown when using the voiced-unvoiced speech model in comparison to the conventional unvoiced speech model.	binaural beats;codebook;kerrison predictor;long short-term memory;parametric polymorphism;speech enhancement;stationary process	Mathew Shaji Kavalekalam;Mads Græsbøll Christensen;Jesper Bünsow Boldt	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7952239	voice activity detection;binaural recording;noise measurement;pattern recognition;artificial intelligence;speech coding;speech recognition;speech enhancement;computer science;parametric statistics;intelligibility (communication);speech production	Robotics	-13.57915385143381	-93.30544408654764	18461
c59135551e86277174d6cc3bd37f76dab9ac87e7	on the suitability of vocalic sandwiches in a corpus-based tts engine		Unit selection speech synthesis systems generally rely on target and concatenation costs for selecting the best unit sequence. The role of the concatenation cost is to insure that joining two voice segments will not cause any acoustic artefact to appear. For this task, acoustic distances (MFCC, F0) are typically used but in many cases, this is not enough to prevent concatenation artefacts. Among other strategies, the improvement of corpus covering by favoring units that naturally support well the joining process (vocalic sandwiches) seems to be effective on TTS. In this paper, we investigate if vocalic sandwiches can be used directly in the unit selection engine when the corpus was not created using that principle. First, the sandwich approach is directly transposed in the unit selection engine with a penalty that greatly favors concatenation on sandwich boundaries. Second, a derived fuzzy version is proposed to relax the penalty based on the concatenation cost, with respect to the cost distribution. We show that the sandwich approach, very efficient at the corpus creation step, seems to be inefficient when directly transposed in the unit selection engine. However, we observe that the fuzzy approach enhances synthesis quality, especially on sentences with high concatenation costs.	acoustic cryptanalysis;concatenation;fuzzy logic;netware file system;speech synthesis;visual artifact	David Guennec;Damien Lolive	2016		10.21437/Interspeech.2016-1222	fuzzy logic;speech recognition;pattern recognition;artificial intelligence;computer science;mel-frequency cepstrum;speech synthesis;concatenation	NLP	-19.74431007349665	-83.53529280451164	18475
aaf31ff69656b04c16bb3d8d608254b015e77586	harnessing twitter for answering opinion list queries		Opinion list (OL) queries like “valentines day gift ideas” and “best anniversary messages for your parents” are quite popular on web search engines. Users expect instant answers comprising of a list of relevant items (OL) for such a query. Surprisingly, current search engines do not provide any crisp instant answers for queries in this critical query segment. To the best of our knowledge, we present the first system that tackles such queries. Although such social factors are heavily discussed on online social networks like Twitter, extracting such lists from tweets is quite challenging. The challenges lie in discovering such lists from tweets, rank the discovered list items as well as handle lists with very low cardinality (tail OLs). We present an end-to-end system that: 1) identifies these “OLs” from a large number of Twitter hashtags using a classifier trained using novel task-specific features; 2) extracts suitable list answers from relevant tweets using carefully designed regex patterns; 3) uses the learning to rank framework to present a ranked list of these items; and 4) handles tail lists using a novel algorithm to borrow list items from similar lists. Crowd-sourced evaluation shows that the proposed system can extract OLs with a good accuracy.		Ankan Mullick;Pawan Goyal;Niloy Ganguly;Manish Gupta	2018	IEEE Transactions on Computational Social Systems	10.1109/TCSS.2018.2881186	sentiment analysis;learning to rank;cardinality;data mining;search engine;computer science;classifier (linguistics);social network;ranking	Web+IR	-24.257661574118167	-53.28130776216226	18479
54eb795a26e432a89c077a3aba352507bb113c58	machine learning for reading order detection in document image understanding	image understanding;learning methods;machine learning;document image processing;domain specificity;spatial information;semantic analysis	Document image understanding refers to logical and semantic analysis of document images in order to extract information understandable to humans and codify it into machine-readable form. Most of the studies on document image understanding have targeted the specific problem of associating layout components with logical labels, while less attention has been paid to the problem of extracting relationships between logical components, such as cross-references. In this chapter, we investigate the problem of detecting the reading order relationship between components of a logical structure. The domain specific knowledge required for this task is automatically acquired from a set of training examples by applying a machine learning method. The input of the learning method is the description of “chains” of layout components defined by the user. The output is a logical theory which defines two predicates, first to read/1 and succ in reading/2, useful for consistently reconstructing all chains in the training set. Only spatial information on the page layout is exploited for both single and multiple chain reconstruction. The proposed approach has been evaluated on a set of document images processed by the system WISDOM++.	computer vision;cross-reference;embedded system;human-readable medium;inductive logic programming;inductive reasoning;information extraction;information retrieval;machine learning;rendering (computer graphics);sensor;test set	Donato Malerba;Michelangelo Ceci;Margherita Berardi	2008		10.1007/978-3-540-76280-5_3	natural language processing;computer science;machine learning;document layout analysis;data mining;design document listing	AI	-13.589773367276498	-63.62261224060169	18482
d89b73512c49f8bd1e4904640fa7580ea5d9b5b4	medical entity recognition: a comparaison of semantic and statistical methods		Medical Entity Recognition is a crucial step towards efficient medical texts analysis. In this paper we present and compare three methods based on domain-knowledge and machine-learning techniques. We study two research directions through these approaches: (i) a first direction where noun phrases are extracted in a first step with a chunker before the final classification step and (ii) a second direction where machine learning techniques are used to identify simultaneously entities boundaries and categories. Each of the presented approaches is tested on a standard corpus of clinical texts. The obtained results show that the hybrid approach based on both machine learning and domain knowledge obtains the best performance.	categorization;conditional random field;ensemble learning;entity;experiment;machine learning;mer;question answering;text corpus	Asma Ben Abacha;Pierre Zweigenbaum	2011			natural language processing;computer science;pattern recognition;data mining	NLP	-24.63432114218095	-69.63766863262445	18506
b7b61b2624fdb0eeae1ab06a1e2920c8e360620e	a preliminary study for building the basque propbank	basque propbank;semantic annotation;propbank;semantic roles	This paper presents a methodology for adding a layer of semantic annotation to a syntactically annotated corpus of Basque (EPEC), in terms of semantic roles. The proposal we make here is the combination of three resources: the model used in the PropBank project (Palmer et al., 2005), an in-house database with syntactic/semantic subcategorization frames for Basque verbs (Aldezabal, 2004) and the Basque dependency treebank (Aduriz et al., 2003). In order to validate the methodology and to confirm whether the PropBank model is suitable for Basque and our treebank design, we have built lexical entries and labelled all argument and adjuncts occurring in our treebank for 3 Basque verbs. The result of this study has been very positive, and has produced a methodology adapted to the characteristics of the language and the Basque dependency treebank. Another goal of this study was to study whether semi-automatic tagging was possible. The idea is to present the human taggers a pre-tagged version of the corpus. We have seen that many arguments could be automatically tagged with high precision, given only the verbal entries for the verbs and a handful of examples.	coherence (physics);eurowordnet;lexicon;propbank;semiconductor industry;text corpus;treebank;verbnet;word-sense disambiguation;wordnet	Eneko Agirre;Izaskun Aldezabal;Jone Etxeberria;Eli Pociello	2006			natural language processing;speech recognition;computer science;treebank;linguistics	NLP	-28.650615172641313	-72.45016751254711	18536
c26ac45ae1f3507b874e72249ea5ab4873259f78	excitatory or inhibitory: a new semantic orientation extracts contradiction and causality from the web	million contradiction pair;automatic acquisition method;new semantic orientation;excitation knowledge;semantic property;million page web corpus;causality pair;reasonable precision;million plausible causality hypothesis;contradiction pair	This supplementary material describes the list of seed templates, the list of connectives, the English translations of our annotation manuals, and additional examples of contradiction, causality, and the causality hypothesis. Although this supplementary material may seem oversupplied, we believe that we must describe all of the details, since we propose a novel concept, Excitation, a novel task, Excitation knowledge acquisition, and novel knowledge acquisition methods based on Excitation in our paper.	causality;knowledge acquisition;logical connective;world wide web	Chikara Hashimoto;Kentaro Torisawa;Stijn De Saeger;Jong-Hoon Oh;Jun'ichi Kazama	2012			natural language processing;artificial intelligence;machine learning	NLP	-26.771769000900665	-70.64558879881619	18537
09d62b61b136c676e6d9250a1dd55dc7eccae73e	end-to-end goal-driven web navigation		We propose a goal-driven web navigation as a benchmark task for evaluating an agent with abilities to understand natural language and plan on partially observed environments. In this challenging task, an agent navigates through a website, which is represented as a graph consisting of web pages as nodes and hyperlinks as directed edges, to find a web page in which a query appears. The agent is required to have sophisticated high-level reasoning based on natural languages and efficient sequential decision-making capability to succeed. We release a software tool, called WebNav, that automatically transforms a website into this goal-driven web navigation task, and as an example, we make WikiNav, a dataset constructed from the English Wikipedia. We extensively evaluate different variants of neural net based artificial agents on WikiNav and observe that the proposed goal-driven web navigation well reflects the advances in models, making it a suitable benchmark for evaluating future progress. Furthermore, we extend the WikiNav with questionanswer pairs from Jeopardy! and test the proposed agent based on recurrent neural networks against strong inverted index based search engines. The artificial agents trained on WikiNav outperforms the engined based approaches, demonstrating the capability of the proposed goal-driven navigation as a good proxy for measuring the progress in real-world tasks such as focused crawling and question-answering.	agent-based model;artificial neural network;automated planning and scheduling;benchmark (computing);focused crawler;high- and low-level;hyperlink;intelligent agent;inverted index;natural language;natural language understanding;paging;programming tool;question answering;recurrent neural network;testbed;web navigation;web page;web search engine;wikipedia	Rodrigo Nogueira;Kyunghyun Cho	2016			web modeling;computer science;artificial intelligence;machine learning;web navigation;data mining;database;world wide web	ML	-17.214820674831575	-68.32946067664189	18549
14b2d744a92534a9983207114e8604595a5a6a38	session 13: csr search	real time;search algorithm;speech recognition;speaker adaptation	This session had five papers related to different topics in CSR Search. The topics ranged from integration of many knowledge sources within a practical system, to different search algorithms for real-time large vocabulary speech recognition.	real-time transcription;search algorithm;speech recognition;vocabulary	Richard M. Schwartz	1994			natural language processing;speaker recognition;speaker diarisation;speech recognition;computer science;search algorithm	ML	-15.583954873995335	-86.29784441555444	18564
1a0499212b0a75dd11a3bd59734e4d3f9edfd64d	the physiological, acoustic, and perceptual basis of high back vowel fronting: evidence from german tense and lax vowels		High back rounded vowels are prone synchronically to fronting in a fronting context and diachronically they are more likely to front than high front vowels are to retract. In order to shed light on the reasons for this back–front asymmetry, tense and lax vowels produced in three place of articulation contexts by seven first-language speakers of German at two speech rates were analysed physiologically, acoustically, and perceptually. An articulographic analysis showed greater magnitudes and peak velocities of horizontal tongue dorsum movement in CV transitions for /u:, o:/ than for /e:, i:/. A second experiment showed that the difference between tense and lax vowels in the tongue dorsum's horizontal position was greater for back than front vowels. A third experiment showed /u:, ʊ/ were more likely to encroach on the /y:, ʏ/ spaces than the other way round for measurements based both on the horizontal tongue dorsum position and on spectral slope; a similar pattern of results emerged in a forced-choice perception experiment. The general conclusion is that high back vowels that are as peripheral as those in German have a high articulatory cost which may explain both the diachronic tendency for back vowels to front and why the absence of a high back vowel often contributes to asymmetric vowel distributions in the world's languages.	acoustic cryptanalysis	Jonathan Harrington;Phil Hoole;Felicitas Kleber;Ulrich Reubold	2011	J. Phonetics	10.1016/j.wocn.2010.12.006	speech recognition;acoustics;relative articulation;communication	NLP	-10.21790707181814	-81.82853224457365	18577
287cb18d6e213c6f9f89a3e3c8470697e61e9035	interpreting fine-grained categories from natural language queries of entity search		The fine-grained target categories/types are very critical for improving the performance of entity search because they can be used for retrieving relevant entities by filtering irrelevant entities with a high confidence. However, most solutions of entity search face an urgent problem, i.e., the lack of fine-grained target categories of queries, which are hard for users to explicitly specify. In this paper, we try to interpret fine-grained categories from natural language based queries of entity search. We observe that entity search queries often contain terms specifying the contexts of the desired entities, as well as a topic of the desired entities. Accordingly, we propose to interpret fine-grained categories of entity search queries from the context perspective and the topic perspective. Therefore, we propose an approach by formalizing both context-based category model and topic-based category model, to tackle the category interpreting task. Extensive experiments on two widely-used test sets: INEX-XER 2009 and SemSearch-LS, indicate significant performance improvement achieved by our proposed method over the state-of-the-art baselines.	natural language	Denghao Ma;Yueguo Chen;Xiaoyong Du;Yuanzhe Hao	2018		10.1007/978-3-319-91452-7_55	data mining;computer science;baseline (configuration management);performance improvement;natural language	NLP	-17.00947450181948	-65.8598797556993	18587
0e4aafe727018cd795f4294dab05b628a530422d	domain specific commonsense relation extraction from bag of concepts metadata	commonsense relation;metadata;relation extraction	"""Existing semantic knowledge bases such as WordNet and Yago contain the information of relations between entities. They do not hold the information about domain specific commonsense relations between concepts like """"horse"""" and """"farm"""" which intuitively have close relations on semantics in the domains of image description. Metadata which is used to describe data is widespread in the data collections of various domains and can be useful resources for relation extraction. However, keywords and tags which are important form of metadata are only list of user generated words. They do not contain syntactic information which many existing works use to extract relations. In this paper we propose an approach to collect commonsense relations for specific domains by mining knowledge of global structure and internal association in the bag of concepts from metadata of data collections. We extract commonsense relations of concepts from social tags of image datasets to show the efficiency of our solution."""	commonsense knowledge (artificial intelligence);commonsense reasoning;entity;relationship extraction;wordnet;yago	Jiyi Li	2015		10.1145/2701126.2701159	relationship extraction;computer science;data mining;database;commonsense knowledge;metadata;information retrieval;metadata repository	AI	-29.68961464997082	-66.3117422417808	18600
8d85242a4e93b377dde676418888147705e7c1c2	named entity recognition using appropriate unlabeled data, post-processing and voting	unlabeled data;named entity recognition	This paper reports how the appropriate unlabeled data, post-processing and voting can be effective to improve the performance of a Named Entity Recognition (NER) system. The proposed method is based on a combination of the following classifiers: Maximum Entropy (ME), Conditional Random Field (CRF) and Support Vector Machine (SVM). The training set consists of approximately 272K wordforms. The proposed method is tested with Bengali. A semi-supervised learning technique has been developed that uses the unlabeled data during training of the system. We have shown that simply relying upon the use of large corpora during training for performance improvement is not in itself sufficient. We describe the measures to automatically select effective documents and sentences from the unlabeled data. In addition, we have used a number of techniques to post-process the output of each of the models in order to improve the performance. Finally, we have applied weighted voting approach to combine the models. Experimental results show the effectiveness of the proposed approach with the overall average recall, precision, and f-score values of 93.79%, 91.34%, and 92.55%, respectively, which shows an improvement of 19.4% in f-score over the least performing baseline ME based system and an improvement of 15.19% in f-score over the best performing baseline SVM based system.	archive;baseline (configuration management);conditional random field;f1 score;heuristic (computer science);maximum entropy spectral estimation;named entity;named-entity recognition;orthographic projection;semi-supervised learning;semiconductor industry;supervised learning;support vector machine;test set;text corpus;unsupervised learning;video post-processing;web archiving	Asif Ekbal;Sivaji Bandyopadhyay	2010	Informatica (Slovenia)		computer science;machine learning;pattern recognition;data mining	AI	-22.891092491009342	-71.05820190629359	18605
e49a63952132ba002b0822d2c5f9d7d521823816	a progressive sentence selection strategy for document summarization	progressive sentence selection;document summarization;asymmetric sentence relationship;saliency and coverage	Saliency and coverage are two of the most important issues in document summarization. In most summarization methods, the saliency issue is usually of top priority. Many studies are conducted to develop better sentence ranking methods to identify the salient sentences for summarization. It is also well acknowledged that sentence selection strategies are very important, which mainly aim at reducing the redundancy among the selected sentences to enable them to cover more concepts. In this paper, we propose a novel sentence selection strategy that follows a progressive way to select the summary sentences. We intend to ensure the coverage of the summary first by an intuitive idea, i.e., considering the uncovered concepts only when measuring the saliency of the sentences. Moreover, we consider the subsuming relationship between sentences to define a conditional saliency measure of the sentences instead of the general saliency measures used in most existing methods. Based on these ideas, a progressive sentence selection strategy is developed to discover the ''novel and salient'' sentences. Compared with traditional methods, the saliency and coverage issues are more integrated in the proposed method. Experimental studies conducted on the DUC data sets demonstrate the advantages of the progressive sentence selection strategy.	automatic summarization	Ouyang You;Wenjie Li;Renxian Zhang;Sujian Li;Qin Lu	2013	Inf. Process. Manage.	10.1016/j.ipm.2012.05.002	natural language processing;speech recognition;computer science;automatic summarization;pattern recognition;information retrieval	NLP	-26.506796127328972	-62.97839875942464	18618
3cc54557ef9f5df31e865ea6be3f0b8139911152	promoting workflow integration with information management services and gem-encoded guidelines	biomedical research;bioinformatics	Background Integrating guideline knowledge with clinical workflow plays a critical role in effective guideline implementation strategies. Eight information management services promote integration of computerized systems into clinical workflow—recommendation, documentation, explanation, presentation, registration, communication, calculation and aggregation. The Guideline Elements Model (GEM) uses XML to standardize encoding of the content of guideline documents. A guideline implementation application designed to incorporate a variety of information management services can apply components from a GEMencoded guideline document to facilitate workflow integration.	documentation;information management;xml	Abha Agrawal;Cynthia Brandt;Richard N. Shiffman	2000			xml;information management;documentation;data mining;workflow;guideline;computer science	DB	-52.09814445479129	-64.84445372176266	18624
5094ad4eb0d45ecd555f3d5e1d6630366c7f7512	cross-linguistic study of the production of turn-taking cues in american english and argentine spanish		We present the results of a series of machine learning experiments aimed at exploring the differences and similarities in the production of turn-taking cues in American English and Argentine Spanish. An analysis of prosodic features automatically extracted from 21 dyadic conversations (12 En, 9 Sp) revealed that, when signaling Holds, speakers of both languages tend to use roughly the same combination of cues, characterized by a sustained final intonation, a shorter duration of turn-final interpausal units, and a distinct voice quality. However, in speech preceding Smooth Switches or Backchannels, we observe the existence of the same set of prosodic turn-taking cues in both languages, although the ways in which these cues are combined together to form complex signals differ. Still, we find that these differences do not degrade below chance the performance of cross-linguistic systems for automatically detecting turn-taking signals. These results are relevant to the construction of multilingual spoken dialogue systems, which need to adapt not only their ASR modules but also the way prosodic turn-taking cues are synthesized and recognized.	dialog system;dyadic transformation;experiment;linguistic systems;machine learning;network switch;sensor	Pablo Brusco;Juan Manuel Vidal Pérez;Agustín Gravano	2017			american english;linguistics;turn-taking;computer science	NLP	-13.257003648190402	-82.58862461349592	18648
512d61f14d5e92e7d48a2343fc9fe364404080a5	voice source features for forensic voice comparison - an evaluation of the glottex software package		GLOTTEX R ⃝ is a software package which extracts information about voice source properties, including estimates of properties related to physical structures of the vocal folds. It has been proposed that the output of GLOTTEX R ⃝ can be used as part of a forensic-voice-comparison system. We test this using manually labeled segments from a database of voice recordings of 60 female Chinese speakers. Performance was assessed relative to a baseline MFCC GMM-UBM system. GMM-UBM systems based on features extracted by GLOTTEX R ⃝ were combined with the baseline system using logistic-regression fusion. System performance was assessed in three channel conditions: high-quality v high-quality, mobile-to-landline v mobileto-landline, and mobile-to-landline v high-quality. Substantial improvements over the baseline system were not observed.	baseline (configuration management);google map maker;landline;logistic regression	Ewald Enzinger;Cuiling Zhang;Geoffrey Stewart Morrison	2012			forensic science;software;speech recognition;computer science	SE	-12.701631295236949	-89.24843411604596	18678
3b2d7701cce5841e3c000adf76aed5f0c740f680	objective-c - a guide to language fundamentals: pocket reference			handbook;objective-c	Andrew M. Duncan	2003			medicine;human–computer interaction;computer science	NLP	-55.618458671543905	-58.08329631644541	18691
0685d347ed72cb5ddf4c6525f99038849171a0c2	audio signal classification using time-frequency parameters	template generation audio signal classification time frequency parameters multimedia technologies content based retrieval audio databases signal characteristics extraction adaptive time frequency decomposition algorithm audio signal decomposition similarity measure correct classification accuracy pattern matching;audio segmentation;audio signal processing;audio classification;time frequency;pattern classification time frequency analysis multimedia databases signal processing audio databases information retrieval content based retrieval spatial databases signal resolution signal generators;adaptive signal processing;decomposition algorithm;feature extraction;signal classification;matching pursuit;audio databases;classification accuracy;audio databases audio signal processing signal classification adaptive signal processing time frequency analysis content based retrieval feature extraction;similarity measure;content based retrieval;time frequency analysis	The ongoing advancements in the multimedia technologies drive the need for efficient classification of the audio signals to make the content-based retrieval process more accurate and much easier from huge databases. The challenge of this task lies in an accurate extraction of signal characteristics so as to derive a strong discriminatory feature suitable for retrieval process. A time-frequency approach for audio classification is proposed. The audio signals were decomposed using an adaptive time-frequency decomposition algorithm, and the signal decomposition parameter octave (scale) was used to create patterns based on a similarity measure of the audio signals. These patterns were used to generate templates to classify the audio signals into different categories. Initial studies have yielded a overall correct classification accuracy of 90% with a database of 64 audio segments.		Karthikeyan Umapathy;Sridhar Krishnan;Shihab Jimaa	2002		10.1109/ICME.2002.1035564	audio mining;speech recognition;time–frequency analysis;computer science;machine learning;speech coding;pattern recognition;information retrieval	ML	-8.291050398718237	-92.04175341953044	18721
aa61e33604988bf2c77d0002df9984bd76ee1a49	a syntax directed system for the recognition of printed arabic mathematical formulas	noise measurement	In this paper we addressed the problem of Arabic mathematical formula recognition, extracted from scanned images of clearly printed documents. Two main stages are followed by the proposed system: symbol recognition and structural analysis of the mathematical formula. For the first stage, our system uses a combination of different statistical features like Run length, Hu and Zernike moments, Bi-level co-occurrence and white pixel's portion and an instance-based classifier K*. High accuracy for the recognition of isolated mathematical symbols is achieved. In the second stage, the system proceeds by top-down and bottom-up parsing scheme based on operator dominance. A set of replacement rules is defined by a coordinate grammar based on symbol recognition and symbol arrangement analysis results. In the proposed system, the recognition and parsing modules interact more closely. Thus, we can use the context information collected during structural analysis to help us guess about the symbols, overcoming our incorrect assumption of perfect symbol recognition. The system provides output in MathML which is easily transmitted for subsequent processing by computer algebra systems. The syntax-directed recognition system, described here, has been successfully demonstrated in many types of formulas and achieved satisfactory results. 91% of formulas are correctly recognized.	printing	Kaouther Khazri Ayeb;Afef Kacem;Abdel Belaïd	2015		10.1109/ICDAR.2015.7333749	computer vision;speech recognition;computer science;noise measurement;machine learning	HCI	-26.23820705161085	-81.90293496957007	18746
18c19c91bbad3e6640ae9294e01a2119459ce25d	is it easier to lipread one's own speech gestures than those of somebody else? it seems not!		In this paper, we attempt to adapt an experimental procedure inspired by Beardsworth and Buckner (1981), in which they studied the ability to recognise one’s own versus somebody else’s walking movements. They showed that certain subjects were better at recognising themselves than at recognising their friends, thanks to “some sort of kinesthetic-visual cross-modal transfer”. We study the lipreading scores of French spoken digits uttered by 6 speakers and identified by the same 6 subjects. It appears that the performances are the same whether or not the subject is also the speaker. Hence we failed in our attempt to demonstrate a perceptuo-motor transfer in this experiment. 1. PERCEPTUO-MOTOR LINKS INSIDE AND OUTSIDE SPEECH 1.1. The “speech” debate There is a classical debate in the field of speech communication about the nature of perceptual representations of speech gestures: purely sensory, and basically auditory, for tenants of auditory theories [1]; or purely motor for tenants of the motor theory [2]. Between these views of perception is one without action constraints, or perception without perceptual representations, as in the direct realist theory [3], we defend a view in which perception serves not only to understand gestures, but also to provide control signals to action: this is the Perception-for-Action-Control Theory [4]. This leads to an integrated sensori-motor framework in which perceptual and motor representations are acquired together and in interaction in the course of speech development, with a modelling approach based on the conceptual tools of “speech robotics” ([5], [6]). 1.2. Perceptuo-motor links outside speech The nature of the perceptuo-motor links is of course also discussed outside the domain of speech communication. Since Johansson [7], there have been a large number of experimental psychology studies on the perception of biological movement, which provided the basis for more recent work on audiovisual speech perception with the same kinds of techniques [8]. cortex, providin and action, has r the discovery of ventral premot responding to bo complex actions important part movement in temporal sulcus [11]. In the l neuroimagery li human cortex. functional anato and observation confirms that th activation for t supplementary m cortex, the supr parietal lobe. Gr experimental res and defines a b specifically for their execution, occipital cortex superior tempo information of th left inferior par with PET and fM in [14]), which activation of Broca-homologu primary and seco summary, the lipreading system temporal, secon supramarginal g	acoustic lobing;call of duty: black ops;control theory;modal logic;motor theory of speech perception;performance;polyethylene terephthalate;robotics	Jean-Luc Schwartz;Christophe Savariaux	2001				ML	-5.5334027932921845	-78.9659690678524	18759
338b340fa3bd3b246005bbe22dcdac7ca9b612a3	objective evaluation methods for chinese text-to-speech systems		To objectively evaluate the performance of text-to-speech (TTS) systems, many studies have been conducted in the straightforward way to compare synthesized speech and natural speech with the alignment. However, in most situations, there is no natural speech can be used. In this paper, we focus on machine learning approaches for the TTS evaluation. We exploit a subspace decomposition method to separate different components in speech, which generates distinctive acoustic features automatically. Furthermore, a pairwise based Support Vector Machine (SVM) model is used to evaluate TTS systems. With the original prosodic acoustic features and Support Vector Regression model, we obtain a ranking relevance of 0.7709. Meanwhile, with the proposed oblique matrix projection method and pairwise SVM model, we achieve a much better result of 0.9115.	acoustic cryptanalysis;machine learning;natural language;netware file system;oblique projection;relevance;speech synthesis;support vector machine	Teng Zhang;Zhipeng Chen;Ji Wu;Sam Lai;Wenhui Lei;Carsten Isert	2016		10.21437/Interspeech.2016-421	speech recognition;computer science;speech synthesis	NLP	-12.492826202314049	-87.50926005641267	18765
11eb2d799b93a4cfc257132c534ed7fc308621a8	classifying the hungarian web	largest hungarian portal;keyword search;hungarian web;large-scale supporting evidence;positive evidence matter;topic classification;simple statistical language;topic classification system;classification system	In this paper we present some lessons learned from building viz s la, the keyword search and topic classification system used on the largest Hungarian portal, [ origo .hu]. Based on a simple statistical language model, and the large-scale supporting evidence from vizsla, we argue that in topic classification only positive evidence matters.	language model;linear algebra;search algorithm;viz: the computer game	András Kornai;Marc Krellenstein;Michael Mulligan;David Twomey;Fruzsina Veress;Alec Wysoker	2003			natural language processing;computer science;data science;data mining;library classification	NLP	-31.6364982724999	-73.386481236497	18783
82db2bbd7ad2e31eac2a75b268fecd79c42ec30c	the blocking effect and korean caki		When the Chinese reflexive ziji is located far from its antecedents, it is not uncommon to see the blocking effect, since the long-distance binding of ziji is normally blocked by the presence of a first (or second) person pronoun intervening in the reported speech. Conversely, it has generally been accepted that Korean caki does not manifest any blocking effects. However, in this paper, we propose that the blocking effect exists in the long-distance binding of Korean caki.	blocking (computing);chinese room	Hyunjun Park;Haihua Pan	2017			blocking effect;cancer research;mathematics	NLP	-10.588779781482883	-80.41932459822995	18792
0fc1eb7151bf2d9541bdf60d7d10d594909eb226	fast and robust stochastic segment model for mandarin digital string recognition	robust stochastic segment model;neural networks;hidden markov model;hmm based system;natural languages;joints;speaker recognition;artificial neural networks;hidden markov models;region based discriminative method robust stochastic segment model mandarin digital string recognition hidden markov model speaker independent performance hmm based system computational complexity;stochastic processes;speaker independent;speaker independent performance;computational complexity;artificial neural networks joints;stochastic processes hidden markov models natural languages speaker recognition;error rate;robustness;mandarin digital string recognition;region based discriminative method	Based on the analysis and comparisons of complexity between stochastic segment model (SSM) and hidden Markov model (HMM) in this paper, we presented a fast and robust SSM, which yields a 94.75% speaker-independent performance on Mandarin digit string recognition. This result is better than HMM based system at the same level of computational complexity and just only a little slower than HMM in the running time. We also studied a region based discriminative method, which achieves 18.0% error rate reduction for substitution error and 95.08% accuracy for Mandarin digit string recognition.	computational complexity theory;hidden markov model;markov chain;super robot monkey team hyperforce go!;time complexity	Wenju Liu;Yun Tang;Shouye Peng	2008	2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)	10.1109/IJCNN.2008.4633987	speech recognition;word error rate;computer science;machine learning;pattern recognition;natural language;computational complexity theory;artificial neural network;hidden markov model;robustness	Vision	-19.581674279077838	-90.92359516061808	18795
cce28ac3548782bb25c671cf2e30765dae0d4ad7	legibility and aesthetic analysis of handwriting		This paper deals with computer-based cognitive analysis towards legibility and aesthetics of a handwritten document. The legible text creates a human perception that the writing can be read effortlessly because of its orthographic clarity. The aesthetic property relates to the beautiful appearance of a handwritten document. In this study, we deal with these properties on offline Bengali handwriting. We formulate both legibility and aesthetic analysis tasks as machine learning problems supervised by the human cognitive system. We employ automatically derived feature-based recurrent neural networks to investigate writing legibility. For aesthetics evaluation, we employ hand-crafted feature-based support vector machines (SVMs). We have collected contemporary Bengali handwritings, on which the subjective legibility and aesthetic scores are provided by human readers. On this corpus containing legibility and aesthetic ground-truth information, we executed our experiments. The experimental results obtained on various handwritings are encouraging.	artificial intelligence;artificial neural network;experiment;ground truth;machine learning;online and offline;orthographic projection;recurrent neural network;support vector machine;synapomorphy;text corpus	Chandranath Adak;Bidyut Baran Chaudhuri;Michael Blumenstein	2017	2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)	10.1109/ICDAR.2017.37	computer science;artificial intelligence;task analysis;natural language processing;pattern recognition;bengali;feature extraction;recurrent neural network;handwriting;cognition;text mining;legibility	AI	-18.572866668781852	-69.1907740609873	18798
35902df1171607321e8f4935c22a1dbf563be391	hands-free speech-sound interactions at home	sound recognition;microphones;domotics speech recognition sound processing sound recognition;acoustic signal processing;speech recognition acoustic signal processing microphones;speech recognition;sound processing;everyday life sounds hands free speech sound interactions hands free speech sound recognition system companionable european project wireless microphone vocal commands;speech recognition decoding speech robots microphones noise acoustics;domotics	This paper describes a hands-free speech/sound recognition system developed and evaluated in the framework of the CompanionAble European Project. The system is intended to work continuously on a distant wireless microphone and detect not only vocal commands but also everyday life sounds. The proposed architecture and the description of each module are outlined. In order to have good recognition and rejection rates, some constraints were defined for the user and the vocabulary was limited. First results are presented; currently project trials are underway.	interaction;microphone;rejection sampling;vocabulary	Pierrick Milhorat;Dan Istrate;Jérôme Boudy;Gérard Chollet	2012	2012 Proceedings of the 20th European Signal Processing Conference (EUSIPCO)		voice activity detection;speech technology;speaker recognition;speech recognition;acoustics;speech processing;acoustic model;communication	Robotics	-14.856129900427861	-86.22982719907314	18818
4be03fd3a76b07125cd39777a6875ee59d9889bd	content-based analysis for accessing audiovisual archives: alternatives for concept-based indexing and search	audio visual systems;semantic indexing;semantics;psi_visics;materials;indexing and retrieval;semantics indexing computer vision multimedia communication face materials streaming media;computer vision;supervised methods content based analysis audiovisual archive accessing concept based indexing concept based search audiovisual material audiovisual analysis concept based retrieval oriented approach knowledge modeling user defined concepts;indexing;streaming media;semantic gap;multimedia communication;indexing audio visual systems content based retrieval;on the fly;face;point of view;content based retrieval;knowledge modeling	Huge amounts of audiovisual material have been digitized recently, resulting in a great source of information relevant both from a cultural and historical point of view. However, in spite of millions of man hours spent on manual annotation and recent advances in (semi-)automatic metadata generation, accessing these archives and retrieving relevant information from them remains a difficult task. Up to recently, the main paradigm to open up archives by automatic tools for audiovisual analysis has been a concept-based indexing and retrieval oriented approach. However, this approach has its limitations, in that it does not scale well, it requires strong supervision, and does not really match well to the user's needs. In this paper, we discuss some upcoming alternative approaches that try to overcome or circumvent some of these issues. This includes i) the use of knowledge modeling to bridge the semantic gap; ii) on-the-fly learning of new, user-defined concepts; and iii) weakly supervised methods that learn from associated text data. We also discuss what we consider important open issues at this time that deserve more attention from the research community.	archive;information source;knowledge modeling;programming paradigm;scalability;supervised learning;text corpus	Tinne Tuytelaars	2012	2012 13th International Workshop on Image Analysis for Multimedia Interactive Services	10.1109/WIAMIS.2012.6226770	face;computer vision;search engine indexing;computer science;semantics;multimedia;world wide web;information retrieval;semantic gap	Web+IR	-15.12042522547647	-56.82844220987195	18860
1d87777d4300bfd9baa81da341a572c0cd3fd439	defection detection: predicting search engine switching	search engine switching;search engine;web search engine;keyword search;defect detection	Searchers have a choice about which Web search engine they use when looking for information online. If they are unsuccessful on one engine, users may switch to a different engine to continue their search. By predicting when switches are likely to occur, the search experience can be modified to retain searchers or ensure a quality experience for incoming searchers. In this poster, we present research on a technique for predicting search engine switches. Our findings show that prediction is possible at a reasonable level of accuracy, particularly when personalization or user grouping is employed. These findings have implications for the design of applications to support more effective online searching.	network switch;personalization;web search engine	Allison P. Heath;Ryen W. White	2008		10.1145/1367497.1367712	cloaking;beam search;organic search;metasearch engine;web search engine;computer science;spamdexing;internet privacy;search analytics;world wide web;information retrieval;search engine	Web+IR	-33.17548085506589	-53.090038444506924	18874
5fed8a8c78d42afe093f9fcaa768bb8e8cdf1537	an interactive document image description for ocr of handwritten forms	document handling;handwriting recognition;optical character recognition;graphical user interfaces;design and implementation;optical character recognition software handwriting recognition speech recognition text recognition insurance data mining information retrieval packaging layout storage automation;local syntax constraints interactive document image description handwritten forms user friendly graphical package interactive layout content description predefined forms automatic processing ocr techniques ocr algorithms;document image processing;interactive systems;graphical user interfaces document image processing document handling optical character recognition handwriting recognition interactive systems	This paper is concerned with the design and implementatzon of a user-friendly graphical package for the interactive layout and content description o document images. The specific roblems addessed are those associated with pregfined forms, such as ro osal forms used in the insurance business, w i i d have been completed b y hand. The documcnt image description allows position and content o the handwritten fields to be defined cified. This information assists the automatic processing of handwritten forms usin OCR techniques b enabling appropriate O d a1 orithms and locafsyntaz constraints to be appliei to each field to produce the best recognition performance.	graphical user interface;optical character recognition;usability	David Monger;Graham Leedham;Andy C. Downton	1993		10.1109/ICDAR.1993.395681	natural language processing;speech recognition;document processing;intelligent character recognition;computer science;intelligent word recognition;document layout analysis;graphical user interface;database;handwriting recognition;optical character recognition;design document listing	Vision	-35.6184284126021	-74.82658943972513	18898
faf618736cae91c6072451d8d47289487a54b145	natural language processing and the web	wikipedia;turning;information retrieval;information filtering;word sense disambiguation;glossary creation;named entity recognition;glossary creation world wide web natural language processing word sense disambiguation wikipedia named entity recognition keyword extraction sentiment analysis;machine intelligence;keyword extraction;web sites;sentiment analysis;world wide web;humans;computational linguistics;information filters;natural language processing;natural language processing computational linguistics humans machine intelligence information retrieval web sites world wide web turning information filtering information filters	This special issue focuses on applications that innovatively use the Web and Web-scale document collections to create useful resources or applications that let end users navigate the Web more easily. This article is part of a special issue on Natural Language Processing and the Web.	natural language processing;world wide web	Dragomir R. Radev;Mirella Lapata	2008	IEEE Intelligent Systems	10.1109/MIS.2008.89	natural language processing;web service;web application security;web development;web modeling;data web;web mapping;html;web design;web accessibility initiative;web standards;computer science;computational linguistics;web navigation;social semantic web;web page;brand;web intelligence;web engineering;web 2.0;world wide web;information extraction;information retrieval;sentiment analysis;mashup	Robotics	-29.8488028721993	-56.95983522637184	18900
90a5a4a96097323d624e5548bcfaa2ef416a7d6d	automated mobile health: designing a social reasoning platform for remote health management		With the drastic expansion of mobile technologies, mobile health has become ubiquitous and versatile to revolutionize healthcare for improved health outcomes. This study takes initiatives to investigate a new paradigm of auto- mated mobile health as the process automation of mobile-enabled health inter- ventions. Through the realisation of the paradigm, a novel social reasoning platform with a comprehensive set of design guidelines are proposed for efficient and effective remote health management. The study considerably contributes to the cumulative theoretical development of mobile health and health decision making. It also provides a number of implications for academic bodies, healthcare practitioners, and developers of mobile health.	mhealth	Hoang D. Nguyen;Danny Chiang Choon Poo	2016		10.1007/978-3-319-39910-2_4	simulation;knowledge management;data mining;computer security	HCI	-60.166297726854495	-57.441183353408654	18902
c1fb423ec8c29baa4c88b49ff6dec80bf256c005	handprinted text reader that learns by experience		Abstract   A microcomputer-based handprinted text reader whose reading performance it is intended will improve with experience is currently being developed. This paper explains the principles behind the ‘intelligent’ machine reading system which when fully developed will be capable of reading unconstrained handwriting styles. Text is analysed syntactically, enabling the optical text reader to offer tolerance to the variability of character shapes in the handprinted text. Some preliminary results are presented to illustrate the performance of the present system which is based on the BBC microcomputer. When fully implemented the system will be capable of continually improving its performance by retaining the vocabulary of the text being read and also by memorizing all the variable shapes characters take when written by the user over a period of time. The motivation for the research is the development of a low-cost effective reading aid for the blind.		R. R. Malyan;Yeshwant Sunthankar	1986	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/0141-9331(86)90340-6	speech recognition;computer science;artificial intelligence	EDA	-28.345625302039167	-84.02659328721373	18905
3595d4c82b030f24deb0fa6b56cf66f13afbe621	cache transition systems for graph parsing		Motivated by the task of semantic parsing, we describe a transition system that generalizes standard transition-based dependency parsing techniques to generate a graph rather than a tree. Our system includes a cache with fixed size m, and we characterize the relationship between the parameter m and the class of graphs that can be produced through the graph-theoretic concept of tree decomposition. We find empirically that small cache sizes cover a high percentage of sentences in existing semantic corpora.	graph theory;parsing;semantic analysis (machine learning);text corpus;transition system;tree decomposition	Daniel Gildea;Giorgio Satta;Xiaochang Peng	2018	Computational Linguistics	10.1162/COLI_a_00308	transition system;tree decomposition;natural language processing;machine learning;cache;parsing;computer science;dependency grammar;graph;artificial intelligence	NLP	-21.178670686720746	-74.30088481624296	18936
a81005837ba75d4618c8da54f4c4805e90b61a07	ktao: a kidney tissue atlas ontology to support community-based kidney knowledge base development and data integration				Yongqun He;Becky Steck;Edison Ong;Laura Mariani;Chrysta Lienczewski;Ulysses J. Balis;Matthias Kretzler;Jonathan Himmelfarb;John F. Bertram;Evren U. Azeloglu;Ravi Iyengar;Deborah Hoshizaki;Sean D. Mooney	2018				AI	-54.6245554027487	-64.43501705904669	18939
3a7694291c23d9dbbc6f7683a7836ed2f0fa763d	visual search in modern human-computer interfaces	computadora;presentation information;affichage graphique;ergonomia;ordinateur;information retrieval;movimiento ocular;relacion hombre maquina;exploracion visual;information layout;tablero;menu;man machine relation;ergonomie;computer;tableau;graphic display;recherche information;visual search;visual display unit;eye movement;comportement utilisateur;array;visualizacion grafica;relation homme machine;user behavior;recuperacion informacion;visual display units;mouvement oculaire;presentacion informacion;ergonomics;spatial frequency;comportamiento usuario;human computer interface;exploration visuelle	Abstract This article reviews aspects of visual search in relation to computer visual display units. Theoretical issues such as eye movements in visual search are discussed as well as practical examples such as the role of array shape, the benefits of cursor-presented status (insert vs overtype) information, conspicuity of peripherally presented information, and possible benefits of anti-aliased fonts. In association with the four experiments relating to these aspects, the respective follow-up eye movement monitoring studies are also described as these allowed quantification of what otherwise might only have been inferred. The review concludes with four major recommendations. First, much scope remains for exploring the optimization of status (e.g., cursor-presented) information. Second, it would be worth exploring whether icons, like verbal labels, are susceptible to an array shape effect. Third, further work is required (e.g. in the possible role of articulatory differences) in accounting for the ease of...		Derek Scott	1993	Behaviour & IT	10.1080/01449299308924378	computer vision;simulation;visual search;computer science;artificial intelligence;human factors and ergonomics;spatial frequency;communication;eye movement	SE	-44.94131862497701	-55.44871277270522	18940
c1031b025d113cbf0dfbeb6cb4a4da897aea5ce1	sp2sp: subnet-partition based 2-level grid service scheduling policy	videoconference;video retrieval;inference mechanisms;video sequences;layout;data mining;ontologies artificial intelligence;ontology based automatic video annotation;inference rule;ontologies indexing video sequences data mining content based retrieval videoconference moon computer science knowledge engineering layout;indexing;digital videos;moon;object ontology;grid service;summarization system;video annotation;ontologies;digital video;computer science;olyvia;content based retrieval;semantic inference rules;knowledge engineering	The need of techniques for automatic video annotation and summarization has been increased because digital videos have been becoming available at an ever-increasing rate. In this paper, we present an automatic video annotation and summarization system which employs the ontologies and semantic inference rules to facilitate the video retrieval. In our work, high -level concepts of shot / group / scene / video level are automatically extracted by applying semantic inference rules to VideoAnnotation ontology and object ontology. Finally, we show the retrieval effectiveness of our approach and discuss the future work.	scheduling (computing);subnetwork	Jin-Woo Jeong;Kyung-Wook Park;Jeong Ho Lee;Young Shik Moon;Sung-Han Park;Dong-Ho Lee	2007		10.1109/SKG.2007.96	layout;search engine indexing;computer science;natural satellite;ontology;artificial intelligence;automatic summarization;knowledge engineering;data mining;videoconferencing;world wide web;information retrieval;rule of inference	HPC	-14.466605060905495	-56.18927903576905	18996
d62a9eddd2bbf87e2a9314b58e57033098bd781c	a development methodology for a stroke rehabilitation monitoring application		The capabilities of mobile devices (e.g. flexibility, portability, and the ability to retrieve information quickly) have been leveraged for the development of clinical performance monitoring applications. In this paper we assess the suitability of a methodology for development of clinical performance monitoring applications to support stroke rehabilitation. We use a case study, with two use cases of patients recovering from stroke events, to design a monitoring application at a conceptual level and compare it to other clinical performance monitoring applications.	gene regulatory network;information needs;mobile device;prototype	Pilar Mata;Craig E. Kuziemsky;Liam Peyton	2016		10.5220/0005785104000405	computer science;knowledge management;rehabilitation;physical therapy;stroke	Mobile	-54.52894157893967	-63.339071818271194	19024
04ae955f3b94dda6c22ad4b7ac215bb32e2ca56f	speech recognition without grammar or vocabulary constraints.	speech recognition			Harald Singer;Jun-ichi Takami	1994			speaker recognition;speech production;audio mining;speech recognition;speech corpus;computer science;speech;acoustic model;speech synthesis	NLP	-15.38074218807617	-86.08506126928654	19051
3946b5530e6b6431e159270623d14190cb7caf39	nonuniform markov models	conditional independence;wall street journal;model performance;formal languages;discrete time;markov model;statistics;formal languages markov processes statistics parameter estimation;markov processes;statistical language model;parameter estimation;statistical language models nonuniform markov models conditional independence prediction length context length wall street journal interpolated markov model parameter numbers parameter value estimation string length statistics;context modeling interpolation history predictive models statistics smoothing methods computer science probability heuristic algorithms	A statistical language model assigns probability to strings of arbitrary length. Unfortunately, it is not possible to gather reliable statistics on strings of arbitrary length from a nite corpus. Therefore, a statistical language model must decide that each symbol in a string depends on at most a small, nite number of other symbols in the string. In this report we propose a new way to model conditional independence in Markov models. The central feature of our nonuniform Markov model is that it makes predictions of varying lengths using contexts of varying lengths. Experiments on the Wall Street Journal reveal that the nonuniform model performs slightly better than the classic interpolated Markov model. This result is somewhat remarkable because both models contain identical numbers of parameters whose values are estimated in a similar manner. The only di erence between the two models is how they combine the statistics of longer and shorter strings.	experiment;interpolation;language model;markov chain;markov model;the wall street journal	Eric Sven Ristad;Robert G. Thomas	1997		10.1109/ICASSP.1997.596046	markov chain;maximum-entropy markov model;markov kernel;discrete time and continuous time;formal language;conditional independence;variable-order bayesian network;markov property;continuous-time markov chain;balance equation;machine learning;hidden semi-markov model;mathematics;markov renewal process;additive markov chain;markov algorithm;markov process;markov chain mixing time;markov model;estimation theory;hidden markov model;statistics;variable-order markov model	ML	-22.352676903275732	-92.1852673271527	19052
baca41b7558a491fa34eaef083f67a5d554c9233	towards a versatile multi-layered description of speech corpora using algebraic relations		This paper presents a software library, namely ROOTS for Rich Object Oriented Transcription System, that helps to describe spoken messages in a coherent manner linking sequences of items on numerous levels (linguistic, phonological, or acoustic). The proposed representation is incremental and can thus describe any or all parts of an utterance. In order to link different levels of description, algebraic relations are used. Instead of relying solely on fixed, pre-determined relations, algebraic composition operators are proposed that can create a missing relation on demand. In terms of software architecture, object classes are defined based on a well-grounded theoretical representation of speech (text, syntax, phonology and acoustics), without particular dependences on an annotation system (e.g. IPA is fully implemented). The API documentation for this software is available online [7].	acoustic cryptanalysis;coherence (physics);documentation;library (computing);linear algebra;medical transcription;software architecture;software development;software framework;text corpus;xml	Nelly Barbot;Vincent Barreaud;Olivier Boëffard;Laure Charonnat;Arnaud Delhay;Sébastien Le Maguer;Damien Lolive	2011			natural language processing;speech recognition;computer science;machine learning;linguistics	NLP	-28.689622459767385	-80.23044813185355	19093
52a0dc54d6b7dbc4660bde6608247f9f294b6182	inductive bias against stem changes as perseveration: experimental evidence for an articulatory approach to output-output faithfulness		Speakers of morphologically-rich languages commonly face what has been called the Paradigm Cell Filling Problem: they know some form of a word but it is inappropriate to the current context, leading them to derive a form of that word they have never encountered (e.g., they know the singular form of a noun, and now need to produce the plural). We suggest that in performing this task speakers perseverate on articulatory gestures comprising the form they know, and that gestures vary in the extent to which speakers perseverate on them. This proposal explains the parallels between findings in loanword adaptation, speech errors, and acquisition of phonology. New experimental data from a miniature artificial language are presented in support of the theory.	inductive bias;parallels desktop for mac;programming paradigm	Matthew Stave;Amy Smolek;Vsevolod Kapatsinski	2013			phonology;articulatory gestures;coarticulation;cognitive psychology;psychology;noun;alternation (linguistics);front vowel;phonetics;linguistics;speech production	ML	-10.61412833713867	-79.30367501067839	19098
ff36037c7a573ae7555f14fe34f48c965b9bf82d	utilizing inter-passage similarities for focused retrieval		Our main goal is studying the merits of using inter-passage similarities for the task of focused retrieval; i.e., ranking passages in documents by their relevance to an information need expressed by a query. As an initial research direction we study the cluster hypothesis for passage (focused) retrieval. We propose a novel suite of cluster hypothesis tests that employ inter-passage similarities and demonstrate that the cluster hypothesis holds for passages. In addition, we present several future directions we intend to pursue.	cluster hypothesis;information needs;relevance	Eilon Sheetrit	2018		10.1145/3209978.3210222	information retrieval;computer science;data mining;information needs;cluster hypothesis;ranking	Web+IR	-32.53954444609044	-59.697910712904836	19101
4fee00c8b61ca82f3bccaea817a345619a31b806	discovery of fuzzy multiple-level web browsing patterns	sequential patterns;concept hierarchy;fuzzy concepts;web mining;browsing behavior;web browsing patterns;web browsing;data collection;web usage mining;web based applications;association rule;web pages;data mining	 Web usage mining is the application of data mining techniques to discover usage patterns from web data. It can be used to better understand web usage and better serve the needs of rapidly growing web–based applications. Discovery of browsing patterns, page clusters, user clusters, association rules and usage statistics are some usage patterns in the web domain. Web mining of browsing patterns including simple sequential patterns and sequential patterns with browsing times has been studied recently. However, most of these works focus on mining browsing patterns of web pages directly. In this work, we introduce the problem of mining browsing patterns on multiple levels of a taxonomy comprised of web pages. The browsing time on each web page is used to analyze the retrieval behavior. Since the data collected are numeric, fuzzy concepts are used to process them and to form linguistic terms. A web usage–mining algorithm to discover multiple–level browsing patterns from linguistic data is thus proposed. Each page uses only the linguistic term with maximum cardinality in later mining processes, thus making the number of fuzzy regions to be processed the same as the number of pages. Computation time can thus be greatly reduced. In addition, the inclusion of concept hierarchy (taxonomy) of web pages produces browsing patterns of different granularity. This allows the views of users’ browsing behavior from various levels of perspectives.	browsing	Shyue-Liang Wang;Wei-Shuo Lo;Tzung-Pei Hong	2002		10.1007/11011620_16	static web page;web mining;web modeling;data web;web mapping;data mining;information retrieval;web page;computer science;web navigation;semantic web stack	Web+IR	-29.134517935523085	-53.31027702431329	19128
4cab32df33bb77d739d7050741ccf78e245ed470	eliciting a hierarchical structure of human consonant perception task errors using formal concept analysis.	hierarchical structure;formal concept analysis	In this paper we have used Formal Concept Analysis to elicit a hierarchical structure of human consonant perception task errors. We have used the Native Listeners experiments provided for the Consonant Challenge session of Interspeech 2008 to analyze perception errors comitted in relation to the place of articulation of the consonants being evaluated for one quiet and six noisy acoustic conditions.	acoustic cryptanalysis;biconnected component;experiment;formal concept analysis	Carmen Peláez-Moreno;Ana I. García-Moral;Francisco J. Valverde-Albacete	2009			natural language processing;speech recognition;computer science;formal concept analysis	NLP	-13.437906545900153	-85.04207309548026	19137
1ea4914b210c91e7dbc3ae2d41036c64c08f0693	a twin-candidate model for learning-based anaphora resolution	learning model;anaphora resolution;computational linguistics;coreference resolution;linguistique informatique;automatic content extraction	The traditional single-candidate learning model for anaphora resolution considers the antecedent candidates of an anaphor in isolation, and thus cannot effectively capture the preference relationships between competing candidates for its learning and resolution. To deal with this problem, we propose a twin-candidate model for anaphora resolution. The main idea behind the model is to recast anaphora resolution as a preference classification problem. Specifically, the model learns a classifier that determines the preference between competing candidates, and, during resolution, chooses the antecedent of a given anaphor based on the ranking of the candidates. We present in detail the framework of the twin-candidate model for anaphora resolution. Further, we explore how to deploy the model in the more complicated coreference resolution task. We evaluate the twin-candidate model in different domains using the Automatic Content Extraction data sets. The experimental results indicate that our twin-candidate model is superior to the single-candidate model for the task of pronominal anaphora resolution. For the task of coreference resolution, it also performs equally well, or better.	ace;anaphora (linguistics);baseline (configuration management);identification scheme;information;os-tan;sampling (signal processing);software deployment;yang	Xiaofeng Yang;Jian Su;Chew Lim Tan	2008	Computational Linguistics	10.1162/coli.2008.07-004-R2-06-57	natural language processing;speech recognition;computer science;artificial intelligence;computational linguistics;linguistics	AI	-18.414244915777232	-68.84494219856707	19152
369b267171c0b777b51a58070e55aa642d4619e4	parallel high-performance grid computing: capabilities and opportunities of a novel demanding service and business class allowing highest resource efficiency	fluorescence correlation spectroscopy;volunteer computing;box counting dimension;genomics;replication;mitosis;architectural sequencing;chromosome territories;visual data base access;in vivo labelling;extreme visualization;namd;molecular transport;yfp;genome organization;h2a;cell nucleus;parallel super computing;genome mechanics;nuclear diffuseness;gene regulation;e human grid ecology;super resolution microscopy;fluorescence in situ hybridization;subchromosomal domains;high performance parallel computing;scaling analysis;auto fluorescent proteins;virtual paper tool;lacunarity dimension;cfp;information browser;h2b;molecular dynamics simulation;nuclear organization;coevolution;structural sequencing;genetics;molecular dynamic simulation;homologous recombination;histones;brownian dynamics;dsred;obstructed diffusion;three dimensional virtual environment;large scale;chromatin rosettes;anomalous diffusion;holistic genetics;nuclear structure;chromatin density distribution;spatial distance measurement;complete sequenced genomes;percolation;exact yard stick dimension;complex system;memory architecture;metaphase;transcription;chromatin loops;nuclear morphology;genome;fusion protein;holistic viewing system;integrative data management;high performance computer;life sciences;parallel computer;long range correlations;genome architecture;cell division;local nuclear dimension;simultaneous co transfection;genome function;grid and gpu computing;chromatin loop aggregates;confocal laser scanning microscopy;systems genomics;chromatin fibre;interphase;monte carlo;gfp;mh2a1 2;dna sequence;spatial precision distance microscopy;chromatin density;grid computing;high performance;fractal analysis;h1 0;repair;persistence length;health care	"""Especially in the life-science and the health-care sectors the huge IT requirements are imminent due to the large and complex systems to be analysed and simulated. Grid infrastructures play here a rapidly increasing role for research, diagnostics, and treatment, since they provide the necessary large-scale resources efficiently. Whereas grids were first used for huge number crunching of trivially parallelizable problems, increasingly parallel high-performance computing is required. Here, we show for the prime example of molecular dynamic simulations how the presence of large grid clusters including very fast network interconnects within grid infrastructures allows now parallel high-performance grid computing efficiently and thus combines the benefits of dedicated super-computing centres and grid infrastructures. The demands for this service class are the highest since the user group has very heterogeneous requirements: i) two to many thousands of CPUs, ii) different memory architectures, iii) huge storage capabilities, and iv) fast communication via network interconnects, are all needed in different combinations and must be considered in a highly dedicated manner to reach highest performance efficiency. Beyond, advanced and dedicated i) interaction with users, ii) the management of jobs, iii) accounting, and iv) billing, not only combines classic with parallel high-performance grid usage, but more importantly is also able to increase the efficiency of IT resource providers. Consequently, the mere """"yes-we-can"""" becomes a huge opportunity like e.g. the life-science and health-care sectors as well as grid infrastructures by reaching higher level of resource efficiency."""	acm/ieee supercomputing conference;central processing unit;complex systems;computation (action);d-grid;electrical connection;electronic billing;genetic heterogeneity;gesellschaft für informatik;goto;grid computing;molecular dynamics;neoplasms;occupations;parallel computing;requirement;simulation;stimulation (motivation);supercomputer;benefit;funding grant	Nick Kepper;Ramona Ettig;Frank Dickmann;Rene Stehr;Frank G. Grosveld;Gero Wedemann;Tobias A. Knoch	2010	Studies in health technology and informatics	10.3233/978-1-60750-583-9-264	biology;simulation;bioinformatics;nanotechnology;grid computing	HPC	-8.196964901409451	-54.154868548294175	19158
51d9e3e5099cea5cec39739cc06275daadb2b010	sentence based sentiment classification from online customer reviews	sentiment classification;contextual information;machine learning;feature extraction;sentiment analysis;reviews mining;support vector machine	Sentiment analysis is the process of analyzing and classifying the rewires contents about a product, event, and place etc into positive, negative or neutral opinion. In this paper; we propose a sentence level machine learning approach for sentiment classification of online reviews. The proposed method extracts the subjective sentences from the reviews and label each sentence either positive or negative based on its word level feature using naïve Naïve Bayesian (NB) classifier. The labeled sentences create an annotated set of sentences called as BOS (Bag-of-Sentences). We train Support Vector machine (SVM) classifier on the BOS for sentences polarity classification. The contextual information in each sentence structure is taken into consideration to calculate the semantic orientation. The effectiveness of the proposed method is evaluated thought simulation. Results show that our machine learning based proposed method on average achieves accuracy of 81% and 83% with some contextual information. This method improves the sentiment classification polarity on sentence level unlike the word level lexical feature based work, by focus on sentences, this also concentrate on contextual information.	bayesian network;machine learning;naive bayes classifier;naivety;sentiment analysis;simulation;support vector machine	Aurangzeb Khan;Baharum Baharudin;Khairullah Khan	2010		10.1145/1943628.1943653	natural language processing;speech recognition;computer science;pattern recognition;sentiment analysis	AI	-19.890517551381752	-68.25452420637689	19183
dbfc68dbadf26a167bc92e529c4882b8d2c0a868	determinants of physician use of an ambulatory prescription expert system	prescription expert system;electronic medical records;physician behavior;physician attitudes;electronic medical record;expert system	PURPOSE To determine whether physician experience with and attitude towards computers is associated with adoption of a voluntary ambulatory prescription writing expert system.   METHODS A prescription expert system was implemented in an academic internal medicine residency training clinic and physician utilization was tracked electronically. A physician attitude and behavior survey (response rate=89%) was conducted six months after implementation.   RESULTS There was wide variability in system adoption and degree of usage, though 72% of physicians reported predominant usage (> or =50% of prescriptions) of the expert system six months after implementation. Self-reported and measured technology usage were strongly correlated (r=0.70, p<0.0001). Variation in use was strongly associated with physician attitude toward issues of system efficiency and effect on quality, but not with prior computer experience, level of training, or satisfaction with their primary care practice. Non-adopters felt that electronic prescribing was more time consuming and also more likely to believe that their patients preferred hand-written prescriptions.   CONCLUSION A voluntary electronic prescription system was readily adopted by a majority of physicians who believed it would have a positive impact on the quality and efficiency of care. However, dissatisfaction with system capabilities among both adopters and non-adopters suggests the importance of user education and expectation management following system selection.	computer;entity name part qualifier - adopted;expert system;heart rate variability;internal medicine specialty;patients;primary health care;strongly correlated material	Joel M. Schectman;John B. Schorling;Mohan M. Nadkarni;John D. Voss	2005	International journal of medical informatics	10.1016/j.ijmedinf.2005.05.011	internal medicine;family medicine;medicine;computer science;artificial intelligence;emergency medicine;expert system	HCI	-60.800725167498	-64.33488804519993	19207
0b62391f43d5862b5e9dbd8a1d450a6925e23ea9	aspects of broad folksonomies	intelligent reranking;fuzzy information retrieval;search engines metasearch web search information retrieval web pages application software computer science frequency iterative algorithms databases;search engine;evolutionary computation;search engines;query formulation;search engines boolean algebra evolutionary computation query formulation;query optimization;data fusion;ordered weighted averaging data fusion query optimization intelligent search evolutionary computing boolean search queries fuzzy information retrieval systems webfusion metasearch engine web search click through behavior intelligent reranking;fuzzy information retrieval systems;boolean search queries;boolean algebra;ordered weighted average;click through behavior;webfusion;meta search engine;web search;metasearch engine;ordered weighted averaging;intelligent search;evolutionary computing	Folksonomies, collaboratively created sets of metadata, are becoming more and more important for organising information and knowledge of communites in the Web. While for a single user the difference to keyword assignment is marginal, the power of folksonomies emerges from the collaborative aspects. Folksonomies are already issue of research. Within this publication we analyse underlying statistical properties of broad folksonomies aiming to identify laws and characteristics, which allow inferring properties for folksonomy based retrieval. The actual benefit of folksonomies for retrieval and the derived methods are concluded from experiments with aggregated data from del.icio.us1.	categorization;document retrieval;emergence;experiment;folksonomy;latent semantic analysis;marginal model;tag (metadata);tf–idf;vocabulary;www;world wide web	Mathias Lux;Michael Granitzer;Roman Kern	2007	18th International Workshop on Database and Expert Systems Applications (DEXA 2007)	10.1109/DEXA.2007.80	beam search;search engine indexing;database search engine;metasearch engine;semantic search;computer science;data mining;database;incremental heuristic search;best-first search;search analytics;web search query;world wide web;information retrieval;search engine;evolutionary computation	Web+IR	-29.761407806503023	-57.67754124060526	19210
24a540c5cebf0bce010d61d133811cdd6a30ae73	creating mappings for ontologies in biomedicine: simple methods work	vocabulary controlled;algorithms	Creating mappings between concepts in different ontologies is a critical step in facilitating data integration. In recent years, researchers have developed many elaborate algorithms that use graph structure, background knowledge, machine learning and other techniques to generate mappings between ontologies. We compared the performance of these advanced algorithms on creating mappings for biomedical ontologies with the performance of a simple mapping algorithm that relies on lexical matching. Our evaluation has shown that (1) most of the advanced algorithms are either not publicly available or do not scale to the size of biomedical ontologies today, and (2) for many biomedical ontologies, simple lexical matching methods outperform most of the advanced algorithms in both precision and recall. Our results have practical implications for biomedical researchers who need to create alignments for their ontologies.	algorithm;biomedicine;download;loom;matching;machine learning;national center for biomedical ontology;ontology (information science);precision and recall;stream cipher;web service	Amir Ghazvinian;Natalya Fridman Noy;Mark A. Musen	2009	AMIA ... Annual Symposium proceedings. AMIA Symposium		idef5;computer science;theoretical computer science;data mining;information retrieval	AI	-32.65862986379347	-67.34637621245939	19221
a29875aa735d53dd626eeb981ee0b9034c7abf3e	perceptual load in sport and the heuristic value of the perceptual load paradigm in examining expertise-related perceptual-cognitive adaptations		In two experiments, we transferred perceptual load theory to the dynamic field of team sports and tested the predictions derived from the theory using a novel task and stimuli. We tested a group of college students (N = 33) and a group of expert team sport players (N = 32) on a general perceptual load task and a complex, soccer-specific perceptual load task in order to extend the understanding of the applicability of perceptual load theory and further investigate whether distractor interference may differ between the groups, as the sport-specific processing task may not exhaust the processing capacity of the expert participants. In both, the general and the specific task, the pattern of results supported perceptual load theory and demonstrates that the predictions of the theory also transfer to more complex, unstructured situations. Further, perceptual load was the only determinant of distractor processing, as we neither found expertise effects in the general perceptual load task nor the sport-specific task. We discuss the heuristic utility of using response-competition paradigms for studying both general and domain-specific perceptual-cognitive adaptations.	acclimatization;adaptation;cognition disorders;experiment;heuristic;interference (communication);programming paradigm;sports	Philip Furley;Daniel Memmert;Simone Schmid	2012	Cognitive Processing	10.1007/s10339-012-0529-x	psychology;simulation;perceptual learning;communication;social psychology	Robotics	-51.39856287710165	-54.83653332997604	19236
c2bfdb2cfbcbd0721ae21290d6e722628578ce85	addressing the barriers to interlingual rule-based machine translation with a concept specification and abstraction semantic representation		Interlingual machine translation offers the prospect of better preservation of meaning and requires fewer language/translation models than big data-based machine translation approaches. However, it lacks popularity primarily because of the extensive training and labour required to define the language rules. To address this, we present a semantic representation that 1) treats all bits of meaning as individual concepts that 2) refine or further specify one another to build a network that relates entities in space and time. Also, the representation can 3) encapsulate propositions and thereby define concepts in terms of other concepts supporting the abstraction of underlying linguistic and ontological details. The proposed natural language generation, parsing, and translation strategies are also amenable to probabilistic modeling and thus to learning directly from example data.	big data;entity;interlingual machine translation;natural language generation;parsing;rule-based machine translation	Patrick Connor	2018	CoRR		natural language processing;machine translation;ontology;abstraction;probabilistic logic;parsing;interlingual machine translation;rule-based machine translation;artificial intelligence;computer science;natural language generation	NLP	-15.971178613663954	-67.68373278414985	19257
67fd6cac6fa9f86ff3205faf696a25ba608cf559	hybrid recommendation: combining content-based prediction and collaborative filtering	analisis contenido;commerce electronique;filtering;evaluation performance;filtrage;electronic commerce;comercio electronico;performance evaluation;learning;informacion electronica;recherche image;evaluacion prestacion;filtrado;recommandation;information space;intelligence artificielle;information access;by product;aprendizaje;information electronique;content analysis;apprentissage;recommender system;collaborative filtering;sous produit;subproducto;machine exemple support;preferencia;acces information;recomendacion;electronic information;artificial intelligence;recommendation;acceso informacion;preference;inteligencia artificial;support vector machine;maquina ejemplo soporte;analyse contenu;vector support machine;content based retrieval;recherche par contenu;electronic trade;historical data;image retrieval	Recommender systems improve access to relevant products and information by making personalized suggestions based on historical data of user's likes and dislikes. They have become fundamental application in electronic commerce and information access, provide suggestions that effective prune large information spaces so that users are directed toward those item that best meet their needs and preferences. Collaborative filtering and content-based recommending are two fundamental techniques that have been proposed for performing recommendation. Both techniques have their own advantages however they cannot perform well in many situations. To improve performance, various hybrid techniques have been considered. This paper proposes a framework to improve the recommendation performance by combining content-based prediction based on Support Vector Machines and conventional collaborative filtering. The experimental results show that SVMs can improve the performance of the recommender system.	collaborative filtering	Ekkawut Rojsattarat;Nuanwan Soonthornphisaj	2003		10.1007/978-3-540-45080-1_44	filter;support vector machine;content analysis;image retrieval;computer science;artificial intelligence;collaborative filtering;machine learning;data mining;world wide web;recommender system	AI	-36.615770426924	-58.128078136881754	19260
f35fb6e3685b9f15f48c58f580fc57bca89bde61	advances in natural language processing : 5th international conference on nlp, fintal 2006, turku, finland, august 23-25, 2006 : proceedings	natural language processing	Keynote Addresses.- Recursion in Natural Languages.- The Explanatory Combinatorial Dictionary as the Key Tool in Machine Translation.- A Finite-State Approximation of Optimality Theory: The Case of Finnish Prosody.- Research Papers.- A Bilingual Corpus of Novels Aligned at Paragraph Level.- A Computational Implementation of Internally Headed Relative Clause Constructions.- A Corpus-Based Empirical Account of Adverbial Clauses Across Speech and Writing in Contemporary British English.- A Korean Syntactic Parser Customized for Korean-English Patent MT System.- A Scalable and Distributed NLP Architecture for Web Document Annotation.- A Straightforward Method for Automatic Identification of Marginalized Languages.- A Text Mining Approach for Definition Question Answering.- Accommodating Multiword Expressions in an Arabic LFG Grammar.- Analysis of EU Languages Through Text Compression.- Applying Latent Dirichlet Allocation to Automatic Essay Grading.- Automatic Acquisition of Semantic Relationships from Morphological Relatedness.- Automatic Feature Extraction for Question Classification Based on Dissimilarity of Probability Distributions.- Cat3LB and Cast3LB: From Constituents to Dependencies.- Classification of News Web Documents Based on Structural Features.- Cognition and Physio-acoustic Correlates - Audio and Audio-visual Effects of a Short English Emotional Statement: On JL2, FL2 and EL1.- Compiling Generalized Two-Level Rules and Grammars.- Computer Analysis of the Turkmen Language Morphology.- Coordination Structures in a Typed Feature Structure Grammar: Formalization and Implementation.- Cue-Based Interpretation of Customeru0027s Requests: Analysis of Estonian Dialogue Corpus.- Czech-English Phrase-Based Machine Translation.- Deep vs. Shallow Semantic Analysis Applied to Textual Entailment Recognition.- Dictionary-Free Morphological Classifier of Russian Nouns.- Discourse Segmentation of German Written Texts.- Document Clustering Based on Maximal Frequent Sequences.- Enriching Thesauri with Hierarchical Relationships by Pattern Matching in Dictionaries.- Evaluation of Alignment Methods for HTML Parallel Text.- Experiments in Passage Selection and Answer Identification for Question Answering.- Extracting Idiomatic Hungarian Verb Frames.- Extracting Term Collocations for Directing Users to Informative Web Pages.- Feasibility of Enriching a Chinese Synonym Dictionary with a Synchronous Chinese Corpus.- Finding Spanish Syllabification Rules with Decision Trees.- Identifying Text Discourse Structure of the Narratives Describing Psychiatric Patientsu0027 Defense Mechanisms.- Implementing a Rule-Based Speech Synthesizer on a Mobile Platform.- Improving Phrase-Based Statistical Translation Through Combination of Word Alignments.- Improving Statistical Word Alignments with Morpho-syntactic Transformations.- Improving Term Extraction with Terminological Resources.- Improving Thai Spelling Recognition with Tone Features.- Incorporating External Information in Bayesian Classifiers Via Linear Feature Transformations.- Is a Morphologically Complex Language Really that Complex in Full-Text Retrieval?.- Language Independent Answer Prediction from the Web.- Language Model Mixtures for Contextual Ad Placement in Personal Blogs.- Local Constraints on Arabic Word Order.- MEDITE: A Unilingual Textual Aligner.- Maximum Likelihood Alignment of Translation Equivalents.- Measuring Intelligibility of Japanese Learner English.- Morphological Lexicon Extraction from Raw Text Data.- On the Use of Topic Models for Word Completion.- Ord i Dag: Mining Norwegian Daily Newswire.- Paraphrase Identification on the Basis of Supervised Machine Learning Techniques.- Passage Filtering for Open-Domain Question Answering.- Persian in MULTEXT-East Framework.- Prerequisites for a Comprehensive Dictionary of Serbian Compounds.- Regular Approximation of Link Grammar.- Segmental Duration in Utterance-Initial Environment: Evidence from Finnish Speech Corpora.- Selection Strategies for Multi-label Text Categorization.- Some Problems of Prepositional Phrases in Machine Translation.- Speech Confusion Index (O): A Recognition Rate Indicator for Dysarthric Speakers.- Statistical Machine Translation of German Compound Words.- Summarizing Documents in Context: Modeling the Useru0027s Information Need.- Supervised TextRank.- Tagging a Morphologically Complex Language Using Heuristics.- Terminology Structuring Through the Derivational Morphology.- Text Segmentation Criteria for Statistical Machine Translation.- The Classificatim Sense-Mining System.- The Role of Verb Sense Disambiguation in Semantic Role Labeling.- The Vowel Game: Continuous Real-Time Visualization for Pronunciation Learning with Vowel Charts.- Towards a Framework for Evaluating Syntactic Parsers.- Towards the Improvement of Statistical Translation Models Using Linguistic Features.- Treating Unknown Light Verb Construction in Korean-to-English Patent MT.- Trees as Contexts in Formal Language Generation.- Two String-Based Finite-State Models of the Semantics of Calendar Expressions.- Using Alignment Templates to Infer Shallow-Transfer Machine Translation Rules.		FinTAL;Tapio Salakoski;Filip Ginter;Sampo Pyysalo;Tapio Pahikkala	2006		10.1007/11816508	natural language processing;speech recognition;computer science;linguistics	Robotics	-23.40687146055784	-70.90900314044055	19272
fca38fa6849ae052f31f71eddc343064b7b19022	toward a theory of music information retrieval queries: system design implications	music query analysis;conference contribution;system design;music information retrieval;music information retrieval system design;music information needs;music information uses;information need;working paper	This paper analyzes a set of 161 music-related information requests posted to the rec.music.country.old-time newsgroup. These postings are categorized by the types of detail used to characterize the poster's information need, the type of music information requested, the intended use for the information, and additional social and contextual elements present in the postings. The results of this analysis suggest that similar studies of 'native' music information requests can be used to inform the design of effective, usable music information retrieval interfaces.	categorization;information needs;information retrieval	J. Stephen Downie;Sally Jo Cunningham	2002			information needs;relevance;cognitive models of information retrieval;computer science;multimedia;pop music automation;information retrieval;systems design;human–computer information retrieval	Web+IR	-36.43677430912883	-53.11737800318946	19285
18186af7c2236be375ac34258a4568db2fcf5f99	part-of-speech tagger for ainu language based on higher order hidden markov model	ainu language;hidden markov model;natural speech processing;part of speech tagging;natural language processing	This paper presents POST-AL, the first part-of-speech tagger for Ainu language. The system uses a hand-crafted dictionary based on Ainu narratives ''yukar''. The system provides three types of information: word/token, part of speech, and translation of the token (in Japanese). Evaluation on a training set provided positive results. The system could be useful in a great number of tasks related to the research on Ainu language, such as content analysis or translation, which till now have been done mostly manually.	brill tagger;hidden markov model;markov chain;part-of-speech tagging	Michal Ptaszynski;Yoshio Momouchi	2012	Expert Syst. Appl.	10.1016/j.eswa.2012.04.031	natural language processing;speech recognition;computer science;machine learning;hidden markov model	ML	-21.76636491075578	-82.3269002883589	19286
1cbf7fd5377e09048f859606cca0e44b355c6c94	kisti at clef ehealth 2015 task 2		Laypeople (e.g., patients and their caregivers) usually use queries which describe a sign, symptom or condition to obtain relevant medical information on the Web. They can fail to find useful information for diagnosing or understanding their health conditions because the search results delivered by existing medical search engines do not fit the information needs of users. To deliver useful medical information, we attempted to combine multiple ranking methods, explicit semantic analysis (ESA), a cluster-based external expansion model (CBEEM), and concept-based document centrality (CBDC), using external medical resources to improve retrieval performance. As a first step, initial documents are searched using a baseline method. Based on the initial documents, ranking methods are selectively applied. Our experiments with combinations of ranking methods aim to find the best means of computing accurate similarity scores using different external medical resources. The best performance was obtained when the CBEEM and the CBDC were used together.	baseline (configuration management);centrality;esa;experiment;explicit semantic analysis;information needs;naivety;web search engine;world wide web	Heung-Seon Oh;Yuchul Jung;Kwang-Young Kim	2015			computer science;data mining;world wide web;information retrieval	NLP	-29.894307708460392	-59.91106599217232	19307
e26e6be8b15dcefc04569a8fecb14be5fd3fc1ba	computerized case history - an effective tool for management of patients and clinical trials	diagnosis;standard protocols;telemedicine;clinical trials;computerized case history;prevention of medical errors	UNLABELLED Monitoring diagnostic procedures, treatment protocols and clinical outcome are key issues in maintaining quality medical care and in evaluating clinical trials. For these purposes, a user-friendly computerized method for monitoring all available information about a patient is needed.   OBJECTIVE To develop a real-time computerized data collection system for verification, analysis and storage of clinical information on an individual patient.   METHODS Data was integrated on a single time axis with normalized graphics. Laboratory data was set according to standard protocols selected by the user and diagnostic images were integrated as needed. The system automatically detects variables that fall outside established limits and violations of protocols, and generates alarm signals.Results. The system provided an effective tool for detection of medical errors, identification of discrepancies between therapeutic and diagnostic procedures, and protocol requirements.   CONCLUSIONS The computerized case history system allows collection of medical information from multiple sources and builds an integrated presentation of clinical data for analysis of clinical trials and for patient follow-up.	apache axis;clinical data;communications protocol;data collection;genus axis;graphics;patients;protocols documentation;real-time clock;requirement;usability;verification of theories;diagnostic procedure	Nikita Shklovskiy-Kordi;Boris Zingerman;Nikolay Rivkind;Saveli Goldberg;Scott Davis;Lyuba Varticovski;Marina Krol;A. M. Kremenetzkaia;Andrei Vorobiev;Ilia Serebriyskiy	2005	Studies in health technology and informatics		data mining;clinical trial;analysis of clinical trials;data collection system;telemedicine;medicine	ML	-57.58475722580357	-65.22227736648635	19344
90076e243f767b63c7ab34c6957b9f22c580d599	contribution to the design of an expressive speech synthesis system for the arabic language		In this paper we will present a contribution to the design of an expressive speech synthesis system for the Arabic language. The system uses diphone concatenation as the synthesis method for the generation of 10 phonetically balanced sentences in Arabic. Rules for the orthographic-to-phonetic transcription are detailed, as well as the methodology employed for recording the diphone database. The sentences were synthesized with both “neutral” and “sadness” expressions and rated by 10 listeners, and the results of the test are provided.	speech synthesis	Lyes Demri;Leila Falek;Hocine Teffahi	2015		10.1007/978-3-319-23132-7_22	natural language processing;speech recognition;linguistics	PL	-15.486847203266235	-84.5021214503027	19406
627c26b153617df17a87c0762b4dd9033864f562	learning-based pronoun resolution for turkish with a comparative evaluation	pronoun resolution;apprentissage automatique;overfitting;anaphore pronominale;underfitting;learning model;anaphora resolution;non linear model;expressive power;machine learning;turc;computational linguistics;turkish;pronominal anaphora;linguistique informatique;linear versus non linear classifiers	The aim of this paper is twofold. On the one hand, it attempts to explore several machine learning models for pronoun resolution in Turkish, a language not sufficiently studied with respect to anaphora resolution and rarely being subjected to machine learning experiments. On the other hand, this paper offers an evaluation of the classification performances of the learning models in order to gain insight into the question of how to match a model to the task at hand. In addition to the expected observation that each model should be tuned to an optimum level of expressive power so as to avoid underfitting and overfitting, the results also suggest that non-linear models properly tuned to avoid overfitting outperform linear ones when applied to the data used in our experiments. 2008 Elsevier Ltd. All rights reserved.	algorithm;anaphora (linguistics);consistency model;experiment;expressive power (computer science);f1 score;feature selection;linear classifier;linear model;machine learning;mathematical optimization;nonlinear system;overfitting;performance;statistical classification;text corpus	Yilmaz Kiliçaslan;Edip Serdar Güner;Savas Yildirim	2009	Computer Speech & Language	10.1016/j.csl.2008.09.001	natural language processing;speech recognition;computer science;computational linguistics;machine learning;overfitting;linguistics;expressive power	AI	-20.990924953763006	-71.96814054007444	19415
6f52353b13b12e42316d7ec539b9cd926bc8187a	using detailed linguistic structure in language modelling		Recently, considerable attention has been accorded to attempts to apply natural language processing techniques to language modelling for speech recognition. Another extension to the standard n-gram technique has been the use of trigger-pair predictors. In the present experiments, we incorporate into language models, information derived from detailed syntactic and semantic parses and taggings. We use a human expert to de ne the interesting features of the history, and these are formalized as triggers and integrated with a trigram language model using the maximum entropy framework. We select maximum entropy because it provides a convenient method of combining multiple information sources. We employ two di erent kinds of triggering events: those based on a knowledge of the full parse of the previous sentences in the document, and those based on knowledge of the syntactic/semantic tags to the left of and in the same sentence as the word being predicted. We contrast results obtained using these events plus a baseline n{gram language model, both with the baseline model itself, and with the baseline model plus triggers based on word triggers chosen automatically. Mutual information selects the best trigger pairs from all candidates generated by combining each of these triggering events with every word in the vocabulary. The grammar and tagset used to express linguistic information about English are unusually detailed. The tagset contains some 3,000 syntactic/semantic tags. Using a 200-million-word training set composed of Wall Street Journal and Associated Press newswire text we reduced test-set perplexity by 11.3% as against the baseline model. Further, our method when combined with long{distance word triggers reduced test-set perplexity by 21.7%.	baseline (configuration management);database trigger;experiment;language model;mutual information;n-gram;natural language processing;parsing;perplexity;principle of maximum entropy;speech recognition;test set;the wall street journal;trigram;vocabulary	Ruiqiang Zhang;Ezra Black;Andrew M. Finch	1999			trigram;syntax;perplexity;linguistics;linguistic description;parsing;language model;pattern recognition;natural language processing;computer science;artificial intelligence;rule-based machine translation;vocabulary	NLP	-23.562930686776046	-78.69326913539403	19443
28de1833dfe1fd12b81ae73d73e863403f4c6c3b	beyond active noun tagging: modeling contextual interactions for multi-class active learning	appearance models;pragmatics;object recognition;noun;uncertainty;active learning;training;image classification;layout;joints;satisfiability;multi class classification;active noun tagging;image entropy;multiclass active learning;scene understanding tasks;appearance models active noun tagging contextual interaction multiclass active learning active learning framework scene understanding tasks multiclass classification classification uncertainty image entropy;learning artificial intelligence entropy image classification;contextual interaction;humans;entropy;computer science;learning artificial intelligence;scene understanding;context modeling;multiclass classification;classification uncertainty;human activity;active learning framework;labeling;tagging context modeling entropy labeling boats humans layout uncertainty computer science educational institutions;tagging;boats	We present an active learning framework to simultaneously learn appearance and contextual models for scene understanding tasks (multi-class classification). Existing multi-class active learning approaches have focused on utilizing classification uncertainty of regions to select the most ambiguous region for labeling. These approaches, however, ignore the contextual interactions between different regions of the image and the fact that knowing the label for one region provides information about the labels of other regions. For example, the knowledge of a region being sea is informative about regions satisfying the “on” relationship with respect to it, since they are highly likely to be boats. We explicitly model the contextual interactions between regions and select the question which leads to the maximum reduction in the combined entropy of all the regions in the image (image entropy). We also introduce a new methodology of posing labeling questions, mimicking the way humans actively learn about their environment. In these questions, we utilize the regions linked to a concept with high confidence as anchors, to pose questions about the uncertain regions. For example, if we can recognize water in an image then we can use the region associated with water as an anchor to pose questions such as “what is above water?”. Our active learning framework also introduces questions which help in actively learning contextual concepts. For example, our approach asks the annotator: “What is the relationship between boat and water?” and utilizes the answer to reduce the image entropies throughout the training dataset and obtain more relevant training examples for appearance models.	active learning (machine learning);information;interaction;multiclass classification	Behjat Siddiquie;Abhinav Gupta	2010	2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2010.5540044	computer vision;computer science;machine learning;multiclass classification;pattern recognition;pragmatics	Vision	-10.648553109323663	-63.85850650328605	19470
26a6e9a693c7e8d9331c7f4a683aa7ef6790b0d3	introducing decision support for smart mobile health behavior change applications	eigenvalues and eigenfunctions;standards;decision aid;health behaviours discriminant analysis decision aid;discriminant analysis;indexes;health behaviours;system on chip;user experience improvement decision support smart mobile health behavior change applications therapeutic lifestyle change decision aid system unhealthy behaviors information collection healthy lifestyles optimal self reported parameter set data entry minimization application users primary health behavior choice determinant identification tlc da system discriminant analysis optimal predictor set healthy behavior choices computer mediated decision aid baseline variables primary variables consumer decision making smoking status smoking cessation success estimation self efficacy body mass index diet status choice smoking cessation choice weight management choice physical activity choice;mobile communication;system on chip correlation education mobile communication indexes standards eigenvalues and eigenfunctions;smart phones decision support systems medical information systems mobile computing;correlation	We developed Therapeutic Lifestyle Change Decision Aid (TLC DA) system to support an informed choice about which behavior change to work on when multiple unhealthy behaviors are present. The system collects significant amount of information which is used to generate tailored messages to consumers in order to persuade them in following certain healthy lifestyles. One of the current limitations of the system is the necessity to collect vast amount of information from users who have to manually enter all required data. By identifying optimal set of self-reported parameters we should be able to minimize the data entry burden of the app users. The main goal of this study was to identify primary determinants of health behavior choices made by patients after using the TLC DA system. Using discriminant analysis an optimal set of predictors was identified which determined healthy behavior choices of users of a computer-mediated decision aid. We were able to reduce the initial set of 45 baseline variables to 5 primary variables driving consumer decision making regarding health behavior choice. The resulting set included smoking status, smoking cessation success estimate, self-efficacy, body mass index and diet status. Prediction of smoking cessation choice was the most accurate (73%) followed by weight management choice (67%). Physical activity and diet choices were much better identified in a combined cluster (76%-87%). The resulting minimized parameter set can significantly improve user experience.	baseline (configuration management);decision support system;human body weight;linear discriminant analysis;mhealth;multi-level cell;user experience	Rita Kukafka;In Cheol Jeong;Joseph Finkelstein	2015	2015 International Conference on Big Data and Smart Computing (BIGCOMP)	10.1109/35021BIGCOMP.2015.7072856	simulation;engineering;operations management;social psychology	HCI	-62.84652339848635	-62.23254332173073	19488
6d958dd2b41bca3279a28146dd37e9fbe39ccfb3	content and context: two-pronged bootstrapped learning for regex-formatted entity extraction		Regular expressions are an important building block of rulebased information extraction systems. Regexes can encode rules to recognize instances of simple entities which can then feed into the identification of more complex cross-entity relationships. Manually crafting a regex that recognizes all possible instances of an entity is difficult since an entity can manifest in a variety of different forms. Thus, the problem of automatically generalizing manually crafted seed regexes to improve the recall of IE systems has attracted research attention. In this paper, we propose a bootstrapped approach to improve the recall for extraction of regex-formatted entities, with the only source of supervision being the seed regex. Our approach starts from a manually authored high precision seed regex for the entity of interest, and uses the matches of the seed regex and the context around these matches to identify more instances of the entity. These are then used to identify a set of diverse, high recall regexes that are representative of this entity. Through an empirical evaluation over multiple real world document corpora, we illustrate the effectiveness of our approach.	algorithm;artificial neural network;bootstrapping (compilers);brown corpus;encode;entity;information extraction;iteration;iterative method;logistic regression;named-entity recognition;point of sale;regular expression;seed;text corpus;word lists by frequency	Stanley Simoes;P Deepak;Munu Sairamesh;Deepak Khemani;Sameep Mehta	2018			computer science;machine learning;artificial intelligence;data mining;bootstrapping	AI	-22.445341155968904	-67.6232373464393	19507
6bbbf1b25b747512a29123214ac9547ba0571c18	extraction of product information object for trustworthiness	information extraction;e commerce;web sites electronic commerce internet retail data processing;semantics;trustworthiness;product reviews e commerce information extraction semantics trustworthiness;data mining entropy web pages semantics dictionaries html feature extraction;product reviews;product review extraction product information object extraction online market fake products shoddy products product authenticity product reliability product credibility trustworthiness evaluation product reviews project background web page pretreatment	Online market brings a wealth of information to consumers. However, online market is also flooded with fake and shoddy products. Trustworthiness is a solution to solve these problems which is raised by the authors' laboratory. The model of trustworthiness evaluates the product in three dimensions, which is authenticity, reliability and credibility. But the data to analyse of trustworthiness evaluation is mainly from manual import. Product Information Object is the organic whole of product information and product reviews. Extraction of Product Information Object for Trustworthiness in this paper will expand the data source of product information and product reviews for Trustworthiness effectively. In this paper, combining with project background, we made a deep research on pages capture, pre-treatment of web page, extraction of web information. And we find out that the accurate rate of extraction of product information is not high and the coverage rate of extraction of product reviews is very low. To solve these problems, we improve the related algorithms, and design a module which can extract product information objects automatically. In this way, the requirement of analysis data for Trustworthiness will be satisfied.	algorithm;authentication;dark web;data dictionary;denial-of-service attack;trust (emotion);web page	Shenglong Mi;Yinsheng Li;Hao Chen;Yong Fang	2014	2014 IEEE 11th International Conference on e-Business Engineering	10.1109/ICEBE.2014.50	e-commerce;trustworthiness;computer science;marketing;data mining;database;semantics;world wide web;information extraction;information retrieval	DB	-22.591184632138354	-56.721145016116	19513
031e0103c3e6d2bab1c62b41f277e67f4fa6626f	bridging the gap: incorporating a semantic similarity measure for effectively mapping pubmed queries to documents	learning to rank;pubmed literature search;semantic similarity;word mover’s distance;word embeddings	Abstract The main approach of traditional information retrieval (IR) is to examine how many words from a query appear in a document. A drawback of this approach, however, is that it may fail to detect relevant documents where no or only few words from a query are found. The semantic analysis methods such as LSA (latent semantic analysis) and LDA (latent Dirichlet allocation) have been proposed to address the issue, but their performance is not superior compared to common IR approaches. Here we present a query-document similarity measure motivated by the Word Mover’s Distance. Unlike other similarity measures, the proposed method relies on neural word embeddings to compute the distance between words. This process helps identify related words when no direct matches are found between a query and a document. Our method is efficient and straightforward to implement. The experimental results on TREC Genomics data show that our approach outperforms the BM25 ranking function by an average of 12% in mean average precision. Furthermore, for a real-world dataset collected from the PubMed ® search logs, we combine the semantic measure with BM25 using a learning to rank method, which leads to improved ranking scores by up to 25%. This experiment demonstrates that the proposed approach and BM25 nicely complement each other and together produce superior performance.	anterior descending branch of left coronary artery;bridging (networking);complement system proteins;computational genomics;document;information retrieval;latent dirichlet allocation;latent semantic analysis;learning to rank;lichen sclerosus et atrophicus;preparation;pubmed;ranking (information retrieval);search problem;semantic similarity;silo (dataset);similarity measure;word embedding;trec	Sun Kim;Nicolas Fiorini;W. John Wilbur;Zhiyong Lu	2017	Journal of biomedical informatics	10.1016/j.jbi.2017.09.014	data mining;computer science;latent dirichlet allocation;learning to rank;ranking (information retrieval);information retrieval;semantic similarity;similarity measure;artificial intelligence;ranking;latent semantic analysis;pattern recognition;trec genomics	Web+IR	-27.290179688295055	-62.85266671046568	19536
19abd8312a3a2087789124bc2406e00e6209caf2	deriving human-readable labels from sparql queries	web of data;log files;linked data;qa75 electronic computers computer science;semantic web;sparql;labels	Over 80% of entities on the Semantic Web lack a human-readable label. This hampers the ability of any tool that uses linked data to offer a meaningful interface to human users. We argue that methods for deriving human-readable labels are essential in order to allow the usage of the Web of Data. In this paper we explore, implement, and evaluate a method for deriving human-readable labels based on the variable names used in a large corpus of SPARQL queries that we built from a set of log files. We analyze the structure of the SPARQL graph patterns and offer a classification scheme for graph patterns. Based on this classification, we identify graph patterns that allow us to derive useful labels. We also provide an overview over the current usage of SPARQL in the newly built corpus.	comparison and contrast of classification schemes in linguistics and metadata;data logger;entity;human-readable medium;linked data;sparql;semantic web;text corpus;world wide web	Basil Ell;Denny Vrandecic;Elena Paslaru Bontas Simperl	2011		10.1145/2063518.2063535	named graph;computer science;sparql;data mining;database;information retrieval	Web+IR	-30.45364643721169	-66.76085108653055	19579
f057424dcab5795ee8fc4c4cb6941de6b8f69f06	biologically plausible speech recognition using spike-based phase locking cues	biology computing;spike based phase locking cues;speech signal;prototypes;speech processing;speech;speech recognition biology computing prototypes signal to noise ratio noise robustness humans information processing speech enhancement speech processing signal processing;speech enhancement;human recognition;vowel dataset;noise robustness;speech signal biologically plausible speech recognition spike based phase locking cues vowel dataset baseline algorithm human recognition information processing;mel frequency cepstral coefficient;hidden markov models;feature extraction;signal processing;biologically plausible speech recognition;information processing;baseline algorithm;speech recognition;humans;neurons;signal to noise ratio;encoding	A biologically plausible algorithm is proposed for phoneme recognition that makes use of spikes for computation. The prototype system is demonstrated on voiced phonemes and shows competitive performance with state-of-the-art systems on a vowel dataset in the presence of noise. Using novel phase locking cues, the algorithm performs surprisingly well down to SNR values as low as 5dB, where the performance of the baseline algorithm used for comparison drops considerably. These results suggest, not only a possible explanation for the extreme noise robustness of human recognition, but also a novel technique for information processing for speech signals with computers.	algorithm;arnold tongue;baseline (configuration management);computation;computer;feature extraction;information processing;lock (computer science);prototype;signal-to-noise ratio;speech analytics;speech recognition;vocabulary	Ismail Uysal;John G. Harris	2009	2009 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2009.5117695	speech recognition;information processing;feature extraction;computer science;speech;signal processing;pattern recognition;speech processing;prototype;signal-to-noise ratio;encoding	Arch	-12.732374861314902	-89.8825797066782	19595
c0718651e022d0c642ed649afbc205d72f82dff9	generalized concept overlay for semantic multi-modal analysis of audio-visual content	content management;broadcast news;ontology concepts;audio visual systems;varying semantic importance modality;broadcast news content generalized concept overlay semantic multimodal analysis audio visual content uni modal analysis techniques non learning based approach account potential variability ontology concepts varying semantic importance modality;video analysis;uni modal analysis techniques;data mining;ontologies artificial intelligence;media;optical character recognition software;visualization;audio visual content;semantic multi modal analysis video analysis;multimedia communication;streaming media ontologies performance analysis information analysis speech analysis informatics telematics multimedia communication availability web and internet services;ontologies artificial intelligence audio visual systems broadcasting content management;speech recognition;audio visual;ontologies;semantic multi modal analysis;broadcast news content;generalized concept overlay;non learning based approach;account potential variability;modal analysis;broadcasting;semantic multimodal analysis	In this work, the problem of performing multi-modal analysis of audio-visual streams by effectively combining the results of multiple uni-modal analysis techniques is addressed. A non-learning-based approach is proposed to this end, that takes into account the potential variability of the different uni-modal analysis techniques in terms of the decomposition of the audio-visual stream that they adopt, the concepts of an ontology that they consider, the varying semantic importance of each modality, and other factors. Preliminary results from the application of the proposed approach to broadcast News content reveal its effectiveness.	experiment;modal logic;modality (human–computer interaction);semantic analysis (compilers);spatial variability	Vasileios Mezaris;Spyros Gidaros;Yiannis Kompatsiaris	2009	2009 Fourth International Workshop on Semantic Media Adaptation and Personalization	10.1109/SMAP.2009.13	computer science;multimedia;world wide web;information retrieval	AI	-15.523289683094848	-57.9226483920994	19603
8d8f87f881122d7bd4890ef5f18b0dceb05613a8	automatic persian wordnet construction	automatic persian wordnet construction;persian wordnet;persian wordnet construction;prenceton wordnet;persian word-net;maximum score;automatic method;candidate synset;pwn synsets;persian word	In this paper, an automatic method for Persian WordNet construction based on Prenceton WordNet 2.1 (PWN) is introduced. The proposed approach uses Persian and English corpora as well as a bilingual dictionary in order to make a mapping between PWN synsets and Persian words. Our method calculates a score for each candidate synset of a given Persian word and for each of its translation, it selects the synset with maximum score as a link to the Persian word. The manual evaluation on selected links proposed by our method on 500 randomly selected Persian words, shows about 76.4% quality respect to precision measure. By augmenting the Persian WordNet with the un-ambiguous words, the total accuracy of automatically extracted Persian WordNet is about 82.6% which outperforms the previously semi-automated generated Persian WordNet by about 12.6%.	bilingual dictionary;pwn;randomness;semiconductor industry;synonym ring;text corpus;wordnet	Mortaza Montazery;Heshaam Faili	2010			natural language processing;speech recognition;computer science;information retrieval	NLP	-24.745548639134842	-76.51535057517403	19605
58eff6a342bcb3c075e80ad7684b0b2481144a03	stylization of pitch with syllable-based linear segments	stylized pitch contour resyntheses;piecewise linear approximation;speech synthesis;gaussian processes;syllables;speech processing;speech analysis;speech segmentation;speech analysis piecewise linear approximation speech processing frequency estimation speech coding gaussian approximation testing linear approximation information analysis software standards;linear approximation;speech;momel algorithm;approximation theory;pitch tracking;gaussian fitting;piece wise linear;pitch stylization;piecewise linear approximation speech analysis speech processing;fundamental frequency contours;linear approximation pitch stylization syllable based linear segments fundamental frequency contours speech speech segmentation syllables energy envelope gaussian fitting stylized pitch contour resyntheses momel algorithm;artificial intelligence;electrical engineering;energy envelope;syllable based linear segments;fundamental frequency;speech synthesis approximation theory gaussian processes speech processing	Fundamental frequency contours for speech, as obtained by common pitch tracking algorithms, contain a great deal of fine detail that is unlikely to hold much perceptual significance for listeners. In our experiments, a radically reduced pitch contour consisting of a single linear segment for each syllable was found to judged as equally natural as the original pitch track by listeners, based on high-quality analysis- synthesis. We describe the algorithms both for segmenting speech into syllables based on fitting Gaussians to the energy envelope, and for approximating the pitch contour by independent linear segments for each syllable. We report our web-based test in which 40 listeners compared the stylized pitch contour resyntheses to equivalent resyntheses based on the original pitch track, and also to pitch tracks stylized by the existing Momel algorithm. Listeners preferred the original pitch contour to the linear approximation in only 60% of cases, where 50% would indicate random guessing. By contrast, the original was preferred over Momel in 74% of cases.	experiment;linear approximation;pitch (music);pitch detection algorithm;syllable;web application	Suman V. Ravuri;Daniel P. W. Ellis	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4518527	relative pitch;speech recognition;computer science;speech;pitch detection algorithm;gaussian process;speech processing;mathematics;fundamental frequency;speech segmentation;speech synthesis;audio time-scale/pitch modification;linear approximation;approximation theory	Vision	-8.70679315257514	-88.3167614078515	19607
3f14407f5144213a02bee93b54c3af4963b48039	resolution of lexical ambiguities in spoken dialogue system		The development of conversational multidomain spoken dialogue systems poses new challenges for the reliable processing of less restricted user utterances. Unlike in controlled and restricted dialogue systems a simple oneto-one mapping from words to meanings is no longer feasible here. In this paper two different approaches to the resolution of lexical ambiguities are applied to a multi-domain corpus of speech recognition output produced from spontaneous utterances in a spoken dialogue system. The resulting evaluations show that all approaches yield significant gains over the majority class baseline performance of .68, i.e. fmeasures of .79 for the knowledge-driven approach and .86 for the supervised learning approach.	baseline (configuration management);dialog system;speech recognition;spoken dialog systems;spontaneous order;supervised learning	Berenike Litz;Robert Porzel	2004			natural language processing;computer science;supervised learning;artificial intelligence	NLP	-18.02034860034441	-78.15835921503395	19608
78b513fcb78bcec28c47f9641546e860aab6abca	improving the performance of topic tracking system by ensemble	tracking system;information retrieval;binary classification problem;training;information filtering;text analysis;text classification;text classification majority voting ensemble topic tracking system ensemble technology binary classification problem;ensemble;text analysis pattern classification;adaptation model;ensemble topic tracking system;classification algorithms;tdt3 corpus;pattern classification;tdt3 corpus topic tracking ensemble;computational linguistics;majority voting;information filters;ensemble technology;topic tracking;information filtering information filters information retrieval classification algorithms educational institutions voting computer science software engineering monitoring performance evaluation	The aim of topic tracking is to monitor the stream of news stories to find additional stories on a topic that was identified by several sample stories. In this paper, we adopt majority voting to ensemble several topic tracking systems. The experiment results on TDT3 corpus show that our ensemble topic tracking system performs better than each individual topic tracking system, and that the performance of topic tracking system could be improved by ensemble technology.	tracking system	Xiangju Qin;Yang Zhang	2008	2008 International Conference on Computer Science and Software Engineering	10.1109/CSSE.2008.984	ensembl;majority rule;tracking system;computer science;artificial intelligence;computational linguistics;machine learning;pattern recognition;information retrieval	SE	-21.40079367315951	-61.36530760810305	19615
321bc5a252da1076d54307c5f3411e3c9f79400d	invention property-function network analysis of patents: a case of silicon-based thin film solar cells	explotacion texto;analyse bibliometrique;textual analysis;technological innovation;text mining;patent analysis;information mapping;tratamiento del lenguaje natural;fouille texte;cartografia informacion;function;property;network analysis;emerging technology;patents;thin film solar cell;social network analysis;cartographie information;bibliometric analysis;patente;recherche scientifique;analyse de reseau;patent mining;brevet;scientific research;technology analysis;natural language processing;traitement du langage naturel;investigacion cientifica;technological trend;analisis bibliometrico	Technology analysis is a process which uses textual analysis to detect trends in technological innovation. Co-word analysis (CWA), a popular method for technology analysis, encompasses (1) defining a set of keyword or key phrase patterns which are represented in technology-dependent terms, (2) generating a network that codifies the relations between occurrences of keywords or key phrases, and (3) identifying specific trends from the network. However, defining the set of keyword or key phrase patterns heavily relies on effort of experts, who may be expensive or unavailable. Furthermore defining keyword or key phrase patterns of new or emerging technology areas may be a difficult task even for experts. To solve the limitation in CWA, this research adopts a property-function based approach. The property is a specific characteristic of a product, and is usually described using adjectives; the function is a useful action of a product, and is usually described using verbs. Properties and functions represent the innovation concepts of a system, so they show innovation directions in a given technology. The proposed methodology automatically extracts properties and functions from patents using natural language processing. Using properties and functions as nodes, and co-occurrences as links, an invention property-function network (IPFN) can be generated. Using social network analysis, the methodology analyzes technological implications of indicators in the IPFN. Therefore, without predefining keyword or key phrase patterns, the methodology assists experts to more concentrate on their knowledge services that identify trends in technological innovation from patents. The methodology is illustrated using a case study of patents related to silicon-based thin film solar cells.	cognitive work analysis;natural language processing;social network analysis;solar cell	Janghyeok Yoon;Sungchul Choi;Kwangsoo Kim	2010	Scientometrics	10.1007/s11192-010-0303-8	text mining;social network analysis;social science;scientific method;network analysis;computer science;information mapping;artificial intelligence;thin film solar cell;data mining;property;emerging technologies;operations research;world wide web;function	Security	-36.47989137106326	-65.23482474596513	19637
3d8fff588791ab029aee365058a7ce01b90cfc9f	large-scale multimodal movie dialogue corpus	film;dialogue;corpus;vad;multimodal;dnn;movie	We present an outline of our newly created multimodal dialogue corpus that is constructed from public domain movies. Dialogues in movies are useful sources for analyzing human communication patterns. In addition, they can be used to train machine-learning-based dialogue processing systems. However, the movie files are processing intensive and they contain large portions of non-dialogue segments. Therefore, we created a corpus that contains only dialogue segments from movies. The corpus contains 165,368 dialogue segments taken from 1,722 movies. These dialogues are automatically segmented by using deep neural network-based voice activity detection with filtering rules. Our corpus can reduce the human workload and machine-processing effort required to analyze human dialogue behavior by using movies.	artificial neural network;deep learning;dialog system;machine learning;multimodal interaction;text corpus;voice activity detection	Ryu Yasuhara;Masashi Inoue;Ikuya Suga;Tetsuo Kosaka	2016		10.1145/2993148.2998523	natural language processing;film;speech recognition;computer science	NLP	-17.723147594345672	-80.86401675018344	19660
3ce3cd447948938d384b647aa3a3ec6f4b53d103	convergence classification and replication prediction for simulation studies		Providing assistance systems for simulation studies can support the user by performing monotonous tasks and keeping track of relevant results. In this paper we present approaches to estimate, if - and when - statistically sig- nificant results are expected for certain investigations. This information can be used to control simulation runs or to provide information to the user for interac- tion. The first approach is used to classify if significance is expected to occur for given samples and the second approach estimates the needed replications until significance is expected be reached. For an initial evaluation of the approaches, experiments are performed on samples drawn from normal distributions.		Andreas D. Lattner;Tjorben Bogon;Ingo J. Timm	2011		10.1007/978-3-642-29966-7_17	simulation;computer science;data mining	Metrics	-38.591995364399374	-61.704532238548246	19667
73ff09b9717fb60508ed49e6ff835faa68f46840	improving failure mode and effects analysis through electronic health record-assisted team identification			failure mode and effects analysis	Gayle Shier Kricke;Matthew B. Carson;Young Ji Lee;Nicholas D. Soulakis	2015			medical record;failure mode and effects analysis;engineering;reliability engineering	HCI	-56.16547517538418	-64.54712790076834	19670
a3448a7de18f84f90f08708017232e1c690a005b	a conversational virtual human as autonomous assistant for elderly and cognitively impaired users? social acceptability and design considerations		In this paper we explore how a conversational virtual human could be designed to be deployed as a socially acceptable autonomous assistive system for elderly and cognitively impaired users. In particular, we focus on a system’s functionality in helping to maintain a well-structured daily life. We present initial findings from two types of studies: (1) Conducting interviews and focus groups considering users' attitudes and design considerations for assistance in maintaining well-structured daily routines and (2) analyzing interaction between the system and its users' while entering data into a calendar application. Analysis has revealed a set of design considerations for developing a socially acceptable system. Microanalytic investigation of one user’s concrete interaction with a Wizard-of-Oz version of the system has shown that a cognitively impaired person is able – by himself and only through interacting with the system – to gain insights into the system’s possibilities and limits and to mistrust the system’s competencies once the system initiates a repair sequence.	acceptable use policy;autonomous robot;autonomous system (internet);calendaring software;cognition;distrust;exact cover;focus group;german research centre for artificial intelligence;interaction;microphone;sensor;sleep mode;transcription (software);virtual actor	Marcel Kramer;Ramin Yaghoubzadeh;Stefan Kopp;Karola Pitsch	2013			focus group;human–computer interaction;competence (human resources);psychology	HCI	-57.69523716431083	-52.47659548414304	19695
b926eca4c18a7cb0c897df2913e9d6d574c5296e	quasi comprehension of natural language simulated by means of information traces	natural language	Abstract   Good information storage and retrieval systems should not only meet the looseness of human memory but also the looseness of natural language. Our information trace method allows for the creation of mechanical systems that combine looseness with indulgence. In accordance with this method, information records consisting of pieces of natural language are attached to each other, leaving fuzzy semantic traces behind in the form of sets of three-letter-strings which have been derived from the original texts. The system calculates an “importance value” for each possible answer to a user query. Queries also may be given in the form of a simple piece of natural language. Answers are communicated to the user, if he so wishes, in descending order of “importance value” as calculated by the system.	natural language;tracing (software)	T. de Heer	1979	Inf. Process. Manage.	10.1016/0306-4573(79)90041-4	natural language processing;language identification;natural language programming;computer science;machine learning;mathematics;linguistics;natural language;world wide web;information retrieval;algorithm	NLP	-35.86601883867881	-80.63218072659113	19700
30388a4a29649b073defc92a28fae307e5cac6ee	geographic information retrieval and text mining on chinese tourism web pages	web pages;geographic information system;text mining;geographic information retrieval;web mining;regular expression	The World Wide Web (WWW) offers an enormous wealth of information and data, and assembles a tremendous amount of knowledge. Much of this knowledge, however, comprises either non-structured data or semi-structured data. To make use of these unexploited or underexploited resources more efficiently, the management of information and data gathering has become an essential task for research and development. In this paper, the author examines the task of researching a hostel or homestay using the Google search web service as a base search engine. From the search results, mining, retrieving and sorting out location and semantic data were carried out by combining the Chinese Word Segmentation System with text mining technology to find geographic information gleaned from web pages. The results obtained from this particular searching method allowed users to get closer to the answers they sought and achieve greater accuracy, as the results included graphics and textual geographic information. In the future, this method may be suitable for and applicable to various types of queries, analyses, geographic data collection, and in managing spatial knowledge related to different keywords within a document.	google search;graphics;information retrieval;semi-structured data;semiconductor industry;sorting;text mining;text segmentation;www;web page;web search engine;web service;world wide web	Ming-Cheng Tsou	2010	IJITWE	10.4018/jitwe.2010010104	local information systems;web mining;text mining;computer science;web page;data mining;database;geographic information system;web intelligence;programming language;world wide web;information retrieval;regular expression	Web+IR	-31.62176516793244	-57.6164672473023	19701
ca5409a7f6a4cd2a47862bc46b1d4f3117f3a070	dialogue learning with human teaching and feedback in end-to-end trainable task-oriented dialogue systems		In this work, we present a hybrid learning method for training task-oriented dialogue systems through online user interactions. Popular methods for learning task-oriented dialogues include applying reinforcement learning with user feedback on supervised pretraining models. Efficiency of such learning method may suffer from the mismatch of dialogue state distribution between offline training and online interactive learning stages. To address this challenge, we propose a hybrid imitation and reinforcement learning method, with which a dialogue agent can effectively learn from its interaction with users by learning from human teaching and feedback. We design a neural network based task-oriented dialogue agent that can be optimized end-toend with the proposed learning method. Experimental results show that our end-to-end dialogue agent can learn effectively from the mistake it makes via imitation learning from user teaching. Applying reinforcement learning with user feedback after the imitation learning stage further improves the agent’s capability in successfully completing a task.	artificial neural network;dialog system;end-to-end principle;feedback;interaction;network model;online and offline;reinforcement learning;text corpus	Bing Liu;Gökhan Tür;Dilek Z. Hakkani-Tür;Pararth Shah;Larry P. Heck	2018			natural language processing;machine learning;artificial neural network;end-to-end principle;artificial intelligence;reinforcement learning;computer science;imitation;interactive learning	AI	-15.069643011411147	-75.44411268840888	19734
c40d821b6358bada6baaa164753920c63e7aab83	lexical url analysis for discriminating phishing and legitimate e-mail messages	electronic mail;performance evaluation;training;emails;computer crime;phishing attacks;accuracy;data analysis;emails phishing attacks lexical analysis urls;radio frequency;feature extraction;web sites;web sites computer crime data analysis pattern classification unsolicited e mail;pattern classification;urls;electronic mail feature extraction accuracy training radio frequency performance evaluation humans;humans;unsolicited e mail;classification accuracy;phishing email classification domain lexical url analysis approach phishing e mail message legitimate e mail message e mail message discrimination phishing web site malware code;lexical analysis	Phishing emails contain socially engineered messages to lure victims into performing certain actions, such as clicking on a URL where a phishing website is hosted, or executing a malware code. In a previous study, we proposed a lexical URL analysis approach for detecting phishing websites. In this study, we extend the approach to the phishing email classification domain. The primary motive behind this study is that most phishing email messages contain URLs that point to phishing websites, and lexically analyzing the URLs can enhance the classification accuracy of email messages. As evaluated in this study, the addition of URL lexical analysis in phishing email classification is effective and results in a highly accurate anti-phishing email classifier.	email filtering;lexical analysis;malware;performance evaluation;phishing;sensor;statistical classification	Mahmoud Khonji;Youssef Iraqi;Andrew Jones	2011	2011 International Conference for Internet Technology and Secured Transactions		html email;phishing;spoofed url;computer science;email spoofing;internet privacy;world wide web;computer security;email authentication	SE	-19.92461986798406	-56.1999616213816	19737
67fbb74a41403fe8e724df757cae01e562e98c01	automatic music genre classification in small and ethnic datasets.		Automatic music genre classification commonly relies on a large amount of well-recorded data for model fitting. These conditions are frequently not met in ethnic music collections due to low media availability and ill recording environments. In this paper, we propose an automatic genre classification technique especially designed for small, noisy datasets. The proposed technique uses handcrafted features and a vote-based aggregation process. Its performance was evaluated over a Brazilian ethnic music dataset, showing that using the proposed technique produces higher F1 measures than using traditional data augmentation methods and state-of-the-art, Deep Learning-based methods. Therefore, our method can be used in automatic classification processes for small datasets, which can be helpful in the organization of ethnic music collections.		Tiago Fernandes Tavares;Juliano Henrique Foleiss	2017		10.1007/978-3-030-01692-0_3	natural language processing;speech recognition;music information retrieval;ethnic group;deep learning;computer science;artificial intelligence	ML	-17.16639617277865	-79.88320884314555	19782
144e357b1339c27cce7a1e69f0899c21d8140c1f	recurrent neural network language model training with noise contrastive estimation for speech recognition	vocabulary graphics processing units language translation learning artificial intelligence recurrent neural nets speech recognition;history;speech recognition language model recurrent neural network gpu noise contrastive estimation;training;vocabulary;testing;training noise recurrent neural networks testing vocabulary history speech recognition;speech recognition;cpu recurrent neural network language model training noise contrastive estimation rnnlm training computational cost softmax function normalization term nce large vocabulary conversational telephone speech recognition task gpu;recurrent neural networks;noise	In recent years recurrent neural network language models (RNNLMs) have been successfully applied to a range of tasks including speech recognition. However, an important issue that limits the quantity of data used, and their possible application areas, is the computational cost in training. A signi??cant part of this cost is associated with the softmax function at the output layer, as this requires a normalization term to be explicitly calculated. This impacts both the training and testing speed, especially when a large output vocabulary is used. To address this problem, noise contrastive estimation (NCE) is explored in RNNLM training. NCE does not require the above normalization during both training and testing. It is insensitive to the output layer size. On a large vocabulary conversational telephone speech recognition task, a doubling in training speed on a GPU and a 56 times speed up in test time evaluation on a CPU were obtained.	algorithmic efficiency;artificial neural network;central processing unit;computation;graphics processing unit;language model;period-doubling bifurcation;recurrent neural network;softmax function;speech recognition;speedup;vocabulary	Xie Chen;Xunying Liu;Mark J. F. Gales;Philip C. Woodland	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7179005	natural language processing;speech recognition;computer science;noise;recurrent neural network;machine learning;time delay neural network;software testing	Vision	-18.458238913511252	-89.05813686796422	19803
c8e9e0bceb80a508da15700dd007a5d8d1d6b1e4	expert guided natural language processing using one-class classification	one class classification;novelty detection;feature selection;natural language processing	INTRODUCTION Automatically identifying specific phenotypes in free-text clinical notes is critically important for the reuse of clinical data. In this study, the authors combine expert-guided feature (text) selection with one-class classification for text processing.   OBJECTIVES To compare the performance of one-class classification to traditional binary classification; to evaluate the utility of feature selection based on expert-selected salient text (snippets); and to determine the robustness of these models with respects to irrelevant surrounding text.   METHODS The authors trained one-class support vector machines (1C-SVMs) and two-class SVMs (2C-SVMs) to identify notes discussing breast cancer. Manually annotated visit summary notes (88 positive and 88 negative for breast cancer) were used to compare the performance of models trained on whole notes labeled as positive or negative to models trained on expert-selected text sections (snippets) relevant to breast cancer status. Model performance was evaluated using a 70:30 split for 20 iterations and on a realistic dataset of 10 000 records with a breast cancer prevalence of 1.4%.   RESULTS When tested on a balanced experimental dataset, 1C-SVMs trained on snippets had comparable results to 2C-SVMs trained on whole notes (F = 0.92 for both approaches). When evaluated on a realistic imbalanced dataset, 1C-SVMs had a considerably superior performance (F = 0.61 vs. F = 0.17 for the best performing model) attributable mainly to improved precision (p = .88 vs. p = .09 for the best performing model).   CONCLUSIONS 1C-SVMs trained on expert-selected relevant text sections perform better than 2C-SVMs classifiers trained on either snippets or whole notes when applied to realistically imbalanced data with low prevalence of the positive class.		Erel Joffe;Emily J. Pettigrew;Jorge R. Herskovic;Charles F. Bearden;Elmer V. Bernstam	2015	Journal of the American Medical Informatics Association : JAMIA	10.1093/jamia/ocv010	natural language processing;computer science;artificial intelligence;machine learning;pattern recognition;data mining;feature selection;one-class classification	NLP	-45.953120236725205	-70.01332275623848	19811
06568c53a6dcb767f781995afdbe53522d7d5b13	prague dependency treebank: enrichment of the underlying syntactic annotation by coreferential mark-up			gene ontology term enrichment;treebank	Lucie Kucová;Eva Hajicová	2004	Prague Bull. Math. Linguistics		syntax;natural language processing;linguistics;treebank;artificial intelligence;computer science;annotation	NLP	-30.071451365711535	-76.35123259339652	19816
ebb90d21b947edd2f1dd1d001b8153f572367520	visual-verbal consistency of image saliency		When looking at an image, humans shift their attention towards interesting regions, making sequences of eye fixations. When describing an image, they also come up with simple sentences that highlight the key elements in the scene. What is the correlation between where people look and what they describe in an image? To investigate this problem, we look into eye fixations and image captions, two types of subjective annotations that are relatively task-free and natural. From the annotations, we extract visual and verbal saliency ranks to compare against each other. We then propose a number of low-level and semantic-level features relevant to the visualverbal consistency. Integrated into a computational model, the proposed features effectively predict the consistency between the two modalities on a large dataset with both types of annotations, namely SALICON [1].	computation;computational model;computer vision;eye tracking;high- and low-level	Haoran Liang;Ming Jiang;Ronghua Liang;Qi Zhao	2017	2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2017.8123171	visualization;machine learning;modalities;salience (neuroscience);computer science;artificial intelligence;pattern recognition	Vision	-11.840109524312592	-70.35440830753988	19834
1b3ed8f95fb09b22accb265d85aef7570b5e2c82	a mathematical model of prediction-driven instability: how social structure can drive language change	incrementation;prediction driven instability;mathematical model;language change;social structure;language variation	I discuss a stochastic model of language learning and change. During a syntactic change, each speaker makes use of constructions from two different idealized grammars at variable rates. The model incorporates regularization in that speakers have a slight preference for using the dominant idealized grammar. It also includes incrementation: The population is divided into two interacting generations. Children can detect correlations between age and speech. They then predict where the population’s language is moving and speak according to that prediction, which represents a social force encouraging children not to sound out-dated. Both regularization and incrementation turn out to be necessary for spontaneous language change to occur on a reasonable time scale and run to completion monotonically. Chance correlation between age and speech may be amplified by these social forces, eventually leading to a syntactic change through prediction-driven instability.	instability;interaction;mathematical model;run to completion scheduling;social force model;social structure;spontaneous order	W. Garrett Mitchener	2011	Journal of Logic, Language and Information	10.1007/s10849-011-9136-y	natural language processing;artificial intelligence;social structure;mathematical model;mathematics;linguistics;algorithm;statistics	NLP	-11.14842748427866	-80.66633833429945	19838
dda3cce2ea99ddd09df3159cbd486b144ff11fe8	the effect of user characteristics on the efficiency of visual querying	task performance;cognitive style;student characteristics;information systems;predictor variables;computer graphics;information retrieval;data;information filtering;college students;interactive visualisation;information discovery;graphs;visual querying;graphic displays;navigation information systems;data analysis;visualization;foreign countries;use studies;visual inspection;visual search;experiments;search strategies;computer software;visual aids;information system;user interaction;criteria;human factors engineering	This article may be used for research, teaching and private study purposes. Any substantial or systematic reproduction, redistribution , reselling , loan or sub-licensing, systematic supply or distribution in any form to anyone is expressly forbidden. The publisher does not give any warranty express or implied or make any representation that the contents will be complete or accurate or up to date. The accuracy of any instructions, formulae and drug doses should be independently verified with primary sources. The publisher shall not be liable for any loss, actions, claims, proceedings, demand or costs or damages whatsoever or howsoever caused arising directly or indirectly in connection with or arising out of the use of this material. Information systems increasingly provide options for visually inspecting data during the process of information discovery and exploration. Little research has dealt so far with user interactions with these systems, and specifically with the effects of characteristics of the displayed data and the user on performance with such systems. The study reports an experiment on users' performance with a visual exploration system. Participants had to identify target graphs within a large set of candidate graphs by using visual filtering criteria that differed in their efficiency for reducing the number of candidate graphs. A pay-off matrix and a time limit served to motivate users to select filter criteria efficiently. Performance was measured as the number of correct identifications of target graphs within a time-limit, and the number, type and position of filter criteria selected for the search. Efficiency was somewhat biased by users' preference to select filter criteria sequentially, starting from the left to the right. Rational and experiential cognitive styles affected performance, and they interacted with learning and the types of filter criteria chosen. The study shows that not only visual search tools can be used effectively but also that data and user characteristics affect task performance with such systems. 1. Introduction In meteorology, bio-informatics, finance, production management and many other fields, it is often necessary to explore large time-related data sets to detect patterns in these time series, a problem that is by no means simple (Hochheiser and Shneiderman 2004, Aris et al. 2005, Buono et al. 2005). Because of the difficulty of pattern detection by algorithm, systems have been developed that visualise time series data and allow users to explore the displayed data (Hochheiser and Shneiderman 2001, 2002). Users can manipulate the data directly …	aris express;algorithm;bioinformatics;british informatics olympiad;information discovery;information system;interaction;pattern recognition;primary source;time series;word lists by frequency	Peter Bak;Joachim Meyer	2011	Behaviour & IT	10.1080/0144929X.2010.511264	human–computer interaction;computer science;artificial intelligence;human factors and ergonomics;data mining;multimedia;world wide web;information retrieval;information system;statistics	HCI	-37.15094940130819	-52.26754524559192	19874
e69115c862a638281ab45cf40a9faad44ed6537d	audio and video combined for home video abstraction	image motion analysis;hidden markov models speech support vector machines support vector machine classification data mining abstracts event detection computer science multimedia systems music;audio signal processing;image segmentation;multimedia;home video abstraction;video content classification;support vector machines;video signal processing;hidden markov model;audio content classification;hmm;image classification;speech;video segmentation;event detection;data mining;multimedia systems;video features;motion level;feature extraction home video abstraction multimedia audio features video features audio content classification svm support vector machines hidden markov models hmm video content classification motion level blur degree;hidden markov models;abstracts;feature extraction;signal classification;support vector machine classification;svm;computer science;support vector machine;blur degree;audio features;music;image segmentation signal classification image classification hidden markov models audio signal processing video signal processing feature extraction support vector machines image motion analysis	With the increasing number of people who can afford to make videos to record their lives, home videos play more and more important role in multimedia. Video abstraction is an efficient way to help review such a huge amount of home videos. In this paper, a home video abstraction technique combining audio and video features is presented. The audio contents are firstly classified as silence, pure speech, non-pure speech, music and background sound using SVMs. Then non-pure speech is further classified into song and other non-pure speech using SVM, and background sound is classified into laughter, applause, scream and others using Hidden Markov Models (HMMs). For video contents , motion level and blur degree are acquired. Finally, video segments containing special effects, such as speech, laughter, song, applause, scream, and specified motion level and blur degree, are extracted as the main parts of the abstract. The remaining parts of the abstract are generated using key frame information. The experimental results show that the proposed algorithm can extract desired parts of home video to generate satisfactory video abstracts.	algorithm;gaussian blur;hidden markov model;key frame;markov chain;support vector machine	Ming Zhao;Jiajun Bu;Chun Chen	2003		10.1109/ICASSP.2003.1200046	video compression picture types;support vector machine;computer vision;speech recognition;computer science;machine learning;video tracking;pattern recognition;video processing;hidden markov model	HCI	-5.73367525293479	-94.11877752205275	19877
a7aac8ce7a1637ad1e1614ca81ef91f4f90b0368	using bracketed parses to evaluate a grammar checking application	hand-bracketed parses;underlying performance;parser-grammar system;grammar checking application;unbracketed format;resulting error report;randomly-selected set;grammar checker	We describe a method for evaluating a grammar checking application with hand-bracketed parses. A randomly-selected set of sentences was submitted to a grammar checker in both bracketed and unbracketed formats. A comparison of the resulting error reports illuminates the relationship between the underlying performance of the parsergrammar system and the error critiques presented to the user. I N T R O D U C T I O N The recent development of broad-coverage natural language processing systems has stimulated work on the evaluation of the syntactic component of such systems, for purposes of basic evaluation and improvement of system performance. Methods utilizing hand-bracketed corpora (such as the University of Pennsylvania Treebank) as a basis for evaluation metrics have been discussed in Black et al. (1991), Harrison et al. (1991), and Black et al. (1992). Three metrics discussed in those works were the Crossing Parenthesis Score (a count of the number of phrases in the machine produced parse which cross with one or more phrases in the hand parse), Recall (the percentage of phrases in the hand parse that are also in the machine parse), and Precision (the percentage of phrases in the machine parse that are in the hand parse). We have developed a methodology for using hand-bracketed parses to examine both the internal and external performance of a grammar checker. The internal performance refers to the behavior of the underlying system--i.e, the tokenizer, parser, lexicon, and grammar. The external performance refers to the error critiques generated by the system. 1 Our evaluation methodology relies on three separate error reports generated from a corpus of randomly selected sentences: 1) a report based on unbracketed sentences, 2) a report based on optimally bracketed sentences with our current system, and 3) a report based on the optimal bracketings with the system modified to insure the same coverage as the unbracketed corpus. The bracketed report from the unmodified system tells us something about the coverage of our underlying system in its current state. The bracketed report from the modified system tells us something about the external accuracy of the error reports presented to the user. Our underlying system uses a bottom-up, funambiguity parser. Our error detection method relies on including grammar rules for parsing errorful sentences, with error critiques being generated from the occurrence of an error rule in the parse. Error critiques are based on just one of all the possible parse trees that the system can find for a given sentence. Our major concern about the underlying system is whether the system has a correct parse for the sentence in question. We are also concerned about the accuracy of the selected parse, but our current methodology does not directly address that issue, because correct error reports do not depend on having precisely the correct parse. Consequently, our evaluation of the underlying grammatical coverage is based on a simple metric, namely the parser success rate for satisfying sentence bracketings (i.e. correct parses). Either the parser can produce the optimal parse or it can't. We have a more complex approach to evaluating the performance of the system's ability to detect errors. Here, we need to look at both the 1. We use the term critique to represent an instance of an error detected. Each sentence may have zero or more critiques reported for it.	bottom-up parsing;error detection and correction;grammar checker;lexical analysis;lexicon;natural language processing;parse tree;randomness;syntax (logic);text corpus;top-down and bottom-up design;treebank;word lists by frequency	Richard H. Wojcik;Philip Harrison;John Bremer	1993			natural language processing;computer science;programming language;attribute grammar;algorithm	NLP	-27.427306387328166	-73.61325949553202	19883
af6fed7e3ea71c90ddfdb65bc7d19d02c56901a8	exemplar-based voice conversion in noisy environment	noise robustness voice conversion exemplar based sparse coding non negative matrix factorization;speech dictionaries noise noise measurement feature extraction sparse matrices speech processing;speech synthesis matrix decomposition signal denoising source separation speaker recognition speech coding;speech synthesis;speech coding;speaker recognition;nonnegative matrix factorization exemplar based voice conversion noisy environment parallel exemplars source speech signal encoding target speech signal synthesis source exemplars target exemplars text utterance source speakers target speakers source signal decomposition speaker conversion tasks clean speech data noise added speech data;matrix decomposition;source separation;signal denoising	This paper presents a voice conversion (VC) technique for noisy environments, where parallel exemplars are introduced to encode the source speech signal and synthesize the target speech signal. The parallel exemplars (dictionary) consist of the source exemplars and target exemplars, having the same texts uttered by the source and target speakers. The input source signal is decomposed into the source exemplars, noise exemplars obtained from the input signal, and their weights (activities). Then, by using the weights of the source exemplars, the converted signal is constructed from the target exemplars. We carried out speaker conversion tasks using clean speech data and noise-added speech data. The effectiveness of this method was confirmed by comparing its effectiveness with that of a conventional Gaussian Mixture Model (GMM)-based method.	dictionary;encode;mixture model	Ryoichi Takashima;Tetsuya Takiguchi;Yasuo Ariki	2012	2012 IEEE Spoken Language Technology Workshop (SLT)	10.1109/SLT.2012.6424242	voice activity detection;speaker recognition;speech recognition;computer science;speech coding;pattern recognition;speech processing;matrix decomposition;speech synthesis	Visualization	-16.022281799729654	-92.69284875534156	19915
0f4166c746cbcab8ad577569c98626ede6c97668	discovering and visualizing prototypical artists by web-based co-occurrence analysis	web pages;reference point;web mining;rank correlation	Detecting artists that can be considered as prototypes for particular genres or styles of music is an interesting task. In this paper, we present an approach that ranks artists according to their prototypicality. To calculate such a ranking, we use asymmetric similarity matrices obtained via co-occurrence analysis of artist names on web pages. We demonstrate our approach on a data set containing 224 artists from 14 genres and evaluate the results using the rank correlation between the prototypicality ranking and a ranking obtained by page counts of search queries to Google that contain artist and genre. High positive rank correlations are achieved for nearly all genres of the data set. Furthermore, we elaborate a visualization method that illustrates similarities between artists using the prototypes of all genres as reference points. On the whole, we show how to create a prototypicality ranking and use it, together with a similarity matrix, to visualize a music repository.	similarity measure;software prototyping;web page;web search query	Markus Schedl;Peter Knees;Gerhard Widmer	2005			web mining;computer science;web page;data mining;multimedia;world wide web;rank correlation	Web+IR	-29.061269231284403	-53.76909814431534	19934
d433215b16748c6bbff243103c9a72066373944d	using natural language processing on the free text of clinical documents to screen for evidence of homelessness among us veterans	natural language processing;algorithms;data mining	Information retrieval algorithms based on natural language processing (NLP) of the free text of medical records have been used to find documents of interest from databases. Homelessness is a high priority non-medical diagnosis that is noted in electronic medical records of Veterans in Veterans Affairs (VA) facilities. Using a human-reviewed reference standard corpus of clinical documents of Veterans with evidence of homelessness and those without, an open-source NLP tool (Automated Retrieval Console v2.0, ARC) was trained to classify documents. The best performing model based on document level work-flow performed well on a test set (Precision 94%, Recall 97%, F-Measure 96). Processing of a naïve set of 10,000 randomly selected documents from the VA using this best performing model yielded 463 documents flagged as positive, indicating a 4.7% prevalence of homelessness. Human review noted a precision of 70% for these flags resulting in an adjusted prevalence of homelessness of 3.3% which matches current VA estimates. Further refinements are underway to improve the performance. We demonstrate an effective and rapid lifecycle of using an off-the-shelf NLP tool for screening targets of interest from medical records.		Adi V. Gundlapalli;Marjorie Carter;Miland N. Palmer;Thomas Ginter;Andrew Redd;Steve Pickard;Shuying Shen;Brett R. South;Guy Divita;Scott L. DuVall;Thien M. Nguyen;Leonard W. D'Avolio;Matthew H. Samore	2013	AMIA ... Annual Symposium proceedings. AMIA Symposium		veterans affairs;natural language processing;flags register;data mining;medical record;artificial intelligence;test set;medicine	NLP	-46.467996559641676	-70.32044867175935	19970
21f5ce22d589d89ae3f3a2fa11de10ffd25f31cd	one week with a corporate search engine: a time based analysis of intranet information seeking		In this explorative work, we have focused on understanding information seeking behaviour amongst intranet users. By carrying out a time-based analysis of a week’s worth of log data from a corporate-internal search engine, we have been able to observe patterns of usage as it shifts over days and hours. The results show that numbers of started sessions and activities correlate and follow business hours closely but also that the number of terms per query differs significantly over the day but is constant over the week. The number of active users and the number of sessions are higher early in the week and declines as the week progresses, and we also note that frequent search engine users also log more activities per visit. This study shows that intranet seeking behaviour differs from what is known about public web searching. The main contribution is the baseline for more targeted intranet studies that this study provides.	baseline (configuration management);information seeking behavior;intranet;web 2.0;web search engine	Dick Stenmark	2005			intranet;computer science;world wide web;business hours;search engine;database search engine;information retrieval;information seeking	Web+IR	-33.77745917018345	-53.311312136974934	19983
30dabf3de56667448bb14f07db6d847f68684dfa	an application of plausible reasoning to information retrieval	bepress selected works;information retrieval;plausible inferences	Two versions of the extended plausible reasoning system were implemented, one using dominance weights (described in the paper) and the other using &id’ (Term Frequency Inverse Document Frequency) weights. Experiments were conducted using the titles and abstracts of the CACM collection and it was found that both versions of the extended plausible reasoning system are better than the vector space model and the system using dominance weights performed better than the system with ~. id~weights.	dominance drawing;information retrieval;reasoning system;tf–idf	Farhad Oroumchian;Robert N. Oddy	1996		10.1145/243199.243271	cognitive models of information retrieval;computer science;information retrieval	AI	-27.43328906176141	-66.86687009221306	19996
63985a4be09065dcf51400eea42146203261dde4	detecting cues to deception from children's facial expressions: on the effectiveness of two visual manipulation techniques		This paper looks into the extent to which facial expressions may reveal whether a person is telling the truth or not. More specifically, it studies selected clips from video recordings of children who had participated in an interactive story paradigm that naturally elicits minimal pairs of truthful and deceptive utterances in participants. In two perception experiments, these pairs of clips (with audio removed) were shown to adult observers who were given the task to guess which member of the pair contained the deceptive utterance. Experiment 1 tested whether the likelihood of correct detection could be enhanced when clips were slowed down compared to clips at normal speed. Results revealed that this manipulation indeed had a positive effect on lie detection, albeit that the effect interacted with the order of presentation (lies are easier to see when they are shown after rather than before the truthful utterance) and kind of lie (second attempts of children to lie reveal more cues than their first attempts). Experiment 2 explored whether lie detection is different for recordings in which the full face of a child is shown, or for recordings in which either the eye or mouth region is hidden after digital manipulation. This experiment revealed that the partial presentations of the face lead to more correct deception detection than the full face presentation. Implications of the outcomes of Experiment 1 and 2 for lie detection and for a general model of nonverbal communication are discussed. & 2013 Elsevier Ltd. All rights reserved.	clips;experiment;interactive storytelling;programming paradigm;sensor	Marc Swerts;Anniek van Doorenmalen;Lynn Verhoofstad	2013	J. Phonetics	10.1016/j.wocn.2013.07.003	psychology;computer vision;communication;social psychology	AI	-50.85948453885004	-52.133487047334945	20009
df391950a6b6e9cfcb96ffe38693af95d1fe7ac0	utilizing implicit user feedback to improve interactive video retrieval	interactive video;user feedback	This paper describes an approach to exploit the implicit user feedback gathered during interactive video retrieval tasks. We propose a framework, where the video is first indexed according to temporal, textual, and visual features and then implicit user feedback analysis is realized using a graph-based methodology. The generated graph encodes the semantic relations between video segments based on past user interaction and is subsequently used to generate recommendations. Moreover, we combine the visual features and implicit feedback information by training a support vector machine classifier with examples generated from the aforementioned graph in order to optimize the query by visual example search. The proposed framework is evaluated by conducting real-user experiments. The results demonstrate that significant improvement in terms of precision and recall is reported after the exploitation of implicit user feedback, while an improved ranking is presented in most of the evaluated queries by visual example.	algorithm;experiment;feedback;graph (discrete mathematics);online and offline;precision and recall;real-time operating system;scalability;scale-invariant feature transform;support vector machine;video content analysis	Stefanos Vrochidis;Yiannis Kompatsiaris;Ioannis Patras	2011	Adv. in MM	10.1155/2011/310762	computer science;theoretical computer science;database;multimedia;world wide web;information retrieval	Web+IR	-16.600947699053304	-54.198540842244086	20023
3ab036b680e8408ec74f78a918f3ffbf6c906d70	saying what you're looking for: linguistics meets video search	event recognition;detectors;retrieval;single word query video corpora search natural language query compositional semantics sentential query natural language parser video clip object tracking object detection;semantics;video signal processing image retrieval natural language processing object detection object tracking search engines;sentential video retrieval retrieval video language tracking object detection event recognition;detectors semantics object detection feature extraction target tracking yttrium;yttrium;feature extraction;sentential video retrieval;language;video;target tracking;tracking;object detection	We present an approach to searching large video corpora for clips which depict a natural-language query in the form of a sentence. Compositional semantics is used to encode subtle meaning differences lost in other approaches, such as the difference between two sentences which have identical words but entirely different meaning: The person rode the horse versus The horse rode the person. Given a sentential query and a natural-language parser, we produce a score indicating how well a video clip depicts that sentence for each clip in a corpus and return a ranked list of clips. Two fundamental problems are addressed simultaneously: detecting and tracking objects, and recognizing whether those tracks depict the query. Because both tracking and object detection are unreliable, our approach uses the sentential query to focus the tracker on the relevant participants and ensures that the resulting tracks are described by the sentential query. While most earlier work was limited to single-word queries which correspond to either verbs or nouns, we search for complex queries which contain multiple phrases, such as prepositional phrases, and modifiers, such as adverbs. We demonstrate this approach by searching for 2,627 naturally elicited sentential queries in 10 Hollywood movies.	body of uterus;encode;hollywood;lexicon;linguistics;movies;natural language;object detection;parser;phrases;physical object;public queries study contact;question (inquiry);sensor;text corpus;track (course);video clip;sentence	Daniel Paul Barrett;Andrei Barbu;N. Siddharth;Jeffrey Mark Siskind	2016	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2015.2505297	natural language processing;detector;video;feature extraction;computer science;yttrium;semantics;tracking;language;information retrieval	Vision	-17.216692165641692	-56.47281329894004	20040
32a8aff6652006d8a569f08ff8b8061726e1ee54	spoken term detection albayzin 2014 evaluation: overview, systems, results, and discussion	signal image and speech processing;acoustics;mathematics in music;engineering acoustics;article	Spoken term detection (STD) aims at retrieving data from a speech repository given a textual representation of the search term. Nowadays, it is receiving much interest due to the large volume of multimedia information. STD differs from automatic speech recognition (ASR) in that ASR is interested in all the terms/words that appear in the speech data, whereas STD focuses on a selected list of search terms that must be detected within the speech data. This paper presents the systems submitted to the STD ALBAYZIN 2014 evaluation, held as a part of the ALBAYZIN 2014 evaluation campaign within the context of the IberSPEECH 2014 conference. This is the first STD evaluation that deals with Spanish language. The evaluation consists of retrieving the speech files that contain the search terms, indicating their start and end times within the appropriate speech file, along with a score value that reflects the confidence given to the detection of the search term. The evaluation is conducted on a Spanish spontaneous speech database, which comprises a set of talks from workshops and amounts to about 7 h of speech. We present the database, the evaluation metrics, the systems submitted to the evaluation, the results, and a detailed discussion. Four different research groups took part in the evaluation. Evaluation results show reasonable performance for moderate out-of-vocabulary term rate. This paper compares the systems submitted to the evaluation and makes a deep analysis based on some search term properties (term length, in-vocabulary/out-of-vocabulary terms, single-word/multi-word terms, and in-language/foreign terms).		Javier Tejedor;Doroteo Torre Toledano;Paula Lopez-Otero;Laura Docío Fernández;Carmen García-Mateo;Antonio Cardenal López;Julián D. Echeverry-Correa;Alejandro Coucheiro-Limeres;Julia Olcoz;Antonio Miguel	2015	EURASIP J. Audio, Speech and Music Processing	10.1186/s13636-015-0063-8	natural language processing;speech recognition;acoustics;speech corpus;computer science;physics	NLP	-23.443901696238505	-83.49218632295924	20047
340f9b0f1355f2c5932dc99747ea8e8a5a55d617	word re-embedding via manifold dimensionality retention		Word embeddings seek to recover a Euclidean metric space by mapping words into vectors, starting from words cooccurrences in a corpus. Word embeddings may underestimate the similarity between nearby words, and overestimate it between distant words in the Euclidean metric space. In this paper, we re-embed pre-trained word embeddings with a stage of manifold learning which retains dimensionality. We show that this approach is theoretically founded in the metric recovery paradigm, and empirically show that it can improve on state-of-the-art embeddings in word similarity tasks 0.5 − 5.0% points depending on the original space.	algorithm;euclidean distance;experiment;lero (software engineering);microsoft word for mac;natural language processing;nonlinear dimensionality reduction;programming paradigm;text corpus;word embedding	Souleiman Hasan;Edward Curry	2017			manifold;artificial intelligence;machine learning;computer science;curse of dimensionality;embedding	NLP	-19.595371852508197	-74.41070307329171	20050
cd8277356f6ad24c005b5673cdf42d15e179f57d	verification of speech recognition results incorporating in-domain confidence and discourse coherence measures	verification;evaluation performance;tecnologia electronica telecomunicaciones;confiance;performance evaluation;confidence measure;evaluacion prestacion;confidence;automatic recognition;reconocimiento voz;confianza;discourse coherence;speech recognition;reconnaissance parole;verificacion;tecnologias;grupo a;utterance verification;reconocimiento automatico;reconnaissance automatique;in domain confidence	Conventional confidence measures for assessing the reliability of ASR (automatic speech recognition) output are typically derived from “low-level” information which is obtained during speech recognition decoding. In contrast to these approaches, we propose a novel utterance verification framework which incorporates “high-level” knowledge sources. Specifically, we investigate two measures: in-domain confidence, the degree of match between the input utterance and the application domain of the back-end system, and discourse coherence, the consistency between consecutive utterances in a dialogue session. A joint confidence score is generated by combining these two measures with an orthodox measure based on GPP (generalized posterior probability). The proposed framework was evaluated on an utterance verification task for spontaneous dialogue performed via a (English/Japanese) speech-to-speech translation system. Incorporating the two proposed measures significantly improved utterance verification accuracy compared to using GPP alone, realizing reductions in CER (confidence error-rate) of 11.4% and 8.1% for the English and Japanese sides, respectively. When negligible ASR errors (that do not significantly affect translation) were ignored, further improvement was achieved for the English side, realizing a reduction in CER of up to 14.6% compared to the GPP case. key words: speech recognition, confidence measure, utterance verification, in-domain confidence, discourse coherence	application domain;automated system recovery;end system;graph partition;high- and low-level;machine translation;speech recognition;spontaneous order	Ian R. Lane;Tatsuya Kawahara	2006	IEICE Transactions	10.1093/ietisy/e89-d.3.931	verification;speech recognition;computer science;artificial intelligence;confidence	NLP	-20.637747787054074	-89.99655819317802	20052
99fba75331afc210f4840faf87c5a24cb7a2cd18	reconexp: a way to reduce the data loss of the experiencing sampling method	distributed application;mobile device;young children;awareness systems;day reconstruction method;family communication;data quality;sampling methods;field study;experience sampling method	This paper presents Reconexp, a diary method supported by a distributed application, which partly runs on a mobile device and partly on a website, enabling us to survey user attitudes, experiences and requirements in field studies. Reconexp combines aspects of the Experience Sampling Method and the Day Reconstruction Method aiming to reduce data loss, improve data quality and reduce burden put upon participants. We discuss our first experiences of using this method in the context of a study of communication needs of working parents with young children.	data quality;distributed computing;experience;gibbs sampling;mobile device;requirement;sampling (signal processing);zero-day (computing)	Vassilis-Javed Khan;Panos Markopoulos;Berry Eggen;Wijnand A. IJsselsteijn;Boris E. R. de Ruyter	2008		10.1145/1409240.1409316	sampling;real-time computing;simulation;data quality;computer science;operating system;data mining;mobile device;experience sampling method;field research	HCI	-62.59334416478635	-53.8375507346293	20056
46bc63f12576f6d06c42936f2611f6c5297185c9	hardware-based support vector machine for phoneme classification	discrete wavelet transforms;kernel;dwt;multispeaker phoneme recognition;speaker independent field programmable gate arrays support vector machines phoneme recognition;support vector machines;fpga;speaker recognition;radial basis function networks;computer architecture;accuracy;radial basis function;speaker independent;svm method;phoneme classification;feature extraction hardware based support vector machine phoneme classification digital hardware implementation multispeaker phoneme recognition one against one multiclass method svm method radial basis function xilinx virtex ii xc2v3000 fpga timit corpus phoneme recognition system discrete wavelet transforms dwt;feature extraction;speech recognition equipment;speech recognition;phoneme recognition;support vector machines hardware computer architecture kernel speech recognition accuracy;xilinx virtex ii xc2v3000;field programmable gate arrays;timit corpus;digital hardware implementation;hardware based support vector machine;one against one multiclass method;support vector machines discrete wavelet transforms electronic design automation feature extraction field programmable gate arrays radial basis function networks speaker recognition speech recognition equipment;hardware;phoneme recognition system;electronic design automation	This paper presents the design of a digital hardware implementation based on Support Vector Machines (SVMs), for the task of multi-speaker phoneme recognition. The One-against-one multiclass SVM method, with the Radial Basis Function (RBF) kernel was considered. Furthermore, a priority scheme was also included in the architecture, in order to forecast the three most likely phonemes. The designed system was synthesised on a Xilinx Virtex-II XC2V3000 FPGA, and evaluated with the TIMIT corpus. This phoneme recognition system is intended to be implemented on a dedicated chip, along with the Discrete Wavelet Transforms (DWTs) for feature extraction, to further improve the resultant performance.	cmos;dictionary;digital electronics;feature extraction;field-programmable gate array;natural language processing;radial (radio);radial basis function kernel;resultant;support vector machine;timit;virtex (fpga);wavelet transform	Michelle Cutajar;Edward Gatt;Ivan Grech;Owen Casha;Joseph Micallef	2013	Eurocon 2013	10.1109/EUROCON.2013.6625206	speaker recognition;speech recognition;electronic design automation;computer science;machine learning;pattern recognition;field-programmable gate array	ML	-8.460615122597956	-96.72444489027157	20057
3e27529f55fa966201613b2d8b56b1934ce6af54	natural language processing with small feed-forward networks		We show that small and shallow feedforward neural networks can achieve near state-of-the-art results on a range of unstructured and structured language processing tasks while being considerably cheaper in memory and computational requirements than deep recurrent models. Motivated by resource-constrained environments like mobile phones, we showcase simple techniques for obtaining such small neural network models, and investigate different tradeoffs when deciding how to allocate a small memory budget.	artificial neural network;baseline (configuration management);computation;feedforward neural network;mobile phone;natural language processing;requirement	Jan A. Botha;Emily Pitler;Ji Ma;Anton Bakalov;Alex Salcianu;David I Weiss;Ryan T. McDonald;Slav Petrov	2017			machine learning;natural language processing;artificial intelligence;computer science;feed forward;artificial neural network	NLP	-15.811183654321283	-76.13081945346518	20092
14ba55fa7bbc80d3e5415165128d80eef2c9cd5f	dynamic cascades with bidirectional bootstrapping for spontaneous facial action unit detection	support vector machines face recognition;detectors;support vector machines;negative samples;training;expression recognition;data mining;facial expression analysis;gold;face recognition;shape;support vector machines dynamic cascades bidirectional bootstrapping spontaneous facial action unit detection facial expression analysis positive samples negative samples expression recognition each action unit;face detection gold support vector machines support vector machine classification psychology facial muscles face recognition robots training data databases;bidirectional bootstrapping;action unit;spontaneous facial action unit detection;face;positive samples;experimental evaluation;support vector machine;dynamic cascades;encoding;each action unit	A relatively unexplored problem in facial expression analysis is how to select the positive and negative samples with which to train classifiers for expression recognition. Typically, for each action unit (AU) or other expression, the peak frames are selected as positive class and the negative samples are selected from other AUs. This approach suffers from at least two drawbacks. One, because many state of the art classifiers, such as Support Vector Machines (SVMs), fail to scale well with increases in the number of training samples (e.g. for the worse case in SVM), it may be infeasible to use all potential training data. Two, it often is unclear how best to choose the positive and negative samples. If we only label the peaks as positive samples, a large imbalance will result between positive and negative samples, especially for infrequent AU. On the other hand, if all frames from onset to offset are labeled as positive, many may differ minimally or not at all from the negative class. Frames near onsets and offsets often differ little from those that precede them. In this paper, we propose Dynamic Cascades with Bidirectional Bootstrapping (DCBB) to address these issues. DCBB optimally selects positive and negative class samples in training sets. In experimental evaluations in non-posed video from the RU-FACS Database, DCBB yielded improved performance for action unit recognition relative to alternative approaches.	adaboost;bcs-facs;daisy digital talking book;jumbo frame;offset (computer science);onset (audio);randomness;sampling (signal processing);scale-invariant feature transform;spontaneous order;support vector machine;test set	Yunfeng Zhu;Fernando De la Torre;Jeffrey F. Cohn;Yu-Jin Zhang	2009	2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops	10.1109/ACII.2009.5349603	psychology;facial recognition system;support vector machine;speech recognition;computer science;machine learning;pattern recognition;geometry;communication	Vision	-7.614531449018763	-95.57903881419942	20093
ce1570d741060e346a9f25ce829ff63ea897ff5b	health related quality of life in a sample of older people who are members of open care centers for the elderly		The consequences of demographic ageing, demographic and social-economic changes, changes in the structure of family, in the shrinkage of Welfare state, influenced the lifestyle of the older people and their social relations. The institution of Open Care Centers for the Elderly (KAPI) was created in the context of these conditions. This cross-sectional study examined the lifestyle of the older people who are member of KAPI and their association with all HRQoL dimensions. The results of this research showed that the main reason for the participants to become members of the KAPI was entertainment and/or companionship (98.4%). Additionally subjects who suffered from a chronic disease had significantly lower scores on all HRQoL dimensions, while participants who were members in the activity groups of KAPI had significantly higher scores. The elderly within the KAPI have many opportunities to meet with other persons of their age, and develop various activities and interests.		Paraskevi Ponirou;Marianna Diomidous;Athina Kalokairinou;John Mantas;Chariklia Tsimahidou;Chara Tzavara	2014	Studies in health technology and informatics	10.3233/978-1-61499-423-7-269	social support;social medicine;ageing;disease;interpersonal relationship;social relation;quality of life;nursing;medicine;welfare state	HCI	-58.90341043804569	-56.18689051076272	20096
40908f3e3b309eb21700a50e79340e0ea8e708c3	zipf's law, hyperbolic distributions and entropy loss	computational linguistics;entropy;information theory;natural languages;zipf's law;entropy loss;expressive powers;flexibility;hyperbolic distributions;information theoretical characterization;stability;word frequency;expressive power;natural language	Zipf’s law is an empirical observation which relates rank and frequency of words in natural languages. The law suggests modelling by distributions of “hyperbolic type” . We present a general definition and an information theoretical characterization of such distributions. This leads to a property of stability and flexibility, explaining that a language can develop towards higher and higher expressive powers without changing its basic structure.	natural language;zipf's law	Peter Harremoës;Flemming Topsøe	2006		10.1007/11889342_50	natural language processing;information theory;computer science;computational linguistics;mathematics;natural language	ML	-35.79870265050547	-81.78327129527031	20112
ca0d17b440ddc7f2b194740599e368379b5bd3a4	from literature to knowledge: exploiting pubmed to answer biomedical questions in natural language		Researchers, practitioners and the general public strive to be constantly upi¾?to date with the latest developments in the subjects of bio-medical research of their interest. Meanwhile the collection of high quality research papers freely available on the Web has increase dramatically in the last few years and this trend is likely to continue. This state of facts brings about opportunities as well as challenges for the construction of effective web-based searching tools. Question/Answering systems based on user interactions in Natural Language have emerged as a promising alternative to traditional keyword based search engines. However this technology still needs to mature in order to fulfill its promises. In this paper we present and test a new graph-based proof-of-concept paradigm for processing the knowledge base and the user queries expressed in natural Language. The user query is mapped as a subgraph matching problem onto the internal graph representation, and thus can handle efficiently also partial matches. Preliminary user-based output quality measurements confirm the viability of our method.	natural language;pubmed	Pinaki Bhaskar;Marina Buzzi;Filippo Geraci;Marco O O Pellegrini	2015		10.1007/978-3-319-22741-2_1	computer science;data mining;world wide web;information retrieval	NLP	-32.399456758931514	-56.541025640699196	20141
c35f8e714a490613cd23b1afad12a7540e03a83c	the constrato model of handover: a tool to support technology design and evaluation	healthcare;za4050 electronic information resources;data collection;design evaluation;handover;design;evaluation;r medicine general	Handovers are a specific kind of multidisciplinary team meeting. Shift handovers and transfers are both regular features of hospital work but there is currently great variation in how such handovers are conducted, presenting a challenging for those seeking to develop technology to support handover. This paper presents the ConStratO model of handover, which captures aspects of the context that influence how the handover is conducted, a range of different handover strategies relating to different aspects of the handover, and possible outcomes of handover. The model is based on detailed data collection in a range of clinical settings. We present the model as a tool for developing and evaluating technology support for handover.	expectation propagation;shift jis	Rebecca Randell;Stephanie M. Wilson;Peter Woodward;Julia Galliers	2011	Behaviour & IT	10.1080/0144929X.2010.547220	design;real-time computing;simulation;computer science;handover;evaluation;management;statistics;computer network;data collection	HCI	-60.51463858199931	-60.891174214821056	20165
18be1af2cc54da5038a9ec4bf485bf11bbd9ffa1	nexing corpus: a corpus of verbal protocols on syllogistic reasoning		In this paper, we describe the Nexing Corpus and report on the tools implemented and the tasks undertaken for its development. The Nexing Corpus includes (i) a collection of written transcriptions of verbal data elicited during a psycholinguistic experiment on syllogistic reasoning; and (ii) performance data concerning that experiment, such as latencies, confidence levels and accuracy of answers provided. The verbal productions recorded in the corpus are of a specific linguistic type that is seldom, if at all, represented in corpora. These data are relevant for the development of human language technologies aimed at modeling this type of linguistic behavior, which is not uncommon in evolved interactions of cooperative agents. This corpus with thinking aloud data on syllogistic reasoning is also an important source of material for cognitive science, in particular for research on the nature of human deductive reasoning.	cognitive science;interaction;language technology;speech corpus;text corpus	António Branco;José Leitão;João Ricardo Silva;Luís Gomes	2002			deductive reasoning;artificial intelligence;natural language processing;syllogism;computer science;transcription (linguistics);think aloud protocol	NLP	-32.91553119077706	-79.86893831251552	20180
084e4ee4b222d700b5f16426c883052576ce6402	(1) obstacles and options for big-data applications in biomedicine: the role of standards and normalizations	health care bioinformatics biomedical engineering data handling;healthcare related data big data applications biomedicine large scale data integration large scale data inferencing high throughput clinical analytics predictive models;biomedical engineering;informatics educational institutions standards medical services bioinformatics genomics training;data handling;bioinformatics;health care	"""Advances in computing capabilities are palpably evident throughout many industries manifest by unprecedented, large-scale data integration and inferencing. Branded as """"big-data"""" in many cases, the question of whether such techniques can leverage advances in biomedicine and clinical practice are obvious. High-throughput clinical analytics, synthesizing genomic and clinical attributes of a particular patient, portends predictive models that can directly influence clinical care decisions. However, to make this widely shared vision practical and scalable, barriers attributable to data heterogeneity dominate. Methods and strategies to increase the comparability and consistency of healthcare related data will be discussed."""	big data;predictive modelling;scalability;throughput	Christopher G. Chute	2012	2012 IEEE International Conference on Bioinformatics and Biomedicine	10.1109/BIBM.2012.6392651	computer science;bioinformatics;data science;machine learning;group method of data handling;data mining;health care	DB	-54.75885022440384	-60.446568423731954	20192
32df2669ab4651a10a5b4b1796f3c8affd36249e	mt-equal: a toolkit for human assessment of machine translation output		MT-EQuAl (Machine Translation Errors, Quality, Alignment) is a toolkit for human assessment of Machine Translation (MT) output. MT-EQuAl implements three different tasks in an integrated environment: annotation of translation errors, translation quality rating (e.g. adequacy and fluency, relative ranking of alternative translations), and word alignment. The toolkit is webbased and multi-user, allowing large scale and remotely managed manual annotation projects. It incorporates a number of project management functions and sophisticated progress monitoring capabilities. The implemented evaluation tasks are configurable and can be adapted to several specific annotation needs. The toolkit is open source and released under Apache 2.0 license.	bitext word alignment;comparison of command shells;machine translation;multi-user;open-source software	Christian Girardi;Luisa Bentivogli;Mohammad Amin Farajian;Marcello Federico	2014			computer-assisted translation;computer science;data mining;database;world wide web	NLP	-34.393270468280114	-73.24192153154144	20246
920f01fdb11af0c5da2d09cfca04a6e968988ef3	the bible and multilingual optical character recognition	optical character recognition;language technology	The Bible---an unlikely resource for language technology research---proves ideal for evaluating OCR techniques.	language technology;optical character recognition	Tapas Kanungo;Philip Resnik;Song Mao;Doe-Wan Kim;Qigong Zheng	2005	Commun. ACM	10.1145/1064830.1064837	natural language processing;speech recognition;intelligent character recognition;computer science;optical character recognition	Logic	-24.970969725453457	-82.03578995977992	20261
4f012a3bfb97f31e9f31e57ab2165cecce7371e8	enhancing brazilian portuguese textual entailment recognition with a hybrid approach		Corresponding Author: Allan de Barcelos Silva Applied Computing Graduate Program Program, Universidade do Vale do Rio dos Sinos UNISINOS, São Leopoldo, Brazil Email: albarsil@gmail.com Abstract: Previous work on textual entailment has not fully exploited aspects of deep linguistic relations, which have been shown as containing important information for entailment identification. In this study, we present a new method to compute semantic textual similarity between two sentences. Our proposal relies on the integration of a set of deep linguistic relations, lexical aspects and distributed representational resources. We used our method with a large set of annotated data available from the ASSIN Workshop in the PROPOR 2016 event. The achieved results score among the best-known results in the literature. A perceived advantage of our approach is the ability to generate good results even with a small corpus on training tasks.		Allan de Barcelos Silva;Sandro José Rigo	2018	JCS	10.3844/jcssp.2018.945.956	artificial intelligence;machine learning;textual entailment;computational linguistics;computer science;brazilian portuguese;logical consequence	NLP	-26.57468328616702	-72.96287946922153	20267
074a19e4ef11fa8ae427c454b19f7eb5a0ebdb44	towards robust phoneme classification: augmentation of plp models with acoustic waveforms	time invariant classification robust phoneme classification plp model augmentation perceptual linear prediction phoneme segments generative classifiers speech representations white gaussian noise acoustic waveform classifiers convex combination log likelihood combined decision function additive noise noise modelling;accuracy acoustics signal to noise ratio training robustness testing;speech recognition acoustic signal processing gaussian noise prediction theory signal classification	The robustness of classification of phoneme segments using generative classifiers is investigated for the PLP and acoustic waveform speech representations in the presence of white Gaussian noise. We combine the strengths of both representations, specifically the excellent classification accuracy of PLP in quiet conditions with the additional robustness of acoustic waveform classifiers. This is achieved using a convex combination of their respective log-likelihoods to produce a combined decision function. The resulting combined classifier is uniformly as accurate as PLP alone and is significantly more robust to the presence of additive noise during testing. Issues of noise modelling and time-invariant classification of acoustic waveforms are also considered with initial solutions used to improve accuracy.	acoustic cryptanalysis;additive white gaussian noise;consistency model;database;experiment;generative model;linear classifier;pl/p;signal-to-noise ratio;statistical classification;time-invariant system;utility functions on indivisible goods;waveform;waveform graphics;while	Matthew Ager;Zoran Cvetkovic;Peter Sollich;Bin Yu	2008	2008 16th European Signal Processing Conference		speech recognition;computer science;machine learning;pattern recognition	ML	-13.29736457810487	-91.42521012770166	20269
486208a2d069284878f111c8cdd77e8637f40681	web service discovery based on keyword clustering and ontology	graph theory;topology;service request;pattern clustering;web service discovery;wsdl language;weighted bipartite graph web service discovery keyword clustering ontology web service technology wsdl language concept expansion service request service matching web service content extraction web service content retrieval;information retrieval;web service content extraction;optimal matching;web service;service matching;ontologies artificial intelligence;concept expansion;web service technology;chromium;weighted bipartite graph;cognition;web services;web service content retrieval;ontologies;keyword clustering;service discovery;bipartite graph;web services ontologies chromium cognition bipartite graph optimal matching topology;ontology;web services graph theory information retrieval ontologies artificial intelligence pattern clustering	One of the challenging problem that Web service technology is now facing is effective service discovery. To solve the deficiencies of Web service description, matching and choosing under WSDL language, this paper presents a web service discovery method based on keyword clustering and concept expansion, mainly from the content of Web service, reasoning of service request and service matching, through classification and subsumption of concept, this paper retrieve and extract Web service content. Meanwhile, use weighted bipartite graph to match user request and published Web service, so as to enhance Web service discovery ability.	cluster analysis;keyword extraction;natural language;ontology (information science);service discovery;subsumption architecture;web services description language;web service	Ming Zhou;Tianlei Zhang;Hui Meng;Liping Xiao;Guisheng Chen;Deyi Li	2008	2008 IEEE International Conference on Granular Computing	10.1109/GRC.2008.4664699	web service;web development;web modeling;web mapping;web standards;computer science;graph theory;ws-policy;ontology;social semantic web;database;service discovery;web 2.0;world wide web;information retrieval;universal description discovery and integration;mashup	Web+IR	-28.724545101250087	-57.98699370661637	20271
a97317e53bb6e3a05dedc5fa9f773a57f237460e	on the making of an ubiquitous and altruistic application for medical first responses	routing;smart phones;data mining;medical services cellular phones mobile communication application software proposals bluetooth communication channels hospitals computer science filtering;web application design;medical services;internet;web 2 0 application;altruistic health application;mobile computing emergency services health care internet;communication channels;medical first response;mobile computing;cellular phones;medical diagnostic imaging;ubiquitous health application;emergency services;medical first response ubiquitous health application altruistic health application web 2 0 application;health care	We report and discuss on the basic stages of development of a web application designed to support an ubiquitous and altruistic health application based on the Web 2.0 paradigm. Our starting idea is that several technological tools are mature enough to enable such class of applications. In particular, we present the main features of our system, the most prominent, even if preliminary, result is a new approach to filtering and discovery, to associate the most appropriate doctor to a patient in the case of an emergency.	cidr;computer science;content-control software;programming paradigm;web 2.0;web application;world wide web	Alessandro Amoroso;Marco Roccetti	2009	2009 International Conference on Ultra Modern Telecommunications & Workshops	10.1109/ICUMT.2009.5345633	routing;the internet;computer science;operating system;multimedia;mobile computing;world wide web;computer security;health care;computer network;channel	DB	-57.308243575219244	-58.861830445605925	20275
26797b5a56a17372d4cdeac1021fdffa4e339f6d	structured prediction with reinforcement learning	modelizacion;graph theory;politica optima;tree transformation;teoria grafo;learning algorithm;labeled tree;descomposicion funcion;supervised learning;proceso markov;sequence labeling;availability;disponibilidad;xml language;reinforcement learning;complex structure;decision markov;internal structure;algorithme apprentissage;optimal policy;fonction perte;funcion perdida;classification;theorie graphe;modelisation;large scale;html to xml;apprentissage renforce;ball joint;loss function;decomposition fonction;processus markov;marcacion grafo;estructura datos;structure prediction;markov process;structured prediction;markov decision;structure donnee;sequence prediction;estructura interna;apprentissage supervise;markov decision process;articulacion de rotulas;marquage graphe;aprendizaje reforzado;politique optimale;aprendizaje supervisado;algoritmo aprendizaje;modeling;disponibilite;data structure;structure interne;clasificacion;langage html;langage xml;lenguaje xml;graph labelling;html language;lenguaje html;function decomposition;liaison rotule	We formalize the problem of Structured Prediction as a Reinforcement Learning task. We first define a Structured Prediction Markov Decision Process (SP-MDP), an instantiation of Markov Decision Processes for Structured Prediction and show that learning an optimal policy for this SP-MDP is equivalent to minimizing the empirical loss. This link between the supervised learning formulation of structured prediction and reinforcement learning (RL) allows us to use approximate RL methods for learning the policy. The proposed model makes weak assumptions both on the nature of the Structured Prediction problem and on the supervision process. It does not make any assumption on the decomposition of loss functions, on data encoding, or on the availability of optimal policies for training. It then allows us to cope with a large range of structured prediction problems. Besides, it scales well and can be used for solving both complex and large-scale real-world problems. We describe two series of experiments. The first one provides an analysis of RL on classical sequence prediction benchmarks and compares our approach with state-of-the-art SP algorithms. The second one introduces a tree transformation problem where most previous models fail. This is a complex instance of the general labeled tree mapping problem. We show that RL exploration is effective and leads to successful results on this challenging task. This is a clear confirmation that RL could be used for large size and complex structured prediction problems.	approximation algorithm;central processing unit;code;data structure;encode;experiment;gene prediction;loss function;markov chain;markov decision process;microsoft word for mac;regular expression;reinforcement learning;sp-devs;sequence labeling;sparse matrix;structured prediction;supervised learning;treemapping;triplet state;universal instantiation	Francis Maes;Ludovic Denoyer;Patrick Gallinari	2009	Machine Learning	10.1007/s10994-009-5140-8	markov decision process;sequence labeling;functional decomposition;availability;xml;systems modeling;html;data structure;biological classification;computer science;artificial intelligence;graph theory;machine learning;generalized complex structure;structured prediction;markov process;supervised learning;reinforcement learning;algorithm;loss function;ball joint	ML	-12.822966963148422	-63.79763480709856	20277
0e3eafb0df7d5c5d326bfe81fa1c0e658a864f35	catching zika fever: application of crowdsourcing and machine learning for tracking health misinformation on twitter		In February 2016, World Health Organization declared the Zika outbreak a Public Health Emergency of International Concern. With developing evidence it can cause birth defects, and the Summer Olympics coming up in the worst affected country, Brazil, the virus caught fire on social media. In this work, we use Zika as a case study in building a tool for tracking the misinformation around health concerns on Twitter. We collect more than 13 million tweets regarding the Zika outbreak and track rumors outlined by the World Health Organization and Snopes fact checking website. The tool pipeline, which incorporates health professionals, crowdsourcing, and machine learning, allows us to capture health-related rumors around the world, as well as clarification campaigns by reputable health organizations. We discover an extremely bursty behavior of rumor-related topics, and show that, once the questionable topic is detected, it is possible to identify rumor-bearing tweets using automated techniques.	crowdsourcing;data science;lazy evaluation;machine learning;social media;usability	Amira Ghenai;Yelena Mejova	2017	2017 IEEE International Conference on Healthcare Informatics (ICHI)	10.1109/ICHI.2017.58	data mining;artificial intelligence;machine learning;computer science;misinformation;internet privacy;public health;outbreak;social media;crowdsourcing	SE	-21.893690171094462	-55.56363353276813	20420
ac82ee0703197a40971703e3d50cb1e848e98e93	voice conversion based on feature combination with limited training data	gaussian mixture models gmm;voice conversion;dynamic kernel partial least square regression dkpls;feature combination	Typically, voice conversion systems just use one type of spectral feature to convert acoustical characteristics of one speaker to another speaker. In this paper, we first study four different spectral features. Then, we compare these features and choose two features that perform better than others. Our experiments showed that cepstral features are more suitable than all-pole features for clustering and all-pole features are better for the analysis/synthesis stages. Hence, we propose a new voice conversion algorithm that uses both cepstral and all-pole features in order to utilize their desired properties simultaneously. We have two ideas to utilize this feature combination strategy. Our first idea is to apply feature combination to classical Gaussian mixture models (GMM)-based voice conversion method. The second idea is to apply feature combination to dynamic kernel partial least square regression (DKPLS) method. Results of our evaluations show that our proposed methods outperform the modern voice conversion methods in terms of speech quality and speaker individuality. Our methods are also robust to limited training data.		Mostafa Ghorbandoost;Abolghasem Sayadiyan;Mohsen Ahangar;Hamid Sheikhzadeh;Abdoreza Sabzi Shahrebabaki;Jamal Amini	2015	Speech Communication	10.1016/j.specom.2014.12.004	speech recognition;computer science;machine learning;pattern recognition	NLP	-16.135056651762078	-92.03849638298604	20429
0b849ff6f1fe8110d854b9ede1bd9dc2c0ef1b79	smoothing methods for a morpho-statistical approach of automatic diacritization arabic texts (méthodes de lissage d'une approche morpho-statistique pour la voyellation automatique des textes arabes) [in french]		We present in this work a new approach for the Automatic diacritization for Arabic texts using three stages. During the first phase, we integrated a lexical database containing the most frequent words of Arabic with morphological analysis by Alkhalil Morpho Sys which provided possible diacritization for each word. The objective of the second module is to eliminate the ambiguity using a statistical approach in which the learning phase was performed on a corpus composed by several Arabic books. This approach uses the hidden Markov models (HMM) with Arabic unvowelized words taken as observed states and vowelized words are considered as hidden states. The system uses smoothing techniques to circumvent the problem of unseen word transitions in the corpus and the Viterbi algorithm to select the optimal solution. The third step uses a HMM model based on the characters to deal with the case of unanalyzed words. Mots-clés : Langue arabe, voyellation automatique, analyse morphologique, modèle de Markov caché, corpus, lissage, algorithme de Viterbi.	book;hidden markov model;lexical database;linear algebra;markov chain;smoothing;text corpus;viterbi algorithm	Amine Chennoufi;Azzeddine Mazroui	2014				NLP	-26.152921419553035	-78.74162394510793	20431
75564b8ed569655ce66046fd921a704e08f04bb9	testing open-source implementations for detection response tasks	tactile;detection response tasks;drt;visual;open source	Open-Source variants of Detection Response Task (DRT) were implemented on an Arduino board and Android smartphone. These systems were tested in an experiment under single task (only the DRT), double task (DRT + driving simulation), and triple task conditions (DRT + driving simulation + 2-back cognitive task). All DRT variants reflected different task set load, nevertheless, the smartphone setup exhibited lower internal correlations and in one condition failed to reach significance.	android;arduino;driving simulator;simulation;smartphone	Michael Krause;Antonia S. Conti;Moritz Späth;Klaus Bengler	2014		10.1145/2662253.2662313	embedded system;real-time computing;visual system;computer hardware;computer science	Embedded	-46.62841819986418	-52.375041009151396	20449
56c37db2b324359911a5c53400844cfdf8e543b9	database researchers: plumbers or thinkers?	text tagging;query processing;information extraction;real time;knowledge management;large scale;ai applications;transaction data;digital content;natural language;indexation;disambiguation;robustness;semantic search;scalability;machine reading;named entity;question answering;knowledge base	"""DB researchers have traditionally focused on engine-centered issues such as indexing, query processing, and transactions. Data mining has broadened the community's viewpoint towards algorithmic and statistical issues. However, DB research has always had a tendency to shy away from seemingly elusive long-term challenges with AI flavor. On the other hand, the current explosion of digital content in enterprises and the Internet, is mostly caused by user-created information like text, tags, photos, videos, and not by seeing more well-designed databases of the traditional kind.  In this situation, I question the traditional skepticism of DB researchers towards """"AI-complete"""" problems and the DB community's reluctance to embark on seemingly non-DB-ish grand challenges. Big questions that I see as great opportunities also for DB research include: 1) automatic extraction of relational facts from natural-language text and multimodal contexts [4, 6, 21], 2) automatic disambiguation of named-entity mentions and general phrases in text and speech [10, 11], 3) large-scale gathering of factual-knowledge candidates and their reconciliation into comprehensive knowledge bases [1, 2, 8, 13, 19], 4) reasoning on uncertain hypotheses, for knowledge discovery and semantic search [9, 14, 16, 17, 20], 5) deep and real-time question answering, e.g., to enable computers to win quiz game shows [7], 6) machine-reading of scientific publications and fictional literature, to enable corpus-wide analyses and enable researchers in science and humanities to develop hypotheses and quickly focus on the most relevant issues [3, 5].  I believe that successfully tackling these topics requires efficient data-centric algorithms, scalable methods and architectures, and system-level thinking - virtues that are richly available in the DB research community. Moreover, I would encourage our community to look across the fence and get more engaged on the exciting challenges outside the traditionally narrow boundaries of the DB realm. I will illustrate these points by examples from my own research on knowledge management [12, 15, 18, 19]. Breakthroughs will require long-term stamina. In the meantime, steady incremental progress is better than not embarking on these important problems at all."""	ai-complete;algorithm;computer;data mining;database;digital recording;grand challenges;internet;knowledge management;multimodal interaction;natural language;question answering;real-time web;scalability;scientific literature;semantic search;tag (metadata);word-sense disambiguation	Gerhard Weikum	2011		10.1145/1951365.1951368	applications of artificial intelligence;knowledge base;scalability;question answering;semantic search;computer science;data science;transaction data;data mining;database;natural language;information extraction;information retrieval;robustness	DB	-40.02973300160887	-61.54546633797923	20458
aaaac057c8ed1c5afe7001f3c05967b57401f0aa	complexity equals change	computer science and information systems;pattern;complexity;change;entropy;structure	Traditionally, models of complexity used in psychology have been based on probabilistic and algorithmic paradigms. While these models have inspired a great deal of research, they are generally opaque about the relationship between complexity and the cost of information processing. We argue that the psychological complexity is easily defined and quantified in terms of change and support this argument with a measure of complexity for binary patterns. We extend our measure to 2-D binary arrays, and show that it correlates well with a number of existing complexity and randomness measures, both subjective and objective. We suggest that measuring change represents an intuitively and mathematically transparent way of defining and quantifying psychological complexity which provides the missing link between subjective and objective approaches to complexity. 2011 Elsevier B.V. All rights reserved.	binary number;blum axioms;complexity;computer;hierarchical editing language for macromolecules;information processing;list of code lyoko episodes;programming paradigm;randomness;royer oscillator	Aleksandar Aksentijevic;Keith Gibson	2012	Cognitive Systems Research	10.1016/j.cogsys.2011.01.002	structure;entropy;complexity;decision tree model;artificial intelligence;theoretical computer science;machine learning;worst-case complexity;communication complexity;complexity index;pattern;descriptive complexity theory	AI	-5.4499892486259505	-78.33728655974022	20464
503b962db7de3217613bea3f5c4260d61f866215	smart medication management system and multiple interventions for medication adherence	health monitoring;modeling and performance;interventions;smart systems;medication adherence	To keep healthcare costs under control, a high-level of medication adherence, or compliance with medication regimen, must be achieved. The multifaceted nature of medication adherence, due to a large number of underlying factors, presents several critical challenges including how to evaluate the current level of adherence, how to improve and how to maintain the required level of medication adherence, especially for long-term chronic conditions. Several interventions to improve adherence have been proposed in the healthcare literature, however these are complex, costly and difficult to implement. It is also not clear which ones would be effective at what levels of adherence in what conditions and types of patients. Therefore, there is a need to model, evaluate and compare the interventions individually as well as in combinations for their impact on medication adherence. To address this, the design and evaluation of smart medication management system (SMMS) for improving medication adherence are presented in this paper. We also present an analytical model for evaluating the medication adherence using multiple interventions that are supported from SMMS, namely context-aware reminders, improved scheduling of medications, and support from healthcare professionals. The performance results show that very high medication adherence is achievable by SMMS for single and multiple medications even for patients with mild cognitive deficiency. Several powerful ''composite'' interventions are also proposed and evaluated for medication adherence. It is also shown that the total healthcare savings are significant even for slightly improved medication adherence. With higher hospitalization cost, the savings due to improved medication adherence become even more significant. The proposed work forms the basis to design personalized interventions to patients for improving medication adherence in different surroundings.		Upkar Varshney	2013	Decision Support Systems	10.1016/j.dss.2012.10.011	smart system;data mining;psychological intervention	HCI	-61.76383334269073	-61.499087723106065	20465
db77c0fc80cbf738ed072b507c9ea3192a81b712	an analysis of learned proximity functions	information retrieval;proximity;learning to rank	A lot of recent work has shown that the proximity of terms can be exploited to improve the performance of information retrieval systems. We review a recent approach that uses an intuitive framework to incorporate proximity functions into vector based information retrieval systems. More importantly, we present several proximity functions that were learned within this framework and show that they adhere to previously developed constraints regarding the shape of a good proximity function. Finally, we include results of all of the learned functions on unseen test data that shows the consistency of the learning approach used.	information retrieval;test data	Ronan Cummins;Colm O'Riordan;Mounia Lalmas	2010			computer science;machine learning;data mining;information retrieval	ML	-19.46308205651702	-62.663132591324164	20471
9de94d531417f6179733c38ccb0d936f6311bbed	context generalization for information extraction from the web	backend system;information extraction;application software;e commerce;catalogs;web service;data mining;machine learning;intelligent agent;telecom corporation;humans;induction generators;buildings;labeling;data mining application software catalogs humans buildings machine learning induction generators labeling	Many online data sources, such as product catalogs, on-line directories, etc. are available on the web. Extracting information from such sources is a hard task since these sources are designed to be presented to human users. Many researchers have tackled the problem of building wrappers for such sources. The state of the art approach is to use machine learning techniques based on fully labeled example pages. In this paper we propose and study an approach based on example instances. This allows the user to build a wrapper using only a handful of examples of the whole source allowing to take into account structural differences. The patterns obtained allow to extract the instances of the relation described by the examples and contained in the same data source.	information extraction;machine learning;online and offline;world wide web;wrapper library	Benjamin Habegger;Mohamed Quafafou	2004	IEEE/WIC/ACM International Conference on Web Intelligence (WI'04)	10.1109/WI.2004.48	web service;induction generator;labeling theory;application software;computer science;artificial intelligence;machine learning;data mining;database;world wide web;intelligent agent;information extraction	DB	-24.367957440748295	-62.986259283405964	20486
0db09381c6fe288f5512161801d4d18460e875f9	statistical parametric speech synthesis based on speaker and language factorization	language use;interpolation;decision tree;speech synthesis;hidden markov model;hidden markov models transforms decision trees speech speech synthesis adaptation models vectors;hidden markov models;maximum likelihood linear regression;matrix decomposition;statistical parametric speech synthesis hidden markov models hmms speaker and language factorization;speech recognition;regression analysis;decision trees;speech synthesis decision trees hidden markov models interpolation matrix decomposition regression analysis speech recognition;maximum likelihood linear regression statistical parametric speech synthesis language factorization speaker factorization speech recognition systems hidden markov models cluster dependent decision trees cluster mean interpolation	An increasingly common scenario in building speech synthesis and recognition systems is training on inhomogeneous data. This paper proposes a new framework for estimating hidden Markov models on data containing both multiple speakers and multiple languages. The proposed framework, speaker and language factorization, attempts to factorize speaker-/language-specific characteristics in the data and then model them using separate transforms. Language-specific factors in the data are represented by transforms based on cluster mean interpolation with cluster-dependent decision trees. Acoustic variations caused by speaker characteristics are handled by transforms based on constrained maximum-likelihood linear regression. Experimental results on statistical parametric speech synthesis show that the proposed framework enables data from multiple speakers in different languages to be used to: train a synthesis system; synthesize speech in a language using speaker characteristics estimated in a different language; and adapt to a new language.	decision tree;hidden markov model;interpolation;markov chain;speech synthesis	Heiga Zen;Norbert Braunschweiler;Sabine Buchholz;Mark J. F. Gales;Kate Knill;Sacha Krstulovic;Javier Latorre	2012	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2012.2187195	speaker recognition;speech recognition;computer science;machine learning;decision tree;pattern recognition;markov model;hidden markov model	ML	-17.74082278020083	-91.83032281381588	20489
d60a5c6b1be9824b1bf7d9cbac50cecd4cccc5a7	using isofrequency neural column for harmonic sound scene decomposition				Zdravko Kacic;Bogomir Horvat	1993			speech recognition;computer science;computer vision;harmonic;artificial intelligence	ML	-13.188493563862714	-86.8445058958311	20502
5ba1357d2540003f7af9a6c2385df7a64fc8c128	comparison of methods for evaluation of medical terminological systems		The importance of terminological systems (TS) for the medical domain is widely recognized. The usability of such a system depends primarily on its content. We have designed four methods to evaluate the content of TS and applied them in a case study	medical records systems, computerized;usability	Danielle G. T. Arts;Ronald Cornet;Evert de Jonge;Nicolette de Keizer	2003	AMIA ... Annual Symposium proceedings. AMIA Symposium		applied mathematics;statistics;mathematics	Embedded	-54.19265025888866	-67.51057241359187	20504
0116e12fd4dcbc7e71ee7d3a477b7bd8fc6220ae	mining translations of web queries from web click-through data	query translation pair;bilingual dictionary;timely translation;mining translation;click-through data;timely query translation pair;web click-through data;new query;commercial online translation system;query translation;machine translation system;web query;bilingual url pair pattern	Query translation for Cross-Lingual Information Retrieval (CLIR) has gained increasing attention in the research area. Previous work mainly used machine translation systems, bilingual dictionaries, or web corpora to perform query translation. However, most of these approaches require either expensive language resources or complex language models, and cannot achieve timely translation for new queries. In this paper, we propose a novel solution to automatically acquire query translation pairs from the knowledge hidden in the click-through data, that are represented by the URL a user clicks after submitting a query to a search engine. Our proposed solution consists of two stages: identifying bilingual URL pair patterns in the click-through data and matching query translation pairs based on user click behavior. Experimental results on a real dataset show that our method not only generates existing query translation pairs with high precision, but also generates many timely query translation pairs that could not be obtained by previous methods. A comparative study between our system and two commercial online translation systems shows the advantage of our proposed method.	bilingual dictionary;cross-language information retrieval;language model;machine translation;text corpus;web search engine	Rong Hu;Weizhu Chen;Jian Hu;Yansheng Lu;Zheng Chen;Qiang Yang	2008			sargable;query optimization;query expansion;web query classification;computer science;data mining;rdf query language;rule-based machine translation;web search query;world wide web;information retrieval;query language	AI	-30.66602918869781	-59.10917267880747	20510
07dc1c9c9b0d07763fb1fbc05979bbb188206297	identifying objects on the basis of spatial contrast: an empirical study	empirical study;linguistic analysis;reference systems;spatial language	"""In contrast to most research on spatial reference, the scenario in our human-robot experiments focuses on identifying rather than localising objects using spatial language. The relevant question in such a task is """"Which"""" rather than """"Where"""". In order to gain insights about the kind of language to expect in such a scenario, we collected participants' linguistic choices in a web-based empirical study. Spatial scenarios were presented that varied with respect to number, shape, and location of elements, and with respect to possible perspectives. The linguistic analysis reveals that speakers adhere to underlying principles similar to those known for non-spatial object reference. If objects only differ in spatial position, a reference system and spatial axis is chosen that is suitable for contrasting the target object from competing ones. The exact spatial location is usually not specified if there are no competing objects closeby."""	apache axis;blue (queue management algorithm);cognition;data retrieval;experiment;german research centre for artificial intelligence;human–robot interaction;michael j. fischer;spatial reference system;spatial variability;transistor;web application	Thora Tenbrink	2004		10.1007/978-3-540-32255-9_8	computer vision;computer science;data mining;communication	HCI	-7.807988100596322	-77.47541825694918	20511
c1be238424eb08360906bfdc05b3a85b58a1a826	efficient hyperlink analysis using robust proportionate prestige score in pagerank algorithm	prestige analysis;link based search;web mining	Existing PageRank algorithm exploits the Hyperlink Structure of the web with uniform transition probability distribution to measure the relative importance of web pages. This paper proposes a novel method namely Proportionate Prestige Score (PPS) for prestige analysis. This proposed PPS method is purely based on the exact prestige of web pages, which is applicable to Initial Probability Distribution (IPD) matrix and Transition Probability Distribution (TPD) matrix. This proposed PPS method computes the single PageRank vector with non-uniform transition probability distribution, using the link structure of the web pages offline. This non-uniform transition probability distribution has efficiently overcome the ink-based search restige analysis dangling page problem than the existing PageRank algorithm. This paper provides benchmark analysis of ranking methods: PageRank and proposed PPS. These methods are tested with real social network data from three different domains: Social Circle:Facebook, Wikipedia vote network and Enron email network. The findings of this research work propose that the quality of the ranking has improved by using the proposed PPS method compared with the existing PageRank algorithm. © 2014 Elsevier B.V. All rights reserved.	algorithm;benchmark (computing);email;hyperlink;interpupillary distance;markov chain;online and offline;pagerank;propagation delay;social network;web page;wikipedia	V. Lakshmi Praba;T. Vasantha	2014	Appl. Soft Comput.	10.1016/j.asoc.2014.07.012	web mining;computer science;artificial intelligence;machine learning;data mining;world wide web;information retrieval	AI	-27.915629282253786	-61.10430164077469	20553
1b4c8be17bfeaa240634d2790e1c60eeff7ebfd9	telemedmail: free software to facilitate telemedicine in developing countries.	image processing;data compression;and forward;text entry;digital camera;data encryption;video conferencing;network connectivity;developing country;digital image;free software	Telemedicine offers the potential to alleviate the severe shortage of medical specialists in developing countries. However lack of equipment and poor network connections usually rule out video-conferencing systems. This paper describes a software application to facilitate store-and-forward telemedicine by email of images from digital cameras. TeleMedMail is written in Java and allows structured text entry, image processing, image and data compression, and data encryption. The design, implementation, and initial evaluation are described.	data compression;digital camera;email;encryption;image processing;java programming language;store and forward;structured text;telemedicine	Hamish S. F. Fraser;Darius Jazayeri;Lech Bannach;Peter Szolovits;St John D. McGrath	2001	Studies in health technology and informatics	10.3233/978-1-60750-928-8-815	computer vision;computer science;digital image processing;multimedia;world wide web	HCI	-51.94728587345567	-61.25378468909559	20572
22c6e6c293484d001b29a688295d72af260085a7	classification-based melody transcription	content management;modelizacion;modelo markov oculto;analisis estadistico;transcription automatique;modele markov cache;pitch acoustics;audio retrieval;hidden markov model;musica;periodic structure;acoustique musicale;armonica;gestion contenido;harmonic;intelligence artificielle;tonie;aprendizaje probabilidades;estructura periodica;classification;transcripcion automatica;musical acoustics;modelisation;sound recording;musique;statistical analysis;harmonique;enregistrement son;multiway classification;acustica musical;audio;altura sonida;registro sonido;machine exemple support;analyse statistique;hidden markov model imbalanced data sets;gestion contenu;structure periodique;melody transcription;apprentissage probabilites;artificial intelligence;melodia;inteligencia artificial;support vector machine;maquina ejemplo soporte;vector support machine;analisis sonido;sound analysis;automatic transcription;modeling;content based retrieval;music;clasificacion;recherche par contenu;analyse son;probability learning;imbalanced data sets;melody;melodie	The melody of a musical piece—informally, the part you would hum along with—is a useful and compact summary of a full audio recording. The extraction of melodic content has practical applications ranging from content-based audio retrieval to the analysis of musical structure. Whereas previous systems generate transcriptions based on a model of the harmonic (or periodic) structure of musical pitches, we present a classification-based system for performing automatic melody transcription that makes no assumptions beyond what is learned from its training data. We evaluate the success of our algorithm by predicting the melody of the ADC 2004 Melody Competition evaluation set, and we show that a simple frame-level note classifier, temporally smoothed by post processing with a hidden Markov model, produces results comparable to state of the art model-based transcription systems.	algorithm;attribute-value system;hidden markov model;markov chain;medical transcription;smoothing;transcription (software)	Daniel P. W. Ellis;Graham E. Poliner	2006	Machine Learning	10.1007/s10994-006-8373-9	support vector machine;melody;speech recognition;content management;computer science;artificial intelligence;musical acoustics;harmonic;music;hidden markov model	ML	-16.587583079519302	-85.15087477309395	20578
c7925cc8b5fc44fcc6fe114e3e037fc7ea56f1f2	interactive computer aids for acquiring proficiency in mandarin	sistema interactivo;lenguaje natural;dialogue system;fonction potentiel;web pages;red www;man machine dialogue;software agent;telephone;speech processing;lengua extranjera;langage naturel;reseau web;tratamiento palabra;language translation;traitement parole;traduction connaissance;classroom;langue etrangere;chino;agent logiciel;systeme conversationnel;spoken dialogue system;software agents;spoken language translation;internet;interactive system;student teacher;natural language;funcion potencial;aula clase;world wide web;dialogo hombre maquina;native speaker;speaker;multilinguisme;potential function;locutor;foreign language;chinois;salle cours;telefono;chinese;non native speaker;multilingualism;locuteur;multilinguismo;dialogue homme machine;role play	It is widely recognized that one of the best ways to learn a foreign language is through spoken dialogue with a native speaker. However, this is not a practical method in the classroom due to the one-to-one student/teacher ratio it implies. A potential solution to this problem is to rely on computer spoken dialogue systems to role play a conversational partner. This paper describes several multilingual dialogue systems specifically designed to address this need. Students can engage in dialogue with the computer either over the telephone or through audio/typed input at a Web page. Several different domains are being developed, in which a student’s conversational interaction is assisted by a software agent functioning as a “tutor” which can provide them with translation assistance at any time. Thus, two recognizers are running in parallel, one for English and one for Chinese. Some of the research issues surrounding high-quality spoken language translation and dialogue interaction with a non-native speaker are discussed.	360-degree video;chao (sonic);computer;content-control software;dialog system;experiment;finite-state machine;graphics;humans;interaction;julia set;julian day;machine translation;one-to-one (data model);peabody award;programming paradigm;software agent;super robot monkey team hyperforce go!;video post-processing;web page;wheatley (portal)	Stephanie Seneff	2006		10.1007/11939993_1	speech recognition;computer science;artificial intelligence;software agent;speech processing;linguistics	NLP	-28.758647077045193	-82.33810415288137	20593
6b19d73c9c025126fbd9a63fe95514fa64ee4397	utilizing document classification for grooming attack recognition	software;topology;document handling;security of data decision making document handling pattern classification;classification algorithm;minor user hazard;support vector machines;naive bayes;dialogs;sexual exploitation;accuracy;internet;comparative evaluation document classification grooming attack recognition word matching dialogs minor user hazard decision making method ranked algorithm;cyber crime;classification algorithms;pattern classification;ranking algorithm;grooming recognition;word matching;support vector machine;classification algorithms support vector machines decision making topology accuracy software internet;document classification;comparative evaluation;grooming attack recognition;security of data;ranked algorithm;decision making method;sexual exploitation grooming recognition document classification naive bayes cyber crime	Grooming attack recognition is a complex issue that is difficult to address using simple word matching in order to identify potential hazard for minor users. In this paper, the utilization of document classification to create patterns from real dialogs is proposed. Furthermore, a decision making method that results in generating proper warning signals based on the classification results is introduced. The decision making method is then applied using the best ranked algorithm with a comparative evaluation which conducted on seven document classification algorithms.	algorithm;document classification	Dimitrios Michalopoulos;Ioannis Mavridis	2011	2011 IEEE Symposium on Computers and Communications (ISCC)	10.1109/ISCC.2011.5983950	statistical classification;support vector machine;computer science;machine learning;pattern recognition;data mining;computer security	Vision	-20.646269163504385	-57.175988802144374	20639
506f399a97dfa9bff1e79451e8a50685bf85262d	automatic annotation of historical paper documents	artificial intelligent;machine learning;data model;system architecture;software component;knowledge representation	The European Community project COLLATE (Collaboratory for Annotation, Indexing and Retrieval of Digitized Historical Archive Material) is concerned with digitised historical/cultural material. One of the main features of COLLATE system architecture is the integration of software components that exploit state-of-the-art techniques coming from the area of Artificial Intelligence and Knowledge Representation. This work describes the results achieved by applying Machine Learning methods for automatic classification and labelling of documents. Furthermore, we also discuss the advantages obtained by exploiting brand new research achievements in KR for the design of COLLATE data model.	archive;artificial intelligence;component-based software engineering;data model;knowledge representation and reasoning;machine learning;systems architecture	Stefano Ferilli;Luigi Iannone;Giovanni Semeraro;Teresa Maria Altomare Basile;Nicola Di Mauro;Ignazio Palmisano	2004	Intelligenza Artificiale		automatic image annotation;computer science;knowledge representation and reasoning;image retrieval;search engine indexing;data model;systems architecture;collaboratory;information retrieval;data mining;annotation	AI	-13.482150050966299	-59.13816984479491	20659
28959123ef7beabfe2e08827792220a4782a6292	site-based dynamic pruning for query processing in search engines	search engine;query processing;inverted index;cluster skipping;indexing and retrieval;web search engine;dynamic query;dynamic query pruning	Web search engines typically index and retrieve at the page level. In this study, we investigate a dynamic pruning strategy that allows the query processor to first determine the most promising websites and then proceed with the similarity computations for those pages only within these sites.	computation;database;web search engine	Ismail Sengör Altingövde;Engin Demir;Fazli Can;Özgür Ulusoy	2008		10.1145/1390334.1390543	search-oriented architecture;sargable;search engine indexing;query optimization;query expansion;web query classification;ranking;inverted index;web search engine;computer science;database;rdf query language;web search query;world wide web;information retrieval;query language;search engine	DB	-31.73829676937366	-55.811978597065185	20685
631cc3832d15ceefc1b76ddec07ebc2567db5986	segmental duration control with asymmetric causal retro-causal neural networks.		The generation of pleasant prosody parameters is very important for speech synthesis. A Prosody generation unit can be seen as a dynamical system. In this paper sophisticated time-delay recurrent neural network (NN) topologies are presented which can be used for the modeling of dynamical systems. Within the prosody prediction task left and right context information is known to influence the prediction of prosody control parameters. This can be modeled by causal-retro-causal information flows [1]. Since information being available during training is partially unavailable during application, there is a structural switching from training to application. This structural change of the information flow is handled by two asymmetric architectures. These proposed new architectures allow the integration of further a priori knowledge. By this we are able to improve the performance of our duration control unit within our text-to-speech (TTS) systemPapageno.	artificial neural network;causal filter;causal system;causality;control unit;dynamical system;information flow (information theory);netware file system;recurrent neural network;semantic prosody;speech synthesis	Çaglayan Erdem;Hans-Georg Zimmermann	2001			artificial neural network;artificial intelligence;pattern recognition;computer science	ML	-17.442375246949343	-86.95762996497753	20688
32c77e09466f57705210dbbe57503c3d6f0158f4	user questionnaire to evaluate the radiological workspace	job satisfaction;pacs;radiology information systems;workplace design;analog digital conversion;picture archiving and communication system;workstations;human engineering;design;humans;digital image;questionnaires;physicians;ergonomics;efficiency organizational;hospital design and construction	Over the past few years, an increase in digitalization of radiology departments can be seen, which has a large impact on the work of the radiologists. This impact is not only demonstrated by the increased use of digital images but also by changing demands on the whole reading environment. In this study, we evaluated the satisfaction of our radiologists with our digital Picture Archival and Communication System environment and their workspace. This evaluation was performed by distribution of a questionnaire consisting of a score sheet and some open questions to all radiologists and residents. Out of 25 questionnaires, 12 were adequately answered and returned. Results clearly showed that most problems were present in the area of reading room design and layout and comfort and ergonomics. Based on the results from this study, adaptations were made and the results were also used in the planning of the redesign of the entire department of radiology.	adaptation;digital image;human factors and ergonomics;radiology;workspace	Peter M. A. van Ooijen;Allya P. Koesoema;Matthijs Oudkerk	2006	Journal of Digital Imaging	10.1007/s10278-006-0629-1	simulation;radiology;medicine;computer science;knowledge management;human factors and ergonomics;multimedia;picture archiving and communication system	HCI	-60.2186236803419	-64.36586840965064	20712
8defae562c8c718e1ea3ac6a4f38a93628ec4fb6	audio bandwidth extension using ensemble of recurrent neural networks	signal image and speech processing;echo state network;acoustics;mathematics in music;spectral translation;engineering acoustics;ensemble of recurrent neural networks;audio bandwidth extension	In audio communication systems, the perceptual audio quality of the reproduced audio signals such as the naturalness of the sound is limited by the available audio bandwidth. In this paper, a wideband to super-wideband audio bandwidth extension method is proposed using an ensemble of recurrent neural networks. The feature space of wideband audio is firstly divided into different regions through clustering. For each region in the feature space, a specific recurrent neural network with a sparsely connected hidden layer, referred as the echo state network, is employed to dynamically model the mapping relationship between wideband audio features and high-frequency spectral envelope. In the following step, the outputs of multiple echo state networks are weighted and fused by means of network ensemble, in order to further estimate the high-frequency spectral envelope. Finally, combining the high-frequency fine spectrum extended by spectral translation, the proposed method can effectively extend the bandwidth of wideband audio to super wideband. Objective evaluation results show that the proposed method outperforms the hidden Markov model-based bandwidth extension method on the average in terms of both static and dynamic distortions. In subjective listening tests, the results indicate that the proposed method is able to improve the auditory quality of the wideband audio signals and outperforms the reference method.	artificial neural network;bandwidth extension;cluster analysis;distortion;echo state network;extension method;feature vector;hidden markov model;markov chain;newton's method;recurrent neural network;sound quality	Xin Liu;Changchun Bao	2016	EURASIP J. Audio, Speech and Music Processing	10.1186/s13636-016-0090-0	speech recognition;acoustics;bandwidth extension;audio signal processing;computer science;sound quality;audio signal flow;echo state network;physics	AI	-10.230044255548066	-91.21109868884928	20719
34a7bf2b660bf018110bd1bf4d61ed5a48dc9831	combining classifiers for word sense disambiguation	classifier combination;filologias;feature space;system performance;word sense disambiguation;linguistica;combining classifier;grupo a;transformation based learning	Classifier combination is an effective and broadly useful method of improving system performance. This article investigates in depth a large number of both well-established and novel classifier combination approaches for the word sense disambiguation task, studied over a diverse classifier pool which includes feature-enhanced Naive Bayes, Cosine, Decision List, Transformation-based Learning and MMVC classifiers. Each classifier has access to the same rich feature space, comprised of distance weighted bag-of-lemmas, local ngram context and specific syntactic relations, such as Verb-Object and Noun-Modifier. This study examines several key issues in system combination for the word sense disambiguation task, ranging from algorithmic structure to parameter estimation. Experiments using the standard SENSEVAL2 lexical-sample data sets in four languages (English, Spanish, Swedish and Basque) demonstrate that the combination system obtains a significantly lower error rate when compared with other systems participating in the SENSEVAL2 exercise, yielding state-of-the-art performance on these data sets.	word sense;word-sense disambiguation	Radu Florian;Silviu Cucerzan;Charles Schafer;David Yarowsky	2002	Natural Language Engineering	10.1017/S1351324902002978	natural language processing;margin classifier;feature vector;computer science;machine learning;pattern recognition;computer performance	NLP	-21.83083614941803	-72.13928708148188	20721
8758b0650ab8e3c904d4b79304de7569c46d10a7	a global, boundary-centric framework for unit selection text-to-speech synthesis	modal decomposition;traitement signal;join cost;text to speech synthesis;analyse modale;optimisation;fiabilidad;reliability;discontinuity perception;feature extraction paradigm;unit selection;speech synthesis;optimizacion;cost function;analisis modal;distance measure;formant;analyse linguistique;analyse fourier;speech processing;speech synthesis acoustic waves signal synthesis acoustic measurements assembly cost function feature extraction frequency measurement bandwidth modal analysis;concatenacion;frequency measurement;funcion coste;senal vocal;concatenation;linguistic analysis;assembly;signal vocal;distance measurement;medicion distancia;feature extraction;signal processing;fiabilite;analisis linguistico;prosodie;segment concatenation;unit selection discontinuity perception distance measure join cost modal analysis segment concatenation text to speech synthesis;optimality criteria;prosody assessment;signal acoustique;fourier analysis;bandwidth;fonction cout;analisis fourier;optimization;sintesis palabra;acoustic signal;signal synthesis;experimental evaluation;acoustic waves;modal analysis;extraction caracteristique;prosody;formante;vocal signal;acoustic measurements;procesamiento senal;fourier analysis speech synthesis speech processing feature extraction;unit selection process;senal acustica;synthese parole;discontinuity perception text to speech synthesis unit selection process prosody assessment feature extraction paradigm fourier analysis modal decomposition segment concatenation;prosodia;mesure de distance	The level of quality that can be achieved by modern concatenative text-to-speech synthesis heavily depends on the optimization criteria used in the unit selection process. While effective cost functions arise naturally for prosody assessment, the criteria typically selected to quantify discontinuities in the speech signal do not closely reflect users' perception of the resulting acoustic waveform. This paper introduces an alternative feature extraction paradigm, which eschews general purpose Fourier analysis in favor of a modal decomposition separately optimized for each boundary region. The ensuing transform framework preserves, by construction, those properties of the waveform which are globally relevant to each concatenation considered. In addition, it leads to a novel discontinuity measure which jointly, albeit implicitly, accounts for both interframe incoherence and discrepancies in formant frequencies/bandwidths. Experimental evaluations are conducted to characterize the behavior of this new metric, first on a contiguity prediction task, and then via a systematic listening comparison using a conventional metric as baseline. The results underscore the viability of the proposed framework in quantifying the perception of discontinuity between acoustic units.	acoustic cryptanalysis;baseline (configuration management);concatenation;euclidean distance;feature extraction;feature vector;fourier analysis;mathematical optimization;modal logic;netware file system;normal mode;programming paradigm;reflections of signals on conducting lines;region of interest;semantic prosody;singular value decomposition;speech synthesis;waveform	Jerome R. Bellegarda	2006	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TSA.2005.858048	concatenation;acoustic wave;speech recognition;formant;acoustics;feature extraction;computer science;signal processing;modal analysis;pattern recognition;reliability;speech processing;assembly;linguistics;fourier analysis;prosody;speech synthesis;bandwidth;statistics	Vision	-8.391586162308789	-89.13308400667727	20727
5c4abd675982c5c9c06ac3b5b9e605d1917a8214	memory-restricted latent semantic analysis to accumulate term-document co-occurrence events	partial update algorithm;dimensionality reduction;co occurrence;latent semantic analysis	0167-8655/$ see front matter 2012 Elsevier B.V. A http://dx.doi.org/10.1016/j.patrec.2012.05.002 ⇑ Corresponding author. E-mail addresses: nash@comp.nus.edu.sg (S.-H (J.-H. Lee). This paper addresses a novel adaptive problem of obtaining a new type of term-document weight. In our problem, an input is given by a long sequence of co-occurrence events between terms and documents, namely, a stream of term-document co-occurrence events. Given a stream of term-document co-occurrences, we learn unknown latent vectors of terms and documents such that their inner product adaptively approximates the target query-based term-document weights resulting from accumulating cooccurrence events. To this end, we propose a new incremental dimensionality reduction algorithm for adaptively learning a latent semantic index of terms and documents over a collection. The core of our algorithm is its partial updating style, where only a small number of latent vectors are modified for each term-document co-occurrence, while most other latent vectors remain unchanged. Experimental results on small and large standard test collections demonstrate that the proposed algorithm can stably learn the latent semantic index of terms and documents, showing an improvement in the retrieval performance over the baseline method. 2012 Elsevier B.V. All rights reserved.	algorithm;apply;baseline (configuration management);converge;dimensionality reduction;eigen (c++ library);fixed-point arithmetic;fixed-point iteration;iterative method;latent dirichlet allocation;latent semantic analysis;mpeg media transport;matrix multiplication;midwoofer-tweeter-midwoofer;performance;reinforcement learning;x00	Seung-Hoon Na;Jong-Hyeok Lee	2012	Pattern Recognition Letters	10.1016/j.patrec.2012.05.002	latent semantic indexing;co-occurrence;latent semantic analysis;computer science;machine learning;document-term matrix;pattern recognition;data mining;probabilistic latent semantic analysis;dimensionality reduction	ML	-16.658274477176935	-67.66385532318752	20740
9684219e4e524694b8a1d5b2b458fd372a266482	a new approach to use concepts definitions for semantic relatedness measurement	gloss;semantic relatedness measurement;path;definition of concepts;ontology	Semantic Relatedness Measurement (SRM) is one of the most important applications of reasoning by ontologies and different disciplines of AI, e.g. Information Retrieval, are firmly tied to it. The accuracy of SRM by lexical resources is largely determined by the quality of the knowledge modeling by the knowledge base. The limited types of relations modeled by ontologies have caused most of the SRM methods to be able to detect and measure only a few special types of semantic relationships that is very far from the concept of semantic relatedness in human brain. Concepts of lexical resources are usually accompanied with a plain text narratively defines the concept. The information included in the definition of concepts sound very promising for SRM. This paper intends to treat this information as formal relations to improve SRM by distance-base methods. In order to do so, concepts glosses are mined for the semantic relations that are not modeled by the ontology. Then, these relations are employed in combination with classic relations of the ontology for semantic relatedness measurement according to the shortest path between concepts. Our evaluation demonstrated qualitative and quantitative improvement in detection of previously unknown semantic relationships and also stronger correlation with human judgment in SRM.	semantic similarity	Ehsan KhounSiavash;Kamran Zamanifar	2011		10.1007/978-3-642-25832-9_64	semantic similarity;semantic computing;computer science;knowledge management;data mining;information retrieval	Web+IR	-29.061810815999152	-63.0267998864243	20742
f9e6765525088ab5c169ec2561f4246852ea2ffb	a method using linguistic and acoustic features to detect inadequate utterances in medical communication	support vector machines;support vector machines biomedical communication computational linguistics;linguistic features svm support vector machine medical communication inadequate utterances acoustic features;acoustics pragmatics feature extraction support vector machines medical services educational institutions conferences;support vector machine medical communication utterance classification linguistic feature acoustic feature;computational linguistics;biomedical communication	We have previously proposed two methods using both linguistic and acoustic features separately to detect inadequate utterances in medical communication. However, some inadequate utterances could not be detected because these methods only considered either linguistic or acoustic features, whereas, in general, people use both features to judge an utterance. In this paper, we propose a method using both linguistic and acoustic features. The linguistic features are based on not only word frequency but also sentence and conversation structures. The acoustic features are based on the variances of power and fundamental frequency (F0). A Support Vector Machine (SVM) is used to learn these two types of features compositely. The experimental results showed that the precision of proposed method using both linguistic and acoustic features increased 6% from the traditional recognition method and recall of the proposed method increased 14% from the traditional method.	acoustic cryptanalysis;algorithm;artificial neural network;cepstrum;coefficient;machine learning;mixture model;numerical analysis;support vector machine;word lists by frequency	Michihisa Kurisu;Kazuya Mera;Ryunosuke Wada;Yoshiaki Kurosawa;Toshiyuki Takezawa	2013	2013 IEEE 6th International Workshop on Computational Intelligence and Applications (IWCIA)	10.1109/IWCIA.2013.6624814	natural language processing;support vector machine;speech recognition;computer science;computational linguistics;machine learning	NLP	-12.557748005420297	-86.22495785617087	20781
1ac76546c5912320403295d2edfe7f9a4ba169d7	merging a syntactic resource with a wordnet: a feasibility study of a merge between sto and dannet	machine translation;feasibility study;information retrieval;language technology	This paper presents a feasibility study of a merge between SprogTeknologisk Ordbase (STO), which contains morphological and syntactic information, and DanNet, which is a Danish WordNet containing semantic information in terms of synonym sets and semantic relations. The aim of the merge is to develop a richer, composite resource which we believe will have a broader usage perspective than the two seen in isolation. In STO, the organizing principle is based on the observable syntactic features of a lemma’s near context (labeled syntactic units or SynUs). In contrast, the basic unit in DanNet is constituted by semantic senses or in wordnet terminology synonym sets (synsets). The merge of the two resources is thus basically to be understood as a linking between SynUs and synsets. In the paper we discuss which parts of the merge can be performed semi-automatically and which parts require manual linguistic matching procedures. We estimate that this manual work will amount to approx. 39% of the lexicon material.	approximation;lexicon;matching (graph theory);observable;organizing (structure);semiconductor industry;slater-type orbital;synonym ring;wordnet	Bolette S. Pedersen;Anna Braasch;Lina Henriksen;Sussi Olsen;Claus Povlsen	2008			semantic similarity;artificial intelligence;semantic computing;natural language processing;semantic grid;semantic technology;semantic web stack;wordnet;computer science;explicit semantic analysis;rule-based machine translation	NLP	-29.936989987461512	-71.16162480401418	20798
f64122fd83c8b72eb8bd9bfaeca7131f79dba49e	making use of external company data to improve the classification of bank transactions		This project aims to explore to what extent external semantic resources on companies can be used to improve the accuracy of a real bank transaction classification system. The goal is to identify which implementations are best suited to exploit the additional company data retrieved from the Bronnoysund Registry and the Google Places API, and accurately measure the effects they have. The classification system builds on a Bag-of-Words representation and uses Logistic Regression as classification algorithm. This study suggests that enriching bank transactions with external company data substantially improves the accuracy of the classification system. If we compare the results obtained from our research to the baseline, which has an accuracy of 89.22%, the Bronnoysund Registry and Google Places API yield increases of 2.79pp and 2.01pp respectively. In combination, they generate an increase of 3.75pp.		Erlend Vollset;Eirik Folkestad;Marius Rise Gallala;Jon Atle Gulla	2017		10.1007/978-3-319-69179-4_54	data mining;implementation;computer science;logistic regression;exploit;database transaction	DB	-18.943517851995413	-53.28383412807387	20811
cddaa15f238397d4cc0484fef5ff0e036dc8b8f9	consumer health vocabulary: a proposal for a brazilian portuguese language		Studies show a gap between the expressions commonly used by health consumers and health professionals. To bridge this gap, consumer health vocabularies are presented as a solution. The aim of this paper is to describe an on-going project to create a consumer health vocabulary (CHV) in the Brazilian Portuguese language. This project will be developed in three phases: terms extraction and connection to compose a CHV graph structure, human validation, and computacional application development. We expect to make a CHV beta version (including approximately 5,000 valid consumer terms stored in a database graph) available. This project can contribute to the improvement of CHVs.		Josceli Maria Tenório;Ivan Torres Pisa	2015	Studies in health technology and informatics	10.3233/978-1-61499-564-7-1089	data mining;brazilian portuguese;natural language processing;expression (mathematics);vocabulary;artificial intelligence;graph;computer science	HCI	-50.47303585778752	-67.04841235267202	20813
123396c8c1106b69c7b11bf341f13897dd7ead24	evaluation of technical measures for workflow similarity based on a pilot study	distributed system;groupware;pilot study;empirical study;control de calidad;systeme reparti;methode empirique;relacion orden;metodo empirico;interrogation base donnee;empirical method;service web;pertinencia;interrogacion base datos;ordering;metric;web service;state dependence;similitude;intergiciel publication souscription;relation ordre;sistema repartido;internet;intergicial editor suscriptor;pertinence;similarity;dependencia del estado;controle qualite;utilisabilite;workflow;metrico;relevance;similitud;service discovery;usabilidad;quality control;usability;collecticiel;similarity measure;publish subscribe middleware;database query;metrique;servicio web;dependance de l etat	Service discovery of state dependent services has to take workflow aspects into account. To increase the usability of a service discovery, the result list of services should be ordered with regard to the relevance of the services. Means of ordering a list of workflows due to their similarity with regard to a query are missing. In this paper different similarity measures are presented and evaluated based on a pilot of an empirical study. In particular the different measures are compared with the study results. It turns out that the quality of the different measures differ significantly.	information retrieval;n-gram;relevance;service discovery;similarity measure;usability	Andreas Wombacher	2006		10.1007/11914853_16	computer science;data mining;database;empirical research;world wide web	Web+IR	-36.77972839395799	-58.91730991445435	20816
3634a269343366930e7d4c162fe7de540329ec47	extracting hypernym pairs from the web		We apply pattern-based methods for collecting hypernym relations from the web. We compare our approach with hypernym extraction from morphological clues and from large text corpora. We show that the abundance of available data on the web enables obtaining good results with relatively unsophisticated techniques.	sentence extraction;text corpus;world wide web	Erik F. Tjong Kim Sang	2007			computer science;data mining;world wide web;information retrieval	NLP	-25.645136703950456	-67.57893632051561	20844
62ae276afcea3d44c88f348bf8e2d5b7a612d421	pdtb xml: the xmlization of the penn discourse treebank 2.0.		The current study presents a conversion and unification of the Penn Discourse TreeBank 2.0 under the XML format. The converted corpus allows for a simultaneous search for syntactically specified discourse information on the basis of the	daisy digital talking book;lambda calculus;treebank;unification (computer science);xml	Xuchen Yao;Irina Borisova;Mehwish Alam	2010			natural language processing;treebank;linguistics;programming language	NLP	-30.549128730724682	-77.93896084901863	20872
73326bff821025a5c4306c9b6c8929ccf89b347c	using rich document representation in xml information retrieval	vector space model;document representation;indexing terms;xml retrieval;xml information retrieval;failure analysis;document processing	In this paper we present a method of document representation called Rich Document Representation (RDR) to build XML retrieval engines with high specificity. RDR is a form of document representation that utilizes single words, phrases, logical terms and logical statements for representing documents. The Vector Space model is used to compute index terms weight and similarity between each element and query. This system has participated in INEX 2006 and tested with the Content Only queries of the given collection. The results have been very weak but a failure analysis has revealed that it has been caused by an error in document processing which has produced inconsistent IDs and caused a mismatch between the IDs assigned to document elements such as single terms, phrases and logical terms. However similar experiment on INEX2004 collection yielded very good precision on high specificity task with s3e123 quantization.	document processing;failure analysis;information retrieval;restrictive design rules;sensitivity and specificity;xml retrieval	Fahimeh Raja;Mostafa Keikha;Masoud Rahgozar;Farhad Oroumchian	2006		10.1007/978-3-540-73888-6_29	well-formed document;xml catalog;document retrieval;xml validation;xml encryption;simple api for xml;xml;document clustering;processing instruction;xml schema;streaming xml;document type definition;document structure description;xml framework;data mining;xml database;xml schema;database;xml signature;vector space model;xml schema editor;information retrieval;efficient xml interchange	Web+IR	-34.00957261684943	-62.87124164670108	20894
d528e929bcbed675f6fb056b59c8d19007311559	ranking based clustering for social event detection	social event detection;ranking scores;clustering a large document collection	The problem of clustering a large document collection is not only challenged by the number of documents and the number of dimensions, but it is also affected by the number and sizes of the clusters. Traditional clustering methods fail to scale when they need to generate a large number of clusters. Furthermore, when the clusters size in the solution is heterogeneous, i.e. some of the clusters are large in size, the similarity measures tend to degrade. A ranking based clustering method is proposed to deal with these issues in the context of the Social Event Detection task. Ranking scores are used to select a small number of most relevant clusters in order to compare and place a document. Additionally, instead of conventional cluster centroids, cluster patches are proposed to represent clusters, that are hubs-like set of documents. Text, temporal, spatial and visual content information collected from the social event images is utilized in calculating similarity. Results show that these strategies allow us to have a balance between performance and accuracy of the clustering solution gained by the clustering method.	archive;cluster analysis;document	Taufik Sutanto;Richi Nayak	2014			correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;document clustering;k-medians clustering;fuzzy clustering;flame clustering;canopy clustering algorithm;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;mathematics;cluster analysis;single-linkage clustering;brown clustering;information retrieval;affinity propagation;clustering high-dimensional data	Web+IR	-26.691762537039594	-59.285147702088885	20898
8bcbcc4ff6bbfeda1212380de9990d56ee2eaf5b	a hierarchical approach for music chord modeling based on the analysis of tonal characteristics	music notes effect;artificial intelligence and image processing not elsewhere classified;neural networks;support vector machines;probabilistic space;harmonics;signal analysis;tonal characteristics extraction;acoustic signal processing;testing;psychology;musical instruments;musical acoustics;tempo proportional signal segmentation;multiple signal classification;hidden markov models;signal representation acoustic signal detection acoustic signal processing feature extraction harmonics musical acoustics musical instruments;psychoacoustical approach;feature extraction;signal representation;acoustic signal detection;frequency psychoacoustic models psychology multiple signal classification hidden markov models support vector machines neural networks signal analysis harmonic analysis testing;tonal characteristics extraction music notes effect music chord detection pitch class profile approach psychoacoustical approach harmonics hierarchical approach probabilistic space tempo proportional signal segmentation;psychoacoustic models;frequency;music chord detection;fundamental frequency;hierarchical approach;harmonic analysis;pitch class profile approach	This paper first discusses how the signal segmentation and tonal characteristics of music notes effect in music chord detection. Two approaches, pitch class profile approach and psycho-acoustical approach, which differently represent these tonal characteristics, are examined for chord detection. The analysis of the tonal characteristics reveals that not only the fundamental frequency of music note but also its harmonics and sub-harmonies in different octaves contribute for detecting related music chord. A hierarchical approach, which transforms the music chord tonal characteristics in each octave onto probabilistic space, is then proposed for modeling the music chord. Our experimental results show that detection of chord type, major, minor, diminish, and augmented, and individual chords, 12 chords per chord type, are improved with the proposed hierarchical chord modeling approach. Experimental results also reveal that the tempo proportional signal segmentation is more effective extracting tonal characteristics than using fixed length segmentation	sensor	Namunu Chinthaka Maddage;Mohan S. Kankanhalli;Haizhou Li	2006	2006 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2006.262676	support vector machine;speech recognition;feature extraction;computer science;multiple signal classification;machine learning;frequency;musical acoustics;harmonic analysis;mathematics;software testing;fundamental frequency;harmonics	SE	-8.193234445363936	-91.663697898161	20911
106eeae4afee47c0e5c414f9c4ef44566214f2c2	short-term and long-term evaluations of melody editing method based on melodic outline		In this paper, we describe short-term and long-term evaluations of our melody editing method based on a melodic outline. There have been a lot of attempts at automatic music composition, but only few allow musically untrained users to easily edit the melodies automatically composed. This is an important issue because it is difficult for such users to express what kind of melody they want in a machine-readable form and accordingly the generated melodies are often different from what they want. Based on this motivation, we proposed a melody editing method based on a melodic outline in which notewise information is hidden. Although we obtained promising results through a small user test, we did not conduct sufficient experiments. In this paper, we report the results of two experiments: one short-term and one long-term. In the shortterm experiment, we compared our method to the pianoroll interface. In the long-term experiment, we followed how users’ minds change through continously using our system. The results of both experiments showed the effects of our melody editing method.	experiment;human-readable medium	Tetsuro Kitahara;Yuichi Tsuchiya	2014			melody;natural language processing;speech recognition;musical composition;artificial intelligence;computer science	HCI	-16.225386900654073	-80.35187845334595	20925
fdb4a545af76e9af30d6d893f0011e670d2859bb	the impact of natural language processing-based textual analysis of social media interactions on decision making	text mining	Organizations typically use sentiment analysis-based systems, or even resort to simple manual analysis, to try to derive useful meaning from the public digital “chatter” of their customers. Motivated by the need for a more accurate way to qualitatively mine valuable productand brandoriented consumer-generated text, this paper experimentally tests the ability of an NLP-based analytics approach to extracting knowledge from highly unstructured text. Results indicate that for detecting problems from social media data, natural language processing outperforms sentiment analysis. Surprisingly, the experiment indicates that sentiment analysis is not only no better than manual analysis of social media data toward the goal of supporting organizational decision-making, but may even prove disadvantageous to such efforts.	experiment;interaction;natural language processing;sensor;sentiment analysis;social media	Keri Larson;Richard T. Watson	2013			natural language processing;text mining;computer science;data mining;linguistics	NLP	-21.592927989974996	-58.01274941361555	20928
951a47a84cb269836cd447f7e12abde900823f4e	erica: expert guidance in validating crowd answers	guiding user feedback;validation;crowdsourcing	Crowdsourcing became an essential tool for a broad range of Web applications. Yet, the wide-ranging levels of expertise of crowd workers as well as the presence of faulty workers call for quality control of the crowdsourcing result. To this end, many crowdsourcing platforms feature a post-processing phase, in which crowd answers are validated by experts. This approach incurs high costs though, since expert input is a scarce resource. To support the expert in the validation process, we present a tool for \emph{ExpeRt guidance In validating Crowd Answers (ERICA)}. It allows us to guide the expert's work by collecting input on the most problematic cases, thereby achieving a set of high quality answers even if the expert does not validate the complete answer set. The tool also supports the task requester in selecting the most cost-efficient allocation of the budget between the expert and the crowd.	cost efficiency;crowdsourcing;display resolution;stable model semantics;video post-processing;web application	Nguyen Quoc Viet Hung;Chi Thang Duong;Matthias Weidlich;Karl Aberer	2015		10.1145/2766462.2767866	simulation;computer science;knowledge management;data mining;world wide web;crowdsourcing	HCI	-46.191842865946725	-69.71939569228252	20930
74f6cb5ef61e25a2595f324ce82d37afd338b076	health information systems: evaluation and performance of a help desk		UNLABELLED A Help Desk (HD) is crucial in a computerized hospital.   OBJECTIVE to describe the performance of a HD.   DESIGN retrospective cohort study.   RESULTS the sociodemographic characteristics of users, as well as their relationship with the institution influence behaviour when requesting support to a HD. Also we observed a relationship between the flow of users request and the functioning of hospital services.   CONCLUSIONS complexity of HD process realizes the need to identify and define standards to ensure quality of service.	help-seeking behavior;information systems;quality of service;standards characteristics	Leandro Pintos;Adriana Stieben;Gabriela García;Agustina Briatore;Agustina Bertoia;Sonia E. Benitez;Diego Giunta;Analía Baum;Fernán Gonzalez Bernaldo de Quirós	2015	Studies in health technology and informatics	10.3233/978-1-61499-512-8-536	knowledge management;data mining;health informatics;desk;medicine	HCI	-59.30551837094575	-63.65017475109756	20936
