id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
dea05be5420416287bfd4de5b3467905522f759e	video and sensor data integration in a service-oriented surveillance system		Video surveillance systems have become very popular these days. In households, enterprises and even cities the monitoring systems have been deployed to improve the sense of safety. Additional sensors which produce large amounts of data are very often used independently. A real challenge is a meaningful integration of video and sensor streams, which could assist people operating these systems. This challenge requires a new, smart approach to browsing, analyzing and integrating information of various modalities and from various sources. Nowadays motion detection, face recognition and integration with external sensors are the popular subject of research. We notice the lack of efficient and flexible solutions for managing video surveillance data for public or private security. The problem is scalability and manageability of more and more complicated systems connecting large amounts of devices. In this paper we would like to focus on managing video and other data sources in the service-oriented manner for a large scale domain. Simplifying monitoring configuration and usage is also in the scope of our interest.	automatic number plate recognition;closed-circuit television;complex event processing;enterprise integration;facial recognition system;image quality;loose coupling;prototype;requirement;scalability;sensor;service-oriented device architecture;streaming media;usability;video	Damian Mierzwinski;Dariusz Walczak;Marcin Wolski;Marcin Wrzos	2011	Scalable Computing: Practice and Experience	10.12694/scpe.v12i1.690	simulation;engineering;video tracking;data mining;computer security	Mobile	-42.47218970156563	47.79329739730246	13540
86e35de2be1d54a004f3e64e2da4fee078089ea3	the internet of things, fog and cloud continuum: integration and challenges		The Internet of Things needs for computing power and storage are expected to remain on the rise in the next decade. Consequently, the amount of data generated by devices at the edge of the network will also grow. While cloud computing has been an established and effective way of acquiring computation and storage as a service to many applications, it may not be suitable to handle the myriad of data from IoT devices and fulfill largely heterogeneous application requirements. Fog computing has been developed to lie between IoT and the cloud, providing a hierarchy of computing power that can collect, aggregate, and process data from/to IoT devices. Combining fog and cloud may reduce data transfers and communication bottlenecks to the cloud and also contribute to reduced latencies, as fog computing resources exist closer to the edge. This paper examines this IoT-Fog-Cloud ecosystem and provides a literature review from different facets of it: how it can be organized, how management is being addressed, and how applications can benefit from it. Lastly, we present challenging issues yet to be addressed in IoT-Fog-Cloud infrastructures.	aggregate data;apache continuum;cloud computing;computation;ecosystem;fog computing;internet of things;requirement	Luiz Fernando Bittencourt;Roger Immich;Rizos Sakellariou;Nelson Luis Saldanha da Fonseca;Edmundo Roberto Mauro Madeira;Marília Curado;Leandro A. Villas;Luiz C. da Silva;Craig Lee;Omer Rana	2018	CoRR	10.1016/j.iot.2018.09.005	computation;computer science;storage as a service;distributed computing;cloud computing;continuum (design consultancy);hierarchy;internet of things	HPC	-43.04142013027823	48.98608112596662	13589
df9d6ae587460aec6b1496df3c0701dcb701d6df	a multiagent system for resource distribution into a cloud computing environment		It is undeniable that the term Cloud Computing has gained in importance at a remarkable pace. It is a technology which is becoming a common element of our life, due to the variety of devices related to the Internet of Things. In this technological frame, there are not many studies in which a Multiagent system has facilitated the management of a cloud-based computational environment; although a first sight its features (autonomy, decentralization, auto-organization, etc.) seem suitable for the task. This study presents the +Cloud which is a cloud platform managed by a Multiagent System.	agent-based model;cloud computing;multi-agent system	Fernando de la Prieta;Sara Rodríguez;Javier Bajo;Juan Manuel Corchado	2013		10.1007/978-3-642-38073-0_4	cloud computing	HPC	-44.11284156720501	46.49603962527608	13677
429e8a7098019c4c451b9c5b886aec27f476a562	design of experiments for effective pre-silicon verification of automotive electronics	automotive electronics;design of experiments;systemc/systemc-ams model;automotive electronic control unit;automotive electronics;effective pre-silicon verification;heterogeneous electronic systems;statistical design of experiments	The paper presents a method to validate the compliance with value-ranged requirements of heterogeneous electronic systems with variations in a time effective way. Statistical Design of Experiments methodology is applied and adapted to plan, implement and analyze simulations of the system model, to determine key parameters that impact system performance. This is applied on a SystemC/SystemC-AMS model of an automotive Electronic Control Unit.	design of experiments;electronic control unit;experiment;requirement;simulation;systemc;verilog-ams	Monica Rafaila;Christoph Decker;Georg Pelz;Christian Grimm	2009	2009 Forum on Specification & Design Languages (FDL)		embedded system;computer science;automotive engineering;computer performance;design of experiments;statistics	Embedded	-42.16613812812338	34.73655353810322	13708
447cd408c89286b4d989590b0f37bee205e68fa8	valuation and reporting of security assurance at operational systems level		Security Assurance is commonly defined as the ground for confidence on the security mechanisms to meet their objectives. Current approaches to evaluating Security Assurance have mainly focused on the software development stage or at the end product software. The few attempts to address Security Assurance at runtime assume a system security model to be static.#R##N#However most often, it is after the deployment or implementation phase that a system's security may be violated. A cause of security breach that has often been overlooked is the improper deployment/ implementation of the security mechanisms or, generally speaking, their incorrect posture at a given time. Such a security lax may create a false sense of security and lead to negative impacts on the stakeholders.#R##N#The motivation behind this work stems from the challenges relating: what Security Assurance is and; how it may be appraised and reported for a better understanding of an operational system's security posture. The novelty of this work lies in the provision of the metrics and a#R##N#methodology that could help address such a challenge. Hence, this thesis provides a contribution towards the improvement of the Security Assurance information required for the understanding of the security situation from a security practitioner perspective, taking into account: the quality of the verification process or software probe used for the verification; the reported correctness status of a security mechanism at a given time; and the estimated effectiveness level for a security mechanism, in case such information is available. Guidance on what tasks may or may#R##N#not be performed given the security posture of a security mechanism is provided for those users without much understanding of security and is based on the security criticality of the context in which the system operates. The aforementioned metrics are subsequently integrated in an overall methodology which helps compute the Security Assurance level of a component or service through aggregation techniques. Another important feature of the methodology is that it allows the security practitioner to adapt the security model or the metrics in case of newly emerged vulnerabilities. Evaluation of this contribution is described through use of theoretical criteria, tool implementation and application to a case study. Furthermore, Information security professionals have reviewed and evaluated the metrics and methodology proposed by this thesis and provided opinions on their applicability.	operational system;value (ethics)	Moussa Ouedraogo	2011			software security assurance;computer security model;standard of good practice;cloud computing security;reliability engineering;countermeasure;security through obscurity;security information and event management;security engineering;security convergence;covert channel;asset;engineering;threat;information security;data mining;security service;security testing;computer security	Logic	-55.602290726260286	48.057577079088155	13828
fa2f0eca93c6725ba48e35bb0a74e1bf7826a8ae	time between vulnerability disclosures: a measure of software product vulnerability		Time between vulnerability disclosure (TBVD) for individual analysts is proposed as a meaningful measure of the likelihood of finding a zero-day vulnerability within a given timeframe. Based on publicly available data, probabilistic estimates of the TBVD of various software products are provided. 69,646 vulnerabilities from the National Vulnerability Database (NVD) and the SecurityFocus Vulnerability Database were harvested, integrated and categorized according to the analysts responsible for their disclosure as well as by the affected software products. Probability distributions were fitted to the TBVD per analyst and product. Among competing distributions, the Gamma distribution demonstrated the best fit, with the shape parameter, k, similar for most products and analysts, while the scale parameter, θ, differed significantly. For forecasting, autoregressive models of the first order were fitted to the TBVD time series for various products. Evaluation demonstrated that forecasting of TBVD on a per product basis was feasible. Products were also characterized by their relative susceptibility to vulnerabilities with impact on confidentiality, integrity and availability respectively. The differences in TBVD between products is significant, e.g. spanning differences of over 500% among the 20 most common software products in our data. Differences are further accentuated by the differing impact, so that, e.g., the mean working time between disclosure of vulnerabilities with a complete impact on integrity (as defined by the Common Vulnerability Scoring System) for Linux (110 days) exceeds that of Windows 7 (6 days) by over 18 times.	adobe flash player;autoregressive model;categorization;confidentiality;curve fitting;data security;file spanning;firefox;firmware;full disclosure (computer security);google chrome;internet explorer;java virtual machine;linux;list of speech recognition software;mac os x public beta;microsoft windows;national vulnerability database;operating system;php;palo;safari (web browser);securityfocus;shutdown (computing);system 7;system integrity;time series;unicom system architect;vulnerability (computing);zero-day (computing);ios	Pontus Johnson;Dan Gorton;Robert Lagerström;Mathias Ekstedt	2016	Computers & Security	10.1016/j.cose.2016.08.004	data mining;computer security	Security	-61.327649089567345	46.685787226234794	13889
47d9dc011634575b1a97264275e62d688d839f54	towards dynamic configuration of distributed applications	distributed application;reflexive approach;information security;software reusability distributed object management java;ejb;ccm;runtime environment;high level features;ejb dynamic configuration distributed applications high level features transaction synchronization corba component model reflexive approach service integration ccm enterprise java beans;transaction;distributed applications;visualization;transaction databases;synchronization;programming profession;programming profession reflection java context aware services visualization runtime environment data security information security transaction databases open source software;software reusability;distributed object management;corba component model;component model;service integration;reflection;enterprise java beans;open source software;enterprise java bean;java;dynamic configuration;context aware services;data security	Configuring distributed applications at deployment time requires the introduction of high-level features such as transaction and synchronization into application’s code. Component models like CORBA Component Model (CCM) or Enterprise Java Beans (EJB) allow programmers to declare in deployment descriptors which services have to be plugged into components. However, these approaches do not allow a dynamic integration of new services. In this paper, we propose a reflexive approach allowing service integration into component at runtime. When a new service is added to a component, the combination with existing services is managed dynamically by the platform.	common object request broker architecture;distributed computing;enterprise javabeans;high- and low-level;java platform, enterprise edition;programmer;run time (program lifecycle phase);software deployment;synchronization (computer science)	Mireille Blay-Fornarino;Anne-Marie Pinna-Dery;Michel Riveill	2002		10.1109/ICDCSW.2002.1030816	synchronization;real-time computing;reflection;visualization;common component architecture;computer science;information security;operating system;component object model;database;distributed computing;data security;programming language;java;computer security	SE	-34.40541384287718	42.478539416682175	13920
99541f959f2764705b0755ce097afd5214e8d0c2	multiparty negotiation of dynamic distributed object services	scripting languages;object oriented programming;distributed objects;client server;workflow;negotiation;local area network;scripting language;long lived transactions;coordination;document management	Object-oriented programming (OOP) has proven a very useful paradigm for supporting client-server computing within the context of local-area networks, where stable assumptions can be made about the available resources and services and where interactions between clients and servers are relatively simple. By implementing servers as objects, access to services can be kept separate from implementation, thus making client-server applications both more exible and easier to maintain. Now that we are moving from single enterprise computing to the inter-organizational information world of the Internet and WWW, object-oriented programming must adapt itself to new client-server requirements. Speciically, there is need of coping with situations where new services can be dynamically added to servers, and where clients may need to coordinate the access to multiple services, rather than to single individual ones. In this paper, we describe the object model of the Coordination Language Facility, a programming framework that extends OOP with constructs that support dynamic services and multi-service coordination. We illustrate the use of these constructs through the application domain of distributed workkow.	application domain;client–server model;common look and feel;distributed object;enterprise software;interaction;programming paradigm;requirement;server (computing);www	Jean-Marc Andreoli;François Pacull;Daniele Pagani;Remo Pareschi	1998	Sci. Comput. Program.	10.1016/S0167-6423(97)00020-8	computer science;database;distributed computing;scripting language;distributed object;services computing;programming language;world wide web	PL	-34.854291050109296	43.22287402088542	13921
029e930160bb7fcdab920e68526a79b1960ad89c	comprehensive and efficient protection of kernel control data	software;virtual machine monitor;kernel;security of data operating systems computers program compilers;kernel prototypes computer security process control system design and analysis virtual machine monitors indexes;prototypes;system design and implementation intrusion prevention and tolerance software;control flow graph;intrusion prevention and tolerance;computer security;virtual machine monitors;indexes;system design;indexation;process control;intrusion prevention;program compilers;control transfer instructions kernel control data protection rootkit defenders rootkit authors excessive performance overhead indexed hooks kernel control flow enforcement legal jump targets rootkit attacks compiler based prototype freebsd 8 0 kernel;system design and implementation;security of data;operating systems computers;process control system;system design and analysis	Protecting kernel control data (e.g., function pointers and return addresses) has been a serious issue plaguing rootkit defenders. In particular, rootkit authors only need to compromise one piece of control data to launch their attacks, while defenders need to protect thousands of such values widely scattered across kernel memory space. Worse, some of this data (e.g., return addresses) is volatile and can be dynamically generated at run time. Existing solutions, however, offer either incomplete protection or excessive performance overhead. To overcome these limitations, we present indexed hooks, a scheme that greatly facilitates kernel control-flow enforcement by thoroughly transforming and restricting kernel control data to take only legal jump targets (allowed by the kernel's control-flow graph). By doing so, we can severely limit the attackers' possibility of exploiting them as an infection vector to launch rootkit attacks. To validate our approach, we have developed a compiler-based prototype that implements this technique in the FreeBSD 8.0 kernel, transforming 49 025 control transfer instructions (~7.25% of the code base) to use indexed hooks instead of direct pointers. Our evaluation results indicate that our approach is generic, effective, and can be implemented on commodity hardware with a low performance overhead (<;5% based on benchmarks).	benchmark (computing);bounce address;commodity computing;compiler;context-free grammar;control flow graph;dspace;experiment;freebsd;function pointer;kernel (operating system);overhead (computing);pivot table;prototype;rootkit;run time (program lifecycle phase);synthetic intelligence;vector (malware);volatile memory	Jinku Li;Zhi Wang;Tyler K. Bletsch;Deepa Srinivasan;Michael C. Grace;Xuxian Jiang	2011	IEEE Transactions on Information Forensics and Security	10.1109/TIFS.2011.2159712	database index;kernel;real-time computing;computer science;operating system;machine learning;process control;prototype;hypervisor;kernel preemption;computer security;intrusion prevention system;algorithm;systems design;control flow graph	Security	-55.64813394338825	56.16432639328756	13953
f3b7a50fac51e3e809e3658d3aa5835a92aa1604	comparing control flow graphs of binary programs through match propagation	binary program analysis;software comparison;software comparison control flow graph binary program analysis;program control structures data flow analysis java;control flow graph;java resilience malware conferences reliability computers software;structural matching method control flow graphs binary programs match propagation block matching java applications	In this paper, we present an approach to comparing control flow graphs of binary programs by matching their basic blocks. We first set up an initial match and propagate it to reach a stable state. We consider the matched pairs to identify overall similarities. To evaluate the proposed method, we perform experiments on real-world Java applications, and compare their performance with previous structural matching method. In the experimental results, the proposed method shows more reliable results than previous method at distinguishing similar control flow graphs.	basic block;control flow graph;experiment;java;software propagation	Hyun-Il Lim	2014	2014 IEEE 38th Annual Computer Software and Applications Conference	10.1109/COMPSAC.2014.84	computer science;theoretical computer science;operating system;database;distributed computing;programming language;control flow graph	SE	-59.2155949544987	37.72005980603354	13978
516c077507ea4024bb3cb829c90c0cf3b9b3caf7	protecting user privacy in remotely managed applications	online services;prashanth;information flow control;computer science protecting user privacy in remotely managed applications university of california;computer security;berkeley david e culler mohan;data privacy;differential privacy;computer science;data security	This thesis presents an end-to-end system architecture for online services to provide itu0027s users with a rprivacy guaranteer. The privacy guarantee as described in this thesis relates to the technological enforcement of the useru0027s privacy policies by these online applications that are otherwise untrusted by the user.Applications on the Internet are complex in that they integrate different types of functionalities into a consistent interface for the user. This thesis categorizes these functionalities into three generic components n a learning module that operates on the data from multiple users to gather higher level trends and aggregates, a data storage and transformation module that provides the core functionality of data presentation and finally a client-side component that interacts with the cloud-side functionalities and is responsible for sourcing input from the user and presenting them on the useru0027s device in a secure fashion.This thesis looks at the privacy risks introduced by each of these components and describes a rtrusted systemr that can be used by these online services to prove that the user specified privacy policies are enforced. The system consists of multiple independently developed solutions n Gupt, Rubicon, Bubbles and MobAds. These solutions work at tandem with each other to provide an end-to-end privacy perspective.While privacy policies and EULAs have largely been enforced in the realm of legal proceedings, this prototype implementation of an end-to-end privacy enforcement architecture demonstrates that it is both feasible and practical to enforce user privacy policies within the system.		Prashanth Mohan	2013			privacy software;information privacy;privacy by design;computer science;internet privacy;world wide web;computer security	Crypto	-46.40038306484924	59.409817389514906	13992
08fe29f3e118937a476e82f5057feb7830537ff3	pervasive computing helps fans get into the game	pervasive computing blacklight led enabled camera barcodes decoding image processing technique wi fi network mobile device sports;image processing technique;mobile device;pervasive computing;light emitting diodes;blacklight led enabled camera;sports;barcodes decoding;mobile radio;image processing techniques;ubiquitous computing;wireless lan;sport;pervasive computing devices;wireless lan cameras light emitting diodes mobile radio sport ubiquitous computing;cameras;wi fi network;pervasive computing fans ink space technology data mining decoding cameras cellular phones commercialization business	Pervasive computing devices give fans more control over their experience and more information about the games they love, even when they're miles away. Mobile devices have put sports into the pocket of the fan. Mobile content and messaging deals let consumers access their sports when and where they want to. Prior to the use of the wireless caddie, the application provider, set up a Wi-Fi network around each course. with the new devices, the existing network offers more ubiquitous coverage with less set-up time. Extraction relies on an image-processing technique that can pull out information and decode the barcodes. According to IBM, when this system is in place, any mobile device can be used with a blacklight-LED-enabled camera as the reader.	barcode;caddie;image processing;mobile device;ubiquitous computing;uptime;wireless router	Maya Dollarhide	2007	IEEE Pervasive Computing	10.1109/MPRV.2007.63	embedded system;human–computer interaction;computer science;operating system;sport;mobile device;internet privacy;computer security;ubiquitous computing	HCI	-38.59777305149695	54.48852221741382	14135
d5303cade168bb2bb32f59ea915d3f29e218da12	an anomaly detection method for individual service on web-based system by selection of dummy variables in multiple regression	recall rate anomaly detection method individual service web based system dummy variable selection multiple regression analysis resource usage estimation regression coefficient multiple correlation coefficient anomaly time identification anomaly service identification precision rate;electronic mail;trademarks;multiple regression analysis;resource allocation;anomaly detection;accuracy;web services;regression analysis correlation educational institutions electronic mail accuracy gaussian distribution trademarks;dummy variable;dummy variable anomaly detection multiple regression analysis correlation coefficient;regression analysis;correlation;correlation coefficient;security of data;web services regression analysis resource allocation security of data;gaussian distribution	This paper addresses detecting anomalies of individual services from their total resource usage on web-based system. Because the total resource usage is a linear combination of the number of accesses to each service, multiple regression analysis can be applied to estimate a resource usage per an access to each service as regression coefficient. However, the regression coefficients differ from the resource usage per an access of the services, which is caused by unstable resource usage per an access. We propose a method based on a multiple correlation coefficient to identify anomaly time and anomaly services. The proposed method identifies anomaly time when the correlation coefficient is decreased. And the proposed method identifies the anomaly service by judging whether the correlation coefficient is increased or not after the selection of the dummy variable. The experimental result shows that the proposed method can identify all the anomaly time, and improves precision rate and recall rate of detecting anomaly services by 20% at least, respectively.	anomaly detection;coefficient;control theory;dummy variable (statistics);sensitivity and specificity;sensor;web application	Yuki Tsuda;Nguyen Ngoc Tan;Masaki Samejima;Masanori Akiyoshi;Norihisa Komoda;Matsuki Yoshino	2012	2012 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/ICSMC.2012.6378011	econometrics;anomaly detection;computer science;machine learning;data mining;regression analysis;statistics	Mobile	-50.4337855800555	44.21043830087665	14170
17354fe90b71443258d4b61e52fba830ab7b4b84	dynamically reconfiguring multimedia components: a model-based approach	distributed application;video streaming;dynamic reconfiguration;distributed multimedia system;component composition;model based approach;digital television;long term evolution;multimedia systems;structural change;distributed multimedia applications;reconfigurable architecture;media processing;consistency checking;media streaming;surgical procedure;component architecture;admission control	Distributed multimedia systems are potentiall y subject to frequent and ongoing evolution of application structures. In such systems it is often unacceptable for reconfigurations to fail or to only partiall y succeed. This paper describes the reconfiguration architecture of the DJINN multimedia programming framework. We introduce the concept of multimedia transactions for structuring changes into atomic units that preserve application consistency and extend these with the smoothness condition to maintain temporal as well as data integrity across reconfigurations. We present a technique for scheduling configuration changes that trades off the perceived level of smoothness against the avail able resources and the desired timeliness of the re configuration.	data integrity;scheduling (computing)	Scott Mitchell;Hani Naguib;George Coulouris;Tim Kindberg	1998		10.1145/319195.319202	real-time computing;computer science;distributed computing;multimedia	SE	-37.14060139127943	41.99694587806204	14219
416f497d641e9da8d100f39d924dfe38bd769607	an iot trust and reputation model based on recommender systems	collaboration;computational modeling;cryptography;peer to peer computing;recommender systems;data models	In recent years, the Internet of Things (IoT) has been an inseparable part of our lives. IoT is typically heterogeneous in nature and requires interconnection with different types of devices or “things”. Being able to secure such a distributed environment is an onerous task. The heterogeneity of IoT, along with other factors, poses a challenge when it comes to securing communication between these devices. In this paper, we propose a novel IoT trust and reputation model that employs distributed probabilistic neural networks (PNNs) to classify trustworthy nodes from malicious ones. Our model tackles the cold start problem in IoT environments by predicting ratings for newly joined devices based on their characteristics and learns over time. Processing is completely distributed and is handled by the nodes themselves. This guarantees better availability, since there is no single point of failure. Moreover, our model can accommodate the various capabilities and types of IoT devices. Unlike other proposed models in the literature, our model provides different levels of security depending on the sensitivity of the data being transmitted.	artificial neural network;cold start;computation;distributed computing;imperative programming;interconnection;internet of things;overhead (computing);recommender system;reliability engineering;separable polynomial;single point of failure;transmitter;trust (emotion)	Sarah Asiri;Ali Miri	2016	2016 14th Annual Conference on Privacy, Security and Trust (PST)	10.1109/PST.2016.7907017	computer science;cryptography;internet privacy;world wide web;computer security;recommender system;collaboration	AI	-42.931801041170175	56.798815557430196	14280
f76431b9cb292333ae9a2af0dfe9100ff100f98e	human-centred analysis of the dependencies within sets of proofs		A failure in safety-critical systems, such as vehicles, aircrafts, etc., may endanger human lives. It is crucial that these system behave correctly, in accordance to their specification and fulfilling all the safety-critical properties defined for them. Safety could be guaranteed through the application of analysis techniques, in the form of formal verification and testing. However, verification of a real-size system means having a large number of lemmas, depending on each other. An update in one lemma might require changes in many other lemmas. It would be inefficient to find the existing dependencies out when the proof process is already running. We present a methodology for analysis of the dependencies between lemmas within the provided set of proofs. The application of this methodology to the Isabelle/HOL theories is supported by a tool. The tool provides a quick automated analysis and easy-to-check representation of the results, as well as allows optimisation of the provided set of proofs.		Maria Spichkova;Milan Simic	2017		10.1016/j.procs.2017.08.256	artificial intelligence;machine learning;theoretical computer science;lemma (mathematics);usability;mathematical proof;formal verification;computer science;intelligent decision support system;hol	Crypto	-45.81037499243399	35.248678459736205	14318
5def9e3e1d7662883eb077fbb2e30664c2005789	online reliability prediction via motifs-based dynamic bayesian networks for service-oriented systems	system of systems online reliability prediction time series service oriented computing;time series analysis quality of service web services throughput software reliability time factors	A service-oriented System of Systems (SoS) considers a system as a service and constructs a robust and value-added SoS by outsourcing external component systems through service composition techniques. Online reliability prediction for the component systems for the purpose of assuring the overall Quality of Service (QoS) is often a major challenge in coping with a loosely coupled SoS operating under dynamic and uncertain running environments. It is also a prerequisite for guaranteeing runtime QoS of a SoS through optimal service selection for reliable system construction. We propose a novel online reliability time series prediction approach for the component systems in a service-oriented SoS. We utilize Probabilistic Graphical Models (PGMs) to yield near-future, time series predictions. We assess the approach via invocation records collected from widely used real Web services. Experimental results have confirmed the effectiveness of the approach.	apple sos;dynamic bayesian network;graphical model;loose coupling;outsourcing;quality of service;service composability principle;service-oriented software engineering;system of systems;time series;web service	Hongbing Wang;Lei Wang;Qi Yu;Zibin Zheng;Athman Bouguettaya;Michael R. Lyu	2017	IEEE Transactions on Software Engineering	10.1109/TSE.2016.2615615	reliability engineering;computer science;data mining;world wide web	SE	-47.7396459308537	40.39055664693307	14356
57c9c7d29c57f875e3007de24dac28224fcd4ecc	building trusted components	building trusted components	The most attractive benefits of component-based development can only be realized if the components are of a level of quality much higher than run-of-the-mill application software. This presentation will review a set of techniques for producing and qualifying such components. It will conclude with a discussion proof issues and techniques for proving programs with extensive use of pointers, as O-O components often do. This is a fairly technical presentation that assumes good knowledge of O-O and component techniques and some prior exposure to formal techniques.	component-based software engineering;formal methods;pointer (computer programming)	Bertrand Meyer	2001		10.1109/TOOLS.2001.10024	direct anonymous attestation;trusted network connect	SE	-50.15397554080111	38.23629759635673	14376
4bc9193a068f5b77dcb7054369f79b2ede701840	jaca-mm: a user-centric bdi multiagent communication framework applied for negotiating and scheduling multi-participant events - a jason/cartago extension framework for diary scheduling events permitting a hybrid combination of multimodal devices based on a microservices architecture		In this research work, we present a novel BDI (Belief-Desire-Intention) multiagent software architecture for registering and scheduling multi-participant events under an automatic and semi-automatic negotiating process in a BDI multiagent context. Interactions between users and software agents are performed using a user-centric combination of multimodal devices including traditional GUI software for PC or Web, and modern omnipresent mobile and wearable devices. The communication framework is an extension of the JaCa (Jason/Cartago) Platform for permitting multimodal interaction between BDI agents and users over an SOA microservices architecture. Most work on multiagent software is centred on traditional software architectures and devices like PCs. However, web interfaces and mobile and wearables devices are nearest to users having sufficient computing resources, including CPU, memory sizes, and multimodal capabilities, for permitting a richer human-software agent interaction.	agent-based model;algorithm;central processing unit;input/output;internet;jason;microservices;multi-agent system;multimodal interaction;personal computer;scheduling (computing);semiconductor industry;software agent;software architecture;software deployment;user agent;user experience design;wearable computer;wearable technology;widget toolkit;world wide web	Juan Luis López Herrera;Homero V. Ríos-Figueroa	2018		10.5220/0006751703180330	architecture;computer science;data mining;microservices;scheduling (computing);negotiation;user-centered design;distributed computing	AI	-40.14470470635251	45.064474656003846	14460
a25cf37674cd6e58e55db6882b1c7cc13fe464bd	méthodologie de provisionnement automatique d'applications métier orientées service sur les environnements cloud. (method for automated provisioning of service-oriented cloud business applications)		Service-oriented computing and cloud computing offer many opportunities for developing and deploying applications. On the one hand, service-oriented computing allows to compose several functionalities from distributed services developed by different organizations. On the other hand, cloud computing allows to provision on demand scalable development and deployment environments. In this research work, we propose and describe a Methodology for AutomateD prOvisioning of cloud-based service-oriented busiNess Applications (MADONA). MADONA covers the whole application’s lifecycle and is based on cloud orchestration tools that manage the deployment, the configuration, and the composition of business services. Our objective is to propose a methodology to automatically provision serviceoriented applications while reducing the necessary technical knowledge required from the user. To this end, we bring three major contributions. Firstly, our system automates the whole application provisioning. In fact, MADONA phases are fully automated. The user intervenes only in requirement elicitation and when the application is deployed and ready to use. The business application is automatically generated by composing business services, and deployed in an automatically preselected IaaS. Secondly, we enrich the description of services by integrating concepts describing services’ interactions. Service description languages usually describe services as isolated components and do not consider the interactions between services. We define composition interactions which describe for each business service, its necessary services and the services with which it can be composed. Thirdly, we allow the user to express her requirements abstracting composition and deployment technical details. To this end, we define a RequIrement VocAbuLary (RIVAL) to formalize user’s functional and non-functional requirements. The methodology has been implemented and evaluated qualitatively and quantitatively following several scenarios showing its faisability.		Hind Benfenatki	2016				Web+IR	-45.52417707634289	42.04201462757771	14495
ae591492d5da2f23aac3d903905f932818ef420c	agents and peer-to-peer computing: a promising combination of paradigms	web based applications;p2p;information sharing;service model;client server;peer to peer computing	P2P personal information-sharing services will see explosive growth, reaching 35% of all online users by 2006. Once personal P2P applications are common and the infrastructure supports them, computing will change. Developers of Web-based applications will realize that adding P2P functionality makes their apps come alive -user communication is the secret sauce for enhancing client- server applications.		Gianluca Moro;Aris M. Ouksel;Claudio Sartori	2002		10.1007/3-540-45074-2_1	computer science;database;distributed computing;utility computing;world wide web	NLP	-36.520918137113895	49.11771517723789	14565
30a3c3d2572b23aa8ee782d13db3eaff616211a9	open code coverage framework: a consistent and flexible framework for measuring test coverage supporting multiple programming languages	java software syntactics software measurement semantics testing;software;software testing;measurement tool;multiple programming languages;open code coverage framework;programming language;software measurement;framework software testing test coverage code coverage metrics;code coverage;semantics;metrics;testing;abstract syntax tree;test coverage measurement;lines of code;program testing;syntactics;test coverage;multiple programming languages open code coverage framework test coverage measurement;framework;java	Test coverage is an important indicator of whether software has been tested sufficiently. However, existing measurement tools for test coverage are associated with several problems such as their cost of development and maintenance, inconsistency and inflexibility in measurement. We propose a framework for consistent and flexible measurement of test coverage, called the Open Code Coverage Framework (OCCF), that supports multiple programming languages. OCCF extracts commonalities from multiple programming languages focusing on only small syntax differences in programming languages using an abstract syntax tree. OCCF provides guidelines to support several test coverage criteria. Moreover, OCCF let users expand features to add user-defined test coverage and new programming language. As a result, we reduced the lines of code required to implement measurement tools for test coverage by about 90%and the time to implement a special coverage criterion by80% or more in an experiment that compared OCCF with conventional tools developed individually without using the framework.	abstract syntax tree;code coverage;code refactoring;fault coverage;functional programming;language-independent specification;pl/i;parse tree;programming language;source lines of code;test case	Kazunori Sakamoto;Hironori Washizaki;Yoshiaki Fukazawa	2010	2010 10th International Conference on Quality Software	10.1109/QSIC.2010.42	real-time computing;computer science;software engineering;database;semantics;software testing;code coverage;programming language	SE	-55.58528403016371	32.51860017160984	14647
c898ed9e010d7b7f89820065780488231baa4842	collective awareness and action in urban superorganisms	collective urban awareness urban superorganisms ict devices ict technologies large scale socio technical superorganism;ubiquitous computing middleware socio economic effects;ubiquitous computing;middleware;middleware cyber physical convergence pervasive computing self organization;computer architecture urban areas middleware robot sensing systems cities and towns social network services;socio economic effects	Future urban scenarios will be characterized by the close integration of ICT devices and humans. Citizens using their own capabilities integrated with ICT technologies could collaboratively constitute a large-scale socio-technical superorganism to support collective urban awareness and activities. This position paper, with the help of a representative case study, identifies the key challenges for future urban superorganisms and proposes a two-tier architecture to support their development.	middleware;multitier architecture;prototype;sociotechnical system;superorganism	Nicola Bicocchi;Damiano Fontana;Marco Mamei;Franco Zambonelli	2013	2013 IEEE International Conference on Communications Workshops (ICC)	10.1109/ICCW.2013.6649227	simulation;computer science;knowledge management;operating system;middleware;computer security;ubiquitous computing	Robotics	-43.20415221564105	47.33724991345766	14680
8ec6d736af130db838370feb24a4e67a0c6e3b9f	towards demonstrably correct compilation of java byte code	java bytecode;formal specification;tool support;b method	In this paper we investigate the feasibility of a demonstrably correct compiler for Java bytecode. We first examine the suitability of adapting the existing high assurance compiler DeCCo for the Pascal-like language PASP, based on a Z formalisation of the compiler manually transcribed to Prolog. During the investigation we have uncovered several problematic issues and argue that these can be avoided by formally deriving the code of the compiler from the formal specification, rather than manually transcribing it. We have conducted a case study, developing a compiler for a subset of Java bytecode to an idealised RISC processor using the B-method. We show that refinement is a natural way to model compilation and that the B-method can in principle be used to develop a demonstrably correct compiler. In particular, the tool support for B turned out to be extremely valuable: animation, automated refinement checking, and proof each uncovered a series of mistakes.	ahead-of-time compilation;byte;java bytecode	Michael Leuschel	2008		10.1007/978-3-642-04167-9_7	b-method;computer architecture;compiler;real-time computing;dynamic compilation;compiler correctness;computer science;java modeling language;compiler construction;formal specification;bootstrapping;compilation error;programming language;java;functional compiler	Arch	-56.75092558742938	38.626901406190846	14687
a45dcc1e1b240996fcc5c8b1857143f027c4ab0a	reducing masking effects in combinatorialinteraction testing: a feedback drivenadaptive approach	adaptive testing;covering arrays;electronic mail;software quality assurance;testing and debugging;software systems;software fault tolerance;q science general;testing;testing adaptive arrays educational institutions scalability servers electronic mail software systems;public domain software;servers;program testing;combinatorial testing;software fault tolerance program testing public domain software;testing tools;adaptive arrays;testing strategies;masking effects reduction perfect test scenarios fault location fault detection error locating arrays open source software systems fault characterization approaches potential masking effects detection t way combinations fda cit process interaction testing criterion system execution test case failures covering arrays software systems configuration spaces feedback driven adaptive approach cit approach combinatorial interaction testing;scalability;software quality assurance combinatorial testing adaptive testing covering arrays;test design	The configuration spaces of modern software systems are too large to test exhaustively. Combinatorial interaction testing (CIT) approaches, such as covering arrays, systematically sample the configuration space and test only the selected configurations. The basic justification for CIT approaches is that they can cost-effectively exercise all system behaviors caused by the settings of t or fewer options. We conjecture, however, that in practice some of these behaviors are not actually tested because of unanticipated masking effects - test case failures that perturb system execution so as to prevent some behaviors from being exercised. While prior research has identified this problem, most solutions require knowing the masking effects a priori. In practice this is impractical, if not impossible. In this work, we reduce the harmful consequences of masking effects. First we define a novel interaction testing criterion, which aims to ensure that each test case has a fair chance to test all valid t-way combinations of option settings. We then introduce a feedback driven adaptive combinatorial testing process (FDA-CIT) to materialize this criterion in practice. At each iteration of FDA-CIT, we detect potential masking effects, heuristically isolate their likely causes (i.e., fault characterization), and then generate new samples that allow previously masked combinations to be tested in configurations that avoid the likely failure causes. The iterations end when the new interaction testing criterion has been satisfied. This paper compares two different fault characterization approaches - an integral part of the proposed approach, and empirically assesses their effectiveness and efficiency in removing masking effects on two widely used open source software systems. It also compares FDA-CIT against error locating arrays, a state of the art approach for detecting and locating failures. Furthermore, the scalability of the proposed approach is evaluated by comparing it with perfect test scenarios, in which all masking effects are known a priori. Our results suggest that masking effects do exist in practice, and that our approach provides a promising and efficient way to work around them, without requiring that masking effects be known a priori.	cit program tumor identity cards;failure cause;floor and ceiling functions;heuristic;iteration;open-source software;perturbation theory;psychoacoustics;scalability;sensor;software system;test case	Cemal Yilmaz;Emine Dumlu;Myra B. Cohen;Adam A. Porter	2014	IEEE Transactions on Software Engineering	10.1109/TSE.2013.53	reliability engineering;embedded system;real-time computing;scalability;computer science;engineering;operating system;software engineering;software testing;computerized adaptive testing;test design;public domain software;software fault tolerance;server;statistics;software system	SE	-61.19590930594735	37.36604785034764	14786
ce4d5e1a9afb41f3a65aad002be0af84b183f9b2	security automation for information security continuous monitoring: research framework	information security;service monitoring and tracking;security automation;continuous monitoring	Although automation is already an integral part of many cyber security operations, there still are challenges to overcome to fully achieve Information Security Continuous Monitoring (ISCM) capabilities: real-time threat detection, incident response and risk-based decision making capabilities. Our ongoing research seeks to further refine our framework to enhance ISCM capabilities by leveraging security automation.	automation;computer security incident management;floor and ceiling functions;information security;real-time clock;threat (computer)	Tina Alsadhan;Joon Sung Park	2016	2016 IEEE World Congress on Services (SERVICES)	10.1109/SERVICES.2016.28	control system security;information security audit;computer security model;standard of good practice;cloud computing security;security through obscurity;security information and event management;engineering;knowledge management;information security;data mining;security service;security testing;computer security;information security management	EDA	-56.721737542232454	49.44640451895822	14853
0407494d6a015315b257b5de4ac096551b2988e8	automated client-side monitoring for web applications	instruments;functional testing;computer crashes;client side code testing;application software;prototypes;web service web application automated client side monitoring web browser configuration client side code testing client side code debugging;system monitoring;testing;general techniques;web service;runtime;web applications;online front ends;computerized monitoring;html;servers;program testing;web services online front ends program debugging program testing system monitoring;monitoring;automated client side monitoring;web application;displays;web services;computerized monitoring application software testing runtime remote monitoring computer crashes uniform resource locators space technology displays html;web browser configuration;client side code debugging;remote monitoring javascript web applications;space technology;remote monitoring;program debugging;uniform resource locators;javascript;conferences	Web applications have become very popular today in a variety of domains. Given the varied nature of client-side environments and browser configurations, it is difficult to completely test or debug the client-side code of web applications in-house. There are tools that facilitate functional testing on various browsers, but they cannot mimic all of the possible client-side environments. In modern web browsers, the client-side code can interact with numerous web services to get more data and even to update itself, which can in turn affect the behavior of the client in unforeseen ways. In these situations, monitoring the client-side code allows for gathering valuable runtime information about its behavior. In this paper, we propose a general technique for performing such monitoring. We also present a preliminary evaluation of the technique where we discuss its efficiency, effectiveness, and possible application scenarios.	client-side;functional testing;html;prototype;run time (program lifecycle phase);web application;web service;web worker	Shauvik Roy Choudhary;Alessandro Orso	2009	2009 International Conference on Software Testing, Verification, and Validation Workshops	10.1109/ICSTW.2009.44	web service;web application;web modeling;computer science;operating system;web navigation;database;web engineering;world wide web	SE	-54.33763522361022	37.99566506506849	14854
dd49b65d216d6a5eb3cca9e53bf51c4b73954bb7	anti-doping: an approach for grid integrity verification	distributed data;computer network security;data integrity;information security;anti doping;data security grid computing authentication testing computer security privacy resource management information security doping computer networks;authentication;resource management;testing;distributed data integrity;null;data distribution;computer networks;computer network;computer security;doping;telecommunication security;distributed data integrity anti doping grid integrity verification computer network security authentication access control privacy nonrepudiation grid computing authorization;authorization;access control;nonrepudiation;grid computing;computer networks grid computing telecommunication security security of data data integrity;security of data;privacy;grid integrity verification;data security	Security in traditional computer networks requires authentication, access control, integrity, privacy and non-repudiation, which can be also be applied to assure security in grid computing. Work related to authentication, authorization, and privacy in grids has been presented in the literature. However, there is a need to guarantee to grid users and administrators that grid machines are not corrupting or modifying data distributed in the shared resources. Thus, the paper proposes an approach to verify distributed data integrity in grid machines. This approach applies an algorithm to test resources in a grid, which provides more security and also makes possible the integrity verification of external entities.	access control;algorithm;authentication;authorization;data integrity;doping (semiconductor);entity;grid computing;non-repudiation;privacy;verification and validation	Rodrigo Almeida dos Santos;Mario Fiallos Aguilar;Paulo Benicio Melo de Sousa;Joao C. Sousa;Rossana M. de Castro Andrade	2005	Advanced Industrial Conference on Telecommunications/Service Assurance with Partial and Intermittent Resources Conference/E-Learning on Telecommunications Workshop (AICT/SAPIR/ELETE'05)	10.1109/AICT.2005.27	non-repudiation;semantic grid;computer science;information security;access control;resource management;data integrity;data grid;authentication;database;software testing;data security;authorization;internet privacy;doping;privacy;computer security;grid computing	HPC	-46.61392752380732	55.97749705023065	14878
75e8bd79d75e174be709da377f6adc05403d27a7	parsifal: a pragmatic solution to the binary parsing problems	protocols;standards;preprocessor;code generation;internet;parsifal;density estimation robust algorithm;preprocessor parsifal binary parsers code generation ocaml;writing;robustness;binary parsers;containers robustness protocols internet density estimation robust algorithm writing standards;ocaml;program compilers;parser robustness binary parsing problems pervasive software basic blocks parsifal framework ocaml parser format complexity;containers	Parsers are pervasive software basic blocks: as soon as a program needs to communicate with another program or to read a file, a parser is involved. However, writing robust parsers can be difficult, as is revealed by the amount of bugs and vulnerabilities related to programming errors in parsers. It is especially true for network analysis tools, which led the network and protocols laboratory of the French Network and Information Security Agency (ANSSI) to write custom tools. One of them, Parsifal, is a generic framework to describe parsers in OCaml, and gave us some insight into binary formats and parsers. After describing our tool, this article presents some use cases and lessons we learned about format complexity, parser robustness and the role the language used played.	basic block;information security;network theory;ocaml;parsing;software bug;vulnerability (computing)	Olivier Levillain	2014	2014 IEEE Security and Privacy Workshops	10.1109/SPW.2014.35	communications protocol;the internet;computer science;theoretical computer science;database;programming language;writing;preprocessor;code generation;robustness	Security	-57.89740696877459	55.322422064667094	14910
1affdd5708cb13a83e736700a236e3f2d8e8c10d	improving multi-objective code-smells correction using development history	refactoring;search based software engineering;code smells	One of the widely used techniques to improve the quality of software systems is refactoring. Software refactoring improves the internal structure of the system while preserving its external behavior. These two concerns drive the existing approaches to refactoring automation. However, recent studies demonstrated that these concerns are not enough to produce correct and consistent refactoring solutions. In addition to quality improvement and behavior preservation, studies consider, among others, construct semantics preservation and minimization of changes. From another perspective, development history was proven as a powerful source of knowledge in many maintenance tasks. Still, development history is not widely explored in the context of automated software refactoring. In this paper, we use the development history collected from existing software projects to propose new refactoring solutions taking into account context similarity with situations seen in the past. We propose a multi-objective optimization-based approach to find good refactoring sequences that (1) minimize the number of code-smells, and (2) maximize the use of development history while (3) preserving the construct semantics. To this end, we use the non-dominated sorting genetic algorithm (NSGA-II) to find the best trade-offs between these three objectives. We evaluate our approach using a benchmark composed of five medium and large-size open-source systems and four types of codesmells (Blob, spaghetti code, functional decomposition, and data class). Our experimental results show the effectiveness of our approach, compared to three different state-of-the-art approaches, with more than 85% of code-smells fixed and 86% of suggested refactorings semantically coherent when the change history is	benchmark (computing);code refactoring;code smell;coherence (physics);genetic algorithm;mathematical optimization;multi-objective optimization;object file;open-source software;software system;sorting;spaghetti code	Ali Ouni;Marouane Kessentini;Houari A. Sahraoui;Katsuro Inoue;Mohamed Salah Hamdi	2015	Journal of Systems and Software	10.1016/j.jss.2015.03.040	reliability engineering;search-based software engineering;computer science;systems engineering;software engineering;data mining;programming language;code refactoring;code smell	SE	-58.18635580088902	34.503062759000905	14911
7096e4dbb8cf25f2145bce571f144716515057de	performance signatures: a qualitative approach to dependency guidance		This paper describes Performance Signatures, a simple qualitative approach to helping software developers improve their products’ performance. Performance Signatures allow development tools to give as-you-type prescriptive guidance and facilitate improved analysis and interpretation during traditional profiling sessions. Performance Signatures emphasize ease of adoption and prevention of common and/or large mistakes.	computation;electronic signature;library (computing);nat friedman;programmer;programming tool;real-time computing;software developer;vocabulary	Rico Mariani	2006			data mining;world wide web;computer science	SE	-55.82824576411615	39.1210744183729	14931
578dcb0dbf67140f8a0241f32d2d742d94acbdb3	adaptive and flexible smartphone power modeling	adaptive power modeling;data collection;smart power management;mobile devices;security applications	Mobile devices have become the main interaction mean between users and the surrounding environment. An indirect measure of this trend is the increasing amount of security threats against mobile devices, which in turn created a demand for protection tools. Protection tools, unfortunately, add an additional burden for the smartphone’s battery power, which is a precious resource. This observation motivates the need for smarter (security) applications, designed and capable of running within adaptive energy goals. Although this problem affects other areas, in the security area this research direction is referred to as “green security”. In general, a fundamental need to the researches toward creating energy-aware applications, consist in having appropriate power models that capture the full dynamic of devices and users. This is not an easy task because of the highly dynamic environment and usage habits. In practice, this goal requires easy mechanisms to measure the power consumption and approaches to create accurate models. The A. A. Nacci · F. Trovò · F. Maggi · M. Ferroni · A. Cazzola · D. Sciuto · M. D. Santambrogio ( ) Politecnico di Milano, Milano, Italy e-mail: marco.santambrogio@polimi.it A. A. Nacci e-mail: nacci@elet.polimi.it F. Trovò e-mail: trovo@elet.polimi.it F. Maggi e-mail: maggi@elet.polimi.it M. Ferroni e-mail: matteo.ferroni@mail.polimi.it A. Cazzola e-mail: andrea.cazzola@mail.polimi.it D. Sciuto e-mail: sciuto@elet.polimi.it existing approaches that tackle this problem are either not accurate or not applicable in practice due to their limiting requirements. We propose MPower, a power-sensing platform and adaptive power modeling platform for Android mobile devices. The MPower approach creates an adequate and precise knowledge base of the power “behavior” of several different devices and users, which allows us to create better device-centric power models that considers the main hardware components and how they contributed to the overall power consumption. In this paper we consolidate our perspective work on MPower by providing the implementation details and evaluation on 278 users and about 22.5 million power-related data. Also, we explain how MPower is useful in those scenarios where low-power, unobtrusive, accurate power modeling is necessary (e.g., green security applications).	android;client-side;device driver;email;knowledge base;low-power broadcasting;mobile device;requirement;smartphone;unobtrusive javascript	A. A. Nacci;Francesco Trovò;F. Maggi;Matteo Ferroni;Andrea Cazzola;Donatella Sciuto;Marco D. Santambrogio	2013	MONET	10.1007/s11036-013-0470-y	embedded system;simulation;telecommunications;computer science;operating system;mobile device;computer security;computer network;data collection	Mobile	-39.706269842789965	56.47100721360955	15010
dbfaf1710936321d316e7491696cd9cc7818970c	a speech-act negotiation protocol: design, implementation, and test use	electronic mail;organizational computing systems;building block;speech acts;negotiation protocol;distributed artificial intelligence;speech act theory;negotiation	Existing negotiation protocols used in Distributed Artificial Intelligence (DAI) systems rarely take into account the results from negotiation research. We propose a negotiation protocol, SANP (Speech-Act-based Negotiation Protocol), which is based on Ballmer and Brennenstuhl's speech act classification and on negotiation analysis literature. The protocol is implemented as a domain-independent system using Strudel, which is an electronic mail toolkit. A small study tested the potential use of the protocol. Although a number of limitations were found in the study, the protocol appears to have potential in domains without these limitations, and it can serve as a building block to design more general negotiation protocols.	distributed artificial intelligence;email	Man Kit Chang;Carson C. Woo	1994	ACM Trans. Inf. Syst.	10.1145/185462.185477	computer science;knowledge management;computer security;negotiation	Security	-43.518042840067984	56.215971191103726	15014
6a65a4cb9c0b899563b8779af874e407dee8c735	avoiding sensitive data disclosure: android system design and development data leaks detection thesis master degree computer engineering		The data leaks problem is a security key issue in the worldwide connection, communication and interoperability functions among the huge number of mobile devices, especially for apps where sensitive data are exchanged. In order to tackle this dangerous problem, an innovative and powerful tool, named Android JADAL (JAva DAta Leak), has been developed based on hybrid approach which combines both static and dynamic code analysis techniques. This tool has been validated successfully by means of significant tests carried out on sensitive data leak applications. During the test, the user receives notification when sensible data is caught during the execution of apps. The design and testing activities have been conducted in cooperation with the Mathematics and Computer Science Department of Catania University.	computer engineering	Vincenzo Pomona	2016		10.1007/978-3-319-70578-1_14	systems design;interoperability;android (operating system);reverse engineering;computer science;computer engineering;jar;dynamic program analysis;mobile device;java	SE	-59.833954759874324	55.8054154008514	15036
3bdc716d8cd62ff2ce0e516c8726bb87d23fa96f	formal modelling and verification of pervasive computing systems	004;system analysis formal modelling and verification reliability analysis	Pervasive computing (PvC) systems are emerging as promising solutions to many practical problems, e.g., elderly care in home. However, such systems have long been developed without sufficient verification. Formal methods, eps. model checking are sound techniques for complex system analysis regarding correctness and reliability requirements. In this work, a formal modelling framework is proposed to model the general the system design (e.g., concurrent communications) and the critical environment inputs (e.g., human behaviours). Correctness requirements are specified in formal logics which are automatically verifiable against a system model. Furthermore, Markov Decision Processes (MDPs) are adopted for modelling probabilistic behaviours of PvC systems. Three problems are analysed which are overall reliability prediction based on component reliabilities, reliability distribution w.r.t., how reliable should the component be to reach an overall reliability requirement and sensitivity analysis w.r.t., how does a component reliability affect the overall reliability. Finally, the usefulness of our approaches are demonstrated on a smart healthcare system with unexpected bugs and system flaws exposed. 1998 ACM Subject Classification D.2.4 Software/Program Verification	complex system;correctness (computer science);encapsulated postscript;formal methods;formal verification;markov chain;markov decision process;model checking;reliability engineering;requirement;software bug;system analysis;systems design;ubiquitous computing	Yan Liu	2013		10.4230/OASIcs.FSFMA.2013.61	real-time computing;formal verification;computer science	SE	-44.38694384151734	35.478570823688	15064
2ce2d7a7338eb4be79123cfa720c638a8738c9a4	implementing refactorings in intellij idea	program transformation;injection attack;input rectification policy;extraction method	IntelliJ IDEA was one of the first Java IDEs to cross the Refactoring Rubicon [1], by implementing the Extract Method refactoring for Java in early 2001. Since that time, IntelliJ IDEA has evolved greatly to support a wide array of refactorings for Java, cross-language refactoring and other advanced features. This paper gives an overview of the key architectural components of IntelliJ IDEA involved in implementing refactorings. It also describes some of the problems we're facing when implementing refactorings and possible directions for future development.	code refactoring;intellij idea;java	Dmitry Jemerov	2008		10.1145/1636642.1636655	real-time computing;computer science;systems engineering;programming language	PL	-53.31914398210114	33.494957748559294	15129
9b98ec15ae29abab1503351e4b375acb6fabdfde	bpel orchestration of secure webmail	verification;webmail;delivery system;web service;smtp misuse cases;wsemail;bpel;smtp use cases;use case	WebMail proposes to migrate existing SMTP-based mail systems to Web-Services. We show how a verifiably-correct, generic mail service that enables extensions of SMTP-based standard mail use cases that avoids known misuse cases can be specified using WSDL and orchestrated using BPEL.	business process execution language;correctness (computer science);email;formal specification;internet privacy;misuse case;secure transmission;web services description language;web service;webmail	Saket Kaushik;Duminda Wijesekera;Paul Ammann	2006		10.1145/1180367.1180383	use case;web service;verification;business process execution language;computer science;database;law;world wide web;computer security	Security	-50.218642858864335	54.86944439759155	15157
9a1329e2edcc24cd0c3d51860f82b97e6da0e9e1	detecting wearable app permission mismatches: a case study on android wear		Wearable devices are becoming increasingly popular. These wearable devices run what is known as wearable apps. Wearable apps are packaged with handheld apps, that must be installed on the accompanying handheld device (e.g., phone).   Given that wearable apps are tightly coupled with the handheld apps, any wearable permission must also be requested in the handheld version of the app on the Android Wear platform. However, in some cases, the wearable apps may request permissions that do not exist in the handheld app, resulting in a permission mismatch, and causing the wearable app to error or crash. In this paper, we propose a technique to detect wear app permission mismatches. We perform a case study on 2,409 free Android Wear apps and find that 73 released wearable apps suffer from the permission mismatch problem.	android wear;crash (computing);handheld game console;mobile device;play store;requirement;wearable computer;wearable technology	Suhaib Mujahid	2017		10.1145/3106237.3121279	android wear;computer science;wearable technology;embedded system;wearable computer;internet privacy;mobile device;permission;crash	Mobile	-54.64889032143101	42.74626829037336	15174
727c7d5d914f049141e104498a07f79f8f9098d8	assessing agents interaction quality via multi-agent runtime verification	runtime verification;software quality assurance;multi agent systems;agents interaction;message passing	Interaction between agents is the main process executed within multi-agent systems. Agents autonomously and proactively interact with each other to perform tasks. However, there are possibilities that during execution interaction could fail due to external factors such as modified user inputs, inactive hosts and compromised network. Based on this observation, it is important to assess the quality of interaction process during multi-agent systems execution in order for the systems to be continuously trusted to perform critical tasks and improved in terms of its security and reliability from time to time. Thus, in this research, a solution called Multi-agent Runtime Verification framework is proposed to assess agents interaction quality. In this paper, the availability and trustability metrics are addressed based on supporting contextual information. The verification process assesses the quality of the messages transmitted during interaction by assigning scores for each of the defined metrics. The scores are used to determine whether the communication is going to fail or succeed. The framework aims to reduce agents interaction failure during runtime. Finally, an experiment is set up to evaluate the effectiveness of the proposed solution.		Najwa Abu Bakar;Ali Selamat	2013		10.1007/978-3-642-40495-5_18	message passing;real-time computing;computer science;artificial intelligence;multi-agent system;distributed computing;runtime verification	AI	-40.95018404996333	36.648283739887184	15177
8968010b4f767819371146e98a7ea71809cb885a	macroprogramming spatio-temporal event detection and data collection in wireless sensor networks: an implementation and evaluation study	event detection wireless sensor networks base stations abstracts runtime environment computer science monitoring spatiotemporal phenomena fires petroleum;data collection;telecommunication computing;event detection;space time;three dimensional;wireless sensor network;integrated abstraction macroprogramming spatio temporal event detection data collection wireless sensor networks spacetime oriented programming;temporal resolution;sensor nodes;wireless sensor networks data acquisition programming telecommunication computing;programming;data acquisition;evaluation studies;wireless sensor networks	This paper proposes and evaluates a spatio-temporal macroprogramming paradigm for wireless sensor networks (WSNs). The proposed paradigm, called SpaceTime oriented programming (STOP), is designed to reduce the complexity to program event detection and data collection by specifying them from a global viewpoint as a whole rather than a viewpoint of sensor nodes as individuals. STOP treats space and time as first-class citizens and combines them as space- time continuum. A spacetime is a three dimensional object that consists of a two spatial dimensions and a time playing the role of the third dimension. STOP allows application developers to program event detection and data collection to spacetime, and abstracts away the details in WSNs. The notion of spacetime provides an integrated abstraction to seamlessly express event detection and data collection for both the past and future in arbitrary spatio-temporal resolutions. This paper describes the implementation of STOP, and evaluates the performance of applications built with STOP.	algorithm;microcode;network packet;programming paradigm;runtime system;three-dimensional integrated circuit;triune continuum paradigm	Hiroshi Wada;Pruet Boonma;Junichi Suzuki	2008	Proceedings of the 41st Annual Hawaii International Conference on System Sciences (HICSS 2008)	10.1109/HICSS.2008.237	real-time computing;wireless sensor network;computer science;artificial intelligence;theoretical computer science;operating system;database;distributed computing;key distribution in wireless sensor networks	Mobile	-37.00466732492999	45.71772101003961	15255
7b8d7178a1acf646b1919f236d80a30347a5ab0e	development of a cloud-computing-based equipment monitoring system for machine tool industry	production engineering computing;computational modeling;graphical user interfaces;computerised numerical control;monitoring;service oriented architecture cloud computing computerised monitoring computerised numerical control fault diagnosis graphical user interfaces machine tools production engineering computing quality control;testing operations cloud computing based equipment monitoring system ccems cnc machine tool industry graphical user interface equipment faults production quality nwaif service oriented architecture soa rich internet application technology ria web based gui;predictive models;machine tools;computerised monitoring;service oriented architecture;graphical user interfaces data models cloud computing monitoring predictive models computational modeling data acquisition;quality control;data acquisition;fault diagnosis;cloud computing;data models	This paper aims to present the design of a cloud computing-based equipment monitoring system (EMS), called CCEMS, for the CNC machine tool industry to illustrate the paradigm shift of EMSs from basing on the Internet to basing on the cloud. In the CCEMS, Graphical User Interface (GUI) plays an important role that allows users to interact with the system for controlling and operating equipment, monitoring equipment performance and statuses, detecting and diagnosing equipment faults, conjecturing production quality and precision of equipment, and so on. To overcome the shortcomings of traditional Web GUIs, this paper proposes a novel Web application implementation framework, called NWAIF, for the CCEMS, by using Service-Oriented Architecture (SOA) and Rich Internet Application (RIA) technology. Finally, a paradigm CCEMS for monitoring CNC machine tools is constructed and used to validate the efficacy of the CCEMS and NWAIF. Testing results show that the paradigm CCEMS meets the desired functional requirements and the GUIs created by using the proposed NWAIF is superior to traditional Web-based GUIs in terms of shorter response times to complete the testing operations.	cloud computing;functional requirement;graphical user interface;model–view–controller;programming paradigm;rich internet application;sensor;service-oriented architecture;service-oriented device architecture;web application	Min-Hsiung Hung;Yu-Chuan Lin;Tran Quoc Huy;Haw Ching Yang;Fan-Tien Cheng	2012	2012 IEEE International Conference on Automation Science and Engineering (CASE)	10.1109/CoASE.2012.6386500	embedded system;real-time computing;engineering;computer engineering	Embedded	-47.9683229894737	40.84321096167268	15368
fd040b13dd512838a298a330a59618c806874c6d	do null-type mutation operators help prevent null-type faults?		The null-type is a major source of faults in Java programs, and its overuse has a severe impact on software maintenance. Unfortunately traditional mutation testing operators do not cover null-type faults by default, hence cannot be used as a preventive measure. We address this problem by designing four new mutation operators which model null-type faults explicitly. We show how these mutation operators are capable of revealing the missing tests, and we demonstrate that these mutation operators are useful in practice. For the latter, we analyze the test suites of 15 open-source projects to describe the trade-offs related to the adoption of these operators to strengthen the test suite.		Ali Parsai;Serge Demeyer	2019		10.1007/978-3-030-10801-4_33	programming language;operator (computer programming);discrete mathematics;computer science;mutation testing;software maintenance;new mutation;java;test suite;mutation	HCI	-62.10041936450791	35.777979863816	15373
222a2b767d58f62e36611e6c9e523c4d9d49badc	tracing with a minimal number of probes	program diagnostics;probe placement algorithm program execution execution overhead control flow graph program trace reconstruction minimal vertex probe placement profile execution;radiation detectors;resource management;frequency measurement;probes;probes frequency measurement radiation detectors resource management conferences real time systems;conferences;real time systems	When a program execution must be observed, probes are injected to trace its execution. Probes are intrusive, causing execution overhead and modifying a program's real time properties. It is therefore desirable to place them efficiently, so that their negative effects are mitigated. This paper presents a new algorithm for efficiently allocating probes at vertices in the control flow graph of a program in a way that makes it possible to reconstruct program traces. Despite of the intractability of finding minimal vertex probe placement, a fact proven in this paper, the presented algorithm is capable of identifying a locally minimal set of vertices, which reduces the set of needed probes beyond the current state-of-the-art. First, the paper shows the advantages of placing probes at vertices, rather than edges. Next, the known methods for reducing the set of probes needed to profile execution are proven to be applicable to reduce the set needed to trace execution. Further, the new algorithm is described along with the algorithm for reconstructing the executed path from a trace file. In addition, the new algorithm is combined with methods for placing profiling probes to reduce the execution overhead even further. Finally, the new probe placement algorithm is compared with known algorithms to elaborate on its benefits.	algorithm;code coverage;code injection;computational complexity theory;control flow graph;experiment;np-completeness;overhead (computing);profiling (computer programming);software deployment;tracing (software);vertex (geometry)	David Baca	2013	2013 IEEE 13th International Working Conference on Source Code Analysis and Manipulation (SCAM)	10.1109/SCAM.2013.6648187	parallel computing;real-time computing;computer science;resource management;distributed computing;particle detector	EDA	-60.66492123622316	37.91076050186727	15413
3f2d06f7e112f5bd94f547271d02bf783578c50e	polychronous modeling, analysis, verification and simulation for timed software architectures	simulink;formal analysis and verification;aadl;s ignal;timing modeling;p olychrony;polychrony;model driven engineering;signal	High-level modeling languages and standards, such as Simulink, SysML, MARTE and AADL (Architecture Analysis & Design Language), are increasingly adopted in the design of embedded systems so that system-level analysis, verification and validation (V&V) and architecture exploration are carried out as early as possible. This paper presents our main contribution in this aim by considering embedded systems architectural modeling in AADL and functional modeling in Simulink; an original clock-based timing analysis and validation of the overall system is achieved via a formal polychronous/multi-clock model of computation. In order to avoid semantics ambiguities of AADL and Simulink, their features related to real-time and logical time properties are first studied. We then endue them with a semantics in the polychronous model of computation. We use this model of computation to jointly analyze the non-functional real-time and logical-time properties of the system (by means of logical and affine clock relations). Our approach demonstrates, through several case-studies conducted with Airbus and C-S Toulouse in the European projects CESAR and OPEES, how to cope with the system-level timing verification and validation of high-level AADL and Simulink components in the framework of Polychrony, a synchronous modeling framework dedicated to the design of safety-critical embedded systems.	architecture analysis & design language;embedded system;high- and low-level;model of computation;modeling and analysis of real time and embedded systems;real-time clock;signal (programming language);simulation;simulink;static timing analysis;systems modeling language;verification and validation	Huafeng Yu;Yue Ma;Thierry Gautier;Loïc Besnard;Paul Le Guernic;Jean-Pierre Talpin	2013	Journal of Systems Architecture - Embedded Systems Design	10.1016/j.sysarc.2013.08.004	embedded system;model-driven architecture;computer architecture;real-time computing;computer science;software engineering;signal	Embedded	-40.463143508666896	32.82660207405195	15475
44c63b9f7e85e0bb034943b72a96a119ff852e9e	a privacy-preserving qos prediction framework for web service recommendation	collaboration;会议论文;privacy preservation;servers;data privacy;collaborative filtering;web services;qos prediction;qos dataset privacy preserving qos prediction framework qos based web service recommendation system high quality services collaborative filtering techniques personalized qos prediction qos values user privacy data obfuscation techniques;web services collaborative filtering data privacy quality of service recommender systems;predictive models;web service recommendation;quality of service;privacy preservation web service recommendation qos prediction collaborative filtering;privacy;quality of service web services collaboration data privacy privacy servers predictive models	QoS-based Web service recommendation has recently gained much attention for providing a promising way to help users find high-quality services. To facilitate such recommendations, existing studies suggest the use of collaborative filtering techniques for personalized QoS prediction. These approaches, by leveraging partially observed QoS values from users, can achieve high accuracy of QoS predictions on the unobserved ones. However, the requirement to collect users' QoS data likely puts user privacy at risk, thus making them unwilling to contribute their usage data to a Web service recommender system. As a result, privacy becomes a critical challenge in developing practical Web service recommender systems. In this paper, we make the first attempt to cope with the privacy concerns for Web service recommendation. Specifically, we propose a simple yet effective privacy-preserving framework by applying data obfuscation techniques, and further develop two representative privacy-preserving QoS prediction approaches under this framework. Evaluation results from a publicly-available QoS dataset of real-world Web services demonstrate the feasibility and effectiveness of our privacy-preserving QoS prediction approaches. We believe our work can serve as a good starting point to inspire more research efforts on privacy-preserving Web service recommendation.	collaborative filtering;experiment;personalization;privacy;quality of service;recommender system;usage data;web service	Jieming Zhu;Pinjia He;Zibin Zheng;Michael R. Lyu	2015	2015 IEEE International Conference on Web Services	10.1109/ICWS.2015.41	web service;mobile qos;quality of service;information privacy;computer science;collaborative filtering;data mining;predictive modelling;internet privacy;privacy;world wide web;computer security;server;collaboration	Web+IR	-40.569417711987484	57.63523083086347	15566
7a97b878a95fc5c196637be102293cb111ce1fe2	automated prioritization of metrics-based design flaws in uml class diagrams	engineering;software;measurement;motion pictures;methods;electrical electronic;software quality assurance;uml;metrics;software quality model based development uml class diagrams software quality assurance metrics software design flaws;software engineering;software design flaws;software validation automated prioritization metric based design flaws uml class diagrams software architecture software development prolongs software life cycle quality assurance sdmetrics metricview bx approach posdef;theory;unified modeling language;class diagrams;measurement unified modeling language software design motion pictures fault diagnosis educational institutions;model based development;computer science;unified modeling language program verification software architecture software quality;software design;software quality;fault diagnosis	The importance of software architecture in software development prolongs throughout the entire software life cycle. This is because quality of the architectural design defines the structural aspects of the system that are difficult to change, and hence will affect most of the subsequent development and maintenance activities. This paper considers software design flaws (related to the system structure) and not flaws identified at run time (by testing). These design flaws are akin to what is described in the literature as anti-patterns, bad smells or rotting design. Recently, two tools that have been developed for quality assurance of software designs represented in the UML notation: SDMetrics and Metric View. However these tools are not considered practical because they report many design flaws which are not considered by developers (false positives). This paper explores an approach that tries to identify which design flaws should be considered important and which are not. To this end, we propose an approach for automated prioritization of software design flaws (BX approach), to facilitate developers to focus on important design flaws more effectively. We designed and implemented a tool (PoSDef) that implements this approach. The BX approach and the PoSDef tool have been validated using two open source projects and one large industrial system. Our validation consists of comparing our approach and tool with the existing design flaw tools. The evaluation has shown that the proposed approach could facilitate developers to identify and prioritize important design flaws effectively.	anti-pattern;code smell;flaw hypothesis methodology;open-source software;run time (program lifecycle phase);software architecture;software design;software development;software release life cycle;unified modeling language	Michel R. V. Chaudron;Brian Katumba;Xuxin Ran	2014	2014 40th EUROMICRO Conference on Software Engineering and Advanced Applications	10.1109/SEAA.2014.82	reliability engineering;unified modeling language;computer science;systems engineering;software engineering;model-based design	SE	-58.729362685852344	33.25452106147169	15579
aa87c2fb0f35b8171ee65763042aa8d7bd7851ee	automating sdn composition: a database perspective	vpp;programmable data planes;pvpp;domain specific languages dsl;compiler optimization;software switch	To keep up with the complexity of SDN management, it is generally agreed that modular development of the controller software plays a key role. However, forming a correct modular composition is still a challenging task. An operator needs to understand the module internals and to manually wire inter-module interactions that often depend on the underlying packets. In contrast, this poster presents a novel database approach towards automatic, packet-agnostic composition out of black-box modules.	black box;interaction;software-defined networking	Anduo Wang;Jason Croft	2017		10.1145/3050220.3060612	real-time computing;computer science;theoretical computer science;database	AI	-51.66128929092215	44.83689140945203	15623
a5cd72fa1bcf034f6451ac9f0b9b7195098a9201	brokering positioning data from heterogeneous infrastructures	location based services;context aware;location based service;positioning;ubiquitous computing;middleware;quality of service;open interfaces;heterogeneous network	Over the last years, cellular operators, service and content providers, have been trying to identify the needs of a fully connected user, facilitating pervasiveness communications and ubiquitous computing concepts. One of the most promising directions is the so-called context-aware environment. Positioning is an essential component for the deployment of the evolving context-aware concepts. This article introduces a unified positioning component (POS), which establishes a generic, open, modular, and efficient quality of service (QoS) enabled framework, offering independence from the underlying heterogeneous network infrastructures and positioning techniques. The design objectives of POS are presented, as well as its functionality, the technical specifications and the prototype implementation of the POS component, elaborating on the features and services that it offers to the PoLoS platform and middleware location brokering applications.		Giannis F. Marias;Nikos Priggouris;Giorgos Papazafeiropoulos;Stathes Hadjiefthymiades;Lazaros F. Merakos	2004	Wireless Personal Communications	10.1023/B:WIRE.0000049402.33897.15	computer science;operating system;location-based service;hybrid positioning system;world wide web;computer security;ubiquitous computing;computer network	Mobile	-38.48743747531911	48.77717609052105	15690
892dc591ddbb1d6419166a41db5412fed69a8082	modularization metrics: assessing package organization in legacy large object-oriented software	software metrics;coupling;measurement couplings software systems indexes organizations context;measurement;software modularization;packages;software maintenance;information hiding;software systems;object oriented software;object oriented programming;data encapsulation;indexes;software reusability data encapsulation object oriented programming software maintenance software metrics software packages;cohesion;reusability modularization metrics package organization legacy large object oriented software software packages software systems software modularization information hiding changeability;object oriented;software reusability;modularity;organizations;couplings;context;software packages	There exist many large object-oriented software systems consisting of several thousands of classes that are organized into several hundreds of packages. In such software systems, classes cannot be considered as units for software modularization. In such context, packages are not simply classes containers, but they also play the role of modules: a package should focus to provide well identified services to the rest of the software system. Therefore, understanding and assessing package organization is primordial for software maintenance tasks. Although there exist a lot of works proposing metrics for the quality of a single class and/or the quality of inter-class relationships, there exist few works dealing with some aspects for the quality of package organization and relationship. We believe that additional investigations are required for assessing package modularity aspects. The goal of this paper is to provide a complementary set of metrics that assess some modularity principles for packages in large legacy object-oriented software: Information-Hiding, Changeability and Reusability principles. Our metrics are defined with respect to object-oriented dependencies that are caused by inheritance and method call. We validate our metrics theoretically through a careful study of the mathematical properties of each metric.	application programming interface;cohesion (computer science);encapsulation (networking);existential quantification;method (computer programming);software maintenance;software system	Hani Abdeen;Stéphane Ducasse;Houari A. Sahraoui	2011	2011 18th Working Conference on Reverse Engineering	10.1109/WCRE.2011.55	reliability engineering;computer science;systems engineering;engineering;package development process;software engineering;database;coupling;programming language;package;object-oriented programming	SE	-57.1033698901955	34.029292422004104	15722
6a3517414ab7d1bd220467df4a314872ff3b8914	internet scada utilizing api's as data source	control system;application program interface	control system;application program interface	application programming interface	Rosslin John Robles;Haeng-Kon Kim;Tai-Hoon Kim	2010		10.1007/978-3-642-17610-4_31	operating system;world wide web;computer security	SE	-35.54995857782375	50.24408041067784	15768
ea71ff2b15620d7071d8e17b5c9033d55b652f2e	shielding against sql injection attacks using admire model	databases;analytical models;databases application software data security computer errors web server computer security protection permission computational intelligence computer science;application software;sql;admire model;computational intelligence;web application security sql injection threat modeling vulnerability;risk management;computer crime;vulnerability;data mining;sql risk management security of data;computer security;protection;servers;permission;shielding;web application;threat modeling;sql injection;risk assessment;tools and techniques;sql injection attack;web server;computer science;malafide reasons;security;sql injection attacks;risk mitigation;malafide reasons shielding sql injection attacks admire model threat modeling risk assessment risk mitigation threat risk model;security of data;computer errors;threat risk model;data models;data security	In recent years, web applications have become tremendously popular. However, vulnerabilities are pervasive resulting in exposure of organizations and firms to a wide array of risks. In spite of many tools and techniques, attacks on web application especially through SQL Injection Attacks are at a rise. Threat modeling is an important risk assessment and mitigation practice that provides the capability to secure a web application. A comprehensively designed threat model can provide a better understanding of the risks and help determine the extent of mitigation action. This paper aims to initiate the threat risk model ADMIRE which is a comprehensive, structured and stepwise approach, which would help to identify and mitigate SQL Injections attacks and shield the database lying in the database servers, which may be unauthorizedly accessed for malafide reasons from the web applications.	database server;financial risk modeling;pervasive informatics;risk assessment;sql injection;stepwise regression;threat model;vulnerability (computing);web application	Supriya Madan	2009	2009 First International Conference on Computational Intelligence, Communication Systems and Networks	10.1109/CICSYN.2009.58	web threat;sql injection;risk management;computer science;data mining;world wide web;computer security	Security	-48.95475535582195	58.25905046478623	15780
2d5da22184b127c4f5d096b02226c5bda9510c06	exploiting dynamic information in ides improves speed and correctness of software maintenance tasks	performance measure;dynamic programming;performance evaluation;measurement;dynamic information exploiting dynamic information ide software maintenance tasks eclipse source code runtime behavior software systems object oriented systems dynamic binding runtime architecture;software maintenance;software systems;controlled experiment;object oriented programming;runtime;professional development;performance measures object oriented programming integrated environments restructuring reverse engineering reengineering complexity measures;dynamic information;dynamic binding;object oriented systems;runtime measurement java context software maintenance concrete weaving;integrated environments;polymorphism;restructuring;performance measures;complexity measures;source code;weaving;program compilers;reengineering;software maintenance dynamic programming object oriented programming program compilers;context;concrete;reverse engineering;java	Modern IDEs such as Eclipse offer static views of the source code, but such views ignore information about the runtime behavior of software systems. Since typical object-oriented systems make heavy use of polymorphism and dynamic binding, static views will miss key information about the runtime architecture. In this paper, we present an approach to gather and integrate dynamic information in the Eclipse IDE with the goal of better supporting typical software maintenance activities. By means of a controlled experiment with 30 professional developers, we show that for typical software maintenance tasks, integrating dynamic information into the Eclipse IDE yields a significant 17.5 percent decrease of time spent while significantly increasing the correctness of the solutions by 33.5 percent. We also provide a comprehensive performance evaluation of our approach.	column (database);computational complexity theory;correctness (computer science);dynamic data;eclipse;integrated development environment;interactive visualization;java;late binding;miller columns;performance evaluation;run-time type information;software maintenance;software system;source code editor;subroutine;test case;tooltip	David Röthlisberger;Marcel Harry;Walter Binder;Philippe Moret;Danilo Ansaloni;Alex Villazón;Oscar Nierstrasz	2012	IEEE Transactions on Software Engineering	10.1109/TSE.2011.42	professional development;polymorphism;real-time computing;concrete;business process reengineering;computer science;systems engineering;operating system;software engineering;restructuring;dynamic programming;programming language;object-oriented programming;software maintenance;java;weaving;reverse engineering;measurement;software system;source code	SE	-55.34366439127077	37.017495653142625	15839
ae1ff061bc0064430f4786271ad602eb7ea6059b	symbolic analysis of programmable logic controllers	and fault tolerance;reliability;memory structures;hidden markov model;hidden markov models uncertainty reliability markov processes probabilistic logic abstracts syntactics;plc;testing;performance analysis and design aids;probabilistic model checking programmable logic controller symbolic analysis plc system symbolic analysis plc reliability plc system uncertainty characterization hidden markov model regular markov model;probabilistic analysis;programmable controllers formal verification hidden markov models probability;i o and data communications;hardware;probabilistic analysis plc hidden markov model	Programmable Logic Controllers (PLC) are widely used in industry. The reliability of the PLC is vital to many critical applications. This paper presents a novel approach to the symbolic analysis of PLC systems. The approach includes, (1) calculating the uncertainty characterization of the PLC system, (2) abstracting the PLC system as a Hidden Markov Model, (3) solving the Hidden Markov Model with domain knowledge, (4) combining the solved Hidden Markov Model and the uncertainty characterization to form a regular Markov model, and (5) utilizing probabilistic model checking to analyze properties of the Markov model. This framework provides automated analysis of both uncertainty calculations and performance measurements, without the need for expensive simulations. A case study of an industrial, automated PLC system demonstrates the effectiveness of our work.	abstract syntax tree;baum–welch algorithm;embedded system;hidden markov model;markov chain;model checking;operating environment;parse tree;power-line communication;programmable logic device;simulation;statistical model;welch's method	Hehua Zhang;Yu Jiang;William N. N. Hung;Xiaoyu Song;Ming Gu;Jia-Guang Sun	2014	IEEE Transactions on Computers	10.1109/TC.2013.124	probabilistic analysis of algorithms;computer science;theoretical computer science;programmable logic controller;machine learning;reliability;software testing;markov algorithm;markov model;hidden markov model;statistics	SE	-46.495069244458946	32.94805426608193	15846
cef949ea631306ac11c041d5e12252b0589740cc	nonmonotonicity in trust management		The work discusses nonmonotonicity in terms of trust management systems and presents model allowing for credential revocation in the Rolebased Trust-management Framework . A freshness constraints have been adopted into RT Framework in order to overcome nonmonotonicity and turn it to be temporarily monotonic . The proposed model allows for freshness requirements specification on policy level and utilises freshness graph in order to perform propagation of freshness requirements along credential c hains. Finally, an evaluation of the model against real-life scenario has been performed.	credential;real life;replay attack;requirement;software propagation;software requirements specification;trust management (information system);trust management (managerial science)	Wojciech Pikulski	2012		10.1007/978-3-642-32808-4_34	real-time computing;monotonic function;revocation;software requirements specification;credential;management system;software security assurance;computer science;graph	SE	-53.86032620925029	48.16222146486662	15892
ada5cafd74748a5a38a45bab624ceb97500cb909	automated conversion of table-based websites to structured stylesheets using table recognition and clone detection	hierarchical structure;clone detection	Web standards such as XHTML and CSS are rapidly coming into practice and have many advantages, including compatibility, consistency across browsers, and increased ease of maintenance. Unfortunately large numbers of existing websites still use the deprecated table-based layout style in which page style is unique to each page. Existing tools for automating the transition to stylesheets provide little help, converting page-by-page using a flattened structure and local inline styles rather than a common CSS stylesheet. This approach ignores hierarchical structure and defeats the main purpose of moving to the new standard, losing all of the advantages.  In this work we present an automated method for converting table-based layout websites to standards-compliant modern CSS stylesheet-based websites using a two-step process. Pages of the site are first converted page-by-page using table recognition technology to preserve hierarchical structure and layout semantics in local styles. Software clone detection technology is then utilized to recognize common layout styles in the pages and extract and minimize them to a common CSS stylesheet for the site. The result is a maintainable, efficient modern standards-compliant website with the same look and feel as the original but with all the maintenance advantages of a custom programmed new site.	cascading style sheets;deprecation;duplicate code;look and feel;standards-compliant;video game clone;web standards;xhtml	Andy Y. Mao;James R. Cordy;Thomas R. Dean	2007		10.1145/1321211.1321214	computer science;operating system;software engineering;data mining;database;distributed computing;world wide web	HCI	-59.43266438306691	40.86177256229032	16021
b85bc1dfc8ad8d40501c1a0b13ab81efe229c1fb	a formal model for hierarchical policy contexts	information flow restrictions formal model hierarchical policy contexts role based access control rbac models policy interface security administration rbac systems policy specification metapolicy approach large scale security environments;formal specification;formal model;authorisation;role based access control;large scale;information flow;computer network management;distributed object management;distributed object management authorisation formal specification computer network management;context modeling access control information security computer interfaces laboratories computer security guidelines conferences	Role-based access control (RBAC) models specify a policy interface for security administration, but do not provide guidelines for how large organisations should manage their roles. Parameterised RBAC systems are even more expressive; however, this adds to the risk of dangerous mistakes during policy specification. In this paper we define a formal model for hierarchical policy contexts: an RBAC meta-policy approach for subdividing the administration of large-scale security environments and for enforcing information flow restrictions over policies.	formal language;mathematical model;role-based access control	András Belokosztolszki;Ken Moody;David M. Eyers	2004	Proceedings. Fifth IEEE International Workshop on Policies for Distributed Systems and Networks, 2004. POLICY 2004.	10.1109/POLICY.2004.1309159	computer security model;computer science;knowledge management;database;network security policy;computer security	Security	-50.14218474636332	50.720010596023506	16064
860190be227ff41a02d367e3a987675e156108ae	middleware for data sensing and processing in vanets	data processing vanet osgi middleware services;vanet;data processing;vehicular ad hoc networks;application program interfaces;middleware;services;osgi;vehicular ad hoc networks application program interfaces java middleware;api prototype middleware data sensing data processing vanet urban environments mobile devices traffic flow pollution weather condition vehicular ad hoc networks osgi framework;java	In urban environments nowadays there are a multitude of mobile devices that have several ways of communicating with each other, which become more and more powerful and used in every day activity. The most important characteristic of these devices is that they can supply useful information for a detailed vision of the area that they cover, especially when they travel a large distance. By gathering all this particular data and then processing it, we can obtain a global image of different aspects like traffic flow, pollution, weather condition, and all this with the help of the mobile users. In this paper we present a middleware for sensing data in Vehicular Adhoc Networks that uses the OSGi framework and tries to simplify the work made by application programmers by using the API prototype provided. This concludes in building, deploying and maintaining applications that will process the data acquired from sensors and deliver real-time information useful to the users.	application programming interface;middleware;mobile device;osgi;programmer;prototype;real-time computing;real-time data;sensor	Stefan Nour;Raluca Negru;Fatos Xhafa;Florin Pop;Ciprian Dobre;Valentin Cristea	2011	2011 International Conference on Emerging Intelligent Data and Web Technologies	10.1109/EIDWT.2011.33	vehicular ad hoc network;embedded system;middleware;computer science;middleware;computer security;computer network	Mobile	-40.73460282720835	48.61645638587785	16074
9770b86050de183d14c64b2b555f3442bce881d8	using lattice of class and method dependence for change impact analysis of object oriented programs	impact factor;object oriented programming;change impact analysis;formal concept analysis;lattice of class and method dependence	Software change impact analysis (CIA) is a key technique to identify unpredicted and potential effects caused by software changes. In this paper, we propose a new CIA technique based on a compact and effective representation for object oriented programs, called lattice of class and method dependence (LoCMD). This novel representation can effectively capture the dependences between classes and methods. Based on the LoCMD, our CIA technique calculates a ranked list of potential impacted methods according to a metric, impact factor, which corresponds to the priority of these methods to be inspected. Initial case study validates the reasonability of our two assumptions, and demonstrates the effectiveness of our technique.	formal concept analysis;reachability	Xiaobing Sun;Bixin Li;Sai Zhang;Chuanqi Tao;Xiang Chen;Wanzhi Wen	2011		10.1145/1982185.1982495	simulation;computer science;formal concept analysis;theoretical computer science;machine learning;database;programming language;object-oriented programming;change impact analysis	SE	-59.20153371673222	33.49634776810516	16096
53f5091f38cb989bbbb714fac99382f179ef5758	dust: real-time code offloading system for wearable computing		Runtime performance seriously bothers application developers and users for wearable devices such as Apple Watch and Google Glass. To ease such pains, one approach is to transplant the computation to the cloud, known as cloud offloading. This paradigm works well when the Internet is accessible. Another solution, known as device-to-device(D2D) offloading,is to utilize nearby devices for computation offloading. In this paper, propose the Dust, a D2D code offloading prototype for wearable computing. To our best knowledge, Dust is the first code offloading system for wearable computing with an implementation on Google Glass. Dust includes a programmer friendly framework based on Java annotation, a lightweight offloading service, and a runtime task scheduler to make offloading decisions. Extensive experiments demonstrate that Dust achieves real-time performance with speedup of 6.1X.	apple watch;cloud computing;computation offloading;experiment;glass;internet;java annotation;programmer;prototype;real-time transcription;scheduling (computing);speedup;wearable computer;wearable technology;windows task scheduler	Di Huang;Luning Yang;Sanfeng Zhang	2014	2015 IEEE Global Communications Conference (GLOBECOM)	10.1109/GLOCOM.2014.7417877	embedded system;wearable computer;cloud computing;computer hardware;computer science;operating system;glass	Embedded	-34.96670510679112	54.62019391780646	16098
76d7efad85633a0b6d83f9573fda0c8f735c0254	a pin-based dynamic software fault injection system	software fault injection;software;software testing;circuit faults;fault tolerant;software systems benchmark testing genetic mutations software testing web server system testing fault tolerance hardware emulation software design;jin ang jiang jian hui lou jun gang hu jia wei 软件故障注入系统 程序设计 计算机技术 设计模式 a pin based dynamic software fault injection system;software verification;prototypes;pin based dynamic software fault injection system;emulation;software fault tolerance;software fault tolerance program testing program verification;dependability benchmarking fault injection software fault injection pin;dependability benchmarking;program verification;fault injection design pattern pin based dynamic software fault injection system software verification fault tolerant mechanism software testing dependability benchmarking;program testing;monitoring;design pattern;fault injection design pattern;source code;pin;fault tolerant mechanism;fault injection;benchmark testing;fault location	Fault injection plays a critical role in the verification of fault-tolerant mechanism, software testing and dependability benchmarking for computer systems. In this paper, according to the characteristics of software faults, we propose a new fault injection design pattern based on the PIN framework provided by Intel company, and develop a PIN-based dynamic software fault injection system (PDSFIS). Faults can be injected by PDSFIS without the source code of target applications under assessment, nor does the injection process involve interruption or software traps. Experimental assessment results of an Apache Web server obtained by the dependability benchmarking are presented to demonstrate the potentials of PDSFIS.	benchmark (computing);dependability;emulator;fault injection;fault tolerance;graphical user interface;interrupt;orthogonal defect classification;personal identification number;server (computing);software bug;software design pattern;software testing;web server	Ang Jin;Jianhui Jiang;Jiawei Hu;Jungang Lou	2008	2008 The 9th International Conference for Young Computer Scientists	10.1109/ICYCS.2008.329	embedded system;benchmark;emulation;fault tolerance;real-time computing;software verification;computer science;prototype;software testing;design pattern;programming language;software fault tolerance;source code	SE	-58.12185409886013	41.284136214339355	16164
b2395ce69900e283b7398c3e7f1101fcd7e0cb7d	common statement kind changes to inform automatic program repair		The search space for automatic program repair approaches is vast and the search for mechanisms to help restrict this search are increasing. We make a granular analysis based on statement kinds to find which statements are more likely to be modified than others when fixing an error. We construct a corpus for analysis by delimiting debugging regions in the provided dataset and recursively analyze the differences between the Simplified Syntax Trees associated with EditEvent's. We build a distribution of statement kinds with their corresponding likelihood of being modified and we validate the usage of this distribution to guide the statement selection. We then build association rules with different confidence thresholds to describe statement kinds commonly modified together for multi-edit patch creation. Finally we evaluate association rule coverage over a held out test set and find that when using a 95% confidence threshold we can create less and more accurate rules that fully cover 93.8% of the testing instances.		Mauricio Soto;Claire Le Goues	2018	2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)	10.1145/3196398.3196472	computer science;syntax;debugging;data mining;recursion;software;software bug;association rule learning;test set;restrict	SE	-59.683265455397965	38.640250976433904	16176
aa51c79e46aeefc6eb2a53fb694eeffdd593ac48	sok: cryptographically protected database search		Protected database search systems cryptographically isolate the roles of reading from, writing to, and administering the database. This separation limits unnecessary administrator access and protects data in the case of system breaches. Since protected search was introduced in 2000, the area has grown rapidly, systems are offered by academia, start-ups, and established companies. However, there is no best protected search system or set of techniques. Design of such systems is a balancing act between security, functionality, performance, and usability. This challenge is made more difficult by ongoing database specialization, as some users will want the functionality of SQL, NoSQL, or NewSQL databases. This database evolution will continue, and the protected search community should be able to quickly provide functionality consistent with newly invented databases. At the same time, the community must accurately and clearly characterize the tradeoffs between different approaches. To address these challenges, we provide the following contributions:1) An identification of the important primitive operations across database paradigms. We find there are a small number of base operations that can be used and combined to support a large number of database paradigms.2) An evaluation of the current state of protected search systems in implementing these base operations. This evaluation describes the main approaches and tradeoffs for each base operation. Furthermore, it puts protected search in the context of unprotected search, identifying key gaps in functionality.3) An analysis of attacks against protected search for different base queries.4) A roadmap and tools for transforming a protected search system into a protected database, including an open-source performance evaluation platform and initial user opinions of protected search.	complex systems;copy protection;cryptography;database;dynamic data;encryption;federated identity;insider threat;list of cryptographers;microsoft outlook for mac;newsql;nosql;open-source software;overhead (computing);partial template specialization;performance evaluation;sql;server (computing);snapshot (computer storage);time complexity;uncontrolled format string;usability	Benjamin Fuller;Mayank Varia;Arkady Yerukhimovich;Emily Shen;Ariel Hamlin;Vijay Gadepally;Richard Shay;John Darby Mitchell;Robert K. Cunningham	2017	2017 IEEE Symposium on Security and Privacy (SP)	10.1109/SP.2017.10	database;internet privacy;computer security	Security	-48.43389323720667	60.328714820225414	16206
da0f5d4354f3c0a8e2b59e9f73f57e12a7733085	programming languages popularity and implications to testing programmable logic controllers	software testing;structured text;google trends;plc;programmable logic controllers;function block diagrams;iec 1131 3;programming languages	The popularity of domain-specific programming languages has implications on how we test software in these domain industries. For example for programmable logic controllers five standard languages were defined and used in practice. Detailed data on popularity of these languages should show some implications on what languages to target when testing. We suggest that massive new data sources resulting from programmers may offer a new perspective on how we test domain-specific languages. By analyzing Google query volumes for search terms related to programmable logic controllers languages, we find patterns that may be interpreted as signs of actual usage in practice. Comparing with the current testing approaches proposed by researchers our results illustrate both potentials and threats on what is needed in reality when testing programmable logic controllers.	domain-specific language;programmable logic device;programmer;programming language	Eduard Paul Enoiu	2015	PeerJ PrePrints	10.7287/peerj.preprints.879v1	natural language processing;computer science;theoretical computer science;third-generation programming language;programmable logic controller;functional logic programming;ontology language;hardware description language;fifth-generation programming language;programming language;second-generation programming language;comparison of multi-paradigm programming languages;algorithm;query language	SE	-53.3520437083139	38.34789406508709	16272
42f6a2b79318f56aa5a97fd9e516ddf202d0e6e4	methodology and application of meta-diagnosis on avionics test benches		This paper addresses Model Based Diagnosis for the test of avionics systems that combines aeronautic computers with simulation software. Just like the aircraft, those systems are complex since additional tools, equipments and simulation software are needed to be consistent with the test requirements. We propose a structural diagnostic framework based on the lattice concept to reduce the time of unscheduled maintenance when the tests cannot be performed. Here, we also describe a diagnosis algorithm that is based on the formal lattice description and designed for test systems. The benefits is to capture the system structure and communication specificities to diagnose the configuration, the equipments, the connections, and the simulation software.	automatic control;avionics;computer;iteration;java;medical algorithm;model-based definition;requirement;simulation software;software architecture	Ronan Cossé;Denis Berdjag;Sylvain Piechowiak;David Duvivier;Christian Gaurel	2015			simulation software;avionics;reliability engineering;computer science	SE	-46.5264908853546	33.123937243917254	16322
e40089e31752ee9f283d9f3a5530e824cab7682b	paradigms for mobile agent based active monitoring of network systems	databases;ajanta mobile agent system;mobile agents computerized monitoring remote monitoring computer networks information filtering event detection databases computer network management large scale systems telecommunication traffic;control functions;auditing;information retrieval;mobile agents;telecommunication control;information filtering;subscriber agents;event detection;active monitoring;inspector agents;computer networks;software agents;computerized monitoring;telecommunication traffic;trigger events;mobile agent system;monitoring;monitor agents;computer network management;remote monitoring;mobile agent;networked systems;information retrieval monitoring computer network management software agents telecommunication control auditing;active monitoring information filtering control functions monitor agents subscriber agents auditor agents inspector agents trigger events ajanta mobile agent system computer networks;large scale systems;auditor agents	We present here a framework together with a set of paradigms for mobile agent based active monitoring of network systems. In our framework mobile agents are used to perform remote information filtering and control functions. Such agents can detect basic events or correlate existing events that are stored in a database to enforce system policies. A system administrator can securely modify the monitoring policies and information filtering functions of its agents, or install new agents at a node. The framework presented here includes monitor, subscriber, auditor and inspector agents. The policies and itineraries of these agents can be modified dynamically. In response to certain trigger events agents may change their itineraries to correlate event data. We present here a set of experiments that we have conducted using the Ajanta mobile agent system to evaluate and demonstrate the capabilities of our mobile agent framework.	agent-based model;control function (econometrics);database;experiment;information filtering system;mobile agent;system administrator	Anand R. Tripathi;Tanvir Ahmed;Sumedh Pathak;Megan Carney;Paul Dokas	2002		10.1109/NOMS.2002.1015546	embedded system;real-time computing;computer science;software agent;mobile agent;audit;computer security;computer network;rmon	AI	-45.164959223981015	56.788180025142616	16330
f56c631b8fc5409e4ca5edca21fcd30814652965	proxi-annotated control flow graphs: deterministic context-sensitive monitoring for intrusion detection	sensibilidad contexto;modele comportement;auxiliary variable;modelizacion;behavior model;program graph;intruder detector;model based reasoning;model specification;raisonnement base sur modele;context aware;analyse statique;surveillance;behavior modeling;model generation;graph flow;modelo comportamiento;flot donnee;securite informatique;distributed computing;langage evolue;false alarm rate;intrusion detection;flujo datos;variable auxiliar;program verification;modele statique;control flow graph;flow graphs;approche deterministe;analisis estatica;analisis programa;flujo grafo;deterministic approach;computer security;modelisation;verificacion programa;vigilancia;internet;monitoring;specification modele;especificacion modelo;graphe programme;flot graphe;seguridad informatica;modelo estatico;enfoque determinista;graphe flux;intrusion detection systems;calculo repartido;static model;lenguaje evolucionado;graphe fluence;program analysis;monitorage;sensibilite contexte;analyse programme;static analysis;detecteur intrus;data flow;monitoreo;taux fausse alarme;verification programme;grafo programa;high level language;modeling;detector intruso;grafo fluencia;porcentaje falsa alarma;calcul reparti;systeme detection intrusion;intrusion detection system;fluence graph;variable auxiliaire;dynamic analysis	Model or specification based intrusion detection systems have been effective in detecting known and unknown host based attacks with few false alarms [12, 15]. In this approach, a model of program behavior is developed either manually, by using a high level specification language, or automatically, by static or dynamic analysis of the program. The actual program execution is then monitored using the modeled behavior; deviations from the modeled behavior are flagged as attacks. In this paper we discuss a novel model generated using static analysis of executables (binary code). Our key contribution is a model which is precise and runtime efficient. Specifically, we extend the efficient control flow graph (CFG) based program behavioral model, with context sensitive information, thus, providing the precision afforded by the more expensive push down systems (PDS). Executables are instrumented with operations on auxiliary variables, referred to as proxi variables. These annotated variables allow the resulting context sensitive control flow graphs obtained by statically analyzing the executables to be deterministic at runtime. We prove that the resultant model, called proxi-annotated control flow graph, is as precise as previous approaches which use context sensitive push-down models and in-fact, enhances the runtime efficiency of such models. We show the flexibility of our technique to handle different variations of recursion in a program efficiently. This results in better treatment of monitoring programs where the recursion depth is not pre-determined.	behavioral modeling;binary code;control flow graph;executable;high-level programming language;information sensitivity;intrusion detection system;recursion;resultant;run time (program lifecycle phase);sensor;specification language;static program analysis	Samik Basu;Prem Uppuluri	2004		10.1007/978-3-540-30555-2_41	intrusion detection system;simulation;computer science;programming language;computer security;algorithm	SE	-56.30233084454903	54.01436715173096	16358
5bd6890320e8def7eb07fe530ce6e4891a634b94	authorization constraint enforcement for information system security	information systems security;information systems;information security;conflict of interest;fraud prevention;insurance data processing;authorisation;access authorizations;health insurance;health insurance claim processing system authorization constraint enforcement information system security fraud prevention separation of duty logic based systems access authorizations;role based access control;logic programming authorisation health care information systems insurance data processing;health insurance claim processing system;data mining;companies;separation of duty;policy compliance;logic programming;heuristic algorithms;policy compliance role based access control separation of duty information security conflict of interest;authorization;information system security;information system;authorization constraint enforcement;security;authorization information systems information security runtime management information systems computer security inference mechanisms delay access control protection;insurance;health care;logic based systems	Managing access authorities is critical to the security of information systems. To prevent fraud or abuse due to conflict of interests, a well-known authorization constraint called separation of duty (SoD) is commonly applied. SoD ensures that no single user receives too many authorities. Enforcement of authorization constraints such as SoD in large organizations can be difficult due to the large number of information system users, the variety of assets involved, and tasks that require roles that may be shared or delegated at multiple levels. Most existing work in this area focuses on specifications of SoD constraints and assumes that constraints can be enforced by logical inference mechanisms at run-time. A drawback of this approach is that when violations occur, finding alternative role activations at run-time may not be feasible. This can result in delays or even failure for critical service transactions. Moreover, logic-based systems are difficult to understand and do not scale easily. This paper presents an algorithmic set-based approach that automatically checks for SoD compliance prior to run-time by searching for a set of valid role activations. The paper discusses details of this approach and illustrates its use in managing access authorizations in a health insurance claim processing system.	algorithm;authorization;distributed computing;enterprise information system;information system;role-based access control;run time (program lifecycle phase);security management;world-system	Rattikorn Hewett;Phongphun Kijsanayothin	2008	2008 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2008.4811840	actuarial science;computer science;information security;data mining;authorization;computer security;information system	SE	-49.47386519813115	52.005797120804566	16370
18912c39ca1f8d8ec34c52d91864fa21a491939a	a prototype system for static and dynamic program understanding	program understanding;type definition window;program diagnostics;software tool;high level structure chart;call graph;software maintenance;prototypes;prototypes information analysis vehicle dynamics runtime user interfaces vehicles testing flow graphs maintenance engineering humans;graphical user interface;maintenance engineering;static analysis information;testing;dynamic program;prototype system;type definition window software tool program testing runtime information software reengineering software maintenance prototype system pundit program understanding investigation tool static analysis information c source analyzer graphical user interface high level structure chart dynamic call graph control flow graph program execution;control flow graph;runtime;flow graphs;dynamic call graph;program execution;dynamic information;graphical user interfaces;program testing;c source analyzer;runtime information;software tools graphical user interfaces program diagnostics software maintenance;graphic user interface;software tools;program understanding investigation tool;humans;software reengineering;vehicles;static analysis;information analysis;user interfaces;pundit;vehicle dynamics	We describe a tool we call PUNDIT (Program UNDer.>tanding Investigation Tool), a prototype intended to senlt as a vehicle for exploring anti testing ideas in the areti of program understanding. It combines static ana1vsi.c information with information collected at runtime We describe the architecture of PUNDIT and its two main components: the C sourre analyzer and a gruphicul user interface. We explain several of the views provided by the tool, including a high level structure chart, U dynamic call graph, a control flow graph animated during program execution, a type definition window, ancl others. By integrating static and dynamic information, the tool provides a more comprehensive understtinding of a program as the first step to reengineering or maintaining the application than can he obtainetl by static analysis alone. Program understanding As m early step in the process of re-engineering, maintaining, or extending an application, whether automatic or manual, it is necessary to “understand’ part or all of the programl71. It has been our observation that the use of static analysis alone may give an imperfect understanding of the program, and that it is often important to comprehend how the program behaves at runtime. This is especially true with multi-threaded, distributed, or parallel programs, and also with object-oriented programs, for in all of these cases it becomes increasingly difficult to figure out what is going on from static analysis alone. Since the goal is to help provide aid for the programmer in what is a highly intellectual activity requiring significant amounts of human cognitive activity. a reasonable question is “what is comprehension and how does it work?’ Psychological studies[ 1-3, 9, 1 1, 121 have attempted to provide information that could lead to theories of comprehension. Due to the nebulous and complex nature of the problem, most studies have focused on one task or aspect of programming and have attempted to show how a programmer gathers and utilizes information to achieve a more meaningful understanding of a program. Although various theories of comprehension have arisen from these studies, there is as yet no generally accepted view. Lacking a firm theoretical basis, program understanding is currently at a point where the best approach may be to “try it out and use what works.” In this spirit, PUNDIT is a tool intended to allow us to explore the pragmatic question of how best to actually provide useful information to a programmer. It has been implemented, and has been used at this point by a few tens of users, whose feedback has been invaluable in trying to construct a tool which is both practical and useful.	call graph;code refactoring;control flow graph;feedback;high-level programming language;level structure;list comprehension;program comprehension;programmer;prototype;run time (program lifecycle phase);static program analysis;structure chart;theory;thread (computing);user interface	D. Olshefski;A. Cole	1993		10.1109/WCRE.1993.287775	maintenance engineering;real-time computing;computer science;systems engineering;operating system;software engineering;graphical user interface;programming language	SE	-54.7614464533657	36.35086009609076	16515
1dce1fb7a9abe523233f58c2f167fd06d77a1ce6	towards elimination of cross-site scripting on mobile versions of web applications	client side filter;mobile web;xss;regular expression	In this paper, we address the overlooked problem of CrossSite Scripting (XSS) on mobile versions of web applications. We have surveyed 100 popular mobile versions of web applications and detected XSS vulnerabilities in 81 of them. The inspected sites present a simplified version of the desktop web application for mobile devices; the survey includes sites by Nokia, Intel, MailChimp, Dictionary, Ebay, Pinterest, Statcounter and Slashdot. Our investigations indicate that a significantly larger percentage (81% vs. 53%) of mobile web applications are vulnerable to XSS, although their functionality is drastically reduced in comparison to the corresponding desktop web application. To mitigate XSS attacks for mobile devices, this paper presents a lightweight, black-list and regular expressions based XSS filter for the detection of XSS on mobile versions of web applications, which can be deployed on client or server side. We have tested our implementation against five different publicly available XSS attack vector lists; none of these vectors were able to bypass our filter. We have also evaluated our filter in the client-side scenario by adding support in 2 open source mobile applications (WordPress and Drupal); our experimental results show reasonably low overhead incurred due to the small size of the filter and computationally fast regular expressions. We have contributed an implementation of our XSS detection rules to the ModSecurity firewall engine, and the filter is now part of OWASP ModSecurity Core Rule Set (CRS).	client-side;cross-site scripting;desktop computer;dictionary;drupal;firewall (computing);mobile app;mobile device;modsecurity;open-source software;overhead (computing);regular expression;server (computing);server-side;slashdot;statcounter;vector (malware);web application;wordpress	Ashar Javed;Jörg Schwenk	2013		10.1007/978-3-319-05149-9_7	cross-site scripting;mobile web;computer science;internet privacy;world wide web;computer security;regular expression	Web+IR	-55.78147104447374	60.03765542876078	16571
6c8c4d50d49a8c53f4296f23ad0540593916a799	automatic comparison of load tests to support the performance analysis of large enterprise systems	pca load test performance counters;performance losses;stress;redundant performance data;performance counters;performance gains;radiation detectors;software performance evaluation;loading;large scale system;testing;automatic comparison;radiation detectors principal component analysis testing loading correlation stress monitoring;load testing;software performance evaluation large scale systems principal component analysis program debugging program testing;large enterprise systems;performance bugs;program testing;monitoring;performance analysts;principal component analysis;performance analysis;test generation;performance losses automatic comparison load tests performance analysis large enterprise systems load testing functional bugs performance bugs large scale systems performance analysts performance counters redundant performance data statistical technique principal component analysis pca performance gains;statistical technique;functional bugs;enterprise system;program debugging;statistical techniques;correlation;load tests;pca;load test;large scale systems	Load testing is crucial to uncover functional and performance bugs in large-scale systems. Load tests generate vast amounts of performance data, which needs to be compared and analyzed in limited time across tests. This helps performance analysts to understand the resource usage of an application and to find out if an application is meeting its performance goals. The biggest challenge for performance analysts is to identify the few important performance counters in the highly redundant performance data. In this paper, we employed a statistical technique, Principal Component Analysis (PCA) to reduce the large volume of performance counter data, to a smaller, more meaningful and manageable set. Furthermore, our methodology automates the process of comparing the important counters across load tests to identify performance gains/losses. A case study on load test data of a large enterprise application shows that our methodology can effectively guide performance analysts to identify and compare top performance counters across tests in limited time.	enterprise software;enterprise system;load testing;principal component analysis;profiling (computer programming);software bug;test data	Haroon Malik;Zhen Ming Jiang;Bram Adams;Ahmed E. Hassan;Parminder Flora;Gilbert Hamann	2010	2010 14th European Conference on Software Maintenance and Reengineering	10.1109/CSMR.2010.39	reliability engineering;real-time computing;simulation;load testing;performance engineering;computer science;engineering;operating system;principal component analysis	HPC	-62.32073061448392	37.03316619424439	16626
28d567ce15a35715c352f928580ad6f5bb5ce475	a transformational approach to facilitate monitoring of high-level policies	libraries;software;low level state machine policies;uml sequence diagram;transformational approach;unified modeling language natural languages security condition monitoring conferences informatics computerized monitoring libraries computer industry;runtime environment;high level security policies;state machine;natural languages;computer industry;high level sequence diagram policies;uml sequence diagrams;computerized monitoring;finite state machines;condition monitoring;monitoring;unified modeling language finite state machines security of data;unified modeling language;informatics;policy formalization process;research report;policy formalization process transformational approach high level security policies uml sequence diagrams high level sequence diagram policies low level state machine policies;security;sequence diagram;security of data;conferences	We present a method for (1) specifying high-level security policies using UML sequence diagrams and (2) transforming high-level sequence diagram policies into low-level state machine policies that can be enforced by monitoring mechanisms. We believe that the method is both easy to use and useful since it automates much of the policy formalization process.	deontic logic;finite-state machine;high- and low-level;interaction;requirement;sequence diagram;unified modeling language	Fredrik Seehusen;Ketil Stølen	2008	2008 IEEE Workshop on Policies for Distributed Systems and Networks	10.1109/POLICY.2008.21	reliability engineering;computer science;systems engineering;database	Metrics	-45.77099970278239	32.530552251632336	16657
76fdce69551e02ecffb531c53095acefa26b6bf1	reputation propagation in composite services	component services;airports;trust management;web services object oriented programming security of data;reputation;object oriented programming;web service;data mining;conference paper;composite services;trust management reputation propagation composite services reputation management component services web services;web services environmental management australia usa councils conference management technology management standards development humans electronic government security;business;web services;keywords composite services;mathematical model;fair share;reputation management;fair distribution;web services composite services;security of data;composite services web services trust management reputation;reputation propagation	"""This paper investigates the problem of reputation management in composite services. Our focus is on developing a method of distribution of reputation received by a composite service to its component services. The proposed method enables the composite service to provide a fair distribution of reputation values so that a component service is neither penalized nor awarded for the bad and good performances respectively, of other component services. Experiment results show that the proposed technique propagates the """"fair share"""" of reputation from the composite service to its component services."""	experience;experiment;fairness measure;negative feedback;performance;reputation management;reputation system;service composability principle;service-oriented modeling;software propagation;trust management (information system)	Surya Nepal;Zaki Malik;Athman Bouguettaya	2009	2009 IEEE International Conference on Web Services	10.1109/ICWS.2009.54	web service;computer science;law;world wide web	Robotics	-50.1820358325343	44.1963635497806	16745
714722789eeda4b666344bbab4dbb4026058f8c1	an industrial control system testbed based on emulation, physical devices and simulation		This paper demonstrates the utility of an industrial control system testbed that incorporates a universal, realistic, measurable, controllable and reusable experimental platform for cyber security research and testing. The testbed has a layered architecture that leverages physical devices and emulation and simulation technologies. The testbed enables researchers to create experiments of varying levels of fidelity for vulnerability discovery, product evaluation and system certification. The utility of the testbed is demonstrated via a case study involving an industrial boiler control system.	control system;emulator;simulation;testbed	Haihui Gao;Yong Peng;Zhonghua Dai;Ting Wang;Xuefeng Han;Hanjing Li	2014		10.1007/978-3-662-45355-1_6	embedded system;simulation;computer engineering	Robotics	-56.930949391615314	51.98012901356106	16834
13ed91a95408d12e124a6b85ba0eb0ff06517cb6	trusted base stations-based privacy preserving technique in location-based services		With the advent in mobile and internet technologies, there is a significant increase in the number of users using smartphones and other internet based applications. There are a large number of applications available online that use the internet and provide useful information to the users. These include ones that provide location-based services e.g. google maps etc. These applications provide many facilities to the users who want information regarding a specific area or directions using an optimal path to a destination. Due to these reasons, the number of clients using these applications is increasing on a daily basis. Although these services are very useful and are making it easy for us to get information about our surroundings, some issues are also linked with the use of these applications and their services. One of the more significant issues of using these services is privacy with respect to sending personal location information to location-based services servers. Researchers have provided many solutions to solve these issues. One of the solutions is through caching and use of k-anonymity techniques. In this paper, we have proposed a method to solve the privacy issue that uses caching data approach to reduce the number of queries sent to the location-based services server. We also discuss the use of the concept of k-anonymity when no relevant data is available in cache, and queries are sent to the server.	cpu cache;cache (computing);internet protocol suite;location-based service;map;mobile device;privacy;server (computing);smartphone	Muhammad Aqib;Jonathan Cazalas	2015	Computer and Information Science	10.5539/cis.v8n4p93	computer science;data mining;services computing;internet privacy;world wide web	Metrics	-39.35060459283057	60.35105202199383	16850
150f2183a608d305ffab4d9415df077724d25c48	k-zero day safety: measuring the security risk of networks against unknown attacks	zero day vulnerability;unknown vulnerability;abstract model;network asset;hardware fault;novel security metric;existing practice;security risk;unknown attack;k-zero day safety;zero day attack	The security risk of a network against unknown zero day attacks has been considered as something unmeasurable since software flaws are less pre­ dictable than hardware faults and the process of finding such flaws and devel­ oping exploits seems to be chaotic. In this paper, we propose a novel security metric, k-zero day safety, based on the number of unknown zero day vulnerabili­ ties. That is, the metric simply counts how many unknown vulnerabilities would be required for compromising a network asset, regardless of what vulnerabilities those might be. We formally define the metric based on an abstract model of net­ works and attacks. We then devise algorithms for computing the metric. Finally, we show the metric can quantify many existing practices in hardening a network.	algorithm;hardening (computing);vulnerability (computing);zero;zero-day (computing)	Lingyu Wang;Sushil Jajodia;Anoop Singhal;Steven Noel	2010		10.1007/978-3-642-15497-3_35	internet privacy;computer security	Security	-58.595689846120806	57.67302049053251	16899
a018bd2dbecb63745a13ef8008b86a82709e0944	overview of http/2		The Hypertext Transfer Protocol is a protocol used for distributed, hypermedia information systems. Its recent update of May 2015, HTTP/2, has raised positive as well as negative voices, either praising its performances or criticizing its possible weaknesses. Seeing through the time the evolution of the protocol will allow to better understand currents issues about HTTP/2.	http/2;hypermedia;hypertext transfer protocol;information system;performance	Anne-Sophie Brylinski;Aniruddha Bhattacharjya	2017		10.1145/3018896.3065841	cross-site scripting;computer network;world wide web;http/2;hypermedia;hypertext transfer protocol;computer science;information system	Theory	-54.56847987671945	60.342214588952324	16908
3b013205b07c88cde2dc78853727b84bca479f5e	more accurate recommendations for method-level changes		During the life span of large software projects, developers often apply the same code changes to different code locations in slight variations. Since the application of these changes to all locations is time-consuming and error-prone, tools exist that learn change patterns from input examples, search for possible pattern applications, and generate corresponding recommendations. In many cases, the generated recommendations are syntactically or semantically wrong due to code movements in the input examples. Thus, they are of low accuracy and developers cannot directly copy them into their projects without adjustments.   We present the Accurate REcommendation System (ARES) that achieves a higher accuracy than other tools because its algorithms take care of code movements when creating patterns and recommendations. On average, the recommendations by ARES have an accuracy of 96% with respect to code changes that developers have manually performed in commits of source code archives. At the same time ARES achieves precision and recall values that are on par with other tools.	algorithm;amazon kindle;archive;black box;care-of address;cognitive dimensions of notations;human-readable medium;java;open-source software;precision and recall;recommender system;run time (program lifecycle phase)	Georg Dotzler;Marius Kamp;Patrick Kreutzer;Michael Philippsen	2017		10.1145/3106237.3106276	precision and recall;recommender system;computer science;change patterns;source code;data mining;software;code refactoring;program transformation	SE	-59.38018020096515	39.38798317000323	16909
8ba8e08180fecea8fd89c8c3657bbfdc3a64668c	vieslaf framework: facilitating negotiations in clouds by applying service mediation and negotiation bootstrapping		Cloud computing represents a novel and promising computing paradigm where computing resources have to be allocated to software for their execution. Self-manageable Cloud infrastructures are required in order to achieve that level of flexibility on one hand, and to comply to users’ requirements specified by means of Service Level Agreements (SLAs) on the other. However, many assumptions in Cloud markets are old fashioned assuming same market conditions as for example in computational Grids. One such assumptions is that service provider and consumer have matching SLA templates and common understanding of the negotiated terms or that they provide public templates, which can be downloaded and utilized by the end users. Moreover, current Cloud negotiation systems have based themselves on common protocols and languages that are known to the participants beforehand. Matching SLA templates and a-priori knowledge about the negotiation terms and protocols between partners are unrealistic assumption in Cloud markets where participants meet on demand and on a case by case basis. In this paper we present VieSLAF, a novel framework for the specification and management of SLA mappings and meta-negotiations facilitating service mediation and negotiation bootstrapping in Clouds. Using VieSLAF users may specify, manage, and apply SLA mappings bridging the gap between non-matching SLA templates without a-priori knowledge about negotiation protocols, required security standards or negotiated terms. We exemplify two case studies where VieSLAF represents an important contribution towards the development of open and liquid Cloud markets.	autonomic computing;autonomic networking;bridging (networking);case-based reasoning;cloud computing;exemplification;knowledge management;middleware;performance evaluation;programming paradigm;requirement;service-level agreement	Ivona Brandic;Dejan Music;Schahram Dustdar	2010	Scalable Computing: Practice and Experience		knowledge management;business;world wide web;computer security	Metrics	-46.92928480852498	43.92809411177392	16932
042eb6c47f5f9c0c13f52ebc304f8d28ffa5c33e	ripple: reflection analysis for android apps in incomplete information environments		Despite its widespread use in Android apps, reflection poses graving problems for static security analysis. Currently, string inference is applied to handle reflection, resulting in significantly missed security vulnerabilities. In this paper, we bring forward the ubiquity of incomplete information environments (IIEs) for Android apps, where some critical data-flows are missing during static analysis, and the need for resolving reflective calls under IIEs. We present Ripple, the first IIE-aware static reflection analysis for Android apps that resolves reflective calls more soundly than string inference. Validation with 17 popular Android apps from Google Play demonstrates the effectiveness of Ripple in discovering reflective targets with a low false positive rate. As a result, Ripple enables FlowDroid to find hundreds of sensitive data leakages that would otherwise be missed.	ripple	Yifei Zhang;Yue Li;Tian Tan;Jingling Xue	2018	Softw., Pract. Exper.	10.1002/spe.2577	computer science;internet privacy;world wide web;computer security	SE	-56.754944119314	57.85500525605477	16962
2414ab698c6a46039379f1bc954407ea96715e04	enabling mobile phones to support large-scale museum guidance	object recognition;museum applications;phoneguide;mobile device;pervasive tracking;museum guidance system;local awareness;wlan;mobile handsets large scale systems bluetooth wireless lan radiofrequency identification neural networks object recognition intrusion detection mobile radio mobility management computer vision;wireless lan exhibitions humanities multimedia computing object recognition radiofrequency identification ubiquitous computing;mobile phone;multimedia computing;large scale;bluetooth object recognition pervasive tracking museum applications mobile devices machine learning mobile phones local awareness;on device object recognition;machine learning;humanities;rfid;exhibitions;ubiquitous computing;bluetooth;wireless lan;mobile phones;multimedia content;rfid mobile phones museum guidance system phoneguide on device object recognition pervasive tracking multimedia content wlan;mobile devices;radiofrequency identification	We present a museum guidance system called PhoneGuide that uses widespread camera-equipped mobile phones for on-device object recognition in combination with pervasive tracking. It also provides location- and object-aware multimedia content to museum visitors, and is scalable to cover a large number of museum objects.	guidance system;mobile phone;outline of object recognition;scalability	Erich Bruns;Benjamin Brombach;Thomas Zeidler;Oliver Bimber	2007	IEEE MultiMedia	10.1109/MMUL.2007.33	computer science;operating system;mobile device;multimedia;internet privacy;world wide web;ubiquitous computing	Mobile	-39.39530873665705	51.14425391732284	17003
e9a1d2a1ecd631f528c64f8b60a2e305c1aa9e62	bridging models and systems at runtime to build adaptive user interfaces	design model;model combination;adaptive user interfaces;dynamic reconfiguration;user interface;interactive system;feedback loop;executable models;adaptive applications;information exchange;model driven engineering;model based user interface development;adaptive user interface	Adapting applications and user interfaces at runtime requires a deeper understanding of the underlying design. Models formalize this design, express the underlying concepts and make them accessible to machines. In our work we utilize runtime models to reflect the state of the interactive system (its UI respectively) and to change its underlying configuration. So called executable models combine design information, runtime state, and execution logic. From the perspective of adaptive UIs this allows the dynamic reconfiguration of UIs according to design information and the current state of the application at runtime. Dedicated elements of the model create a causal interconnection between model and user interface and facilitate a continuous information exchange between the two. This creates a feedback loop between model and UI where external stimulations influence the model execution and where projections to the outside allow the dynamic alteration of user interfaces.	adaptive user interface;bridging (networking);causal filter;executable;feedback;information exchange;interactivity;interconnection;run time (program lifecycle phase)	Marco Blumendorf;Grzegorz Lehmann;Sahin Albayrak	2010		10.1145/1822018.1822022	user interface design;real-time computing;human–computer interaction;computer science;distributed computing;user interface	HCI	-40.562532005948114	39.84342431229004	17069
570d8aa41af4e4369b3c6f0b340416b7e3983338	formal verification of fault-tolerant software design: the csp approach	fault tolerant;bepress selected works;communicating sequential process;formal method;formal verification;model checking;fault tolerance;software design;fault tolerance formal verification model checking software design	Software design techniques for tolerating both hardware and software faults have been developed over the past few decades. Paradoxically, it is essential that fault-tolerant software is designed with the highest possible rigour to prevent faults in itself. Such rigour is provided by formal methods and aided by model checking. We illustrate an approach to fault-tolerant software design based on communicating sequential processes through a running example.	fault tolerance;fault-tolerant software;formal verification;software design	W. L. Yeung;S. A. Schneider	2005	Microprocessors and Microsystems	10.1016/j.micpro.2004.07.005	fault tolerance;verification and validation;formal methods;formal verification;software verification;computer science;package development process;software design;software reliability testing;theoretical computer science;component-based software engineering;software development;software design description;software construction;formal specification;formal equivalence checking;programming language;goal-driven software development process;software fault tolerance	SE	-48.54986770097416	32.88964781191267	17114
ea70d1f71ad209ed237d51dc9643fd06d12a7d4a	iagree studio: a platform to edit and validate ws---agreement documents		The widespread use of SLA-regulated Cloud services, in which the violation of SLA terms may imply a penalty for the parties, have increased the importance and complexity of systems supporting the SLA lifecycle. Although these systems can be very different from each other, ranging from service monitoring platforms to auto-scaling solutions according to SLAs, they all share the need of having machine-processable and semantically valid SLAs. In this paper we present iAgree studio, the first application, up to our knowledge, that is able to edit and semantically validate agreement documents that are compliant with the WS–Agreement specification by checking properties such as its consistency, and the compliance between templates and agreement offers. In addition, it reports explanations when documents are not valid. Moreover, it allows users to combine the validation and explanation operations by means of a scenarios developer. 1 Overview and Motivation SLAs are widely used nowadays as a means to regulate the terms and conditions under which a service is provided. As the use of SLAs in Cloud services and applications in which the violation of SLA terms may imply a penalty for the parties increases, the complexity and demand of systems supporting the SLA lifecycle also increases. These systems include service monitoring platforms that use SLAs to decide which service metrics should be monitored, auto-scaling solutions that automates the provisioning or deprovisioning of resources according to the SLA, and billing components that calculate the penalties incurred during the use of a service, amongst others. Although very different from each other, all of these systems require having semantically valid SLAs (i.e., without semantic errors) and defined in a machine processable manner. WS–Agreement [1] is arguably the most widespread recommendation for defining machine processable SLAs. It specifies a template-based agreement creation protocol and an XML Schema that defines the basic structure of an SLA and the other documents used in the agreement creation protocol like agreement templates and agreement offers. However, WS–Agreement leaves open how the different elements of a WS–Agreement document such as a Service Level Objective (SLO) must be specified. ? This work was partially supported by the European Commission (FEDER), the Spanish and the Andalusian R&D&I programmes (grants TIN2009–07366 (SETI), TIN2012–32273 (TAPAS), TIC–5906 (THEOS)).	autoscaling;electronic billing;image scaling;platform as a service;provisioning;search for extraterrestrial intelligence;service-level agreement;theos;xml schema	Carlos Müller;Antonio Manuel Gutiérrez;Manuel Resinas;Pablo Fernandez;Antonio Ruiz Cortés	2013		10.1007/978-3-642-45005-1_63	computer science;operating system;data mining;database;programming language;world wide web	Web+IR	-47.116636029019105	43.80867625319992	17183
cf40278ee835505399160f16be8f90c32c07fd21	improvement of applications' stability through robust apis	exceptions;articulo;improvement of applications stability through robust apis;application programming interfaces;stack traces;documentation	Modern programs require useful and robust APIs to guarantee applications' responsiveness. Given that large APIs can be used by novice developers and that not all experts are infallible, a well-designed API should inform developers about all the possible exceptions that an application can throw when it calls specific API methods. This research aims to identify automatically which exceptions should be included in an API reference. For this, there will be an evaluation of the Android API with an emphasis on its Java error-handling mechanism. First goal will be the automatic identification of as many as possible critical exceptions that each API method of the system can throw.Second goal will be the recommendation of undocumented exceptions that can lead client applications to execution failures (crashes). Consequently, the contribution of this research would be the decrease of possible application crashes that are associated with undocumented exceptions.	android;application programming interface;automatic identification and data capture;crash (computing);java;responsiveness;robustness (computer science);undocumented feature	Maria Kechagia	2014		10.1145/2642937.2653473	real-time computing;documentation;computer science;operating system;software engineering;data mining;programming language;computer security	SE	-62.59780538117993	37.17851641913743	17190
4bdd8e671ef1ff9c8e85c5e3c01d2c40f5633bf1	query processing in sensor networks	high resolution;wireless devices;ubiquitous computing intelligent sensors query processing database management systems;actuators query processing intelligent networks intelligent sensors computer networks energy management wireless sensor networks sensor phenomena and characterization radio spectrum management computer architecture;query processing;database management systems;smart sensor;sensor network;ubiquitous computing smart sensor network wireless computing device query processing architecture database technology;sensor networks;sensors and actuators;ubiquitous computing;smart sensors;intelligent sensors	Smart sensors are small wireless computing devices that sense information such as light and humidity at extremely high resolutions. A smart sensor query-processing architecture using database technology can facilitate deployment of sensor networks. Smart-sensor technology enables a broad range of ubiquitous computing applications. Their low cost, small size, and untethered nature lets them sense information at previously unobtainable resolutions. We discuss about query processing in sensor networks.	database;sensor;smart transducer;software deployment;ubiquitous computing	Yong Yao;Johannes Gehrke	2003	IEEE Pervasive Computing	10.1109/MPRV.2004.1269131	embedded system;wireless sensor network;human–computer interaction;computer science;distributed computing;key distribution in wireless sensor networks;mobile wireless sensor network;internet of things;ubiquitous computing;computer network;intelligent sensor;visual sensor network	Mobile	-39.874743804477305	47.75509531772984	17240
42d874cdd484c64a9f09a5c68d0acadcdb935c5f	enhancing web services availability	central hub web service availability mission critical application;web service;web services availability mission critical systems middleware application software service oriented architecture security network servers computer science computer architecture;internet business data processing;central hub;internet;business data processing;mission critical application;web service availability	Little work has been reported on highly available Web services which are essential for mission critical applications. In this paper we propose architecture for highly available Web services for mission critical applications. The central idea is the enhancement of Web services by the introduction of a central hub to increase the availability of Web services	business process execution language;mission critical;specification language;usb hub;web service	Subil Abraham;Mathews Thomas;Johnson P. Thomas	2005	IEEE International Conference on e-Business Engineering (ICEBE'05)	10.1109/ICEBE.2005.62	web service;web application security;middleware;web development;web modeling;the internet;data web;web standards;ws-policy;service-oriented architecture;web navigation;ws-addressing;database;services computing;web intelligence;ws-i basic profile;web 2.0;law;world wide web;computer security	DB	-35.81421837411699	48.065807038198976	17263
1ab1360f0768fdee567f5a82dbecdfb0ea8ca8b0	an exploratory study for identifying and implementing concerns in integer programming	refactoring;operations research;integer programming;aspect oriented programming;integer program;branch and cut;aspectc;branch and bound;exploratory study;open source	In this paper, we analyze the suitability of refactoring the integer programming algorithms  Branch and Bound  and  Branch and Cut  with aspects implemented in Computational Infrastructure for Operations Research (COIN-OR), an open source library for Operations Research. For identifying the concerns in the code, we propose a classification of concerns in terms of requirements. We transformed the rules of an existing Aspect-Oriented Programming (AOP) refactoring catalog for  Java  to a corresponding catalog for  AspectC++  and developed a refactored version of the implemented algorithms using our transformed rules. The execution time of  Branch and Bound  and  Branch and Cut  was measured and the impact of using AOP was analyzed. The results are very encouraging and we assess that besides a customizable code, the execution time did not degrade with AOP.	exploratory testing;integer programming	Norelva Niño;Christiane Metzner;Alejandro Crema;Eliezer Correa	2009	Trans. Aspect-Oriented Software Development	10.1007/978-3-642-02059-9_2	real-time computing;integer programming;aspect-oriented programming;computer science;operating system;programming language;branch and bound;exploratory research;code refactoring;branch and cut	SE	-57.59664807275532	35.026730674649826	17264
362769b5a37f72c21be37ce326a5be61b7a55a92	spreading fear on facebook		October 2012 Network Security 15 patching and node over-writing policies and procedures. It may also be worth reviewing contractual arrangements to ensure the SLA guarantees adequate security/post-incident protection. The cloud will certainly continue to grow in popularity as a means of providing IT resources to organisations of all types and sizes, in part because the compelling cost, flexibility and scalability benefits it can offer may be judged to outweigh any related risks. But it’s wise to adopt a specialised set of security policies in relation to your use of cloud technologies, to mitigate these and other emerging risks in this swiftly evolving area.	network security;scalability;service-level agreement	Danny Bradbury	2012	Network Security	10.1016/S1353-4858(12)70094-6	engineering;internet privacy;world wide web;computer security	Security	-49.848712544622316	58.8739509309089	17279
da757eb7fa2f43e980dbbd4aac9c2041502feebe	developing adapters for structural adaptation of component-based applications	ubiquitous computing object oriented programming service oriented architecture;fine grained template adapter component based application service oriented architecture abstract component heterogeneous resource pervasive environment dynamic structural adaptation approach abstract application functional behaviour;adapter template;structural adaptation;pervasive environments structural adaptation component based applications adapter template;object oriented programming;pervasive environments;component based applications;abstracts protocols context runtime java web services connectors;ubiquitous computing;service oriented architecture	Building upon the Service Oriented Architecture, applications can be defined as an assembly of abstract components that are resolved by integrating transparently functionalities provided by heterogeneous resources. However, applications in pervasive environments have to operate under highly dynamic and unpredictable context. This may give rise to several problems denoting that there are mismatches between their descriptions and the execution environments (e.g., heterogeneity of services discovery protocols). Therefore, there is a crucial need to adapt applications in order to overcome these mismatches. In this work, we present a dynamic structural adaptation approach for abstract applications. Our approach is based on the transformation of applications by injecting adapters into their descriptions with respect to their functional behaviour. We propose a fine-grained template to define these adapters. We also give an overview of some results of our evaluations to validate our approach.	component-based software engineering;formal verification;knowledge representation and reasoning;pervasive informatics;service-oriented architecture	Imen Ben Lahmar;Djamel Belaïd	2013	2013 Workshops on Enabling Technologies: Infrastructure for Collaborative Enterprises	10.1109/WETICE.2013.40	embedded system;real-time computing;computer science;operating system;service-oriented architecture;distributed computing;object-oriented programming;world wide web;ubiquitous computing;adaptation	SE	-40.83801205403968	40.05876623790607	17353
46e11292fee0e4c6505d831f8968e2e504256c11	a java internet computing platform via communication libraries			java	Chun-Mok Chung;Pil-Sup Shin;Shin-Dug Kim	1999			computer science;world wide web;internet traffic;internet appliance;the internet;distributed computing;end-user computing;java	Theory	-35.49406526386413	50.15457533605937	17367
ef6749aff4c9b1e958376d2e19b0d27a410282f3	trace comprehension operators for executable dsls		Recent approaches contribute facilities to breathe life into metamodels, thus making behavioral models directly executable. Such facilities are particularly helpful to better utilize a model over the time dimension, e.g., for early validation and verification. However, when even a small change is made to the model, to the language definition (e.g., semantic variation points), or to the external stimuli of an execution scenario, it remains difficult for a designer to grasp the impact of such a change on the resulting execution trace. This prevents accessible trade-off analysis and design-space exploration on behavioral models. In this paper, we propose a set of formally defined operators for analyzing execution traces. The operators include dynamic trace filtering, trace comparison with diff computation and visualization, and graph-based view extraction to analyze cycles. The operators are applied and validated on a demonstrative example that highlight their usefulness for the comprehension specific aspects of the underlying traces.	executable;list comprehension	Dorian Leroy;Erwan Bousse;Anaël Megna;Benoît Combemale;Manuel Wimmer	2018		10.1007/978-3-319-92997-2_19	computer science;programming language;operator (computer programming);theoretical computer science;computation;verification and validation;visualization;grasp;executable;domain-specific language;comprehension	NLP	-54.959962998395824	36.86685693027646	17399
824acbfc04a085f5d076872a0e75a47a4b4c4b14	an incentive compatible reputation mechanism	trust;r agent incentive compatible reputation mechanism large marketplace distributed marketplace software agent security open multiagent environment incentive compatibility side payment scheme broker agent reputation information cryptography integrity protection agent identity agent reputation incentive compatible reputation reporting;incentive compatible reputation reporting;electronic commerce;open multiagent environment;incentive compatibility;information security;application software;software agent;r agent;cryptography software agents open systems electronic commerce distributed shared memory systems;reputation information;side payment scheme;agent reputation;reputation;integrity protection;distributed marketplace;software agents;computer security;reputation mechanism;protection;broker agent;cryptography;distributed shared memory systems;large marketplace;artificial intelligence;incentive compatible reputation mechanism;computer science;open systems;security;environmental management;software agents artificial intelligence laboratories computer science computer security information security application software cryptography protection environmental management;agent identity	Traditional centralized approaches to security are difficult to apply to large, distributed, multi-agent systems (MAS). Developing a notion of trust that is based on the reputation of agents can provide a softer notion of security that is sufficient for many MAS applications. The most reliable reputation information can be derived from an agent’s own experience. However, much more data becomes available when reputation information is shared among an agent community. Such mechanisms have been proposed and also practically implemented. The various rating services on the Internet as well as research results presented in [5, 6] are examples of such mechanisms. It is however not at all clear that it is in the best interest of an agent to truthfully report reputation information because (1) reporting any reputation information provides a competitive advantage to others, and (2) by reporting positive ratings, the agent slightly decreases its own reputation with respect to the average of other agents, so it may be a disadvantage to report them truthfully. For centralized auctioning systems, the problem of eliciting honest reporting is addressed in [2] and [4]. In this paper we present a decentralized reputation mechanism that is incentive-compatible and secure enough to be used in real world applications.	centralized computing;internet;multi-agent system	Radu Jurca;Boi Faltings	2003		10.1145/860575.860778	computer science;artificial intelligence;information security;software agent;internet privacy;computer security	ECom	-43.62118971398095	56.32257008893484	17473
8a816752a3b83bad54d7dc877644a87f845a0e41	implementation of a pc-based robot controller with open architecture	software performance evaluation;open architecture robot control system;corba;robust control;robotics;object oriented programming;open architecture;software performance evaluation controllers distributed object management object oriented programming open systems robots robust control;robot control;pc based controller;robotics corba open architecture pc based controller;controllers;robots;software development;distributed object management;movemaster ex robot;software component;open architecture system;pc based robot controller;robot control computer architecture hardware control systems automatic control programming scalability communication system control application software computer industry;open systems;movemaster ex robot pc based robot controller open architecture system open architecture robot control system corba software development	The demand for increased capability and flexibility has led to a dramatic increase in the use of controllers based on open architecture. Knowledge from past research on open architecture system indicates that a component-based philosophy should be the solution. In this paper, a PC-based open architecture robot control system is presented and the technologies related to component-ware are investigated. By assembling controller from off-the-shell hardware and software components, the benefits of reduced cost and improved robustness have been realized. CORBA is exploited in software development to ensure the extensibility, scalability and portability of the software. Finally, by applying to the Movemaster-EX robot, the performance of the system is evaluated	apache axis;common object request broker architecture;component-based software engineering;computer science;control system;emoticon;extensibility;high- and low-level;high-level programming language;linux;microsoft windows;open architecture;pc system design guide;parabolic antenna;qnx;rtlinux;real-time clock;real-time computing;real-time operating system;real-time transcription;reduced cost;robot control;scalability;software development;software portability;step response;warez	Liandong Pan;Xinhan Huang	2004	2004 IEEE International Conference on Robotics and Biomimetics	10.1109/ROBIO.2004.1521883	robust control;robot;control engineering;reference architecture;embedded system;space-based architecture;real-time computing;open architecture;computer science;artificial intelligence;component-based software engineering;software development;common object request broker architecture;hardware architecture;robot control;open system;robotics;object-oriented programming;resource-oriented architecture;systems architecture	Robotics	-34.25113807894781	37.056636414070184	17515
990391b3d7a5acae140d8d3d441a8594e81d1888	hidden process detection using kernel functions instrumentation		Process hiding is a common attack used by long-lived malicious processes to conceal their presence from security and administration tools. Multiple techniques based on Virtual Machine Introspection (VMI) were proposed to detect the presence of hidden running process in virtual machines. However, existing techniques are not practical for real world cloud environments as they suffer from evasion attacks or use manually provided and too OS-specific information. In this paper we present HPD, a VMI-based Hidden Process Detector that instruments guest OS kernel functions to automatically and reliably detect and terminate execution of hidden processes. We designed and implemented a prototype of HPD on KVM hypervisor. Its evaluation on multiple Linux kernels shows that from the hypervisor level, HPD detects successfully the presence of hidden running processes and safely terminate their execution.	evasion (network security);hypervisor;introspection;linux;malware;operating system;prototype;terminate (software);virtual machine	Yacine Hebbal;Sylvie Laniepce;Jean-Marc Menaud	2017	2017 IEEE Conference on Dependable and Secure Computing	10.1109/DESEC.2017.8073805	kernel (statistics);embedded system;virtual machine introspection;hypervisor;cloud computing;real-time computing;virtual machine;detector;instrumentation;engineering	Security	-55.959508235015086	57.289831277190345	17612
a03ddf4d95ff050a70294e40369f236227d0ebd2	combining smart phone and infrastructure sensors to improve security in enterprise settings	security;context	There is an increasing trend among employees to bring in their own personal device to work, thereby making the enterprise more vulnerable to security attacks such as data leakage from phones. Additionally, users are increasingly running phone apps in a mixed-mode i.e. both for enterprise and personal commitments. For example, phone cameras and microphones are used to record business meetings, often resulting in the case that both employers and employees become unaware of the existence of business data on the phone at a later point in time. The lack of employer control over personal devices raises enterprise data leakage threats, when an employee's phone is lost or stolen. In this paper we describe a system that leverages sensors available on the phone as well as on the enterprise infrastructure to identify business data resident on the phone for further secure handling. Office spaces have traditionally been instrumented with badge swipe readers, cameras, wifi access points etc. that can be used to provide passive sensory data about employees. For example, badge swipes can be used provide approximate location information of an employee where as calendar entries provide information about their schedule and activities. We propose a distributed architecture that leverages the context of the user for speculatively identifying enterprise data from personal data. The basic idea is to understand whether a user is engaged in enterprise or personal work by inferring her context from a combination of phone and infrastructure sensors. The contextual attributes in our system, such as location, can be sourced from a plurality of sensors on the phone as well as on the infrastructure. We exploit this diversity and propose a cost optimized distributed rule execution framework that chooses the optimal set of predicates to sense on the phone as well as on the infrastructure to reduce sensing cost. Furthermore, the framework also chooses the appropriate site for rule evaluation, either on the infrastructure or phone, to optimize for network transfer cost incurred due to shipping of sensed predicates between the two sites. Combined together,the above two optimizations reduce the battery drain caused due to context inferencing on the phone.	approximation algorithm;distributed computing;microphone;mixed-signal integrated circuit;mobile app;personally identifiable information;sensor;smartphone;spectral leakage;web badge;wireless access point	Palanivel A. Kodeswaran;Dipanjan Chakraborty;Parikshit Sharma;Sougata Mukherjea;Anupam Joshi	2013		10.1145/2494091.2499773	embedded system;human–computer interaction;gsm services;computer science;information security;operating system;internet privacy;world wide web;computer security;computer network	Security	-42.10774073712257	58.8667510889903	17632
9c8b72dc2b6f644c7a3095b3045bfe9c39b86991	policy provisioning and its access control beyond administrative and collaborative domains		A policy provisioning framework is described that supports management of the lifecycle of personal information and its data-handling policies distributed beyond security domains. A model for creating data-handling policies reflecting the intentions of its system administrator and the privacy preferences of the data owner is explained. Also, algorithms for systematically propagating and integrating data-handling policies from system entities in different administrative domains are presented. This framework enables data-handling policies to be properly deployed and enforced in a way that enhances security and privacy.	administrative domain;algorithm;computer science;computer security;control system;credential;database;elisa bertino;enterprise privacy authorization language;entity;expert system;final fantasy tactics advance;identity governance framework;identity management system;institute of electronics, information and communication engineers;international federation for information processing;interoperation;markup language;p (complexity);p3p;personally identifiable information;privacy policy;provisioning;real-time cmix;role-based access control;specification language;sticky bit;system administrator;systems science;trust management (information system);vldb;xacml	Hidehito Gomi	2012	JIP	10.2197/ipsjjip.20.196	knowledge management;computer security	DB	-48.36989784556277	53.67642264017014	17694
60eb1895b16f393a23950c2c32509e0da427d988	understanding and communicating it security specifications with uml	security assurance security engineering;uml use case diagrams;it security;it security objectives	Security specifications of IT products and systems are inherently complex and may subject products to semantic threats due to misunderstanding of key aspects of security objectives by developers, customers and end users. A study is conducted on expressing the security specifications by specially interpreted UML use case diagrams to avoid misunderstanding by peer groups, i.e. to prevent semantic threats at the development phase through improved comprehension of security specifications. We base our results on engineering frameworks for comprehensive security and demonstrate the need for improved communication by concrete examples of semantic threats. The threats result from the use of complex textual artifacts as a means of communicating the security requirements. We demonstrate the use of a diagrammatic technique for expressing and communicating security specifications in a less ambiguous manner and illustrate how the technique assists in communication.	diagram;requirement;unified modeling language	Jussipekka Leiwo;Teemupekka Virtanen	2005	International Journal of Software Engineering and Knowledge Engineering	10.1142/S0218194005002580	software security assurance;information security audit;computer security model;standard of good practice;cloud computing security;reliability engineering;security through obscurity;security information and event management;security engineering;covert channel;asset;computer science;systems engineering;information security;software engineering;human-computer interaction in information security;security service;security testing;computer security	Security	-55.21124318357229	48.23946880120046	17696
07bea24b823f6a9eb186cf8efe84b31be1639efb	application of the operational profile in software performance analysis	redundant computation;software performance;program optimization;operational profile;performance analysis;software reliability;software performance analysis	Most of the existing operational profile studies have been proposed for software reliability. Operational profile describes how the program to be used. Our experience with software performance analysis shows that the operational profile may play a role in the performance analysis of a program. In this paper we investigate the application of the operational profile in software performance analysis. We introduce a new type of performance deficiency referred to as redundant computation and show how the operational profile can guide in detecting such deficiency. Our initial experiment shows the application of operational profile in enhancing program's performance.	angular defect;computation;profiling (computer programming);sensor;software performance testing;software reliability testing	Zakarya A. Alzamil	2004		10.1145/974044.974053	reliability engineering;real-time computing;software performance testing;computer science;systems engineering;program optimization;operational acceptance testing;programming language;software quality	SE	-62.08815050561893	33.478792480001005	17716
b26cca250f05875b3c030cb9574020b239399ba0	network hardening		In defending networks against potential intrusions, certain vulnerabilities may seem acceptable risks when considered in isolation, whereas an intruder may combine such vulnerabilities for a multi-step intrusion and successfully infiltrate a seemingly well-guarded network. Relying on human analyst’s experiences and skills to identify such a threat is error-prone and renders the task of network hardening an art, rather than a science. Existing tools based on attack graphs can reveal such threats by enumerating all possible attack paths leading to critical resources, but they cannot provide a direct solution to remove the threats. In this book, we introduce automated solutions for hardening a network against sophisticated multi-step intrusions. Specifically, we first review necessary background information on related concepts, such as attack graphs and their application to network hardening. We then describe a network hardening technique to generate hardening solutions comprised of initially satisfied conditions, which makes the solution more enforceable. Following a discussion of the complexity issues, we devise an improved technique that takes into consideration the dependencies between hardening options and employs a near-optimal approximation algorithm to scale linearly with the size of the inputs, whose performance is validated experimentally.	approximation algorithm;cognitive dimensions of notations;experience;experiment;hardening (computing);rendering (computer graphics);threat (computer)	Lingyu Wang;Massimiliano Albanese;Sushil Jajodia	2014		10.1007/978-3-319-04612-9		Security	-60.530789540520495	57.44017472950726	17746
28e7e0b12019c9b5a86b74fecfcae94fe9848670	oracle-based regression test selection	choppers circuits;regression test selection;regression testing;test oracles;iso standards;testing fault detection choppers circuits iso standards security software systems buffer storage;software systems;buffer storage;testing;program testing;test oracles regression testing regression test selection;fault detection;oracle based regression test selection regression testing cost reduction software system test case selection industrial systems abb fault detection internal oracle rules system states system execution program chopping code change identification empirical study rts techniques;security;fault diagnosis;program testing fault diagnosis	Regression test selection (RTS) techniques attempt to reduce regression testing costs by selecting a subset of a software system's test cases for use in testing changes made to that system. In practice, RTS techniques may select inordinately large sets of test cases, particularly when applied to industrial systems such as those developed at ABB, where code changes may have far-reaching impact. In this paper, we present a new RTS technique that addresses this problem by focusing on specific classes of faults that can be detected by internal oracles - oracles (rules) that enforce constraints on system states during system execution. Our technique uses program chopping to identify code changes that are relevant to internal oracles, and selects test cases that cover these changes. We present the results of an empirical study that show that our technique is more effective and efficient than other RTS techniques, relative to the classes of faults targeted by the internal oracles.	data dependency;open-source software;regression testing;software system;test case	Tingting Yu;Xiao Qu;Mithun Acharya;Gregg Rothermel	2013	2013 IEEE Sixth International Conference on Software Testing, Verification and Validation	10.1109/ICST.2013.34	reliability engineering;regression testing;real-time computing;computer science;engineering;information security;operating system;software engineering;software testing;computer security;fault detection and isolation;software system	SE	-61.68229336467888	34.56161008299124	17777
0499b609ebf41fe4c6fdf4675f2708129a925158	real-world identification: towards a privacy-aware mobile eid for physical and offline verification	electronic identities;requirements;privacy;mobile eid	There are many systems that provide users with an electronic identity (eID) to sign documents or authenticate to online services (e.g. governmental eIDs, OpenID). However, current solutions lack in providing proper techniques to use them as regular ID cards that digitally authenticate their holders to another physical person in the real world. We envision a fully mobile eID which provides such functionality in a privacy-preserving manner, fulfills requirements for governmental identities with high security demands (such as driving licenses, or passports) and can be used in the private domain (e.g. as loyalty cards). In this paper, we present potential use cases for such a flexible and privacy-preserving mobile eID and discuss the concept of privacy-preserving attribute queries. Furthermore, we formalize necessary functional, mobile, security, and privacy requirements, and present a brief overview of potential techniques to cover all of them.	authentication;digital identity;e-services;mobile security;online and offline;openid;privacy;requirement	Michael Hölzl;Michael Roland;René Mayrhofer	2016		10.1145/3007120.3007158	requirements analysis;computer science;internet privacy;privacy;world wide web;computer security	Security	-45.782960477972	60.04560346211645	17880
641563122a6cbc29a30d488dd583d3cf6cb2605a	rapyuta: a cloud robotics platform	protocols;platform as a service paas cloud robotics cloud based mapping networked robots;cloud computing middleware parallel processing programming environments public domain software robot programming software packages;data structures;robots;data throughput network connection latency network connection quality performance optimization collaborative real time 3d mapping application task grasp planning object recognition ecosystem mobiles phones browsers ros based robots asynchronous communication mechanisms websocket based communication protocol parallel computing architectures amazon data center ami amazon machine image robotic middleware ros packages outsourcing multiprocess high bandwidth robotics google app engine commercial data center robot onboard computational processes proof of concept demonstrations benchmarking use cases robotic teams deployment roboearth knowledge repository secured customizable computing environments open source cloud robotics platform rapyuta;robots protocols containers data structures open source software cloud computing;software packages cloud computing middleware parallel processing programming environments public domain software robot programming;rapyuta data throughput network connection latency network connection quality performance optimization collaborative real time 3d mapping application task grasp planning object recognition ecosystem mobiles phones browsers ros based robots asynchronous communication mechanisms websocket based communication protocol parallel computing architectures amazon data center ami amazon machine image robotic middleware ros packages outsourcing multiprocess high bandwidth robotics google app engine commercial data center robot onboard computational processes proof of concept demonstrations benchmarking use cases robotic teams deployment roboearth knowledge repository secured customizable computing environments open source cloud robotics platform	In this paper, we present the design and implementation of Rapyuta, an open-source cloud robotics platform. Rapyuta helps robots to offload heavy computation by providing secured customizable computing environments in the cloud. The computing environments also allow the robots to easily access the RoboEarth knowledge repository. Furthermore, these computing environments are tightly interconnected, paving the way for deployment of robotic teams. We also describe three typical use cases, some benchmarking and performance results, and two proof-of-concept demonstrations. Note to Practitioners - Rapyuta allows to outsource some or all of a robot's onboard computational processes to a commercial data center. Its main difference to other, similar frameworks like the Google App Engine is that it is specifically tailored towards multiprocess high-bandwidth robotics applications/middlewares and provides a well-documented open-source implementation that can be modified to cover a large variety of robotic scenarios. Rapyuta supports the outsourcing of almost all of the current 3000+ ROS packages out of the box and is easily extensible to other robotic middleware. A pre-installed Amazon Machine Image (AMI) is provided that allows to launch Rapyuta in any of Amazon's data center within minutes. Once launched, robots can authenticate themselves to Rapyuta, create one or more secured computational environments in the cloud and launch the desired nodes/processes. The computing environments can also be arbitrarily connected to build parallel computing architectures on the fly. The WebSocket-based communication protocol, which provides synchronous and asynchronous communication mechanisms, allows not only ROS based robots, but also browsers and mobiles phones to connect to the ecosystem. Rapyuta's computing environments are private, secure, and optimized for data throughput. However, its performance is in large part determined by the latency and quality of the network connection and the performance of the data center. Optimizing performance under these constraints is typically highly application-specific. The paper illustrates an example of performance optimization in a collaborative real-time 3-D mapping application. Other target applications include collaborative 3-D mapping, task/grasp planning, object recognition, localization, and teleoperation, among others.	amazon machine image;authentication;cloud computing;cloud robotics;communications protocol;computation;data center;ecosystem;google app engine;mathematical optimization;middleware;on the fly;open-source software;optimizing compiler;out of the box (feature);outline of object recognition;outsourcing;parallel computing;performance tuning;pre-installed software;rapyuta;real-time clock;roboearth;robot operating system;software deployment;thinking outside the box;throughput;websocket	Mohanarajah Gajamohan;Dominique Hunziker;Raffaello D'Andrea;Markus Waibel	2015	IEEE Transactions on Automation Science and Engineering	10.1109/TASE.2014.2329556	robot;embedded system;communications protocol;simulation;data structure;cloud computing;computer science;operating system;distributed computing	Robotics	-34.382460143274486	53.77937768321814	17943
e61978701e849bbca6e3148adac2d77c7015c3eb	smartpowerchair: a pervasive system of systems	usability powerchair pervasive technology system of systems assistive technology accessibility;standards;vehicles usability relays modeling communities standards;constituent system smartpowerchair standard powered wheelchair quality of life enhancement independent living traditional systems engineering finite number high level two dimensional system of systems model pervasive technologies capability levels component levels usability evaluations workload measurements;vehicles;communities;relays;wheelchairs handicapped aids ubiquitous computing;usability;modeling	This paper presents the characterisation of a concept System of Systems called the SmartPowerchair, in which existing pervasive technologies are integrated into a standard powered wheelchair to enhance the quality of life through independent living. Traditional Systems Engineering focuses on building the right system whereas System of Systems focuses on selecting the right combination of systems and their interactions to satisfy a set of frequently changing requirements. The SmartPowerchair can be characterised as a System of Systems due to the integration of a finite number of constituent systems which are independent and interoperable, and networked together for a period of time to achieve a certain higher goal. A high-level two-dimensional System of Systems model is developed to illustrate the lifecycle stages of System of Systems and different levels including the Component, System, System of Systems and Capability levels. Usability evaluations and workload measurements of a constituent system is also provided.	high- and low-level;interaction;interoperability;pervasive informatics;requirement;system of systems;systems engineering;the quality of life;usability	Paul Whittington;Huseyin Dogan	2015	2015 10th System of Systems Engineering Conference (SoSE)	10.1109/SYSOSE.2015.7151932	usability goals;embedded system;component-based usability testing;simulation;systems modeling;usability;system of systems;computer science;systems engineering;engineering;operating system;computer security;systems design	SE	-40.989017991029414	43.37651769676415	17978
5857a9ca0b4455da3f580531e48677c4aa8f4a39	micro-architecture support for integrity measurement on dynamic instruction trace	remote attestation;integrity measurement;software vulnerability;trusted computing	Trusted computing allows attesting remote system’s trustworthiness based on the software stack whose integrity has been measured. However, attacker can corrupt system as well as measurement operation. As a result, nearly all integrity measurement mechanism suffers from the fact that what is measured may not be same as what is executed. To solve this problem, a novel integrity measurement called dynamic instruction trace measurement (DiT) is proposed. For DiT, processor’s instruction cache is modified to stores back instructions to memory. Consequently, it is designed as a assistance to existing integrity measurement by including dynamic instructions trace. We have simulated DiT in a full-fledged system emulator with level-1 cache modified. It can successfully update records at the moment the attestation is required. Overhead in terms of circuit area, power consumption, and access time, is less than 3% for most criterions. And system only introduces less than 2% performance overhead in average.	access time;cpu cache;emulator;overhead (computing);trust (emotion);trusted computing	Hui Lin;Gyungho Lee	2010	J. Information Security	10.4236/jis.2010.11001	embedded system;real-time computing;vulnerability;computer science;trustworthy computing;computer security	Arch	-54.2829233317757	55.64582018902932	18073
6233f5a183acfa75cf4046eed6ec6a4a3d23cb34	mettle fatigue: vw's single-point-of-failure ethics	software;standards;volkswagen;systems engineering;failure analysis automobiles automotive engineering ethical aspects;software engineering;vw single point of failure ethics mettle fatigue engineering ethics dieselgate defeat device software diesel vehicles volkswagen;professional ethics;defeat device;security;professional ethics automobile industry defeat device volkswagen security software software engineering standards systems engineering;automotive electronics companies ethics engines licenses;automobile industry	"""After a year of denials, Volkswagen admitted in August/September 2015 that multiple makes and models of its diesel vehicles contained defeat device software. The decisions leading to """"Dieselgate"""" involved a corruption of engineering ethics that the profession ought to address."""	diesel;single point of failure	Roland L. Trope;Eugene K. Ressler	2016	IEEE Security & Privacy	10.1109/MSP.2016.6	professional ethics;computer science;automotive industry;information security;computer security	Security	-59.214253005350606	48.980531502426494	18139
13e7fa0da4ef1903cf07b738ab8e6b397a08c025	computing program weakness using module coupling	computer program;organization;uml;object;multi agent systems mas;role;agent;modeling	In this paper, a method has been developed to measure the weakness of the program using module weakness and different types of module coupling. The paper justifies the need of considering coupling effect on program weakness. Various types of coupling are computed using parameters/variables and module weakness is found using average number of live variables and average life of variables in the module. The module coupling and module weakness values are used to compute program weakness, which can be used to indicate the maintainability and testability of the program.	software testability	K. K. Aggarwal;Yogesh Singh;Jitender Kumar Chhabra	2002	ACM SIGSOFT Software Engineering Notes	10.1145/566493.566497	reliability engineering;unified modeling language;real-time computing;simulation;systems modeling;computer science;organization;engineering;object;role;programming language	SE	-49.38327108947939	34.76415179684856	18153
626ecc03d13cab950ea681c492ada39e44cedda8	cybertrust in the iot age		IoT generates new opportunities but creates new challenges with respect to trustworthiness [1]. Computing, architecture, and verification changes are inevitable to meet these challenges, particularly if predictions of 20 billion to 50 billion new IoT devices being created within the next three years come true. What will be required to provide trust in IoT? And what new opportunities will IoT bring to the computing profession and to consumers? To better understand this, let’s look at a few key concerns.	cybertrust;trust (emotion)	Jeffrey M. Voas;D. Richard Kuhn;Constantinos Kolias;Angelos Stavrou;Georgios Kambourakis	2018	IEEE Computer	10.1109/MC.2018.3011043	computer science;computer security;cyber-physical system;ubiquitous computing;big data;cybercrime;internet of things	Arch	-44.74696515446821	50.409845113011386	18169
e8204028db1d9edb64dff834d4aea8aa9c1e25c8	maintaining soa systems of the future - how can ontological modeling help?		Many future Services Oriented Architecture (SOA) systems may be pervasive SmartLife applications that provide real-time support for users in everyday tasks and situations. Development of such applications will be challenging, but in this position paper we argue that their ongoing maintenance may be even more so. Ontological modelling of the application may help to ease this burden, but maintainers need to understand a system at many levels, from a broad architectural perspective down to the internals of deployed components. Thus we will need consistent models that span the range of views, from business processes through system architecture to maintainable code. We provide an initial example of such a modelling approach and illustrate its application in a semantic browser to aid in software maintenance tasks.	business process;real-time locating system;software maintenance;systems architecture	Bilal Gonen;Xingang Fang;Eman El-Sheikh;Sikha Bagui;Norman Wilde;Alfred Zimmermann;Ilia Petrov	2014		10.5220/0005132903760381		SE	-42.41321921391641	39.29631577396	18221
4a8cb930504989675acc20308276f7b31c60a98b	a method for developing algorithms for assessing cyber-risk cost		We present a method for developing executable algorithms for quantitative cyber-risk assessment. Exploiting techniques from security risk modeling and actuarial approaches, the method pragmatically combines use of available empirical data and expert judgments. The input to the algorithms are indicators providing information about the target of analysis, such as suspicious events observed in the network. Automated execution of the algorithms facilitates continuous assessment.	algorithm;documentation;executable;financial risk modeling;risk assessment	Gencer Erdogan;Alejandra Gonzalez;Atle Refsdal;Fredrik Seehusen	2017	2017 IEEE International Conference on Software Quality, Reliability and Security (QRS)	10.1109/QRS.2017.29	skeleton (computer programming);probabilistic analysis of algorithms;data modeling;data mining;continuous assessment;algorithm;artificial intelligence;executable;machine learning;computer science	SE	-60.46758432349855	46.093105003205665	18225
f2020f878adbec789e7c7c2015ac48568f0771f3	efficient processing of location-cloaked queries	location cloaking;location based service;query processing data privacy mobile computing;measurement;query processing;air indexing;servers indexing schedules query processing measurement batteries;privacy protection;air indexing location cloaking query processing scheduling;servers;indexing;data privacy;scheduling;indexation;batteries;schedules;mobile computing;lcq location cloaked queries location based services circular geographic region rectangular geographic region privacy protection query processing	When requesting location-based services, users can associate their queries with a purposely blurred location such as a circular or rectangular geographic region instead of their exact position. This strategy makes it possible for privacy protection, but presents problems in query processing. Since the server does not know a user's exact position, it has to retrieve query results for each position inside the user's cloaking region. While the server workload dramatically increases, a client downloading all query results will waste its battery power, because most of the data may be irrelevant to its query interest. This paper considers the problems of efficient processing of location-cloaked queries (LCQs). Our key observation is that queries may overlap in their cloaking regions and thus share some query results. In light of this, we propose to process queries as a batch instead of one by one independently. The technical contributions of this paper are threefold. 1) We propose to decompose queries into subqueries based on their interested region. Since the subqueries with a common region need to be processed only once, the server workload is minimized. 2) We propose a novel scheduling technique that addresses the dilemma between minimizing server latency and ensuring good fairness in query processing. 3) We present a personalized air indexing technique by which a client can filter out and download only the needed query results, thus avoiding the waste of energy in downloading irrelevant data.	database;download;fairness measure;location-based service;personalization;privacy;relevance;sql;scheduling (computing);server (computing)	Patricio Galdames;Ying Cai	2012	2012 Proceedings IEEE INFOCOM	10.1109/INFCOM.2012.6195639	search engine indexing;query optimization;query expansion;web query classification;schedule;computer science;operating system;location-based service;database;internet privacy;web search query;mobile computing;range query;scheduling;world wide web;server;measurement;spatial query	DB	-38.74394147531633	60.22389507018701	18235
01ab286cddbcedb38ed318ff3216c4c9590123d4	corrigendum to new rational parties relying on reputation				Yilei Wang;Zhe Liu;Qiuliang Xu	2016	Security and Communication Networks	10.1002/sec.1477	computer security	Crypto	-43.45478240100678	57.44331912228466	18324
301abad8b7adaeb65fc08d2e426bf8a21d892511	a model-driven approach for generating embedded robot navigation control software	embedded coding;modeling technique;domain specific modeling;robot navigation;real time embedded system;general techniques;embedded system;critical system;visual modeling;model integrated computing;undergraduate research;generative programming;lego mindstorms;generic programming	Real-time embedded systems are time-critical systems that are hard to implement as compared to traditional commercial software, due to the large number of conflicting requirements. This paper describes undergraduate research into the use of advanced modeling techniques to improve the development of embedded systems. In particular, we have developed domain-specific models that describe the configuration and layout of a hazardous environment, which is symbolically represented as an area contaminated with hazardous materials (e.g., land mines), as well as objects to be rescued (e.g., babies). The motivation is to model a disaster site that is too dangerous for humans to search for survivors. From the visual model specifications, model interpreters will generate the embedded code that will control two LEGO Mindstorms robots. The mission of the robots is to traverse the hostile terrain and rescue the surviving babies. The modeling environment and generative techniques are described.	commercial software;domain-specific language;embedded system;lego mindstorms;model-driven architecture;real-time transcription;requirement;robot;robotic mapping;traverse;visual modeling;window of opportunity	Bina Shah;Rachael Dennison;Jeffrey G. Gray	2004		10.1145/986537.986618	embedded system;real-time computing;simulation;engineering	Robotics	-43.71922481553138	34.93252232820985	18370
e3a1f864629a246f40ced9aa537aff9051873609	hybrid role hierarchy for the extended role based access control model	object oriented methods authorisation inference mechanisms;electronic mail;object oriented methods;complexity theory;access control model extended role;hybrid role hierarchy;information science;authorisation;inference rule hybrid role hierarchy access control model extended role hybrid inheritance inheritance relations;inference mechanisms;role based access control;inference rule;computational modeling;permission;access control models;access control permission computational modeling information science security electronic mail complexity theory;inheritance relations;access control;security;hybrid inheritance	Role based access control (RBAC) has emerged as a leading access control model to other traditional access control models. However, the traditional RBAC models can not capture fine-grained authorization with mono-type inheritance. In this paper, we discuss the hybrid inheritance based on our extended RBAC model, which is very desirable for capturing the fine-grained access control permissions. When the hybrid inheritances coexist in a role hierarchy, inferring such indirect relations between a pair of roles can became very complex. In particular, we study how the new inheritance relations between roles that are indirectly related can be easily derived through the inference rules, which provides a basis for formally analyzing the hybrid inheritances.	authorization;coexist (image);role hierarchy;role-based access control	Zhenxing Luo;Jing Chen;Zuoquan Lin	2008	2008 33rd IEEE Conference on Local Computer Networks (LCN)	10.1109/LCN.2008.4664239	information science;computer science;access control;role-based access control;data mining;database;authorization;computational model;computer security;rule of inference	Security	-49.15871693859157	50.825298229229475	18431
ec16746118e4bbf8cf7b5a1aee891f8f2b5e96df	developing models for physical attacks in cyber-physical systems		In this paper, we analyze the security of cyber-physical systems using the ADversary VIew Security Evaluation (ADVISE) meta modeling approach, taking into consideration the effects of physical attacks. To build our model of the system, we construct an ontology that describes the system components and the relationships among them. The ontology also defines attack steps that represent cyber and physical actions that affect the system entities. We apply the ADVISE meta modeling approach, which admits as input our defined ontology, to a railway system use case to obtain insights regarding the system's security. The ADVISE Meta tool takes in a system model of a railway station and generates an attack execution graph that shows the actions that adversaries may take to reach their goal. We consider several adversary profiles, ranging from outsiders to insider staff members, and compare their attack paths in terms of targeted assets, time to achieve the goal, and probability of detection. The generated results show that even adversaries with access to noncritical assets can affect system service by intelligently crafting their attacks to trigger a physical sequence of effects. We also identify the physical devices and user actions that require more in-depth monitoring to reinforce the system's security.	adversary (cryptography);cyber-physical system;entity;hardening (computing);ontology (information science);vulnerability (computing)	Carmen Cheh;Ken Keefe;Brett Feddersen;Binbin Chen;William G. Temple;William H. Sanders	2017		10.1145/3140241.3140249	data mining;insider;cyber-physical system;ontology;adversary;ranging;system model;engineering;graph	Security	-55.45991553599711	48.172392323261654	18449
9f2c06206235aa76c54f62e4b4a3e81db1463d51	automating the addition of fault tolerance with discrete controller synthesis	parallel composition;fault tolerant;labeled transition system;satisfiability;embedded system;programming model;controller synthesis;discrete controller synthesis;fault tolerant system;automatic fault tolerance;model checking;fault tolerant systems;state space;power consumption	Discrete controller synthesis (DCS) is a formal approach, based on the sam state-space exploration algorithms as model-checking. Its interest lies in the ability to obtain automatically systems satisfying by construction formal properties specified a priori. In this paper, our aim is to demonstrate the feasibility of this approach for fault tolerance. We start with a fault intolerant prog ram, modeled as the synchronous parallel composition of finite labeled transition systems; we specify formally a fault hypothesis; we state some fault tolerance requirements; and we use DCS to obtain automatically a program, having the same behavior as the initial fault intolerant one in the absence of faults, and satis fying the fault tolerance requirements under the fault hypothesis. Our original contribution resid es in the demonstration that DCS can be elegantly used to design fault tolerant systems, with guarantees on key properties of the obtained system, such as the fault tolerance level, the satisfaction of quantitative co nstraints, and so on. We show with numerous examples taken from case studies that our method can add ress different kinds of failures (crash, value, or Byzantine) affecting different kinds of hardware components (processors, communication links, actuators, or sensors). Besides, we show that our method also o ffers an optimality criterion very useful to synthesize fault tolerant systems compliant to the constraints of embedd ed systems, like power consumption.	algorithm;central processing unit;computer programming;crash (computing);fault tolerance;model checking;optimality criterion;random-access memory;requirement;sensor;speech synthesis;state space	Alain Girault;Éric Rutten	2009	Formal Methods in System Design	10.1007/s10703-009-0084-y	fault tolerance;real-time computing;fault coverage;computer science;stuck-at fault;segmentation fault;fault model;distributed computing;programming language;general protection fault;software fault tolerance	Embedded	-35.13859001787099	33.492562049263746	18458
afca3d8c3524a8c71aea87222c58e405a17dd53d	automatically testing web services choreography with assertions	service composition;ws cdl;automatic testing;web service;satisfiability;symbolic execution;web services;choreography;test oracle	Web Service Choreography Description Language gives a global view on the collaborations among a collection of services involving multiple participants or organizations. Since WS-CDL is aimed at a design specification for service composition, there are few approaches to be proposed to test WS-CDL programs. In this paper, we present an approach to testing WS-CDL programs automatically. The dynamic symbolic execution technique is applied to generate test inputs and assertions are treated as the test oracles. Moreover, a simulation engine for WS-CDL is used to perform the execution of WS-CDL programs during the process of symbolic execution. At the end of each execution, the path conditions collected by symbolic execution are put into a SMT solver to generate new input data that will guide the next simulation. Meanwhile, the SMT solver is applied to decide whether the assertion predicates can be satisfied under current path conditions for all test data which improves the quality of testing further.	web service	Lei Zhou;Jing Ping;Hao Xiao;Zheng Wang;Geguang Pu;Zuohua Ding	2010		10.1007/978-3-642-16901-4_11	web service;real-time computing;computer science;database;programming language;concolic testing	SE	-56.3877458043203	32.494014690159744	18589
e2b789bdbfbb8189f94f459d949da17e90ecebdd	an intelligent platform for hosting medical collaborative services		Recent developments in cloud computing technologies, the widespread use of mobile smart devices and the expansion of electronic health record system, raise the need of on-line collaboration among geographically distributed medical personnel. In this context, the paper presents a web based intelligent platform, capable of hosting medical collaborative services and featuring intelligent medical data management and exchange. Our work emphasizes on client-side medical data processing over an intelligent online workflow library. We introduce a Remote Process Calling scheme based on WebRTC (peer-to-peer) communication paradigm, eliminating the typical bandwidth bottleneck of centralized data sharing and allowing the execution of intelligent workflows.		Christos Andrikos;Ilias Maglogiannis;Efthymios Bilalis;George Spyroglou;Panayiotis Tsanakas	2014		10.1007/978-3-319-07064-3_28	web application;computer network;data mining;computer science;data management;webrtc;workflow;cloud computing;bottleneck;telemedicine;data sharing	Web+IR	-33.74124671209533	54.397568198398474	18637
9c81f039dfc451f035575bf92a1bd58a29fc0b55	sqlidds: sql injection detection using query transformation and document similarity	phrase similarity;database firewall;sql injection detection;query transformation;document similarity;sql injection prevention	SQL Injection Attack has been a major security threat to web applications since last 15 years. Nowadays, hackers use automated tools to discover vulnerable websites and launch mass injection attacks. Accurate run-time detection of SQL injection has been a challenge in spite of extensive research in this area. This paper presents a novel approach for real-time detection of SQL injection attacks using query transformation and document similarity measure. Acting as a database firewall, the proposed system named SQLiDDS, can protect multiple web applications using the database server. With additional inputs from human expert, SQLiDDS can also become more robust over time. Our experimental results confirm that this approach can effectively detect and prevent all types of SQL injection attacks with good accuracy yet negligible impact on system performance. The approach was tested on web applications built using PHP and MySQL, however it can be easily adopted in other platforms with minimal changes.	sql injection	Debabrata Kar;Suvasini Panigrahi;Srikanth Sundararajan	2015		10.1007/978-3-319-14977-6_41	data transformation services;sql injection;stored procedure;computer science;query by example;user-defined function;data mining;database;sql/psm;language integrated query;world wide web	DB	-57.62953333646239	59.04070541975324	18644
0459258bb1d45711ee3fe7edfe511abc394ff723	hybrid approach for selective delivery of information streams in data-intensive monitoring systems	monitoring systems;information filtering;complex event processing cep;semantic web sw technologies	BackgroundPresent day control and monitoring systems are equipped with a large number of heterogeneous devices and are operated by many users with different roles and responsibilities. The information generated by these devices, although preprocessed and filtered, is usually delivered to users regardless of their actual information needs, thus overwhelming cognitive capacities and potentially affecting safety of the system. ObjectivesThis work aims to reduce information load to the users of the data-intensive monitoring systems by delivering selected information to each user based on his/her roles in the system and responsibilities. MethodsThe proposed approach combines Semantic Web (SW) technologies and Complex Event Processing (CEP) for configuration purposes and run-time analyzing. The approach is exemplified with implemented tools and feasibility study based on the performance tests. The paper describes principles of proposed approach, demonstrates illustrative scenarios from building automation domain, gives description of implemented tools, and presents results of the initial performance tests. ResultsThe combination of SW and CEP brings two major advantages: (1) the behavior of the system could be easily changed by configuring only underlying ontology and (2) utilization of CEP at runtime makes system event-driven and reactive to frequent changes in the environment. The performance tests demonstrated the response time of implemented tools within one second for 1000 updates per second (which corresponded to 10,000 devices in the performed experiments). ConclusionsIt is expected that the proposed approach is able to make monitoring systems personal oriented and thus safer during the operation. The results of the performance tests suggest feasibility of the approach for such systems as building and home automation, and non-critical industrial automation.	data-intensive computing	Yulia Evchina;José L. Martínez Lastra	2016	Advanced Engineering Informatics	10.1016/j.aei.2016.07.006	embedded system;real-time computing;simulation;engineering;artificial intelligence	DB	-41.819325493578035	42.48147176162762	18649
a209f61d4ed32b1641f6a969addefcf4851fe6cc	vulnerability-based backdoors: threats from two-step trojans	software trojan horses runtime control systems software metrics educational institutions;backdoor;vulnerability;malicious code vulnerability based backdoor two step trojan installation antivirus software system vulnerability vbb threat;social engineering vulnerability backdoor;social engineering;invasive software	Attackers like to install trojans in a target system to control it. However, it becomes more and more difficult to deceive a user into installing such trojans. One reason is that antivirus software uses more strict policies on the first run of unknown software. The other reason is that users also become more cautious. Some attackers try to find system vulnerabilities to evade the antivirus software and users. But it is not easy to find suitable vulnerabilities because they are usually patched in a short time. In this paper, we present a new type of threat called vulnerability-based backdoor (VBB). It is a two-step trojan. In the first step, attackers deceive users into installing an application. This application is transformed from the original one such as “Adobe PDF Reader” by only creating one or more vulnerabilities in it. It runs as a normal one without any malicious code. So it can escape the detection of antivirus software and users. In the second step, attackers can make use of the vulnerability and control the target system just as they use a pre-existing vulnerability. We present a method to automatically create a VBB in several minutes. In this process, no source code is needed. VBB is stable enough to reside in a system for a long time since it does not conflict with operating systems, antivirus software, other backdoors or even other VBBs. We also show how to prevent VBBs.	antivirus software;backdoor (computing);malware;operating system;patch (computing);portable document format;trojan horse (computing);vulnerability (computing)	Kai Chen;Yingjun Zhang;Yifeng Lian	2013	2013 IEEE 7th International Conference on Software Security and Reliability	10.1109/SERE.2013.19	vulnerability management;engineering;internet privacy;world wide web;computer security	SE	-56.90658942108767	59.17337992029353	18665
417148c5d201e3b486e6ea893a0c5e32e0bc8dc7	a security vulnerability of java card on array access in financial system	array access java card security vulnerability;virtual machines application program interfaces java microprocessor chips operating systems computers security of data smart cards;virtual machines;smart cards;application program interfaces;multi application system array access financial system cos chip operating system jcvm java card virtual machine api application programming interface;security of data;operating systems computers;microprocessor chips;java	Generally, Java Card mainly consists of the following parts: COS (Chip Operating System), JCVM (Java Card Virtual Machine), and API (Application Programming Interface). As a multi-application system, Java Card itself is very complicated, so it may inevitably exist some security vulnerabilities inside. Based on these parts of Java Card, we can find out some detectable points to its security vulnerabilities. This paper presents a method containing a specific case to test Java Card on array access, aiming to detect the possible security vulnerabilities of JCVM. In this paper, three different kinds of Java Cards have been tested and the test result has been described. From the test result, we successfully find out a security vulnerability of JCVM.	application programming interface;cos;java card;operating system;smart card;virtual machine;vulnerability (computing)	Jiang-pei Xu;Li-ji Wu;Xiang-jun Yang;Yu-zhong Wang;Xiang-min Zhang	2013	2013 22nd Wireless and Optical Communication Conference	10.1109/WOCC.2013.6676466	embedded system;smart card;java card;jsr 94;java concurrency;computer hardware;java card openplatform;computer science;virtual machine;operating system;strictfp;embedded java;real time java;card reader;open smart card development platform;java;basiccard;java applet	SE	-54.41910431343973	55.89802509413364	18716
532ca632f461828b996f16c562092f15ecacdb5d	handling fault detection latencies in automata-based scheduling for embedded control software	game theory;scheduling automata theory control engineering computing fault diagnosis game theory;game theoretic approach fault detection latencies automata based scheduling embedded control software;games schedules control theory vegetation software stability fault detection;scheduling;reliable scheduling embedded control systems automata based scheduling fault tolerant system;automata theory;control engineering computing;fault diagnosis	There has been recent interest in automata-based scheduling for dynamic adaptation of schedules for embedded control software. Recently we have shown how automata-based scheduling can be extended to account for the possibility of faults in application of control. In this paper, we address the problem of automata-based scheduling when latencies are associated with detection of faults. We show that the game-theoretic approach that is used for handling faults under full visibility may be suitably augmented so as to decide the scheduling strategy in the presence of latencies in fault detection.	automata theory;automaton;embedded system;fault detection and isolation;game theory;indo;microsoft research;scheduling (computing)	M. Santhosh Prabhu;Aritra Hazra;Pallab Dasgupta;P. P. Chakrabarti	2013	2013 IEEE Conference on Computer Aided Control System Design (CACSD)	10.1109/CACSD.2013.6663473	fair-share scheduling;fixed-priority pre-emptive scheduling;real-time computing;dynamic priority scheduling;computer science;rate-monotonic scheduling;theoretical computer science;two-level scheduling;distributed computing;round-robin scheduling	Embedded	-33.81438993621549	37.436522746546615	18735
17f85de68da5acf844192a39951625fe6db152f8	characterizing user behavior on a mobile sms-based chat service	service usage patterns user behavior characterization mobile sms based chat service mobile instant messaging services mobile im services cellphone carrier resources sms platforms short message service platforms service performance improvement user experience improvement;mobile;instant messaging;mobile computing behavioural sciences computing electronic messaging;mobile user behavior characterization instant messaging;characterization;mobile communication clustering algorithms navigation instant messaging context companies mobile computing;user behavior	The use of mobile instant messaging (IM) services has grown significantly last years. Usually, mobile chat services work over the Internet using cellphone carriers' resources, such as the SMS (Short Message Service) platforms. Understanding the user behavior in this environment is paramount to improve service performance and user experience. In this article, we present and discuss a characterization of the user behavior on a mobile SMS-based chat service. We describe the usage patterns of this service providing a daily perspective of user behavior. We show that a very small group of heavy users consumes a significant amount of carrier's resources. Moreover, we also present the transitions and navigation patterns of this very small group of users to understand their peculiar behavior.	instant messaging;internet;mobile phone;user experience	Rafael de Almeida Oliveira;Wladmir Cardoso Brandão;Humberto Torres Marques-Neto	2015	2015 XXXIII Brazilian Symposium on Computer Networks and Distributed Systems	10.1109/SBRC.2015.25	mobile search;mobile web;gsm services;mobile database;computer science;mobile technology;sms banking;multimedia;internet privacy;mobile station;mobile computing;world wide web;short message service;mobile payment	Mobile	-37.61297542777699	52.988652816565015	18876
640c8314ace938e81cd7a8ccecfa4011b5eecb35	pervaho: a specialized middleware for mobile context-aware applications	context aware application;context aware;mobile applications;publish subscribe;performance analysis;communication protocol;middleware;mesh network;mobile application;location based publish subscribe	The concept of context-awareness offers a great potential for the future of mobile applications. In order to be developed in an optimal way, mobile contextaware applications need appropriate middleware services. This paper introduces Pervaho, an integrated middleware aimed specifically at supporting the development and testing of mobile context-aware applications. To illustrate the use of Pervaho, we walk through the development of a concrete mobile application and show how it can be built on top of Pervaho’s location-based publish/subscribe service. We also illustrate how a specialized mobility testing tool significantly simplifies the process of testing proximity-based semantics. We then present the implementation of Pervaho, which is based on a set of communication protocols geared at mesh networks. Finally, we provide a performance analysis of our implementation.	context awareness;flocking (behavior);high- and low-level;hoc (programming language);interaction;java platform, micro edition;mesh networking;middleware;mobile app;mobile device;profiling (computer programming);publish–subscribe pattern;simulation;software deployment;test automation;usability;web search engine	Patrick Th. Eugster;Benoît Garbinato;Adrian Holzer	2009	Electronic Commerce Research	10.1007/s10660-009-9042-4	communications protocol;middleware;real-time computing;mobile search;mobile web;computer science;mesh networking;middleware;distributed computing;publish–subscribe pattern;mobile computing;world wide web	Mobile	-37.43036266935665	50.4737375545587	18901
f97a1f24b409804ded00d2e662ba4ab36fa063af	architectural tactics for cyber-foraging: results of a systematic literature review	mobile cloud computing;cyber foraging;software architecture	Mobile devices have become for many the preferred way of interacting with the Internet, social media and the enterprise. However, mobile devices still do not have the computing power and battery life that will allow them to perform effectively over long periods of time, or for executing applications that require extensive communication, computation, or low latency. Cyber-foraging is a technique to enable mobile devices to extend their computing power and storage by offloading computation or data to more powerful servers located in the cloud or in single-hop proximity. This article presents the results of a systematic literature review (SLR) on architectures that support cyber-foraging. Elements of the identified architectures were codified in the form of Architectural Tactics for Cyber-Foraging. These tactics will help architects extend their design reasoning toward cyber-foraging as a way to support the mobile applications of the present and the future. © 2015 Elsevier Inc. All rights reserved. c 2 a a T s c 8 t s t	cloud computing;computation;cyber foraging;interaction;internet;mobile app;mobile device;social media;systematic review	Grace A. Lewis;Patricia Lago	2015	Journal of Systems and Software	10.1016/j.jss.2015.06.005	embedded system;software architecture;real-time computing;mobile search;simulation;computer science;engineering;operating system;software engineering;mobile computing	Arch	-37.360694217972124	55.27994732086912	18983
f69208d2fc4584fc7b30b42aadb86345108de2cf	behavior driven development for circuit design and verification	network synthesis;software prototyping;formal verification;formal verification behavior driven development circuit design circuit verification design flow bdd agile technique software development natural language acceptance tests;data structures boolean functions hardware design languages timing calculators semantics circuit synthesis;circuit cad;computational linguistics;software prototyping circuit cad computational linguistics formal verification network synthesis	The design of hardware systems is a challenging and erroneous task where about 70% of the effort in designing these systems is spent on verification. In general, testing and verification are usually tasks that are being applied as a post-process to the implementation. In this paper, we propose a new design flow based on Behavior Driven Development (BDD), an agile technique for the development of software in which acceptance tests written in natural language play a central role and are the starting point in the design flow. We advance the flow such that the specifics that arise when modeling hardware are taken into account. Furthermore, we present a technique that allows for the automatic generalization of test cases to properties that are suitable for formal verification. This allows the designer to apply formal verification techniques based on test cases without specifying properties. We implemented our approach and evaluated the flow for an illustrative example that successfully demonstrates the advantages of the proposed flow.	acceptance testing;agile software development;behavior-driven development;circuit design;correctness (computer science);cucumber;design flow (eda);failure;formal verification;hardware description language;natural language;natural language processing;observable;software bug;test card;test case;test-driven development;verilog	Melanie Diepenbeck;Mathias Soeken;Daniel Große;Rolf Drechsler	2012	2012 IEEE International High Level Design Validation and Test Workshop (HLDVT)	10.1109/HLDVT.2012.6418237	network synthesis filters;embedded system;verification;formal methods;formal verification;software verification;physical verification;computer science;theoretical computer science;computational linguistics;formal equivalence checking;high-level verification;runtime verification;electronic system-level design and verification;programming language;intelligent verification;algorithm;functional verification	EDA	-36.31717913602044	33.71024034563254	19002
a41f880e4d4ce3ad6b7a9e156803bf09e4672ee3	tools for a robust, sustainable agent community	self-sustaining agent community;information world;intelligent agent;agent community;exchange help;self-interested agent;adaptive group;group resource;sustainable agent community;intelligent information agent;successful agent community	We believe that intelligent information agents will represent their users interest in electronic marketplaces and other forums to trade, exchange, share, identify, and locate goods and services. Such information worlds will present unforeseen opportunities as well as challenges that can be best addressed by robust, self-sustaining agent communities. An agent community is a stable, adaptive group of self-interested agents that share common resources and must coordinate their efforts to effectively develop, utilize and nurture group resources and organization. More specifically, agents will need mechanisms to benefit from complementary expertise in the group, pool together resources to meet new demands and exploit transient opportunities, negotiate fair settlements, develop norms to facilitate coordination, exchange help and transfer knowledge between peers, secure the community against intruders, and learn to collaborate effectively. In this talk, I will summarize some of our research results on trust-based computing, negotiation, and learning that will enable intelligent agents to develop and sustain robust, adaptive, and successful agent communities. D. Kinny et al. (Eds.): PRIMA 2011, LNAI 7047, p. 2, 2011. c © Springer-Verlag Berlin Heidelberg 2011	intelligent agent;lecture notes in computer science;robustness (computer science);springer (tank)	Sandip Sen	2011		10.1007/978-3-642-25044-6_2	simulation;knowledge management	AI	-42.828573797768705	56.376256062765115	19019
259cd8c9afbc3052d7f22155504b609e6e243759	configuration lifting: verification meets software configuration	software kernel linux runtime driver circuits analytical models software systems;configurable linux kernel;meta program;feature modeling;product line;program verification;software configuration;configurable linux kernel configuration lifting software configuration software product line spl meta program verification techniques static analysis model checking deduction based approaches;software reusability configuration management linux operating system kernels product development program verification;model checking;side effect;software reusability;configuration lifting;deduction based approaches;linux;operating system kernels;static analysis;software product line;meta programming;configuration management;verification techniques;spl;product development	Configurable software is ubiquitous, and the term software product line (SPL) has been coined for it lately. It remains a challenge, however, how such software can be verified over all variants. Enumerating all variants and analyzing them individually is inefficient, as knowledge cannot be shared between analysis runs. Instead of enumeration we present a new technique called lifting that converts all variants into a meta-program, and thus facilitates the configuration-aware application of verification techniques like static analysis, model checking and deduction-based approaches. As a side-effect, lifting provides a technique for checking software feature models, which describe software variants, for consistency. We demonstrate the feasibility of our approach by checking configuration dependent hazards for the highly configurable Linux kernel which possesses several thousand of configurable features. Using our techniques, two novel bugs in the kernel configuration system were found.	knowledge-based configuration;lambda lifting;lifting scheme;linux;model checking;natural deduction;side effect (computer science);software bug;software feature;software product line;static program analysis	Hendrik Post;Carsten Sinz	2008	2008 23rd IEEE/ACM International Conference on Automated Software Engineering	10.1109/ASE.2008.45	metaprogramming;model checking;real-time computing;software configuration management;software verification;computer science;operating system;software construction;configuration management;programming language;static analysis;side effect;linux kernel;new product development	SE	-50.45094772947375	34.31993568488963	19020
524bb0d9222a21cdb25b63d3edbe292c07dc5443	an execution-semantic and content-and-context-based code-clone detection and analysis	source code software program diagnostics programming languages public domain software software metrics;computer languages;prototypes;cloning;visualization;heuristic algorithms;open source products execution semantic based code clone detection content and context based code clone detection arbitrary granularity model programming languages control sentences lambda lazy evaluation clone metrics code search execution semantic;context;object oriented modeling;cloning prototypes context object oriented modeling computer languages heuristic algorithms visualization	This paper presents a code-clone detection and its analysis method, based on an execution-semantic and arbitrary-granularity model[8] of code fragments. The principal goal of introducing the proposed detection method is to provide a code-clone detection method suitable for programming languages, where software developers can define their own “control sentences” with such as lambda or lazy evaluation. Code clones detected with the proposed method are a kind of type-3 clone, where code fragments exist across boundaries of procedures or modules. The model also seems useful as clone metrics (for a clone triage) based on the contents and contexts of code fragments in a clone class and extensible to a unified method of code-clone detection and code search. This paper introduces an execution-semantic and content-and-context based code clone, describes its definition, a detection method, an analysis method, and a prototype implementation of a tool chain, which was applied to two open-source products as an preliminary empirical evaluation.	algorithm;arbitrary code execution;duplicate code;functional programming;haskell;lazy evaluation;open-source software;programming language;prototype;software developer;toolchain;type system	Toshihiro Kamiya	2015	2015 IEEE 9th International Workshop on Software Clones (IWSC)	10.1109/IWSC.2015.7069882	kpi-driven code analysis;computer science;theoretical computer science;computer programming;database;programming language;static program analysis;source code	SE	-56.47940902467209	36.44708825697933	19087
670abab48a100e07c57bd88806af190ad710e2af	cross-platform feature matching for web applications	mobile web;cross platform	With the emergence of new computing platforms, software written for traditional platforms is being re-targeted to reach the users on these new platforms. In particular, due to the proliferation of mobile computing devices, it is common practice for companies to build mobile-specific versions of their existing web applications to provide mobile users with a better experience. Because the differences between desktop and mobile versions of a web application are not only cosmetic, but can also include substantial rewrites of key components, it is not uncommon for these different versions to provide different sets of features. Whereas some of these differences are intentional, such as the addition of location-based features on mobile devices, others are not and can negatively affect the user experience, as confirmed by numerous user reports and complaints. Unfortunately, checking and maintaining the consistency of different versions of an application by hand is not only time consuming, but also error prone. To address this problem, and help developers in this difficult task, we propose an automated technique for matching features across different versions of a multi-platform web application. We implemented our technique in a tool, called FMAP, and used it to perform a preliminary empirical evaluation on nine real-world multi-platform web applications. The results of our evaluation are promising, as FMAP was able to correctly identify missing features between desktop and mobile versions of the web applications considered, as confirmed by our analysis of user reports and software fixes for these applications.	cognitive dimensions of notations;desktop computer;emergence;mobile computing;mobile device;user experience;web application	Shauvik Roy Choudhary;Mukul R. Prasad;Alessandro Orso	2014		10.1145/2610384.2610409	web service;web modeling;mobile search;mobile web;computer science;cross-platform;data mining;database;web engineering;world wide web	SE	-54.8202702768479	42.228879821139905	19233
d2188974cffc98b29f5ee03ab9bccbb44c278d51	enterprise level security	authentication;computer architecture;monitoring;ecosystems;authorization	Mission success and effectiveness depends on timely, secured delivery of information to authorized personnel or systems. Unfortunately, the current security paradigm of building a fortress to protect systems in the network is not sufficient in cyberspace. Further access control measures, such as role-based permissions, to prevent intrusions/disruptions are also insufficient. We present an alternative mission-based approach to a more granular access control paradigm, based on 14 years of research and pilot efforts. This distributed security approach has no need for passwords or system accounts, thus eliminating the associated management overhead. At each step in the authorization process, the system determines validated identities and claims for appropriate access and privileges. The techniques employed are resilient, secure, extensible, and scalable. The system, called Enterprise Level Security (ELS), is currently being researched. This paper discusses the ELS, a web-based security architecture designed to select and incorporate technology into a cohesive set of policies and rules for an enterprise information system. ELS provides application and data level access control automatically, based on the warfighter's current mission profile. As the warfighter's profile changes, authorized accesses are automatically deleted and new ones established to provide relevant, least privileged, mission information at the time of need. The paper begins by introducing ELS, its design principles and architecture, along with its foundational role in developing a forward-looking enterprise baseline; then, it continues by presenting ELS' current status and performance metrics, along with future plans for expansion of capabilities.	authorization;baseline (configuration management);computer security;cyberspace;enterprise information system;extreme loading for structures;fortress;overhead (computing);password;programming paradigm;role-based access control;scalability;web application	Eric D. Trias;William R. Simpson;Kevin E. Foltz;Frank P. Konieczny	2016	MILCOM 2016 - 2016 IEEE Military Communications Conference	10.1109/MILCOM.2016.7795297	simulation;engineering;world wide web;computer security;enterprise information security architecture	Security	-49.60208299208803	56.88155561461863	19293
1c3606623adcf3b76f80c0ce51bf883ec53c6d2b	goal lifecycles and ontological models for intention based assistive living within smart environments	goal recognition;goal modelling;assistive living;activities of daily living;article;ontology;smart environments	Current ambient assistive living solutions have adopted a traditional sensor-centric approach, involving data analysis and activity recognition to provide assistance to individuals. The reliance on sensors and activity recognition in this approach introduces issues with scalability and ability to model activity variations. This study introduces a novel approach to assistive living which intends to address these issues via a paradigm shift from a sensor centric approach to a goal-oriented one. The goaloriented approach focuses on identification of user goals in order to pro-actively offer assistance by either pre-defined or dynamically constructed instructions. This paper introduces the architecture of this goal-oriented approach and describes an ontological goal model to serve as its basis. The use of this approach is illustrated in a case study which focuses on assisting a user with activities of daily living.	activity recognition;assistive technology;programming paradigm;scalability;sensor;smart environment	Joseph Rafferty;Liming Chen;Chris D. Nugent;Jun Liu	2015	Comput. Syst. Sci. Eng.		simulation;activities of daily living;knowledge management;ontology	HCI	-42.34011171767816	45.875124747915955	19308
493c18956a08f35bf4ad5cd9df5ae6d0cd10b467	testing .net code with yeti	software;language use;programming language;automatic testing;code generation;testing;random testing;software engineering;assembly;quality assessment;software engineering automatic test software;program testing;automatic test software;net code;end user code;point of view;york extendible testing infrastructure;net code quality assessment end user code york extendible testing infrastructure programming language;computer bugs;programming languages program testing;programming languages;documentation;testing java computer bugs software assembly documentation;java	Testing code is one of the central techniques for quality assessment of code. Generating test cases manually, however, is costly and inherently biased by the human point of view. While this might not be an issue for end-user code, it is problematic for developing libraries. The York Extendible Testing Infrastructure (YETI) is an automated random testing infrastructure supporting multiple programming languages (Java, JML, and . NET). It tests code at random and decouples the engine from the strategies and the language used. This article presents the . NET binding.	dna binding site;extensibility;java modeling language;library (computing);programming language;random testing;randomness;test case	Manuel Oriol;Sotirios Tassis	2010	2010 15th IEEE International Conference on Engineering of Complex Computer Systems	10.1109/ICECCS.2010.58	random testing;dead code;white-box testing;documentation;computer science;operating system;software engineering;redundant code;database;assembly;programming language;java;code generation;unreachable code;source code	SE	-57.11049975729354	36.84056053665191	19355
2b49568e8a990fbf0d2c0e619e9881a85d0c9bae	poliseer: a tool for managing complex security policies	special issue on trust management	Few tools exist for decomposing complex security policies into simpler modules. The policy-engineering tools that do exist either encapsulate entire policies as atomic, indecomposable modules or allow fine-grained modularization but are complicated and lack policyvisualization capabilities. This paper briefly presents PoliSeer, the first tool we are aware of that allows complex policies to be specified, visualized, modified, and enforced as compositions of simpler policy modules.	high- and low-level;integrated development environment;low-level programming language;software engineer	Daniel Lomsak;Jay Ligatti	2011	JIP	10.2197/ipsjjip.19.292	embedded system;computer science;operating system;data mining;computer security;algorithm	HCI	-42.36481901575905	33.28412967471671	19379
ff1e5db359bd07b4638779846a19709e98717f8d	middleware services for interoperability in open mobile agent systems	agent platform;programming paradigm;service management;mobile agent system;design and implementation;object oriented;middleware;mobile agent;networked systems;legacy system	Despite the design and implementation of several mobile agent (MA) platforms, widely diffused services based on the MA programming paradigm are still lacking. Apart from the security challenges imposed by the MA technology, the paper claims that interoperability between MAs, legacy systems and heterogeneous MA platforms is a major obstacle to the MA diffusion. The paper discusses solutions to permit the interworking between MA platforms and other systems, even non-MA-based, via compliance with either accepted or emerging interoper-ability standards. In particular, it focuses on compliance with CORBA, the accepted standard for object-oriented components, but also with MASIF and FIPA, respectively, the OMG speci®cation for the support of agent mobility and management, and the framework for standard agent platforms and communication languages. The discussed solutions have guided the design and implementation of the middleware interoperability service in the secure and open mobile agents (SOMA) programming framework. The SOMA interoperability service is structured in a layered and modular way: its components can be dynamically distributed and installed only where and when needed. The paper also presents an application scenario in the area of network, systems and service management, where the interoperability components permit the interworking of SOMA agents, SNMP-compliant elements, legacy resources, and non-SOMA MA platforms. q 2001 Elsevier Science B.V. All rights reserved.	application domain;common object request broker architecture;divergence (computer science);e-commerce;entity;global network;information retrieval;interoperability;legacy system;middleware;mobile agent;mobile computing;naruto shippuden: clash of ninja revolution 3;overhead (computing);programming language;programming paradigm;prototype;simple network management protocol;software deployment;systems management;xml	Paolo Bellavista;Antonio Corradi;Cesare Stefanelli	2001	Microprocessors and Microsystems	10.1016/S0141-9331(01)00100-4	embedded system;real-time computing;service management;computer science;operating system;middleware;mobile agent;distributed computing;programming paradigm;programming language;object-oriented programming;legacy system;computer network	SE	-35.94303984096694	43.592456365458055	19404
f16d11606cfd54890b5fb9c05ab6262f86f571c3	a mobile agent approach to dynamic architecture-based software adaptation	distributed system;high availability;dynamic software adaptation;architectural reflection;mobile agents;adaptive dynamics;dynamic software architecture;software architecture;architecture meta model;feedback loop;mobile code;on the fly;mobile agent;system architecture;meta model;architectural style	Increasingly, software are required to be ready to adapt itself to the changing environment caused by a wide range of maintenance, evolution and operation problems. Furthermore, in large complex distributed systems and continuous running systems, the traditional approaches to bringing about change require that the system be taken offline temporarily, which is often undesirable due to requirements for high availability. To address this new kind of capability, dynamic software adaptation, which refers to software changes in both structure and behavior without bring it down, is proposed.In this paper, we explore an architecture-based mobile agent approach to dynamic software adaptation. Our goal is to automate the software adaptation on the fly on the basis of explicating and reasoning about architectural knowledge about the running system. For that, we introduce the dynamic software architecture, which means the architecture itself can also be introspected and altered at runtime, to guide and control the adaptation. We use the architectural reflection to observe and control the system architecture, while use the architectural style to ensure the consistency and correctness of the architecture reconfiguration. To handle the adaptation of the running system, mobile agents, which is well suited for complex management issues, is employed. Mobile agents carry self-contained mobile code and act upon running components.The usage of meta-architecture and the mobile agents not only forms an adaptation feedback loop onto the running system, it also separates the concerns among the architectural model, the target system and the facilities use for adaptation. It will simplify the developing, deploying and maintaining of the system, while pose a good basis for enabling the reuse of the adaptation facilities.	code mobility;correctness (computer science);distributed computing;feedback;high availability;mobile agent;on the fly;online and offline;optimization problem;requirement;run time (program lifecycle phase);software architecture;systems architecture	Qun Yang;Xian-Chun Yang;Manwu Xu	2006	ACM SIGSOFT Software Engineering Notes	10.1145/1127878.1127889	metamodeling;reference architecture;embedded system;software architecture;real-time computing;simulation;architectural pattern;computer science;engineering;software engineering;feedback loop;mobile agent;high availability;resource-oriented architecture;systems architecture	SE	-41.237979319233816	39.279296480274006	19419
d41399a3382ca42d6c4e7ee25486149793089c9d	policy driven remote attestation		Increasingly organisations need to exchange and share data amongst their employees as well as with other organisations. This data is often sensitive and/or confidential, and access to it needs to be protected. Architectures to protect disseminated data have been proposed earlier, but absence of a trusted enforcement point on the end-user machine undermines the system security. The reason being, that an adversary can modify critical software components. In this paper, we present a policy-driven approach that allows us to prove the integrity of a system and which decouples authorisation logic from remote attestation.	adversary (cryptography);authorization;component-based software engineering;computer security;confidentiality;coupling (computer programming);hardware restriction;information privacy;trusted computing;trusted platform module	Anandha Gopalan;Vaibhav Gowadia;Enrico Scalavino;Emil C. Lupu	2011		10.1007/978-3-642-30244-2_13	computer security	Security	-52.10822951333397	56.33111221496626	19426
9195f00cc50acc73961856d1e477e7cf0fbc1141	a privacy enabled fast dynamic authentication and authorization for b3g/4g mobility	vid;policy;authentication;authorization;security;privacy	Mobile technologies make their headway by offering more flexibility to end-users and improve the productivities. Within the application of ubiquitous access and pervasive communication, security (or privacy) and QoS (Quality of Service) are two critical factors during global mobility, so how to get a smooth and fast handover based on a user privacy protected infrastructure is our focus. Based on a user-centric virtual identity defined by EU IST project Daidalos, this paper firstly proposes an effective infrastructure which protects the context-driven access policies for online services in order to avoid attacks by malicious eavesdroppers. In the proposed infrastructure, SMAL and Diameter are used to securely protect and deliver authenticated and authorized entities and XACML is used to authorize the user-level privacy policy. On the basis of it, a dynamic fast authentication and authorization handover mechanism is proposed which can save one trip communication time consummation between administrative domains.	authentication;authorization;e-services;entity;pervasive informatics;privacy policy;quality of service;simulation;software testing controversies;user space;xacml	Zhikui Chen;Song Yang	2009	Communications and Network	10.4236/cn.2009.12012	information privacy;privacy by design;computer science;information security;authentication;authorization;internet privacy;privacy;computer security;computer network	Security	-44.45007842778222	58.719644758705165	19445
b5bc4cf991ff29be25aaffbda4d7ac2b804836f5	two-tier data-driven intrusion detection for automatic generation control in smart grid	smart power grids power engineering computing power system control security of data;automatic generation control markov processes prediction algorithms smart grids mathematical model correlation intrusion detection;markov models automatic generation control smart grid legacy energy infrastructures data driven two tier intrusion detection system agc short term adaptive predictor area control error ace	Legacy energy infrastructures are being replaced by modern smart grids. Smart grids provide bi-directional communications for the purpose of efficient energy and load management. In addition, energy generation is adjusted based on the load feedback. However, due to the dependency on the cyber infrastructure for load monitoring and reporting, generation control is inherently vulnerable to attacks. Recent studies have shown that the possibility of data integrity attacks on the generation control can significantly disrupt the energy system. In this work, we present simple yet effective data-driven two-tier intrusion detection system for automatic generation control (AGC). The first tier is a short-term adaptive predictor for system variables, such as load and area control error (ACE). The first tier provides a real-time measurement predictor that adapts to the underlying changing behavior of these system variables, and flags out the abnormal behavior in these variables independently. The second tier provides deep state inspection to investigate the presence of anomalies by incorporating the overall system variable correlation using Markov models. Moreover, we expand our second tier inspection to include multi-AGC environment where a behavior of one AGC is validated against the behavior of the interconnected AGC. The combination of tier-1 light-weight prediction and tier-2 offline deep state inspection offers a great advantage to balance accuracy and real-time requirements of intrusion detection for AGC environment. Our results show high detection accuracy (95%) under different multi-attack scenarios. Second tier successfully verified all the injected intrusions.	24-hour clock;ace;algorithm;anomaly detection;automatic gain control;data integrity;energy systems language;environment variable;input/output;intrusion detection system;kerrison predictor;load management;malware;markov chain;markov model;model checking;multitier architecture;online and offline;real-time clock;requirement;run time (program lifecycle phase);software bug;statistical model	Muhammad Qasim Ali;Reza Yousefian;Ehab Al-Shaer;Sukumar Kamalasadan;Quanyan Zhu	2014	2014 IEEE Conference on Communications and Network Security	10.1109/CNS.2014.6997497	embedded system;real-time computing;computer science;computer security	Security	-57.76982068998147	51.80790950613671	19449
ac9bcaeb1ef7e3e5e99e41ba19931358ba251e8d	minimal role mining method for web service composition	role base access control rbac;composite web service;agent based;chao huang jian ling sun xin yu wang yuan jie si minimal role mining method for web service composition;role based access control;web service;web service composition;access control policy;web service security;role mapping;greedy algorithm;web services security;role mining	Web service composition is a low cost and efficient way to leverage the existing resource and implementation. In current Web service composition implementations, the issue of how to define the role for a new composite Web service has been little addressed. Adjusting the access control policy for a new composite Web service always causes substantial administration overhead from the security administrator. Furthermore, the distributed nature of Web service based applications makes traditional role mining methods obsolete. In this paper, we analyze the minimal role mining problem for Web service composition, and prove that this problem is NP-complete. We propose a sub-optimal greedy algorithm based on the analysis of necessary role mapping for interoperation across multiple domains. Simulation shows the effectiveness of our algorithm, and compared to the existing methods, our algorithm has significant performance advantages. We also demonstrate the practical application of our method in a real agent based Web service system. The results show that our method could find the minimal role mapping efficiently.	access control;agent-based model;bipartite dimension;context (computing);greedy algorithm;interoperation;np-completeness;overhead (computing);service composability principle;simulation;vertex cover;web service	Chao Huang;Jianling Sun;Xinyu Wang;Yuanjie Si	2009	Journal of Zhejiang University SCIENCE C	10.1631/jzus.C0910186	web service;web application security;greedy algorithm;web modeling;data web;web analytics;computer science;ws-policy;role-based access control;data mining;database;web intelligence;world wide web;mashup	Web+IR	-49.99529165080229	49.323991144759034	19458
cccfefb9becae33c5a1a6645de713b8a894b5317	anomaly detection using negative security model in web application	web application firewall;log files;security model;computer network security;information retrieval;anomaly detection;real time;authoring languages;computer crime;rule set;logging;logging web intrusion web application firewall sql injection cross site scripting attacker rule set;hypermedia;attacker;internet;sql injection;web server databases intrusion detection computer hacking fires monitoring;traffic monitoring;real time analysis unethical hackers ethical hackers web sites anomaly detection hacking scripting languages data retrieval www threats negative security model web application firewall web server log profile http traffic monitoring;ethical aspects;web intrusion;scripting language;internet authoring languages computer crime computer network security ethical aspects hypermedia information retrieval;cross site scripting	Today's combat zone for both ethical and unethical hackers is the web. Rapid growth of web sites and web applications gives way to deliver complex business applications through the web. As the web dependency increases, so do the web hacking activities. Web applications are normally written in scripting languages like JavaScript, PHP embedded in HTML allowing connectivity to the databases, retrieving data and putting them in the WWW site. A web application is vulnerable to many kinds of threats and attacks. In order to detect known attacks, some set of attack rules and detections are needed. In this paper, a negative security model based on misuse of web applications is used. This negative security model provides a Web Application Firewall(WAF) engine with a rule set, to ensure critical protection across every web architecture. WAFs are deployed to establish an increased external security layer to detect and/or prevent attacks before they reach web applications. This paper has been tested with apache web server's log file. We have tested successfully almost all the common attacks. This paper also allows for HTTP traffic monitoring and real-time analysis with little or no changes to existing infrastructure.	algorithm;anomaly detection;computer security model;database;embedded system;html;hypertext transfer protocol;internet addiction disorder;javascript;php;real-time clock;scripting language;sensor;server (computing);threat (computer);web application;web server	Michael Auxilia;D. Tamilselvan	2010	2010 International Conference on Computer Information Systems and Industrial Management Applications (CISIM)	10.1109/CISIM.2010.5643461	application firewall;computer security model;web service;cross-site scripting;ajax;web application security;anomaly detection;web development;web application;the internet;web analytics;web-based simulation;sql injection;web design;computer science;web api;operating system;web navigation;data mining;scripting language;internet privacy;web 2.0;world wide web;computer security;web server;computer network;logging	Web+IR	-57.78206181057944	59.33647510738056	19516
131f6f8b06cbf5b7ed9f6769564f4a694c6ecd00	a model-based statistical usage testing of communication protocols	statistical usage testing;protocols;unit testing framework junit;java programming;unit testing;protocols software testing design engineering unified modeling language automatic testing interconnected systems formal verification system testing automatic programming control engineering computing;statistical testing java protocols;sip protocol stack;generic test case generator;test case compiler;test bed;work environment;class testbed;test cases;sip invite client transaction;state design pattern;operational profile;test case generation;test harness;operational profile model interpreter;design pattern;class fsmsystem;software reliability model based statistical usage testing communication protocol state design pattern java programming class fsmsystem generic modeling environment operational profile model paradigm operational profile model interpreter generic test case generator test case compiler unit testing framework junit class testbed sip invite client transaction sip protocol stack;model based statistical usage testing;operational profile model paradigm;communication protocol;generic modeling environment;statistical testing;software reliability;test case generator;test harness generic modeling environment statistical usage testing operational profile test cases software reliability test case generator test bed;java	In this paper, we present our original approach to the model-based statistical usage testing of a class of communication protocol implementations that are based on the state design pattern and Java programming environment augmented with the class FSMSystem. The approach is based on the working environment that has been proven on a number of real-world projects. The working environment is created with the following set of tools: generic modeling environment with the operational profile model paradigm registered to it, operational profile model interpreter, generic test case generator, test case compiler, and the unit testing framework JUnit extended with the class TestBed that acts as both test driver and stub thus providing the complete test harness. In the paper, we present the methodology of the model-based statistical usage testing of a class of communication protocol implementations, the tools that support this methodology, and the case study - the model based statistical usage testing of SIP INVITE client transaction, a part of the SIP protocol stack	communications protocol;compiler;generic modeling environment;integrated development environment;junit;java;programming paradigm;protocol stack;software design pattern;test case;test harness;unit testing	Miroslav Popovic;Ilija Basicevic;Ivan Velikic;Jelena Tatic	2006	13th Annual IEEE International Symposium and Workshop on Engineering of Computer-Based Systems (ECBS'06)	10.1109/ECBS.2006.11	embedded system;communications protocol;real-time computing;computer science;operating system;programming language	SE	-48.0704079360668	34.50131339332932	19523
3f3d63821f19b02b7a91fa226ec6c3aa0adc4156	a web portal for the certification of open source software	software analysis;haslab haslab uminho;software certification;programming languages;open source software	This paper presents a web portal for the certification of open source software. The portal aims at helping programmers in the internet age, when there are (too) many open source reusable libraries and tools available. Our portal offers programmers a web-based and easy setting to analyze and certify open source software, which is a crucial step to help programmers choosing among many available alternatives, and to get some guarantees before using one piece of software. The paper presents our first prototype of such web portal. It also describes in detail a domain specific language that allows programmers to describe with a high degree of abstraction specific open source software certifications. The design and implementation of this language is the core of the web portal.		Pedro Henrique Martins;João Paulo Fernandes;João Saraiva	2012		10.1007/978-3-642-54338-8_20	long-term support;verification and validation;computer science;software framework;component-based software engineering;software development;software analysis pattern;software engineering;software construction;software release life cycle;database;programming language;software deployment;world wide web;software peer review	SE	-54.869192395826914	33.57101661855693	19704
e4f25426180b95e71f099ecd54dd9e81c8ebd4f6	model based integration of safety analysis and development	telematics;unified modeling language formal specification formal verification safety critical software security of data software architecture systems analysis;modeling technique;formal specification;application software;uml;architecture description language;system analysis and design;architecture description languages;safety properties;modeling language;systems engineering and theory;safety analysis models;computer architecture;software architecture;formal verification;safety analysis;software safety;complex system;systems analysis;architectural description languages;safety critical software;safety architecture;software development;unified modeling language;software safety computer architecture costs software architecture unified modeling language application software programming systems engineering and theory telematics architecture description languages;software requirements and specifications;safety engineering;programming;security of data;uml software development safety critical software formal verification safety engineering software architecture architectural description languages safety analysis models safety architecture unified modeling language;data security	The development of safety critical software applications has always been done in accordance to strict methods. These systems require the application of verification techniques that guarantee safety properties. Often, they are complex systems that require the integration of different types of engineers such as safety engineers and software architects. Currently, different groups of engineers apply different analysis and modeling techniques (e.g. architectural description languages, and safety analysis models); these differences create inconsistencies between different types of model. In this paper we introduce some solutions to reduce these problems in some safety architectures. These solutions integrate developing modeling languages such as UML and specific safety analysis languages such as FTA and FMECA	complex systems;failure mode, effects, and criticality analysis;fault tree analysis;software architect;software engineer;unified modeling language	Miguel A. de Miguel;Javier Fernández Briones;Juan Pedro Silva;Alejandro Alonso	2006	Ninth IEEE International Symposium on Object and Component-Oriented Real-Time Distributed Computing (ISORC'06)	10.1109/ISORC.2006.53	safety engineering;unified modeling language;computer science;software engineering	Embedded	-46.70027586913151	32.95062582418539	19706
54ded5c583a40fac6fa7cf1d69e1c130b8b7f20e	xsed - xml-based description of status-event components and systems	qa75 electronic computers computer science;temporal properties;xml;ubiquitous computing infrastructure;status event analysis;reflective dialogue notation	Most user interfaces and ubiquitous systems are built around event-based paradigms. Previous work has argued that interfaces, especially those heavily depending on context or continuous data from sensors, should also give attention to status phenomena – that is continuously available signals and state. Focusing on both status and event phenomena has advantages in terms of adequacy of description and efficiency of execution. This paper describes a collection of XML-based specification notations (called XSED) for describing, implementing and optimising systems that take account of this dual status–event nature of the real world. These notations cover individual components, system configuration, and separated temporal annotations. Our work also presents a implementation to generate Status-Event Components that can run in a stand-alone test environment. They can also be wrapped into a Java Bean to interoperate with other software infrastructure, particularly the ECT platform.	assembly language;computer;deployment environment;discretization;electroconvulsive therapy;embedded system;high- and low-level;high-level programming language;input/output;interoperability;java platform, enterprise edition;mathematical optimization;pointer (computer programming);process calculus;refinement (computing);sensor;system configuration;test and evaluation master plan;user interface;xml namespace	Alan J. Dix;Jair C. Leite;Adrian Friday	2007		10.1007/978-3-540-92698-6_13	computer science;theoretical computer science;data mining;database	SE	-39.05659124751842	41.5937995119003	19719
c44759b46d33a80d1719162b0ddb7cf84edbf9b1	past, present and future of the contextnet iomt middleware		The Internet of Things with support to mobility is already transforming many application domains, such as smart cities and homes, environmental monitoring, health care, manufacturing, logistics, public security etc. in that it allows to collect and analyze data from the environment, people and machines, and to implement some form of control or steering on these elements of the physical world. But in order to speed the development of applications for the Internet of Mobile Things (IoMT), some middleware is required. This paper summarizes seven years of research and development on the ContextNet middle ware aimed at IoMT, discusses what we achieved and what we have learned so far. We also share our vision of possible future challenges and developments in the Internet of Mobile Things.	augusto sampaio;elixir;future internet;gauss–lucas theorem;internet of things;logistics;middleware;pc bruno;personal and ubiquitous computing;security descriptor definition language;smart city;winsock	Markus Endler;Francisco Silva E. Silva	2018	OJIOT		computer security;computer science;health care;the internet;middleware;internet of things	AI	-44.73435178086818	49.229885614045514	19833
755ca2410fbed958bec19a07320e663b1c0c4300	ethical aspects of internet of things from islamic perspective		The Internet of Things (IoTs) is an evolving new face of technology that provides state of the art services using ubiquitously connected smart objects. These smart objects are capable of sensing, processing, collaborating, communicating the events and provide services. The IoT is a collection of heterogeneous technologies like Sensor, RFID, Communication and nanotechnology. These technologies enable smart objects to identify objects, collect information about their status, communicating the collected information for taking some desired actions. Widespread adaptations of IoT based devices and services raised the ethical challenges for their users. In this paper we highlight ethical challenges raised by IoT and discuss the solutions and methods for encouraging people to properly use these technologies according to Islamic teachings.	cybercrime;dark side;internet of things;radio-frequency identification;sensor;smart objects	Wazir Zada Khan;Mohammed Zahid;Mohammed Y. Aalsalem;Hussein Mohammed Zangoti;Quratul-Ain Arshad	2017	2017 9th IEEE-GCC Conference and Exhibition (GCCCE)		knowledge management;smart objects;internet of things;islam;computer science	HCI	-43.638262236104765	48.031929621404934	19847
7206c8cacf947f0df33557ef4359aa50045421b0	push vs. pull: an energy perspective (short paper)	energy efficiency;pull;client notification;telecommunication power management;push;mobile handsets energy consumption batteries servers mobile communication payloads energy measurement;servers;energy measurement;energy consumption;batteries;mobile communication;battery management systems;mobile handsets;payloads;energy consumption client notification push pull energy efficiency;telecommunication power management battery management systems energy consumption mobile handsets;energy savings energy perspective traffic guidance ambient living mobile applications energy consumption battery constraints contemporary mobile devices pull based notification scenario push based notification scenario payload sizes notification intervals application scenario	In many application scenarios, such as traffic guidance or ambient living, services need to notify mobile applications about status changes. Such notifications to mobile devices can be realized using two principal approaches, namely push- and pull-based. Apart from functional differences, the two options likely result in different energy consumption, which is an important aspect due to the battery constraints of contemporary mobile devices. This paper provides a detailed assessment of energy consumption in pull- and push-based notification scenarios, considering different payload sizes and notification intervals. Our results indicate that an educated choice among both options may, depending on the specific application scenario, facilitate energy savings of up to 19%.	mobile app;mobile device	Daniel Burgstahler;Ulrich Lampe;Nils Richerzhagen;Ralf Steinmetz	2013	2013 IEEE 6th International Conference on Service-Oriented Computing and Applications	10.1109/SOCA.2013.17	embedded system;payload;simulation;mobile telephony;telecommunications;computer science;operating system;efficient energy use;server	SE	-36.88353893057138	55.630951723848504	19856
08224bb3dc2314deaf443832754d31924e4e1632	testing-based formal verification for theorems and its application in software specification verification		Verifying a specification for software can be converted into theorem proving. In this paper, we describe a testing-based formal verification (TBFV) method for automatically testing theorems. The advantage of the method over conventional theorem proving is that it can quickly detect faults if the theorem is not valid and quickly provide us with confidence in the validity of the theorem if it is valid. We discuss the principle and algorithms for test case generation in TBFV and present an example to illustrate how TBFV can be applied in checking designs. We also present a prototype supporting tool we have developed and a controlled experiment for evaluating the performance of TBFV. The result shows that TBFV is effective and efficient to find faults in certain setting.	formal specification;formal verification	Shaoying Liu	2016		10.1007/978-3-319-41135-4_7	verification and validation;verification;formal methods;formal verification;software verification;theoretical computer science;software construction;formal specification;high-level verification;runtime verification;programming language;intelligent verification;functional verification	SE	-48.765792606268576	33.821779205085406	19859
e096709b48f6882dbf0be16d586bcb747fc5a14f	an incentive compatible reputation mechanism for ubiquitous computing environments	incentive compatibility;wireless network;trust management;information sharing;reputation mechanism;distributed environment;mobile ad hoc networks;probability theory;mobile ad hoc network;ubiquitous computing;service provision;ubiquitous computing environment	The vision of ubiquitous computing is becoming a reality thanks to the advent of portable devices and the advances in wireless networking technologies. It aims to facilitate user tasks through seamless utilization of services available in the surrounding environments. In such distributed environments featuring openness, interactions, especially service provision and consumption, between entities that are unknown or barely known to each other, are commonplace. Trust management through reputation mechanism to facilitate such interactions is recognized as an important element of ubiquitous computing. It is, however, faced by the problems of how to stimulate reputation information sharing and honest recommendation elicitation. We present in this paper an incentive compatible reputation mechanism to facilitate the trustworthiness evaluation in ubiquitous computing environments. It is based on probability theory and supports reputation evolution and propagation. Our reputation mechanism not only shows robustness against lies, but also stimulates honest and active recommendations. The latter is realized by ensuring that active and honest recommenders, compared to inactive or dishonest ones, can elicit the most honest (helpful) recommendations and thus suffer the least number of wrong trust decisions, as validated by simulation based evaluation.	entity;interaction;openness;personal digital assistant;robustness (computer science);seamless3d;simulation;software propagation;trust (emotion);trust management (managerial science);ubiquitous computing	Jinshan Liu;Valérie Issarny	2006		10.1145/1501434.1501478	mobile ad hoc network;computer science;internet privacy;world wide web;computer security;ubiquitous computing	HCI	-42.83348708600961	56.791292351523595	19875
f5889799ec48c66c401371f9070f1f17d9988409	a smart gateway framework for iot services	protocols;semantics;logic gates;cognition;ontologies;context;cloud computing	With various connectivity and communication protocols, it is conceivable that all raw data from sensor devices can be made available in the Internet. However, this imposes a significant intricacy when dealing with the amount of information and the management of the devices. To alleviate the problem, this paper presents a design and implementation of a prototype IoT gateway framework that exposes high-level abstraction for accessing and manipulating the devices of an ambient environment and the data that they produce. Along with integrating and consolidating edge devices, the gateway framework enables the construction of high-level knowledge to represent the property of the environment and to support privacy protection. Thus, intelligent services, created with reasoning rules or event triggers, can be adaptable to application environment. To illustrate the capability of the IoT gateway framework, an application scenario and its performance evaluation are included in the paper.	answer set programming;boolean satisfiability problem;complex event processing;edge device;high- and low-level;internet;performance evaluation;prototype;python;responsiveness;semantic reasoner;solver	Yann-Hang Lee;Shankar Nair	2016	2016 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)	10.1109/iThings-GreenCom-CPSCom-SmartData.2016.44	embedded system;communications protocol;cognition;logic gate;cloud computing;computer science;ontology;operating system;data mining;semantics;world wide web;computer security;computer network	Robotics	-42.9554781060411	46.36756847412759	19901
a58b102244fb7145fdfd6054bc87291511623563	detecting colluding attackers in distributed grid systems		Distributed grid systems offer possible benefits in terms of fast computation of tasks. This is accompanied by potential drawbacks due to their openness, the heterogeneity of participants, and the unpredictability of agent behaviour, since agents have to be considered as black-boxes. The utilisation of technical trust within adaptive collaboration strategies has been shown to counter negative effects caused by these characteristics. A major challenge in this context is the presence of colluding attackers that try to exploit or damage the system in a coordinated fashion. Therefore, this paper presents a novel approach to detect and isolate such colluding attackers. The concept is based on observations of interaction patterns and derives a classification of agent communities. Within the evaluation, we demonstrate the benefit of the approach and highlight the highly reliable classification.	affinity propagation;algorithm;black box;cluster analysis;computation;computer cluster;desktop computer;frame rate control;graph (discrete mathematics);grid systems corporation;grid computing;interaction;malware;markov chain;openness;sensor;simulation;threat model	Jan Kantert;Melanie Kauder;Sarah Edenhofer;Sven Tomforde;Christian Müller-Schloer	2016		10.5220/0005708301980206	grid;computer science;computation;multi-agent system;exploit;distributed computing	Metrics	-42.38099636600096	56.42421136946135	19986
4c0968b47222d3322c28288fc8b057e0dfb8b50c	cloud security: a virtualized vlan (v2lan) implementation	virtualization;multi tenancy;v2lan;identification;authorization;cloud computing	Cloud computing is an emergent technology that brings together all aspects of IT infrastructure from software installation and upgrade to platform oriented services to network and to hardware and storage. However, there are various security concerns that prevent customers from taking benefits of the cloud. Many studies have offered a wide range of possible solutions to deal with cloud security issues. Some of these solutions are very expensive therefore not suitable for Cloud. For example, data encryption is considered as a vital tool and mechanism for securing business data. However, it is not feasible to deploy data encryption on every piece of data. Anthes argues that encryption is sometimes seen as the ultimate security measure, but in fact, encryption is a complex and costly process since encrypted data needs to be downloaded and decrypted for local use and then possibly uploading the results [1]. This study offers a robust, fast and cost effective security measures for protecting Cloud data residing on virtual machines VMs without the need for any additional monitoring package or introspection at VM level.	cloud computing security;virtual lan	Farid Shirazi;Alexander Krasnov	2016		10.1007/978-3-319-39510-4_56	identification;cloud computing security;virtualization;cloud computing;client-side encryption;computer science;operating system;multitenancy;data security;authorization;internet privacy;world wide web;computer security;encryption	Crypto	-50.445271249795994	58.18945596705163	20064
d65402d8f0223f822ae2b5acf88287c16a4c32ef	secure and flexible certificate access in ws-security through ldap component matching	certificate repository;lightweight directory access protocol;web service;matching function;it security;security requirements;x 509 certificate;pki;web services security;ldap;directory service;component matching	As an integral part of the Web Services Security (WS-Security), directory services are used to store and access X.509 certificates. Lightweight Directory Access Protocol (LDAP) is the predominant directory access protocol for the Internet, and hence for the Web services. Values of LDAP attribute and assertion value syntaxes, though defined using ASN.1, are encoded in simple octet string formats which generally do not preserve the complete structure of the abstract values. As a result, LDAP matching rules for certificates need to be provided in a certificate-syntax specific way, while X.500 matching rules can be constructed from structured ASN.1 syntax definition. Moreover, LDAP has traditionally lacked the capability to make assertions against components of values of complex syntaxes such as X.509 certificates. The WS-Security needs to be able to locate a target X.509 certificate by matching against arbitrary certificate components in its security token references. Therefore, WS-Security requires the directory server to be prepared with all the possible matching functions for maximum flexibility. This is very cumbersome due to the lack of ASN.1 awareness in LDAP server implementations. This led to development of remedies such as the recently proposed Certificate Parsing Server (XPS). XPS extracts relevant components of the certificate and stores them in separate and searchable attributes. Due to the significant downside of these remedies, we decided to seek after an ASN.1 based Component Matching alternative in an attempt to make an LDAP directory server ASN.1 aware. With Component Matching and ASN.1 awareness, LDAP can provide WS-Security with various matching rules flexibly. In this paper, we describe our implementation of the Component Matching and ASN.1 awareness in OpenLDAP Software. This paper will also describe the use of the Component Matching technology in various security components of Web Services, especially in the context of WS-Security and XKMS. The experimental results show that flexible and secure certificate access can be accomplished without sacrificing performance and manageability.	abstract syntax notation one;assertion (software development);directory service;floor and ceiling functions;internet;lightweight directory access protocol;list of ldap software;octet (computing);open xml paper specification;openldap software;parsing;soap;security token;server (computing);ws-security;web service;world wide web;x.500;x.509;xkms	Sang Seok Lim;Jong Hyuk Choi;Kurt D. Zeilenga	2004		10.1145/1111348.1111358	lightweight directory access protocol;directory service;x.500;computer science;metadirectory;operating system;data mining;database;public key certificate;root certificate;law;world wide web;organizational unit;computer security	Security	-49.72754430193492	54.41930120410071	20090
1260719fef8e56dc59f1e2a148e07fcf7bbe89aa	comparison of unit-level automated test generation tools	software;unit level automated test generation tools;manuals;software testing;unit software testing;maintenance;unit testing;probability density function;automatic testing;testing;random testing;automated test generation;satisfiability;data mining;software engineering;experimentation unit software testing;automatic testing software testing vehicle crash testing java genetic mutations software tools maintenance automatic generation control conferences software engineering;program testing;automatic test software;automatic generation control;software projects;vehicle crash testing;software projects unit level automated test generation tools software quality public accessible unit test data generation tools java;software tools;time to market;software quality automatic test software java program testing;genetic mutations;experimentation;software quality;conferences;java;public accessible unit test data generation tools	Data from projects worldwide show that many software projects fail and most are completed late or over budget. Unit testing is a simple but effective technique to improve software in terms of quality, flexibility, and time-to-market. A key idea of unit testing is that each piece of code needs its own tests and the best person to design those tests is the developer who wrote the software. However, generating tests for each unit by hand is very expensive, possibly prohibitively so. Automatic test data generation is essential to support unit testing and as unit testing is achieving more attention, developers have a greater need for automated unit test data generation tools. However, developers have very little information about which tools are effective. This experiment compared three well-known public-accessible unit test data generation tools, JCrasher, TestGen4j, and JUB. We applied them to Java classes and evaluated them based on their mutation scores. As a comparison, we created two additional sets of tests for each class. One test set contained random values and the other contained values to satisfy edge coverage. Results showed that the automatic test data generation tools generated tests with almost the same mutation scores as the random tests.	java;test data generation;test set;unit testing	Shuang Wang;A. Jefferson Offutt	2009	2009 International Conference on Software Testing, Verification, and Validation Workshops	10.1109/ICSTW.2009.36	reliability engineering;black-box testing;regression testing;test data generation;test data;white-box testing;integration testing;computer science;systems engineering;software engineering;test compression;dynamic testing;software testing;unit testing;test management approach;test harness	SE	-60.97247467519447	34.27898238622563	20106
8ab0ceb3c90576dfebaa2b2da76c2d2e9f0fff86	a mapping from aadl to java-rtsj	architecture analysis and design languages aadl;architecture description language;real time specification for java;architecture description languages;architecture analysis;real time specification for java rtsj;java;real time systems	In this paper, we study a mapping from AADL to Java-RTSJ. After reviewing the basic concepts of the AADL execution model, we present the basic notions of Java-RTSJ, we rely on, for our mapping. Then, we propose a mapping taking into account a given subset of AADL. A related works section reviews existing works and elaborates on some comparisons.	architecture analysis & design language;embedded system;executable;experiment;formal language;linux;real time java;real-time clock;scheduling (computing)	Jean-Paul Bodeveix;Raphaël Cavallero;David Chemouil;Mamoun Filali;Jean-François Rolland	2007		10.1145/1288940.1288965	architecture description language;computer architecture;real-time computing;architecture analysis & design language;computer science;software architecture description;programming language;java	NLP	-39.875030714940124	32.92882378699303	20159
36b96e42de38e7dafda6bc4748942e93017f0e16	internet of things security and privacy: design methods and optimization	software;hardware and architecture;computer networks and communications	Recent advances in information and communication technologies and embedded systems have given rise to a new disruptive technology: the Internet of Things (IoT). This major development will lead to major changes in usage and to a transformation of the technological ecosystem in all its complexity. IoT will allow people and objects in the physical world as well as data and virtual environments to interact with each other so as to create smart environments such as smart transport systems, smart cities, smart health, smart energy, etc., as part of a prosperous digital society. IoT is likely to improve the quality of people’s lives, create new markets and new jobs, increase economic growth and be an impetus for competition. However, IoT raises important questions and introduces new challenges for the security of systems and processes and the privacy of individuals. Some IoT applications are tightly linked to sensitive infrastructures and strategic services such as the distribution of water and electricity and the surveillance of assets. Other applications handle sensitive information about people, such as their location and movements, or their health and purchasing preferences. Confidence in and acceptance of IoT will depend on the protection it provides to people’s privacy and the levels of security it guarantees to systems and processes. IoT will enable objects to become active participants: these objects will be able to recognize events and changes in their environment and to sense and react autonomously without human intervention. Introducing objects into the control processes makes IoT security very difficult to address. Indeed, the Internet of Things is a complex system in which people interact with the technological ecosystem based on smart objects through complex processes. The interactions of these four IoT components: persons, intelligent objects, technological ecosystem, and processes highlight a systemic and cognitive dimension to the security of IoT. The interaction of people with the technological ecosystem requires the protection of their privacy. Similarly, their interaction with control processes requires to guaranteeing their safety. Processes must ensure their reliability and realize the objectives for which they are designed. The move towards a greater autonomy for objects will bring the security of technologies and processes and the privacy of individuals into sharper focus. Furthermore, in parallel with the increasing autonomy of objects to perceive and act on the environment, IoT security should move towards a greater autonomy in perceiving threats and reacting to attacks. The purpose of this special issue is to study and evaluate architectures and solutions that ensure Internet of Things Security and Privacy. The special issue consists of 7 papers proposing solutions for securing Internet of Things, providing efficient privacy and confidentiality in spite of the ubiquitous nature of IoT and the constrained resources and capacities: Paper ‘‘OSCAR: Object Security Architecture for the Internet of Things’’ proposes an architecture for end-toend security in the Internet of Things. It is based on the concept of object security that relates security with the application payload. The architecture includes Authorization Servers that provide clients with Access Secrets that enable them to request resources from constrained CoAP nodes. The results show that OSCAR outperforms a security scheme based on DTLS when the number of nodes increases. OSCAR also results in low energy consumption and latency. The paper ‘‘Survey on Secure Communication Protocols for the Internet of Things’’ presents security challenges in IoT and surveys security protocols for IoT. Then, authors discuss suitability of proposed solutions to IoT context and constraints. In ‘‘Providing Destructive Privacy and Scalability in RFID Systems Using PUFs’’, authors propose a scalable authentication protocol for RFID systems. The solution utilizes Physically Unclonable Functions (PUFs) as a secure storage to keep secrets of the tag in order to achieve higher level of privacy with constant identification time. It provides destructive privacy according to the Vaudenay’s privacy and security	authentication protocol;authorization;autonomous robot;complex system;confidentiality;constrained application protocol;datagram transport layer security;ecosystem;embedded system;information sensitivity;interaction;internet of things;job stream;mathematical optimization;privacy;purchasing;scalability;secure communication;smart city;smart environment;smart objects;virtual reality	Yacine Challal;Enrico Natalizio;Sevil Sen;Anna Maria Vegni	2015	Ad Hoc Networks	10.1016/j.adhoc.2015.05.010	software security assurance;internet architecture board;privacy software;computing;the internet;web of things;computer security compromised by hardware failure;computer science;hardware architecture;internet security;internet privacy;computer security;computer network	Security	-45.67144732387219	50.76760332767641	20163
f44100050a7d68d9e9176d62c81d997e36131e3d	designing hard real-time systems	life cycle;non functional requirement;design method;hard real time system;structural design;hard real time;real time systems	This paper presents a systems life cycle and a structured design method which are tailored towards the construction of real-time systems in general, and hard real-time systems in particular. The standard systems life cycle is modified to take into account the expression and satisfaction of non-functional requirements. The HOOD design method is extended to support abstractions which explicitly cater for the characteristics and properties of hard real-time systems. The new method is called HRT-HOOD (Hard Realtime HOOD).	ada;functional requirement;hood method;non-functional requirement;real-time clock;real-time computing;real-time locating system;real-time transcription;structured analysis;system lifecycle;systems design;eric	Alan Burns;Andy J. Wellings	1992		10.1007/3-540-55585-4_11	biological life cycle;real-time computing;design methods;computer science;non-functional requirement	Embedded	-41.253220508243636	33.11831389568102	20207
49fec79d3325d74ba0fdcf49f72d9500aaf6416e	on requirements representation and reasoning using answer set programming	social network services;software;dynamic redirection requirement representation requirement reasoning answer set programming artificial intelligence techniques software engineering adaptive business systems commercial software development metamodel runtime requirements formal language computational model instal software architecture distributed system monitoring business system development e tailing runtime monitor normative framework specification normative framework verification normative framework application distributed intelligent systems agile systems robot controls autonomous components;computational modeling;monitoring;system monitoring artificial intelligence business data processing formal languages formal specification logic programming object oriented programming program verification software architecture;business;cognition;programming;business computational modeling monitoring software cognition social network services programming	We describe an approach to the representation of requirements using answer set programming and how this leads to a vision for the role of artificial intelligence techniques in software engineering with a particular focus on adaptive business systems. We outline how the approach has developed over several years through a combination of commercial software development and artificial intelligence research, resulting in: (i) a metamodel that incorporates the notion of runtime requirements, (ii) a formal language for their representation and its supporting computational model (InstAL), and (iii) a software architecture that enables monitoring of distributed systems. The metamodel is the result of several years experience in the development of business systems for e-tailing, while InstAL and the runtime monitor is on-going research to support the specification, verification and application of normative frameworks in distributed intelligent systems. Our approach derives from the view that in order to build agile systems, the components need to be structured more like software that controls robots, in that it is designed to be relatively resilient in the face of a non-deterministic, dynamic, complex environment about which there is incomplete information. Thus, degrees of autonomy become a strength and an opportunity, but must somehow be constrained by informing these autonomous components what should be done in a certain situation or what system state ought to be achieved through norms as expressions of requirements. Because such a system made up of autonomous components is potentially behaviourally complex and not just complicated, it becomes essential to monitor both whether norms/requirements are being fulfilled and if not why not. Finally, because control over the system can be expressed through requirements in the form of data that can be changed, a route is opened to adjustment and dynamic re-direction of running systems.	agile software development;answer set programming;artificial intelligence;autonomous robot;commercial software;computation;computational model;distributed computing;e-commerce;formal language;metamodeling;requirement;software architecture;software engineering;stable model semantics	Julian Padget;Emad Eldeen Elakehal;Ken Satoh;Fuyuki Ishikawa	2014	2014 IEEE 1st International Workshop on Artificial Intelligence for Requirements Engineering (AIRE)	10.1109/AIRE.2014.6894854	requirements analysis;software requirements specification;system requirements;business requirements;computer science;systems engineering;software design;theoretical computer science;software framework;component-based software engineering;software development;requirement;software engineering;software construction;systems development life cycle;resource-oriented architecture;system programming;software development process;software system	SE	-43.15975322935222	38.71610615089746	20324
3cebe4f4bc90797bc6338a7ed7efa4613c673bab	goaldebug: a spreadsheet debugger for end users	change inference process goaldebug spreadsheet debugger end users user specified output ranking heuristics;empirical study;software maintenance;programming profession error correction performance evaluation automatic testing fault diagnosis nist software debugging software testing;faulty codes;system performance;proof of concept;user centred design program debugging;expected value;design patterns;user centred design;program debugging	We present a spreadsheet debugger targeted at end users. Whenever the computed output of a cell is incorrect, the user can supply an expected value for a cell, which is employed by the system to generate a list of change suggestions for formulas that, when applied, would result in the user-specified output. The change suggestions are ranked using a set of heuristics. In previous work, we had presented the system as a proof of concept. In this paper, we describe a systematic evaluation of the effectiveness of inferred change suggestions and the employed ranking heuristics. Based on the results of the evaluation we have extended both, the change inference process and the ranking of suggestions. An evaluation of the improved system shows that change inference process and the ranking heuristics have both been substantially improved and that the system performs effectively.	debugger;heuristic (computer science);spreadsheet	Robin Abraham;Martin Erwig	2007	29th International Conference on Software Engineering (ICSE'07)	10.1109/ICSE.2007.39	reliability engineering;software design pattern;real-time computing;computer science;operating system;software engineering;database;computer performance;programming language;empirical research;software maintenance;proof of concept;expected value	SE	-60.88807798507921	35.56775568711022	20343
08bb25aec50ebb762ab2b0e970bc4f43dc9c4523	efficient data-intensive event-driven interaction in soa	eda;soa;data intensive applications;publish subscribe;web services	This paper presents a middleware that enables the efficient delivery of events carrying large attachments. We transparently decouple event-description from event-data, in order to avoid useless data-transfers and modifications to endpoints business logic. Our solution relieves the event-delivery system of large data transfers, by enabling direct, but transparent, publisher to subscriber data-exchange. The experiments show that we can reduce the average event delivery time by half, compared to a standard approach requiring the full mediation of the event-delivery system.	attachments;business logic;data-intensive computing;event-driven programming;experiment;middleware;service-oriented architecture	Quirino Zagarese;Gerardo Canfora;Eugenio Zimeo;Iyad Alshabani;Laurent Pellegrino;Françoise Baude	2013		10.1145/2480362.2480715	web service;real-time computing;electronic design automation;computer science;operating system;service-oriented architecture;database;publish–subscribe pattern;world wide web;computer security	HPC	-33.85116746874521	51.15494797960876	20382
fb7d2d3aeb66387c76ae0e6676a785782f88cf2c	soapbox: a platform for ubiquitous computing research and applications	distributed system;systeme reparti;context aware;informatique mobile;user interface;intelligent environment;wireless communication;sistema repartido;ubiquite;design and implementation;toma de conciencia;awareness;contexto;contexte;ubiquitous computing;ubiquity;mobile computing;low power consumption;context;prise conscience	Designing, implementing and evaluating prototypes is a normal way of doing technical research. In recent years we have seen lots of research prototypes specifically designed for context awareness, future user interfaces and intelligent environment research. The problem with this type of specialised prototypes is that their lifetime is rather short and the valuable work done for them is not easily reusable. Our approach has been different as we have deliberately aimed towards a multipurpose platform that would be suitable for various ubiquitous computing related research themes. In this article we present the design and implementation of the platform that is named as SoapBox (Sensing, Operating and Activating Peripheral Box). Its main features are wired and wireless communications, in-built sensors, small size and low power consumption. We also introduce some results of research projects that have already used the platform successfully. Finally we conclude the paper with application scenarios for further work.		Esa Tuulari;Arto Ylisaukko-oja	2002		10.1007/3-540-45866-2_11	embedded system;simulation;awareness;computer science;artificial intelligence;operating system;database;distributed computing;user interface;mobile computing;computer security;ubiquitous computing;wireless	HCI	-37.634194361814465	47.199712429823386	20456
957eada5bfc44eef6525d1ee1ba11214595baa31	challenges in using search-based test generation to identify real faults in mockito		The cost of test creation can potentially be reduced through automated generation. However, to impact testing practice, automated tools must be at least as effective as manual generation. The Mockito project—a framework for mocking portions of a system—offers an opportunity to assess the capabilities of test generation tools on a complex real-world system. We have identified 17 faults in the Mockito project, added those to the Defects4J database, and assessed the ability of the EvoSuite tool to detect these faults. In our study, EvoSuite was only able to detect one of the 17 faults. Analysis of the 16 undetected faults yields lessons in how to improve generation tools. We offer these faults to the community to assist in benchmarking future test generation advances.	evosuite;fault detection and isolation;fitness function;mock object;mockito;software versioning;world-system	Gregory Gay	2016		10.1007/978-3-319-47106-8_17		SE	-61.14028060062649	36.708198144179846	20470
2c06a4f3d16abaed6ca9fe1e868d8f86f7e9ace7	jug: a junit generation, time complexity analysis and reporting tool to streamline grading	automated testing;time complexity;unit testing;complexity analysis;generation time;evaluation;generative programming;generic programming	The JUnit Generation (JUG) system provides fast, semiautomated feedback to students. It uses a Java-like script to generate unit tests and time complexity tests, then runs those tests to generate reports. The goals for JUG are improved feedback for students, and decreased preparation and grading time for instructors and grading assistants.	adobe streamline;analysis of algorithms;junit;java;time complexity;unit testing	Christopher Brown;Robert Pastel;Bill Siever;John Earnest	2012		10.1145/2325296.2325323	time complexity;simulation;computer science;theoretical computer science;evaluation;unit testing;programming language;generation time;generic programming	AI	-58.12952674045725	37.14328145263621	20521
7657667b94830a23d5591db725e2e0ded3be8847	applying frequent sequence mining to identify design flaws in enterprise software systems.	software systems;sequence mining	In this paper we show how frequent sequence mining (FSM) can be applied to data produced by monitoring distributed enterprise applications. In particular we show how we applied FSM to run-time paths to highlight repeating sequences of interest by using alternative support counting techniques. We show how the patterns identified, can be used to highlight design flaws in enterprise applications. We also discuss some algorithm scalability problems related to applying FSM to run-time paths and give solutions to these issues.	a.i. artificial intelligence;algorithm;apriori algorithm;association rule learning;data mining;database;dependability;edmund m. clarke;enterprise javabeans;enterprise software;entity–relationship model;flaw hypothesis methodology;java platform, enterprise edition;maxima and minima;path tracing;preprocessor;run time (program lifecycle phase);sigkdd;scalability;sequential pattern mining;software release life cycle;trie	Trevor Parsons;John Murphy;Patrick O'Sullivan	2007			sequential pattern mining;software mining;computer science;software development;software construction;data mining;software analytics;world wide web;software system	DB	-58.2417963388832	39.85774134555033	20522
7e17146c41cf057056ca537bca7245f3d1846902	context as autonomic intelligence in a ubiquitous computing environment	ans;autonomic networked services;funcion utilidad;aplicacion medical;medical patients;coronary heart disease;utility functions;sensors;fonction utilite;localization;reseau ordinateur;autonomic frameworks;utility function;healthcare technology;localizacion;qualite service;computer network;informatique omnipresente;qoc;localisation;internet protocol technology;red informatica;ubiquitous computing;medical application;patient monitoring;remote monitoring;quality of context;autonomic computing;service quality;application medicale;calidad servicio;ubiquitous computing environment	This paper presents the ANS architecture that uses ubiquitous computing to monitor medical patients in the home. Since there is no notion of the patient carrying out maintenance of such a system, it must be self-managing or autonomic. In the ANS sensors, such as temperature, location, etc., use a form of logic to abduce a context, i.e. the state/quality of a given device or its function. Our contribution lies in the emergent autonomicity of the architecture driven by its ability to derive the most appropriate source for a particular application. This is done by allowing the application to define mathematically its own notion of what level of service provides the best satisfaction and is based on Quality of Context attributes that describe each alternative. These applicationspecific definitions allow the ANS to optimally adapt to alternative sources when context providers fail or their quality changes. Further, we uniquely evaluate the trustworthiness of the different sources of context, thus allowing the applications to choose how much risk they are willing to take in the hope of receiving satisfaction. The ANS framework is lightweight and can provide real-time adaptation, which is necessary in resource-starved ubiquitous computing environments that support medical applications.	abductive reasoning;autonomic computing;emergence;real-time transcription;self-management (computer science);sensor;trust (emotion);ubiquitous computing	Markus C. Huebscher;Julie A. McCann;Asher Hoskins	2007	IJIPT	10.1504/IJIPT.2007.011595	embedded system;simulation;internationalization and localization;computer science;sensor;remote patient monitoring;computer security;ubiquitous computing;service quality;computer network;autonomic computing;rmon	HCI	-43.2011862009536	44.78374297946472	20535
182834e912ce58d26f89255fd5c6fae42c5834d2	hot-patching a web server: a case study of asap code repair	software maintenance internet program debugging security of data;software maintenance;internet;program debugging;sandbox dynamic binary translation hot patching web server asap code repair software updates software bugs multithreaded apache installation service restart frequency minimization maintenance window system integrity code repair as soon as possible on the fly update system;security of data;security software libraries computer bugs maintenance engineering virtualization runtime	Software updates are the current standard to respond to software bugs. The software developer provides an update fix that is then applied by the administrator: the binary is modified and the service is restarted. Restarting a service inevitably leads to downtime and service unavailability; in the case of a multithreaded installation of Apache, restart takes several seconds and depending on the load of the web server, several hundred or even thousand client requests will be rejected with an error. Given the cost of restarts, system administrators attempt to minimize the frequency of service restarts or postpone a restart until the next maintenance window. However, to ensure the integrity of the system, code repair must happen as soon as possible (ASAP). We describe here the effectiveness of an on-the-fly update system that provides ASAP repair by integrating dynamic patches with a sandbox based on dynamic binary translation. To investigate the feasibility of ASAP code repair, we analyze the software updates released for Apache 2.2 between Dec 1st, 2005 and Feb 18, 2013. The study shows that such a system allows patching 45 of 49 bugs at runtime. Of the 4 unpatchable bugs, 1 bug is not applicable to dynamic update mechanisms, and 3 bugs require a restart. Furthermore, a performance evaluation of the prototype implementation shows that our approach adds low execution overhead (below 7% for different configurations that request a 287kB file).	binary translation;downtime;hot swapping;overhead (computing);patch (computing);performance evaluation;prototype;run time (program lifecycle phase);server (computing);software bug;software developer;system administrator;thread (computing);unavailability;web server	Mathias Payer;Thomas R. Gross	2013	2013 Eleventh Annual Conference on Privacy, Security and Trust	10.1109/PST.2013.6596048	real-time computing;the internet;software bug;computer science;operating system;software maintenance;world wide web;computer security	OS	-57.003296669861406	55.14210880696599	20573
79f21610db08bea01328e9f5e42418098dcdc543	visualization of program dependence and slices	software;graphs;arbitrary node sets program visualization program slices program dependence graph source code fine grained visualization;visualization layout software maintenance technological innovation application software software testing area measurement software measurement data flow computing iterative methods;source code;program slicing;program slicing program visualisation;program visualisation;program dependence graph	The program dependence graph (PDG) itself and the computed slices within the program dependence graph are results that should be presented to the user in a comprehensible form, if not used in subsequent analyses. A graphical presentation would be preferred as it is usually more intuitive than textual ones. This work describes how a layout for the PDGs can be generated to enable an appealing presentation. However, experience shows that the graphical presentation is less helpful than expected and a textual presentation is superior. Therefore, this work contains an approach to textually present slices of PDGs in source code. The innovation of this approach is the fine-grained visualization of arbitrary node sets based on tokens and not on complete lines like in other approaches. Furthermore, a major obstacle in visualization and comprehension of slices is the loss of locality. Thus, this work presents a simple, yet effective, approach to limit the range of a slice. This approach enables a visualization of slices where the local effects stand out against the more global effects. A second, more sophisticated approach visualizes the influence range of chops for variables and procedures. This enables a visualization of the impact of procedures and variables on the complete system.	cluster analysis;declarative programming;graph drawing;graphical user interface;high- and low-level;locality of reference;point of interest;program dependence graph;programmer;reverse engineering;software maintenance	Jens Krinke	2004	20th IEEE International Conference on Software Maintenance, 2004. Proceedings.	10.1109/ICSM.2004.1357801	software visualization;program slicing;information visualization;computer science;theoretical computer science;software engineering;database;graph;programming language;source code	SE	-55.06725674660749	35.73033404395752	20670
0e959914cba17dea067a550554a0ce9196e6fd04	an architecture for pull-based public health interventions	sensors;public healthcare sensors data collection servers privacy distributed databases computer architecture;data collection;population health;computer architecture;servers;distributed databases;mobile health;public healthcare;smart phones medical information systems;public health mobile health population health;privacy;public health;strong privacy support pull based public health interventions smartphone devices	Public health interventions consisting of information dissemination to affect behavior have long been a significant form of public health campaign. These interventions can be considered 'push-based' as they push information out to a population. New smartphone technologies for the first time provide a platform for a new type of informational public health intervention, which can be referred to as 'pull-based' interventions. In such interventions, it is the device via automatically collecting data relevant to the individual's health that triggers the 'request' for and receipt of the informational public health intervention. This will enable far more targeted and personalized public health interventions than previously possible. Such techniques do however also pose privacy and security challenges. In this paper we introduce the architecture for pull-based public health interventions upon smartphone devices that also provides strong privacy support. We also evaluate its performance and scalability benefits relative to non-pull-based interventions.	personalization;scalability;smartphone	Robert Steele;Andrew Clarke	2016	2016 49th Hawaii International Conference on System Sciences (HICSS)	10.1109/HICSS.2016.406	public relations;public health;computer science;sensor;hrhis;internet privacy;privacy;computer security;server;statistics;data collection	HCI	-43.2322545326367	60.13428408839087	20680
2ea8afdbdceeb4dd1e2153e075ca92e13d0e9120	millimeter-scale computing platform for next generation of internet of things	ultra low power;iot;ultra low power wireless sensor node wsn internet of things iot;wsn;wireless sensor node;internet of things;antenna size millimeter scale computing platform internet of things next generation smart physical objects quality of machine service processing information synergy millimeter scale sensors signal sources surveillance targets climate targets biomedical targets ultra low power consumption;sensors batteries wireless communication internet of things receivers wireless sensor networks peer to peer computing;wireless sensor networks internet of things next generation networks power consumption	"""The internet of things has enabled the communication of """"smart"""" physical objects and improved the quality of machine service to humans. These achievements are based on the synergy of processing information collected by different sources. This trend will continue with the next class of computing platforms enabled by millimeter-scale sensors, diversifying the signal sources including biomedical, climate and surveillance targets. The key challenge of such systems is to achieve small die size, ultra-low power consumption, sufficient communication distance with the limited antenna size and accurate system duty-cycling."""	die (integrated circuit);duty cycle;internet of things;next-generation network;sensor;synergy	Tae-Kwang Jang;Myungjoon Choi;Yao Shi;Inhee Lee;Dennis Sylvester;David Blaauw	2016	2016 IEEE International Conference on RFID (RFID)	10.1109/RFID.2016.7487997	telecommunications;engineering;key distribution in wireless sensor networks;computer security;internet of things;computer network	Embedded	-40.39166561542927	48.15377462623615	20682
cf03cb1a6a3718c40c2b88a7b65117ad92e9ed07	modeling and evaluating information leakage caused by inferences in supply chains	modelizacion;analyse risque;controle acces;salida;logistique;risk analysis;securite informatique;risk evaluation;conceptual analysis;conceptual model;fuite;analisis conceptual;confidencialidad de datos;information sharing;computer security;modelisation;analisis riesgo;adverse effect;logistics;data privacy;seguridad informatica;inferencia;leak;supply chain;mitigation;access control;analyse conceptuelle;modeling;information leakage;confidentialite donnee;control de acceso;inference;logistica	While information sharing can benefit supply chains significantly, it may also have an adverse effect, namely, information leakage. A limitation common to many existing solutions for preventing information leakage in supply chains is that they rely, either implicitly or explicitly, upon two unrealistic assumptions. First, what information is confidential is well known. Second, confidential information will not be revealed, if only it is not shared, regardless of how much other information is being shared. As we shall show in this paper, those assumptions are not always true due to potential information leakage caused by inferences. Specifically, we propose a conceptual model of such information leakage. The model will enable companies in a supply chain to better understand how their confidential information may be leaked through inferences. On the basis of the proposed conceptual model, we then devise a quantitative approach to evaluating the risk of information leakage caused by inferences when a given amount of information is shared. The quantitative approach will allow companies in a supply chain to measure and consequently mitigate the risk of information leakage. Finally, we discuss a case study to illustrate how the proposed approaches work in practice. 2010 Elsevier B.V. All rights reserved.	confidentiality;information leakage;spectral leakage	Da Yong Zhang;Yong Zeng;Lingyu Wang;Hongtao Li;Yuanfeng Geng	2011	Computers in Industry	10.1016/j.compind.2010.10.002	logistics;systems modeling;risk analysis;information privacy;adverse effect;computer science;engineering;conceptual model;access control;operations management;data mining;supply chain;computer security	AI	-53.27102559765497	46.44012404051939	20686
2ee1f05dbe939e27e4fd5fb2fc2a83c0d22cef86	checking enforcement of integrity constraints in database applications based on code patterns	code quality;empirical analysis;quality improvement;application integration;code patterns;automatic detection;php;integrity constraints;static analysis;integrity constraint enforcement;open source	Integrity constraints (including key, referential and domain constraints) are unique features of database applications. Integrity constraints are crucial for ensuring accuracy and consistency of data in a database. It is important to perform integrity constraint enforcement (ICE) at the application level to reduce the risk of database corruption. We have conducted an empirical analysis of open-source PHP database applications and found that ICE does not receive enough attention in real-world programming practice. We propose an approach for automatic detection of ICE violations at the application level based on identification of code patterns. We define four patterns that characterize the structures of code implementing integrity constraint enforcement. Violations of these patterns indicate the missing of integrity constraint enforcement. Our work contributes to quality improvement of database applications. Our work also demonstrates that it is feasible to effectively identify bugs or problematic code by mining code patterns in a specific domain/application area.	data integrity	Hongyu Zhang;Hee Beng Kuan Tan;Lu Zhang;Xi Lin;Xiaoyin Wang;Chun Zhang;Hong Mei	2011	Journal of Systems and Software	10.1016/j.jss.2011.06.044	quality management;computer science;data integrity;data mining;database;computer security;static analysis;software quality	SE	-60.496257292919395	40.73623191707438	20750
0689d500ca09fc35d425a0614c49aa599f9c72bc	software compatibility: what was promised, what we have, what we need	time constraint	Software compatibility is an extremely complex and pervasive topic. Unfortunately it is not well defined nor well documented. To say anything useful within the space and time constraints we must confine ourselves to generalities and readily admit in advance that there are many exceptions.	computer compatibility;pervasive informatics	John A. Gosden	1968		10.1145/1476589.1476605	computer science;artificial intelligence;operations management;algorithm	AI	-45.84007317169565	46.16219028202017	20761
8786d04c05fa8389992c20c66023d1c7be56965d	designing for change	design tool;formal specification;object oriented design;user requirements;design for change;design methodology	The ambition of every designer is the software equivalent of a cathedral. But maintenance programmers are more comfortable in a farmhouse than a cathedral. We argue that current design methodologies are oriented towards cathedrals, and we propose object oriented design techniques and tools that are suitable for farmhouses. During the lifetime of a useful program, its users’ requirements change and the code changes to track the requirements. The code drifts away from the original design, becomes increasingly brittle, and eventually can no longer be maintained; each repair introduces new faults. The cure for these ills—design for change—is well-known, but current design methodologies and tools do not facilitate useful changes. We describe a design tool that supports evolutionary object oriented design. Designers can create and modify designs, view them in textual and graphical form, check their internal consistency, and match them to requirements and code. To accomplish this, we use text, tables, and diagrams with multiple levels of formality. The tool processes formal entities completely (as a compiler can process source code completely); it stores, retrieves, and displays informal entities (whereas a compiler discards comments); and it can perform limited operations on semiformal entities. Our work borrows from formal specification techniques, but is intended for software that evolves.	compiler;design tool;diagram;entity;formal specification;graphical user interface;programmer;requirement;table (database);the cathedral and the bazaar	Peter Grogono	1994		10.1145/782206	simulation;design methods;computer science;user requirements document;operating system;object-oriented design;software engineering;formal specification;database;distributed computing;design education;programming language	SE	-52.03267844975042	34.031292029595136	20832
0256a05aeb0909e2758589de3e865f6b615fbd80	mobile computing: technologies for a disconnected society	application software;pervasive computing;distributed computing;mobile computer;information and communication services;journal article;computer networks;mobile computing pervasive computing application software ubiquitous computing computer networks mobile communication portable computers wireless communication internet societies;wireless communication;internet;portable computers;mobile communication;communication networks and services;ubiquitous computing;societies;computer science;electronic computers computer science;mobile computing;information and computing sciences;mobile data networks and services	"""T he term mobile computing is used to describe an array of technologies supporting personal mobility for computer users. Such mobility can take the form of users moving between fixed terminals anywhere in the world or users taking mobile devices with them wherever they move. In both cases, the user should have a consistent working environment with access to their usual files, email, and so on. Mobility should therefore support the seamless movement of people, data, and/or applications between different locations. The technologies required are a combination of various communications media, data servers, and (possibly portable) computers. Fundamentally, mobile computing aims to provide a totally ubiquitous computing environment where people can work and play anywhere , and at anytime. The term """" ubiquitous computing """" is usually first attributed to Mark Weiser, a chief technologist at Xerox PARC. Weiser defined the term well in a description of the future and potential of mobile computing: The technology of literacy when first invented, and for thousands of years afterwards, was expensive, tightly controlled, precious. Today it effortlessly , unobtrusively surrounds us. Look around now: how many objects and surfaces do you see with words on them? Computers in the workplace can be as effortless, and ubiquitous, as that. Long-term the PC and worksta-Three broad challenges face mobile computing today: reliable wireless communications, support for disconnected operation, and mobile applications development."""	anytime algorithm;cardiovascular technologist;email;mobile app;mobile computing;mobile device;personal computer;seamless3d;server (computing);ubiquitous computing;user (computing)	Paddy Nixon;Vinny Cahill	1998	IEEE Internet Computing	10.1109/MIC.1998.656061	application software;mobile search;the internet;mobile web;mobile telephony;human–computer interaction;computer science;theoretical computer science;operating system;mobile technology;distributed computing;mobile computing;ubiquitous computing;wireless	HCI	-38.65435088811492	54.44257472875061	21008
1373005d27e69df167b35118936dc178bdaeae48	a component-based approach for integrating mobile agents into the existing web infrastructure	agent platform;security architecture component based approach mobile agent web infrastructure internet javabeans web server java servlet;components;web agents;mobile agents;components mobile agents web agents;web agent;object oriented programming;internet;security of data object oriented programming java mobile agents internet;agent technology;security architecture;mobile agent;mobile agents web server internet protection information security java service oriented architecture yarn application software distributed computing;security of data;java	Mobile agents provide a new abstraction for deploying functionality over the existing Internet infrastructure. In the MA (mobile agents) framework there are no agent platforms. Instead applications become agent-enabled by using simple JavaBeans components. In this paper we present an architecture that allows currently available Web servers to become capable of sending and receiving agents in an easy way. By using this approach, existing Web infrastructure can be maintained, while gaining a whole new potential by being able to make use of agent technology. Our approach involves wrapping the components inside a Java Servlet that can be included in any Web server supporting the Servlet specification. This Servlet enables the servers to receive and send agents that can query local information, and also enables the agents to behave as Servlets themselves. We currently have used the framework with several existing commercial Web servers, inclusively having the security mechanisms of the framework correctly running and integrated with the security architecture of the server.	component-based software engineering;computer security;internet;java servlet;mobile agent;server (computing);web server;wrapping (graphics)	Haeng-Kon Kim	2005	Third ACIS Int'l Conference on Software Engineering Research, Management and Applications (SERA'05)	10.1109/SERA.2005.5	real-time computing;the internet;computer science;operating system;mobile agent;distributed computing;programming language;object-oriented programming;java;world wide web;application server;enterprise information security architecture	SE	-35.257805474144135	42.79131969237894	21095
01639d926309dbdfd566779a6d6e26613b097823	reliability and performance of component based software systems with restarts, retries, reboots and repairs	integrated approach;system reliability;program diagnostics;component based systems;software performance evaluation;software systems;attribute analysis;software fault tolerance;object oriented programming;component based software;component based software systems;software performance;software architecture;system recovery;software systems hardware failure analysis availability mission critical systems application software software architecture software measurement performance analysis software performance;operating system;queueing network model;mission critical applications;attribute analysis software reliability software performance component based software systems mission critical applications fault recovery software architecture system analysis;software component;system analysis;software reliability;fault recovery;system recovery object oriented programming program diagnostics software fault tolerance software performance evaluation;modeling and analysis;markov chain	High reliability and performance are vital for software systems handling diverse mission critical applications. Such software systems are usually component based and may possess multiple levels of fault recovery. A number of parameters, including the software architecture, behavior of individual components, underlying hardware, and the fault recovery measures, affect the behavior of such systems, and there is a need for an approach to study them. In this paper we present an integrated approach for modeling and analysis of component based systems with multiple levels of failures and fault recovery both at the software, as well as the hardware level. The approach is useful to analyze attributes such as overall reliability, performance, and machine availabilities for such systems, wherein failures may happen at the software components, the operating system, or at the hardware, and corresponding restarts, retries, reboots or repairs are used for mitigation. Our approach encompasses Markov chain, and queueing network modeling, for estimating system reliability, machine availabilities and performance. The approach is helpful for designing and building better systems and also while improving existing systems	booting;component-based software engineering;markov chain;mission critical;operating system;queueing theory;software architecture;software system	Vibhu Saujanya Sharma;Kishor S. Trivedi	2006	2006 17th International Symposium on Software Reliability Engineering	10.1109/ISSRE.2006.39	reliability engineering;software architecture;markov chain;real-time computing;software performance testing;computer science;systems engineering;engineering;component-based software engineering;operating system;software engineering;system analysis;programming language;object-oriented programming;software quality;software fault tolerance;software system	Arch	-47.443471350454175	34.545322825863074	21176
ad5c91c63d153197ea37eb6a3ad333ef59421d45	visualizing dynamic software system information through high-level models	programming environments;tool support;execution trace;programming environment;perforation;program comprehension;performance;software systems;software structure;software engineering;dynamic information;object oriented systems;qualitative study;performance tuning;software visualization	Dynamic information collected as a software system executes can help software engineers perform some tasks on a system more effectively. To interpret the sizable amount of data generated from a system's execution, engineers require tool support. We have developed an off-line, flexible approach for visualizing the operation of an object-oriented system at the architectural level. This approach complements and extends existing profiling and visualization approaches available to engineers attempting to utilize dynamic information. In this paper, we describe the technique and discuss preliminary qualitative studies into its usefulness and usability. These studies were undertaken in the context of performance tuning tasks.	iterative method;online and offline;performance tuning;software engineer;software framework;software system;system information (windows);usability;utility	Robert J. Walker;Gail C. Murphy;Bjørn N. Freeman-Benson;Darin Wright;Darin Swanson;Jeremy Isaak	1998		10.1145/286936.286966	software visualization;personal software process;verification and validation;computing;real-time computing;software sizing;performance;computer science;qualitative research;package development process;software design;social software engineering;software framework;component-based software engineering;software development;software construction;software testing;software walkthrough;programming language;software analytics;resource-oriented architecture;software deployment;software quality;software metric;software system	PL	-55.1967870126955	36.622319877036055	21191
56127bca7abc18b71a5a4ac4c9d0446064a1ef4e	secure programming via visibly pushdown safety games	secure programming;pushdown safety game;high-level policy;high-level security policy;strong security policy;practical program;policy-weaving problem;privilege-aware operating system;practical privilege-aware system;system call;application interacts;recent operating system	Several recent operating systems provide system calls that allow an application to explicitly manage the privileges of modules with which the application interacts. Such privilege-aware operating systems allow a programmer to a write a program that satisfies a strong security policy, even when it interacts with untrusted modules. However, it is often non-trivial to rewrite a program to correctly use the system calls to satisfy a high-level security policy. This paper concerns the policyweaving problem, which is to take as input a program, a desired highlevel policy for the program, and a description of how system calls affect privilege, and automatically rewrite the program to invoke the system calls so that it satisfies the policy. We present an algorithm that solves the policy-weaving problem by reducing it to finding a winning modular strategy to a visibly pushdown safety game, and applies a novel game-solving algorithm to the resulting game. Our experiments demonstrate that our algorithm can efficiently rewrite practical programs for a practical privilege-aware system.	algorithm;experiment;high- and low-level;operating system;programmer;rewrite (programming);stack (abstract data type);system call;video game developer	William R. Harris;Somesh Jha;Thomas W. Reps	2012		10.1007/978-3-642-31424-7_41	real-time computing;computer science;distributed computing;programming language;algorithm	Logic	-54.01882607888291	53.58477621678699	21316
3998e12975a57031fa6f19e3b3997063b3caad19	design and realization of intersection traffic control simulation system using connected vehicle technology		The connected vehicle technology is a remarkable trend in the field of intelligent transportation system. Since the actual deployment of the connected vehicle system is still lacking hitherto, simulation is widely adopted as the major method of verification in related researches. Although traditional commercial traffic simulation systems perform well in macroscopic simulations, they seem redundant in the circumstance of small-size traffic. In addition, those systems are unable to simulate the communication between vehicles and road-side infrastructures. This study designs a platform for the simulation of intersection traffic control using connected vehicle technology. By providing an abundance of customizable parameters, the platform can simulate at various levels of speed to meet difference requirements, serving as a basis for testing further research. keywords connected vehicle; intersection traffic control; simulation system	connected car;requirement;simulation;software deployment	Weitong Zhang;Shuai Liu;Daoya Yao	2018	CoRR		control theory;software deployment;mathematics;control engineering;intelligent transportation system;traffic simulation	Robotics	-47.14648501483082	49.209494424869405	21325
11b34098ffae173c8786240a80f5b6b46b4295d3	access: describing and contrasting - authentication mechanisms		The password the almost universal authentication solution yet is buckling under the strain. It demonstrates insufficiency and weakness due to poor choice, reuse and ease of sharing Graphical passwords, biometrics, and hardware tokens have been proposed as alternatives. However, industry has not embraced these alternatives. One possible explanation is the complexity of the choice process, i.e. for which situation and which person which alternative is most appropriate. To support authentication decision-markers in this process we suggest a framework called ACCESS (Authentication ChoiCE Support System) which captures situation and user related requirements, consults a knowledge base of existing authentication mechanisms and their properties, and suggests those mechanisms that match the specified requirements.	authentication;biometrics;buckling;convergence insufficiency;graphical user interface;knowledge base;password;requirement;security token	Karen Renaud;Melanie Volkamer;Joseph Maguire	2014		10.1007/978-3-319-07620-1_17	computer science;internet privacy;world wide web;computer security	Security	-45.89608821574552	57.25300319458367	21351
355c9bd1a97780d5a012371de6d142169e25ffd2	can signal injection	computer debugging;controller area networks;vehicles relays delay jitter logic gates safety software;controller area network can signal injection can system monitoring can bus network nodes short term test debugging requirement real time vehicle bus;real time systems computer debugging controller area networks;real time systems	There is a wide range of commercial tools for monitoring CAN systems and simulating test conditions. However, current solutions are not able to selectively modify individual signals on a full, live, CAN bus. This new functionality is potentially useful to developers when network nodes cannot otherwise be influenced or when a short term test, or debugging requirement, does not warrant a firmware change. The CAN Signal Injection (CSI) system is a prototype tool, capable of modifying, in real time, signals on the vehicle bus with minimum disruption. In this paper the design and implementation of the CSI will be described, as well as a successful case study.	can bus;debugging;denial-of-service attack;engine control unit;fail-safe;fault detection and isolation;firmware;microcontroller;prototype;relay;safety engineering;scheduling (computing);simulation;technical standard;user interface;vehicle bus	Maria Loukadaki;Tim Edwards	2012	2012 IEEE International Conference on Vehicular Electronics and Safety (ICVES 2012)	10.1109/ICVES.2012.6294299	embedded system;electronic engineering;real-time computing;engineering;bus network	Embedded	-36.60288423155745	36.79802141503478	21467
60ecae410dcaf47184ab095127d77138141fe808	towards a paradigm for activity modeling	microwave integrated circuits;control systems;formal specification;information systems;history;heterogeneous systems;frequent reconfiguration;system specification;software systems;information systems microwave integrated circuits monitoring intelligent sensors software systems decision making history dispatching user interfaces control systems;monitoring;virtual machines;specification languages;model integrated computing;specification languages virtual machines formal specification configuration management;domain specific language;information system;information system activity modeling paradigm model integrated computing domain specific language modeling environment creator heterogeneous systems frequent reconfiguration system specification monitoring functionality decision making;configuration management;user interfaces;intelligent sensors;dispatching;activity modeling paradigm;monitoring functionality;modeling environment creator	Model Integrated Computing involves defining a domainspecific language that allows for someone to effectively program an environment at whatever level the modelingenvironment-creator deems appropriate. We’ve taken several complex, heterogeneous systems that posed problems of needing integration and requiring frequent reconfiguration at very high levels. This paper discusses gathering the specifications for these systems, how the systems can be represented at these high levels in a paradigm also capturing the specifications, and then how reconfiguring this group can proceed. All of the activities constituting monitoring functionality and the resulting decision making exposed by the information system will be shown to have been solved through utilization of Model Integrated Computing techniques.	client (computing);computer simulation;graphical user interface;information system;instance (computer science);interconnection;programming paradigm;prototype;remote computer;run-time infrastructure (simulation);server (computing);system integration	Jason T. Garrett;Ákos Lédeczi;Frank DeCaria	2000		10.1109/ICSMC.2000.884355	real-time computing;computer science;control system;database;information system	DB	-39.33016659993048	40.75009057926612	21502
16e48101adf45f36ab02ea90c0eec4546328788f	the use of virtualization technology in the dynamic analysis of software code		The problem of software code security analysis has been considered. The significance of using dynamic code analysis methods if the source code is unavailable has been justified. Modern approaches to the problem have been examined. A class of dynamic code analysis methods based on the virtualization technology has been selected. The methodology of using emulators in order to carry out the dynamic software code analysis has been presented.	dynamic program analysis;emulator;static program analysis;x86 virtualization	A. Yu. Chernov;Artem S. Konoplev	2015	Automatic Control and Computer Sciences	10.3103/S0146411615080234	kpi-driven code analysis;development testing;code review;computer science;theoretical computer science;software engineering;dynamic program analysis;linear code sequence and jump;code coverage;legacy code;static program analysis;source code	SE	-62.82197107876231	35.465716200271984	21688
1e34138c955a13b3e8b369aec55e51ab4ba1ac11	using a hypervisor to migrate running operating systems to secure virtual machines	virtual machine;virtual machine monitor;virtual machine monitors kernel security virtual machining registers monitoring;kernel;virtualization;memory management;hypershield;virtual machines data visualisation linux security of data storage management;storage management;security oriented hypervisors;virtual machining;virtual machine monitors;data visualisation;operating system;monitoring;virtual machines;buffer overflow;registers;linux;operating systems security virtual machine monitors hypervisors;hypervisors;security;security of data;memory management operating system virtual machine security oriented hypervisors hypershield linux virtualization;operating systems	We propose HyperShield, which is a hypervisor that can be inserted into and removed from a running operating system, for improving security. While many existing security-oriented hypervisors require modifying or rebooting an overlying operating system, HyperShield does not require this. HyperShield is intended to be a general framework for various security mechanisms. The current implementation provides two mechanisms for preventing kernel-level buffer overflow. One detects the execution of user code with the kernel privilege, and the other detects malicious modification of a return address in a control stack. HyperShield is implemented on Linux as a loadable kernel module. When the module is inserted, it places itself under the operating system and executes as a hypervisor. The operating system is migrated into a virtual machine and managed by the hypervisor. HyperShield detects attacks by combining virtualization of memory management with a hardware-assisted execution-bit feature. We have confirmed through experiments that HyperShield successfully prevented kernel-level buffer overflow attacks.	benchmark (computing);buffer overflow;call stack;code page;compiler;context switch;device driver;experiment;goto;hypervisor;kernel (operating system);kilobyte;linux;loadable kernel module;memory management;operating system;overhead (computing);page (computer memory);page fault;page table;resolution enhancement technology;return statement;virtual machine;visual intercept;x86 virtualization	Tsutomu Nomoto;Yoshihiro Oyama;Hideki Eiraku;Takahiro Shinagawa;Kazuhiko Kato	2010	2010 IEEE 34th Annual Computer Software and Applications Conference	10.1109/COMPSAC.2010.11	embedded system;real-time computing;storage hypervisor;computer science;virtual machine;information security;operating system;database;hypervisor;computer security;data visualization	OS	-55.02101390813482	56.173337232927835	21781
a00bbbb9547fff99295b3c5241cb9c7df37bbbd7	a capability-based transparent cryptographic file system	capability based transparent cryptographic file system;data integrity;storage device theft;heterogeneous computing;modular cryptography;operating systems computers cryptography data integrity file organisation mobile computing open systems;microsoft windows nt xp family;operating system;file system;cryptography;industrial espionage;modular cryptography capability based transparent cryptographic file system mobile internetworked working environments storage device theft industrial espionage heterogeneous computing microsoft windows nt xp family operating systems;mobile internetworked working environments;mobile computing;open systems;operating systems computers;cryptography file systems operating systems control systems protection secure storage data security collaboration internetworking computer graphics;file systems;operating systems;file organisation	Data on the file system in mobile internetworked working environments are exposed data to a number of threats ranging from physical theft of storage devices to industrial espionage and intelligence activities. This paper describes a fully transparent, capability-based file system security mechanism for use in heterogeneous computing environments with emphasis on the implementation on the Microsoft Windows NT/XP family of operating systems. This mechanism can provide confidentiality and integrity protection for on- and off-line use through modular cryptographic means and is interoperable between several operating system platforms	capability-based security;computer security;confidentiality;cryptography;data integrity;filesystem-level encryption;heterogeneous computing;interoperability;microsoft windows;online and offline;operating system;security controls;windows nt	Frank Graf;Stephen D. Wolthusen	2005	2005 International Conference on Cyberworlds (CW'05)	10.1109/CW.2005.1	self-certifying file system;embedded system;industrial espionage;computer science;cryptography;operating system;data integrity;mobile computing;computer security	OS	-48.193367214016526	59.46755272622216	21792
0a1e0ff58e2a02caeb4fa780c9eeb81b5c902fca	ariadnima — android component flow reconstruction and visualization	androids;runtime;flow graphs;receivers;visualization;humanoid robots;context	Android applications are built with a set of different component types that serve specific purposes. Thanks to Android's system design they can interact with each other in a variety of ways, which makes it possible to complete complex tasks. The downside of the flexible interconnectivity of system components is that the relationship between them can be very complex and hard to understand. To give a better understanding of the interconnectivity of components, this work focuses on the reconstruction of relationships between the components of an app. Our approach focusses on real world applications by minimizing the need for complex calculations (e.g., flow analysis) where possible. On the basis of static code analysis component transitions will be reconstructed, and in a second step these transitions are then visualized to allow an analyst to easily understand the relationships of the app's components.	android;call graph;connected component (graph theory);data-flow analysis;dataflow;graph (discrete mathematics);interconnectedness;real life;screenshot;static program analysis;systems design	Dennis Titze;Konrad Weiss;Julian Schütte	2017	2017 IEEE 31st International Conference on Advanced Information Networking and Applications (AINA)	10.1109/AINA.2017.20	simulation;visualization;computer science;humanoid robot;artificial intelligence;theoretical computer science;operating system;database;computer security	SE	-53.86785968041379	36.03570714491359	21793
90921268bdb68c97205e2863ca1692e82f144e52	improved developer support for the detection of cross-browser incompatibilities		Various tools are available to help developers detect cross-browser incompatibilities (XBIs) by testing the documents generated by their code. We propose an approach that enables XBIs to be detected earlier in the development cycle by providing support in the IDE as the code is being written. This has the additional advantage of making it clear to the developers where the sources of the problems are and how to fix them. We present wIDE which is an extension to an IDE designed specifically to support web developers. wIDE uses a compatibility knowledge base to scan the source code for XBIs. The knowledge base is extracted automatically from online resources and periodically updated to ensure that the compatibility information is always up-to-date. In addition, developers can query documentation from within the IDE to access descriptions and usage examples of code statements. We report on a qualitative user study where developers provided positive feedback about the approach, but raised some issues to address in future work.		Alfonso Murolo;Fabian Stutz;Maria Husmann;Moira C. Norrie	2017		10.1007/978-3-319-60131-1_15	web application;world wide web;documentation;source code;knowledge base;computer science;cross-browser	SE	-53.98578910025127	37.95957953163877	21794
49d726cf1b4c64983682e880304ab5c7fd7a7444	a flexible read-write abortion protocol to prevent illegal information flow	flexible read write abortion frwa protocols;information flow control;probability flexible read write abortion protocol illegal information flow prevention information systems illegal access suspicious object read operations frwa protocol;security of data information systems probability;impossible write;meaningless read;illegal write;suspicious read;lost read;flexible read write abortion frwa protocols illegal write suspicious read impossible write meaningless read lost read information flow control;protocols permission access control synchronization upper bound nist conferences	Information systems have to be secure in presence of illegal access. A transaction illegally reads an object if the transaction reads the object which includes data in other objects which is not allowed to be read. A transaction illegally writes an object after illegally reading some object. In addition, we consider suspicious object whose data is not allowed to flow to another object. A transaction suspiciously reads a suspicious object. A transaction impossibly writes an object after reading a suspicious object. Write-abortion (WA) and read-write abortion (RWA) protocols to prevent illegal information flow are already discussed. In the WA protocol, a transaction is aborted once issuing an illegal or impossible write. Reads are meaninglessly performed since the reads are undone due to the abortion of the transaction. In the RWA protocol, a transaction is aborted once issuing an illegal read or impossible write. Here, read operations to be performed after an illegal read are lost since a transaction is aborted just on issuing an illegal read. In this paper, we newly propose a flexible read-write abortion (FRWA) protocol to reduce the number of meaningless and lost reads. Here, a transaction is aborted with some probability if the transaction illegally reads an object. We evaluate the FRWA protocols compared with the WA and RWA protocols. We show the execution time of each transaction in the FRWA protocols is shorter than the WA and more number of reads can be performed in the RWA protocols.	database transaction;information systems;information flow;read-write memory;run time (program lifecycle phase);undo	Shigenari Nakamura;Dilawaer Duolikun;Tomoya Enokido;Makoto Takizawa	2015	2015 IEEE 29th International Conference on Advanced Information Networking and Applications	10.1109/AINA.2015.180	computer science;database;world wide web;computer security	DB	-36.76455676433573	58.842443543469294	21849
1c9d53bc70276f97f60ed316c9ccb61848629b4b	heterogeneity for increasing performance and reliability of self-reconfigurable multi-robot organisms	reconfigurable system;performance evaluation;software design	Homogeneity and heterogeneity represent a wellknown trade-off in the design of modular robot systems. This work addresses the heterogeneity concept, its rationales, design choices and performance evaluation. We introduce challenges for self-reconfigurable systems, show advances of mechatronic and software design of heterogeneous platforms and discuss experiments, which intend to demonstrate usability and performance of this system.	artificial life;bus (computing);clanking replicator;design rationale;docking (molecular);experiment;heterogeneous computing;mathematical optimization;mechatronics;morphing;performance evaluation;reconfigurable computing;requirement;self-reconfiguring modular robot;software design;software framework;swarm;symbrion;usability	Serge Kernbach;Florian Schlachter;Raja Humza;Jens Liedke;Sergej Popesku;Sheila Russo;Tommaso Ranzani;Luigi Manfredi;Cesare Stefanini;Rene Matthias;Christopher S. F. Schwarzer;Benjamin Girault;P. Alschbach;Eugen Meister;Oliver Scholz	2011	CoRR		simulation;computer science;systems engineering;engineering;software design;computer engineering	Robotics	-34.06335465196859	36.351833125878684	21869
11ba8322747a1460e64df0a91e8bbdab53310f37	threshold-free code clone detection for a large-scale heterogeneous java repository	unsupervised learning;type 3 clone detection tools threshold free code clone detection algorithms heterogeneous java repository software ecosystems method granularity dissimilarity threshold clone benchmark open source java projects f measure;large scale repository;clustering;clone detection;public domain software java;threshold free clone detection clone search clustering unsupervised learning large scale repository;cloning benchmark testing clustering algorithms java software systems google optimization methods;clone search;threshold free	Code clones are unavoidable entities in software ecosystems. A variety of clone-detection algorithms are available for finding code clones. For Type-3 clone detection at method granularity (i.e., similar methods with changes in statements), dissimilarity threshold is one of the possible configuration parameters. Existing approaches use a single threshold to detect Type-3 clones across a repository. However, our study shows that to detect Type-3 clones at method granularity on a large-scale heterogeneous repository, multiple thresholds are often required. We find that the performance of clone detection improves if selecting different thresholds for various groups of clones in a heterogeneous repository (i.e., various applications). In this paper, we propose a threshold-free approach to detect Type-3 clones at method granularity across a large number of applications. Our approach uses an unsupervised learning algorithm, i.e., k-means, to determine true and false clones. We use a clone benchmark with 330,840 tagged clones from 24,824 open source Java projects for our study. We observe that our approach improves the performance significantly by 12% in terms of F-measure. Furthermore, our threshold-free approach eliminates the concern of practitioners about possible misconfiguration of Type-3 clone detection tools.	algorithm;benchmark (computing);cluster analysis;duplicate code;entity;java;k-means clustering;open-source software;software ecosystem;software repository;unsupervised learning	Iman Keivanloo;Feng Zhang;Ying Zou	2015	2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)	10.1109/SANER.2015.7081830	computer science;bioinformatics;database;world wide web	SE	-60.901124752730006	39.543058434586264	21927
039f53f390fdddf2028207b4d2c7cb61fcee8ef5	apibot: question answering bot for api documentation		As the carrier of Application Programming Interfaces (APIs) knowledge, API documentation plays a crucial role in how developers learn and use an API. It is also a valuable information resource for answering API-related questions, especially when developers cannot find reliable answers to their questions online/offline. However, finding answers to API-related questions from API documentation might not be easy because one may have to manually go through multiple pages before reaching the relevant page, and then read and understand the information inside the relevant page to figure out the answers. To deal with this challenge, we develop APIBot, a bot that can answer API questions given API documentation as an input. APIBot is built on top of SiriusQA, the QA system from Sirius, a state of the art intelligent personal assistant. To make SiriusQA work well under software engineering scenario, we make several modifications over SiriusQA by injecting domain specific knowledge. We evaluate APIBot on 92 API questions, answers of which are known to be present in Java 8 documentation. Our experiment shows that APIBot can achieve a Hit@5 score of 0.706.	application programming interface;benchmark (computing);documentation;java version history;online and offline;question answering;sirius;software engineering;software quality assurance	Yuan Tian;Ferdian Thung;Abhishek Sharma;David Lo	2017	2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)		application programming interface;documentation;world wide web;knowledge extraction;computer science;probabilistic logic;data mining;software;natural language;question answering;java	SE	-61.04345939905086	39.52938184453108	21991
efc2f40a6c076660be81d8e3832ad28befdcdaa2	seamlessly securing web services by a signing proxy	proxy pattern;http;xml based;soap;web service;web services;security	Web services offer a way for very different systems to collaborate independent of the programming language used or the involved operating systems. Their basis is the XML-based SOAP protocol, which can be used over any protocol that is able to transport a byte stream. Due to the fact that Web services do not depend on any operating system and there is no burden of a underlying paradigm, they are ideal for the integration of even completely inhomogeneous systems. However, SOAP does not (and does not have to) deal with security issues, which is nevertheless important for the involved systems. This article describes an add-on for existing Internet proxies to achieve user and developer transparent security features for Web services. This approach allows corporate firewalls to handle authentication. A first step is to add corporate signatures to all outgoing SOAP messages to enable a corporate trust relationship. A second improvement is to use proxy authentication as defined in RFC 2616 and RFC 2617 to add personal signatures assuming that the proxy has access to some key management system.	add-ons for firefox;antivirus software;authentication;bitstream;byte;firewall (computing);hypertext transfer protocol;key management;operating system;programming language;programming paradigm;proxy server;soap;web service;xml	Mario Jeckle;Ingo Melzer	2004	Int. J. Web Service Res.	10.4018/jwsr.2004070105	web service;web application security;hypertext transfer protocol;computer science;information security;ws-policy;service-oriented architecture;soap;ws-addressing;internet privacy;ws-i basic profile;law;world wide web;computer security;universal description discovery and integration	Security	-49.53349865442234	54.97614663858168	22174
4d34f3b45f5e2faba6b8e12e44b3b4e4aa0c85ad	cod - a dynamic data flow analysis system for cobol	cobol;dynamic data;flow analysis	This paper presents a description of an automated data flow analysis system for Cobol programs — COD. It detects all data flow anomalies as well as certain kinds of errors and has been found to be a very helpful tool for testing and developing Cobol programs.	cobol;data-flow analysis;dataflow;dynamic data	T. Y. Chen;H. Kao;M. S. Luk;W. C. Ying	1987	Information & Management	10.1016/0378-7206(87)90061-9	dynamic data;computer science;software engineering;data-flow analysis;database;cobol;programming language	PL	-57.606391572309924	40.405377533526085	22197
ba20fbdadc934dd987a830253bb20cbec12289da	towards location-based real-time monitoring systems in u-lbs	dynamic load balancing;location based service;mobile device;query processing;energy efficient;real time;real time monitoring;spatial index;monitoring system;location awareness;high performance	Recently, ubiquitous location-based services (u-LBS) has been utilized in a variety of practical and mission-critical applications such as security services, personalization services, location-based entertainment, and location-based commerce. The essence of u-LBS is actually the concept of  location-awareness  where location-aware devices perform more intelligent services for users by utilizing their locations. In order to realize/achieve this concept, a mobile device should continuously monitor the real-time contextual changes of a user; this is what we call  location-based monitoring  . In this paper, we discuss the research and technical issues on designing location-based real-time monitoring systems in u-LBS along with three major subjects: (1) high-performance spatial index, (2) monitoring query processing engine, and (3) distributed monitoring system with dynamic load balancing capability and energy-efficient management.	location-based service;real-time transcription	MoonBae Song;Hyunseung Choo	2009		10.1007/978-3-642-02457-3_45	embedded system;real-time computing;computer science;spatial contextual awareness;operating system;location-based service;mobile device;database;efficient energy use;world wide web;spatial database	Embedded	-40.035732494495655	48.41750618320152	22201
73c1765f70bec641c16f9317c3be59093a843106	framework and service allocation for network robot platform and execution of interdependent services	network robot;heterogeneous robots;platform;service allocation;user interaction;matching method;distributed robotics	This research introduces a framework for network robot platform (NWR-PF) and a service allocation method for heterogeneous distributed robots; they cooperate to perform services that include interdependent tasks. The proposed framework is composed of three layers: connection units, the area management gateway, and the robot-user interaction database. The area management gateway and robot-user interaction database treat the information of the users, robots, services, and service history in a uniform manner, because the connection units hide the differences, such as format and protocol, among the heterogeneous robots. The 4W1H matching method is also proposed for service allocation. The method selects the most suitable robots by comparing the elements of user, robot, and service information. Moreover, it generates robot commands in a common format, by combining the service scenario, and service history. A feature of the method is that both robotic functions to execute service and functions that robots possess are described as independent parameters. After the service trigger is received, both the robot and scenario needed to execute the service are decided by matching these parameters. Experiments show that robots controlled by NWR-PF can perform interdependent services by referring to the service history. © 2008 Elsevier B.V. All rights reserved.		Yukihiro Nakamura;Tamotsu Machino;Manabu Motegi;Yoshiyuki Iwata;Takanori Miyamoto;Satoshi Iwaki;Shin-yo Muto;Ken-ichiro Shimokura	2008	Robotics and Autonomous Systems	10.1016/j.robot.2008.06.008	real-time computing;simulation;computer science;distributed computing;platform	Robotics	-37.91291848451496	44.40016982582027	22210
ba8294f6808a144d18fc1f881c08430b7458101f	model checking at ibm	formal methods;formal method;development tool;formal verification;model checking;hardware design;industrial design	Over the past nine years, the Formal Methods Group at the IBM Haifa Research Laboratory has made steady progress in developing tools and techniques that make the power of model checking accessible to the community of hardware designers and verification engineers, to the point where it has become an integral part of the design cycle of many teams. We discuss our approach to the problem of integrating formal methods into an industrial design cycle, and point out those techniques which we have found to be especially effective in an industrial setting.	algorithm;floor and ceiling functions;formal methods;model checking	Shoham Ben-David;Cindy Eisner;Daniel Geist;Yaron Wolfsthal	2003	Formal Methods in System Design	10.1023/A:1022905120346	model checking;formal methods;formal verification;computer science;formal specification;formal equivalence checking;programming language	EDA	-36.83692744605718	33.04554832626995	22298
fbcabe796e54a0b3d8259a6469230b80d1b8feb6	designing an xml-based context-aware transformation framework for mobile execution environments using cc/pp and xslt	application development;context aware;context information;mobile device;xslt;user interface;separation of concern;user preferences;device independence;cc pp;programming model;execution environment;mobile execution environment;xml;xul;transformation;programming;article;markup language;embedded device	Mobile and embedded devices provide the function of surfing the Internet anytime and anywhere. There are several kinds of mobile execution environments (MExE) built on these appliances, such as WAP, J2ME, PJava, and Microsoft CLI. It is difficult for programmers to write a program only once and then execute it on these mobile devices. The primary reason is there are a variety of devices with different runtime environments and diverse hardware/software capabilities. Therefore, in order to accomplish the following: (1) applications can be designed regardless of what kind of the target mobile device belongs to; (2) the program of an application can be automatically adapted to the target MExE environments. We propose an XML-based Context-Aware transformation Framework (X-CAF). In this framework, we design an XML-based programming model to divide programmers into two roles, user interface (UI) designer and logic programmer, so as to efficiently develop an application in separation-of-concern way. Besides, we exploit the XSLT/XPath transformation mechanism to transform documents of XML User-interface Language (XUL) and LoGic Markup Language (LGML) into others of the target MExE languages by means of the context information, device capabilities and user preferences. Moreover, to generate codes of the applications flexibly and efficiently, we divide the code processing of an application into that of the user interface occurring at runtime and that of the event-handling logic occurring at static time. In brief, our paper contributes an XML-based application development environment and transformation framework to the access to device independence. D 2003 Elsevier B.V. All rights reserved.	anytime algorithm;c++;code;compiler;computation;computational logic;device independence;embedded system;emoticon;event (computing);graphical user interface;integrated development environment;java platform, micro edition;logic programming;markup language;mobile information device profile;mobile device;personaljava;programmer;programming language;programming model;real-time transcription;run time (program lifecycle phase);runtime system;sap composite application framework;separation of concerns;software development kit;standard library;string (computer science);uiml;ubiquitous computing;user (computing);ws-caf;web service;website meta language;xml;xpath;xslt	Tzu-Han Kao;Shyan-Ming Yuan	2004	Computer Standards & Interfaces	10.1016/j.csi.2003.09.003	transformation;programming;xml;xslt;separation of concerns;streaming xml;computer science;operating system;software engineering;xml framework;mobile device;xul;database;markup language;programming paradigm;programming language;user interface;rapid application development;world wide web;computer security	HCI	-35.66757282101565	40.55473544330384	22307
b802d233b781193d6ea8ad6a42f4a0507c9d9622	benefits and challenges for bpm in the cloud		"""Business processes are not only variable, they are dynamic as well. A key benefit of BPM is the ability to adjust processes accordingly in response to changing market requirements. In parallel to BPM, enterprise cloud computing technology has emerged to provide a more cost effective solution to businesses and services while making use of inexpensive computing solutions, which combines pervasive, internet, and virtualization technologies. Despite the slow start the business benefits of cloud computing are as such that the transition of BPM to the cloud is now underway. Cloud services refer to the operation of a virtualized, automated, and service-oriented IT landscape that allows the flexible provision and usage-based invoicing of resources, services, and applications via a network or the Internet. The generic term """"X-as-a-Service"""" summarized the business models delivering almost everything as a service. BPM in the cloud is often regarded as a SaaS application. More recently, BPM is being regarded as a PaaS as it facilitates the creation and deployment of applications, in this case business process solutions. The PaaS landscape is the least developed of the four cloud based software delivery models previously discussed. PaaS vendors, such as IBM, Oracle, Microsoft delivered an application platform with managed cloud infrastructure services however more recently the PaaS market has begun to evolve to include other middleware capabilities including process management. BPM PaaS is the delivery of BPM technology as a service via a cloud service provider. In order to be classified as a PaaS a BPM suite requires the following capabilities: the architecture should be multi-tenant, it should be hosted off premise and it should offer elasticity and metering by use capabilities. When we refer to BPM in the cloud what we are really referring to is a combination of BPM PaaS and BPaaS Business Process as a Service. Business Process as a Service BPaaS is a set of pre-defined business processes that allows the execution of customized business processes in the cloud. BPaaS is a complete pre-integrated BPM platform hosted in the cloud and delivered as a service, for the development and execution of general-purpose business process application. Although such a service harbors an economic potential, questions that need to be answered are as follows: Can an individual and company-specific business process supported by a standardized cloud solution, or should we protect process creativity and competitive differentiation by allowing the company to design the processes individually and solely support basic data flows and structures? Does it make sense to take a software solution """"out of the box"""" that handles both data and process in a cloud environment, or would this hinder the creativity of business process development leading to a lower quality of processes and consequently to a decrease in the competitive positioning of a company? How to manage the inherent compliance and security topic. Within a completely integrated business application system, all required security aspects can be implemented as safeguards with just enough money. Within the cloud, however, advanced standards and identity prove is required to monitor and measure information exchange across the federation. Thereby there seems to be no need for developing new protocols, but a standardized way to collect and evaluate the collected information."""	beam propagation method;cloud computing	Ute Riemann	2015	IJOCI	10.4018/IJOCI.2015010103	simulation;business process management;operating system;data mining;database;management;world wide web;computer security;business process modeling	Metrics	-46.48472942546722	44.684893787101146	22322
05c6cff663410c9c92ac8bb266de34ffbb2b1bcc	a service-oriented approach towards context-aware mobile learning management systems	context aware computing;internet access;context aware;universities;context information;management system;mobile device;learning;elearning;user situation context;service orientation;prototypes;computer aided instruction;mobile computer;germany;symposia;learning management system;software architecture;universal service;mobile learning;internet;acceleration sensor;software architecture computer aided instruction mobile computing;learn management system context aware computing service oriented architecture mobile computing elearning;mobile learning management systems;mobile communication;service oriented architecture;mobile computing;learn management system;context aware services least squares approximation web and internet services costs availability displays acceleration sensor phenomena and characterization cameras mobile computing;mobile internet;university computing infrastructure;high speed;university computing infrastructure service oriented architecture context aware mobile system mobile learning management systems position sensor acceleration sensor camera user situation context;infrastructure;students;camera;position sensor;context aware mobile system	Thanks to the public and low cost availability of wireless high speed internet access students are increasingly equipped with mobile internet enabled devices to connect to university services like Learning Management System (LMS). But the applications services like LMS are still unable to adapt themselves to modern mobile devices with restrictions like reduced display size. By recognizing the device and its restrictions it is possible to optimize the LMS interface. Additionally by using device features like position, acceleration sensors, or the camera it is possible to detect the intentions of the user. The context of the user's situation determines which university services are helpful and interesting. Gathering these context information and reasoning on them is the foundation of our context- and service-oriented approach towards a mobile LMS. This enables us to personalize the mobile learning experience with location-sensitive lecture streaming, campus navigation, and ubiquitous features of the whole university computing infrastructure. By using a service-oriented architecture we are able to compose a variety of different university and external services towards a pervasive university.	bluetooth;display size;graphical user interface;internet access;management system;message-oriented middleware;microphone;mobile device;open-source software;personalization;pervasive informatics;scalability;sensor;service-orientation;service-oriented architecture;service-oriented device architecture;streaming media;usability;window blind	Philipp Lehsten;Raphael Zender;Ulrike Lucke;Djamshid Tavangarian	2010	2010 8th IEEE International Conference on Pervasive Computing and Communications Workshops (PERCOM Workshops)	10.1109/PERCOMW.2010.5470656	embedded system;software architecture;the internet;simulation;mobile telephony;internet access;human–computer interaction;computer science;position sensor;operating system;service-oriented architecture;e-learning;mobile device;management system;prototype;multimedia;mobile computing;world wide web	Mobile	-39.07132718209492	50.322115035656445	22334
940fcf48d58cc2fb254d7dfadcd60cde661e14da	the sis project: a distributed platform for the integration of telecommunication management systems	minas gerais;management system;network elements interface sis project distributed platform telecommunication management systems integration system for the integration of supervision telecommunication network management itu t network management standard tmn system design data architecture software architecture functional architecture hardware architecture data distribution man machine interface brazil federal university of minas gerais ufmg minas gerais state telecommunications company telemig;distributed processing;telecommunication computing;maintenance engineering;software engineering;data distribution;network interfaces;telecommunication standards;project management control systems switches telecommunication switching telecommunication control computer network management telecommunication network management computer networks telecommunication computing remote monitoring;user interfaces maintenance engineering telecommunication network management distributed processing telecommunication standards software engineering telecommunication computing network interfaces;man machine interface;network management;user interfaces;telecommunication networks;telecommunication network management	This paper presents SIS (System for the Integration of Supervision), a system that has established a new paradigm for the management of telecommunication networks. The SIS approach is based on the gradual integration of the existing management systems-which are often proprietary-to a reference integrating platform, while concurrently pursuing the conformance of the platform to the ITU-T network management standard, that is, TMN. The paper presents, besides the platform itself, the methodology adopted in the design of the whole system. It includes the statement of the original problem, requirements, functionalities, and restrictions. The functional, hardware, software and data architectures are also presented, as well as the data distribution schemes and the man-machine interface. The problems found and the solutions adopted throughout the development and deployment phases are also summarized. The platform has been developed in Brazil by the Federal University of Minas Gerais (UFMG) jointly and funded by the Minas Gerais State Telecommunications Company (TELEMIG). Today the system is totally deployed, distributed over a large area, collecting data from thousands of points and executing actions upon hundreds of devices.		José Marcos S. Nogueira;Dilmar Malheiros Meira	1996		10.1109/NOMS.1996.539457	human–machine interface;maintenance engineering;network management;computer science;network interface;operating system;management system;user interface;computer security;computer network	DB	-33.867548718820174	40.85007751171649	22340
07ccb3580378e271a9706a8aac91218aa6871745	user-relative names for globally connected personal devices	bob;cluster computing;smart phone;digital camera;internet architecture;operating system;peer to peer;local area network	Personal devices such as mobile phones, digital music players, personal digital assistants, console gaming systems, and digital cameras are now ubiquitous in the lives of ordinary people. As these devices proliferate, peer-to-peer connectivity between them is increasingly important. For example, a user may copy photos from a camera to a PC for storage, to a web page for publishing, or to a photo iPod to take on the road, and perhaps from there to a friend’s iPod. One current transfer mechanism—plugging devices together via USB cable—is both straightforward and secure: the cable itself physically indicates which devices should participate in the transfer, and the isolated physical medium guarantees its security. As personal devices begin to support wireless networking and Internet connectivity, we would like to extend the simplicity and security of a USB cable to device connectivity on a global scale. Alice should be able to connect her WiFi-enabled iPod to her home PC via a “virtual cable,” so that she can browse photos or play music stored there from a WiFi-enabled coffee shop or friend’s house. Setting up this “virtual cable” should not require technical knowledge or special configuration on Alice’s part, and it should continue working even when the devices it connects are behind firewalls or NATs. If Alice meets Bob in a coffee shop, she should easily be able to share with him information or services located on any of her personal devices. Bob should be able to connect to Alice’s devices even after he leaves the coffee shop, until she chooses to sever their relationship. No one else should be able to impersonate Bob, however, in order to gain access to Alice’s shared resources. The User Information Architecture , or UIA, is a peerto-peer connectivity architecture that provides users a simple, intuitive, and secure way to share information and services between personal devices by assigning ad hoc names that act like “virtual cables.” Users assign names by “introducing” devices to each other on a common network. Unlike the ephemeral names used in rendezvous schemes such as Apple Bonjour [1], however, UIA names persist and remain securely bound to the global cryptographic identities of their targets [11,12,1 6] as devices migrate. Once Alice introduces her iPod to her home PC, her iPod can continue accessing her PC by the same name from anywhere she finds Internet access. In a network of billions of users, globally unique names would inevitably have to look something like ipod.alicesm5186.myisp.com, substantially limiting their conciseness and readability. UIA names are insteaduser-relative: users control their own private namespaces much as they control their mobile phones’ address books today. Unlike a conventional address book, however, a UIA namespace is shared across all the devices a user owns: changes made on one device automatically propagate to the others. Users assign user-relative UIA names not only to their own devices but also to other users. Bob might know Alice as “Alice”, her company directory might list her as “Alice Smith, Marketing”, and her son might simply name her “Mom”. If Alice gives Bob access to some files on her PC, he accesses them via a name analogous to “Alice’s PC”. In this way, UIA adapts peer-to-peer social networking ideas previously explored for other purposes [3,10,15] to form a secure peer-to-peer naming infrastructure. The next section presents the goals of UIA’s naming system, and Section 3 describes its operation from a non-technical user’s viewpoint. Section 4 develops the technical details of UIA’s design, and Section 5 summarizes implementation status. Section 6 presents related work, and Section 7 concludes.	alice and bob;book;browsing;cryptography;digital camera;directory (computing);distributed social network;firewall (computing);hoc (programming language);information architecture;internet access;microsoft ui automation;mobile phone;network address translation;peer-to-peer;personal computer;personal digital assistant;usb;web page;ipod;mdnsbrowser	Bryan Ford;Jacob Strauss;Chris Lesniewski-Laas;Sean C. Rhea;M. Frans Kaashoek;Robert Tappan Morris	2006	CoRR		local area network;telecommunications;computer cluster;computer science;operating system;internet privacy;world wide web;computer security;computer network	HCI	-38.893124260532915	55.27559070822701	22379
6bf09fec0557a3d95f874c841e830a1b9a4d3346	security and privacy in cloud computing	trust;availability cloud computing security privacy trust confidentiality integrity accountability;outsourcing;availability;network security;threat models;trust management;biological system modeling;defense strategy;privacy preservability;security issues;confidentiality;privacy issues;computational modeling;integrity;data privacy;security;network security cloud computing privacy trust management computational modeling biological system modeling outsourcing;accountability;privacy;cloud computing	Recent advances have given rise to the popularity and success of cloud computing. However, when outsourcing the data and business application to a third party causes the security and privacy issues to become a critical concern. Throughout the study at hand, the authors obtain a common goal to provide a comprehensive review of the existing security and privacy issues in cloud environments. We have identified five most representative security and privacy attributes (i.e., confidentiality, integrity, availability, accountability, and privacy-preservability). Beginning with these attributes, we present the relationships among them, the vulnerabilities that may be exploited by attackers, the threat models, as well as existing defense strategies in a cloud scenario. Future research directions are previously determined for each attribute.	business software;cloud computing;confidentiality;data integrity;outsourcing;privacy;threat model	Zhifeng Xiao;Yang Xiao	2013	IEEE Communications Surveys & Tutorials	10.1109/SURV.2012.060912.00182	cloud computing security;availability;confidentiality;cloud computing;information privacy;privacy by design;computer science;information security;network security;internet privacy;threat model;trustworthy computing;privacy;computational model;world wide web;computer security;outsourcing	Security	-48.911931919184035	58.26272637692038	22426
50140f1a100c9a35278b5ac15f18a686d0e4f528	test case prioritization using relevant slices	software testing;resource constraint;software fault tolerance;test case prioritization;program testing;software fault tolerance program testing;fault detection;software development;size of test;fault detection test case prioritization software testing software development lifecycle;early detection;software development lifecycle;software testing fault detection computer science programming life testing computer errors time factors	Software testing and retesting occurs continuously during the software development lifecycle to detect errors as early as possible. The sizes of test suites grow as software evolves. Due to resource constraints, it is important to prioritize the execution of test cases so as to increase chances of early detection of faults. Prior techniques for test case prioritization are based on the total number of coverage requirements exercised by the test cases. In this paper, we present a new approach to prioritize test cases based on the coverage requirements present in the relevant slices of the outputs of test cases. We present experimental results comparing the effectiveness of our prioritization approach with that of existing techniques that only account for total requirement coverage, in terms of ability to achieve high rate of fault detection. Our results present interesting insights into the effectiveness of using relevant slices for test case prioritization	code coverage;computation;experiment;fault detection and isolation;requirement;software development process;software testing;test case;test suite	Dennis Jeffrey;Neelam Gupta	2006	30th Annual International Computer Software and Applications Conference (COMPSAC'06)	10.1109/COMPSAC.2006.80	non-regression testing;test strategy;keyword-driven testing;reliability engineering;development testing;verification and validation;regression testing;test data generation;real-time computing;requirement prioritization;fault coverage;white-box testing;manual testing;computer science;systems engineering;engineering;software reliability testing;software development;software engineering;software construction;test suite;smoke testing;software testing;test case;software development process;test management approach;fault detection and isolation;software fault tolerance;test harness	SE	-61.14710626131112	34.65476524054926	22454
ab7e4789b6c9a44089a5ad53b2e7c2e874391248	a practical chinese wall security model in cloud computing	cloud resources practical chinese wall security model cloud computing virtualization technology on demand scalability intervm attacks centralized control mechanism graph coloring algorithm;virtualization;color;cloud;virtual machine monitors;servers;virtual machines;color security virtual machine monitors hardware centralized control servers cloud computing;cloud chinese wall security policy virtualization;chinese wall security policy;centralized control;security;security of data;virtualisation;graph colouring;cloud computing;hardware;virtualisation cloud computing graph colouring security of data virtual machines	Virtualization technology is widely adopted in clouds to meet the requirements of rapid provision and on-demand scalability in cloud computing. Although virtualization improves the usage of hardware devices and flexibility, it brings new security challenges. Users face a new type of attacks, called inter-VM attack, which targets at the VMs running on the same physical machine. To eliminate the possible inter-VM attacks from competitors, we propose a centralized control mechanism based on the Chinese Wall security policy to forbid deploying and running the competitors' VMs on the same physical machines so that physical isolation is achieved. We build the Chinese Wall Central Management System (CWCMS) with the proposed centralized control mechanism in an internal-built experimental cloud. CWCMS effectively manages the VMs and enforce the Chinese Wall security policy in the cloud. Furthermore, CWCMS employs the graph coloring algorithm to achieve the better utilization of cloud resources.	algorithm;centralized computing;chinese wall;cloud computing;graph coloring;hardware virtualization;management system;requirement;scalability	Tien-Hao Tsai;Yen-Chung Chen;Hsiu-Chuan Huang;Pei-Ming Huang;Kuo-Sen Chou	2011	2011 13th Asia-Pacific Network Operations and Management Symposium	10.1109/APNOMS.2011.6076992	cloud computing security;virtualization;cloud computing;computer science;information security;operating system;distributed computing;world wide web;computer security;computer network	Security	-50.81167236897587	57.592174457501216	22533
131252d608cd31a1f639787dcf7eb6e8bb13302a	conflicts among the pillars of information assurance	information assurance data security nonrepudiation data authentication data integrity data confidentiality data availability graphical forms;quality assurance;data integrity;information technology;authentication;information assurance;us department of defense;information processing;information technology information assurance security;authorization;security of data data integrity;security;security of data;asynchronous transfer mode;information processing authorization authentication us department of defense asynchronous transfer mode quality assurance	Interactions between the five pillars of information assurance-availability, integrity, authentication, confidentiality, and nonrepudiation-can be problematic. Measures taken to further the goal of one pillar are often blind to the needs of another pillar. The author explores such interactions using graphical forms to better represent conflicts. For example, availability might introduce conflicts with confidentiality, integrity and authentication, but confidentiality and integrity are largely complementary. This article is part of a special issue on security.	authentication;confidentiality;graphical user interface;information assurance;interaction;non-repudiation	Kelce S. Wilson	2013	IT Professional	10.1109/MITP.2012.24	software security assurance;certified information security manager;quality assurance;information processing;asset;computer science;threat;information security;asynchronous transfer mode;data integrity;authentication;database;authorization;internet privacy;information technology;computer security;information security management	HCI	-46.750798540342906	57.2749793565815	22585
42583ed7b9c08b598a6026ed2fc6a1d7ae46a002	smu: towards cloud oriented service mashup	social computing;mashups;web 2 0 technology;cloud computing web service mashup;service provider;internet resources;web services internet;service providers;web service;data mining;internet;personal service applications;web services;cloud oriented service mashup system prototype;mashup;mashups web and internet services web services cloud computing costs middleware grid computing computer science social network services prototypes;personal service applications cloud computing social computing web 2 0 technology internet resources service providers cloud oriented service mashup system prototype;smu;meteorology;application programming interface;heterogeneous network;cloud computing	Based on increasing popularity of cloud computing, social computing and web 2.0 technology, Internet resources are extremely increasing. How to provide service invoking interfaces ceaselessly while minimizing the cost of service development, which can meet the growing needs of end users, becomes a challenging issue for service providers. Meanwhile, service mashup technology is getting more attention in both enterprise and academia for building new end users applications fast in the complex and heterogeneous network environment. Therefore, we propose a new method of service mashup with the advantages of the cloud, grid, web services and other technologies. We develop cloud oriented Service MashUp system prototype (SMU). In SMU, we support the service information interaction, classification, and process during the procedure of service mashup to meet various needs of the multi-level and the multi-role of service applications. Our experiments show that SMU can reach the purpose of building personal service applications quickly and easily.	cloud computing;experiment;mashup (web application hybrid);prototype;service-oriented architecture;social computing;system management unit;web 2.0;web service	Xia Xie;Ke Fan;Xuanhua Shi;Song Wu;Hai Jin	2009	2009 Fifth International Conference on Semantics, Knowledge and Grid	10.1109/SKG.2009.71	service provider;web service;service level requirement;service catalog;differentiated service;computer science;service delivery framework;service design;database;internet privacy;law;world wide web;social computing;mashup	HPC	-35.930426337739576	48.94388852295512	22600
12b9199c3ebeb561d325fa7a6ad2d280649a92bb	rank-directed layout of uml class diagrams	class diagram;automatic layout;pagerank;forced directed	UML class diagram layout is an important task in software visualization to enhance people's comprehension about the systems. In this paper, we describe a novel UML class diagram layout algorithm, called rank-directed method, which captures the difference in relationships among classes and stresses significant classes. As a layout algorithm, rank-directed method supports the clustering of classes according to the inherent characteristics of classes. To recognize the significance of classes, we applied PageRank algorithms through abstracting relationships among different classes as the link among web pages. We assume that important classes have more relationships with other classes. To emphasize the important classes, rank-directed method adopts a sub graph layout method based on clustering of classes. We have developed a UML class diagram layout platform to evaluate our method. Our evaluation shows that rank-directed method could effectively recognize the important classes and layout the class diagram with higher readability than traditional layout methods do.	algorithm;class diagram;cluster analysis;force-directed graph drawing;pagerank;software visualization;unified modeling language;web page	Hao Hu;Jun Fang;Zhengcai Lu;Fengfei Zhao;Zheng Qin	2012		10.1145/2384416.2384420	computer science;theoretical computer science;class diagram;database;engineering drawing	SE	-55.900643893620526	34.36332316955457	22746
690104f3403ea61e27a5315a6c4fe840edee4e2e	an empirical investigation of software fault distribution	control systems;computer languages;system software product;program modules;programming language;software measurement;system software;software systems;size measurement;computer industry;software reliability software quality;fault rates;programming languages software fault distribution system software product program modules fault rates changed lines;system software computer industry software measurement computer languages size measurement research and development programming fault diagnosis software systems control systems;lines of code;research and development;changed lines;qualitative reasoning;software fault distribution;software reliability;programming;software quality;programming languages;fault diagnosis	This paper investigates the distribution of faults within three evolutionary versions or releases of a system software product. A greater concentration offaults was found in certain software program modules as compared to other modules. The fault rates are correlated to the percentage of new and changed lines of code, the module size, and the programming languages used. Additionally, a number of purely qualitative reasons for the observed concentration of faults are asserted.	bsd;computer program;programming language;source lines of code	Karl Heinz Möller;Daniel J. Paulish	1993		10.1109/METRIC.1993.263798	reliability engineering;n-version programming;computer science;engineering;control system;software engineering;programming language;software quality;computer engineering	SE	-61.76728922918791	32.64947210090185	22797
d687ca8bbb51772198c105bcbf069a6317841c41	evaluating the fittest automated testing tools: an industrial case study	protocols;electronic mail;resource allocation program testing;resource allocation;software testing fittest automated testing tools fittest eu project ibm system management product resource management networked environment ibm research testing practices tsfittest;size measurement;testing;servers;program testing;testing concrete servers protocols size measurement electronic mail;concrete	This paper aims at evaluating a set of automated tools of the FITTEST EU project within an industrial case study. The case study was conducted at the IBM Research lab in Haifa, by a team responsible for building the testing environment for future development versions of an IBM system management product. The main function of that product is resource management in a networked environment. This case study has investigated whether current IBM Research testing practices could be improved or complemented by using some of the automated testing tools that were developed within the FITTEST EU project. Although the existing Test Suite from IBM Research (TSibm) that was selected for comparison is substantially smaller than the Test Suite generated by FITTEST (TSfittest), the effectiveness of TSfittest, measured by the injected faults coverage is significantly higher (50% vs 70%). With respect to efficiency, by normalizing the execution times, we found the TSfittest runs faster (9.18 vs. 6.99). This is due to the fact that the TSfittest includes shorter tests. Within IBM Research and for the testing of the target product in the simulated environment: the FITTEST tools can increase the effectiveness of the current practice and the test cases automatically generated by the FITTEST tools can help in more efficient identification of the source of the identified faults. Moreover, the FITTEST tools have shown the ability to automate testing within a real industry case.	entry point;ibm research;systems management;test automation;test case;test suite;virtual reality	Duy Cu Nguyen;Bilha Mendelson;Daniel Citron;Onn Shehory;Tanja E. J. Vos;Nelly Condori-Fernández	2013	2013 ACM / IEEE International Symposium on Empirical Software Engineering and Measurement	10.1109/ESEM.2013.61	non-regression testing;test strategy;communications protocol;simulation;concrete;resource allocation;computer science;systems engineering;engineering;operations management;software engineering;software testing;management;server	SE	-61.411595364813614	35.33101345360713	22831
9947315e69160e839d6d5a36be174ffde26884b2	to b or not to b: blessing os commands with software dna shotgun sequencing	security of data biology computing dna operating systems computers;command injection;taint inference security injection command injection taint tracking;injection;dna sequential analysis operating systems servers security computer architecture;taint tracking;taint inference;operating system blessing os commands software dna shotgun sequencing biologically inspired approach combat os injection attacks software error taint tracking techniques binary programs;security	We introduce Software DNA Shotgun Sequencing (S3), a novel, biologically-inspired approach to combat OS Injection Attacks, the #2 most dangerous software error as identified by MITRE. To thwart such attacks, researchers have advocated various forms of taint-tracking techniques. Despite promising results, e.g., few missed attacks and few false alarms, taint-tracking has not seen widespread adoption. Impediments to adoption include high overhead and difficulty of deployment. S3 is based on a novel technique: positive taint inference which dynamically reassembles string fragments from a binary to infer blessed, i.e. trusted, parts of an OS command. S3 incurs negligible performance overhead and is easy to deploy as it operates directly on binary programs.	operating system;overhead (computing);software bug;software deployment;taint checking	Anh Nguyen-Tuong;Jason Hiser;Michele Co;Nathan Kennedy;David Melski;William Ella;David Hyde;Jack W. Davidson;John C. Knight	2014	2014 Tenth European Dependable Computing Conference	10.1109/EDCC.2014.13	code injection;taint checking;real-time computing;computer science;engineering;information security;operating system;distributed computing;world wide web;computer security	Security	-56.78648545290213	57.19018185645631	22834
fba1c8f64aaabba3ad3628cfa606cff89c66f522	extensible co-simulation framework for electric vehicle charging infrastructure testing	protocols;vectors electric vehicles;ocpp extensible cosimulation framework electric vehicle charging infrastructure testing european wide interoperability function level interoperability testing single charging stations heterogeneous compounds real charging infrastructure hardware integration test vectors communication power interfaces tool set distributed charging management algorithm open charge point protocol;testing;computer architecture;protocols charging stations computer architecture data models electric vehicles testing adaptation models;charging stations;electric vehicles;adaptation models;data models	With more and more manufacturers positioning their electric vehicles on the market, the necessity for ensuring European-wide interoperability of charging infrastructure and electric vehicles becomes eminent. While standards are in place to guarantee compatibility, testing of function-level interoperability is still an open issue. Besides already widely performed testing of single charging stations, the evaluation of a heterogeneous compounds of charging infrastructure and thereby caused mutual influence is highly relevant. To allow for such challenging, big scale evaluation that also involves real charging infrastructure, co-simulation with hardware integration is the way to go. This paper deals with an appropriate architecture and setup of such a co-simulation framework that can be used to generate test vectors on communication and power interfaces to test charging stations and electric vehicle charging interfaces. The effectiveness of the tool-set is validated with a case study demonstrating a distributed charging management algorithm operating on charging stations connected via the Open Charge Point Protocol (OCPP).	algorithm;co-simulation;conformance testing;extended validation certificate;function-level programming;hardware-in-the-loop simulation;interoperability;message-oriented middleware;microsoft outlook for mac;protocol converter;relevance;routing;test automation;unit testing;way to go	Mario Faschang;Martin Nohrer;Johannes Stockl;Friederich Kupzog	2014	2014 IEEE International Conference on Smart Grid Communications (SmartGridComm)	10.1109/SmartGridComm.2014.7007643	embedded system;real-time computing;simulation;online charging system;engineering	Robotics	-49.71308599354492	45.289446742805254	22904
fd412b89699d845d9718d3bcd87b61df7880c3a0	reducing the window of opportunity for android malware gotta catch ’em all	malicious sample;heuristics engine;techniques mobile malware author;malicious mobile applications result;android malware gotta catch;efficient market crawler;different weight;different nature;android malware;fact android application;different flag	Spotting malicious samples in the wild has always been difficult, and Android malware is no exception. Actually, the fact Android applications are (usually) not directly accessible from market places hardens the task even more. For instance, Google enforces its own communication protocol to browse and download applications from its market. Thus, an efficient market crawler must reverse and implement this protocol, issue appropriate search requests and take necessary steps so as not to be banned. From end-users’ side, having difficulties spotting malicious mobile applications results in most Android malware remaining unnoticed up to 3 months before a security researcher finally stumbles on it. To reduce this window of opportunity, this paper presents a heuristics engine that statically pre-processes and prioritizes samples. The engine uses 39 different flags of different nature such as Java API calls, presence of embedded executables, code size, URLs… Each flag is assigned a different weight, based on statistics we computed from the techniques mobile malware authors most commonly use in their code. The engine outputs a risk score which highlights samples which are the most likely to be malicious. The engine has been tested over a set of clean applications and malicious ones. The results show a strong difference in the average risk score for both sets and in its distribution, proving its use to spot malware.	android;application programming interface;browsing;communications protocol;darknet market;download;droidkungfu;embedded system;executable;heuristic (computer science);java class library;mobile app;mobile malware;performance tuning;perl;preprocessor;protocol buffers;prototype;requirement prioritization;selectivity (electronic);static program analysis;web crawler;window of opportunity	Axelle Apvrille;Tim Strazzere	2012	Journal in Computer Virology	10.1007/s11416-012-0162-3	operating system;cryptovirology;internet privacy;world wide web;computer security	Security	-56.79018609195218	59.529634966853266	22943
01cfec4ca6637fb90cec8afee2a2694aebb83a61	efficient context-sensitive intrusion detection	state machine;static analysis;programming model;intrusion detection	Model-based intrusion detection compares a process’s execution against a program model to detect intrusion attempts. Models constructed from static program analysis have historically traded precision for efficiency. We address this problem with our Dyck model, the first efficient statically-constructed context-sensitive model. This model specifies both the correct sequences of system calls that a program can generate and the stack changes occurring at function call sites. Experiments demonstrate that the Dyck model is an order of magnitude more precise than a context-insensitive finite state machine model. With null call squelching, a dynamic technique to bound cost, the Dyck model operates in time similar to the contextinsensitive model. We also present two static analysis techniques designed to counter mimicry and evasion attacks. Our branch analysis identifies between 32% and 64% of our test programs’ system call sites as affecting control flow via their return values. Interprocedural argument capture of general values recovers 32% to 69% more arguments than previously reported techniques.	context-sensitive grammar;control flow;dyck language;evasion (network security);finite-state machine;intrusion detection system;static program analysis;system call	Jonathon T. Giffin;Somesh Jha;Barton P. Miller	2004			real-time computing;computer science;finite-state machine;computer security;programming paradigm;distributed computing;intrusion detection system;anomaly-based intrusion detection system	Security	-56.6930125357369	55.19709553249602	22991
e9b0943b7fbbcd4a7ecca8450fbaf5a5ff969269	a framework for user control over media data based on a trusted point	trusted computing authorisation client server systems data privacy public key cryptography;media privacy process control cloud computing cryptography conferences;media;cryptography;process control;user intervention minimization user control functionalities trusted point media services video data processing video data exchange security level privacy level media data processing media data sharing;privacy;conferences;cloud computing	The rapid evolution of media services has having a strong impact on the way users process and share their data. At the same time, users are becoming every day more aware of the risks that might arise while exchanging or processing video data through the net. The need for frameworks enabling processing and sharing of the content under user control is becoming a very important issue since adequate level of security and privacy must be provided to the final users. After reviewing the state of the art in the area of media processing and sharing under user control, this paper presents a framework aiming at ensuring user control over his data while minimizing the user intervention needed for enabling the control functionalities.	privacy;user interface	Cristian Perra	2015	2015 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2015.7066294	media;cloud computing;computer science;cryptography;operating system;process control;internet privacy;privacy;world wide web;computer security	Robotics	-43.260449196666016	59.52426046278543	22997
a2641db8b67094a542da8a94a53d908e58ae742c	automatic detection of instability architectural smells	databases;software;detectors;measurement;engines;filtering algorithms;java	Code smells represent well known symptoms of problems at code level, and architectural smells can be seen as their counterpart at architecture level. If identified in a system, they are usually considered more critical than code smells, for their effect on maintainability issues. In this paper, we introduce a tool for the detection of architectural smells that could have an impact on the stability of a system. The detection techniques are based on the analysis of dependency graphs extracted from compiled Java projects and stored in a graph database. The results combine the information gathered from dependency and instability metrics to identify flaws hidden in the software architecture. We also propose some filters trying to avoid possible false positives.	algorithm;application programming interface;code smell;cognitive dimensions of notations;compiler;graph (discrete mathematics);graph database;instability;java;open-source software;sensor;software architecture;technical debt	Francesca Arcelli Fontana;Ilaria Pigazzini;Riccardo Roveda;Marco Zanoni	2016	2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)	10.1109/ICSME.2016.33	detector;real-time computing;computer science;theoretical computer science;programming language;java;world wide web;code smell;measurement	SE	-57.31837757852375	37.07933471553771	23053
a95a3d54a039b14ef074804d38f53e039e65edd7	cyber–physical systems: a perspective at the centennial	computers;cyberspace;control systems;cybernetics;protocols;wireless sensor networks cyber physical systems cpss hybrid systems model driven development networked control systems real time systems security verification and validation;control technology;networked control systems;sensors;cyber physical systems cpss;medical systems;cyber physical systems;model driven development;next generation engineered systems;formal verification;transportation cyber physical systems next generation engineered systems control technology medical systems networked control system real time networking real time computing wireless sensor networks model driven development;transportation;stability analysis;stability analysis control systems computers wireless sensor networks sensors protocols delay;networked control system;real time networking;verification and validation;security;wireless sensor networks;real time computing;hybrid systems;real time systems	Cyber-physical systems (CPSs) are the next generation of engineered systems in which computing, communication, and control technologies are tightly integrated. Research on CPSs is fundamentally important for engineered systems in many important application domains such as transportation, energy, and medical systems. We overview CPS research from both a historical point of view in terms of technologies developed for early generations of control systems, as well as recent results on CPSs in many relevant research domains such as networked control, hybrid systems, real-time computing, real-time networking, wireless sensor networks, security, and model-driven development. We outline the potential for CPSs in many societally important application domains.	application domain;control system;cyber-physical system;hybrid system;mit engineering systems division;model-driven engineering;model-driven security;next-generation network;point of view (computer hardware company);real-time computing;real-time locating system	Kyoung-Dae Kim;Panganamala Ramana Kumar	2012	Proceedings of the IEEE	10.1109/JPROC.2012.2189792	control engineering;communications protocol;transport;von neumann stability analysis;real-time computing;verification and validation;wireless sensor network;cybernetics;formal verification;computer science;networked control system;engineering;sensor;control theory;cyber-physical system;hybrid system;computer engineering	Embedded	-45.410280118988965	37.10077410512251	23079
5f08dd1ae43753a39391ea4096ad5b15594e9e43	semantic location based services for smart spaces	location based service;pervasive computing;context aware service;computer experiment;spatial representation;semantic web;smart spaces;physical environment;knowledge base	Enhancing the physical environment of users with IT and communication elements is one of the main objectives of the pervasive computing paradigm. The so-called “smart spaces”, which are typical pervasive computing environments, combine computing infrastructure with intelligent and context-aware services in order to advance the users’ computing experience. In this paper we describe the basic metadata-related infrastructure that is required for delivering semantics-aware location-based services in smart spaces. This infrastructure involves geometric and ontological spatial representation as well as graphand knowledge-based navigation algorithms.	algorithm;business rules engine;context-aware network;location-based service;negation as failure;ontology (information science);overhead (computing);programming paradigm;real-time clock;real-time computing;semantic web rule language;semantic reasoner;spaces;ubiquitous computing;vocabulary;web ontology language	Kostas Kolomvatsos;Vassilis Papataxiarhis;Vassileios Tsetsos	2007		10.1007/978-0-387-77745-0_51	knowledge base;semantic computing;computer experiment;computer science;knowledge management;artificial intelligence;semantic web;location-based service;semantic web stack;database;world wide web	HCI	-41.450880278077086	45.00222504433948	23081
a1785dad98135a2abd9415428319c9abb8ba5462	machine learning for finding bugs: an initial report	complexity theory;neural networks;training;080109 pattern recognition and data mining;machine learning;feature extraction;source code;static analysis;computer bugs;benchmark testing;data models	Static program analysis is a technique to analyse code without executing it, and can be used to find bugs in source code. Many open source and commercial tools have been developed in this space over the past 20 years. Scalability and precision are of importance for the deployment of static code analysis tools - numerous false positives and slow runtime both make the tool hard to be used by development, where integration into a nightly build is the standard goal. This requires one to identify a suitable abstraction for the static analysis which is typically a manual process and can be expensive. In this paper we report our findings on using machine learning techniques to detect defects in C programs. We use three offthe- shelf machine learning techniques and use a large corpus of programs available for use in both the training and evaluation of the results. We compare the results produced by the machine learning technique against the Parfait static program analysis tool used internally at Oracle by thousands of developers. While on the surface the initial results were encouraging, further investigation suggests that the machine learning techniques we used are not suitable replacements for static program analysis tools due to low precision of the results. This could be due to a variety of reasons including not using domain knowledge such as the semantics of the programming language and lack of suitable data used in the training process.	daily build;list of tools for static code analysis;machine learning;neutral build;open-source software;programming language;scalability;software bug;software deployment;static program analysis;text corpus	Timothy Chappelly;Cristina Cifuentes;Padmanabhan Krishnan;Shlomo Gevay	2017	2017 IEEE Workshop on Machine Learning Techniques for Software Quality Evaluation (MaLTeSQuE)	10.1109/MALTESQUE.2017.7882012	computer science;theoretical computer science;online machine learning;machine learning;data mining;shape analysis;static program analysis	SE	-60.90244891034615	39.04827013321848	23141
e2a4c9cd9f6b3a1b7430d05ea18ca61998dfa4b7	model-driven cross-platform apps: towards business practicability	journal article;peer reviewed	Due to the incompatibility of mobile device platforms such as Android and iOS, apps have to be developed separately for each target platform. Cross-platform development approaches based on Web technology have significantly improved over the last years. However, since they do not provide native apps, these frameworks are not feasible for all kinds of business apps. Moreover, the way apps are developed is cumbersome. Advanced cross-platform approaches such as MD, which is based on model-driven development (MDSD) techniques, are a much more powerful yet less mature choice. We introduce MD as one solution to fulfill typical requirements of business apps. Moreover, we highlight a businessoriented enhancement that further increases its business practicability.	android;mobile app;mobile device;model-driven engineering;model-driven integration;requirement;software incompatibility;speech enhancement;ios	Tim A. Majchrzak;Jan Ernsting;Herbert Kuchen	2015			peer review;computer science;multimedia;internet privacy;world wide web	Mobile	-51.900290730755536	43.4742616983513	23146
96ae3ef2755fda268f49e5c06659f4f8c644e0f6	towards decentralised security policies for e-health collaborations	trust;medical administrative data processing;policy;direct trust negotiation decentralised security policies e health collaborations decentralised collaborative environments autonomous security domains cross boundary issues remote security credentials dynamic trust negotiation multi domain collaboration;collaboration;hospitals;security of data medical administrative data processing;multi domain collaboration;contracts;cross boundary issues;qa75 electronic computers computer science;collaborative environment;remote security credentials;dynamic trust negotiation;e health collaborations;security contracts collaboration peer to peer computing hospitals access control joining processes;multi domain;decentralised security policies;negotiation policy trust;joining processes;decentralised collaborative environments;autonomous security domains;access control;direct trust negotiation;peer to peer computing;security;security policy;security of data;trust negotiation;negotiation;conference proceeding	Security in decentralised collaborative environments present huge challenges where many entities from different autonomous security domains want to access and share resources. This is largely due to cross-boundary issues where security credentials and policies are heterogeneous, and where yielding control to a centralised authority is not an option. Numerous cross-boundary approaches exist today and trust negotiation remains a promising solution that is rapidly evolving. In this paper we present dynamic trust negotiation, an approach that folds remote security credentials into local security credentials through trust contracts, thereby bridging the gap making decentralised security policies for multi-domain collaboration difficult. We show how trust can be realised between strangers through trusted intermediaries where direct trust negotiation between these strangers is otherwise unacceptable.	autonomous robot;bridging (networking);centralisation;credential;delay-tolerant networking;entity;virtual organization;ws-trust	Oluwafemi O. Ajayi;Richard O. Sinnott;Anthony Stell	2008	2008 Second International Conference on Emerging Security Information, Systems and Technologies	10.1109/SECURWARE.2008.15	computer security model;cloud computing security;security through obscurity;computer science;knowledge management;security policy;information security;access control;trustworthy computing;computer security;negotiation;computational trust;collaboration	Security	-44.509753157311486	56.05677099469318	23189
2ccf1cebe19f51563c2a79a285b1be267967d80c	software reliability engineering for mobile code	reliability engineering;software portability;software testing;certification;application software;software portability software reliability distributed programming;internet;distributed programming;fault tolerance;mobile code;software reliability engineering;code mobility software reliability engineering mobile code;code mobility;software reliability reliability engineering costs application software internet certification software testing fault tolerance tail programming;software reliability;programming;tail	This paper examines the nature of mobile code and compares it with past software, both in regard to similarities and differences. It then looks at how greater code mobility is likely to effect the practice of software reliability engineering (SRE).	code mobility;reliability engineering;software reliability testing	John D. Musa	1998		10.1109/ISSRE.1998.730879	software portability;kpi-driven code analysis;reliability engineering;programming;personal software process;fault tolerance;verification and validation;application software;the internet;code review;software sizing;computer science;social software engineering;software development;software engineering;software construction;software testing;software walkthrough;certification;tail;code mobility;software quality;computer engineering	SE	-62.25622874326467	32.69627623747139	23252
33d190e68341a4bc82a6b04e911462f16cca2fd2	sharing choreographies in openknowledge: a novel approach to interoperability	service choreography;semantic service composition;health informatics;index terms— service oriented architecture;service oriented architecture	As computer systems grow in size and complexity, their integration, while a necessity, becomes more difficult. Service oriented architectures and middleware systems in general deal with the issue by decoupling the components. However, these architectures are still designed and enacted from a centralised perspective: a single process invokes remote services, unaware of being part of a larger, more complex workflow. We claim that the orchestration-based approach does not scale well with increasing complexity and heterogeneity of the components, such as those required in the enactment of medical guidelines and workflows. Medical guidelines encode different aspects of a medical procedures, and they specify different level of abstraction in the procedure. They have to be adapted to the realities of different clinics and hospitals, to advances in knowledge, and to the changing of available resources within an institution. We address the problem of representing and enacting medical guidelines using a fully distributed approach. The framework provided by OpenKnowledge, based on sharing choreographies among actors, allows the representation and enactment of the coordination aspect of guidelines and the discovery of the medical knowledge provided by the distributed actors.	centralisation;complex systems;coupling (computer programming);encode;entity;expectation propagation;high- and low-level;interaction;interoperability;middleware;programming paradigm;requirement;scalability;service-oriented architecture;software portability	Paolo Besana;Vivek Patkar;Adam Barker;David Stuart Robertson;David Glasspool	2009	JSW		health informatics;computer science;knowledge management;service-oriented architecture;database;computer security	ML	-44.538852788866016	43.287985940930604	23269
cbd674169320e171ff1b324b38ce8e6a65be6c25	an xml based access control architecture for pervasive computing	xml documents;transmission security;transmission security xml based access control architecture pervasive computing mobile devices internet information access information dissemination information representation information exchange information storage xml documents cryptography;xml cryptography information dissemination information storage internet mobile computing;mobile device;pervasive computing;information access;computer architecture;xml access control computer architecture pervasive computing internet information representation data security information security foot batteries;information storage;access control policy;internet;engines;cryptography;information exchange;information representation;information dissemination;xml;mobile handsets;access control for pervasive computing;xml document;security access control for pervasive computing xml based access control;access control;mobile computing;xml based access control architecture;security;mobile devices;xml based access control	There is a huge prevalence of mobile devices being connected to the Internet because of high demands for information access and dissemination. It is now well understood that XML plays a vital role as a means for information representation, exchange, and storage. Naturally, XML data is exchanged and stored as these mobile devices communicate with each other, and over the web. A major concern for one device requesting data (objects, services or raw data) from another device is security. Access control policies are important models that control access to data for authorized devices. In an XML setting, access control policies are necessary to control access to parts of XML documents. It becomes challenging in pervasive computing environment as the devices have small memory foot print, disconnection, low battery powers, etc. In this paper, we propose an XML based access control along with cryptography for secure transmission of XML data in pervasive environments.	access control;authorization;cryptography;information access;internet;mobile device;secure transmission;ubiquitous computing;xml	Mohammad M. Molla;Praveen Madiraju;Srilaxmi Malladi;Sheikh Iqbal Ahamed	2009	2009 IEEE International Conference on Pervasive Computing and Communications	10.1109/PERCOM.2009.4912894	binary xml;xml;service interface for real time information;computer access control;computer science;operating system;soap;database;internet privacy;mobile computing;world wide web;computer security;ubiquitous computing;computer network	DB	-43.90438402531714	59.310230573428015	23276
47c1711c5e53317cd4cd61351d0a52cb385b443f	scientific engineering for distributed java applications		The use of open technologies and standards have made easier the integration of Web services into end-applications. These interoperable services have been organized on distributed architectures over Internet in accordance with shared functional principles. But these Web-service architectures have not resolved the distributed computing difficulty in ”gluing together” multiple and independent Web services. This paper presents an approach based on Java technology and Internet standard protocols and data formats for resolving coordination problems among Web services. Interaction models based on distributed events over HTTP are supported for providing the required coordination functionality. Cooperation problems and their solutions have been studied in the prototypical context of Location-Based Services.	distributed computing;hypertext transfer protocol;interoperability;java;location-based service;service-oriented architecture;web service	Stephan Philippi	2003		10.1007/3-540-36520-6	computational science;software engineering;real time java;programming language	SE	-35.03463415769816	43.279886614578146	23326
3a615948cf9882e9a1a7edc06dd44d334a174569	an optimized ws-eventing for large-scale networks	reliability;standards;prototypes;web services embedded systems internet of things ip networks operating systems computers;subscriptions;real time capability optimized ws eventing large scale networks embedded devices internet of things web services ws standards embedded systems ws eventing dpws scalability problem adhoc networks event source unicast ip communication automation infrastructures;real time systems subscriptions scalability reliability automation standards prototypes;scalability;real time systems;automation	Web Services are becoming more and more relevant also in the domain of embedded devices as they are becoming an important aspect of the Internet of Things. Embedded devices, especially in the field of automation, require real-time behavior. The Devices Profile for Web Services defines WS-Standards for embedded systems. The WS-Eventing is one of this standards within DPWS and enables the distributions of events. However, WS-Eventing has a scalability problem as in ad-hoc networks the notification is transmitted sequential by an event source. Under these circumstances, it is not suitable for large-scale networks with a high amount of devices. Therefore, a new approach is presented to solve this issue by acquiring helping devices. The communication is only based on unicast IP communication and thus is easy to integrate into automation infrastructures and could be established over several subnets. This approach has been investigated with an experimental setup and shows the high scalability compared to the original notification mechanism. Additionally, the real-time capability is given due to the chosen platform and real-time operating system. Thus, the new approach enables the idea of Internet of Things in automation scenarios.	devices profile for web services;embedded system;hoc (programming language);internet of things;real-time clock;real-time operating system;real-time transcription;scalability;subnetwork;unicast;web service;web standards	Jan Skodzik;Vlado Altmann;Peter Danielis;Moritz Koal;Dirk Timmermann	2014	Proceedings of the 2014 IEEE Emerging Technology and Factory Automation (ETFA)	10.1109/ETFA.2014.7005248	embedded system;real-time computing;scalability;computer science;operating system;automation;reliability;prototype;world wide web;statistics;computer network	Embedded	-38.917613722708786	46.64644825666892	23369
1a214060e1832f1c06e57701b9dfdb077874a3fb	secure applications need flexible operating systems	secure applications;system call interface;naming services;kernel;user id;operating systems application software access control wide area networks kernel data security laboratories computer science information security protection;discretionary access control secure applications flexible operating systems information exchange wide area networks firewalls malicious attackers workstation security malicious processes domino effect posix like interface hierarchically named capabilities unix user id group id low level system calls exokernel operating system system call interface;hierarchically named capabilities;information security;malicious attackers;application software;authorisation;low level system calls;domino effect;group id;discretionary access control;flexible operating systems;posix like interface;protection;operating system;information exchange;application program interfaces;workstations;firewalls;exokernel operating system;workstation security;access control;computer science;application program interfaces authorisation operating systems computers wide area networks workstations unix naming services electronic data interchange;operating systems computers;malicious processes;unix;wide area network;electronic data interchange;wide area networks;operating systems;data security	As information exchange over wide area networks becomes an increasingly essential component of new applications, firewalls will no longer provide an adequate defense against malicious attackers. Individual workstations will need to provide strong enough security to contain malicious processes and prevent the domino effect of a pierced firewall. Some of the most commonly found security holes today result from the fact that simple operations can be surprisingly difficult to implement correctly on top of a traditional POSIX-like interface. We claim that by combining hierarchically-named capabilities, a novel generalization of the Unix user and group ID concept, with the low-level system calls of an exokernel operating system, we can achieve a system call interface flexible enough to avoid much of the complexity that often leads to security holes in discretionary access control operating systems like Unix.	discretionary access control;exokernel;firewall (computing);group identifier;high- and low-level;information exchange;operating system;posix;software bug;system call;unix;unix-like;vulnerability (computing);workstation	David Mazières;M. Frans Kaashoek	1997		10.1109/HOTOS.1997.595183	embedded system;application software;kernel;workstation;information exchange;discretionary access control;computer science;information security;access control;operating system;domino effect;electronic data interchange;user identifier;data security;authorization;unix;computer security	Security	-52.80108703882593	58.54489954121462	23410
52ec2ac0b7143b74e2a57a2dce8c6368291c4148	application programming interface for configuration of multi-robot sensor networks	robot sensing systems;data transmission;mobility management mobile radio;routing protocols;sensor phenomena and characterization;telecommunication network topology application program interfaces multi robot systems wireless sensor networks mobility management mobile radio ad hoc networks delay systems mobile robots;wireless devices;multirobot sensor network;manet;api;prototypes;manet application programming interface api multirobot sensor network mrsn intentional mobility delay tolerance mobile ad hoc network;bridges;actuators;mrsn;mobile robots;application program interface;sensor network;wireless communication;intentional mobility;delay tolerance;application program interfaces;multi robot systems;ad hoc networks;mobile ad hoc network;delay systems;peer to peer computing;telecommunication network topology;wireless sensor networks;application programming interface;peer to peer computing wireless sensor networks prototypes robot sensing systems delay routing protocols sensor phenomena and characterization actuators wireless communication bridges	In this paper we propose a design of application programming interfaces (APIs) for configuration of multi-robot sensor networks (MRSNs). Such networks have unique aspects: intentional mobility and delay tolerance. These aspects limit the applicability of conventional approaches proposed for mobile ad-hoc networks (MANETs). APIs for configuration of MRSNs must take these aspects into account. In addition, an application program should be transparent from changing of physical devices such as wireless devices. In this paper we describe the design and prototype implementation of APIs for configuration of MRSNs.	application programming interface;hoc (programming language);prototype;robot	Junya Yamashita;Ryohei Suzuki;Kei Sawai;Hiroki Saito;Tsuyoshi Suzuki;Yoshito Tobe;Niwat Thepvilojanapong;Kaoru Sezaki	2006	20th International Conference on Advanced Information Networking and Applications - Volume 1 (AINA'06)	10.1109/AINA.2006.102	embedded system;application programming interface;computer science;operating system;distributed computing;computer network	Mobile	-36.8186164035177	46.686645132953245	23468
034cbf3472f0d24a0c4205a6307224858851aee2	a holistic iot-based management platform for smart environments	ministerio;centro universitario de la defensa;cartagena;general;context smart buildings energy consumption architecture intelligent sensors;upct;open systems building management systems computer network management computer network reliability internet internet of things;aire;cud;context awareness iot smart buildings energy efficiency;universidad politecnica de cartagena;academia;mde;ingenieria;aga;c u d;centro;universitario;energy balance holistic iot based management platform smart environments internet of things future internet iot enabled nodes network infrastructure reliability network infrastructure interoperability data collection smart building field indoor energy efficiency building management techniques;organizacion industrial;universidad;defensa;politecnica	Internet of Things (IoT) is a vision towards Future Internet where “things” are provided with enough intelligence to communicate with each other without the human intervention. In the near future, the number of IoT-enabled nodes is expected to grow substantially, therefore the heterogeneous nature of implementations demands effective IoT deployments that ensure proper interoperability and reliability of network infrastructures. In this line, we present a holistic IoT-based platform to gather and analyze information, and propose concrete actions according to different goals in smart environments. This platform bases on the optimal integration and use of data collected from a plethora of different sources. It has been deployed in a real IoT context framed in the smart buildings field. For its validation and evaluation, a reference scenario for indoor energy efficiency and comfort has been considered, where a set of tests have been carried out to validate the building management techniques implemented to maintain user comfort while assuring a good energy balance.	algorithm;experiment;extrapolation;future internet;holism;interconnection;internet of things;interoperability;layered system;reference implementation;sensor;smart city;smart environment;software deployment;systems architecture;universal instantiation;user interface	María Victoria Moreno Cano;José Santa;Miguel A. Zamora;Antonio F. Gómez-Skarmeta	2014	2014 IEEE International Conference on Communications (ICC)	10.1109/ICC.2014.6883917	simulation;computer science;cud;computer security	Robotics	-43.74552862145747	45.653020212369945	23502
f4cfd5007e9b8abc1114134c6902c5ba098ccacd	adaptable service composition for intelligent logistics: a middleware approach	silicon;logistics monitoring quality of service middleware concrete silicon;service composition;logistics service composition adaptable service composition intelligent logistics middleware internet of things real world devices web services functionality networking environment dynamicity networking environment heterogeneity iot resources service oriented architecture soa real world services user centric composition process environment aware composition process;web services internet of things logistics middleware service oriented architecture;internet of things;logistics;monitoring;middleware;quality of service;monitoring internet of things service composition middleware logistics;concrete	Recent years have witnessed a rapid growth in using Internet of Things (IoT), which facilitates and simultaneously raises challenges for the intelligent logistics. The real-world devices can provide their functionality as Web services. However, because of the dynamicity and heterogeneity of the target networking environment, the services offered by IoT resources cannot be composed by simply extending existing Service Oriented Architecture (SOA) approaches. For a logistics service, it may need the integration of a number of real-world services, which is a user-centric and environment-aware composition process. In this paper, we propose a middleware approach to solve the logistics service composition in IoT, where a decentralized coordination mechanism is used to monitor the component services with few resources efficiently. Through a set of experiments, the effectiveness and robustness of our approach are evaluated.	algorithm;business process;correctness (computer science);end-to-end principle;experiment;internet of things;logistics;middleware;requirement;semi-continuity;service composability principle;service-oriented architecture;service-oriented modeling;web service	Rong Yang;Bing Li;Can Cheng	2014	2014 International Conference on Cloud Computing and Big Data	10.1109/CCBD.2014.10	logistics;middleware;quality of service;concrete;differentiated service;computer science;operating system;integrated logistics support;middleware;database;silicon;world wide web;internet of things;computer network	Robotics	-43.282427164087274	44.124069708480114	23519
a382a4be0981a9b831e1dcfd1a58c7382855f1ea	architecting reconfigurable publish/subscribe applications for wireless sensor networks	subscribe;reconfigurability;wsns;middleware;reconfigurable publish 47;lbs models;local broker subcomponent;wireless sensor networks	This paper presents an approach, called LBS, for modelling and implementing the architecture of wireless sensor networks (WSNs) publish/subscribe (Pub/Sub) applications using a local broker subcomponent (LBS) model. Considering the reconfigurability of WSNs applications which is necessary for WSNs middleware, a graph-oriented local broker subcomponent (GOLBS) model is defined based on the LBS model by integrated WSNs middleware with Pub/Sub. Focusing on the evolution specification of reconfiguration and non-predefined dynamic reconfiguration, we define the notations of what is the GOLBS’ graph grammars and GOLBS’ graph, and show a derivative procedure. We design and implement a prototype system on GOLBS, called Pub/Sub-GOLBS. Performance evaluations imply that GOLBS can easily be constructed, while ensuring good reconfigurability.	location-based service;middleware;mobile phone;prototype;publish–subscribe pattern;reconfigurability	Biao Dong;Jinhui Chen	2015	IJSNet	10.1504/IJSNET.2015.069587	embedded system;real-time computing;wireless sensor network;computer science;operating system;middleware;distributed computing;computer network	Embedded	-39.04662568416647	45.77427030398369	23586
6e8ea171bff3818cbf0c428003e69367285b9e18	test-driven development of web and enterprise agents	test driven development;unit testing;software agents;multiagent middlewares	Test-driven development asserts that the test code is equally important as the production code. In this paper we propose a new unit testing framework in which the unit of work is an agent. The framework has emerged as a necessity during the development of our enterprise-scale multiagent middleware, and has therefore been designed as a practical solution. It includes a message-based and an interceptor-based architecture, and supports simultaneous testing of both client-side and server-side agents.	agent-based model;client-side;database transaction;interceptor pattern;list of unit testing frameworks;message passing;middleware;server (computing);server-side;test-driven development	Dejan Mitrovic;Mirjana Ivanovic;Zoran Budimac	2015		10.1145/2801081.2801112	test-driven development;real-time computing;simulation;computer science;software agent;unit testing;programming language	SE	-35.02286878923349	42.160417610990656	23641
b48bcecbafa983be0ce9a0cc739e2bf5d5e080a4	reducing unauthorized modification of digital objects	file organization;kernel;public key digital signatures access controls malware file organization operating systems;delivery system;authorisation;protection mechanisms;prototypes;digital signatures;operating systems protection mechanisms software release management and delivery system integration and implementation access controls file organization;public key;operating system kernels authorisation digital signatures file organisation industrial property;access controls;operating system;malware;file system;digital signature;system integration and implementation;software release management and delivery;access control;industrial property;organizations;overhead cost minimisation unauthorized modification reduction malicious modification problem unauthorized digital object replacement digital signatures file system binaries linux operating system protection kernel modification;operating system kernels;mechanism design;public key infrastructure;operating systems;file organisation	We consider the problem of malicious modification of digital objects. We present a protection mechanism designed to protect against unauthorized replacement or modification of digital objects while still allowing authorized updates transparently. We use digital signatures without requiring any centralized public key infrastructure. To explore the viability of our proposal, we apply the approach to file-system binaries, implementing a prototype in Linux which protects operating system and application binaries on disk. To test the prototype and related kernel modifications, we show that it protects against various rootkits currently available while incurring minimal overhead costs. The general approach can be used to restrict updates to general digital objects.	antivirus software;authorization;binary file;centralized computing;digital signature;linux;operating system;overhead (computing);protection mechanism;prototype;public key infrastructure;public-key cryptography;rootkit	Paul C. van Oorschot;Glenn Wurster	2012	IEEE Transactions on Software Engineering	10.1109/TSE.2011.7	digital signature;computer science;operating system;database;computer security	Security	-48.01335282988909	59.19999612197728	23655
bc93f81f4ab104090d092c67160a2b6b59dca225	internet services: the palmpilot and the handheld revolution	dp industry notebook computers wireless lan global positioning system cryptography;web and internet services personal digital assistants handheld computers application software laboratories computer science handwriting recognition operating systems consumer electronics scheduling;handheld computer;dp industry;wireless network;global position system;global positioning system;cryptography;cryptography palmpilot handheld computers 3com portable computers wireless networking global positioning systems;notebook computers;wireless lan	With many failed and still-failing efforts, handheld computation has been taking a long time to arrive. However, the unquestionable success of 3Com's PalmPilot heralds the start of a revolution in computation. The author discusses the many guises of portable computers and considers the reasons for Pilot's success. He discusses three enabling technologies: wireless networking, Global Positioning Systems and cryptography.	web service	Michael K. McCandless	1997	IEEE Expert	10.1109/64.642952	embedded system;simulation;global positioning system;computer science;cryptography;wireless network	Vision	-38.5093147050321	54.60485313237155	23671
8227c5b121b65710f8b72b765fc002aecfba3ac6	declarative specification of robot perception architectures	dsl;robotics;perception specification	Service robots become increasingly capable and deliver a broader spectrum of services which all require a wide range of perceptual capabilities. These capabilities must cope with dynamically changing requirements which make the design and implementation of a robot perception architecture a complex and tedious exercise which is prone to error. We suggest to specify the integral parts of robot perception architectures using explicit models, which allows to easily configure, modify, and validate them. The paper presents the domain-specific language RPSL, some examples of its application, the current state of implementation and some validation experiments.	declarative programming;robot	Nico Hochgeschwender;Sven Schneider;Holger Voos;Gerhard K. Kraetzschmar	2014		10.1007/978-3-319-11900-7_25	embedded system;simulation;digital subscriber line;computer science;artificial intelligence;robotics	Robotics	-42.3296020978009	38.7804723305259	23706
eab4588d5d5a19793d00ea5384198316cb2ae313	the exploitation and discussion of new mobile healthcare system model based on smart phone	smart phones intelligent sensors cameras mobile communication bluetooth servers;mobile health smart phone android pulse self organizing network;signal detection;smartphone based self organizing medical datasharing network mobile healthcare system model mobile medical platform mobile medical model mobile medical use android based smartphone pulse signal analysis pulse signal transmission pulse signal acquisition mobile phone camera network communications capabilities;smart phones;smart phones biomedical communication health care signal detection;biomedical communication;health care	Smart phone is becoming increasingly popular. In view of the characteristics of its portability, the smart phone has been widely used as a mobile medical platform. However, display is the main function in current mobile medical model based on smart phone. The deeper analysis potentials of this model had not yet been developed. In this paper, use pulse signal as an example, new modes of smart phones in the mobile medical use of Android-based Smartphone is explored. The acquisition, transmission and analysis of pulse signal were achieved through the development of a new mobile healthcare system. Especially, the pulse signal was captured based on the mobile phone camera and further analyzed by smart phone. As Smartphone complete signal acquisition, analysis, and network communications capabilities, a Smartphone-based self-organizing medical data-sharing network was proposed in this paper.	android;camera phone;entry point;organizing (structure);self-organization;smartphone;software portability	Tianyu Zhu;Lei Wang;Jun Meng	2013	2013 10th IEEE INTERNATIONAL CONFERENCE ON NETWORKING, SENSING AND CONTROL (ICNSC)	10.1109/ICNSC.2013.6548789	radio access network;embedded system;mobile identification number;mobile search;mobile web;imt advanced;gsm services;engineering;mobile technology;feature phone;mobile phone tracking;multimedia;mobile station;mobile computing;computer security;health care;detection theory	Mobile	-39.23922472656695	53.28798152993424	23766
028f60e6c592f357cd55c55afcbb1a74f7a22a28	a reflective approach for supporting the dynamic evolution of component types	informatica;runtime type evolution;formal specification;software architecture formal specification object oriented programming;probability density function;software systems;aosd;object oriented programming;data mining;runtime;connectors;software architecture;aosd runtime type evolution dynamic evolution software architecture reflection;weaving;component types;component types dynamic evolution software systems software architecture component specifications;reflection;component specifications;software architecture runtime software systems error correction connectors proposals computer architecture information systems finishing concrete;dynamic evolution	The increasing complexity of software systems requires a continuous revisions process in order to correct errors or to add new functionalities. However, the nature of some systems makes unfeasible their stopping to integrate changes. Dynamic evolution of types is a feature that provides support for changing completely at runtime the types that a system is composed of. Thus, a system is able to integrate new types, or to modify/remove existing ones, while it is running. In software architecture, these types are component specifications, and its instantiations, component instances. This paper presents a reflective approach for providing dynamic evolution of component types and instances in a decentralized way. Each type can be evolved separately from others, and each one of its instances evolves asynchronously, only after finishing their running transactions. The approach is reflective since it dynamically provides editable specifications of the type to evolve, and reflects changes on both types and instances while they are running.	evolution;reflection (computer programming);run time (program lifecycle phase);software architecture;software system	Cristóbal Costa Soria;David Hervas-Muoz;Jennifer Pérez;José A. Carsí	2009	2009 14th IEEE International Conference on Engineering of Complex Computer Systems	10.1109/ICECCS.2009.35	software architecture;probability density function;real-time computing;reflection;computer science;systems engineering;operating system;software engineering;formal specification;distributed computing;programming language;object-oriented programming;weaving;software system	SE	-38.86995722167692	39.10770936914404	23844
ac6e9df9e09a1b5e3422c9fa8792f398009a54ab	a fail safe programmable logic controller	control application;fail safe comparator;result and status indication;software verification;real time;low complexity;safety integrity level;safety integrity level 3;safety related control;fault detection;semantic gap;function block diagrams;ternary logic;function block diagram;processing speed;programmable logic controller	To architecturally support the programming of safety related control applications in the graphical language function block diagram and the verification of such software meeting the requirements of Safety Integrity Level SIL 3, a dedicated, low complexity execution platform is presented. Its hardware is fault detecting to immediately initiate emergency shut-downs in case of malfunctions. With their low processing speeds, currently available fail safe comparators constitute bottlenecks and, in case of malfunctions, do not distinguish between comparison errors and comparator errors. To solve these problems, a novel fail safe comparator of two binary inputs is presented, which does not only indicate a result, but also its status. Built in a modified CMOS technology, it can match the execution speed of digital computers. In contrast to all earlier designs of fail safe comparators, by employing ternary logic it provides three different output values, allowing to distinguish between the three indications “inputs equal and comparator working properly,” “inputs unequal and comparator working properly” as well as “comparator malfunctioning.” By design, there is no semantic gap between the programming and machine execution levels of the controller, enabling the safety licensing of application software by extremely simple, but rigorous methods, viz., diverse back translation and inspection. Operating in a strictly periodic fashion, the platform exhibits fully predictable real time behaviour. © 2003 Elsevier Ltd. All rights reserved.	cmos;comparator;diagram;fail-safe;graphical user interface;jakobson's functions of language;programmable logic device;requirement;sensor;three-valued logic;visual programming language;viz: the computer game	Marek Sniezek;Josef von Stackelberg	2003	Annual Reviews in Control	10.1016/S1367-5788(03)00008-7	embedded system;electronic engineering;real-time computing;software verification;computer science;engineering;programmable logic controller;safety integrity level;control theory;function block diagram;fault detection and isolation;semantic gap	Embedded	-37.21685985791642	35.73394811045593	23845
616ea2e7be5ba868bcebd5a33969e1095a250374	behavior decomposition: aspect-level browser extension clustering and its security implications	browser security;graph isomorphism;behavior clustering	Browser extensions are widely used by millions of users. However, large amount of extensions can be downloaded from webstores without sufficient trust or safety scrutiny, which keeps users from differentiating benign extensions from malicious ones. In this paper, we propose an aspect-level behavior clustering approach to enhancing the safety management of extensions. We decompose an extension’s runtime behavior into several pieces, denoted as AEBs (Aspects of Extension Behavior). Similar AEBs of different extensions are grouped into an “AEB cluster” based on subgraph isomorphism. We then build profiles of AEB clusters for both extensions and categories (of extensions) to detect suspicious extensions. To the best of our knowledge, this is the first study to do aspect-level extension clustering based on runtime behaviors. We evaluate our approach with more than 1,000 extensions and demonstrate that it can effectively and efficiently detect suspicious extensions.	analog expansion bus;browser extension;cluster analysis;online shopping;sensor;subgraph isomorphism problem	Bin Zhao;Peng Liu	2013		10.1007/978-3-642-41284-4_13	browser security;computer science;database;graph isomorphism;world wide web;computer security	Web+IR	-58.03462342411073	60.31678444454106	23851
f553dccbb0dffe94a88ee92406e281c63fbcd5cc	secure embedded hypervisor based systems for automotive	automotive engineering;virtualization;certification;iommu;prototypes;automotive;hypervisor;computer security;virtual machine monitors;cybersecurity;standardization	Hypervisors are an existing solution for security challenges in automotive domain. Nevertheless, advancedcybersecurity threats cannot be mitigated without hardware support, further software components, and harmonized security standardization.	component-based software engineering;embedded hypervisor	Stefaan Sonck Thiebaut;Antonio De Rosa;Ralph Sasse	2016	2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshop (DSN-W)	10.1109/DSN-W.2016.37	iommu;embedded system;virtualization;storage hypervisor;computer science;operating system;prototype;hypervisor;certification;computer security;standardization	EDA	-54.8352855848843	50.275586045638306	23914
52099d38eec74c697f164b67eed41b34cc066dde	an overview of pervasive computing	home computing;pervasive computing;wearable computers;distributed computing;smart phones;home appliances;computer networks;wireless communication;pervasive computing mobile computing distributed computing embedded computing computer networks wearable computers wireless communication smart phones home appliances home computing;mobile computing;embedded computing	he importance of pervasive computing is rapidlyincreasingwith the current trend toward universal presence of mobile computing, computer networks, and wireless communications ineveryday life. In thelastdecade, there has been a dramatic increase in the use of companion devices and embedded computing devices. For example, wirelessly connectcd organizers and smart phones are becoming popular, and digital computing in some form is now an integral part of numerous evcryday appliances. This has led to a change in thc waywe perceive computing andcomputers. Computers are no longer standalone special-purpose machines to beusedbyexperts; rather, they arc ubiquitously present in a networked environment to scrve myriad needs of everyday life. The change in our perception of computing and computing devices, and their ever increasing presence in our everyday life is the subject of the new ficld known as pervasive computing. The word pervasive	ubiquitous computing	Sandeep K. S. Gupta;Wang-Chien Lee;Apratim Purakayastha;Pradip K. Srimani	2001	IEEE Personal Commun.	10.1109/MPC.2001.943997	embedded system;real-time computing;context-aware pervasive systems;wearable computer;ambient intelligence;cloud computing;computer science;operating system;end-user computing;distributed computing;smart environment;mobile computing;ubiquitous computing;wireless;autonomic computing	Visualization	-39.81891509226512	51.79500029929423	23925
f50d9c33554c0fd22b124119f8422a67a1ed5ac8	scheduling & routing time-triggered traffic in time-sensitive networks				Naresh Ganesh Nayak	2018				Networks	-34.2081529713007	46.76830624974574	23982
8bbf078234e508c0cb6dc0f5d81151304f1ddf23	hacker-proof coding		Software verification helps find the faults, preventing hacks.	hacker;software verification	Esther Shein	2017	Commun. ACM	10.1145/3105423	coding (social sciences);theoretical computer science;modified huffman coding;hacker;tunstall coding;computer science;context-adaptive binary arithmetic coding	SE	-58.36466852223297	57.27938720007026	24111
edada0d203be8f30d3385d76a93d0724149b107f	modeling and analyzing security patterns using high level petri nets		Security has become an essential and critical nonfunctional requirement of modern software systems, especially cyber physical systems. Security patterns aim at capturing security expertise in the worked solutions to recurring security design problems. This paper presents an approach to formally model and analyze six security patterns to detect potential incompleteness, inconsistency, and ambiguity in the textual descriptions; and to prevent their incorrect implementation. These patterns are modeled using high level Petri nets in our tool environment PIPE+. Simulation is used to analyze various security relevant properties. The validated formal models of individual security patterns serve as the building blocks for system design involving the composition of multiple security patterns.	high-level programming language;information security;non-functional requirement;petri net;simulation;software system;systems design	Xudong He;Yujian Fu	2016		10.18293/SEKE2016-010	process architecture	Security	-55.10409611232386	48.359797549928246	24140
35a4e5610b3086ee346ca56dc571b0cb406f5d40	labeling source code with information retrieval methods: an empirical study	information retrieval;program comprehension;software artifact labeling;empirical studies	To support program comprehension, software artifacts can be labeled—for example within software visualization tools—with a set of representative words, hereby referred to as labels. Such labels can be obtained using various approaches, including Information Retrieval (IR) methods or other simple heuristics. They provide a bird-eye’s view of the source code, allowing developers to look over software components fast and make more informed decisions on which parts of the source code they need to analyze in detail. However, few empirical studies have been conducted to verify whether the extracted labels make sense to software developers. This paper investigates (i) to what extent various IR techniques and other simple heuristics overlap with (and differ from) labeling performed by humans; (ii) what kinds of source code terms do humans use when labeling software artifacts; and (iii) what factors—in particular what characteristics of the artifacts to be labeled—influence the performance of automatic labeling techniques. We conducted two experiments in which we asked a group of students (38 in total) to label 20 classes from two Java software systems, JHotDraw and eXVantage. Then, we analyzed to what extent the words identified with an automated technique—including Vector Space Models, Latent Semantic Indexing (LSI), latent Dirichlet allocation (LDA), as well as customized heuristics extracting words from specific source code elements—overlap with those identified by humans. Results indicate that, in most cases, simpler automatic labeling techniques—based on the use of words extracted from class and method names as well as from class comments—better reflect human-based labeling. Indeed, clustering-based approaches (LSI and LDA) are more worthwhile to be used for source code artifacts having a high verbosity, as well as for artifacts requiring more effort to be manually labeled. The obtained results help to define guidelines on how to build effective automatic labeling techniques, and provide some insights on the actual usefulness of automatic labeling techniques during program comprehension tasks.	automatic differentiation;cluster analysis;comment (computer programming);component-based software engineering;experiment;heuristic (computer science);hoc (programming language);information retrieval;java;latent dirichlet allocation;list comprehension;open-source software;program comprehension;programming language;software developer;software release life cycle;software system;software visualization;topic model;type signature;viable system model	Andrea De Lucia;Massimiliano Di Penta;Rocco Oliveto;Annibale Panichella;Sebastiano Panichella	2013	Empirical Software Engineering	10.1007/s10664-013-9285-5	computer science;data mining;database;empirical research;information retrieval	SE	-56.79729586437404	35.022212871710195	24215
2efa01a82b3fc4c29b6f03ee8076e324c3876ceb	a tale of two browsers	mining repositories;software repositories;release history;text mining;open source	We explore the space of open source systems and their user communities by examining the development artifact histories of two popular web browsers -- Firefox and Chrome -- as well as usage data. By examining the data and addressing a number of research questions, two very different profiles emerge: Firefox, as the older and established system, with long product version cycles but short bug fix cycles, and a user base that is slow to adopt newer versions; and Chrome, as the new and fast evolving system, with short version cycles, longer bug fix cycles, and a user base that very quickly adopts new versions as they become available (due largely to Chrome's mandatory automatic updates).	firefox;google chrome;open-source software;patch (computing);usage data;windows update	Olga Baysal;Ian J. Davis;Michael W. Godfrey	2011		10.1145/1985441.1985481	text mining;computer science;engineering;data mining;database;world wide web	Security	-61.3834387258684	43.09281646317484	24276
21af209a2d3c516b4f0c4f364c8d944a8f0e0f58	object model driven code generation for the enterprise	signal generators;replication;off the shelf equipment;concurrent computing;force multiplier;software prototyping;software maintenance;availability;information infrastructure;code generation;client server systems;object oriented modeling signal generators concurrent computing web server signal synthesis software prototyping software maintenance programming profession availability computer architecture;object oriented programming;program synthesis;repetitive code;software engineering;synthesis;computer architecture;concurrency;distributed objects;joint task force;programming profession;object model driven code generation;next generation;object oriented programming program compilers;code generator;network architecture;web server;signal synthesis;interoperability;availability object model driven code generation code generator object oriented servers repetitive code force multiplier concurrency replication security;program compilers;distributed data processing;defense advanced research project agency;security;distributed collaboration;object oriented servers;software reuse;templates;object oriented modeling;wide area network;geographic distribution;data dissemination;wide area networks;object model	Joint Task Force–Advanced Technical Demonstration (JTF–ATD) was a Defense Advanced Research Projects Agency (DARPA) project in the field of distributed, collaborative computing. In a typical JTF command hierarchy, the critical people, relevant data, and their supporting computers are geographically distributed across a wide-area network. This causes many problems that would not exist if they were all in the same location. The goal of JTF–ATD was to make it easier for people to work together. A system that facilitated the sharing of data and ideas without compromising security, timeliness, flexibility, availability, or other desirable qualities was needed. After experimentation with numerous architectures and implementations, the JTF–ATD concluded that an enterprise solution to data dissemination and access was needed. It also became apparent that the different types of data needed to support JTF missions were as ubiquitous as the missions themselves. Therefore, planning systems would need the ability to associate previously unknown data elements to their plan composition. A distributed, object-oriented design held the most promise to meet these goals.	code generation (compiler);computer;model-driven integration;openness;software evolution	William J. Ray;Andy Farrar	2001		10.1109/IWRSP.2001.933843	kpi-driven code analysis;dead code;embedded system;real-time computing;concurrent computing;object code;code access security;computer science;operating system;software engineering;dead code elimination;redundant code;distributed computing;programming language;code mobility;code generation;unreachable code;source code	HPC	-34.15040300620568	41.378919923090265	24377
07720ce6c546695fd39110b11bcaf5182dc5a66c	efficient privilege de-escalation for ad libraries in mobile apps	app instrumentation;mobile apps;previlege de escalation;ad libraries;static analysis	The proliferation of mobile apps is due in part to the advertising ecosystem which enables developers to earn revenue while providing free apps. Ad-supported apps can be developed rapidly with the availability of ad libraries. However, today?s ad libraries essentially have access to the same resources as the parent app, and this has caused signi?cant privacy concerns. In this paper, we explore ef?cient methods to de-escalate privileges for ad libraries where the resource access privileges for ad libraries can be different from that of the app logic. Our system, PEDAL, contains a novel machine classi?er for detecting ad libraries even in the presence of obfuscated code, and techniques for automatically instrumenting bytecode to effect privilege de-escalation even in the presence of privilege inheritance. We evaluate PEDAL on a large set of apps from the Google Play store and demonstrate that it has a 98% accuracy in detecting ad libraries and imposes less than 1% runtime overhead on apps.	ecosystem;instrumentation (computer programming);library (computing);mobile app;obfuscation (software);overhead (computing);play store;privilege escalation;sensor	Bin Liu;Hongxia Jin;Ramesh Govindan	2015		10.1145/2742647.2742668	computer science;operating system;internet privacy;world wide web;computer security;static analysis	Mobile	-55.583242653106524	59.78053022128943	24383
3f71ae14e7976c551d6b99881160b52cc4663db1	optimal software testing	objective function;cost function;design optimization;software testing	In this paper, we formulate an optimal testing procedure for software. The objective function is taken to be the expected value of the cost associated with accepting faulty software plus the expected value of the accumulated cost of testing. The model can be used to design optimal testing plans for unit testing as well as for complete programs. The model assigns different costs for normal bugs and major bugs, and considers the costs of various types of misclassification as well The model is demonstrated by an example, and the cost function is plotted for a range of test cycles to illustrate the minimum cost solution.	loss function;optimization problem;software bug;software testing;unit testing	Moustafa Elshafei;Mehmood Khan;Mohamed Boraie	2004			software;system testing;software reliability testing;reliability engineering;systems engineering;computer science;software performance testing	SE	-61.882083041599635	33.12832629909509	24451
c456c8da74ce80407b7745d02d38ff42668c22b6	guiding the selection of security patterns for real-time systems	analytical models;software;computer architecture;systems architecture;security;context;real time systems	Securing critical systems such as Cyber-Physical Systems (CPS) is an important feature especially when it comes to critical transmitted data in a real-time environment. At the same time, the implementation of security counter-measures in such systems may impact transmission delays of critical tasks. For this reason selecting proper security mechanisms in such critical systems is an important issue. In this context, we propose a model-based approach for selecting proper security solution alternatives composed of security patterns at early design stage against real-time requirements. We provide a generalizable and tool-supported solution to support the approach using UML and its profiles. A validation of the work is presented via a simplified version of SCADA (Supervisory Control and Data Acquisition) system case study.	critical systems thinking;cyber-physical system;data acquisition;download;real-time transcription;requirement;unified modeling language	Anas Motii;Brahim Hamid;Agnes Lanusse;Jean-Michel Bruel	2016	2016 21st International Conference on Engineering of Complex Computer Systems (ICECCS)	10.1109/ICECCS.2016.027	software security assurance;computer security model;real-time computing;security information and event management;security engineering;covert channel;computer science;systems engineering;operating system;software engineering;security testing;computer security;systems architecture;systems design	Embedded	-54.91923631918308	49.00988617464555	24559
1819482b06ab6f336f3729b65feaad7e310461c0	network-level security and privacy control for smart-home iot devices	telecommunication control computer network security data privacy internet of things software defined networking;security monitoring privacy control systems prototypes home appliances context;dynamically block quarantine devices network level security privacy control smart home iot devices smart home appliances attack vectors device level protections suspicious behavior detection software defined networking technology	The increasing uptake of smart home appliances, such as lights, smoke-alarms, power switches, baby monitors, and weighing scales, raises privacy and security concerns at unprecedented scale, allowing legitimate and illegitimate entities to snoop and intrude into the family's activities. In this paper we first illustrate these threats using real devices currently available in the market. We then argue that as more such devices emerge, the attack vectors increase, and ensuring privacy/security of the house becomes more challenging. We therefore advocate that device-level protections be augmented with network-level security solutions, that can monitor network activity to detect suspicious behavior. We further propose that software defined networking technology be used to dynamically block/quarantine devices, based on their network activity and on the context within the house such as time-of-day or occupancy-level. We believe our network-centric approach can augment device-centric security for the emerging smart-home.	embedded system;entity;home automation;network switch;open-source software;privacy;software-defined networking;snoop	Vijay Sivaraman;Hassan Habibi Gharakheili;Arun Vishwanath;Roksana Boreli;Olivier Mehani	2015	2015 IEEE 11th International Conference on Wireless and Mobile Computing, Networking and Communications (WiMob)	10.1109/WiMOB.2015.7347956	software security assurance;cloud computing security;privacy software;telecommunications;asset;operating system;security service;internet privacy;network access control;computer security;computer network	Mobile	-50.61647532123253	58.13392273282838	24567
0f22962a3558261883fdaced9ba7ab20b50815f4	formal verification - is it real enough?	formal verification investments automotive engineering design engineering power engineering and energy management training logic design industrial training resource management engineering management;functional verification;logic design;design flow;design quality;logic design formal verification engineering resources design quality;formal verification;time to market;logic design formal verification	While formal verification (FV) of logic designs has been described in an industrial context, it has not yet become a mainstream methodology. The purpose of this report is to summarize a body of experience in the application of industrial-scale FV. FV is a realistic means to successfully address the growing complexities of contemporary design. Introducing FV into the design flow is a strategic decision that requires investment in engineering resources (training and methodology adjustment) as well as support and commitment from management. When appropriately applied, FV is a powerful verification vehicle which contributes to increasing design quality and shortening time to market, with a notable return on the investment in engineering resources.	farmville;formal verification;norm (social)	Yaron Wolfsthal;Rebecca M. Gott	2005	Proceedings. 42nd Design Automation Conference, 2005.	10.1145/1065579.1065755	reliability engineering;logic synthesis;verification;formal methods;probabilistic design;formal verification;software verification;computer science;systems engineering;engineering;design flow;programming language;intelligent verification;engineering change order;functional verification	EDA	-50.15652012252461	37.55091727450417	24615
3867ce519e57e544f898433427c22e55ae882030	considerations on estimating the minimal level of attenuation in tempest filtering for it equipments		The main purpose of this research is to improve the security of critical computer systems with minimal costs. One of the main problems in such cases is the secondary emissions generated by electronic equipment that, sometimes, might contain confidential information stored inside a secured computer network. The implementation of a set of measures necessary to prevent information leakage through compromising emissions is generally expensive. This paper analyzes some minimal requirements that have to be fulfilled by the filtering devices in order to protect the existing commercial IT equipment against compromising emissions.	tempest	Mircea Popescu;Razvan Bartusica;Alexandru Boitan;Ioana Marcu;Simona Halunga	2017		10.1007/978-3-319-92213-3_2	filter (signal processing);real-time computing;attenuation;tempest;information leakage;electronic filter;computer science	Metrics	-61.39826872318432	50.08016295824919	24643
6cf255eb6705ebfa75810439b1873beef6749f1a	dynamic software birthmark for java based on heap memory analysis	conference_paper;software birthmark;software protection;java;code theft detection	Code theft has been a serious threat to the survival of the software industry. A dynamic software birthmark can help detect code theft by comparing the intrinsic characteristics of two programs extracted during their execution. We propose a dynamic birthmark system for Java based on the object reference graph. To the best of our knowledge, it is the first dynamic software birthmark making use of the heap memory. We evaluated our birthmark using 25 large-scale programs with most of them of tens of megabytes in size. Our results show that it is effective in detecting partial code theft. No false positive or false negative were found. More importantly, the birthmark remained intact even after the testing programs were obfuscated by the state-of-the-art Allatori obfuscator. These promising results reflect that our birthmark is ready for practical use.	java;megabyte;memory management;obfuscation (software);sensor;software industry	Patrick P. F. Chan;Lucas Chi Kwong Hui;Siu-Ming Yiu	2011		10.1007/978-3-642-24712-5_8	real-time computing;computer science;operating system;programming language;java;computer security	SE	-59.97018794581839	39.6764854447069	24734
7a405f30a53d211216f9d63e8e0898ba5af51865	refactoring affordances in corporate wikis: a case for the use of mind maps	refactoring;affordance;corporate wikis;mediawiki;freemind;mind map	The organization of corporate wikis tends to deteriorate as time goes by. Rearranging categories, structuring articles and even moving sections among articles are cumbersome tasks in current wiki engines. This discourages the layman. But, it is the layman who writes the articles, knows the wiki content, and detects refactoring opportunities. Our goal is to improve the refactoring a ordances of current wiki engines by providing an alternative front-end tuned to refactoring. This is achieved by (i) surfacing the structure of the wiki corpus as a mind map, and (ii) conducting refactoring as mind map reshaping. To this end, we introduce WikiWhirl, a domain-speci c language for wiki refactoring. WikiWhirl is supported as an extension of FreeMind, a popular mind mapping tool. In this way, refactoring operations are intuitively conducted as actions upon mind map nodes. In a refactoring session a user imports the wiki structure as a FreeMind map; next, conducts the refactoring operations on the map, and nally, the e ects are saved in the wiki database. The operational semantics of the WikiWhirl operations follow refactoring good practices (e.g., authorship preservation). Results from a controlled experiment suggest that WikiWhirl outperforms MediaWiki in three main a ordance enablers: understandability, productivity and ful llment of refactoring good practices.	best practice;code refactoring;digital subscriber line;infographic;mediawiki;mind map;operational semantics;usability;wiki	Gorka Puente;Oscar Díaz;Maider Azanza	2015	Enterprise IS	10.1080/17517575.2013.830343	computer science;systems engineering;knowledge management;software engineering;database;affordance;world wide web;code refactoring	SE	-52.48806064352227	34.118825630348184	24746
13f5e646b4d5686dd333fdaf151fb667cb1e165f	an operating system architecture for organic computing in embedded real-time systems	organic computing;embedded real time systems;real time operating system;embedded system;operating system;real time application;autonomic computing	To overcome the rising complexity of computing systems, the paradigms of Autonomic Computing and Organic Computing have been introduced. By using an observer/controller architecture, Organic Computing aims to make embedded systems more life-like by providing them with so-called Self-X properties. Embedded real-time systems can also gain great benefit from these techniques. In this paper, we show what new requirements arise when introducing Autonomic/Organic Computing into the area of real-time applications. These requirements flow into the architecture of the real-time operating system CAROS. CAROS combines several concepts to provide a solid base for the implementation of Self-X techniques in embedded real-time systems. We show the practicability of our concepts with a prototypical implementation on the multithreaded CarCore microcontroller.	autonomic computing;embedded system;management system;memory protection;microcontroller;multithreading (computer architecture);organic computing;real-time clock;real-time computing;real-time operating system;real-time transcription;requirement;scheduling (computing);thread (computing)	Florian Kluge;Jörg Mische;Sascha Uhrig;Theo Ungerer	2008		10.1007/978-3-540-69295-9_28	embedded system;embedded operating system;real-time computing;computer science;operating system;end-user computing;distributed computing;autonomic computing	Embedded	-34.40180947219707	38.14715369326568	24768
f341e443cdd6165cb463ee418f0faf79fbfa8501	an adaptive architecture for presenting interactive media onto distributed interfaces	agent based;distributed networks;interactive media;adaptive architecture	This paper introduces an adaptive architecture for presenting interactive timed media onto distributed networked devices. The architecture is put into the test in a storytelling application for children. The interactive story is documented in StoryML, an XML-based language, and presented to multiple interface devices organized in an agent-based architecture. This allows the separation of the content from concrete physical devices, the definition of abstract media objects and the automatic adaptation of the same content to different environments of physical devices. Since both the content and the interaction are timed, issues of streaming and synchronization in this architecture are also addressed.	adaptive architecture;interactive media	Jun Hu;Loe M. G. Feijs	2003			real-time computing;computer science;applications architecture;distributed computing;multimedia;interactive media	HCI	-37.10460316845079	41.50113040004816	24887
2bce8b7ca45035957d6821f9342ca2a339db5506	feature-based software customization: preliminary analysis, formalization, and methods	libraries;software;software analysis;software customization;software engineering;software maintenance java program slicing;business;bloatware software analysis program slicing software customization;moving target defense feature based software customization java bytecode customization static dataflow analysis enhanced programming slicing technique java programs legacy projects software diversity;software business libraries software engineering security creep java;creep;program slicing;security;bloatware;java	Modern software engineering practice allows us tobuild more complex software than ever before. However, onthe other hand, it causes some negative consequences such asbloatware and feature creep which have been observed in manysoftware evolution and iteration lifecycle. In this paper, we proposean approach to customizing Java bytecode by applying static dataflow analysis and enhanced programming slicing technique. This approach allows developers to customize Java programs based on various users' requirements or remove unnecessaryfeatures from tangled code in legacy projects. We evaluate ourapproach by conducting case studies on removing cross cuttingfeatures from real world Java programs. The results show thatour approach has the potential for practical use. Additionally, we find out that, by increasing the diversity of the software, ourapproach can help achieve moving target defense.	data-flow analysis;dataflow;feature creep;instruction creep;iteration;java bytecode;legacy code;program slicing;redundancy (engineering);requirement;software engineering;structure of observed learning outcome	Yufei Jiang;Can Zhang;Dinghao Wu;Peng Liu	2016	2016 IEEE 17th International Symposium on High Assurance Systems Engineering (HASE)	10.1109/HASE.2016.27	long-term support;creep;verification and validation;program slicing;real-time computing;software sizing;computer science;software bloat;information security;package development process;backporting;software design;social software engineering;software framework;component-based software engineering;software development;operating system;software analysis pattern;software engineering;software construction;real time java;software walkthrough;programming language;resource-oriented architecture;java;software deployment;software quality;software system;software peer review	SE	-58.05955192649126	36.35125674603057	24916
706d96cafccb936c34db36f9dfefd87c44990e28	specifying real-time requirements for sdl specifications - a temporal logic-based approach	real-time requirement;sdl specification;temporal logic-based approach;specification;system engineering;temporal logic;state transition;standardisation;real time;quality of service	The expressiveness of many state-transition based formal description techniques, e.g. the ITU-TS standardised Speciication and Description Language (SDL), does not capture hard real-time requirements. In telecommunications systems engineering, hard real-time requirements, however, are an important class of properties. They occur in the description of progress properties in telecommunications protocols as well as in the speciication of real-time related of Quality of Service (QoS) requirements. We suggest integrating functional system properties, given as SDL speciications, with real-time requirements expressed in terms of real-time temporal logic formulas. We call the resulting speciications`complementary speciications'. First, we show the inexpressiveness of SDL with respect to hard real-time requirements. Next, we deene a common model theoretic foundation which allows SDL speciications to be used jointly with temporal logic speciications. Then we give examples of commonly used real-time related QoS requirements, namely delay bound, delay jitter, and isochronicity. We also discuss the speciication of various QoS mechanisms, like QoS negotiation, QoS monitoring and jitter compensation. Finally, we point at related formal veriication problems.	formal methods;packet delay variation;quality of service;real-time clock;real-time computing;real-time operating system;real-time transcription;requirement;systems engineering;temporal logic;theory	Stefan Leue	1995			reliability engineering;real-time computing;description logic;interval temporal logic;computer science;systems engineering;language of temporal ordering specification	Embedded	-37.807365151933375	32.49899990641014	24937
ec915ebaad03ee3b9e8cc46de6525ef58b5e2700	osmotic flow: osmotic computing + iot workflow	blue skies;cloud;osmotic flow;internet of things iot;data transformation;edge computing	The rapid evolution of Internet of Things (IoT) devices (e.g., sensors and gateways) and the almost ubiquitous connectivity (e.g., 4G, Wi-Fi, RFID/NFC, Bluetooth, IEEE 802.15.4) are forcing us to radically rethink how to effectively deal with massive volume, velocity, and variety of big data produced by such IoT devices. There are currently 6.4 billion IoT devices in use around the world and their number, capabilities, as well as the scope of their use, keeps growing rapidly.	big data;bluetooth;internet of things;near field communication;radio-frequency identification;sensor;velocity (software development)	Matteo Nardelli;Stefan Nastic;Schahram Dustdar;Massimo Villari;Rajiv Ranjan	2017	IEEE Cloud Computing	10.1109/MCC.2017.22	cloud computing;computer science;operating system;internet privacy;data transformation;world wide web;computer security;internet of things	Visualization	-43.27814989323481	49.357689187105684	25061
0c402352a04c4a05a55fdfadaf42ad4280185faa	a thin client approach to supporting adaptive session mobility	mobile device;user interaction	Recent growth in computing devices from the smartphone to the desktop computer has led to users interacting with multiple computing devices throughout the course of the day. Modern computing sessions are a graphically rich, multi-tasking experience, representing a considerable amount of state. There is seldom the ability to automatically move a session from one device to another; instead users must manually restore applications to their previous state. Without session mobility, the problems of unsynchronised information and communication barriers become apparent. We present a thin client approach to supporting session mobility across a broad range of devices. We use an adaptive approach, thereby supporting a fine granularity of devices. Furthermore, our approach enables rich diverse sessions composed of applications specific to a range of platforms, reducing constraints imposed by mobile devices, and boosting productivity by allowing access to a wider range of applications from a single device.		Daniel MacCormac;Mark Deegan;Fredrick Mtenzi;Brendan O'Shea	2007		10.1007/978-3-540-72360-8_60	embedded system;real-time computing;human–computer interaction;computer science;operating system;mobile device;world wide web	HCI	-36.948398534616885	52.03062424545001	25102
d41054d74505ddf9ed2842e5346ef894895d1946	fog computing in healthcare–a review and discussion		Fog computing is an architectural style in which network components between devices and the cloud execute application-specific logic. We present the first review on fog computing within healthcare informatics, and explore, classify, and discuss different application use cases presented in the literature. For that, we categorize applications into use case classes and list an inventory of application-specific tasks that can be handled by fog computing. We discuss on which level of the network such fog computing tasks can be executed, and provide tradeoffs with respect to requirements relevant to healthcare. Our review indicates that: 1) there is a significant number of computing tasks in healthcare that require or can benefit from fog computing principles; 2) processing on higher network tiers is required due to constraints in wireless devices and the need to aggregate data; and 3) privacy concerns and dependability prevent computation tasks to be completely moved to the cloud. These findings substantiate the need for a coherent approach toward fog computing in healthcare, for which we present a list of recommended research and development actions.	aggregate data;boolean algebra;categorization;coherence (physics);computation;dependability;fog computing;informatics;requirement	Frank Alexander Kraemer;Anders Eivind Braten;Nattachart Tamkittikhun;David Palma	2017	IEEE Access	10.1109/ACCESS.2017.2704100	wireless sensor network;computer network;peer review;cloud computing;computer science;distributed computing;utility computing;use case;health informatics;edge computing;dependability	HPC	-46.74506240847783	48.28708808751847	25154
edf0ee361b47d0d20fd2b351e6e9a89d88d5fbff	sw-hw co-design and fault tolerant implementation for the lrid wireless communication system	application development;automated synthesis;casre tool;hdl;hardware software codesign;fault tolerant;casre tool sw hw co design fault tolerant lrid wireless communication system rf identification tag model checking safety critical software design nasa mission software based monitoring architecture firmware automated synthesis hdl esl method rtl design xtmr tool multilayered run time monitoring fault management fpga technology;wireless communication systems;design flow;hardware description languages;software architecture fault tolerant computing field programmable gate arrays formal verification hardware description languages hardware software codesign radiofrequency identification;firmware;sw hw co design;software architecture;fault tolerant computing;formal verification;esl method;model checking;multilayered run time monitoring;xtmr tool;rtl design;safety critical software;rf identification tag;fpga technology;lrid wireless communication system;field programmable gate arrays;software based monitoring architecture;fault management;nasa mission;radiofrequency identification;fault tolerant systems wireless communication system testing nasa monitoring radio frequency radiofrequency identification aerospace safety software safety software design;safety critical software design	This paper presents the development of a wireless communication system, the RF identification tag, built and tested in Heriot-Watt University, Edinburgh. The design flow commences in SPIN, a high level model-checking tool at present deployed towards the verification of safety critical software designs including NASA missions. The formally verified model of the application is then enhanced with software based monitoring architectures comparable with that applied in conventional firmware development such as the watchdog timer defending rational control related execution of the high level system representation. Following automated synthesis into hardware (HDL) with the aid of an ESL method, the generated RTL design can be further protected against increased levels of radiation and SEUs with the aid of the xTMR tool. It is claimed that a development route of this type promotes high levels of algorithmic testability and reliability attained via fault prevention means in the model checking process as well as multi-layered run-time monitoring and fault management strategies leveraging upon the design on the vertical implementation phase. The application developed in the proposed lifecycle and targeting the FPGA technology is finally tested under a lab emulated EMI scheme and system survivability is examined and quantified. Reliability is then estimated and analyzed in the CASRE tool (developed by JPL NASA)	emi;emulator;field-programmable gate array;firmware;formal verification;hardware description language;high-level programming language;model checking;quantifier (logic);radio frequency;shattered world;watchdog timer	Stefanos Skoulaxinos	2006	First NASA/ESA Conference on Adaptive Hardware and Systems (AHS'06)	10.1109/AHS.2006.68	model checking;embedded system;firmware;software architecture;fault tolerance;real-time computing;formal verification;computer science;design flow;operating system;fault management;hardware description language;rapid application development;register-transfer level;field-programmable gate array	EDA	-38.47559601959975	33.71371464878065	25192
e489af2f2dc2829b0118ffd1e7804b43f4206cbd	performance functions for qos prediction in web service composites	interpolation;polynomials;customer satisfaction;prediction methods;computational modeling;web services interpolation quality of service trusted computing;monitoring;web services;performance analysis;predictive models web services quality of service prediction methods performance analysis;predictive models;quality of service;computational trust qos prediction quality of service web service composition performance functions service level agreements sla inter component rating qos information performance function web service quality function interpolation;web services interpolation quality of service polynomials monitoring computational modeling customer satisfaction	We contribute performance functions to model the quality of service (QoS) of web services in horizontal composites. Performance functions are thereby used to formalize service level agreements (SLAs) to enable automated verification. Inter-Component rating is a model for gathering additional QoS information from web services that rate the output of their predecessor web services in composites. Furthermore, we contribute two approaches to verify if a web service adheres to its claimed QoS described with a performance function. For this verification, claimed performance is compared with reading points gathered from spot-testing the quality of the web service. The first approach we discuss is function interpolation, the second one applies methods from computational trust. The paper also discusses the advantages and drawbacks of both approaches as well as the influence of measuring errors.	computational trust;interpolation;is functions;quality of service;web service	Florian Volk;Johanna Sokoli;Max Mühlhäuser	2014	2014 IEEE International Conference on Web Services	10.1109/ICWS.2014.86	web service;service level requirement;service level objective;mobile qos;quality of service;interpolation;computer science;service delivery framework;data mining;database;predictive modelling;customer satisfaction;computational model;law;world wide web;polynomial	HPC	-50.00176065116506	43.9328386077369	25221
9bfb0b947fd9e955adbb2870748ddd7a718b7ad1	architecture-based autonomic deployment of j2ee systems in grids	enterprise system	The deployment of J2EE systems in Grid environments remains a difficult task: the architecture of these applications are complex and the targe t environment is heterogeneous, open and dynamic. In this paper, we sho w how the component-based approach simplifies the design, the deployment and th e reconfiguration of a J2EE system. We propose an extended architecture desc iption language that allows specifying the deployment of enterprise systems in ente rprise Grids, driven by resources and location constraints. With respect to the se constraints we present a deployment process that instantiates propagative ly the application, taking into account resources and hosts availability. Finally, we pr esent an autonomic solution for recovery from failures.	autonomic computing;autonomic networking;component-based software engineering;enterprise system;extensible storage engine;fractal;grid computing;heterogeneous computing;java platform, enterprise edition;mathematical optimization;quality of service;scott continuity;software deployment;web service	Didier Hoareau;Takoua Abdellatif;Yves Mahéo	2007		10.1007/978-3-540-72360-8_31	enterprise architecture framework;enterprise system;real-time computing;simulation;deployment diagram;computer science;architecture domain;applications architecture;enterprise architecture management;solution architecture;software deployment	HPC	-40.47926844706574	40.73012247067806	25230
d873d36ec72252f23b0fd4d1dc2f42543b0be905	sensing and actuation as a service delivery model in cloud edge centric internet of things		Abstract Internet of Things (IoT) is an advanced innovation of the Internet which allows communications among all living or non-living things. The smart devices have become powerful and intelligent that they can sense, communicate, compute and actuate among themselves to provide a smart environment. So, in short, it could be possibly known as the Internet of Everything (IoE). However, with the current architecture, the contributors are not motivated to share the sensed data or provide their actuators to others as a service. The security and various node managements are some of the main issues to be addressed for motivating the contributors. Therefore, we introduce “Sensing and Actuation as a Service Delivery Model (SAaaSDM)”, which is a cloud edge-centric service delivery model. It authorizes access to the IoT Architecture (IoT-A), where sensed, actuated, and computed data from various existing mobile devices can be used by the end user through SAaaSDM on a pay as you go fashion. Participatory node management, virtual node management, and quality review management are the emerging components of this architecture. Participatory nodes along with device owners claim for various challenges like cost, reliability, trustworthiness, quality, utility etc. Similarly, the expectations of the end users also appear as a big challenge. In this paper, we present the SAaaSDM system model which can deal with open issues and also discuss the future directions for the researchers in this field.	internet of things	Suchismita Satpathy;Bibhudatta Sahoo;Ashok K. Turuk	2018	Future Generation Comp. Syst.	10.1016/j.future.2018.04.015	the internet;service delivery framework;end user;architecture;distributed computing;computer science;system model;mobile device;cloud computing;smart environment	Metrics	-43.549407542208094	49.462934717458346	25241
72d02bb50fee9bfb6f4d0b4b65f781d473287715	temporal dependency based checkpoint selection for dynamic verification of fixed-time constraints in grid workflow systems	temporal dependency checkpoint selection fixed time constraints grid workflows;silicon;temporal dependency based checkpoint selection;grid workflows;fixed time;swinburne;program verification;data mining;program verification checkpointing grid computing;fixed time constraints;checkpointing;checkpoint selection;temporal dependency;computational modeling;workflow system;redundancy;monitoring;temporal correctness;overall temporal verification temporal dependency based checkpoint selection dynamic verification fixed time constraints grid workflow systems temporal correctness grid workflow execution checkpoint selection strategy;grid workflow execution;overall temporal verification;checkpoint selection strategy;grid workflow systems;optical wavelength conversion;grid computing;meteorology;monitoring and control;dynamic verification	In grid workflow systems, temporal correctness is critical to assure the timely completion of grid workflow execution. To monitor and control the temporal correctness, fixed-time constraints are often assigned to a grid workflow and then verified. A checkpoint selection strategy is used to select checkpoints along grid workflow execution for verifying fixed-time constraints. The problem of existing representative strategies is that they do not differentiate fixed-time constraints as once a checkpoint is selected, they verify all fixed-time constraints. However, these checkpoints do not need to be taken for those constraints whose consistency can be deduced from others. The corresponding verification of such constraints is consequently unnecessary and can severely impact the efficiency of overall temporal verification. To address the problem, in this paper, we develop a new temporal dependency based checkpoint selection strategy which can select checkpoints according to different fixed-time constraints. With our strategy, the corresponding unnecessary verification can be avoided. The comparison and experimental simulation further demonstrate that our new strategy can improve the efficiency of overall temporal verification significantly over the existing representative strategies.	application checkpointing;consistency (database systems);correctness (computer science);simulation;software verification;transaction processing system;verification and validation	Jinjun Chen;Yun Yang	2008	2008 ACM/IEEE 30th International Conference on Software Engineering	10.1145/1368088.1368108	real-time computing;computer science;database;distributed computing;redundancy;silicon;computational model;grid computing	SE	-49.73946317931877	39.7848986259083	25258
dfda5505994cce17f6da830a3c1e399041f73720	security as an add-on quality in persistent object systems		"""•In System security services like authentication, access control and auditing are becoming increasingly critical for information systems particularly in distributed heterogeneous environments. Since information system archi­ tectures are moving rapidly from centralized, grand unifying architectures towards open, service-oriented and communication-based environments (""""Persistent Object Systems"""") constructed with well-organized compo­ nent technologies it is essential that such structural changes are reflected adequately in the architecture of system security services. In this paper we present an open, library-based approach to the se­ curity of Persistent Object Systems which generalizes and unifies the protection mechanisms that traditionally come bundled with database, communication or operating system services. More specifically, we il­ lustrate how polymorphic typing can be exploited to abstract from par­ ticular commercially available security services, such as Kerberos, and how higher-order functions allow the user to add value to existing secu­ rity services. Furthermore, we demonstrate how higher-order functions, first-class modules and reflection provide a technical framework for the realization of domain-specific security policies and for the systematic con­ struction of security-enhanced activities."""		Andreas Rudloff;Florian Matthes;Joachim W. Schmidt	1994		10.1007/978-1-4471-3577-7_7	computer science;distributed computing;world wide web;computer security	Theory	-47.24629029716465	52.22986921056474	25259
dfc2d190d3b276270af9c177a7c654adff357f62	adaptive agents for cyber-physical systems		This paper proposes a bio-inspired approach to adapting software components in CPSs. It introduces the notions of differentiation and dedifferentiation in cellular slime molds. When a software component delegates a function to another component coordinating with it, if the former has the function, this function becomes lessdeveloped and the latter’s function becomes well-developed like that in cellular differentiation. The approach enables software components on CPSs to be naturally adapted to changes in the cyber and physical world in a self-organizing manner. It is constructed as a middleware system to execute general purpose applications on a CPS. We present several evaluations of the approach in CPSs.	british informatics olympiad;component-based software engineering;cyber-physical system;delegate (cli);java;middleware;organizing (structure);self-organization	Ichiro Satoh	2013			data mining;computer science;computer engineering;cyber-physical system	SE	-38.61801388258191	43.882025561011325	25321
c6f3d0538b61b3bab38865290d9c1c431ff7240c	interpretation of information processing regulations	personal identifiable information;software requirement;information processing;regulation;laws;privacy	Laws and policies impose many information handling requirements on business practices. Compliance with such regulations requires identification of conflicting interpretations of regulatory conditions. Current software engineering methods extract software requirements by converting legal text into semiformal constraints and rules. In this paper we complement these methods with a state-based model that includes all possibilities of information flow. We show that such a model provides a foundation for the interpretation process.	information flow (information theory);information processing;requirement;software engineering;software requirements	Sabah S. Al-Fedaghi	2009	JSEA	10.4236/jsea.2009.22011	personally identifiable information;regulation;information processing;computer science;data mining;management science;privacy;computer security;software requirements	SE	-52.39583548385834	50.96890625870192	25334
546a4b5f406836b4aa988e8fcb20cb947847a10c	visual components for pervasive computing management	pervasive computing;graphical user interfaces;context aware service;ubiquitous computing graphical user interfaces;component framework;smart spaces;ubiquitous computing;document editing pervasive computing management visual component visual interfaces context aware service smart spaces compound document technology location sensing system user friendly gui based manipulation;visual interfaces;pervasive computing context aware services space technology context modeling radiofrequency identification global positioning system embedded computing physics computing rfid tags informatics	This paper presents a component framework for building and operating visual interfaces for context-aware services in smart spaces. By using a compound-document technology, it provides physical entities, places, and services in smart spaces with visual components to annotate and control them. It can automatically assemble visual components into a visual interface for monitoring and managing the spaces according to the spatial containment relationships between their targets in the physical world by using underlying location-sensing systems. End-users can manually customize smart spaces through user-friendly GUI-based manipulations for editing documents. This paper presents the design for this framework and describes its implementation and practical application.	context-aware network;entity;graphical user interface;java;location awareness;prototype;smart tv;ubiquitous computing;usability	Ichiro Satoh	2007	IEEE International Conference on Pervasive Services	10.1109/PERSER.2007.4283885	context-aware pervasive systems;human–computer interaction;computer science;graphical user interface;database;smart environment;world wide web;ubiquitous computing	Robotics	-38.95985220118681	49.31958735797109	25426
f5fe00f3d86e81ef44124db5eeee460bd82ef6f2	threat modeling for services in cloud	cloud security;threat modeling;cloud services;cloud computing	Cloud computing offers various services ranging from infrastructure to computation, software and application. These services have gained popularity in both public and enterprise domains, and they process large amount of user data with varying privacy levels. However, due to the dynamic nature of cloud services, many enterprise level security policies, standards and practices cannot be implemented in cloud which leads to different security threats. These threats can be exploited by various attackers to compromise the cloud services. In this paper threat modeling for cloud services has been done by considering various attackers such as hackers, malicious administrators, malicious users and service providers. After describing possible threats to cloud services from these attackers, methodologies to exploit those threats have been presented. Moreover, the generalization of threat model has been done to determine the threats related to a specific service functionality for various attackers in cloud.	automated reasoning;cloud computing;computation;malware;threat (computer);threat model	Muhammad Kazim;David Evans	2016	2016 IEEE Symposium on Service-Oriented System Engineering (SOSE)	10.1109/SOSE.2016.55	cloud computing security;cloud computing;computer science;operating system;services computing;internet privacy;world wide web;computer security	Security	-49.85056689655004	57.99231125552161	25559
7503fe364063ea594a73047748483d62e9fb2053	ftmes: a failed-test-oriented mutant execution strategy for mutation-based fault localization		Fault localization has been one of the most manual and costly software debugging activities. The spectrum-based fault localization is the most studied and evaluated fault localization approach. Mutation-based fault localization is a promising approach but with a high computational cost. We propose a novel mutation execution strategy named Failed-Test Oriented Mutant Execution Strategy (FTMES) for improving the efficacy of fault localization techniques and also reduce the computational cost of these techniques. Our proposed approach and eight other baselines were evaluated against 221 real faults. The results show that FTMES outperformed others with respect to efficiency (computational cost) while maintaining similar accuracy.		André Assis Lôbo de Oliveira;Celso Gonçalves Camilo-Junior;Eduardo Noronha de Andrade Freitas;Auri Marcelo Rizzo Vincenzi	2018		10.1109/ISSRE.2018.00026	reliability engineering;real-time computing;mutant;debugging;software;computer science	Logic	-60.060485521436455	36.02011645468006	25560
8418bdb2169d72bbe6e16da34e41aae3f7854cc9	principles of data flow integrity: specification and enforcement	dynamic enforcement;data flow integrity;reference monitor;data flow analysis;security policy	Subverting runtime data flow is common in many current software attacks. Data Flow Integrity (DFI) is a policy whose satisfaction can prevent such attacks. This paper develops a formal foundation on DFI specification, and characteristics of its enforcement techniques with formulations of hypotheses and guarantees. Enforcement techniques are based on static analysis and program monitoring at runtime. This foundation can be used for practical satisfaction of DFI and help establish guarantees in every applied platform.	dataflow architecture;run time (program lifecycle phase);static program analysis;type safety	Toktam Ramezanifarkhani;Mohammadreza Razzazi	2015	J. Inf. Sci. Eng.		computer science;security policy;data-flow analysis;database;programming language;computer security	Security	-53.88534369120492	52.38408055555974	25572
20592e310c4a2b90c4cff06cce5437962453827d	co-transformation to cloud-native applications - development experiences and experimental evaluation		Modern software applications following cloud-native design principles and architecture guidelines have inherent advantages in fulfilling current user requirements when executed in complex scheduled environments. Engineers responsible for software applications therefore have an intrinsic interest to migrate to cloud-native architectures. Existing methodologies for transforming legacy applications do not yet consider migration from partly cloud-enabled and cloud-aware applications under continuous development. This work thus introduces a co-transformation methodology and validates it through the migration of a prototypical music identification and royalty collection application. Experimental results demonstrate that the proposed methodology is capable to effectively guide a transformation process, resulting in elastic and resilient cloud-native applications. Findings include the necessity to maintain application self-management even on modern cloud platforms.		Josef Spillner;Yessica Bogado;Walter Benítez;Fabio López-Pires	2018		10.5220/0006790305960607	psychological resilience;distributed computing;database;computer science;elasticity (economics);cloud computing	SE	-44.80126050768524	41.0760413763984	25580
c10844f065f7942efefae997baaac353f46caf89	applying context-awareness to service-oriented architecture	context aware;context aware services service oriented architecture context awareness intelligent sensors ontologies context modeling owl microelectronics partial response channels semantic web;data acquisition service oriented architecture context awareness system ontology based context model domain analyst oriented context reasoning rules semantic web rule language;service selection;service in context ontology based context model rule transformation service selection indoor context acquisition;semantic web rule language;ontologies artificial intelligence;context model;service in context;rule transformation;cognition;mobile communication;semantic web;indoor context acquisition;ontology based context model;ubiquitous computing;ontologies;ubiquitous computing ontologies artificial intelligence semantic web;service oriented architecture;context aware systems;context modeling;domain analyst oriented context reasoning rules;data acquisition;context;context awareness system;context aware services	Service-oriented architecture (SOA) is a new paradigm, where it is imperative to automatically adapt to the changing context in the environment. This paper applies context awareness to service-oriented architecture to build reusable, adaptable, spontaneous and flexible context-awareness systems. We develop an ontology based context model, and domain-analyst oriented context reasoning rules, which are automatically translated to SWRL (Semantic Web Rule Language) to derive active situations. A mapping mechanism is developed to intelligently and dynamically select services based on current situations. Experiments are performed to evaluate the stability of data acquisition, impact of data load, communication distances and the number of rules.	artificial intelligence;bluetooth;context awareness;data acquisition;experiment;imperative programming;jess;programming paradigm;semantic web rule language;semantic reasoner;service-oriented architecture;service-oriented device architecture;spontaneous order	Lian Yu;Yang Yang;Yongchao Gu;Xu Liang;Shan Luo;Frank Tung	2009	2009 IEEE International Conference on e-Business Engineering	10.1109/ICEBE.2009.62	computer science;knowledge management;artificial intelligence;database;context model;world wide web;ubiquitous computing	Robotics	-42.47094587767261	44.015109837165724	25613
b34a2a5b7de65495d221196ab9efef3f7b9b002a	the power of powershell: examples of how powershell scripts can supplement a patch management system to solve unusual problems		Lehigh University currently utilizes the Flexera Corporate Software Inspector (CSI, formerly Secunia CSI) application to patch faculty/staff computers on campus. This system patches third-party applications on all currently supported Windows operating systems by leveraging the Windows Server Update Service (WSUS). This presentation is not meant to focus on a particular patch management product or system, but instead it will discuss how such a system can be used to solve unusual problems when coupled with a powerful scripting language such as PowerShell.  This presentation will focus on specific scenarios that PowerShell solved when combined with our patch management system. The scenarios it will present include: an update to our internet browser software, Mozilla Firefox, which when updated disabled our ad-blocking extension, an issue of unknown origin which caused all of our Windows 7 systems to no longer activate utilizing our Key Management Service (KMS) (this included Microsoft Office products), a glitch in Windows 7 which caused garbage files to generate non-stop and completely fill end user hard drives, and our rather unusual implementation of Java. PowerShell saved us immeasurable time in resolving these issues across our entire campus. Using PowerShell with our patch management solution allows us to resolve almost any issue, campus-wide, with the push of a button.	blocking (computing);computer;firefox;glitch;hard disk drive;java;key management;management system;microsoft windows;operating system;powershell;scripting language	Timothy Palumbo	2017		10.1145/3123458.3123479	the internet;end user;microsoft office;computer science;garbage;software;operating system;windows server;scripting language;java	OS	-59.019843784261106	53.02952322684772	25710
4fe4e2f7d76386ee007acd9ea42a7949e221abdf	from stateflow simulation to verified implementation: a verification approach and a real-time train controller design	software packages automata runtime junctions switches monitoring analytical models;safety critical software automata theory control system synthesis hardware description languages program compilers program debugging rail traffic control real time systems;runtime verification;physical execution environment stateflow simulation verification approach real time train controller design model driven development mdd industrial software systems simulink based development code generation physical execution platforms safety critical applications stateflow based model self contained toolkit timed automata uppaal simulink design verifier nonintrusive vhdl c code software monitor simulink polyspace stateflow based mdd safety critical properties;model driven development;timed automaton;formal verification;simulink stateflow	Simulink is widely used for model driven development (MDD) of industrial software systems. Typically, the Simulink based development is initiated from Stateflow modeling, followed by simulation, validation and code generation mapped to physical execution platforms. However, recent industrial trends have raised the demands of rigorous verification on safety-critical applications, which is unfortunately challenging for Simulink. In this paper, we present an approach to bridge the Stateflow based model driven development and a well- defined rigorous verification. First, we develop a self- contained toolkit to translate Stateflow model into timed automata, where major advanced modeling features in Stateflow are supported. Taking advantage of the strong verification capability of Uppaal, we can not only find bugs in Stateflow models which are missed by Simulink Design Verifier, but also check more important temporal properties. Next, we customize a runtime verifier for the generated nonintrusive VHDL and C code of Stateflow model for monitoring. The major strength of the customization is the flexibility to collect and analyze runtime properties with a pure software monitor, which opens more opportunities for engineers to achieve high reliability of the target system compared with the traditional act that only relies on Simulink Polyspace. We incorporate these two parts into original Stateflow based MDD seamlessly. In this way, safety-critical properties are both verified at the model level, and at the consistent system implementation level with physical execution environment in consideration. We apply our approach on a train controller design, and the verified implementation is tested and deployed on a real hardware platform.	automata theory;code generation (compiler);model-driven engineering;polyspace;real-time transcription;simulation;simulink;software bug;software system;stateflow;timed automaton;uppaal;vhdl	Yu Jiang;Yixiao Yang;Hao Liu;Hui Kong;Ming Gu;Jia-Guang Sun;Lui Sha	2016	2016 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS)	10.1109/RTAS.2016.7461337	embedded system;computer architecture;real-time computing;formal verification;computer science;operating system;runtime verification;programming language;timed automaton	Embedded	-44.019859766236245	32.91657318817194	25726
612a6bbef52a6e9027fee30af8acabc4fd30d61c	compositional scheduling analysis using standard event models		The ever increasing advances in silicon and communication technology allow the integration of increasing functionality in digital embedded systems, ranging from mobile phones through multimedia home platforms to automotive control networks. Embedded system designers have to cope with this increasing system complexity whilst still building competitive systems to survive the market pressure, thus requiring increasing productivity. Component and sub-system specialization and re-use provide this productivity, but system integration is becoming a new bottleneck because the resulting heterogeneous system must be analyzed and verified. This thesis focuses on the performance and timing verification aspect. Many embedded systems are real-time systems and must meet a variety of timing requirements, such as deadlines and limited load or bandwidth, under all possible corner-case operating conditions. But verification is difficult as timing properties depend heavily on interactions between tasks in the system and on the scheduling of individual tasks and communications. Unfortunately, the current practice of specialization and re-use results in increasingly heterogeneous systems, which specifically complicates the scheduling analysis problem. Even today’s best practice of timed simulation is increasingly unreliable, mainly because the corner cases are extremely difficult to find and debug, and it is even more difficult to find simulation patterns to cover them all. As an alternative, formal scheduling analysis techniques systematically analyze corner cases and provide guaranteed bounds on certain timing properties. Many approaches have been proposed, but most of them are limited to subproblems, such as a single operating system. They ignore the complex, heterogeneous nature of today’s applications and architectures, and they cannot be reasonably combined because they use different underlying analysis models. Few approaches target more complex and heterogeneous systems and have their application in specific areas, such as network processor design. These approaches, however, are often too unwieldy and complex to be accepted or iv COMPOSITIONAL SCHEDULING ANALYSIS understood by the embedded system industry. We summarize that none of the existing techniques is sufficiently general to comprehensively cover arbitrary heterogeneous systems. The main objective of this thesis is to develop a scheduling analysis procedure that a) can cope with the increasing complexity and heterogeneity of embedded systems, b) provides the modularity and flexibility that the established, re-use driven system integration style requires, and c) facilitates system integration using a comprehensible analytical model. In this thesis, a novel, structured analysis model is presented, that elegantly captures the system-level component interactions using intuitive event models. These event models represent the interfaces between different components, and the clear interface structure allows their efficient adaptation to different analytical models. Several previously incompatible component and sub-system analysis techniques can now –for the first time– be heterogeneously combined into a system-level analysis. This allows the modular integration of heterogeneous system parts, whilst providing designers with the flexibility to use their preferred local techniques without compromising global scheduling analysis. Based on this structured analysis model, we define a sound analysis procedure that consists of few comprehensible steps. The procedure is general enough that it can also detect and solve subtle design pitfalls, such as scheduling anomalies or cyclic dependencies, which are virtually impossible to find using simulation. The sound view on the component interactions substantially improves designers’ understanding of the key integration problems, and allows them to influence and control these interactions to optimize the system globally. The application of the approach is demonstrated in detail using a variety of expressive examples and experiments. As the new analysis procedure can be efficiently applied in practice, it provides a serious and promising complement to simulation. It allows comprehensive system integration and optimization, and it provides much more reliable performance analysis results, at the same time requiring far less computation time.	best practice;computation;corner case;embedded system;emoticon;experiment;interaction;mathematical optimization;mobile phone;network processor;operating system;partial template specialization;processor design;real-time clock;real-time computing;requirement;scheduling (computing);simulation;static timing analysis;structured analysis;system analysis;system integration;time complexity;while	Kai Richter	2005			real-time computing;philosophy;performance art	Embedded	-41.55102335028251	35.598196738584676	25739
3cc1576e84af918982dc2166c3a2f749da36a240	autonomous service-restoration in smart distribution grids using multi-agent systems	distributed power generation;negotiation activities smart distribution grids multiagent systems active electric distribution grids smart grids sg fault tolerance service availability mas interconnected distributed energy resources der highly distributed mas approach autonomous service restoration single facilitator device single points of failure avoidance island forming recombination situation;generators;power distribution faults;distribution networks;smart grids automation multiagent systems generators computer architecture switches;fault restoration multi agent systems distribution networks smart grids service restoration;smart power grids distributed power generation energy resources fault tolerance multi agent systems power distribution faults power engineering computing;computer architecture;smart grids;multi agent systems;power engineering computing;smart power grids;energy resources;fault tolerance;service restoration;switches;multiagent systems;fault restoration;automation	Future active electric distribution grids, commonly referred to as Smart Grids (SG), impose new requirements on fault tolerance and service availability. The system must be able to meets its designed objectives to the greatest possible extent without human guidance. Multi-Agent Systems (MAS) are generally considered a promising way for handling a large number of different interconnected Distributed Energy Resources (DER) and their services in the context of SGs, as they do not require an explicit model of the overall system behaviour. This paper describes a highly distributed MAS approach for autonomous service restoration that does not rely on a single facilitator device, hence avoiding single-points-of-failure. The design is exemplified by a test case describing an island forming and recombination situation showing the negotiation activities between the agents.	advanced intelligent tape;agent architecture;autonomous robot;circuit restoration;content-control software;electric power quality;fault tolerance;ibm power systems;lunar lander challenge;microsoft outlook for mac;multi-agent system;power supply;real-time transcription;requirement;single point of failure;smart tv;suicidegirls;test case	Alexander Prostejovsky;Wilfried Lepuschitz;Thomas I. Strasser;Munir Merdan	2012	2012 25th IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)	10.1109/CCECE.2012.6334862	control engineering;embedded system;fault tolerance;network switch;computer science;engineering;artificial intelligence;automation;multi-agent system;distributed computing;smart grid	Robotics	-35.636905206458735	35.54929461972959	25741
458438d9d2f824bf10d79d0849bf6764d96dd41e	a regression test case prioritization algorithm based on program changes and method invocation relationship		Regression testing is essential for assuring the quality of a software product. Because rerunning all test cases in regression testing may be impractical under limited resources, test case prioritization is a feasible solution to optimize regression testing by reordering test cases for the current testing version. In this paper, we propose a new test case prioritization algorithm based on program changes and method (function) invocation relationship. Combining the estimated risk value of each program method (function) and the method (function) coverage information, the fault detection capability of each test case can be calculated. The algorithm reduces the prioritization problem to an integer linear programming (ILP) problem, and finally prioritizes test cases according to their fault detection capabilities. Experiments are conducted on 11 programs to validate the effectiveness of our proposed algorithm. Experimental results show that our approach is more effective than some well studied test case prioritization techniques in terms of average percentage of fault detected (APFD) values.	algorithm;benchmark (computing);experiment;fault detection and isolation;greedy algorithm;integer programming;linear programming;out-of-order execution;overhead (computing);regression testing;static program analysis;subroutine;test case;whole earth 'lectronic link	Wenhao Fu;Huiqun Yu;Guisheng Fan;Xiang Ji;Xin Pei	2017	2017 24th Asia-Pacific Software Engineering Conference (APSEC)	10.1109/APSEC.2017.23	regression testing;computer science;test case;algorithm;fault detection and isolation;software;integer programming;prioritization;greedy algorithm	SE	-60.5181422844569	35.550153242351094	25788
ae8a8e950766a6dccdf712863ed18e4ebd64618c	conditional information flow policies and unwinding relations	new policy;noninterference-based property;user b;conditional noninterference;conditional information flow policy;information flow;integrity security property;unwinding relation;useful verification technique;action channel;information flow policy	Noninterference provides a control over information flow in systems for ensuring confidentiality and integrity security properties. In general, user A is not allowed to interfere with user B if A's behaviour cannot cause any difference in B's observation. Unwinding relations are useful verification techniques for noninterference-based properties. This paper defines a framework for the notion of conditional noninterference, which allows to specify information flow policies based on the semantics of action channels. To verify the properties, we present unwinding relations that are both sound and complete for the new policies.	conditional entropy;information flow;loop unrolling	Chenyi Zhang	2011		10.1007/978-3-642-30065-3_14	computer science;computer security;algorithm	OS	-54.14136557008099	52.154553075550155	25834
082ac94c5e60c7adff318501892c75104c7eb295	improved biba model based on trusted computing	security model;nested interval theorem;trusted computing;biba model;low water mark policy	Biba model is hard to implement because the rules are too strict to meet the flexibility of system. To enhance the flexibility, the low-water-mark policy based on the Biba model is proposed by supporting the dynamic change of subject tags. However, the biggest drawback of low-water-mark policy is that the integrity level of the subjects in a system decreases monotonously, which results that some subjects cannot access most of the objects and the system life cycle is cut down. An improved model is proposed based on the Biba model, which not only describes the infection degree of subjects by separating the subject into uninfected and infected subjects and introducing the confidence interval but also reduces the decline rate of integrity level of the subject and prolongs the life time cycle by adopting trusted computing to adjust subject tags. Theory analysis and experiment show that the improved model enhances the availability of system. Copyright © 2015 John Wiley & Sons, Ltd.	biba model;john d. wiley;mandatory integrity control;software bug;system lifecycle;trusted computing	Gang Liu;Jing Zhang;Jinhui Liu;Yuan Zhang	2015	Security and Communication Networks	10.1002/sec.1201	nested intervals;computer security model;simulation;computer science;trustworthy computing;computer security	Security	-47.75749560031504	40.97491781314976	25867
436ac3646af7a009866cfbf4614023f9fc08d7c5	physicalnet: cross-network applications for multi-user sensor and actuator networks	hand held device;human interaction;high temperature;power efficiency;multi user;satisfiability;cyber physical systems;wireless communication;sensors and actuators	Processors keep increasing the speed at which they can execute program instructions. The bandwidth available for both wired and wireless communications keeps growing. Specialized hardware such as sensors, actuators, and hand-held devices keep improving their power efficiency and size factor. Consequently, we can expect that computers, communication infrastructures, and specialized hardware will interact more and more to form complex Cyber-physical Systems (CPSs). CPSs will revolutionize the way humans interact with the physical world. CPSs will allow the programming of applications involving globally accessible sensors and actuators, applications that are not only efficient but also robust and secure. In this essay, we focus on describing a system satisfying a possible set of CPS requirements that we motivate in the first section. As a running example, we consider an application named FireAlarm that monitors the temperature in buildings and raises the alarm in case of abnormally high temperatures.	as-interface;bandwidth (signal processing);computer;cyber-physical system;mobile device;multi-user;performance per watt;requirement;sensor	Pascal Vicaire;John A. Stankovic	2008	SIGBED Review	10.1145/1366283.1366301	embedded system;interpersonal relationship;real-time computing;simulation;electrical efficiency;computer science;operating system;cyber-physical system;wireless;satisfiability	Mobile	-36.77671831007682	54.35982428506577	25892
722276a04f763a9a264e8507cc86fd2c5400020e	rfid in underground-mining service applications	passive tags;radiofrequency identification active rfid tags mining industry tagging passive rfid tags rfid tags;instrumentation;mining;pervasive computing mining rfid passive tags active tags;pervasive computing;radiofrequency identification mining industry;mining industry underground mining service applications rfid radiofrequency identifcation;mining industry;rfid;active tags;radiofrequency identification	This overview of RFID in underground mining service applications is set against the backdrop of the major challenges this technology must overcome for successful implementation in rugged environments. It also considers future prospects of RFID in the mining industry.	backdrop cms;radio-frequency identification;rugged computer;underground	P. K. Mishra;Ron F. Stewart;Miodrag Bolic;Mustapha Chérif-Eddine Yagoub	2014	IEEE Pervasive Computing	10.1109/MPRV.2014.14	mining;human–computer interaction;computer science;data mining;world wide web;computer security;ubiquitous computing	Visualization	-42.24809416634242	50.92357171867015	25903
365f68b722c6a17796d9121ef46127e255e3d900	geo-coding in smart environment: integration principles of smart-m3 and geo2tag		Geo-tagging and smart spaces are two promising directions in modern mobile market. Geo-tagging allows to markup any kind of data by geographical coordinates and time. This is the basis for defining geographical context which can be used in different types of applications e.g. semantic information search, machine-to-machine (M2M) interactions. Smart spaces as the basis for seamless distributed communication field for software services provides semantic level for data processing. Most desired feature of coming software is pro-activeness and context awareness, i.e. services will be able to adapt to the user’s needs and situations and be able to manage decisions and behaviors on behalf of the user. The paper is dedicated discussion of integration most popular open platforms for smart spaces and geo-tagging (Smart-M3 and Geo2Tag) as possible solution for creation context aware proactive location based (LBS) services.	smart environment;smart-m3	Kirill Krinkin;Kirill Yudenok	2013		10.1007/978-3-642-40316-3_10	smart-m3;human–computer interaction;markup language;software;geocoding;geotagging;computer science;data processing;smart environment;context awareness	Logic	-41.693538640370114	46.089918013249424	25920
a82f7d131eb86a7efafd3e170113a392fb7bbaa4	time-triggered ethernet metamodel: design and application		The combination of the SAE Time Triggered Ethernet (TTEthernet) standard with the Integrated Modular Avionics (IMA) architectures supports the design, deployment and integration of mixed-critical avionic applications. In order to cope with the complexity of these tasks, we advocate for a model-driven engineering methodology. The key element of such methodology is the modeling language, which enables producing relevant models of the system. In this paper, we present a metamodel, which captures the main features and concepts defined in the SAE TTEthernet standard. We discuss how a combination of the TTEthernet metamodel with an IMA metamodel can be used to extend the AADL modeling language to model avionic applications deployed a TTEthernet-networked IMA platform. Finally, we present a case study to illustrate our approach.	integrated modular avionics;metamodeling;model-driven engineering;modeling language;sae j1939;selective area epitaxy;software deployment	Robati Tiyam;Amine El Kouhen;Abdelouahed Gherbi;John Mullins	2016	JSW	10.17706/jsw.11.10.1040-1053	parallel computing;real-time computing;computer network	SE	-46.78129096510145	37.606867620214004	25932
a130931df20f706b52b6d29bc70d7c95fc69e619	an overview of the nist cloud computing program and reference architecture		Cloud computing is the next step in the continued evolution of information systems. Cloud computing allows consumers to choose what service they want, how the services will be delivered, and provides usage based. The resource pooling and rapid provisioning of cloud services allow providers to more efficiently supply these resources. This results in the consumers’ needs being better met while at the same time using fewer resources (both physical assets and energy). To achieve these goals a better understanding of the implications of cloud computing along with interoperability, portability, and security standards is needed. The National Institute of Standards and Technology (NIST) has been tasked to help drive adoption of cloud computing by federal agencies through the identification and resolution of high-priority interoperability, portability and security issues.	cloud computing;reference architecture	Eric Simmon;Robert B. Bohn	2012		10.1007/978-1-4471-4426-7_94	operating system;software engineering;database	Arch	-42.54097489669245	55.17941998649824	25942
d27df104ae418062bc01fa1de397b71bfe112c36	specifying reusable security requirements	security engineering;security requirements;functional requirement	Unlike typical functional requirements, security requirements can potentially be highly reusable, especially if specified as instances of reusable templates. In this column, I will discuss the concepts underlying security engineering including its quality subfactors. I will then address the issue of security requirements and how they differ from the architectural mechanisms that will fulfill them. Then, I will discuss the value of reusable parameterized templates for specifying security requirements and provide an example of such a template and its associated usage. Finally, I will outline an asset-based riskdriven analysis approach for determining the appropriate actual parameters to use when reusing such parameterized templates to specify security requirements. 1 CONCEPTS UNDERLYING SECURITY ENGINEERING To specify security requirements, it is critical to first understand the concepts underlying security engineering. And the most important concept of these is ‘security’ itself. Whereas security is often defined as an incomplete subset of its most important quality subfactors (e.g., integrity and privacy), the following figure illustrates that a more general and complete definition of security is that it is the degree to which malicious (i.e., unauthorized and intentional) harm to valuable system assets is prevented, reduced, and properly responded to. Thus, security is about protecting these assets (e.g., data, services, hardware, and personnel) from harm due to various kinds of attacks (e.g., password sniffing, spoofing, viruses) that may be mounted by the various kinds of attackers (e.g., hackers, crackers, disgruntled employees, international cyber-terrorists, industrial spies, governmental spies, foreign military, etc.). These assets are at risk due both to various kinds of threats (e.g., theft, vandalism, unauthorized disclosure, destruction, fraud, extortion, espionage, trespass, etc.) of attack as well as the vulnerabilities the system may 1 Some may argue that the term ‘malicious’ is too strong. What about people who vandalize the website of a company that pollutes the environment? What about someone who uses company computers to surf the Web in violation of company policy. The first example is a cybercrime and the second is an unauthorized use of property. In both cases, the victims would be justified to consider these acts malicious. If the term ‘malicious’ still seems too harsh, just consider it to mean the combination of unauthorized and intentional. SPECIFYING REUSABLE SECURITY REQUIREMENTS 62 JOURNAL OF OBJECT TECHNOLOGY VOL. 3, NO. 1 have. Security requirements are engineered to specify the system’s security policies and both policies and requirements should address these security risks. Security mechanisms (e.g., user IDs, passwords, encryption, firewalls, antivirus software, intrusion detection systems, etc.) are then architected to fulfill the security requirements. Some of these concepts influence the engineering of security requirements (e.g., policies, risks, threats, and assets), whereas others (e.g., security mechanisms, security vulnerabilities, and attacks) are influenced by the security requirements. Fig. 1: Concepts that Influence and are Influenced by Security Requirements The following list defines these security-oriented terms that will be used during the remainder of this column:	antivirus software;authorization;computer virus;correctness (computer science);cybercrime;encryption;extensibility;firewall (computing);functional requirement;internationalization and localization;interoperability;intrusion detection system;malware;norm (social);password;privacy;requirements analysis;security engineering;software portability;software testability;speeded up robust features;spoofing attack;usability;vulnerability (computing);world wide web	Donald Firesmith	2004	Journal of Object Technology	10.5381/jot.2004.3.1.c6	computer security model;reliability engineering;security engineering;computer science;systems engineering;engineering;security testing;computer security;functional requirement	Security	-51.25972135780364	59.526214680659756	26030
f883d091e60e810ed69b3cf91ae17a8b6ef2ef57	inviwo - a visualization system with usage abstraction levels		The complexity of todays visualization applications demands specific visualization systems tailored for the development of these applications. Frequently, such systems utilize levels of abstraction to improve the application development process, for instance by providing a data flow network editor. Unfortunately, these abstractions result in several issues, which need to be circumvented through an abstraction-centered system design. Often, a high level of abstraction hides low level details, which makes it difficult to directly access the underlying computing platform, which would be important to achieve an optimal performance. Therefore, we propose a layer structure developed for modern and sustainable visualization systems allowing developers to interact with all contained abstraction levels. We refer to this interaction capabilities as usage abstraction levels, since we target application developers with various levels of experience. We formulate the requirements for such a system, derive the desired architecture, and present how the concepts have been exemplary realized within the Inviwo visualization system. Furthermore, we address several specific challenges that arise during the realization of such a layered architecture, such as communication between different computing platforms, performance centered encapsulation, as well as layer-independent development by supporting cross layer documentation and debugging capabilities.	computation (action);contain (action);data flow diagram;dataflow;debugging;documentation;encapsulation (networking);flow network;high-level programming language;imagery;principle of abstraction;requirement;systems design;anatomical layer	Daniel K. Jonsson;Peter Steneteg;Erik Sund'en;Rickard Englund;Sathish Kottravel;Martin Falk;Anders Ynnerman;Ingrid Hotz;Timo Ropinski	2018	CoRR			HPC	-43.182239853136174	36.2842456842777	26111
a6e5bbc9eea80a95e4c86866118a2b3b38e1b222	programming storage-centric sensor networks with squirrel	sensor network	We present Squirrel, a stream-oriented programming framework for storage-centric sensor networks. The storage-centric paradigm---where storage operations prevail over communication activity---applies to scenarios such as batch data collection, delay-tolerant mobile applications, and disconnected operations in static networks. Squirrel simplifies developing such applications by decoupling data processing from storage, and by transparently handling the latter. We achieve this through: i) a modular programming abstraction, and ii) a lightweight run-time layer that efficiently allocates data to different storage areas, based on size vs. energy tradeoffs. We demonstrate Squirrel's effectiveness based on three real-world applications, each representing a different storage-centric scenario. The results show that---while relieving programmers from a significant burden---Squirrel achieves efficient utilization of storage areas, enabling energy savings independently of the storage technology.	coupling (computer programming);mobile app;modular programming;programmer;programming paradigm;squirrel	Luca Mottola	2010		10.1145/1791212.1791214	embedded system;real-time computing;wireless sensor network;computer science;operating system;distributed computing	OS	-37.064536811936726	45.76278168058664	26133
89043f4c06b35c17dfa87248ae7eea6a53aeb18a	virtual and augmented reality: improved user experience through a service oriented infrastructure	service orientation;real time;collaboration;virtual reality;industries;collaborative environment;information and communication technology;monitoring;streaming media;shared multimedia content augmented reality user experience service oriented infrastructure virtual environment quality of service collaborative virtual reality;quality of service augmented reality;user experience;unified modeling language;collaborative virtual reality;quality of service unified modeling language monitoring streaming media augmented reality collaboration industries;service oriented infrastructure;virtual environment;augmented reality;quality of service;shared multimedia content;collaborative environments;use case;realtime;service oriented infrastructure virtual reality augmented reality quality of service realtime collaborative environments	Service Oriented Infrastructures allow for on-demand provision of Information and Communication Technologies assets. What virtual environments require from such infrastructures refers to the offered Quality of Service (QoS) levels, which have direct impact on the end users experience. In this paper, we describe a new approach to improve the user experience, with regard to QoS, of collaborative Virtual Reality (VR) and Augmented Reality (AR) working sessions. One of the main issues that have to be addressed in such a use case is the requirement for keeping QoS with regards to real-time of the shared multimedia content of the AR. Therefore we adapted our application to a new and innovative real-time enabled framework for service-based infrastructures, implemented within the framework of the IRMOS EU project. This framework provides the necessary guarantees for successful collaborative VR and AR sessions as derived from the desired user experience. We also demonstrate the operation of the implemented framework and evaluate its effectiveness to the collaborative Virtual and Augmented Reality scenario.	augmented reality;quality of service;real-time clock;real-time transcription;service-oriented architecture;service-oriented infrastructure;user experience;virtual reality	Michael Braitmaier;Dimosthenis Kyriazis	2011	2011 Third International Conference on Games and Virtual Worlds for Serious Applications	10.1109/VS-GAMES.2011.12	simulation;human–computer interaction;computer science;multimedia	Visualization	-39.77974538334004	43.30934959388955	26143
0d89070a137c2c22326e3f3880d704c8856c1ba5	analyzing concurrency bugs using dual slicing	execution indexing;dual slicing;concurrency bugs;dynamic slicing;data races;indexation;causal relation;dynamic analysis	Recently, there has been much interest in developing analyzes to detect concurrency bugs that arise because of data races, atomicity violations, execution omission, etc. However, determining whether reported bugs are in fact real, and understanding how these bugs lead to incorrect behavior, remains a labor-intensive process. This paper proposes a novel dynamic analysis that automatically produces the causal path of a concurrent failure leading from the root cause to the failure. Given two schedules, one inducing the failure and the other not, our technique collects traces of the two executions, and compares them to identify salient differences. The causal relation between the differences is disclosed by leveraging a novel slicing algorithm called dual slicing that slices both executions alternatively and iteratively, producing a slice containing trace differences from both runs. Our experiments show that dual slices tend to be very small, often an order of magnitude or more smaller than the corresponding dynamic slices; more importantly, they enable precise analysis of real concurrency bugs for large programs, with reasonable overhead.	algorithm;atomicity (database systems);causal filter;causality;concurrency (computer science);display resolution;experiment;multiversion concurrency control;overhead (computing);schedule (computer science);software bug;tracing (software)	Dasarath Weeratunge;Xiangyu Zhang;William N. Sumner;Suresh Jagannathan	2010		10.1145/1831708.1831740	real-time computing;computer science;database;dynamic program analysis;programming language;algorithm	SE	-60.48871247227465	38.181511805575155	26225
b93a40cb06cd0f577f2e8787dd7fc2d108e8f340	designing information systems which manage or avoid privacy incidents	privacy protection;information system;080610 information systems organisation;private information	In this paper, we consider an information system (IS) to be a set of technologies together with a set of rules about those technologies. An IS is considered to be prone to a privacy incident if it does not fully protect the private information of a user or if a dishonest user can take advantage of the privacy protection offered by the IS. This work identifies the potential privacy incidents that may occur in an IS, and proposes a framework, the MAPI Framework (Manage or Avoid Privacy Incidents), which designs IS to manage or avoid privacy incidents. The MAPI Framework can also be used for evaluating IS by identifying the missing or inappropriate technologies which may lead to privacy incidents.	case preservation;confidentiality;information system;personally identifiable information;privacy;software framework	Giannakis Antoniou;Lynn Margaret Batten;Parampalli Udaya	2008		10.1007/978-3-540-89900-6_15	privacy software;problem management;private information retrieval;information privacy;privacy by design;computer science;data mining;internet privacy;computer security;information system	DB	-47.73153785317853	60.230026245001156	26242
3d0f78b4763a0736bfdc20e29cbff8b8a8049afb	a mobile agent infrastructure with mobility and management support	distributed application;fault tolerant;software agent;mobile agents;software agents;internet computing;mobile agent system;mobile agent systems mobile agent infrastructure mobility management support internet integrative computing platform mobile agents pathfinder;internet;mobile communication;mobile agent;software agents mobile communication internet	The Internet has the potential to provide a global, integrative computing platform to share the resources, services and computing power. To explore the full potential of the Internet computing, a new computing paradigm is needed to exploit the Internet infrastructure in a configurable, scalable, and customizable way. Mobile agents have been shown to be promising in the Internet computing environment. They offer a configurable, scalable and active computing platform over the Internet. However, in the currently proposed approaches, users cannot retain controls after the mobile agents are dispatched. As a result, an extensive study of the issues involved in the management support is needed before the approaches can be commercially viable. In this paper, we describe the architecture of Pathfinder, which provides facilities and mechanisms for the mobility and management support of software agent that enable distributed application in the Internet environment. These unique features enhance openness, usability, and fault-tolerance of mobile agent systems. An implementation of the infrastructure is then described with the implementation issues discussed.	mobile agent	Wen-Shyen E. Chen;C. Y. Lin;Yao-Nan Lien	1999		10.1109/ICPPW.1999.800108	mobile search;mobile web;computer science;mobile technology;mobile agent;distributed computing;mobile computing;mobile communications over ip;computer security;computer network	HCI	-34.083671555299595	48.28468608699655	26420
9ce479520107661aefaff30742f87ed8fd0f0336	qos-aware service-oriented middleware for pervasive environments	en francais;service orientation;middleware	Pervasive computing is an intuitive evolution of computing paradigms driven by the wide adoption of mobile devices and wireless networks. It introduces a novel way to support users in their everyday life based on open and dynamic environments populated with unobtrusive services able to perform user tasks on the fly. Nevertheless, supporting user tasks from a functional point of view is not enough to gain the user’s satisfaction. Users instead require that their tasks meet a certain Quality of Service (QoS) level. QoS is indeed an inherent and primary requisite of users going along with their required tasks. In the context of pervasive environments, fulfilling user tasks while delivering satisfactory QoS brings about several challenges that are mainly due to the openness, dynamics, and limited underlying resources of these environments. These challenges are mainly about (i) the lack of common QoS understanding among users and service providers, (ii) determining and integrating, on the fly, the services available in the environment and able to fulfill the functional and QoS requirements of users, and (iii) adapting the provided services at run-time to cope with QoS fluctuations and ensure meeting user requirements. To cope with the aforementioned issues, we opt for a middleware-based solution. Middleware represents indeed the appropriate software system to deal with common concerns of user applications such as QoS. In particular, we opt for a specific kind of middleware, viz., Service Oriented Middleware (SOM). SOM can leverage middleware technologies and the Service Oriented Computing (SOC) paradigm to enable pervasive environments as dynamic service environments. Particularly, SOM can provide middleware services that allow for supporting QoS of user applications offered by pervasive environments. This thesis presents a QoS-aware service-oriented middleware for pervasive environments.	mobile device;on the fly;openness;pervasive informatics;point of view (computer hardware company);population;programming paradigm;quality of service;requirement;service-oriented device architecture;service-oriented middleware;software system;ubiquitous computing;user requirements document;viz: the computer game	Nebil Ben Mabrouk	2009			real-time computing;context-aware pervasive systems;computer science;database;distributed computing	HCI	-41.4033700292864	43.35956532193753	26429
4b45add1969f970c95a142de062d8efc56dc005a	controller agents for constraints solving: implementation and use of cacs prototype	software;distributed system;multi agent system;prototypes;multi agent systems constraint handling java;constraints satisfaction distributed systems multi agent system;constraint satisfaction;multi agent systems;distributed constraint satisfaction problem;multi agent systems controller agents constraints solving distributed constraint satisfaction problem;constraints satisfaction;constraint handling;constraint solving;constraint satisfaction problem;constraints solving;controller agents;distributed systems;encoding;proposals;programming;algorithm design and analysis;java;prototypes control systems multiagent systems centralized control encoding artificial intelligence java ecosystems law legal factors	The distributed constraint satisfaction problem (DCSP) which is an emerged field from the integration between two paradigms of different nature: multi-agent systems (MAS) that is characterized by the autonomy and the distribution of its entities and the constraint satisfaction problem paradigm (CSP) where all constraints are treated in central manner as a black-box. In this paper we aim to propose a model called controller-agents for constraints solving or CACS for short to be used for solving DCSP. We aim in this paper to present a model called CACS for (controller-agent for constraints solving). A controller role is to encapsulate and verify some constraints assigned to it. This model allows grouping constraints to form a subset that will be treated together as a local problem inside the controller. Using this model allows also handling non-binary constraints easily and directly so that no translating of constraints into binary ones is needed. Based on CACS, a prototype of DCSP solver is built. This paper presents the implementation outlines of that prototype and its usage methodology. The prototype is built in Java using general interfaces of both MAS and CSP platforms. These interfaces allow users to use the platforms of their choice providing that they implement these interfaces with the chosen platforms.	black box;constraint satisfaction problem;controller (computing);distributed constraint optimization;entity;java;multi-agent system;programming paradigm;prototype;solver	Sami Al-Maqtari;Habib Abdulrab	2008	2008 20th IEEE International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2008.118	algorithm design;programming;real-time computing;simulation;constraint satisfaction;computer science;artificial intelligence;multi-agent system;distributed computing;prototype;java;constraint satisfaction problem;encoding	Robotics	-37.69071801712401	38.63333104972488	26483
b37fec5d9030f2d41e46f6c94b8b58210a7cf03d	radiatus: a shared-nothing server-side web architecture	isolation;web application;security	Web applications are a frequent target of successful attacks. In most web frameworks, the damage is amplified by the fact that application code is responsible for security enforcement. In this paper, we design and evaluate Radiatus, a shared-nothing web framework where application-specific computation and storage on the server is contained within a sandbox with the privileges of the end-user. By strongly isolating users, user data and service availability can be protected from application vulnerabilities.  To make Radiatus practical at the scale of modern web applications, we introduce a distributed capabilities system to allow fine-grained secure resource sharing across the many distributed services that compose an application. We analyze the strengths and weaknesses of a shared-nothing web architecture, which protects applications from a large class of vulnerabilities, but adds an overhead of 60.7% per server and requires an additional 31MB of memory per active user. We demonstrate that the system can scale to 20K operations per second on a 500-node AWS cluster.	amazon web services;computation;flops;overhead (computing);server (computing);server-side;shared nothing architecture;web application;web framework	Raymond Cheng;Will Scott;Paul Ellenbogen;Jon Howell;Franziska Roesner;Arvind Krishnamurthy;Thomas E. Anderson	2016		10.1145/2987550.2987571	web service;web application security;embedded system;web development;web application;web modeling;data web;isolation;computer science;engineering;information security;operating system;web navigation;database;world wide web;computer security;web server;application server;mashup;computer network	Security	-52.56839586724317	57.6495998616629	26501
1f8551ecdeb8a4f6b16c768594e2f06ca99c4d9e	smart world: a better world	smart world emerging techniques sensing;smart world emerging techniques	With the advancement of technologies, our world is becoming a smart world. In this paper, we share our vision of a smart world, demonstrate different application scenarios and introduce the emerging techniques. We envision that in a smart world, we will become more connected, safe, productive and efficient. To enable a smart world, many advanced techniques such as advanced network, ubiquitous sensing and collaborative computation have been developed. More specifically, they include heterogeneous advanced wireless networks, intelligent transportation, accurate indoor localisation, wireless sensor network, unobtrusive human behaviour sensing and mobile cloud computing. Compared with the previous work, the proposed techniques are faster, more accurate and non-invasive. We firmly believe that by exploiting those techniques, the smart world will be a better world.	computation;mobile cloud computing;our world	Guanqing Liang;Jiannong Cao;Xuefeng Liu	2015	2015 IEEE 12th Intl Conf on Ubiquitous Intelligence and Computing and 2015 IEEE 12th Intl Conf on Autonomic and Trusted Computing and 2015 IEEE 15th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom)	10.1007/s11432-016-5518-8	simulation;telecommunications;computer science;smart environment;internet of things	Mobile	-43.04268972684259	50.15955660551483	26507
13c4f1b217cb2f860242ec69c43464c806758e3f	supporting query formulation for text retrieval applications in software engineering	query reformulation;conferences software maintenance;software maintenance;text analysis query formulation software engineering;text retrieval;query formulation source code tr query prediction query reformulation relevant information retrieval software artifacts textual information tr techniques software engineering text retrieval applications;query;query quality text retrieval query query reformulation;query quality;conferences	"""Text Retrieval (TR) techniques have been successfully used to leverage the textual information found in software artifacts with the purpose of aiding developers with their daily tasks. TR techniques require a query as input and the usefulness of the results they retrieve depends greatly on this query. While some queries retrieve relevant information for the current task, others do not, therefore pointing developers in the wrong direction. Developers have a hard time realizing this before going through the search results, which, in the case of """"bad"""" queries means time and effort lost looking at irrelevant information. In this scenario, developers have to reformulate the query, often without pointers on how to improve it. The work presented in this paper introduces novel approaches to address these challenges and makes two main contributions: 1) defines the first approach for predicting the success of a TR query in the context of SE tasks, 2) introduces automatic approaches that analyze a query and improve it by finding the most suited reformulation for it. The approaches were evaluated for the task of concept location in source code and the results of the performed studies reveal their usefulness."""	document retrieval;hard time;information retrieval;relevance;software engineering;transistor;utility	Sonia Haiduc	2014	2014 IEEE International Conference on Software Maintenance and Evolution	10.1109/ICSME.2014.117	online aggregation;sargable;query optimization;query expansion;web query classification;ranking;computer science;engineering;query by example;software engineering;concept search;data mining;database;rdf query language;programming language;web search query;software maintenance;information retrieval;query language;object query language;spatial query	SE	-59.40029685423956	39.439135573838655	26602
d884959372afd8ede747213aa9e9730669a3f87d	an adaptive risk management and access control framework to mitigate insider threats	trust;inference threat;risk management;insider threat;role based access control;access control	Insider Attacks are one of the most dangerous threats organizations face today. An insider attack occurs when a person authorized to perform certain actions in an organization decides to abuse the trust, and harm the organization. These attacks may negatively impact the reputation of the organization, its productivity, and may produce losses in revenue and clients. Avoiding insider attacks is a daunting task. While it is necessary to provide privileges to employees so they can perform their jobs efficiently, providing too many privileges may backfire when users accidentally or intentionally abuse their privileges. Hence, finding a middle ground, where the necessary privileges are provided and malicious usage are avoided, is necessary. In this paper, we propose a framework that extends the role-based access control (RBAC) model by incorporating a risk assessment process, and the trust the system has on its users. Our framework adapts to suspicious changes in users’ behavior by removing privileges when users’ trust falls below a certain threshold. This threshold is computed based on a risk assessment process that includes the risk due to inference of unauthorized information. We use a Coloured-Petri net to detect inferences. We also redefine the existing role activation problem, and propose an algorithm that reduces the risk exposure. We propose a methodology to help administrators managing inference threats. We present experimental evaluation to validate our work. a 2013 Elsevier Ltd. All rights reserved.	algorithm;authorization;coloured petri net;job stream;malware;privilege (computing);risk assessment;risk management;role-based access control	Nathalie Baracaldo;James B. D. Joshi	2013	Computers & Security	10.1016/j.cose.2013.08.001	risk management;computer science;access control;role-based access control;internet privacy;trustworthy computing;computer security	Security	-60.445943644340296	59.44497793292328	26630
f7e73ee6cc87e39707def20176b248a356daab43	rail internet of things: an architectural platform and assured requirements model	rails;passenger assistance software rail internet of things architectural platform assured requirements model public transport passengers tailor generic services ip networking public transport rail service users riot data communication channels situation calculus modelling decentralised feedback control mechanisms;situation calculus assistance assured model inclusive iot rail internet of things riot;networks communications;rails security calculus data models internet of things temperature sensors;temperature sensors;railways internet of things;internet of things;qa75 electronic computers computer science;calculus;security;data models	Given the plethora of individual preferences and requirements of public transport passengers for travel, seating, catering, etc., it becomes very challenging to tailor generic services to individuals' requirements using the existing service platforms. As tens of thousands of sensors have been already deployed along roadsides and rail tracks and on buses and trains in many countries, it is expected that the introduction of IP networking will revolutionise the functionality of public transport in general and rail services in particular. In this paper, we propose a new communication paradigm to improve rail services and address the requirement of rail service users: the Rail Internet of Things (RIoT). To the best of our knowledge, it is the first work to define the RIoT and design an architectural platform that includes its components and the data communication channels. Moreover, we develop an assured requirements model using the situation calculus modelling to represent the fundamental requirements for adjustable, decentralised feedback control mechanisms necessary for the RIoT-ready software systems. The developed formal model is applied to demonstrate the design of passenger assistance software that interacts with the RIoT ecosystem and provides passengers with real-time information that is tailored to their requirements with runtime adaptability.	control system;ecosystem;experiment;feedback;internet of things;programming paradigm;real-time data;requirement;sensor;simulation;situation calculus;software system;travelling salesman problem;usability	Mahmoud Hashem Eiza;Martin Randles;Princy Johnson;Nathan Shone;Jennifer Pang;Amhmed Bhih	2015	2015 IEEE International Conference on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; Pervasive Intelligence and Computing	10.1109/CIT/IUCC/DASC/PICOM.2015.52	data modeling;simulation;telecommunications;computer science;information security;operating system;database;computer security;internet of things;computer network	Robotics	-40.06534740769703	49.58042252280552	26635
b993d74898f15aec4f59c5d400081216449d6606	a web-based cooperative tool for risk management with adaptive security	software;security domain;adaptive security;hardware and architecture;web based risk tools;computer networks and communications;environment;cooperative risk management	Risk management can benefit from Web-based tools fostering actions for treating risks in an environment, while having several individuals collaborating to face the endeavors related to risks. During the intervention, the security rules in place to preserve resources from unauthorized access, might need to be modified on the fly, e.g., increasing the privileges of risk managers or letting rescue teams view the exact position of the victims. Modifications should respect the overall security policies and avoid security conflicts. This paper presents a dynamic access control model for environmental risks involving physical resources. Data structures included in our Web application to represent both risk and security are given. To keep the dynamic security rules compliant with overall organization security objectives, we consider rules grouped in Access Control Domains so that changes do not create security conflicts during collaboration in risk management. Considering work environments as an example, risk and access control models are introduced. Security is built on the ABAC (Attribute Based Access Control) paradigm. A Risk Management System (RMS) is illustrated: it captures events, signals potential risks, and outputs strategies to prevent the risk. Dynamic authorization is included in the RMS to vary subjects' privileges on physical resources based on risk level, people position and so on. These concepts are implemented in a prototype Web application appearing as a Web Dashboard for risk management. We present a web-based cooperative tool for risk management with adaptive security.Based on a motivating scenario the risk and security elements are introduced.Security is built on the ABAC (Attribute Based Access Control) paradigm.A Risk Management System is illustrated that facilitates the cooperation in risk management.Using Event-Condition-Action meta-rules, dynamic authorization based on risk is controlled.	risk management;web application	Maria Grazia Fugini;Mahsa Teimourikia;George C. Hadjichristofi	2016	Future Generation Comp. Syst.	10.1016/j.future.2015.04.015	computer security model;cloud computing security;countermeasure;security management;security information and event management;it risk management;security convergence;risk management;asset;knowledge management;threat;operating system;data mining;database;security service;distributed computing;risk management information systems;natural environment;computer security;factor analysis of information risk	Arch	-48.55859222239275	53.50882510802666	26646
99d988161edd02bfdc3df5b9bbf4b206dff758cb	truly-protect: an efficient vm-based software protection	computer crime;software engineering;virtual machines;virtual machines computer crime software engineering;comprehensive performance analysis truly protect vm based software protection method virtual machine software piracy unlicensed copies 5th international conference on network and system security nss2011 just in time decryption;process virtual machine copy protection drm;software cryptography switches hardware gold games software algorithms	We present Truly-Protect that is a software protection method. Previously published protection methods relied solely on obscurity. Rolles proposed a general approach for breaking systems that are based on obscurity. We show that, under certain assumptions, Truly-Protect is resistant not only to Rolles' attack but also to any other attacks that do not violate the assumptions. Truly-Protect is based on a virtual machine that enables us to execute encrypted programs. Truly-Protect can serve as a platform for preventing software piracy of obtaining unlicensed copies. Truly-Protect by itself is not a digital rights management system but can form a basis for such a system. We discuss several scenarios and implementations and validate the performance penalty of our protection. A preliminary version of this paper appeared in the 5th International Conference on Network and System Security (NSS2011). It was extended by expanding the system's description, adding more efficient parallel implementation, just-in-time decryption, and a comprehensive performance analysis. It also contains all the necessary proofs.	copy protection;cryptography;digital rights management;encryption;just-in-time compilation;profiling (computer programming);security through obscurity;virtual machine	Amir Averbuch;Michael Kiperberg;Nezer Zaidenberg	2013	IEEE Systems Journal	10.1109/JSYST.2013.2260617	software security assurance;telecommunications;computer science;engineering;virtual machine;electrical engineering;artificial intelligence;package development process;backporting;software framework;component-based software engineering;software development;software design description;operating system;software construction;internet privacy;software walkthrough;software deployment;world wide web;computer security;software metric;computer network;software system	EDA	-58.809482371390715	56.36539372685538	26676
f8c04cfa4fa470945fa5bd9ffcd267fca84979c0	a natural classification scheme for software security patterns	software authentication unified modeling language taxonomy availability documentation;software;security properties;object oriented methods;attack patterns natural classification scheme software security patterns security pattern catalogs unique selection criteria software lifecycle phases security flaws security objectives;availability;authentication;software security;design pattern;unified modeling language;taxonomy;secure system;pattern classification;secure system development design patterns software security patterns pattern classification;design patterns;software security patterns;selection criteria;security of data;secure system development;documentation;security of data object oriented methods pattern classification	Software security patterns are a proven solution for recurring security problems. Security pattern catalogs are increasing rapidly. This creates difficulty in selecting appropriate software security patterns for a particular recurring security problem. There are several classification schemes to organize software security patterns. Every classification scheme has unique selection criteria for choosing a security pattern. However, no classification scheme considers security flaws, which is the root cause of software security vulnerabilities. In this paper, we provide a natural classification scheme for software security patterns. Our classification scheme is associated with software lifecycle phases. Security flaws are incorporated in the classification of software security patterns with security objectives in the requirement phase, security properties in the design phase, and attack patterns in the implementation phase. Furthermore, we enhance the existing security pattern template with classification parameters.	application security;attack patterns;comparison and contrast of classification schemes in linguistics and metadata;software development process;vulnerability (computing)	Aleem Khalid Alvi;Mohammad Zulkernine	2011	2011 IEEE Ninth International Conference on Dependable, Autonomic and Secure Computing	10.1109/DASC.2011.42	software security assurance;information security audit;computer security model;unified modeling language;availability;security through obscurity;security information and event management;security engineering;security convergence;documentation;security bug;computer science;data mining;authentication;database;security service;design pattern;security testing;computer security;taxonomy	Security	-56.00826100131134	48.02562961786871	26752
c22dab5d6835b8a60d15c7b847aba9d2e508f0f9	a visual tool for generating extensible mobile application skeltons to reduce failures in java midp applications			java;mobile information device profile;mobile app	Marko Palviainen;Jussi Yliaho;Jarno Soininen	2006			embedded system;operating system;extensibility;computer science;java;mobile computing	SE	-36.33442115633042	50.024032721040975	26851
3b05007983b6b5cc86d5bafb69b35c93cc5344ca	security ontology proposal for mobile applications	protege 4 tool security ontology mobile applications knowledge acquisition owl dl semantic language;mobility;owl dl semantic language;data mining;ontologies artificial intelligence;knowledge representation languages;mobile applications;specification languages knowledge acquisition knowledge representation languages mobile computing ontologies artificial intelligence security of data;specification languages;cryptography;security requirements;knowledge acquisition;mobile communication;ontologies proposals application software data security mobile handsets message service mobile communication conference management middleware context modeling;mobile handsets;ontologies;mobile computing;security;protege 4 tool;ontology;mobile application;security of data;context;mobility security ontology;security ontology;knowledge base	Mobility is an emerging area that comes up with several technologies and stakeholders. Dealing with the security requirement for mobile applications means acquiring all the knowledge and the available technologies for the design and deployment of a reliable and usable countermeasure. Not only the field  lacks of standards but also requires several quality constraints. To assist developers to face such a challenge, we propose a knowledge base solution through the conceptualization of a security ontology. The ontology was implemented in OWL-DL semantic language with Protégé 4 tool. The ontology schema and instantiation are commented, target use is mentioned through its integration in a whole approach for security in the mobile world.	conceptualization (information science);knowledge base;mobile app;protégé;software deployment;universal instantiation;web ontology language	Sofien Beji;Nabil El-Kadhi	2009	2009 Tenth International Conference on Mobile Data Management: Systems, Services and Middleware	10.1109/MDM.2009.100	upper ontology;computer science;knowledge management;cryptography;ontology;operating system;data mining;database;ontology-based data integration;mobile computing;process ontology;suggested upper merged ontology	SE	-43.30523096011797	45.58188954190225	26852
0019cff93d5f26c7c4b9b890d4fab012c03b8196	rotary dial model — a model-driven methodology for autonomic network design	systems engineering;safely critical systems model driven methodology systems engineering autonomics rdm mda mde;metamodeling standards context autonomic systems design methodology;mobile radio;autonomic entities rotary dial model model driven methodology autonomic network design complex multidomain system systems engineering safety critical system rdm standard telephone rotary dial;telephone equipment mobile radio systems engineering;telephone equipment	Designing and developing complex multi-domain systems such as autonomic systems and networks is not only a creative but also a systems engineering challenge. Current systems engineering methodologies are unsuitable, as autonomics is a multi-domain field that requires the knowledge of several domains, such as control-theory, computer networks, software-engineering, etc., and comes with its own set of requirements and challenges. We showcase why and how current systems engineering approaches fail, and advocate for an avant-garde systems engineering approach to designing safety-critical and multidomain systems, such as autonomic entities. In this paper, we introduce a new model-driven systems engineering methodology, called the Rotary Dial Model (RDM), whose functioning is inspired by the standard telephone rotary dial. We describe in detail the working of the RDM model, and showcase its robustness and flexibility for the system design process.	autonomic computing;autonomic networking;control theory;embedded system;entity;iterative method;mechatronics;metamodeling;model-driven architecture;model-driven integration;network planning and design;requirement;rotary system;rotary woofer;simulation;software engineering;systems design;systems engineering;toolchain;verification and validation	Arun Prakash;Ina Schieferdecker;Michael Wagner;Christian Hein	2012	2012 IEEE Globecom Workshops	10.1109/GLOCOMW.2012.6477694	simulation;system of systems;system of systems engineering;telecommunications;systems engineering;operating system;computer network;systems design	DB	-45.806500496594204	37.812215823347636	26864
ff960b1740c77b61d7e57b6299fc68754603c6b9	hamilton: a cost-effective, low power networked sensor for indoor environment monitoring		Operating buildings can be challenging, especially with poor instrumentation of the indoor environment. There are several wireless sensor platforms on the market but most are too difficult to deploy en-masse, requiring the end-user to program devices, or manage infrastructure. Many rely on smart-phones and do not work when unattended. The Hamilton wireless sensor node is a full-stack solution providing a low-cost and low-power high-resolution sensor that operates for more than five years on a battery, along with all the cloud infrastructure required to interact with the data. It is pre-programmed and ready to use, but the firmware can be easily modified by using the standard C language.	ansi c;cloud computing;firmware;image resolution;low-power broadcasting;sensor node;sensor web;smart tv;smartphone	Michael P. Andersen;Hyung-Sin Kim;David E. Culler	2017		10.1145/3137133.3141453	embedded system;firmware;battery (electricity);wireless;cloud computing;computer science;instrumentation;sensor node	Mobile	-43.640944362826296	48.49523024158107	26891
14abc59b8fcc12491cbff7ca665ca5bf14e4477f	iot: secured and automated house		In recent years, dramatic evolution of network has been introduced in the home environment that enables digital technologies to be used as the appliances in the home. These devices can be remotely accessed and controlled using existing network infrastructure, thus allowing a direct integration of computing systems with the physical world. A part of this network evolution is Internet-of-Things (IoT), which is the expansion of the services provided by internet. The IoT can be used for various fields and home automation is one of the applications of IoT. This method allows more opportunities to increase the connectivity between devices within the home and outside the home through internet for automating the home appliances. This paper includes the design and implementation of a secured and automated house using a hybrid communication system such as IoT and mobile communication methods for the communication part with using Arduino Microcontroller, GSM Shield, Ethernet Shield, and varieties of sensors. With the advancement in the field of IoT, there is an increasing demand of real time security for ones need. Secured and automated house is a house which is secured via electronic devices and sensors to protect the house from different kinds of intrusions such as motion in the house and disasters for example fire and gas leakage in the house. This paper focuses on designing a robust and reliable system using the home automation system to overcome these problems and to alert the owner of the house with a message once suspicious act are occurred. The results are very helpful in the desire to achieve an efficient and reliable solution contributing in the field of IoT, considering various aspects which include fast processing, system cost, robustness and precision for the modern, technological and needs.	arduino;closing (morphology);home automation;ip camera;internet;microcontroller;mobile phone;robustness (computer science);sensor;spectral leakage	Hakar Mohsin Saber;Nawzad Kamaran Al-Salihi	2017	2017 International Carnahan Conference on Security Technology (ICCST)	10.1109/CCST.2017.8167862	computer security;robustness (computer science);communications system;web server;gsm;arduino;computer science;the internet;home automation;mobile telephony	HCI	-50.90309040117769	59.91965959808505	26979
ecdc7144a6a32ad9a8575a0d59fad97980214b2f	locust - a brokerage system for accessing idle ressources for web-computing		In this paper we present a brokerage system for making idle cpu ressources of anonymous users of the World Wide Web accessble for distributed Web-Computing. The Locust (LOw cost Computing Utilizing Skimmed idle Time) Broker collects and remarkets the aggregated ressources by means of ubiquitious Web and Java technologies and creates an electronic spot market focused on the mainstream Internet user as suppliers of idle computing ressources. We introduce the economic Price/Market model of Locust enabling the export or import of resource surpluses or lacks between so-called submarkets. Test cases for consumption of aggregated CPU ressources include distributed Raytracing and RC5 decryption. We further describe the RC5 key block management and our efforts to reach performance comparable to native RC5 implementations.	applet;central processing unit;cryptography;distributed computing environment;embarrassingly parallel;idle (cpu);java virtual machine;on the fly;operability;overhead (computing);prototype;ray tracing (graphics);smart common input method;software propagation;www;world wide web	Michael May	1999		10.1109/EUROMICRO.1999.10017		OS	-34.34837036154002	52.19260082810521	27037
6cedbaa50be26aadf08ad0423771639410cc8a5a	v2psense: enabling cellular-based v2p collision warning service through mobile sensing		The C-V2X (Cellular Vehicle-to-Everything) technology is developing in full swing. One of its mainstream services can be the Vehicle-to- Pedestrian (V2P) service. It can protect pedestrians who are mostly vulnerable on the road. In this work, we seek to enable a V2P service that can identify which pedestrians may be nearby a dangerous driving event and then notify them of warning messages. To enable this V2P service, there are two major challenges. First, a low-latency V2P message transport is required for this infrastructure-based service. Second, the pedestrian's smartphone requires an energy- efficient outdoor positioning method instead of power-hungry GPS due to its limited battery life. We thus propose a novel solution, V2PSense, which trades off positioning precision for energy savings while achieving low-latency message transport with LTE high-priority bearers. It does a coarse-grained positioning by leveraging intermittent GPS information and mobile sensing data, which includes step count from the pedometer and cellular signal strength changes. Though the V2PSense's positioning is not as precise as the GPS, it can still ensure that all the pedestrians nearby dangerous spots can be notified. Our results show that it can achieve the average precision ratio 92.6% for estimating where the pedestrian is while saving 20.8% energy, compared with the GPS always-on case.	compaq lte;global positioning system;high availability;information retrieval;sensor;smartphone	Chi-Yu Li;Giovanni Salinas;Po-Hao Huang;Guan-Hua Tu;Guo-Huang Hsu;Tien-Yuan Hsieh	2018	2018 IEEE International Conference on Communications (ICC)	10.1109/ICC.2018.8422981	computer network;collision;real-time computing;signal strength;pedestrian;global positioning system;dangerous driving;computer science;pedometer;server	Mobile	-38.96137845947058	56.67911426569383	27163
b0681c1e945b3a9d2af574e049d2885246ac4f4e	malware behavioral detection by attribute-automata using abstraction from platform and language	attribute grammars;attribute grammar;malware;interpretation;behaviors	Security and Trusted Transactions (MAPS/STT). Context Interest of behavioral detection against unknown malware Theoretically detects, if not innovative malware, at least variants reusing known techniques In AV products, behavioral detectors still rely on too specific characteristics Escape through simple functional modifications (variants multiplication) Problematics Can we describe malicious behaviors generically ? Can we address the semantic gap between the model and data collection ? Can we detect accurately these descriptions in a reasonable time ? Increasing expressiveness of behavioral models 1995 – Simple Finite State Automata [B. L. Charlier et al.] • Alternative sequences of operations 2005 – Information flow analysis [J. Newsome et al., S. Bhatkar et al.] • Operations involving misapropriate data flow 2007 – Graphs with data dependencies Abstract Malicious Behavior Language Describing duplication Detection by attribute-automata Layered architecture Abstraction layer for translation Detection layer by attribute-automata Prototyping Coverage and performance evaluation Detection and errors rates Performance Considerations and perspectives	abstraction layer;automata theory;automaton;data dependency;data-flow analysis;dataflow;finite-state machine;information flow;malware;performance evaluation;sensor	Grégoire Jacob;Hervé Debar;Eric Filiol	2009		10.1007/978-3-642-04342-0_5	interpretation;computer science;theoretical computer science;database;malware;programming language;attribute grammar;human behavior;computer security	Security	-52.83341240792316	48.24744290266066	27181
ef3f59bf42cd780d205db890a29ba515370dd4f5	systematic reduction of gui test sequences		Graphic user interface (GUI) is an integral part of many software applications. However, GUI testing remains a challenging task. The main problem is to generate a set of high-quality test cases, i.e., sequences of user events to cover the often large input space. Since manually crafting event sequences is labor-intensive and automated testing tools often have poor performance, we propose a new GUI testing framework to efficiently generate progressively longer event sequences while avoiding redundant sequences. Our technique for identifying the redundancy among these sequences relies on statically checking a set of simple and syntactic-level conditions, whose reduction power matches and sometimes exceeds that of classic techniques based on partial order reduction. We have evaluated our method on 17 Java Swing applications. Our experimental results show the new technique, while being sound and systematic, can achieve more than 10X reduction in the number of test sequences compared to the state-of-the-art GUI testing tools.	european film gateway;experiment;floor and ceiling functions;graphical user interface testing;model-driven architecture;partial order reduction;static program analysis;test automation;test case	Lin Cheng;Zijiang Yang;Chao Wang	2017	2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)		redundancy (engineering);computer science;theoretical computer science;partial order reduction;graphical user interface testing;test case;software;java;graphical user interface	SE	-58.05685208444725	37.89777464353893	27188
3953558e92b1397c778cd450b4ca58da45932bcc	comparing the effectiveness of software testing strategies	experimental design;software;prueba funcional;high rate;error detection codes;group codes;software testing;empirical study;code reading;interfaces;critical study;metodologia;methode empirique;methode essai;functional testing;off line software review;logiciel;software measurement;methodology evaluation;program verification computers;estudio comparativo;reading;metodo empirico;empirical method;eficacia test;rates per time;detection;value analysis;etude critique;comparison;methodologie;computer programs;strategy;estudio critico;faults;etude comparative;structural testing;programmers;test and evaluation;detection defaut;operational problems;fault detection;coding;comparative study;test methods;rates;logicial;evaluation;operational effectiveness;structural testing code reading empirical study functional testing methodology evaluation off line software review software measurement software testing;evaluacion;test method;sources;experimental methodology;methodology;functional test;system effectiveness;deteccion imperfeccion;software testing fault detection computer science programming profession costs design for experiments software measurement uncertainty military computing nasa;test efficiency;estimates;computer program verification;students;efficacite test;defect detection;essai fonctionnel;metodo ensayo	This study applies an experimentation methodology to compare three state-of-the-practice software testing techniques: a) code reading by stepwise abstraction, b) functional testing using equivalence partitioning and boundary value analysis, and c) structural testing using 100 percent statement coverage criteria. The study compares the strategies in three aspects of software testing: fault detection effectiveness, fault detection cost, and classes of faults detected. Thirty-two professional programmers and 42 advanced students applied the three techniques to four unit-sized programs in a fractional factorial experimental design. The major results of this study are the following. 1) With the professional programmers, code reading detected more software faults and had a higher fault detection rate than did functional or structural testing, while functional testing detected more faults than did structural testing, but functional and structural testing were not different in fault detection rate. 2) In one advanced student subject group, code reading and functional testing were not different in faults found, but were both superior to structural testing, while in the other advanced student subject group there was no difference among the techniques. 3) With the advanced student subjects, the three techniques were not different in fault detection rate. 4) Number of faults observed, fault detection rate, and total effort in detection depended on the type of software tested. 5) Code reading detected more interface faults than did the other methods. 6) Functional testing detected more control faults than did the other methods.	boundary-value analysis;code reading;code coverage;design of experiments;emoticon;equivalence partitioning;fault detection and isolation;functional testing;programmer;software testing;stepwise regression;turing completeness;white-box testing	Victor R. Basili;Richard W. Selby	1987	IEEE Transactions on Software Engineering	10.1109/TSE.1987.232881	reliability engineering;regression testing;simulation;white-box testing;computer science;functional testing;test method;empirical research;algorithm;statistics	SE	-61.023883815818635	33.73825217822406	27202
1c6962f6bb68c5e7985f9b433de39eeaac171c2c	application of symbolic and bounded model checking to the verification of logic control systems	programmable controllers;control system analysis computing;computability;smv language;bdd techniques;programmable control;plc;logic control systems;formal method;bounded model checking;control system;sat techniques;formal verification;binary decision diagrams;model checking;complex system;programmable logic controllers;control engineering computing;programmable controllers binary decision diagrams computability control engineering computing control system analysis computing formal verification;logic control system synthesis programmable control binary decision diagrams automatic control control systems state space methods automation hardware power system modeling;sat techniques model checking logic control systems formal method programmable logic controllers plc smv language bdd techniques	The developer of logic control systems is faced with increasing complexity of the functions to be implemented and, at the same time, increasing demands on the reliability of the resulting software. To analyze the reliability of such complex systems formal methods can be applied. One area of the corresponding research is focused on the application of model checking techniques to Programmable Logic Controllers (PLCs). In this paper a new method to formalize PLC programs together with a model of the cyclic behavior of the PLC is presented. The control systems behavior is modeled, and then the program, written in Instruction List, is formalized and integrated into the model. The formalization in SMV language is suitable for verification using BDD and SAT techniques. Both techniques are compared using first results of a case study	algorithm;binary decision diagram;boolean satisfiability problem;complex systems;control system;formal methods;instruction list;java;logic control;microsoft outlook for mac;model checking;programmable logic device	K. Loeis;Mohammed Bani Younis;Georg Frey	2005	2005 IEEE Conference on Emerging Technologies and Factory Automation	10.1109/ETFA.2005.1612527	real-time computing;computer science;control system;theoretical computer science;programmable logic controller;programming language;algorithm	EDA	-44.620114387552185	32.39387559658541	27255
87ca2687057733d0b75878b6189a3bf2c6441059	development framework for pervasive computing applications	dynamic change;touch screen;user interface;pervasive computing;natural interaction;mobile phone;operating system	Pervasive computing [1] brings the technology closer to the users by enabling the users to use daily-life devices (mobile phones, TVs, touch screen walls, etc.) for controlling their environment and accessing information virtually anywhere. Interacting with such devices does not remind users of classical computers and enables them to more naturally interact with the controlled system, if user interface is designed properly. These devices usually operate in networked environments with every controlling and controlled device connected to a central hub. Bringing easy-to-use applications to such environments faces the challenge of highly heterogeneous, dynamically changing environment and necessity to deploy applications to controlling devices with very different features (display size, input methods, operating systems, etc.).	computer;display size;input method;mobile phone;operating system;touchscreen;usb hub;ubiquitous computing;user interface	Václav Slovácek;Miroslav Macik;Martin Klíma	2009	ACM SIGACCESS	10.1145/1651259.1651262	embedded system;context-aware pervasive systems;human–computer interaction;computer science;operating system;multimedia;user interface;ubiquitous computing	HCI	-38.363495848379756	47.96347046917509	27312
8782d058746839978bcd36902c1b7b444f49570c	semantic middleware for the internet of things	semantic services;composability;intuitive semantic interface semantic middleware internet of things multiple heterogeneous devices area networks interoperability composability application layer solution device semantics semantic services semantic web technology;protocols;application layer solution;grounding;standards;multiple heterogeneous devices;semantics;semantic middleware;user interfaces internet middleware open systems;consumer electronics;semantics bluetooth semantic web standards protocols middleware grounding;semantic web technology;sensor network;internet of things;internet;semantic web;middleware;bluetooth;area networks;interoperability;device semantics;intuitive semantic interface;open systems;user interfaces	The Internet of Things (IoT) refers to extending the Internet to devices such as home appliances, consumer electronics, and sensor networks. As multiple heterogeneous devices attempt to create area networks, one of the major challenges is the interoperability and com-posability of their services. The traditional way to address interoperability is to define standards; however, there are many standards and specifications that are incompatible with each other. In this paper we propose an application layer solution for interoperability. The key idea is to utilize device semantics provided by existing specifications and dynamically wrap them in our middleware into semantic services. Next, with the help of Semantic Web technologies, users can create and then execute complex tasks involving multiple heterogeneous devices. We demonstrate how our framework automates interoperability without any modifications to existing standards, devices, or technologies, while providing to the user an intuitive semantic interface with services that can be executed by combining devices in the network.	internet of things;interoperability;middleware;semantic web;web resource	Zhexuan Song;Alvaro A. Cárdenas;Ryusuke Masuoka	2010	2010 Internet of Things (IOT)	10.1109/IOT.2010.5678448	ground;semantic interoperability;communications protocol;interoperability;the internet;wireless sensor network;computer science;operating system;semantic web;middleware;database;semantics;internet privacy;open system;bluetooth;user interface;cross-domain interoperability;world wide web;internet of things	HCI	-40.075178944676615	46.563679184282414	27431
b6b4f6a068c91b9cb0c00b85d313a897e8e4dd5c	from prolog to java: applying medd to object oriented programming	java object oriented programming object oriented modeling computer errors educational institutions object detection machine learning intelligent systems learning systems error analysis;language use;student model;intelligent tutoring system;conceptual clustering;object oriented programming;learner s errors detection prolog java intelligent tutoring systems student model conceptual clustering multistrategy error detection and discovery object oriented programming machine learning;computer science education;machine learning;error detection java object oriented programming computer science education intelligent tutoring systems;learning object;intelligent tutoring systems;and conceptual clustering;error detection;java	This paper presents how Multistrategy Error Detection and Discovery (MEDD), a student modeling system using machine learning can be applied to the domain of Object Oriented Programming. Java is the language used in learning object oriented programming. MEDD detects the learner’s errors and discovers the misconceptions based on the presence (or absence) of errors.	java;machine learning;prolog	Merlin Cruz;Raymund Sison	2002		10.1109/CIE.2002.1185915	natural language processing;method;error detection and correction;computer science;theoretical computer science;real time java;programming language;object-oriented programming;java;conceptual clustering	PL	-52.26272081474728	38.20102302484979	27449
d279cbe6350878478b55443419c3b19a006c2359	notifying civilians in time - disaster warning systems based on a multilaterally secure, economic, and mobile infastructure	early warning system;location based services;legislation;location based service;disaster management;mobile phone;mobile communication;civil protection;profitability;gsm;egovernment;mobile network	The spread of mobile communication equipment offers new opportunities for disaster management referring to civilians. At the same time, location based services are regarded as privacy invading, and are regulated in many countries by specific legislation. We analyze the requirements of a LBS-based disaster management scenario that enables the timely notification of civilians. In addition, we propose a solution for building a privacy-friendly, multilaterally secure disaster management infrastructure based on robust mobile phone infrastructures with high reachability of citizens. We will also point out additional features based on mobile networks. Traditionally, disaster management is a government domain. We will propose another option to implement and run disaster management. We analyze in how far an early warning system could be profitable for the insurance sector. Our comments will sketch that it is possible to reach a large number of persons, avoid insurance damage, and save costs in disaster warning systems. This work was supported by the IST PRIME project; however, it represents the view of the authors only.	care-of address;galileo (satellite navigation);global positioning system;information security;location-based service;mobile phone;privacy;prototype;provisioning;reachability;requirement;telecommunications network;usability	Tobias Scherner;Lothar Fritsch	2005			simulation;engineering;internet privacy;computer security;disaster recovery	Mobile	-43.50821530535233	52.431711381659035	27493
4f30756e2e746b4c61cfab563be8a773a0bd1482	a review of performance, energy and privacy of intrusion detection systems for iot.		Internet of Things (IoT) is a disruptive technology with applications across diverse domains such as transportation and logistics systems, smart grids, smart homes, connected vehicles, and smart cities. Alongside the growth of these infrastructures, the volume and variety of attacks on these infrastructures has increased highlighting the significance of distinct protection mechanisms. Intrusion detection is one of the distinguished protection mechanisms with notable recent efforts made to establish effective intrusion detection for IoT and IoV. However, unique characteristics of such infrastructures including battery power, bandwidth and processors overheads, and the network dynamics can influence the operation of an intrusion detection system. This paper presents a comprehensive study of existing intrusion detection systems for IoT systems including emerging systems such as Internet of Vehicles (IoV). The paper analyzes existing systems in three aspects: computational overhead, energy consumption and privacy implications. Based on a rigorous analysis of the existing intrusion detection approaches, the paper also identifies open challenges for an effective and collaborative design of intrusion detection system for resource-constrained IoT system in general and its applications such as IoV. These efforts are envisaged to highlight state of the art with respect to intrusion detection for IoT and open challenges requiring specific efforts to achieve efficient intrusion detection within these systems.		Junaid Arshad;Muhammad Ajmal Azad;Khaled Salah;Wei Jie;Razi Iqbal;Mamoun Alazab	2018	CoRR		computer security;overhead (computing);computer science;the internet;smart grid;computer network;intrusion detection system;overhead (business);network dynamics;internet of things	Security	-46.03047554981655	51.13314011021561	27592
16bf552feacd5f34de264e6097376ff68aa963cb	selecting necessary and sufficient checkpoints for dynamic verification of fixed-time constraints in grid workflow systems	distributed system;gestion entreprise;groupware;systeme reparti;executable specification;execution time;redundancia;processus metier;fixed time;firm management;swinburne;temps minimal;journal article;checkpointing;grid;sistema repartido;workflow system;redundancy;rejilla;reference value;punto reanudacion;minimum time;grille;workflow;proceso oficio;temps execution;administracion empresa;point reprise;tiempo ejecucion;collecticiel;tiempo minimo;business process;redondance	In grid workflow systems, existing representative checkpoint selection strategies, which are used to select checkpoints for verifying fixed-time constraints at run-time execution stage, often select some unnecessary checkpoints and ignore some necessary ones. Consequently, overall temporal verification efficiency and effectiveness can be severely impacted. In this paper, we propose a new strategy that selects only necessary and sufficient checkpoints dynamically along grid workflow execution. Specifically, we introduce a new concept of minimum time redundancy as a key reference value for checkpoint selection. We also investigate its relationships with fixed-time constraint consistency. Based on these relationships, we present our strategy which can improve overall temporal verification efficiency and effectiveness significantly.	application checkpointing;formal verification;petri net;transaction processing system;verification and validation	Jinjun Chen;Yun Yang	2006		10.1007/11841760_37	workflow;real-time computing;simulation;computer science;operating system;database;distributed computing;business process;redundancy;grid;management;computer security;algorithm	AI	-49.7861289674926	39.77394994744307	27620
0f2e382bb4c8954cb6a48793824f0e434c048c45	pluggable personal data servers	smart card;secure device;medical care;field experiment;privacy protection;chip;storage model;storage capacity;elderly people;privacy	An increasing amount of personal data is automatically gathered on servers by administrations, hospitals and private companies while several security surveys highlight the failure of database servers to keep confidential data really private. The advent of powerful secure tokens, combining the security of smart card microcontrollers with the storage capacity of NAND Flash chips, introduces a credible alternative to the systematic centralization of personal data. By embedding a full-fledged database server in such device, an individual can now store her personal data in her own secure token, kept under her control, and never disclose in clear her private data to the outside untrusted world. This demonstration shows the benefit of the proposed approach in terms of privacy protection and pervasiveness through a healthcare scenario. This scenario is extracted from a field experiment where medical folders embedded in secure tokens are used to improve the coordination of medical care at home for elderly people. The demonstration also highlights interesting features of the embedded DBMS engine introduced to tackle the secure token's strong hardware constraints.	confidentiality;database server;embedded system;flash memory;information privacy;microcontroller;personally identifiable information;security token;server (computing);smart card	Nicolas Anciaux;Luc Bouganim;Yanli Guo;Philippe Pucheral;Jean-Jacques Vandewalle;Shaoyi Yin	2010		10.1145/1807167.1807328	chip;smart card;field experiment;computer science;database;internet privacy;privacy;world wide web;computer security	Security	-43.934763188480055	60.16798279431273	27625
e89114e5397b321b88b20e3204a00357eca3180c	externally verifiable oblivious ram		We present the idea of externally verifiable oblivious RAM (ORAM). Our goal is to allow a client and server carrying out an ORAM protocol to have disputes adjudicated by a third party, allowing for the enforcement of penalties against an unreliable or malicious server. We give a security definition that guarantees protection not only against a malicious server but also against a client making false accusations. We then give modifications of the Path ORAM [15] and Ring ORAM [9] protocols that meet this security definition. These protocols both have the same asymptotic runtimes as the semi-honest original versions and require the external verifier to be involved only when the client or server deviates from the protocol. Finally, we implement externally verified ORAM, along with an automated cryptocurrency contract to use as the external verifier.	autonomous robot;client (computing);communications protocol;correctness (computer science);cryptocurrency;formal verification;information privacy;iteration;malware;oblivious ram;random-access memory;runtime system;semiconductor industry;server (computing);smart contract;usability;zero-knowledge proof	Joshua Gancher;Adam Groce;Alex Ledger	2017	IACR Cryptology ePrint Archive	10.1515/popets-2017-0021	verifiable secret sharing;computer security;oblivious ram;computer science	Security	-53.43512823524728	58.97297267716965	27647
cdedd415b5ccc1e6f537edfe1aec011e0f523c01	stealthy deception attacks for cyber-physical systems		We study the security of Cyber-Physical Systems (CPS) in the context of the supervisory control layer. Specifically, we propose a general model of a CPS attacker in the framework of Discrete Event Systems (DES) and investigate the problem of synthesizing an attack strategy for a given controlled system. Our model captures a class of deception attacks, where the attacker has the ability to modify a subset of sensor readings and mislead the supervisor, with the goal of inducing the system into an undesirable state. We introduce a new type of a bipartite transition structure, called Insertion-Deletion Attack structure (IDA), to capture the game-like interaction between the supervisor and the environment (which includes the system and attacker). This structure is a discrete transition system that embeds information about all possible attacker's stealthy actions, and all states (some possibly unsafe) that become reachable as a result of those actions. We present a procedure for the construction of the IDA and discuss its properties. Based on the IDA, we discuss the characterization of successful stealthy attacks, i.e., attacks that avoid detection from the supervisor and cause damage to the system.	algorithm;cyber-physical system;insertion sort;liveness;observable;testbed;transition system;vii	Romulo Meira Goes;Eunsuk Kang;Raymond Kwong;Stéphane Lafortune	2017	2017 IEEE 56th Annual Conference on Decision and Control (CDC)	10.1109/CDC.2017.8264281	transition system;robustness (computer science);computer science;control theory;deception;cyber-physical system;distributed computing;supervisor;supervisory control	Logic	-55.354565226766056	52.28241592724298	27712
dc9126b9c6f9d2806edbecf57948176dde924dec	indoor location based services challenges, requirements and usability of current solutions	location based services;location privacy;indoor positioning	Indoor Location Based Services (LBS), such as indoor navigation and tracking, still have to deal with both technical and non-technical challenges. For this reason, they have not yet found a prominent position in people’s everyday lives. Reliability and availability of indoor positioning technologies, the availability of up-to-date indoor maps, and privacy concerns associated with location data are some of the biggest challenges to their development. If these challenges were solved, or at least minimized, there would be more penetration into the user market. This paper studies the requirements of LBS applications, through a survey conducted by the authors, identifies the current challenges of indoor LBS, and reviews the available solutions that address the most important challenge, that of providing seamless indoor/outdoor positioning. The paper also looks at the potential of emerging solutions and the technologies that may help to handle this challenge. © 2017 Elsevier Inc. All rights reserved.	indoor positioning system;location-based service;map;privacy;quality of service;requirement;seamless3d;usability;value (computer science)	Anahid Bassiri;Elena Simona Lohan;Terry Moore;Adam C. Winstanley;Pekka Peltola;Chris Hill;Pouria Amirian;Pedro Figueiredo Silva	2017	Computer Science Review	10.1016/j.cosrev.2017.03.002	computer science;location-based service;hybrid positioning system	HCI	-41.290342742981	53.33943046781377	27745
58e4377268ddff86ca6084c6cfc9242a7d764f4d	signal processing and the internet of things [from the guest editors]		The notion of the Internet of Things (IoT) has emerged as a last-mile solution for connecting various cyber technologies to our everyday life. It envisions a three-tier architecture where highly distributed and heterogeneous sensor data will be collected through a gateway and made available to the Internet to be readily accessible for a wide range of applications. Today, with ever increasing types of IoT devices as well as the growing demand being placed on the end user, the sensing platform, and the computing and storage infrastructure, more is being asked of engineers, designers, and scientists. Discusses how we can leverage today’s pervasive cloud and network infrastructure to foster more intriguing applications with more demanding signal processing and machine learning techniques.	internet of things;signal processing	Chenren Xu;Yan Lindsay Sun;Konstantinos N. Plataniotis;Nic Lane	2018	IEEE Signal Process. Mag.	10.1109/MSP.2018.2846838	world wide web;end user;computer science;theoretical computer science;architecture;signal processing;the internet;cloud computing;default gateway;internet of things;everyday life	Visualization	-43.14276458458277	49.44902028320854	27810
4556944e7e019b7cf892376b292db1214a20b977	a software library, a control backbone and user-specified recovery strategies to enhance the dependability of embedded systems	distributed system;maintainability software library control backbone user specified recovery strategies embedded systems dependability fault tolerance reusable elements user library high level description language development cycle;fault tolerant;software libraries;software maintenance;software maintenance fault tolerant computing software libraries embedded systems;software libraries spine fault tolerance fault tolerant systems application software fault detection runtime costs embedded software control systems;embedded system;embedded systems;fault tolerant computing	Fault tolerance in embedded distributed systems requires flexibility. Solutions should be based on pre-built and reusable elements, customisable for different applications and portable to different platforms. To this end, a framework is presented, consisting of a user library, a control backbone and a high-level description language (RL). The user library contains basic functions for fault tolerance. The control layer hooks these functions together and allows to co-ordinate actions. RL allows the developer to specify the recovery strategies of the application as a sort of second application layer; this separates functional from recovery aspects of an application, shortening the development cycle and improving maintainability. This paper describes the requirements to which this framework adheres and the context in which it can be instantiated; we detail the three entities and explain how an application can be made fault-tolerant in this framework approach.	dependability;embedded system;internet backbone;library (computing)	Geert Deconinck;Vincenzo De Florio;Rudy Lauwereins;Ronnie Belmans	1999		10.1109/EURMIC.1999.794767	embedded system;real-time computing;engineering;software framework;software development;software fault tolerance;software system;computer engineering;avionics software	Embedded	-38.12694754550118	38.999458438258536	27903
132d005ae958ee6c2152b786906ea828273aeb25	efficient model-driven service brokering using web services	efficient model-driven service brokering;web services;novel e-commerce prototype application;web servicesarchitecture;model-drivenservice brokerage;web service;legacy data system;large service providers;application componentsin heterogeneous computing;web services technology;internet access service;web service function;service oriented architecture;object oriented programming;prototypes;data systems;productivity;workflow engine;dsl;e commerce;service provider;distributed application;internet access;system testing;distributed programming;location services;net;electronic commerce;distributed computing;internet;heterogeneous computing;search engines	"""Web services are rapidly becoming the technology of choice for integrating distributed application components in heterogeneous computing environments. In this paper, we present a novel e-commerce prototype application that itself would not be feasible without the leverage provided by underlying Web services technologies for flexible design and rapid prototyping. The application is a model-driven service brokerage that allows service providers to model, advertise, validate, and create a wide range of services (e.g. Internet access services such as DSL, cable, video-on-demand, etc) in an open marketplace in an automated fashion. The application answers real needs expressed by today's large service providers and goes beyond the current state-of-the-art in online communication marketplaces. It consists of several functional components, i.e. a service dependency modeling tool, brokerage engine, location services, and workflow engine - that have been independently developed on diverse platforms, e.g., J2EE, .NET. To efficiently integrate these components, we have designed a distributed Web services architecture, in which one Web service functions as a """"hub"""" between previously disconnected components, another works as a """"wrapper"""" of legacy data systems, while another """"orchestrates"""" invocations of these services. Use of Web services has enabled parallel and independent development and testing, greatly increasing productivity and reducing time to get the system operational. It has also fostered the development of new brokerage features, which would have been difficult to plan without first experimenting with a """"live"""" system."""	.net framework;coherence (physics);component-based software engineering;computer-mediated communication;data system;digital subscriber line;distributed computing;e-commerce;electronic business;experiment;heterogeneous computing;internet access;java platform, enterprise edition;model-driven architecture;model-driven integration;openness;prototype;rapid prototyping;usb hub;web application;web service;workflow engine;xml	Kai Cheng;C. Chung;Munir Cochinwala;D. Egan;Benjamin Falchuk;C. Lee;Fuchun Joseph Lin;Hyong Sop Shim;John Wullert	2004	Proceedings. IEEE International Conference on Web Services, 2004.	10.1109/ICWS.2004.1314790	e-commerce;service provider;web service;the internet;internet access;computer science;operating system;ws-policy;service-oriented architecture;database;distributed computing;services computing;object-oriented programming;web 2.0;law;world wide web;symmetric multiprocessor system	HPC	-34.55661595412815	43.50859303722119	27950
6d07aa489f339f21400bd617282b35908f6c55d4	software-defined robotics - idea & approach		The methodology of Software-Defined Robotics hierarchical-based and stand-alone framework can be designed and implemented to program and control different sets of robots, regardless of their manufacturers’ parameters and specifications, with unified commands and communications. This framework approach will increase the capability of (re)programming a specific group of robots during the runtime without affecting the others as desired in the critical missions and industrial operations, expand the shared bandwidth, enhance the reusability of code, leverage the computational processing power, decrease the unnecessary analyses of vast supplemental electrical components for each robot, as well as get advantages of the most state-of-the-art industrial trends in the cloud-based computing, Virtual Machines (VM), and Robot-as-a-Service (RaaS) technologies.	cloud computing;electronic component;robot;robotics;virtual machine	Ali Al-Bayaty	2017	CoRR		software engineering;engineering;control engineering;software;robot;cloud computing;behavior-based robotics;robotic paradigms;artificial intelligence;virtual machine;reusability;robotics	Robotics	-42.538757409130575	38.13313189377377	28036
a97db0b9c9f1379b3e9505c477da3c0f8e8844e7	attacking supercomputers through targeted alteration of environmental control: a data driven case study	water resources;temperature measurement;valves;security;supercomputers;conferences;cooling	In this paper, we show that a malicious user can attack a large computing infrastructure by compromising the environmental control systems in the facilities that host the compute nodes. Such violations cannot be easily recognized by the administrators who manage the cluster, because of limited observation of the events in the cyber-physical systems. We describe real cases of failures due to problems in the cooling system of Blue Waters, the petascale supercomputer of the University of Illinois at Urbana-Champaign. Blue Waters has cooling cabinets that use chilled water provided by the National Petascale Computing Facility (NPCF). We demonstrate, using real data, that the control systems that provide chilled water can be used as entry points by an attacker to indirectly compromise the computing functionality through the orchestration of clever alterations of sensing and control devices. In this way, the attacker does not leave any trace of his or her malicious activity on the nodes of the cluster. Failures of the cooling systems can trigger unrecoverable failure modes that can be recovered only after service interruption and manual intervention.	blue waters;computer cooling;continuation;control system;cyber-physical system;failure cause;interrupt;malware;petascale computing;security hacker;supercomputer	Key-whan Chung;Valerio Formicola;Zbigniew T. Kalbarczyk;Ravishankar K. Iyer;Alexander Withers;Adam J. Slagell	2016	2016 IEEE Conference on Communications and Network Security (CNS)	10.1109/CNS.2016.7860528	water resources;real-time computing;simulation;telecommunications;temperature measurement;computer science;information security;operating system;computer security;computer network	HPC	-53.19535563801463	57.69181184440221	28106
8e76ffc3acd1360af5398ad93fdfa1f055d84f21	automated support for propagating bug fixes	graph theory;mining software repositories;graph matching algorithm bug fix propagation programming rule extraction procedure dependence graph;prototypes;dependence graph;data mining;programming rule extraction;graph matching;software engineering;mining software repositories bug detection static analysis data mining;software engineering graph theory;graph minor;bug fix propagation;heuristic algorithms;artificial intelligence;static analysis;graph matching algorithm;bug detection;computer bugs;programming;programming profession computer bugs cryptography software maintenance software reliability reliability engineering sun software debugging telephony switching systems;procedure dependence graph;solids	We present empirical results indicating that when programmers fix bugs, they often fail to propagate the fixes to all of the locations in a code base where they are applicable, thereby leaving instances of the bugs in the code. We propose a practical approach to help programmers to propagate many bug fixes completely. This entails first extracting a programming rule from a bug fix, in the form of a graph minor of an enhanced procedure dependence graph. Our approach assists the programmer in specifying rules by automatically matching simple rule templates; the programmer may also edit rules or compose them from scratch. A graph matching algorithm for detecting rule violations is then used to locate the places in the code base where the bug fix is applicable. Our approach does not require that rules occur repeatedly in the code base. We present empirical results indicating that the approach nevertheless exhibits good precision.	algorithm;graph minor;matching (graph theory);patch (computing);programmer;sensor;software bug	Boya Sun;Ray-Yaung Chang;Xianghao Chen;Andy Podgurski	2008	2008 19th International Symposium on Software Reliability Engineering (ISSRE)	10.1109/ISSRE.2008.29	programming;software bug;computer science;graph theory;theoretical computer science;operating system;solid;data mining;database;prototype;programming language;graph minor;static analysis;matching	SE	-59.74983945888709	37.05982111507021	28178
ff0c881b1405bf875d6641bd3cd695aeb5e9d807	wiikey: an innovative smartphone based wi-fi application	application software energy consumption internet wimax bluetooth mobile communication radiofrequency identification communication system security computer science mathematics;mobile device;design and development;0803 computer software;next generation wi fi enabled devices;mobile devices wiikey prototype innovative smartphone wi fi application mobile applications next generation wi fi enabled devices java;wireless communication;servers;mobile applications;innovative smartphone;wiikey;internet;8902 computer software and services;wiikey prototype;ieee 802 11 standards;mobile communication;next generation;mobile handsets;personal trust device;smart object;wireless lan;smartphone;mobile computing;personal trust device wi fi mobile applications smartphone wiikey;wi fi;wi fi application;mobile application;mobile devices;wireless lan java mobile computing mobile handsets;health engineering and science;communication system security;java	Many mobile applications such as email, calendar and web browser make use of smartphones technology. WiiKey, a proposed smart application, aims to extend the current smartphone capabilities to allow users to control the next generation Wi-Fi enabled devices found in some cars or home appliances. In this paper, Wi-Fi standards and their applications are first reviewed, and then the design and development of WiiKey prototype are described. Details of implementation of WiiKey using Java are presented. WiiKey has a potential to control smart objects in next generation of mobile devices. WiiKey brings benefits to users in their daily life and may one day become a true 'personal trust device'.	email;java;list of wii drivechips;mobile app;mobile device;prototype;smart objects;smartphone	Nguyen Vo;Hao Shi;Jakub Szajman	2008	2008 International Multi-symposiums on Computer and Computational Sciences	10.1109/IMSCCS.2008.37	embedded system;engineering;internet privacy;computer network	HCI	-38.86796714679618	53.37681147268657	28187
3edde1131ba72b22f11d0a7262b0d68be41c290c	application of the object-oriented principles for hardware and embedded system design	modelizacion;class diagram;vhdl language;calculateur embarque;articulo sintesis;concepcion sistema;red petri;article synthese;uml;lenguaje uml;separation of concern;langage modelisation unifie;synthese haut niveau;sintesis alto nivel;uml class diagram;modelisation;lenguaje vhdl;high level synthesis;design technique;lenguaje descripcion;embedded system design;system synthesis;object oriented;system design;synthese systeme;unified modelling language;design pattern;boarded computer;sintesis sistema;oriente objet;hardware design;object oriented hardware design;computer hardware;product design;software design;petri net;review;modeling;orientado objeto;materiel informatique;calculador embarque;langage vhdl;conception systeme;langage description;reseau petri;hardware;description language	As the complexity of hardware (HW) and embedded system design is constantly increasing, the researchers are seeking to develop new more abstract and productive design methods or adapt the existing ones from other domains such as software design. This paper addresses the problem of using the object-oriented (OO) design techniques in HW domain. The main OO design techniques are as follows: abstraction, separation of concerns, composition and generalization. The application of the OO design paradigm has many aspects: highlevel specification of HW models using OO formal notations such as Petri Nets and UML diagrams, HW description using OO HW description languages such as VHDL extensions and SystemC, HW design using OO HW architectures, platforms and design patterns. In this paper, we present a comprehensive overview of the application of the OO design paradigm in HW and embedded system design domains and formulate its main principles, discuss the current achievements in the area, and outline the future trends.	algorithmic efficiency;co-simulation;complex systems;computer hardware;design pattern;diagram;embedded system;emoticon;executable;parallel computing;petri net;programming paradigm;real-time clock;separation of concerns;shattered world;simulation;software design;systemc;systems design;uml state machine;unification (computer science);unified modeling language;vhdl	Robertas Damaševičius;Vytautas Stuikys	2004	Integration	10.1016/j.vlsi.2004.08.005	unified modeling language;computer science;systems engineering;class diagram;product design;programming language;algorithm	EDA	-40.92611903956116	33.489671582311715	28223
b8326e53f486dd0b7736d3454880f29732fb4967	test-case generation with iogen	verification;look ahead token;software testing application software visualization;program diagnostics;ll 1 grammar;ada;execution paths;look ahead;left to right;symbolic execution;test case generation;program testing;left to right scan;software interfaces;software tools;static analysis tools;static analysis tool;ll 1 grammar test case generation iogen static analysis tool ada verification software reliability symbolic execution i o pairs execution paths software interfaces left to right scan look ahead token;software reliability;iogen;i o pairs;software tools ada program diagnostics program testing software reliability	The IOGen static-analysis tool for a subset of Ada addresses part of the problem of verification of software reliability. It uses a technique based on symbolic execution and produces a set of I/O pairs that represent execution paths through a program. The authors present IOGen's design and demonstrate how to use it to test programs and validate Ada software interfaces. Although IOGen is specific to an Ada subset, the same technique can be applied to any language with a grammar that uses a left-to-right scan with one look-ahead token producing a leftmost derivation (an LL(1) grammar).<<ETX>>	ada;context-free grammar;input/output;software reliability testing;symbolic execution	Timothy E. Lindquist;Joyce R. Jenkins	1988	IEEE Software	10.1109/52.1996	computer architecture;real-time computing;verification;ada;computer science;programming language;software quality;static program analysis	SE	-58.24096069130045	36.44765781875033	28224
00ed6f708b5ebd4953ef8e67ddab4dfa07dc80c9	scalable static analysis to detect security vulnerabilities: challenges and solutions		Parfait [1] is a static analysis tool originally developed to find implementation defects in C/C++ systems code. Parfait's focus is on proving both high precision (low false positives) as well as scaling to systems with millions of lines of code (typically requiring ~10 minutes of analysis time per million lines). Parfait has since been extended to detect security vulnerabilities in applications code, supporting the Java EE and PL/SQL server stack. In this abstract we describe some of the challenges we encountered in this process including some of the differences seen between the applications code being analysed, our solutions that enable us to analyse a variety of applications, and a summary of the challenges that remain.	c++;image scaling;java platform, enterprise edition;libressl;microsoft sql server;pl/sql;server (computing);source lines of code;static program analysis;vulnerability (computing)	F. A. O. Gauthier;Nathan Keynes;Nicholas Allen;Diane Corney;Padmanabhan Krishnan	2018	2018 IEEE Cybersecurity Development (SecDev)	10.1109/SecDev.2018.00030	parallel computing;sql;scalability;static analysis;computer science;java;source lines of code	Security	-58.44163884491736	39.75422485980259	28227
2d892c4218b1b8144a12c5f25e0bbf8304187f3d	automated property verification for large scale b models with prob	model checking;constraint solving;tools;industrial applications	In this paper we describe the successful application of the ProB tool for data validation in several industrial applications. The initial case study centred on the San Juan metro system installed by Siemens. The control software was developed and formally proven with B. However, the development contains certain assumptions about the actual rail network topology which have to be validated separately in order to ensure safe operation. For this task, Siemens has developed custom proof rules for Atelier B. Atelier B, however, was unable to deal with about 80 properties of the deployment (running out of memory). These properties thus had to be validated by hand at great expense, and they need to be revalidated whenever the rail network infrastructure changes. In this paper we show how we were able to use ProB to validate all of the about 300 properties of the San Juan deployment, detecting exactly the same faults automatically in a few minutes that were manually uncovered in about one man-month. We have repeated this task for three ongoing projects at Siemens, notably the ongoing automatisation of the line 1 of the Paris Métro. Here again, about a man month of effort has been replaced by a few minutes of computation. This achievement required the extension of the ProB kernel for large sets as well as an improved constraint propagation algorithm. We also outline some of the effort and features that were required in moving from a tool capable of dealing with medium-sized examples towards a tool able to deal with actual industrial specifications. We also describe the issue of validating ProB, so that it can be integrated into the SIL4 development chain at Siemens.	algorithm;computation;data validation;local consistency;network topology;out of memory;sensor;software deployment;software propagation	Michael Leuschel;Jérôme Falampin;Fabian Fritz;Daniel Plagge	2010	Formal Aspects of Computing	10.1007/s00165-010-0172-1	model checking;simulation;computer science;programming language	SE	-58.319646017450225	37.89147788830188	28293
fd6eac23387e3c5604522eca0882cdbdefe405d6	integrating software traceability for change impact analysis	change impact analysis	Software maintenance is recognized as the most costly activity in software engineering with typical estimates of more than half of the software development cost. The main problem to a maintainer is that seemingly small changes can ripple throughout the system to cause substantial impact elsewhere. Software traceability and its subsequent impact analysis help relate the consequences or ripple-effects of a proposed change across different levels of software models. In this paper, we present a software traceability approach to support change impact analysis of object oriented software. The significant contribution in our traceability approach can be observed in its ability to integrate the high level with the low level software models that involve the requirements, test cases, design and code. Our approach allows a direct link between a component at one level to other components at any levels. It supports the top down and bottom up traceability in response to tracing for the ripple-effects. We developed a software prototype called Catia to support C++ software, applied it to a case study of an embedded system and discuss the results.	c++;catia;cost estimation in software engineering;embedded system;graphical user interface;high-level programming language;inline linking;mind;prototype;requirement;ripple effect;software development;software maintainer;software maintenance;software prototyping;static program analysis;test case;text-based (computing);top-down and bottom-up design;traceability;visual artifact	Suhaimi Ibrahim;Norbik Bashah Idris;Malcolm Munro;Aziz Deraman	2005	Int. Arab J. Inf. Technol.		change management;verification and validation;software sizing;computer science;package development process;software framework;software development;software design description;software construction;software deployment;requirements traceability;software metric;change impact analysis;avionics software	SE	-57.00565540010889	33.365111715101165	28352
1cb69e8f599b70e9a4471aeadd93fd7b6bfa8387	usd card: a plug&play solution for mobile device to access wireless sensor networks	internet of things;mobile sink;universal sensor data card;wireless sensor networks	The rapid development of wireless sensor network applications and mobile devices has brought the need for a universal plug & play solution to access wireless sensors and sensor network on the go. By reusing the legacy SD memory card slot built in mobile devices, we have developed a platform-independent method to extend low power short range connectivity for mobile devices, namely, universal Sensor Data (uSD) card. The paper gives a comprehensive description to hardware and software design for uSD card, and provides some examples on implementing mobile sink function on mobile devices.	mobile device;plug and play	Canfeng Chen;Xin Zhang;Jinfeng Zhang;Yuezhong Tang	2011		10.1007/978-3-642-23490-3_33	embedded system;wireless sensor network;mobile web;computer science;key distribution in wireless sensor networks;mobile station;mobile wireless sensor network;mobile computing;computer security;internet of things;computer network	Mobile	-39.56301992254563	48.6953243338682	28385
4f762aa3881094a1c4de7fff2dac8d8f3c1106cf	design and optimization of multiclocked embedded systems using formal techniques	control systems;standards;automata clocks synchronization standards control systems embedded systems asynchronous communication;clocks;real time control;embedded computer systems design and construction;automata;embedded systems;system on chip automata theory embedded systems formal specification iec standards optimisation program debugging;synchronization;asynchronous communication;synchronous data transmission systems;real world subway communication control system multiclocked embedded systems formal techniques system on chip distributed systems asynchronous communications multiclock train control system multiprocessor architecture control oriented behaviors interclock domain synchronous dataflow module timed automata standard iec 61 375 bugs	Today's system-on-chip and distributed systems are commonly equipped with multiple clocks. The key challenge in designing such systems is that two situations have to be captured and evaluated in a single framework. The first is the heterogeneous control-oriented and data-oriented behaviors within one clock domain, and the second is the asynchronous communications between two clock domains. In this paper, we propose to use timed automata and synchronous dataflow to model the dynamic behaviors of the multiclock train-control system, and a multiprocessor architecture for the implementation from our model to the real system. Data-oriented behaviors are captured by synchronous dataflow, control-oriented behaviors are captured by timed automata, and asynchronous communications of the interclock domain can be modeled as an interface timed automaton or a synchronous dataflow module. The behaviors of synchronous dataflow are interpreted by some equivalent timed automata to maintain the semantic consistency of the mixed model. Then, various functional properties that are important to guarantee the correctness of the system can be simulated and verified within the framework. We apply the framework to the design of a control system described in the standard IEC 61 375 and several bugs are detected. The bugs in the standard have been fixed, and the new version has been implemented and used in the real-world subway communication control system.	automata theory;central processing unit;clock signal;code;control system;correctness (computer science);dataflow;distributed computing;embedded system;experiment;input/output;mixed model;multiprocessing;program optimization;scalability;shared variables;software bug;system on a chip;systems design;timed automaton;uppaal;vhdl	Yu Jiang;Hehua Zhang;Zonghui Li;Yangdong Deng;Xiaoyu Song;Ming Gu;Jia-Guang Sun	2015	IEEE Transactions on Industrial Electronics	10.1109/TIE.2014.2316234	embedded system;synchronization;real-time computing;real-time control system;computer science;control system;artificial intelligence;asynchronous communication;distributed computing;automaton	Embedded	-38.82628658088816	33.32921201384044	28469
fe46cb09aebc04822a60d690195f5ba63104e79d	jcrypt: towards computation over encrypted data	encryption scheme inference;information flow;polymorphism;data confidentiality;security	Cloud computing allows clients to upload data and computation to untrusted servers, which leads to potential violations to the confidentiality of client data. We propose JCrypt, a static program analysis which transforms a Java program into an equivalent one, so that it performs computation over encrypted data and preserves data confidentiality. JCrypt minimizes computation over encrypted data. It consists of two stages. The first stage is a type-based information flow analysis which partitions the program so that only sensitive parts need to be encrypted. The second stage is an inter-procedural data-flow analysis, similar to the classical Available Expressions. It deduces the appropriate encryption scheme for sensitive variables. We implemented JCrypt for Java and showed that our analysis is effective and practical using five benchmark suites. JCrypt encrypts a significantly larger percentage of benchmarks compared to MrCrypt, the closest related work.	benchmark (computing);cloud computing;computation;confidentiality;data-flow analysis;dataflow;encryption;information flow (information theory);java;static program analysis;upload	Yao Dong;Ana Milanova;Julian Dolby	2016		10.1145/2972206.2972209	polymorphism;information flow;confidentiality;computer science;information security;theoretical computer science;data mining;database;programming language	PL	-55.64457624980825	55.17543091304294	28499
82e51ef20a476136320b391d5063cec48907e90e	an architecture for self-reconfiguration of convergent telecom processes		A convergent process is usually defined as a composition of telecommunication and web services. Automated composition of convergent processes has been addressed actively in the last years. However, during execution phases some services may fail and therefore some mechanisms must be implemented for recovering automatically the normal execution. Furthermore, in Telecommunication environments, this process may be time-consuming and may violate the initial constraints established by the user's context and preferences. Our approach focuses in reducing the reconfiguration time while holding the initial constraints. To achieve this goal, this paper presents an iterative algorithm which does not replace individual services but whole regions of services, specified with Hierarchical Tasks Networks (HTNs). This algorithm is part of the reconfiguration module of the AUTO framework, whose architecture and performance are discussed to show that our approach can efficiently repair convergent processes in telecom environments.	algorithm;data recovery;iterative method;web service	Armando Ordóñez;Jesus David Ramirez;Paolo Falcarin;Oscar Mauricio Caicedo Rendon;Lisandro Zambenedetti Granville	2015	2015 IEEE/ACM 7th International Workshop on Principles of Engineering Service-Oriented and Cloud Systems		embedded system;real-time computing;computer science;distributed computing	SE	-46.625945124367895	41.05499306097511	28504
73440e91aeb94fcfb20432c5210a43e15e09643f	a theoretical framework for understanding mutation-based testing methods	mutant set minimization mutation based testing method test set selection fault localization program repair test differentiator mathematical vector d vector multidimensional space graphical model;software testing;solids systematics maintenance engineering syntactics software testing electronic mail;electronic mail;mutation based testing theoretical framework;theoretical framework;systematics;maintenance engineering;vectors minimisation program testing program verification;syntactics;mutation based testing;solids	In the field of mutation analysis, mutation is the systematic generation of mutated programs (i.e., mutants) from an original program. The concept of mutation has been widely applied to various testing problems, including test set selection, fault localization, and program repair. However, surprisingly little focus has been given to the theoretical foundation of mutation-based testing methods, making it difficult to understand, organize, and describe various mutation-based testing methods. This paper aims to consider a theoretical framework for understanding mutation-based testing methods. While there is a solid testing framework for general testing, this is incongruent with mutation-based testing methods, because it focuses on the correctness of a program for a test, while the essence of mutation-based testing concerns the differences between programs (including mutants) for a test. In this paper, we begin the construction of our framework by defining a novel testing factor, called a test differentiator, to transform the paradigm of testing from the notion of correctness to the notion of difference. We formally define behavioral differences of programs for a set of tests as a mathematical vector, called a d-vector. We explore the multi-dimensional space represented by d-vectors, and provide a graphical model for describing the space. Based on our framework and formalization, we interpret existing mutation-based fault localization methods and mutant set minimization as applications, and identify novel implications for future work.	correctness (computer science);differentiator;formal grammar;graphical model;mathematical optimization;mutation testing;programming paradigm;test set	Donghwan Shin;Doo-Hwan Bae	2016	2016 IEEE International Conference on Software Testing, Verification and Validation (ICST)	10.1109/ICST.2016.22	maintenance engineering;reliability engineering;model-based testing;computer science;engineering;theoretical computer science;software engineering;functional testing;solid;systematics;software testing;test management approach;algorithm	SE	-59.37046045104758	34.27211448156834	28592
3cfd6eadd3d3eedb93527378b38e8bab34f6899d	property networks allowing oracle-based mode-change propagation in hierarchical components	embedded control system;sofa;embedded system;embedded systems;software architecture;component framework;software variability;modes for software architectures;system architecture;change propagation;real time systems	Strong pressure on deployment of embedded control systems on a low-cost hardware leads to the need of optimizing software architectures to minimize resource demands. Nevertheless, releasing the resources not needed in specific phases of system execution is only rarely supported by todays component frameworks, mainly since information about the system state is spread over several components, which makes the idea hard to implement.  The paper introduces a formal model of property networks allowing for efficient capture of modifications of architecture-relevant information and shows, how this model can be used to employ the concept of modes for system architectures in hierarchical component systems.	control system;embedded system;mathematical model;software deployment;software propagation	Tomás Pop;Frantisek Plasil;Matej Outly;Michal Malohlava;Tomás Bures	2012		10.1145/2304736.2304753	embedded system;software architecture;real-time computing;computer science;component-based software engineering;distributed computing;systems architecture;software system;avionics software	Embedded	-39.639900338873815	38.91297939996065	28600
c2959f78f952e96168fc723c50727aa37d9b8a0c	code clone graph metrics for detecting diffused code clones	software metrics;cloning visualization software maintenance costs software systems large scale systems open source software software engineering information science application software;code clone graph metrics;software maintenance;probability density function;program comprehension;software systems;maintenance engineering;program verification;data mining;large scale software;cloning;diffused code clones detection;data visualisation;large scale;visualization;open source software programs;code clone;software metrics data visualisation program verification software maintenance;source code;program comprehension visualization software maintenance;code clone visualization method;open source software programs code clone graph metrics diffused code clones detection large scale software code clone visualization method;open source software	Code clones (duplicated source code in a software system) are one of the major factors in decreasing maintainability. Many code clone detection methods have been proposed to find code clones automatically from large-scale software. However, it is still hard to find harmful code clones to improve maintainability because there are many code clones that should remain. Thus, to help find harmful code clones, we propose a code clone visualization method and a metrics application on the visualized information. Our method enables the location of harmful code clones diffused in a software system. We apply our method to three open source software programs and visualize their code clone information.	duplicate code;open-source software;qr code;software system	Yoshihiko Fukushima;Raula Gaikovina Kula;Shinji Kawaguchi;Kyohei Fushida;Masataka Nagura;Hajimu Iida	2009	2009 16th Asia-Pacific Software Engineering Conference	10.1109/APSEC.2009.53	maintenance engineering;probability density function;visualization;computer science;operating system;software engineering;cloning;database;programming language;software maintenance;software metric;software system;duplicate code;source code	SE	-57.94289905384084	36.02686728395869	28641
9ad49e1dd2676f14573e35997d04f5b3f6746701	security of web browser scripting languages: vulnerabilities, attacks, and remedies	security analysis;scripting language	While conducting a security analysis of JavaScript and VBScript, the most popular scripting languages on the Web, we found some serious aws. Motivated by this outcome, we propose steps towards a sound de nition and design of a security framework for scripting languages on the Web. We show that if such a security framework had been integrated into the respective scripting languages from the very beginning, the probability of preventing the multiple security aws, that we and other research groups identi ed, would have been greatly increased.	javascript;scripting language;vbscript;world wide web	Vinod Anupam;Alain J. Mayer	1998			html scripting;cross-site scripting;web application security;security through obscurity;content security policy;computer science;server-side scripting;scripting language;internet privacy;cross-site request forgery;client-side scripting;security analysis;programming language;world wide web;computer security;cross-zone scripting	Security	-55.021931569810086	60.12529465656755	28662
e4741b14b64390c025b78cd7a19fecabb4fb3d22	supervisory controller synthesis for safe software adaptation	datavetenskap datalogi;computer science	Today’s software systems need to adapt their behavior due to the changes in their operational environments and user requirements. To this end, an adaptive software performs a sequence of adaptations at runtime. Correctness of the behavior of an adaptive software system during dynamic adaptation is an important challenge along the way to realize correct adaptive systems. In this research, we model adaptation as a supervisory control problem and synthesize a controller that guides the behavior of a software system during adaptation. The system during adaptation is modeled using a graph transition system and properties to be enforced are specified using an automaton. To ensure correctness, we then synthesize a controller that imposes constraints on the system during adaptation.	adaptive system;automaton;component-based software engineering;correctness (computer science);game theory;instability;read-write memory;requirement;run time (program lifecycle phase);software system;transition system;user requirements document	Narges Khakpour;Farhad Arbab;Éric Rutten	2014		10.3182/20140514-3-FR-4046.00035	control engineering;real-time computing;simulation;engineering;adaptation	SE	-42.0460883747659	37.62420718584957	28664
8aa64dac7f135abeacf9875a48e62c81f57a296d	dynamic information-flow analysis for multi-threaded applications	runtime monitoring;dedicated runtime predictive approach;information-flow property;confidentiality breach;execution platform;multi-threaded application;dynamic information-flow analysis;information-flow analysis;single parallel execution trace;particular context;existing information-flow analysis technique	Information-flow analysis is one of the promising techniques to leverage the detection of software vulnerabilities and confidentiality breaches. However, in the context of multi-threaded applications running on multicore platforms, this analysis becomes highly challenging due to data races and inter-processor dependences. In this paper we first review some of the existing information-flow analysis techniques and we discuss their limits in this particular context. Then, we propose a dedicated runtime predictive approach. It consists in extending information-flow properties computed from a single parallel execution trace to a set of valid serialisations with respect to the execution platform. This approach can be applied for instance in runtime monitoring or security testing of multi-threaded applications.	confidentiality;data-flow analysis;multi-core processor;overhead (computing);program slicing;prospective search;prototype;requirement;run time (program lifecycle phase);security testing;taint checking;thread (computing);vulnerability (computing)	Laurent Mounier;Emmanuel Sifakis	2012		10.1007/978-3-642-34026-0_27	real-time computing;computer science;data mining;distributed computing;runtime verification	SE	-55.37890803955144	54.710016377830826	28756
1271cfb72ca81ea34290c7ee755ed8672561fdf9	securi cad by foreseeti: a cad tool for enterprise cyber security management	analytical models;software;firewalls computing;security of data cad decision making graph theory;decision making capability securicad foreseeti cad tool enterprise cyber security management kth royal institute of technology kth spin off company attack graph built in security expertise visualization security assessment scenario comparison;cyber security;computer architecture;computational modeling;analysis cyber security enterprise architecture modeling;analysis;computer architecture computational modeling software analytical models firewalls computing;modeling;enterprise architecture	This paper presents a CAD tool for enterprise cyber security management called securi CAD. It is a software developed during ten years of research at KTH Royal Institute of Technology, and it is now being commercialized by foreseeti (a KTH spin-off company). The idea of the tool is similar to CAD tools used when engineers design and test cars, buildings, etc. Specifically, the securi CAD user first models the IT environment, an existing one or one under development, and then securi CAD, using attack graphs, calculates and highlights potential weaknesses and avenues of attacks. The main benefits with securi CAD are, 1) built in security expertise, 2) visualization, 3) holistic security assessments, and 4) scenario comparison (decision-making) capabilities.	computer security;computer-aided design;holism;security management	Mathias Ekstedt;Pontus Johnson;Robert Lagerström;Dan Gorton;Joakim Nydren;Khurram Shahzad	2015	2015 IEEE 19th International Enterprise Distributed Object Computing Workshop	10.1109/EDOCW.2015.40	systems modeling;computer science;systems engineering;engineering;operating system;software engineering;analysis;data mining;database;enterprise architecture;computational model;computer security;enterprise information security architecture	EDA	-57.6020138958674	48.33946202236286	28798
ff7b9f7d83bbba5bb5194b802ffc5391e44e07da	challenges and opportunities in securing industrial control systems	protocols process control security performance evaluation data mining context computer architecture;protocols;computer network security;performance evaluation;security countermeasures industrial control system security ics security industrial process monitoring industrial process operation ics infrastructures cyber threats;data mining;computer architecture;process monitoring;industrial control;process control;process monitoring computer network security industrial control;security;context	Industrial Control Systems (ICS) are used for operating and monitoring industrial processes. Recent reports state that current ICS infrastructures are not sufficiently protected against cyber threats. Unfortunately, due to the specific nature of these systems, the application of common security counter-measures is often not effective. This paper summarizes experiences over a series of research efforts for building tools and mechanisms to improve the security and awareness in ICS. In particular, we discuss challenges and opportunities identified during an extensive analysis of ICS data resources. We believe that such insights are valuable for further research in the ICS context.	control system;multilevel model;threat (computer)	Dina Hadziosmanovic;Damiano Bolzoni;Sandro Etalle;Pieter H. Hartel	2012	2012 Complexity in Engineering (COMPENG). Proceedings	10.1109/CompEng.2012.6242970	control system security;computer security model;cloud computing security;reliability engineering;security through obscurity;engineering;security service;security testing;network security policy;computer security;computer engineering;scada	SE	-56.6342138627056	50.3899667804542	28852
2af8d7ec3e2178d4a872298e317a53fce1ce0d25	mitigating anti-forensics in the cloud via resource-based privacy preserving activity attribution		The multi-tenant Cloud environment creates a plethora of both technical and legal difficulties for digital forensics. Digital forensics typically relies on the analysis of evidence images or residual data. Due to the distributed nature of cloud environments can cause the required dataset to scale rapidly. When coupled with diversely heterogeneous environments, the widespread uptake of anti-forensic data and encryption, privacy requirements and a non-standardised architecture across Cloud Service Providers (CSPs), attribution of any activity for a forensic investigation becomes a tedious task. This paper presents an architecture agnostic, privacy-preserving solution to reducing the digital forensics target search space of a investigation within cloud and edge computing environments which will leverage standard metering and network logs for efficient activity attribution.	adobe streamline;anti-computer forensics;cloud computing;edge computing;encryption;fractal dimension;iterative method;multitenancy;privacy;requirement;traffic analysis;while;xfig	Adeyinka Odebade;Thomas Welsh;Siyakha Mthunzi;Elhadj Benkhelifa	2017	2017 Fourth International Conference on Software Defined Systems (SDS)	10.1109/SDS.2017.7939155	architecture;data mining;encryption;cloud computing;network forensics;computer security;attribution;digital forensics;computer science;edge computing	SE	-50.086542883381	58.029558400588186	28880
7cb23a88f8870777a9710a9fa096ce4b6e6e20dc	protecting the hosted application server	application server;client server systems;application integration;internet;distributed object management;telecommunication security;internet application;telecommunication security internet client server systems distributed object management security of data;object request broker;security of data;outsourced application services hosted application server internet applications web server application server world wide web back end systems corba common object request broker organisations firewall corbagate gateway proxies object key re mapping omg specification;protection web server internet middleware data security laboratories concrete java html information filtering	Internet applications, are evolving from the web server to the more powerful and dynamic application server in order to support the deployment of complex applications integrated with the organization’s back end systems. A key element of the application server architecture is CORBA, the Common Object Request Broker that allows applications to communicate in a transparent and interoperable manner. For this new architecture to succeed, it must guarantee a secure processing environment to the organisations. This paper explains why a conventional firewall can not be used to secure CORBA applications. It describes the architecture of CORBAgate, a gateway based on the concept of proxies and object key re-mapping. The paper compares the CORBAgate solution with the OMG specification for CORBA firewall security. Finally, the paper discusses how these elements combine to enable outsourced application services.	access control;application server;application-level gateway;client (computing);common object request broker architecture;firewall (computing);general inter-orb protocol;interoperability;multitier architecture;outsourcing;proxy server;sms language;server (computing);software deployment;web server	Paola Dotti;Owen Rees	1999		10.1109/ENABL.1999.805193	web application security;interoperable object reference;the internet;clickstream;computer science;microsoft transaction server;object request broker;web api;operating system;internet authentication service;appleshare;common object request broker architecture;internet security;database;distributed computing;law;world wide web;computer security;application server;client–server model;server farm	Security	-46.30426760627353	54.58123618469386	28920
718c34cd442ae4a43f0eba4641abffcde6b08523	msf: an efficient mobile phone sensing framework		Recent evolutions in smartphones, today provided with several sensors, have the strong processing capabilities needed to extract from raw sensed data sensor meaningful high-level views of the physical context around the user. A new promising research area called mobile sensing promotes completely decentralized sensing based on smartphone capabilities only. However, current mobile sensing solutions are not very mature; yet, because they are based on ad hoc software solutions tailored to one specific technical problem (e.g., power management, resource locking, etc.), they are difficult to reuse and integrate in different projects, and they do not focus on the performance efficiency of the monitoring support. To overcome those limitations, this paper proposes Mobile Sensing Framework (MSF), a flexible platform to ease the development of mobile sensing applications through the definition of a common set of facilities that mask all low-level technical details in reading and processing raw sensor data. MSF has been optimized also to enhance performances for Android-based systems, and we report an extensive set of experimental results that assess our architecture and quantitatively compare it with a selection of other mobile sensing systems by showing that MSF outperforms them by presenting lower CPU usage and memory footprints.	microsoft solutions framework;mobile phone	Giuseppe Cardone;Andrea Cirri;Antonio Corradi;Luca Foschini;Dario Maio	2013	IJDSN	10.1155/2013/538937	embedded system;real-time computing;simulation	Mobile	-41.352727009885214	48.996809228643905	28931
ef1328334e9c8885aef33602f951d0b6aeff4372	adapting ble protocol for medical standard ecg transmission		Smart solutions for cardiac monitoring are need of the hour in tackling the devastating effects of heart diseases that are plaguing the world today. Cardiac monitoring and screening technologies like Electrocardiography (ECG) need to be moved out from within the confines of hospital walls to places within easy reach of the wider mass of the countrys population; through remote deployment in primary care centers and homes. Affordability, accuracy and deep penetration are key to the success of any envisaged solution. The accuracy of reports generated by a remotely deployed ECG monitor primarily depend on the accuracy of the actual hardware making the measurement, and can be guaranteed by choosing any of the time tested bio-potential measurement ICs. The penetration however depends on the technology deployed for remote access of the ECG device. This technology must have the potential to be easily integrated into the minimal networking infrastructure existing in the rural areas and hinterlands of the country. A combination of Bluetooth Low Energy (BLE) and cellular networks, utilizing a smartphone, supporting minimum features, as a common access point is a promising solution for remotely accessing an ECG monitor deployed in rural areas. The challenge however is in developing methods to reliably transmit ECG data over both the BLE link and cellular networks meeting all the QoS requirements of medical standard ECG monitoring application. In this paper we first list out a set of QoS requirements for ECG monitoring application, drawn from discussions with cardiologists and from studying existing literature. We then explore the feasibility of a BLE link meeting these requirements, with supporting results from tests performed with a prototype which we developed.	bluetooth;british informatics olympiad;integrated circuit;prototype;quality of service;requirement;smartphone;software deployment;wireless access point	Sudheer Babu;Sai Prem Shaji;Abhinay Vishwanatham;K. V. Sai Sundar;T. Amar Sainath Reddy;Siva Sankara Sai Sanagapati;Satyabrataa Mohanty	2017	2017 IEEE International Conference on Advanced Networks and Telecommunications Systems (ANTS)	10.1109/ANTS.2017.8384166	throughput;software deployment;quality of service;cardiac monitoring;computer network;population;computer science;cellular network;bandwidth (signal processing);server	Mobile	-42.690894672423454	52.801740257085655	28974
265ab9be368928c450e56fe9098ee175a88507d0	bluemall: a bluetooth-based advertisement system for commercial areas	new technology;bluemall;context aware;mobile device;pervasive system;indexing terms;ubiquitous computing;bluetooth;wireless technology;context aware systems;java	Recently, research for context-aware systems has risen significantly. New social needs require the use of brand new technology to build rich ubiquitous computing spaces. Such spaces allow users to use services without being aware of any interaction with the system. For such an important and dedicated task, some context awareness is needed so computers, based on their environment, can react accordingly. Advertising on mobile devices is one of this new needs and has demonstrated to have a large potential due to the very personal and intimate nature of the devices and the possibility of reaching a broad range of targets. In this paper we present BlueMall, a context-aware pervasive system for advertising in large commercial areas. BlueMall uses Java and Bluetooth wireless technology making it a very portable system. We describe the overall architecture and discuss the implementation steps taken to build our application. To evaluate the performance behavior of our system in relation to the environment, we run some experiments in our testbed. Experimental results show that the system provides a viable solution for permission-based mobile advertising.	bluetooth;computer;context awareness;context-aware pervasive systems;experiment;java;mobile device;pervasive informatics;testbed;ubiquitous computing	José-María Sánchez;Juan-Carlos Cano;Carlos Miguel Tavares Calafate;Pietro Manzoni	2008		10.1145/1454630.1454633	embedded system;real-time computing;index term;computer science;operating system;mobile device;bluetooth;java;world wide web;ubiquitous computing;computer network	HCI	-38.21247259018611	52.03107149338598	29009
f273a6109f37410f08157ab506eca55404d987ed	tools to support formal methods	prototypes;formal specifications;distributed computing;permission prototypes software design fault detection distributed computing machinery formal specifications;formal method;hybrid analysis techniques;permission;fault detection;static analysis;software design;machinery;dynamic analysis	signer to reason about llie ~~ropertirs of the sl~ecilica.bioii a.litl so prevent the propaga,tion of errors [7]. A key f&or in the acceptance of high level progra~mning languages has been the tleveloptnent of a coinprelieusive set of lCoOls to support the user. If formal langua.ges for syecifimtioil are lo a,cliieve the sa.iue level of accq)ta.nce, lliey too will require extensive automated support.. This paper describes a. set of prototype 1001s wliich a.re designctl t,o nssist the developer in the use of formal specilhtioii techiqrles. Despite the tlieorelical aclva.nlagcs of using forinal iiotalions for soflwa.re specZca.tioii t.liere a.re significant practical ba.rriers t.0 llieir a.tloption [S]. Il’gl 1 I 011 the list, of 1.11rse pearlicnl hrriers is the absence of ally a.rilonla.tetl 1001s wllicli assist the pmclitioner to crea.te a.ntl untlrrsta.ntl 111e f0rn1a.I spmzifica.tion [S].	formal methods;high-level programming language;prototype	Shrujal Patel;Rodney A. Orr;Mark T. Norris;David W. Bustard	1989		10.1145/74587.74604	machine;real-time computing;formal methods;computer science;systems engineering;software design;operating system;software engineering;formal specification;prototype;dynamic program analysis;programming language;static analysis;fault detection and isolation;computer engineering	DB	-48.69685545261032	33.21412983320995	29029
bb8202e0cc4b4ac74a19988006b38906500302f0	requirements-based access control analysis and policy specification (recaps)	empirical study;software tool;computacion informatica;software systems;grupo de excelencia;software development process;software engineering;method integration;policy specification;requirements engineering;requirement analysis;access control policy;operating system;ciencias basicas y experimentales;system design;security and privacy;access control;technical report;security;requirement specification;requirements analysis	HE, QINGFENG. Requirements-Based Access Control Analysis and Policy Specification. (Under the direction of Dr. Ana (Annie) I. Antón.) Access control is a mechanism for achieving confidentiality and integrity in software systems. Access control policies (ACPs) define how access is managed and the high-level rules of who can access what information under certain conditions. Traditionally, access control policies have been specified in an ad-hoc manner, leaving systems vulnerable to security breaches. ACP specification is often isolated from requirements analysis, resulting in policies that are not in compliance with system requirements. This dissertation introduces the Requirements-based Access Control Analysis and Policy Specification (ReCAPS) method for deriving access control policies from various sources, including software requirements specifications (SRS), software designs, and high-level security/privacy policies. The ReCAPS method is essentially an analysis method supported by a set of heuristics and a software tool: the Security and Privacy Requirements Analysis Tool (SPRAT). The method was developed in two formative case studies and validated in two summative case studies. All four case studies involved operational systems, and ReCAPS evolved as a result of the lessons learned from applying the method to these case studies. Further validation of the method was performed via an empirical study to evaluate the usefulness and effectiveness of the approach. Results from these evaluations indicate that the process and heuristics provided by the ReCAPS method are useful for specifying database-level and application-level ACPs. Additionally, ReCAPS integrates policy specification into software development, thus providing a basic framework for ensuring compliance between different levels of policies, system requirements and software design. The method also improves the quality of requirements specifications and system designs by clarifying ambiguities and resolving conflicts across these artifacts.	access control list;ana (programming language);bus functional model;business continuity planning;computer-aided software engineering;confidentiality;digital light processing;discretionary access control;enterprise privacy authorization language;ftc fair information practice;functional requirement;gqm;heuristic (computer science);high- and low-level;hoc (programming language);low-energy adaptive clustering hierarchy;operational system;privacy policy;programming tool;requirements analysis;requirements engineering;software design;software development;software requirements specification;software system;specification language;system requirements	Qingfeng He;Annie I. Antón	2009	Information & Software Technology	10.1016/j.infsof.2008.11.005	reliability engineering;requirements analysis;software requirements specification;computer science;systems engineering;engineering;requirement;software engineering;system requirements specification;functional specification;database;requirements engineering;computer security	SE	-52.132823114192256	51.586092048991915	29078
450213da33bd3b67b6ad6d1f7a49b72cd798f9ea	a 5-step hunt for faults in java implementations of algebraic specifications	analytical models;algebraic specification;metals;java metals analytical models observers data models conferences software systems;software systems;software fault tolerance;object oriented programming;observers;program testing;software system axiom junit test suite algebraic specification data type java object oriented faulty component fault location failed test case;software fault tolerance algebraic specification java object oriented programming program testing;conferences;data models;java	Executing thorough test suites allows programmers to strengthen the confidence on their software systems. However, given some failed test cases, finding the faults' locations can be a difficult task, thereby techniques that make it easier for the programmer to locate the faulty components are much desirable. In this paper we focus on finding faults in object-oriented, more precisely Java, implementations of data types that are described by algebraic specifications. We capitalize on models for the specification under study and JUnit test suites that cover all axioms of the specification, and present a collection of techniques and underlying methodology, that give the programmer, in a transparent way, a means to locate the fault that causes the implementation to violate the specification. We also present a comparative experiment that was carried out to evaluate this approach where very encouraging results were obtained.	alloy analyzer;failure;junit;java;programmer;software system;test case;test suite	Isabel Nunes;Filipe Luís	2013	2013 IEEE Sixth International Conference on Software Testing, Verification and Validation Workshops	10.1109/ICSTW.2013.28	data modeling;real-time computing;computer science;theoretical computer science;operating system;software engineering;java modeling language;real time java;programming language;object-oriented programming;java;software fault tolerance;scala;java annotation;software system	SE	-58.27796414600652	35.28369852469324	29154
99d735c3e265e5ffc9e56e9e795f17910c759a54	an empirical study on configuration-related issues: investigating undeclared and unused identifiers		The variability of configurable systems may lead to configuration-related issues (i.e., faults and warnings) that appear only when we select certain configuration options. Previous studies found that issues related to configurability are harder to detect than issues that appear in all configurations, because variability increases the complexity. However, little effort has been put into understanding configuration-related faults (e.g., undeclared functions and variables) and warnings (e.g., unused functions and variables). To better understand the peculiarities of configuration-related undeclared/unused variables and functions, in this paper we perform an empirical study of 15 systems to answer research questions related to how developers introduce these issues, the number of configuration options involved, and the time that these issues remain in source files. To make the analysis of several projects feasible, we propose a strategy that minimizes the initial setup problems of variability-aware tools. We detect and confirm 2 undeclared variables, 14 undeclared functions, 16 unused variables, and 7 unused functions related to configurability. We submit 30 patches to fix issues not fixed by developers. Our findings support the effectiveness of sampling (i.e., analysis of only a subset of valid configurations) because most issues involve two or less configuration options. Nevertheless, by analyzing the version history of the projects, we observe that a number of issues remain in the code for several years. Furthermore, the corpus of undeclared/unused variables and functions gathered is a valuable source to study these issues, compare sampling algorithms, and test and improve variability-aware tools.	algorithm;heart rate variability;identifier;sampling (signal processing);spatial variability	Flávio Medeiros;Iran Rodrigues;Márcio Ribeiro;Leopoldo Teixeira;Rohit Gheyi	2015		10.1145/2814204.2814206	simulation;computer science;data mining	SE	-62.83913111950655	36.49368163251357	29202
07df64d4f89294eaad17da419c234ee728bd08a8	retrofitting legacy code for authorization policy enforcement	authorization policy enforcement;electrical capacitance tomography;manuals;security sensitive locations;program diagnostics;policy enforcement;software fault diagnosis;software maintenance;authorisation;legacy code retrofitting;resource management;x clients legacy code retrofitting authorization policy enforcement secure systems program analysis security sensitive locations x11 server;software maintenance authorisation program diagnostics;authorization fingerprint recognition security manuals resource management linux performance analysis electrical capacitance tomography monitoring marine vehicles;marine vehicles;monitoring;fingerprint recognition;secure systems;performance analysis;secure system;linux;authorization;x11 server;program analysis;security;web proxy;x clients	Researchers have argued that the best way to construct a secure system is to proactively integrate security into the design of the system. However, this tenet is rarely followed because of economic and practical considerations. Instead, security mechanisms are added as the need arises, by retrofitting legacy code. Existing techniques to do so are manual and ad hoc, and often result in security holes. We present program analysis techniques to assist the process of retrofitting legacy code for authorization policy enforcement. These techniques can be used to retrofit legacy servers, such as X window, Web, proxy, and cache servers. Because such servers manage multiple clients simultaneously, and offer shared resources to clients, they must have the ability to enforce authorization policies. A developer can use our techniques to identify security-sensitive locations in legacy servers, and place reference monitor calls to mediate these locations. We demonstrate our techniques by retrofitting the X11 server to enforce authorization policies on its X clients	algorithm;analysis of algorithms;application programming interface;authorization;computer security;executable;fingerprint;hoc (programming language);legacy code;program analysis;prototype;proxy server;reference monitor;server (computing);vulnerability (computing);world wide web;x window system	Vinod Ganapathy;Trent Jaeger;Somesh Jha	2006	2006 IEEE Symposium on Security and Privacy (S&P'06)	10.1109/SP.2006.34	computer science;information security;resource management;operating system;database;authorization;world wide web;computer security	Security	-53.7213041149596	57.60829386297886	29252
ba4daa79696bc83452faed94fd64d8429956774b	connected car: technologies, issues, future trends	intelligent vehicular networks;on car infotainment;car mobile integration;autonomous vehicles	The connected car—a vehicle capable of accessing to the Internet, of communicating with smart devices as well as other cars and road infrastructures, and of collecting real-time data from multiple sources—is likely to play a fundamental role in the foreseeable Internet Of Things. In a context ruled by very strong competitive forces, a significant amount of car manufacturers and software and hardware developers have already embraced the challenge of providing innovative solutions for new-generation vehicles. Today’s cars are asked to relieve drivers from the most stressful operations needed for driving, providing them with interesting and updated entertainment functions. In the meantime, they have to comply with the increasingly stringent standards about safety and reliability. The aim of this article is to provide an overview of the possibilities offered by connected functionalities on cars and the associated technological issues and problems, as well as to enumerate the currently available hardware and software solutions and their main features.	connected car;enumerated type;internet of things;real-time data;real-time locating system;smart device	Riccardo Coppola;Maurizio Morisio	2016	ACM Comput. Surv.	10.1145/2971482	simulation;computer security	Embedded	-46.681497731480206	49.40888419265791	29260
d41fa4d41922b8beced070176dc327fd2b208222	service-oriented context-aware application design		Context-aware systems are applications that adapt to several situations involving user, network, data, hardware and the application itself. Researchers in context-awareness have concentrated on how to capture context data and to carry it to the application. In this paper, we study the impact of context on the core of the application, give a new context definition useful for application design, and propose a context-aware architecture providing a functional adaptation to the context.	context awareness;context-aware pervasive systems;experiment;prototype;service-oriented device architecture;web service	Tarak Chaari;Frédérique Laforest;Augusto Celentano	2005			real-time computing;simulation;computer science;data mining;context model	HCI	-41.439010170125236	43.035105703401236	29262
16624a1b99db5311fb82968e5bd8266634766837	one time passwords in everything (opie): experiences with building and using stronger authentication	data transmission security;one time password;computer networks;computer programs;symposia;cryptography;computer access control	The U. S. Naval Research Laboratory's OPIE (Onetime Passwords In Everything) Software Distribution is an enhancement of Bellcore's S/Key 1.0 package. OPIE improves on S/Key in several areas, including FTP service with one-time passwords, and a stronger algorithm for generating one-time passwords. OPIE diverges from S/Key in select design decisions and in the behavior of certain programs. While not a total security solution, OPIE can be an important part of one. OPIE and its evolutionary predecessors have been used for over a year in parts of NRL. Its use has taught the authors lessons on implementation, usability, deployment, and future directions for improvement.	algorithm;opie authentication system;password;software deployment;usability	Daniel L. McDonald;Randall J. Atkinson;Craig Metz	1995			computer access control;computer science;cryptography;theoretical computer science;operating system;world wide web;computer security	HCI	-53.5518923675758	59.30919065819509	29309
d4a1e5228099198f8d69057a873cf5acce8b3ff0	the design and implementation of architectural components for the integration of the ip multimedia subsystem and wireless sensor networks	application development;wireless sensor networks network servers standards publication information management access protocols concrete prototypes information security publishing subscriptions;control application;context aware application;robot sensing systems;value added services;wireless sensor networks internetworking ip networks multimedia communication ubiquitous computing;context information;standards;wsn ims integration;information security;building block;ip based multimedia service;prototypes;publishing;contextual information;actuators;personalized call control architectural component ip multimedia subsystem wireless sensor network ip based multimedia service context aware application presence based architecture wsn ims integration wsn ims gateway presence server context information management pervasive game;standards publication;wireless sensor network;emergency management;actuators prototypes logic gates monitoring robot sensing systems multimedia communication;presence server;wireless communication;network servers;personalized call control;logic gates;monitoring;design and implementation;api ip multimedia subsystem wireless actuator network architectural component small scale device multimedia service value added service environment monitoring emergency management home automation higher level actuation control entity ims gateway integrated architecture;information management;application program interfaces;pervasive game;architectural component;multimedia communication;wsn ims gateway;access protocols;internetworking;subscriptions;ubiquitous computing;ip networks;context information management;wireless sensor networks actuators application program interfaces ip networks multimedia communication;ip multimedia subsystem;logic gate;multimedia services;wireless sensor networks;core network;presence based architecture;concrete;home automation	The IP multimedia subsystem is becoming the de facto standard for IP-based multimedia services, while wireless sensor networks are gaining in popularity due to their ability to capture a rich set of contextual information. Integrating the sensing capabilities of WSNs in the IMS can open the door to a wide range of context-aware applications in areas such as wireless healthcare and pervasive gaming. We have previously proposed a presence-based architecture for WSN/IMS integration. This architecture relies on two key components: a WSN/IMS gateway acting as an interworking unit between WSNs and the IMS; and an extended presence server serving as a context information management node in the core network. In this article we focus on the design and implementation of these two components. Furthermore, two applications (a pervasive game and a personalized call control application) are used to concretely show how new applications can be developed using our architecture. Performance has also been evaluated. Several important findings were made in the course of this work; one is that the IMS integration with a large and evolving variety of WSNs may be a never-ending endeavor - the gateway requiring constant upgrading due to the lack of standard APIs for the interaction with sensors produced by different vendors. Another finding is that while the introduction of context as an application building block in the IMS ensures the availability of additional contextual information in the network and enables fast and easy development of context-aware applications, the lack of mature IMS application development toolkits remains a roadblock.	ip multimedia subsystem;information management system (ims);list of toolkits;personalization;pervasive informatics;sensor;server (computing)	May El Barachi;Arif Kadiwal;Roch H. Glitho;Ferhat Khendek;Rachida Dssouli	2010	IEEE Communications Magazine	10.1109/MCOM.2010.5439075	embedded system;real-time computing;wireless sensor network;logic gate;computer science;information security;operating system;information management;ubiquitous computing;computer network	Mobile	-38.51764698932988	46.94979278215015	29372
72fa0bd7f7583d84545a689ea590acbe406f9361	ginpex: deriving performance-relevant infrastructure properties through goal-oriented experiments	goal orientation;software performance engineering;metamodel;operating system;performance model;experiments;performance prediction;measurements;virtual environment;infrastructure	In software performance engineering, the infrastructure on which an application is running plays a crucial role when predicting the performance of the application. Thus, to yield accurate prediction results, performance-relevant properties and behaviour of the infrastructure have to be integrated into performance models. However, capturing these properties is a cumbersome and error-prone task, as it requires carefully engineered measurements and experiments. Existing approaches for creating infrastructure performance models require manual coding of these experiments, or ignore the detailed properties in the models. The contribution of this paper is the Ginpex approach, which introduces goal-oriented and model-based specification and generation of executable performance experiments for detecting and quantifying performance relevant infrastructure properties. Ginpex provides a metamodel for experiment specification and comes with pre-defined experiment templates that provide automated experiment execution on the target platform and also automate the evaluation of the experiment results. We evaluate Ginpex using two case studies, where experiments are executed to detect the operating system scheduler timeslice length, and to quantify the CPU virtualization overhead for an application executed in a virtualized environment.	central processing unit;cognitive dimensions of notations;computer performance;executable;experiment;metamodeling;model-based specification;operating system;overhead (computing);performance engineering;scheduling (computing);sensor;software performance testing	Michael Hauck;Michael Kuperberg;Nikolaus Huber;Ralf H. Reussner	2011		10.1145/2000259.2000269	metamodeling;real-time computing;simulation;computer science;systems engineering;engineering;virtual machine;operating system;goal orientation;measurement	SE	-43.70628930778263	34.44744678625465	29388
b675855b39603ee1f14f02fe5dff63d23fe76d59	using static analysis to detect type errors and concurrency defects in erlang programs	dynamic typing;race condition;dataflow analysis;type checking;design and implementation;lessons learned;it adoption;datavetenskap datalogi;computer science;static analysis tools;static analysis;type inference;defect detection;open source	This invited talk will present the key ideas in the design and implementation of Dialyzer, a static analysis tool for Erlang programs. Dialyzer started as a defect detection tool using a rather ad hoc dataflow analysis to detect type errors in Erlang programs, but relatively early in its development it adopted a more disciplined approach to detecting definite type clashes in dynamically typed languages. Namely, an approach based on using a constraint-based analysis to infer success typings which are also enhanced with optional contracts supplied by the programmer.#R##N##R##N#In the first part of the talk, we will describe this constraint-based approach to type inference and explain how it differs with past and recent attempts to type check programs written in dynamic languages. In the second part of the talk, we will present important recent additions to Dialyzer, namely analyses that detect concurrency defects (such as race conditions) in Erlang programs. For a number of years now, Dialyzer has been part of the Erlang/OTP system and has been actively used by its community. Based on this experience, we will also critically examine Dialyzer's design choices, show interesting cases of Dialyzer's use, and distill the main lessons learned from using static analysis in open source as well as commercial code bases of significant size.	erlang (programming language);static program analysis	Konstantinos Sagonas	2010		10.1007/978-3-642-12251-4_2	parallel computing;real-time computing;type system;computer science;type inference;distributed computing;race condition;programming language;static analysis;static program analysis	PL	-55.329170552781356	39.65130285155159	29392
6721b9554ffa46deb957769c71a06a9fef17e7cb	reputation and attribute based dynamic access control framework in cloud computing environment for privacy protection	law;computational modeling;access control;privacy;cloud computing	A Reputation and Attribute Based Dynamic Access Control (RA-DAC) Framework for Privacy Protection in Cloud Computing Environment was presented in this paper. First, RA-DAC is used to encourage honest users for their good actions in cloud environment. Second, RA-DAC is also used to constraint malicious users for their destructive actions in cloud environment. And finally, based on RA-DAC we developed the Reputation and Attribute Based Dynamic Access Control System (RA-DACS) which is used to detect the malicious behaviors of dishonest users and automatically prevent their further action to threaten the security of the cloud computing environment.	access control;cloud computing;control system;privacy;traffic collision avoidance system	Donghong Sun;Wu Liu;Ping Ren;Ke Liu	2016	2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)	10.1109/FSKD.2016.7603356	cloud computing security;cloud computing;computer science;access control;internet privacy;privacy;computational model;world wide web;computer security	DB	-48.71567980243206	53.846355065460365	29458
a18ab0e11fcb3fcecbda8f6418f821845e4babe4	a state-transfer-based dynamic policy approach for constraints in rbac	modelizacion;controle acces;gestion informacion;red www;maquina estado finito;securite informatique;reseau web;computer security;modelisation;seguridad informatica;information management;world wide web;access control;gestion information;machine etat fini;modeling;finite state machine	RBAC is widely used in access control field, and this paper proposes an approach to implement dynamic policy transfer on this model. Our approach monitors state-transfers of subjects and transfers policies correspondingly. It holds a finite number of states and a policy transfer set containing the predefined policies. When a state-transfer occurs, an appropriate policy chosen from the policy transfer set will be applied to change the user-role mapping or the role-permission mapping from one to another. This policy transfer not only focuses on the current state, but also takes the previous state into consideration since changing from different state will lead to a different current policy.	role-based access control	Cheng Zang;Zhongdong Huang;Gang Chen;Jinxiang Dong	2005		10.1007/11563952_76	simulation;systems modeling;computer science;access control;information management;computer security	Vision	-36.43700000518943	60.436150242754366	29559
6bb0e0549e9eb8a9b9bb104cb5d58b7e452d4e52	entropy-based detection of incipient faults in software systems	incipient faults;software;software fault tolerance principal component analysis;measurement;software fault tolerance;memory bloat entropy based detection incipient faults software systems software performance system failure principal component analysis pca multivariate data;entropy measurement principal component analysis fault detection servers correlation software;servers;principal component analysis;fault detection;principal component analysis software faults incipient faults entropy;entropy;correlation;software faults	This paper develops and validates a methodology to detect small, incipient faults in software systems. Incipient faults such as memory leaks slowly deteriorate the software's performance over time and if left undetected, the end result is usually a complete system failure. The proposed method combines tools from information theory and statistics: entropy and principal component analysis (PCA). The entropy calculation summarizes the information content associated with the collected low-level metrics and reduces the computational burden incurred by the subsequent PCA step which detects underlying patterns and correlations present in the multivariate data, as well as distortions in the correlations indicative of an incipient fault. We use the technique to detect memory bloat within the Trade6 enterprise application under dynamic workload patterns, showing that small leaks can be detected quickly and with a low false alarm rate. Our method is also robust to the periodic/seasonal patterns affecting the metrics used to detect the fault.	computation;distortion;enterprise software;entropy (information theory);high- and low-level;information theory;memory leak;multistage interconnection networks;overhead (computing);principal component analysis;self-information;software system	Salvador DeCelles;Nagarajan Kandasamy	2012	2012 IEEE 18th Pacific Rim International Symposium on Dependable Computing	10.1109/PRDC.2012.14	reliability engineering;entropy;real-time computing;computer science;engineering;operating system;machine learning;correlation;fault detection and isolation;software fault tolerance;server;measurement;principal component analysis	Arch	-60.722332488387764	42.282506747246764	29565
51980317e5b42a4b015f592264b83ab2a535d01e	towards a dynamic negotiation mechanism for qos-aware service markets		The market value of commercial Service-Based Applications (SBAs) will depend not only on their functionality, but also on the value of QoS parameters referring to its not functional properties. These parameters are not static properties since they may vary according to the provision strategies of providers as well as the demand of users having their own preferences on the application’s QoS values. In this paper we propose a negotiation-based mechanism among service providers and a user requesting a QoS-aware SBA to select services with suitable QoS values, i.e. values that once aggregated satisfy the user’s requirements. The proposed mechanism simulates a market-based provision mechanism that allows to take into account the variability of service QoS attribute values typical of the future market of services, as well as to dynamically set the length of the negotiation process that is usually very time consuming.	quality of service	Claudia Di Napoli;Paolo Pisa;Silvia Rossi	2013		10.1007/978-3-319-00563-8_2	microeconomics;business;commerce	ECom	-47.81543364253489	43.017511061335505	29583
de9d6d68cc3da3e2ef63ccefc621ef38b0f80354	"""corrigendum for """"constraint-based automatic test data generation"""". (r. a. de millo and a. j. offutt, ieee trans. software eng. vol. 17. no. pp 900-910, sept. 1991.)"""	automatic testing;control flow testing constraint based test data generation fortran 77 weak mutation data flow analysis;automatic testing genetic mutations system testing data analysis error correction instruments history arithmetic monitoring control systems;program testing program debugging;fortran 77;program testing;control flow;data flow analysis;constraint based test data generation;fortran;program debugging;control flow testing;data flow;weak mutation	In reference to the above-titled paper by R.A. DeMillo and A.J. Offutt (see ibid., vol.17, no.9, p.900-10, Sept. 1991), the commenter rates that he and M.R. Woodward (1985) implemented a system for FORTRAN-77 programs that integrates weak mutation and data flow analysis. He reports here that experiments have been carried out by them (1986), using the system to compare the error exposing ability of weak mutation, data flow, and control flow testing strategies. >	source-to-source compiler;test data generation	Moheb R. Girgis	1993	IEEE Trans. Software Eng.	10.1109/32.232028	non-regression testing;data flow diagram;computer science;theoretical computer science;data-flow analysis;programming language;control flow;algorithm	SE	-58.29056879530849	35.76598716796201	29604
0a64523900c2b1a96a2e66e02c3d2c57ea9e9721	middleware for automatic dynamic reconfiguration of context-driven services	context awareness;wireless networks;context aware;dynamic reconfiguration;web and internet services;discussion forums;middleware context aware services web and internet services context awareness wireless networks logic devices guidelines mobile computing discussion forums ip networks;limit set;guidelines;user requirements;content adaptation;middleware;ip networks;mobile computing;use case;service provision;logic devices;context aware services	In the emerging ubiquitous Internet scenario users require to access services and contents from anywhere, at anytime and with any device. Due to this requirement, platforms for service and content provisioning have to address several problems related to the new issues of mobility, multimodality, context awareness and content adaptation. However, current ubiquitous service provisioning platforms still suffer a main drawback: they often underestimate platform logic complexity when multiple ubiquity features should be provided. As a result, the proposed solutions lack a unified approach to the ubiquity issues and provide only limited sets of features. We claim that an integrated and comprehensive solution can stem from a simplicity principle: our approach pushes ubiquity features outside the core middleware layer, by keeping only management and coordination responsibilities. That succeeds in making the middleware design clearer and neater. In this paper we present the key architectural aspects of our middleware platform for ubiquitous dynamic context-driven service provisioning and reconfiguration. We also provide implementation details and description of typical use cases our platform successfully realizes.	anytime algorithm;content adaptation;context awareness;coupling (computer programming);internet;load balancing (computing);middleware;mobile agent;provisioning;software testing controversies	Maurelio Boari;Enrico Lodolo;Stephano Monti;Samuele Pasini	2006	11th IEEE Symposium on Computers and Communications (ISCC'06)	10.1109/ISCC.2006.99	use case;limit set;computer science;user requirements document;operating system;wireless network;middleware;distributed computing;mobile computing;world wide web;computer security;computer network	Metrics	-40.359095342113996	45.997158026367174	29624
07d32033fb7e6b58ed0dc1596d8fca8ae77fef16	using event-streams for fault-management in mas	domain specific dependability requirements event streams multiagent system event based fault management system;multi agent system;multiagent systems fault detection runtime computer science computer bugs computer languages programming profession performance analysis knowledge management fault diagnosis;multi agent systems;critical state;fault diagnosis multi agent systems;it evaluation;fault management;domain specificity;fault diagnosis	Dependability is a key issue in the deployment of every multi-agent system (MAS). Only if its services are perceived as dependable (e.g. available, reliable, secure and safe) will the MAS be considered useful to ensure the domain specific dependability requirements, it is essential to enable the MAS to detect and react to critical states of agents. This can be done by either enabling the agent to deal with unexpected situations or by adding a fault-management component to the platform. This work presents an event-based fault-management system and presents the results of its evaluation.	apache axis;autonomous agents and multi-agent systems;case-based reasoning;dependability;intelligent agent;international standard book number;java;multi-agent system;p (complexity);proceedings of the ieee;prototype;requirement;software deployment;testbed;transaction processing;web service	Peng Xu;Ralph Deters	2004	Proceedings. IEEE/WIC/ACM International Conference on Intelligent Agent Technology, 2004. (IAT 2004).	10.1109/IAT.2004.1342989	reliability engineering;real-time computing;engineering;dependability;distributed computing	Robotics	-43.203611632163025	40.71404511896096	29678
8c9a8e9aa663d57b3426346131f0e87639fb0c0c	making dns servers resistant to cyber attacks: an empirical study on formal methods and performance		IRONSIDES is an open-source Domain Name System (DNS) server designed using formal methods to reduce DNS vulnerabilities to cyber attacks. The use of formal methods gives IRONSIDES provable security properties, including the absence of numerous security flaws that plague BIND and Windows DNS. It also raises an empirical question: Does the use of formal methods to generate provably secure code require sacrificing performance? We present the results of an experimental investigation to answer this question. We compared IRONSIDES to BIND, Windows DNS, and numerous other DNS servers on both Windows and Unix. Our results show IRONSIDES performs quite well compared to other DNS servers, both proprietary and open-source, particularly given the resources expended in its development. This suggests that, at least in the DNS domain, increasing security with formal methods to render them less vulnerable to cyber attacks does not require sacrificing performance.	capacitor plague;formal methods;microsoft windows;open-source software;provable security;server (computing);unix	Barry S. Fagin;Bradley Klanderman;Martin C. Carlisle	2017	2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC)	10.1109/COMPSAC.2017.165	empirical research;formal methods;dns hijacking;computer science;computer security;provable security;unix;server;domain name system	Security	-58.764079373765966	56.59321538987017	29689
77462b767a378aa6207cbff5b100379fe8a55f6b	adsplit: separating smartphone advertising from applications	embedded html widget;smartphone application;additional permission request;application author;ad service;smartphone advertising;separate process;malicious application;third-party advertising service;separate user-ids;advertising library	A wide variety of smartphone applications today rely on third-party advertising services, which provide libraries that are linked into the hosting application. This situation is undesirable for both the application author and the advertiser. Advertising libraries require their own permissions, resulting in additional permission requests to users. Likewise, a malicious application could simulate the behavior of the advertising library, forging the user’s interaction and stealing money from the advertiser. This paper describes AdSplit, where we extended Android to allow an application and its advertising to run as separate processes, under separate user-ids, eliminating the need for applications to request permissions on behalf of their advertising libraries, and providing services to validate the legitimacy of clicks, locally and remotely. AdSplit automatically recompiles apps to extract their ad services, and we measure minimal runtime overhead. AdSplit also supports a system resource that allows advertisements to display their content in an embedded HTML widget, without requiring any native code.	android;embedded system;file system permissions;html;library (computing);machine code;mobile app;online advertising;overhead (computing);simulation;smartphone	Shashi Shekhar;Michael Dietz;Dan S. Wallach	2012	;login:		online advertising;operating system;internet privacy;world wide web;contextual advertising	Security	-55.066843230222226	60.23354550827615	29694
997dedbd1703b11f7d33183d4185d5fcc0438339	using existing hardware services for malware detection	software;security in hardware;input validation hardware based metrics malware activity detection defense in depth security model hardware aided proof carrying code;data security malware security in hardware;monitoring;malware;system on chip;malware hardware software ip networks monitoring system on chip;invasive software;ip networks;hardware;data security	The paper is divided into two sections. First, we describe our experiments in using hardware-based metrics such as those collected by the BPU and MMU for detection of malware activity at runtime. Second, we sketch a defense-in-depth security model that combines such detection with hardware-aided proof-carrying code and input validation.	data validation;experiment;malware;memory management unit;proof-carrying code;run time (program lifecycle phase)	Sarat Kompalli	2014	2014 IEEE Security and Privacy Workshops	10.1109/SPW.2014.49	software security assurance;system on a chip;hardware compatibility list;computer security compromised by hardware failure;computer science;operating system;hardware architecture;application security;data security;malware;computer security;computer network	Security	-56.02761980449439	56.3434145483958	29756
8a9c8a7d90bb0b05408fabda53266e4888cda52d	the role of models@run.time in autonomic systems: keynote		Autonomic systems manage their own behaviour in accordance with high-level goals. This paper presents a brief outline of challenges related to Autonomic Computing due to uncertainty in the operational environments, and the role that models@run.time play in meeting them. We argue that the existing progress in Autonomic Computing can be further exploited with the support of runtime models. We briefly discuss our ideas related to the need to understand the extent to which the high-level goals of the autonomic system are being satisfied to support decision-making based on runtime evidence and, the need to support self-explanation.	autonomic computing;high- and low-level	Nelly Bencomo	2017	2017 IEEE International Conference on Autonomic Computing (ICAC)	10.1109/ICAC.2017.55	autonomic computing;real-time computing;computer science	Robotics	-43.09565315268653	38.846348555547834	29791
ed1f53809f3404b2b4397dc04160e997c184e83a	a context-aware security framework for next generation mobile networks	communication system;context aware;community networks;security requirements;mobile communication;next generation;security policy;data acquisition;mobile network	The openness and heterogeneity of next generation communication networks are now highlighting more security issues than those of traditional communication environments. Moreover users’ security requirements can often change in mobile communication environments, depending on the situation in which the user is immersed. Our objective is to define a context-aware security framework for addressing the problems of end-to-end security on behalf of endusers. Based on context data acquisition and aggregation features, the framework uses contextual graphs to define security policies encompassing actions at different layers of communication systems’ architecture, while adapting to changing circumstances.	computer security;data acquisition;end-to-end encryption;high- and low-level;next-generation network;openness;organizing (structure);prototype;requirement;telecommunications network;user requirements document	Matteo Bandinelli;Federica Paganelli;Gianluca Vannuccini;Dino Giuli	2009		10.1007/978-3-642-04434-2_12	software security assurance;computer security model;cloud computing security;cellular network;community of interest;security through obscurity;mobile telephony;security information and event management;security association;security engineering;security convergence;covert channel;telecommunications;asset;computer science;security policy;logical security;operating system;human-computer interaction in information security;database;security service;data acquisition;network access control;network security policy;world wide web;computer security;communications system;computer network	Mobile	-44.330323397032934	54.53383804646196	29792
05bd2c6e82a96e7bb3d7d7262f953fc53ead3d1a	accountable virtual machines	vmware workstation;unmodified binary image;existing cheat;computer system;strong accountability;non-repudiable information;binary software image;ordinary virtual machine;accountable virtual machine;different host	In this paper, we introduce accountable virtual machines (AVMs). Like ordinary virtual machines, AVMs can execute binary software images in a virtualized copy of a computer system; in addition, they can record nonrepudiable information that allows auditors to subsequently check whether the software behaved as intended. AVMs provide strong accountability, which is important, for instance, in distributed systems where different hosts and organizations do not necessarily trust each other, or where software is hosted on third-party operated platforms. AVMs can provide accountability for unmodified binary images and do not require trusted hardware. To demonstrate that AVMs are practical, we have designed and implemented a prototype AVM monitor based on VMwareWorkstation, and used it to detect several existing cheats in Counterstrike, a popular online multi-player game. Disciplines Computer Sciences Comments Haeberlen, A., Aditya, P., Rodrigues, R., & Druschel, P., Accountable Virtual Machines, 9th USENIX Symposium on Operating Systems Design and Implementation (OSDI'10), Oct. 2010, http://static.usenix.org/ events/osdi10/tech/full_papers/Haeberlen.pdf This conference paper is available at ScholarlyCommons: http://repository.upenn.edu/cis_papers/613 Accountable Virtual Machines Andreas Haeberlen Paarijaat Aditya Rodrigo Rodrigues Peter Druschel University of Pennsylvania Max Planck Institute for Software Systems (MPI-SWS)	astronomy visualization metadata;binary image;computation;distributed computing;hardware restriction;prototype;sinewave synthesis;software system;vacuum cleaner;virtual machine	Andreas Haeberlen;Paarijaat Aditya;Rodrigo Rodrigues;Peter Druschel	2010			computer science;operating system;database;world wide web;computer security	OS	-52.263169416030955	59.45907010508411	29793
49987873eb5a17664fa95877dc58932133dc94cf	ubiquitous networked robots		With the growing emergence of ambient intelligence, ubiquitous computing, sensor networks, and wireless networking technologies, “ubiquitous networked robotics” is becoming an active research domain of intelligent autonomous systems. It targets new innovative applications in which robotic systems will be integrated into ubiquitous computing environments as physical autonomous entities. These entities are able to interact autonomously with the ambient environment and provide added value services. So far, robots as cognitive entities will be able to coordinate their activities with other physical or logical entities, move around, sense and explore the ambient environment, and decide and act to respond to the situations they may face. These cognitive operations will become part of these networks of artefacts, to provide, individually or collectively, novel capabilities and various assistive services anywhere and anytime. This paradigm aims to build a bridge between ubiquitous computing and robotics, i.e., the creation of flexible, Internet-based, extensible architectures able to support any sort of intelligent and autonomous robotic services capable of interacting—in a typical “Internet of people, things and services” mode— with virtual or real artefacts. Theparadigmofubiquitousnetworkedrobots(UNR)raises a number of important research challenges such as: (a) realtime communication using heterogeneous wired and wireless networking technologies, in which relevant requirements concern the quality and continuity of communication services; (b) interoperability between the different pieces of hardware and software technologies used to guarantee a seamless interaction between the UNR and the surrounding devices and systems; (c) new paradigms of human–robot–environment interaction, including implicit communication mechanisms and artificial perception and reasoning; and (d) adaptability to the ubiquitous environment and scalability management. The UNR systems require more flexibility, mobility, security, reliability, and robustness through middleware mechanisms such as autonomic discovery of entities and services, service compo	robot	Abdelhamid Mellouk;Norihiro Hagita;Dezhen Song	2012	Annales des Télécommunications	10.1007/s12243-012-0314-y	real-time computing;simulation;ubiquitous commerce;engineering;distributed computing;ubiquitous robot;computer network	Robotics	-41.80007243907026	45.978147460177176	29808
d8b49252dd5ded65fe9edcc8164ac98482858e77	classification and positioning of cloud definitions and use case scenarios for portability and interoperability	nist;cloud definition classification;cloud providers;use cases;cloud providers cloud definition classification cloud definition positioning use case scenarios portability interoperability cloud computing;portability;computer architecture;interoperability nist platform as a service computer architecture software as a service;platform as a service;cloud definition positioning;software as a service;interoperability;open systems cloud computing electronic data interchange;open systems;use cases cloud computing interoperability portability;electronic data interchange;cloud computing;use case scenarios	"""Cloud Computing has rapidly evolved and spread in the past few years, with always new services and functionalities being offered by providers in order to gain larger market sectors. This has caused in many cases a lot of distress and confusion to customers who have been often subjected to the """"vendor lock-in"""" phenomenon, because of interoperability and portability issues often arising among different Cloud providers. In this paper we provide a brief introduction to the basic definitions of Cloud Computing, portability and interoperability and we also describe a set of established use cases. All these notions are mapped to a multi-dimensional space, which is used to classify both definitions and use cases. The focus here is represented by portability and interoperability features."""	cloud computing;distress (novel);interoperability;software portability;vendor lock-in	Beniamino Di Martino;Giuseppina Cretella;Antonio Esposito	2015	2015 3rd International Conference on Future Internet of Things and Cloud	10.1109/FiCloud.2015.119	use case;semantic interoperability;computer science;operating system;database;cross-domain interoperability;world wide web;computer security	Metrics	-42.725139277481595	54.718710399731314	29840
8a7c7f63556431d6c60f6a74c13bea74d4c96d7c	functional testing vs. structural testing of rams	functional testing;structural testing	The object of this paper is to synthetize results that have been published for RAM testing since 1977. These results are concerned with functional and semi-functional testing. It is noted that although the used vocabulary is different, fault hypotheses are very similar. These types of algorithms reached the availability of quasi-optimal algorithms. Structural testing is then introduced, the goal being to relate functional fault hypotheses and structural fault hypotheses, and to investigate the interest of structural type test algorithms compared to functional type test algorithms.	functional testing;white-box testing	H. Sahami;Bernard Courtois	1984		10.1007/978-3-642-69698-5_32	machine learning;functional testing;vocabulary;computer science;artificial intelligence	SE	-59.765515351280385	33.92839004195285	29899
436b5c6a115ad24bd073a1818dd1e5eb97fc08fa	error report driven post-mortem analysis	regulatory agencies;error traces;program comprehension;biomedical imaging;natural languages;satisfiability;slicing criteria;mechanical factors;failure analysis;shape;error handling;medical device;software reliability error handling program slicing;failure analysis biomedical imaging computer errors computer science software debugging natural languages software tools natural language processing shape mechanical factors;software tools;software debugging;abstract interpretation regulatory agencies program slicing error reports slicing criteria execution sequences error traces;computer science;execution sequences;program slicing;abstract interpretation;software reliability;error reports;natural language processing;computer errors	"""Regulatory agencies, such as US FDA, and other third party reviewers of software have the task of comprehending apiece of software in the event of its failure. Program slicing is the preferred technique to analyze software failures. However, program slicing might yield extremely large slices and also demand that an user have intimate knowledge of the software (by specifying a slicing criterion). In this paper we propose solutions to ameliorate both of these problems. The main hypothesis of the paper is that error reports can be used to generate slicing criteria, thus allowing a third-party reviewer to use slicing without any knowledge of the system being analyzed. The first contribution of the paper is a study of how error reports can be formalized and how execution sequences (called error traces) that satisfy an error report can be identified, which can then be sliced. A primary feature of this work is that incomplete error reports could be dealt with. The second contribution of the paper is a scheme for using abstract interpretation in the generation of error traces and (potential) slicing criteria from error reports. These """"abstract"""" error traces can be sliced, much like in dynamic/conditional slicing, with respect to the criteria generated, allowing for inputs to be used to reduce the size of slices. Furthermore, being abstract in nature, they are shorter than exact execution sequences and also allow for easier comprehension. We finally present a case study involving a medical device that illustrates how our approach can aid with program comprehension."""	abstract interpretation;array slicing;ccir system a;complexity;concatenation;conformance testing;correctness (computer science);crash reporter;misuse case;predicate (mathematical logic);program comprehension;program slicing;sensor;software system;state space;test case;tracing (software);usability	Yi Zhang;S. Purushothaman Iyer	2007	15th IEEE International Conference on Program Comprehension (ICPC '07)	10.1109/ICPC.2007.43	exception handling;medical imaging;failure analysis;program slicing;shape;computer science;theoretical computer science;operating system;software engineering;database;natural language;programming language;software quality;satisfiability	SE	-51.05418139625382	34.42860356070633	29907
bd0a4de23c665be506f60244453a7a157d3d2af9	the design of smart battery management systems	management system;smart battery;smbus	Aiming to the defect of expandability and reliability of Li-Ion battery of protection circuit, the multisection series connection Li-ion battery management system was designed based on X3100 chip. The hardware and software design of the system was elaborated. The system can realize the measure, management and auto-protection for various parameters and complete the calculation for tens of command parameters and realize the data exchange with HOST by SMBus. Applications show that the system is characterized by simple structure, perfect function and stable and reliable operation and can be used in the smart battery model of laptop computer, Electric bicycle and other portable equipments.	battery management system;intel gma;laptop;series and parallel circuits;smart battery;software bug;software design;system management bus	Peide Liu;Xiujuan Zhang	2011	JCP	10.4304/jcp.6.11.2484-2490	embedded system;management system	EDA	-35.07838808104134	39.05584524426417	30002
4f57b062eca6a5ccbd4d142ec10ca732fc8eb0a7	evaluation of a robust middleware for numerous distributed task-handling	robustness middleware ubiquitous computing intelligent sensors distributed computing embedded computing pervasive computing runtime network interfaces space technology;distributed computing;middleware ubiquitous computing;rt dragon robust middleware distributed task handling ubiquitous computing environment distributed computing;ubiquitous computing;middleware;everyday life;embedded device;ubiquitous computing environment	A ubiquitous computing environment entails numerous embedded devices with enough computational power. Such distributed computing resources can cooperate to create applications which support users' everyday lives. Such an environment requires robust middleware that can handle a large quantity of messages, such as event data or sensor data are occurred between devices. However, the lack of middleware technologies with such functionalities has been making it difficult to create ubiquitous computing applications that we can depend on in their everyday life. In this paper, we propose a novel middleware, named RT-Dragon, which can handle numerous messages with robustness. RT-Dragon can work effectively on systems both realtime and non-realtime runtime because of the mechanism of two level priority-based task processing. In this paper additionally, we evaluate the effectiveness of RT-Dragon on non-RT runtime.	best-effort delivery;distributed computing;embedded system;middleware;project 25;real-time clock;real-time computing;real-time locating system;run time (program lifecycle phase);soft computing;ubiquitous computing	Masayuki Iwai;Hideyuki Tokuda	2005	11th IEEE International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA'05)	10.1109/RTCSA.2005.44	embedded system;middleware;real-time computing;context-aware pervasive systems;computer science;operating system;end-user computing;middleware;distributed computing;utility computing;ubiquitous robot;ubiquitous computing;grid computing;autonomic computing	HPC	-38.99875028319502	46.464096897665115	30059
110327b2415d69bda5c9103758671afc7f2ab5e9	idrs: combining file-level intrusion detection with block-level data recovery based on iscsi	national security;protocols;storage system;network storage systems;information security;information science;availability;intrusion detection;intrusion detection protocols protection availability data security information security national security laboratories information science data engineering;data engineering;iscsi;protection;network storage systems idrs file level intrusion detection block level data recovery iscsi;file level intrusion detection;security of data digital storage file organisation;digital storage;security of data;block level data recovery;idrs;file organisation;data security	Over the past years, researches on the intrusion detection have been parallelized with those on data recovery. Most of them scarcely try to combine the two issues together to propose an integrated solution, which is employed to defend the pivotal data and to recover the data when the intrusion has taken place. In this paper, we propose a framework of intrusion detection/recovery system (IDRS). This system is capable of detecting the intrusion on the file-level and recovering data on the block-level. Its advantages include that the file-level detection simplifies the implementation and the recovery based on the block-level can decrease the recovery time and raise the utilization ratio of storage devices. Again, considering that iSCSI has increasingly played an important role in network storage systems, we implement the IDRS prototype based on this promising protocol. The result of tests shows the extra storage overheads, arising from IDRS, are small (less than 10%). Therefore, we believe it is feasible to deploy IRDS on iSCSI systems to protect key files from damage.	data recovery;iscsi;intrusion detection system;object storage;object-based language;parallel computing;prototype;sensor;storage networking industry association	Youhui Zhang;Hongyi Wang;Yu Gu;Dongsheng Wang	2008	2008 Third International Conference on Availability, Reliability and Security	10.1109/ARES.2008.59	computer science;operating system;database;computer security;intrusion prevention system	DB	-49.10422314328138	60.26200233307343	30106
e8bfd020813f866ef7237f1cec8d01b7ca5e1679	a dsl for specifying timing requirements	formal specification;timing requirements;distributed processing;model driven development mdd;embedded systems;requirements metamodel;wheels sensors dsl delay synchronization software;timing analysis timing requirements domain specific language requirements metamodel model driven development mdd;multiclock systems dsl model timing requirement specification real time distributed embedded systems time modelling time analysis abstraction levels domain specific language software development life cycle symbolic timing expression modelling multirate system;specification languages;domain specific language;timing analysis;specification languages distributed processing embedded systems formal specification	The engineering of real-time distributed embedded systems becomes more and more complex today due to the amount of new functionalities, constraints applied on these functions and the diversity of hardware supporting software execution and communication. Modeling and analysis of time is a key issue for the correct development of these systems. From an engineering point of view, there is a need of a development process supporting modeling timing requirements at different abstraction levels. In this paper we present a Domain Specific Language (DSL) for specifying timing requirements at the analysis phase of the software development life-cycle. The DSL provides the following features: the modeling of different types of timing requirements, the modeling of symbolic timing expressions, i.e. able to deal with bounded or unset parameters in timing requirements, and the integration of complex concepts of distributed systems such as multi rate and multi clock systems.	digital subscriber line;distributed computing;domain-specific language;embedded system;environment variable;point of view (computer hardware company);real-time clock;requirement;software development process	Arda Goknil;Marie-Agnès Peraldi-Frati	2012	2012 Second IEEE International Workshop on Model-Driven Requirements Engineering (MoDRE)	10.1109/MoDRE.2012.6360074	software requirements specification;real-time computing;specification language;computer science;systems engineering;requirement;system requirements specification;functional specification;programming language	Embedded	-41.49643202835445	33.674336665012106	30121
6b16629c8c5683f2690ddac3baeec1ab2d5e90a8	a component-based approach for the development of automated systems	verification;formal specification;composition;sensors;component based systems;materials requirements planning synchronization concrete system recovery actuators sensors;formal verification formal specification;actuators;b method;refinement;formal verification;system recovery;material requirement planning;component based system automated systems development system control part system operative part software component formal event b method codesign technique refinement semantics;synchronization;verification automated systems composition event b method refinement;software component;materials requirements planning;automated systems;event b method;concrete	This paper addresses a component-based approach using the Event-B method to develop automated systems. These systems are composed of two parts: the control part (controller) and the operative part (controlled component). The first is a software component which controls the operative part that models the physical device and its environment. We propose in this paper the use of the formal Event-B method to develop automated systems applying a codesign technique, where the two components are developed separately, and then, a composition is defined with the Event-B method to prove the automated system correctness. First of all, we define a specification for the composition of these two components in the Event-B method. Second, we give refinement semantics for a component-based system before proposing a method to verify the refinement of a whole system from that of its components.	b-method;benchmark (computing);component-based software engineering;control system;controller (computing);correctness (computer science);peripheral;programming paradigm;refinement (computing);requirement;sorting;synchronization (computer science);user requirements document;verification and validation;whole earth 'lectronic link	Olfa Mosbahi;Mohamed Khalgui;Hans-Michael Hanisch;Zhiwu Li	2011	IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans	10.1109/TSMCA.2010.2093885	real-time computing;computer science;material requirements planning	Embedded	-39.57209985410008	37.35062215064627	30123
e115beb4b89e4c9f7f5a2f040ab359b3efd81619	an accurate and multi-faceted reputation scheme for cloud computing	cloud computing	With the rapid growth of cloud computing, the importance of a reputation system for cloud computing services has attracted a lot of attention. Building an objective and reliable reputation scheme has been crucial to promote the development of cloud computing. To address the challenges of reputation evaluation in cloud computing, including the diverse nature of cloud services and intricacy of malicious ratings, an Accurate and Multi-faceted Reputation scheme for cloud computing (AMRep) is proposed. As the reputation systems of cloud computing are exposed to new vulnerabilities, AMRep introduces a couple of malicious rating detection approaches to improve the accuracy of reputation calculation. Additionally, we establish a multi-faceted reputation evaluation method, which can help user assess and choose cloud services from different angles. Experiments reveal that our AMRep scheme can effectively defend against malicious ratings, and accurately calculate the reputation values of cloud services. c © 2014 The Authors. Published by Elsevier B.V. Selection and peer-review under responsibility of Elhadi M. Shakshuki.	cloud computing;faceted classification;malware;reputation system;simulation	Miao Wang;Guiling Wang;Jie Tian;Hanwen Zhang;Yujun Zhang	2014		10.1016/j.procs.2014.07.021	cloud computing security;internet privacy;world wide web;computer security	Web+IR	-49.091739200357395	58.084368187907224	30172
4322af5636039c7f05bdee713f17379540171fb3	partition testing, stratified sampling, and cluster analysis	sample size;metrics;proving properties;properties of measures;cluster analysis;software measures;measurement theory;test methods;stratification;foundations of measures;software reliability;stratified sampling	We present a new approach to reducing the manual labor required to estimate software reliability. It combines the ideas of partition testing methods with those of stratified sampling to reduce the sample size necessary to estimate reliability with a given degree of precision. Program executions are stratified by using automatic cluster analysis to group those with similar features. We describe the conditions under which stratification is effective for estimating software reliability, and we present preliminary experimental results suggesting that our approach may work well in practice.	cluster analysis;sampling (signal processing);software quality;software reliability testing;stratified sampling	Andy Podgurski;Charles Yang	1993		10.1145/256428.167076	sample size determination;reliability engineering;sampling;stratification;computer science;cluster sampling;stratified sampling;cluster analysis;test method;lot quality assurance sampling;metrics;software quality	SE	-61.41503922094163	33.712684727380626	30188
ac877513b164b359d948c3878b4c1c9a3de7e91e	low cost efficient deliverying video surveillance service to moving guard for smart home	lte;mvc;smart home;wifi;e-mail;instant messaging;software design pattern;video streaming;video surveillance	Low-cost video surveillance systems are attractive for Smart Home applications (especially in emerging economies). Those systems use the flexibility of the Internet of Things to operate the video camera only when an intrusion is detected. We are the only ones that focus on the design of protocols based on intelligent agents to communicate the video of an intrusion in real time to the guards by wireless or mobile networks. The goal is to communicate, in real time, the video to the guards who can be moving towards the smart home. However, this communication suffers from sporadic disruptions that difficults the control and drastically reduces user satisfaction and operativity of the system. In a novel way, we have designed a generic software architecture based on design patterns that can be adapted to any hardware in a simple way. The implanted hardware is of very low economic cost; the software frameworks are free. In the experimental tests we have shown that it is possible to communicate to the moving guard, intrusion notifications (by e-mail and by instant messaging), and the first video frames in less than 20 s. In addition, we automatically recovered the frames of video lost in the disruptions in a transparent way to the user, we supported vertical handover processes and we could save energy of the smartphone's battery. However, the most important thing was that the high satisfaction of the people who have used the system.	closed-circuit television;computer hardware;dental intrusion;email;frame (physical object);generic drugs;home automation;implants;instant messaging;intelligent agent;internet of things;mental suffering;network packet;protocols documentation;real-time computing;server (computer);server (computing);smartphone;software architecture;software design pattern;software framework;streaming media;video server;notification;sensor (device)	Tatiana Gualotuña;Elsa M. Macías;Álvaro Suárez Sarmiento;C. R. Fonseca EfraínR.Fonseca;Andrés Rivadeneira	2018		10.3390/s18030745	intelligent agent;electronic engineering;guard (information security);software framework;engineering;home automation;real-time computing;vertical handover;cost efficiency;software design pattern;video camera	Mobile	-37.7334166888753	56.40401071129991	30225
48406b06e0ecbce427a269073cb91c193ce3344f	design and implementation of an agent-based web services platform for electronic commerce	portals;electronic commerce;agent based;service orientation;distributed computing;web service;multi agent systems;internet;design and implementation;web services electronic commerce application software telecommunications navigation portals computer science distributed computing computer architecture prototypes;service oriented electronic commerce tele portal system agent based web services distributed computing;portals multi agent systems electronic commerce internet	This paper describes the realization of Tele-portal, an agent-based Web services platform. Tele-portal utilizes two important concepts in the field of distributed computing: the concept of services and agent. We present the whole architecture of the Tele-portal system. The main contribution of this work is that it provides a methodology and prototype implementation of a service-oriented electronic commerce environment. We suggest that this approach has the flexibility necessary to provide agent-oriented designs for open and complex application, and has value for future maintenance and extension of these systems.	agent-based model;distributed computing;e-commerce;prototype;service-orientation;service-oriented device architecture;television;web service	Xiaoqin Huang;Linpeng Huang;Lin Chen;Minglu Li	2004	IEEE International Conference onServices Computing, 2004. (SCC 2004). Proceedings. 2004	10.1109/SCC.2004.1358082	computer science;service-oriented architecture;database;distributed computing;services computing;world wide web	Robotics	-35.75186739337417	47.652643297211505	30268
a273276d5756bb566a82141946129a22b314c927	test case generation from android mobile applications focusing on context events		Nowadays mobile apps are developed to address more critical areas of peopleu0027s daily computing needs, which bring concern on the applicationsu0027 quality. Todayu0027s Mobile apps processed not only the traditional GUI events but also accept and react to constantly varying context events which may have an impact on the applicationu0027s behaviour. To build high quality and more reliable applications, there is a need for effective testing techniques to test apps before release. Most of recent testing technique focuses on GUI events only making it difficult to identify other defects in the changes that can be inclined by the context in which an application runs. This paper proposed an approach for testing mobile apps considering the two sets of events: GUI events which we identified through static analysis of bytecode and context events obtained from analysis of manifest.xml file. Results from the experimental evaluation indicated that our approach is effective in identifying and testing context events.	display resolution;graphical user interface;mobile app;static program analysis;test case	Asmau Usman;Noraini Ibrahim;Ibrahim Anka Salihu	2018		10.1145/3185089.3185099	bytecode;real-time computing;android (operating system);computer science	SE	-56.0450848926811	38.7651517060631	30307
a882ac0715dad959b2b1e8526e601378efe50e12	towards a rich sensing stack for iot devices	iot devices;mobile device;inference mechanisms;smartphone;internet of things;context inference;sensing stack;personal sensory data;networking stack;sensing;the internet of things;semantic abstraction;mobile computing;context-aware application;mobile communication;pipelines;privacy	The broad spectrum of interconnected sensors and actuators, available on various mobile devices and smartphones and collectively defined as the Internet of Things (IoT), have evolved into platforms with ability to both collect personal sensory data and also change the users' immediate environment. The continuous streams of richly annotated sensory data on these IoT devices have also enabled the emergence of a new class of context-aware apps that use the data to infer user context and accordingly customize their responses in real-time. However, this growth in the number of apps has not been complemented with adequate system support on the IoT devices resulting in monolithic apps that each implement and execute their own customized sensing pipelines. In this paper, we outline our vision of a sensing stack, akin to a networking stack, that can facilitate the development and execution of context-aware apps on IoT devices. There are several advantages to building a rich sensing stack. First, it allows apps to reuse stages of the sensing pipeline easing their development. Second, the layers of the stack allow for both in- and cross-layer resource optimization. Finally, it allows better control over the shared data as instead of raw-sensor data, higher-level semantic abstractions, such as inferences can now be shared with apps. We describe our initial efforts towards creating the different building blocks of such a sensing stack.	as-interface;call stack;emergence;internet of things;mathematical optimization;mobile device;pipeline (computing);protocol stack;real-time web;smartphone	Chenguang Shen;Haksoo Choi;Supriyo Chakraborty;Mani B. Srivastava	2014	2014 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)		embedded system;mobile telephony;computer science;operating system;pipeline transport;internet privacy;privacy;world wide web;internet of things	Mobile	-39.57891111518161	48.18032737183634	30351
be8f1cc06982ca23c396845bdf4e632a68631a9c	privacy preserving for continuous query in location based services	privacy mobile communication acceleration quality of service joining processes silicon algorithm design and analysis;query processing data privacy mobile computing;query linking privacy;query processing;privacy preserving;location based services lbss;quality of service qos location based services lbss privacy preserving query linking privacy velocity and acceleration similarity;data privacy;qos privacy preserving continuous query location based services lbs mobile user location data sensitive data quality of service;quality of service qos;velocity and acceleration similarity;mobile computing	Location-based services (LBSs) have become a popular and important way to provide real-time information and guidance. The abuse of mobile users' location data, which may violate their sensitive and private personal information, is one of the major challenges faced by LBS. On the other hand, the query launched by mobile users should not be linked to them even if they are required to expose their location information to attain some services. However, many location based systems (e.g., mobile social networks, store finders) are lacking of users' private preserving consideration. In this paper, we focuse-focus on the issues related to query linking privacy. Particularly, we aim to preserve mobile users' privacy in location based mobile systems where their location information may be available, furthermore, while facing attacks, the sensitive data of a specific mobile user launching the query should not be disclosed to an adversary. We present a new query linking privacy preserving algorithm (V-DCA) for continuous LBS by taking the user's velocity and acceleration similarity into consideration. The consecutive generated cloaked sets are used to create the new cloaked region, which decreases the complexity of the algorithm while fulfilling the privacy requirement. The simulation results show that V-DCA can preserve mobile user's privacy as well as provide better Quality of Service (QoS).	adaptive huffman coding;adversary (cryptography);algorithm;experiment;location-based service;mobile social network;personally identifiable information;privacy;quality of service;real-time data;simulation;velocity (software development)	Yong Wang;Long-Ping He;Jing Peng;Ting-ting Zhang;Hong-Zong Li	2012	2012 IEEE 18th International Conference on Parallel and Distributed Systems	10.1109/ICPADS.2012.38	privacy software;computer science;operating system;database;internet privacy;mobile computing;world wide web	DB	-39.539499000988236	60.385958006334455	30377
07f47173788f96cb7985182744ba780c48340ba4	model checking an entire linux distribution for security violations	security properties;security of data linux program verification;model checker;software model checking;software development process;program verification;software development software model checking linux distribution security violation program behavior verification security bug property model checker exploitable bug;security bug property;large scale;lines of code;software verification and validation;model checking;security violation;exploitable bug;software development;program behavior verification;linux;false positive;linux distribution;linux computer bugs packaging software tools programming computer security jacobian matrices software packages safety automata;security of data;data security	Software model checking has become a popular tool for verifying programs' behavior. Recent results suggest that it is viable for finding and eradicating security bugs quickly. However, even state-of-the-art model checkers are limited in use when they report an overwhelming number of false positives, or when their lengthy running time dwarfs other software development processes. In this paper we report our experiences with software model checking for security properties on an extremely large scale - an entire Linux distribution consisting of 839 packages and 60 million lines of code. To date, we have discovered 108 exploitable bugs. Our results indicate that model checking can be both a feasible and integral part of the software development process	exception handling;floor and ceiling functions;linux;model checking;security bug;software bug;software development process;source lines of code;time complexity;tracing (software);verification and validation	Benjamin Schwarz;Hao Chen;David A. Wagner;Jeremy Lin;Wei Tu;Geoff Morrison;Jacob West	2005	21st Annual Computer Security Applications Conference (ACSAC'05)	10.1109/CSAC.2005.39	software security assurance;model checking;computer science;operating system;database;programming language;computer security	SE	-58.68761179063259	56.58574613372741	30380
738128713a9d26f8cf0b23811975583509ab82bd	the implementation of start stop system with the obd-ii interface in the automotive smart key system	vehicle network;obd ii;smart key system;ecu;start stop system	Along with the growing needs for the low energy consumption technology and the strengthening vehicle environmental regulations, the researches on the start stop system, which stops the engine on idle, have been briskly carried out around the automobile makers before the appearance of the alternative energy. In addition, the automobile makers are trying to popularize the start stop system by combining the system to the generalized smart key system to not only arouse purchasing but also provide the convenience and reduce the energy consumption as well. In this paper, we designed and implemented the start stop system algorithm for the aftermarket smart key system which uses the OBD-II interface. The implemented start stop system is capable of controlling two independent systems, both an eco-friendly start stop system and a convenient smart key system, on a single ECU. Furthermore, the implemented start sop system standardizes the interface with the vehicles to reduce the time required for installing the start stop system to the various vehicles, and satisfies every standard response time limit for the vehicle status request signals.	algorithm;benchmark (computing);engine control unit;purchasing;real-time computing;real-time operating system;response time (technology);selective area epitaxy;smart key	Kyeong-seob Kim;In-seong Song;Yun-sub Lee;Sang-bang Choi	2013	Multimedia Tools and Applications	10.1007/s11042-013-1485-x	real-time computing;simulation	Robotics	-38.77165656150273	53.868394739523346	30401
d2cd799b5a95390f5acf0a89bc0d181370d39cdc	a security policy enforcement framework for controlling iot tenant applications in the edge		In the context of edge computing, IoT-as-a-Service (IoTaaS) with IoT data hubs and execution services allow IoT tenant applications (apps) to be executed next to IoT devices, enabling edge analytics and controls. However, this brings up new security challenges on controlling tenant apps in IoTaaS, whilst the great potential of IoTaaS can only be realized by flexible security mechanisms to govern such applications. In this paper, we propose a Model-Driven Security policy enforcement framework, named MDSIoT, for IoT tenant apps deployed in edge servers. This framework allows execution policies specified at the model level and then transformed into the code that can be deployed for policy enforcement at runtime. Moreover, our approach supports for the interoperability of IoT tenant apps when deployed in the edge to access IoTaaS services. The interoperability is enabled by an intermediate proxy layer (gatekeeper) that abstracts underlying communication protocols to the different IoTaaS services from IoT tenant apps. Therefore, our approach supports different IoT tenant apps to be deployed and controlled automatically, independently from their technologies, e.g. programming languages. We have developed a proof-of-concept of the proposed gatekeepers based on ThingML, derived from execution policies. Thanks to the ThingML tool, we can generate platform-specific code of gatekeepers that can be deployed in the edge for controlling IoT tenant apps based on the execution policies.	access control;code generation (compiler);devops;digital subscriber line;edge computing;experiment;extensibility;gatekeeper;interoperability;logistics;model-driven engineering;model-driven security;platform-specific model;programming language;prototype;run time (program lifecycle phase);while	Phu H. Nguyen;Phu H. Phung;Hong Linh Truong	2018		10.1145/3277593.3277602	computer network;computer security;access control;computer science;communications protocol;interoperability;services computing;server;edge computing;security policy;analytics	Mobile	-40.67976412796523	41.36277432592158	30440
5ccd55119656f52c01b4adbe771985ff390abc08	giving wings to your data: a first experience of personal cloud interoperability	personal clouds;syntactic interoperability;vendor lock in;cloud storage	Personal Clouds are becoming increasingly popular storage services for end-users and organizations. However, the competition among Personal Clouds, their proprietary nature and the heterogeneity of synchronization protocols have led to a complete lack of interoperability among them. Regrettably, this situation impedes that users share data transparently across multiple providers. Even worse, the lack of interoperability has associated serious risks, such as vendor lock-in, in which users get trapped in a single provider due to the cost of switching to another one.#R##N##R##N#In this work, we contribute DataWings: The first interoperability protocol for Personal Clouds. DataWings consists of an authentication management protocol and a storage API for file storage, synchronization and sharing that adhere to the current authentication (OAuth) and REST standards, respectively. Moreover, we demonstrate the feasibility of DataWings by implementing the protocol in various providers (NEC, StackSync, eyeOS) and performing a real deployment evaluated with real trace replays of production systems (UbuntuOne, NEC). To our knowledge, this is the first real-world experience of Personal Cloud interoperability. Our experiments provide new insights on the performance implications that different types of user activity and the underlying sharing network topology have on the implementation of our protocol. We conclude that DataWings is flexible enough to leverage interoperability for heterogeneous Personal Clouds, opening the door for a broader adoption by other vendors.	interoperability;personal cloud	Raúl Gracia Tinedo;Cristian Cotes;Edgar Zamora-Gómez;Genís Ortiz;Adrián Moreno-Martínez;Marc Sánchez Artigas;Pedro García López;Raquel Sánchez;Alberto Gomez;Anastasio Illana	2018	Future Generation Comp. Syst.	10.1016/j.future.2017.01.027	cloud storage;world wide web;semantic interoperability;vendor lock-in;vendor;interoperability;cross-domain interoperability;personal cloud;computer security;computer science;distributed computing;authentication	HPC	-42.83514416601826	54.986857276686266	30489
0eeec90d779f2723f83864394fd857ae38d4829b	grammatical aspects: coping with duplication and tangling in language specifications.	context-free grammar;computer language;tool development;aspect-oriented programming principle;different annotation;different generator;language specifications;pretty-printing instruction;semantic action;open-source tool;editing support;grammatical aspects	For the purposes of tool development, computer languages are usually described using context-free grammars with annotations such as semantic actions or pretty-printing instructions. These specifications are processed by generators which automatically build software, e.g., parsers, pretty-printers and editing support.rnrnIn many cases the annotations make grammars unreadable, and when generating code for several tools supporting the same language, one usually needs to duplicate the grammar in order to provide different annotations for different generators.rnrnWe present an approach to describing languages which improves readability of grammars and reduces the duplication. To achieve this we use Aspect-Oriented Programming principles. This approach has been implemented in an open-source tool named GRAMMATIC. We show how it can be used to generate pretty-printers and syntax highlighters.		Andrey Breslav	2010		10.3233/978-1-60750-688-1-197	natural language processing;linguistics;communication	NLP	-52.48914662407428	36.23193553528003	30574
22578a78c5dc522fbaf6e3e9ef2d9171b5a52a0c	passive testing of communicating systems with timeouts	software testing;formal methods;timeouts;communicating systems;passive testing	Context: The design of complex systems demands methodologies to analyze its correct behaviour. It is usual that a correct behaviour is determined by the compliance with temporal requirements. Currently, testing is the most used technology to validate the correctness of systems. Although several techniques that take into account time aspects have been proposed, most of them require the tester interacts with the system. However, if this is not possible, it is necessary to apply a passive testing approach where the tester monitors the behaviour of the system. Objective: The aim of this paper is to propose a methodology to perform passive testing on communicating systems in which the behaviour of their components must fulfill temporal restrictions associated with both performance and delays/timeouts. Method: Our framework uses algorithms for checking traces collected from the systems against invariants which formally represent the most relevant properties that must be fulfilled by the system. In order to support the feasibility of the methodology, we have performed an empirical study on a complex system for automatic recognition of images based on a pipeline architecture. We have analyzed the correctness of the system’s behaviour with respect to a set of invariants. Finally, an experiment, based on mutations of the system, was conducted to study the level of detection of a set of invariants. Results: Different errors were detected and fixed along the development of the system by means of the proposed methodology. The results of the experiments with the mutated versions of the system indicated that the designed set of invariants was more effective in finding errors associated to temporal aspects than those related to communication among components. Conclusion: The proposed technique has been shown to be very useful for analyzing complex timed systems, and find errors when the tester has no control over their behaviour. 2015 Elsevier B.V. All rights reserved.	algorithm;complex system;complex systems;correctness (computer science);distributed computing;experiment;fo (complexity);finite-state machine;invariant (computer science);mutation testing;parallel computing;pipeline (computing);rational clearcase ucm;requirement;serial digital video out;simulation;tracing (software)	Mercedes G. Merayo;Alberto Nuñez	2015	Information & Software Technology	10.1016/j.infsof.2015.03.009	real-time computing;simulation;formal methods;computer science;software engineering;software testing;algorithm	SE	-41.695320891092486	35.828181661874034	30644
4424b63ee1ed0ae1f8c7403710c8c41d18975eb8	intrusion detection: randomized instruction set emulation to disrupt binary code injection attacks	information hiding;automated diversity;emulation;proof of concept;binary translation;language randomization;code injection attack;source code;obfuscation;security;open source	Binary code injection into an executing program is a common form of attack. Most current defenses against this form of attack use a 'guard all doors' strategy, trying to block the avenues by which execution can be diverted. We describe a complementary method of protection, which disrupts foreign code execution regardless of how the code is injected. A unique and private machine instruction set for each executing program would make it difficult for an outsider to design binary attack code against that program and impossible to use the same binary attack code against multiple machines. As a proof of concept, we describe a randomized instruction set emulator (RISE), based on the open-source Valgrind x86-to-x86 binary translator. The prototype disrupts binary code injection attacks against a program without requiring its recompilation, linking, or access to source code. The paper describes the RISE implementation and its limitations, gives evidence demonstrating that RISE defeats common attacks, considers how the dense x86 instruction set affects the method, and discusses potential extensions of the idea.	binary code;code injection;emulator;machine code;open-source software;prototype;randomized algorithm;valgrind;x86	Elena Gabriela Barrantes;David H. Ackley;Trek S. Palmer;Darko Stefanovic;Dino Dai Zovi	2003		10.1145/948109.948147	code injection;dead code;self-modifying code;emulation;code bloat;parallel computing;obfuscation;code access security;computer science;information security;theoretical computer science;dead code elimination;redundant code;information hiding;proof of concept;computer security;code generation;program animation;unreachable code;source code	Security	-56.145094050128684	56.167454571442605	30656
3f3bd059b7d8516932e5b764e15ea51304ba1e9b	towards aspect-based modeling of self-healing web services	web services fault tolerant computing software maintenance software reusability;self healing web service;software maintenance;aspect based modeling;aspect oriented programming aspect based modeling self healing web service;web service;fault tolerant computing;aspect oriented programming;software reusability;web services;computer application;web services context aware services computer crashes encapsulation decision making service oriented architecture condition monitoring permission	In this paper, we overviewed self-healing issues of Web services using AOP. This type of programming supports separation of self-healing concerns from Web services code and promotes maintenance and reusability. Similar to any computing application, Web services are subject to failure and unavailability due to multiple reasons, such as Web service faulty-code and unreliable communication-infrastructure	unavailability;web service	Ghita Kouadri Mostéfaoui;Zakaria Maamar;Nanjangud C. Narendra;Philippe Thiran	2006	2006 IEEE International Conference on Web Services (ICWS'06)	10.1109/ICWS.2006.132	web service;web application security;web development;web modeling;business process execution language;web analytics;web-based simulation;web design;web standards;computer science;operating system;ws-policy;service-oriented architecture;web navigation;ws-addressing;services computing;web intelligence;ws-i basic profile;law;world wide web	Robotics	-43.53210172426493	41.33649985395817	30659
431c83ce6cedc2c9b2a1d4ba5472fde233c73a1a	addressing client needs for cloud computing using formal foundations		Cloud-enabled large-scale distributed systems orchestrate resources and services from various providers in order to deliver highquality software solutions to the end users. The space and structure created by such technological advancements are immense sources of information and impose a high complexity and heterogeneity, which might lead to unexpected failures. In this chapter, we present a model that coordinates the multi-cloud interaction through the specification, validation, and verification of a middleware exploiting monitoring and adaptation processes. The monitoring processes handle collecting meaningful data and assessing the state of components, while the adaptation processes restore the system as dictated by the evolution needs and sudden changes in the operating environment conditions. We employ Abstract State Machines to specify the models and we further make use of the ASMETA framework to simulate and validate them. Desired properties of the system are defined and analysed with the aid of the Computation Tree Logic.	abstract state machines;cloud computing;computation tree logic;distributed computing;middleware;nusmv;operating environment;quality of service;requirement;simulation;software development	Andreea Buga;Sorana Tania Nemes;Atif Mashkoor	2018			computer science;end user;theoretical computer science;cloud computing;abstract state machines;software;operating environment;computation tree logic;distributed computing	SE	-45.09801979130244	40.554801011296334	30675
d11027f7bf72ad83fa42210181e50247822e5106	a temporarily-spatially constrained model based on trbac in workflow system	role based access control;workflow system	A temporarily-spatially constrained model based on TRBAC (Task-Role Based Access Control) is proposed in workflow system. Temporary and spatial Constraint is that user is not only constrained by temporality, but also constrained by spatiality when user executes the task. The model suggests that a property of security level should be increased in task. The newly increased property can make the workflow more safety and flexibility.	role-based access control	Limin Ao;Wei Zhou;Xiaodong Zhu	2007		10.1007/978-0-387-77253-0_18	workflow management system;workflow engine;workflow technology	EDA	-49.13790029948374	52.974343134942806	30828
bb9be8a1f6b7f2140169497901ad3bdb20d4746f	a software architecture for the deployment of executable transformation models	software;deployment;transformation model;difference operator;real time;dynamic model;emergency management;input output;software architecture;adaptivity;executable;decision making process;transformation;active filter;point of view;architecture;models	Emergency management involves collaboration among different operators (e.g. policemen, firemen, medics) on critical and dangerous situations (e.g. fires, floods). Real-time elaborations of a large amount of information and knowledge are needed to automate control and decision making processes. In this scenario raw data are captured and processed through a chain of activities (filtering, fusion, classification) generating abstract information that is selected, diffused and presented to interested users. These activities can be turned into the interpretation of executable transformation models, abstract representations of input-output transformations which can be coded on a calculator. Executable transformation models might not be static; but they may be dynamically generated by observing and processing incoming data. From the architectural point of view the dynamic model generation implies the existence of mechanisms for the dynamic deployment of the executable transformation models. In this paper we present a software architecture aimed at providing these mechanisms.	executable;mathematical model;process (computing);real-time transcription;software architecture;software deployment	Diego Bernini;Daniele Toscani;Marco Frigerio	2009		10.1145/1582379.1582391	transformation;embedded system;software architecture;real-time computing;simulation;telecommunications;computer science;theoretical computer science;architecture;operating system;executable;computer security;active filter;emergency management	SE	-39.618757271856616	36.72952596714764	30845
6b3455bdd4870cc1b6eaf18dcd17dd7d0158844e	towards using code coverage metrics for performance comparison on the implementation level	performance comparison;performance tests;test case generation;algorithm engineering	The development process for new algorithms or data structures often begins with the analysis of benchmark results to identify the drawbacks of already existing implementations. Furthermore it ends with the comparison of old and new implementations by using one or more well established benchmark. But how relevant, reproducible, fair, verifiable and usable those benchmarks may be, they have certain drawbacks. On the one hand a new implementation may be biased to provide good results for a specific benchmark. On the other hand benchmarks are very general and often fail to identify the worst and best cases of a specific implementation. In this paper we present a new approach for the comparison of algorithms and data structures on the implementation level using code coverage. Our approach uses model checking and multi-objective evolutionary algorithms to create test cases with a high code coverage. It then executes each of the given implementations with each of the test cases in order to calculate a cross coverage. Using this it calculates a combined coverage and weighted performance where implementations, which are not fully covered by the test cases of the other implementations, are punished. These metrics can be used to compare the performance of several implementations on a much deeper level than traditional benchmarks and they incorporate worst, best and average cases in an equal manner. We demonstrate this approach by two example sets of algorithms and outline the next research steps required in this context along with the greatest risks and challenges.	benchmark (computing);best practice;code coverage;data structure;evolutionary algorithm;formal verification;model checking;test case	Mathias Menninghaus;Elke Pulvermüller	2016		10.1145/2851553.2858663	real-time computing;algorithm engineering;computer science;theoretical computer science;data mining	SE	-60.523973443011506	34.57151948612301	30856
73c55cd6a0c67699de44b1917f50e3e9a7cc9f21	an agent-based testing approach for web applications	web documents;web pages;performance evaluation;application software;agent based;uncertainty;application software internet web pages system testing html computer science performance evaluation java uncertainty web server;object level testing;web document web application level testing internet open standard technologies control flow mechanisms data flow mechanisms agent based approach function level testing function cluster level testing object level testing abstract level;control flow mechanisms;software agents;html;structural testing;internet;program testing;web document;agent based approach;web application level testing;control flow;abstract level;data flow analysis;data flow mechanisms;system testing;function level testing;web server;computer science;program testing internet data flow analysis software agents;data flow;function cluster level testing;open standard technologies;open standard;java;dynamic behavior	In recent years, Web applications have grown so quickly that they have already become crucial to the success of businesses. However, since they are built on Internet and open standard technologies, Web applications bring new challenges to researchers, such as dynamic behaviors, heterogeneous representations, novel control flow and data flow mechanisms, etc. In this paper, we propose an agent-based approach for Web application testing. While the agent-based framework greatly reduces the complexity of Web applications, a four-level dataflow test approach can be employed to perform structure testing on them. In this approach, data flow analysis is performed as function level testing, function cluster level testing, object level testing, and Web application level testing, from low abstract level to high abstract level. Each test agent in the framework takes charge of the testing in an abstract level for a particular type of Web document or object.	agent-based model;control flow;data-flow analysis;dataflow;internet;web application;web testing	Yu Qi;David Chenho Kung;W. Eric Wong	2005	29th Annual International Computer Software and Applications Conference (COMPSAC'05)	10.1109/COMPSAC.2005.42	web service;data flow diagram;black-box testing;application software;web modeling;the internet;open standard;uncertainty;html;software performance testing;white-box testing;computer science;software agent;operating system;software engineering;data-flow analysis;cloud testing;web page;data mining;database;web engineering;programming language;control flow;java;system testing;world wide web;web server;mashup	SE	-52.99698598466779	39.69396073456379	30860
8aa72aff800137f9e64b979bb678da5447049368	towards a virtualized sensing environment	sensor network;proof of concept	While deploying a sensor network is necessary for proofof-concept experimentation, it is a time-consuming and tedious task that dramatically slows innovation. Treating sensor networks as shared testbeds and integrating them into a federated testbed infrastructure, such as FIRE, GENI, AKARI, or CNGI, enables a broad user community to benefit from time-consuming deployment exercises. In this paper, we outline the challenges with integrating sensor networks into federated testbeds in the context of ViSE, a sensor network testbed we have integrated with GENI, and describe our initial deployment experiences. ViSE differs from typical embedded sensor networks in its focus on highbandwidth steerable sensors.	cloud computing;embedded system;fire;sensor;software deployment;testbed;virtual community	David Emory Irwin;Navin Sharma;Prashant J. Shenoy;Michael Zink	2010		10.1007/978-3-642-17851-1_10	simulation;wireless sensor network;computer science;proof of concept	Mobile	-41.545221211697886	48.732576032189854	30866
799ead88b78fbde80533c8f9d7cdbbb44ef18c6b	service discovery for the internet of things: comparison study of the approaches		The Internet of Things (IoT) has gained a significant attention in the last years. It covers multiple domains and applications such as smart home, smart healthcare, IT transportation…etc. The highly dynamic nature of the IoT environment brings to the service discovery new challenges and requirements. As a result, discovering the desirable services has become very challenging. In this paper, we aim to address the IoT service discovery problem and investigate the existing solutions to tackle this problem in many aspects, therefore we present a new and full comparative study of the most representative (or outstanding) service discovery approaches in the literature over four perspectives: (1) the IoT service description model, (2) the mechanism of IoT service discovery, (3) the adopted architecture and (4) the context awareness. After each of these perspectives, we discuss the existing solutions as well as the main challenges that face the service discovery issue in the IoT domain. Besides, we adopt a classification of the IoT service discovery approaches based on their mechanism of discovery, and we propose a set of requirements that need to be considered when defining an IoT service in the IoT domain.	context awareness;home automation;internet of things;requirement;service discovery	Aziez Meriem;Saber Benharzallah;Bennoui Hammadi	2017	2017 4th International Conference on Control, Decision and Information Technologies (CoDIT)	10.1109/CoDIT.2017.8102660	knowledge management;architecture;service discovery;home automation;data mining;internet of things;context awareness;engineering	Mobile	-44.00711872105394	46.94309267601556	30942
de53385b1e511c28a0279e45e949d34a1e055497	dynamic slicing with soot	debugging;slicing;static analysis;dynamic analysis	Slicing is a powerful technique that can help a developer to understand how the interaction of different parts of a program causes a specific outcome. Dynamic slicing uses runtime information to compute a precise slice for a given execution. However, dynamic slicing is not possible without a static analysis of the underlying code to reveal dependencies between the instructions that have been recorded.  In this paper, we present a new algorithm for computing dynamic slices. We describe how the optimization framework Soot was used to compute specialized intraprocedural dependency graphs that better reflect the programmer's view of a program than previous approaches. Combining these dependency graphs with recorded execution traces allowed us to create debuggable dynamic slices. For this, a mapping of the debugger's model of the execution to the static code model of Soot was needed, and could be found with only few ambiguities.  This results in the ability to produce a dynamic slice that can not only be visualized, but explored interactively and adjusted to better answer specific questions.	algorithm;debugger;interactivity;mathematical optimization;programmer;run time (program lifecycle phase);static program analysis;tracing (software)	Arian Treffer;Matthias Uflacker	2014		10.1145/2614628.2614631	program slicing;real-time computing;computer science;theoretical computer science;distributed computing	PL	-54.9625554154378	36.55387223954799	30989
a867e1aadaa29860545129dfe30578994e42c1b8	plagiarism detection in software designs	electronic commerce;proxy signature;software design;security;electronic voting;plagiarism detection	Detecting plagiarism in software is a computationally complex process. At the same time it is critical, for the lack of a deterrent through detection may result in various losses. Several systems to detect plagiarism have been proposed. However, their lexically-based analysis is not powerfull enough and can be foiled with minimal efforts. To address their shortcomings, we have devised a detection framework with the following salient features: (1) designs, instead of code, are compared; (2) multi--level abstractions of the design are generated; and (3) comparison follows a stepwise process according to the abstraction levels. A comparison with existing systems shows that this strategy results in simpler algorithms and more accurate analyses.	algorithm;chart;software framework;stepwise regression	Boumediene Belkhouche;Anastasia Nix;Johnette Hassell	2004		10.1145/986537.986585	computer science;data mining;world wide web;computer security	SE	-60.334195832156155	55.78121573217817	31036
ce5d857824e1d1b147abb6b4719a53c5342926a8	meeting room - a secure multi-access, cross-platform telemedicine application	portals;teleconferencing;telemedicine;web based application telemedicine teleconference;medical computing;video communication internet medical computing mobile computing patient treatment portals teleconferencing telemedicine;internet;mobile android version meeting room medical advice doctor elderly medical treatment less privileged community segments secure multiaccess cross platform telemedicine application accessible health care services web based approach cross platform devices network connectivity community centers day care centers clinics video conferencing application web portal online meetings web based version;patient treatment;video communication;mobile computing;medical services streaming media portals telemedicine mobile communication web servers	The high costs and inconvenience of visiting a doctor for medical advice have deterred a sizable number of elderly and needy from seeking proper medical treatment promptly and this may cause their conditions to deteriorate and possibly resulting in a more severe impact on their daily lives. Thus, this application is designed with the aim to bring more convenience and health care services to these less privileged segments of the community. In this paper a secure multi-access cross-platform telemedicine application, known as MEETING ROOM, has been designed and developed to provide easily accessible health care services. The MEETING ROOM application adopts a web-based approach in general to provide the flexibility to run on cross-platform devices. Computing devices with network connectivity can be placed in clinics, day care centers, community centers or right at their homes so that they can easily access the application with those devices. It is a hybrid application, comprising of a web portal and a video conferencing application. The web portal is used to manage the system and the users in a secure manner while the video conferencing application provides a convenient communication platform for the patients to consult a doctor on their conditions and for the doctors to hold online meetings. A web-based version and mobile Android version of the video conferencing application are developed to provide the flexibility.	android;authorization;central processing unit;computer;mobile device;smartphone;streaming media;transmitter;web application;world wide web	Geck Keat Chan;Kok Kiong Tan;Arun-Shankar Narayanan	2013	2013 Australasian Telecommunication Networks and Applications Conference (ATNAC)	10.1109/ATNAC.2013.6705381	simulation;medicine;multimedia;world wide web	HCI	-38.971183868389815	54.873656524912974	31047
1e4b2bb81c03cded6f3d3319ab1ecb71ae519476	specification coverage aided test selection	software metrics;software testing;protocols;randomised algorithms conformance testing formal specification formal verification software metrics petri nets;formal specification;bounded model checking specification coverage metric test selection strategy formal conformance testing ioco conformance testing relation on the fly test generation algorithm greedy test selection algorithm randomization;laboratories computer science protocols software testing data communication computer bugs java event detection system testing greedy algorithms;randomised algorithms;greedy algorithms;test selection strategy;event detection;data communication;formal conformance testing;bounded model checking;ioco conformance testing relation;specification coverage metric;formal verification;conformance testing;greedy test selection algorithm;on the fly;system testing;test generation;computer science;petri nets;randomization;computer bugs;on the fly test generation algorithm;java	In this paper test selection strategies in formal conformance testing are considered. As the testing conformance relation we use theioco relation, and extend the previously presented on-the-fly test generation algorithms for ioco to include test selection heuristic based on a specification co verage metric. The proposed method combines a greedy test selection with randomization to guarantee completeness. As a novel implementation technique we employ bounded model checking for lookahead in greedy test selection.	academy;code coverage;conformance testing;experiment;genetic algorithm;greedy algorithm;heuristic;model checking;parsing;real life;software framework;specification language	Tuomo Pyhälä;Keijo Heljanko	2003		10.1109/CSD.2003.1207713	randomization;communications protocol;greedy algorithm;real-time computing;software bug;formal verification;computer science;theoretical computer science;conformance testing;formal specification;software testing;programming language;java;system testing;petri net;software metric	SE	-49.508003672435606	35.25789596226995	31059
2034f398e13f5a21cdf2f03fd3f99a9d8581c9c2	smart environment software reference architecture	acting;software;ubiquitous computing artificial intelligence inference mechanisms software architecture;software reference architecture;computer architecture application software ubiquitous computing software architecture proposals pervasive computing automatic control intelligent sensors software prototyping prototypes;sensors;prediction algorithms;inference mechanisms;smart applications;control network;satisfiability;data mining;ubicomp;office;software architecture;hotel smart environment software reference architecture ubiquitous computing home automation control devices control networks smart applications perception reasoning acting office;cognition;ubicomp smart environments software architecture;control networks;artificial intelligence;ubiquitous computing;perception;reasoning;hotel;reference architecture;smart environments;smart environment;concrete;home automation;automation;control devices	Nowadays ubiquitous computing is spreading to all scopes of our lives. Smart environments are present at every location such us homes with automation and control devices, offices full of control networks to assist workers, or hotels with even more control devices in order to save energy and satisfy guests preferences.This paper focus on the proposal of a Reference Architecture for developing Smart Applications and deploy them in Smart Environments. The proposal consider three main process in the Software Architecture of these applications:a) perception, b) reasoning and c) acting.	emoticon;machine perception;reference architecture;smart environment;software architecture;ubiquitous computing;user (computing)	Alejandro Fernández-Montes;Juan Antonio Ortega;Juan Antonio Álvarez;Luis González Abril	2009	2009 Fifth International Joint Conference on INC, IMS and IDC	10.1109/NCM.2009.115	embedded system;real-time computing;computer science;smart environment;internet of things;ubiquitous computing	Robotics	-41.01800093819248	44.51267667939881	31112
77b232d9926550af00360c844b29c06fd4cfe192	a beautiful story behind the battle of privacy	androids;humanoid robots;mobile communication;sun;authorization;privacy;context aware services	Mobile intelligent devices are widely used in our daily lives. User private information can be easily accessed by mobile apps, which are facing fierce threats. Although Android offers a native permission mechanism and there are many security assistants or research works focus on improving privacy protection, there is still a gap on the context aware and personalized authorization for user requirements. We design and implement a novel authorization mechanism on Android which takes into account both user preference and context. This video describes a fictitious romantic story to show the inspiration behind our design. The story shows the threats we are facing in the mobile scenarios and the gap between existing solutions and user privacy demands. A brief introduction of our model is also given in the video, including the outline of our design and a short clip which shows the evaluation of our prototype implementation on real device.	android;authorization;mobile app;personalization;personally identifiable information;privacy;prototype;requirement;user requirements document	Tianyuan Liu;Guoyun Li;Yaozhi Zhang;Yuqing Sun	2016	2016 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)	10.1109/iThings-GreenCom-CPSCom-SmartData.2016.108	embedded system;mobile telephony;computer science;humanoid robot;operating system;authorization;internet privacy;privacy;world wide web;computer security;computer network	Mobile	-38.93469847091054	53.31420519069104	31212
43807eb67c9d5ed2bd46605e80a4e23ebc3795fe	data gathering for internet of vehicles safety		Internet of Vehicles (IoV) constitutes an important part of the Smart Cities concept, relying on different technologies and including heterogeneous cars types, which raises challenges to ensure and preserve road safety. Throughout this paper, we tackled the data collection and transmission problems related to the safety in the terrestrial domain by identifying the car-types and exposing the different communication data levels and means used in the quest of safety realization.	aerial photography;internet;marine ecosystem;quality of service;relevance;requirement;smart city;terrestrial television;unmanned aerial vehicle	Chahrazed Ksouri;Imen Jemili;Mohamed Mosbah;Abdelfettah Belghith	2018	2018 14th International Wireless Communications & Mobile Computing Conference (IWCMC)	10.1109/IWCMC.2018.8450498	data collection;computer network;the internet;computer science	Mobile	-43.29920606273734	50.981122275689856	31274
8a1831dd1d35c9520f65cb40363e73cdd75a5ce3	an eclipse plug-in: dependency browser	programming profession software maintenance open source software java testing humans reconnaissance computer science educational institutions table lookup;human computer interaction;user friendly interface;eclipse plug in;software maintenance;public domain software;speed;parsing;grammars;human factors;dependency browser;open source applications;plug in algorithms design experimentation human factors dependency search eclipse;eclipse;memory usage eclipse plug in dependency browser software maintenance parsing user friendly interface open source applications usability speed;algorithms;design;program compilers;experimentation;usability;software maintenance grammars human computer interaction program compilers public domain software;dependency search;plug in;memory usage;open source	Dependency search is an important tactic in many software activities, particularly during software maintenance. Eclipse has a few powerful tools to support such activity but they can be more convenient for programmers to use. In this paper, we proposed a light-weight and agile Eclipse plug-in: Dependency Browser, which parses call dependencies and presents them with a user- friendly interface which automatically reacts to some specific programmers' actions. We tested Dependency Browser on four open-source applications to check the usability, speed and memory usage. The test result demonstrated that our new Eclipse plug-in is convenient to use, fast to run, and has high precision. Therefore, the Dependency Browser could be further developed into a useful commercial tool and help programmer to conduct software maintenance tasks.	agile software development;angular defect;eclipse;experiment;java;open-source software;plug-in (computing);programmer;prototype;software development process;software engineering;software maintenance;usability	Dapeng Liu;Shaochun Xu;Zhongyuan Liu	2007	2007 IEEE/ACS International Conference on Computer Systems and Applications	10.1109/AICCSA.2007.370862	eclipse;design;usability;computer science;human factors and ergonomics;operating system;software engineering;parsing;database;speed;programming language;software maintenance;public domain software;world wide web	SE	-53.648307936679764	36.43767792437477	31295
c0b9fedc923ec9cde7e11c4512be1bdbdb563597	codekoan: a source code pattern search engine extracting crowd knowledge		Source code search is frequently needed and important in software development. Keyword search for source code is a widely used but a limited approach. This paper presents CodeKōan, a scalable engine for searching millions of online code examples written by the worldwide programmers' community which uses data parallel processing to achieve horizontal scalability. The search engine relies on a token-based, programming language independent algorithm and, as a proof-of-concept, indexes all code examples from Stack Overflow for two programming languages: Java and Python. This paper demonstrates the benefits of extracting crowd knowledge from Stack Overflow by analyzing well-known open source repositories such as OpenNLP and Elasticsearch: Up to one third of the source code in the examined repositories reuses code patterns from Stack Overflow. It also shows that the proposed approach recognizes similar source code and is resilient to modifications such as insertion, deletion and swapping of statements. Furthermore, evidence is given that the proposed approach returns very few false positives among the search results.	algorithm;anti-pattern;continuous integration;data parallelism;documentation;elasticsearch;identifier;integrated development environment;java;online codes;open-source software;paging;parallel computing;pattern search (optimization);plug-in (computing);programmer;programming language;programming tool;python;refactoring software, architectures, and projects in crisis;repository (version control);scalability;search engine indexing;software development;stack overflow;version control;web search engine	Christof Schramm;Yingding Wang;François Bry	2018	2018 IEEE/ACM 5th International Workshop on Crowd Sourcing in Software Engineering (CSI-SE)	10.1145/3195863.3195864	documentation;scalability;theoretical computer science;software development;search engine;python (programming language);source code;java;computer science;search algorithm	SE	-60.019734921368006	39.71820022690426	31448
8c47ae09e076a41ab74f8aeadac0ffa19e998206	test environment construction method using parameterized test environments		To achieve test automation, all stages such as the creation of test cases and data necessary to execute tests and the construction of test environments should be executed without any involvement of the tester. However, among the stages, the test environment construction process cannot be easily automated because it is executed through the tester's physical acts. To solve this problem, existing studies construct test environments using technologies such as virtualization or Infrastructure as Code. However, existing studies automate only a part of the test environment construction process. Therefore, this paper proposes the parameterized test environments, which are test environments expressed by parameter sets that characterize test environments. When this method is used, test environments can be constructed with only the inputs of parameters so that the problem of test environment construction, which was an obstacle to test automation, can be improved.	deployment environment;hardware virtualization;infrastructure as code;test automation;test case	Dong Hun Song;Yongjin Seo;Hyeon Soo Kim	2017	2017 24th Asia-Pacific Software Engineering Conference (APSEC)	10.1109/APSEC.2017.84	automation;computer science;real-time computing;virtualization;test case;software;virtual machining;parameterized complexity;java	SE	-48.762929082550855	37.01488001638248	31484
ea672286a694c5c1cb283f2b9b2daea5545f994d	spin-ning software architectures: a method for exploring complex	debugging;program verification software architecture software reusability program debugging;model checker;cost function;maintenance;computer architecture middleware cost function debugging software systems security buildings maintenance error correction software engineering;complex systems design;functional properties;complex software systems;software systems;architectural debugging complex systems design multiple non functional properties model checker software architectural composition complex software systems spin software architectures architecture transformation architecture discovery;multiple non functional properties;program verification;architectural composition;architecture transformation;software engineering;spin;software architectural composition;computer architecture;software architecture;architectural debugging;complex system;architecture discovery;error correction;software reusability;middleware;program debugging;security;software architectures;buildings;qa76 computer software	When designing complex systems that provide multiple non-functional properties, it is usual to try to reuse (and finally compose) simpler existing designs, which deal with each of these properties in solitude. This paper describes a method for automatically and quickly identifying all the different ways one can compose such designs, with the aid of a model checker.	algorithm;antivirus software;architectural pattern;complex systems;debugging;middleware;model checking;point of view (computer hardware company);spin;software architecture;speedup	Christos Kloukinas;Valérie Issarny	2001		10.1109/WICSA.2001.948409	model checking;software architecture;complex systems;computer architecture;error detection and correction;computer science;information security;software engineering;middleware;spin;debugging;software system;computer engineering	SE	-48.047417161443434	32.6263644471815	31489
77b797bb86a3edaadb6dcf69fd8f3f7be64bda3f	researches based on subject-oriented security in the cyber-physical system		The security research of Cyber-physical system is a dynamic development process. Because no single technique could ensure the absolute safety of CPS, so its safety problem must be considered from the overall and systematic researches. Based on the structure of CPS, CPS is here divided into some subjects, and various subjects are then discussed in face of security threats in the design of subject oriented CPS security model. This model is supplied with the WPDRRC security system model as a protective layer. With the technology oriented CPS system, subject oriented system security will have superiorities such as initiative, systemic, portability and simplified.	cyber-physical system	Caixia Zhang;Hua Li;Yuanjia Ma;Xiaoyu Wang;Xiangdong Wang	2014		10.1007/978-3-319-13326-3_33	computer security model;system model;distributed computing;software portability;computer security;cyber-physical system;computer science	Crypto	-55.498910607062754	48.97309934171972	31532
1b188c71aa91b321ca3048ed518acb80bbf33a3c	using live distributed objects for office automation: demo proposal	distributed application;distributed system;live distributed objects;collaborative application;soa;web service;distributed objects;middleware;distributed systems;drag and drop;office automation	"""Web services and platforms such as .NET make it easy to integrate interactive end-user applications with backend services. However, it remains hard to build collaborative applications in which information is shared within teams. We present a new drag-and-drop technology, in which standard office documents (spreadsheets, databases, etc.) are interconnected with eventdriven middleware (""""live distributed objects""""), to create distributed applications in which changes to underlying data propagate quickly to downstream applications. Information is replicated in a consistent manner, making it easy for team members to share updates and to coordinate their actions. In this demo, we present our middleware platform in office automation settings which is highly automated and highly configurable."""	.net framework;database;distributed computing;downstream (software development);drag and drop;live distributed object;middleware;replication (computing);spreadsheet	Jong Hoon Ahnn;Kenneth P. Birman;Krzysztof Ostrowski;Robbert van Renesse	2008		10.1145/1462735.1462752	middleware;computer science;middleware;database;distributed computing;distributed object;world wide web	PL	-34.50440594953388	43.43932437083143	31541
4dc060736a330f56e9be03a3e28016d77fd0e313	self-managed cell: a middleware for managing body-sensor networks	performance measure;wireless sensor;adaptive sensing;middleware wireless sensor networks sliding mode control body sensor networks patient monitoring condition monitoring hospitals context aware services availability resource management;cardiology;wireless sensor networks cardiology health care medical computing middleware patient monitoring;self managed cell;medical computing;autonomic management;low power;design and implementation;policy based adaptation;reconfigurable networks;middleware;patient monitoring;reconfigurable networks autonomic management adaptive sensing policy based adaptation;wireless sensor networks;heart monitoring self managed cell middleware body sensor networks low power on body wireless sensors self configuration;body sensor network;mobile user;integrated services;health care	Body sensor networks consisting of low-power on- body wireless sensors attached to mobile users will be used in the future to monitor the health and well being of patients in hospitals or at home. Such systems need to adapt autonomously to changes in context, user activity, device failure, and the availability or loss of services. To this end, we propose a policy- based architecture that uses the concept of a Self-Managed Cell (SMC) to integrate services, managed resources and a policy interpreter by means of an event bus. Policies permit the declarative specification of adaptation strategy for self- configuration and self-management. We present the design and implementation of the SMC and describe its potential use in a scenario for management of heart monitoring. Preliminary performance measurements are also presented and discussed.	architectural pattern;autonomy;complex systems;display resolution;event condition action;feedback;interaction;low-power broadcasting;management information system;middleware;peer-to-peer;requirement;self-management (computer science);sensor	Sye Loong Keoh;Naranker Dulay;Emil C. Lupu;Kevin P. Twidle;Alberto E. Schaeffer Filho;Morris Sloman;Steven Heeps;Stephen Strowes;Joseph S. Sventek	2007	2007 Fourth Annual International Conference on Mobile and Ubiquitous Systems: Networking & Services (MobiQuitous)	10.1109/MOBIQ.2007.4451009	embedded system;real-time computing;wireless sensor network;computer science;operating system;middleware;remote patient monitoring;integrated services;health care;computer network	Mobile	-41.46624576204419	44.69937002982377	31542
9f26d08f29cef7099cd724a4127151de6fd4a722	improving energy efficiency in iot with re-configurable virtual objects	energy efficiency;propagation losses;virtual objects;compression algorithms;iron;semantics;internet semantics compression algorithms energy efficiency propagation losses wireless sensor networks iron;internet of things;ubiquitous computing internet of things;internet;ubiquitous computing;ubiquitous computing energy efficiency reconfigurable virtual objects vo semantic descriptions ict objects physical objects software modules iot services analytical model powered weather station operating modes ws semantic description compression method;wireless sensor networks;internet of things virtual objects re configuration;re configuration	In this paper we present a method for reconfiguring Virtual Objects (VOs) during run-time. VOs are semantic descriptions of ICT objects and the associated physical objects and phenomena they observe. VOs also include software modules to expose ICT object functionalities as IoT services for re-use. In our case study we show, by using an analytical model, how the energy efficiency of a wireless battery powered weather station (WS) can be improved. We introduce descriptions of the available operating modes and store them in the WS semantic description. These operating modes of the WS's VO are linked with the ability of the ICT object to change the compression method used to transmit information during run-time. We evaluate the energy efficiency and latency of three operating modes, namely uncompressed, lossy and lossless modes. The results show that the possibility to re-configure the operating mode during run-time from uncompressed to lossy mode lowers the total energy for transmission to 47.9%.	lossless compression;lossy compression;mathematical optimization;ws-federation	Matti Eteläperä;Massimo Vecchio;Raffaele Giaffreda	2014	2014 IEEE World Forum on Internet of Things (WF-IoT)	10.1109/WF-IoT.2014.6803222	embedded system;real-time computing;human–computer interaction;computer science;operating system;distributed computing;semantics;iron;world wide web;computer security;internet of things;ubiquitous computing;computer network	Mobile	-39.856063458323185	47.005256377866665	31742
736a95f8c7dee6b2cc371900cb1ff56b92f6e3cf	an xml-based component specification model for an adaptive middleware of interactive digital television systems	middleware digital tv hdtv application software operating systems resource management hardware telecommunication computing software systems streaming media;middleware resources;xml digital television formal specification middleware interactive television;formal specification;adaptive middleware;application software;digital tv;resource management;software systems;telecommunication computing;digital television;operating system;component specification;streaming media;middleware resources xml component specification interactive systems digital television operating systems adaptive middleware middleware components;hdtv;xml;middleware;middleware components;interactive television;interactive systems;operating systems;hardware	A middleware for interactive digital television systems should harmonize and abstract discrepancies related to hardware and operating systems issues. In such a context, this paper describes an XML-based model for specifying components of an adaptive middleware for interactive digital television systems, in which component specifications play a fundamental role during negotiation, configuration and management of middleware components and resources.	middleware;operating system;xml	Frederico Borelli;Adilson B. Lopes;Glêdson Elias da Silveira	2004	18th International Conference on Advanced Information Networking and Applications, 2004. AINA 2004.	10.1109/AINA.2004.1283952	embedded system;middleware;real-time computing;digital television;computer science;resource management;message oriented middleware;operating system;middleware;database	HPC	-37.15108763520385	41.722194130925864	31748
a3a2968e1e93fd4b69f94caafaf8b852477385f2	key recovery functional model	generic model;functional model	This document describes a model for key recovery. The means by which plaintext may be recovered or reconstructed from recovered key(s) is not addressed in the key recovery model. The key recovery model is a generalized model that encompasses a wide variety of key recovery systems, including both key backup and encapsulated key recovery information techniques. The key recovery model is a functional model. That is, the components of the model are functional components not system components.	function model	John Kennedy;Stephen M. Matyas;Nevenko Zunic	2000	Computers & Security	10.1016/S0167-4048(00)86360-9	simulation;computer science;function model;computer security	Crypto	-53.88765762494809	49.35700621899234	31776
fdc330c34a79a689534c96558f2822e695096e9d	corroborating user assessments of software behavior to facilitate operational testing	software testing;similar execution profiles;audit information;corroboration based filtering;similar execution profiles user assessments software behavior operational testing software testing software problems corroboration based filtering audit information;software behavior;operational testing;program testing;software testing automatic testing computer industry costs software reliability reliability engineering computer science information filtering information filters internet;software problems;user assessments	"""Operational or """"beta"""" testing of software has a number of benefits for software vendors and has become common industry practice. However, ordinary users are more likely to overlook or misreport software problems than experienced software testers are. To compensate for this shortcoming, we present a technique called corroboration-based filtering for corroborating user assessments of individual operational executions for which audit information has been captured for possible offline review. Independent assessments concerning similar executions are pooled by automatically clustering together executions with similar execution profiles. Executions are chosen for review based on their user assessments, the size of the cluster each execution belongs to, and whether the cluster has already been confirmed by developers to contain an actual failure. We explain the rationale for this technique, analyze it probabilistically, and present the results of empirically comparing it to alternative techniques."""	cluster analysis;design rationale;online and offline;software testing	Vinay Augustine;Andy Podgurski	2007	The 18th IEEE International Symposium on Software Reliability (ISSRE '07)	10.1109/ISSRE.2007.30	non-regression testing;reliability engineering;long-term support;verification and validation;regression testing;software performance testing;system integration testing;computer science;engineering;acceptance testing;package development process;software reliability testing;software development;operating system;software engineering;software construction;data mining;database;operational acceptance testing;software testing;software walkthrough;software deployment;software quality;software metric;software quality analyst;software system;software peer review	SE	-61.165605335441384	35.75042317289724	31786
b13e30e1b8a025e8325f3f7e4455d79fcfb6a35d	a migration-oriented partial adaptation architecture for iot-empowered city platform as a service		Evolution of IoT-empowered city-scale platform leads to migration requirements. All successful IoT systems need to address migration of existing or external systems. The author discusses the challenges of phased migration of heterogeneous IoT systems. Then, the author presents approaches of migratable architecture. Finally, the author proposes a partial adaptation migration architecture to cope with the challenges. The mapping architecture can provide a framework of accommodating evolution of heterogeneous IoT systems.	platform as a service;requirement;software architecture	Toshihiko Yamakami	2017	2017 Twelfth International Conference on Digital Information Management (ICDIM)	10.1109/ICDIM.2017.8244667	data mining;systems engineering;architecture;computer science;internet of things;software	EDA	-44.45107970347873	40.944027115071314	31806
51f30ef98fceba24d4d0bf979e7f79e97748b377	performance and programming effort trade-offs of android persistence frameworks		Abstract A fundamental building block of a mobile application is the ability to persist program data between different invocations. Referred to as persistence , this functionality is commonly implemented by means of persistence frameworks. Without a clear understanding of the energy consumption, execution time, and programming effort of popular Android persistence frameworks, mobile developers lack guidelines for selecting frameworks for their applications. To bridge this knowledge gap, we report on the results of a systematic study of the performance and programming effort trade-offs of eight Android persistence frameworks, and provide practical recommendations for mobile application developers.	android;persistence (computer science)	Zheng Jason Song;Jing Pu;Junjie Cheng;Eli Tilevich	2018	Journal of Systems and Software	10.1016/j.jss.2018.08.038	computer science;systems engineering;android (operating system);persistence (computer science)	SE	-51.6455178068601	40.5851296305549	31837
47900c71e6a9161baceafed117963db32c8cfefa	components, middleware and web services	web based applications;web service;component middleware;software component;middleware	Web services are a logical evolution of software components and middleware. Based on a comparison of Web services with middleware and components we come up with the statement that Web services are components and middleware in one. However, for the Web services to live up to their potential as an integrating technology for mission critical Webbased applications, it is necessary to provide viable coordination and transaction capabilities. We introduce the notion of hyperware for the framework that will play the same role for Web services the middleware plays for components.	component-based software engineering;middleware;mission critical;ultracade technologies;web service;world wide web	Dimka Karastoyanova;Alejandro P. Buchmann	2003			web service;web application security;middleware;web development;web modeling;data web;web api;operating system;service-oriented architecture;web navigation;middleware;database;web 2.0;world wide web;application server	SE	-35.918904232653716	47.85744459109695	31904
a47e23f5fc8f87d5182fa5c1b91fb0429a13b5cd	data oblivious isa extensions for side channel-resistant and high performance computing		Blocking microarchitectural (digital) side channels is one of the most pressing challenges in hardware security today. Recently, there has been a surge of effort that attempts to block these leakages by writing programs data obliviously. In this model, programs are written to avoid placing sensitive data-dependent pressure on shared resources. Despite recent efforts, however, running data oblivious programs on modern machines today is insecure and low performance. First, writing programs obliviously assumes certain instructions in today’s ISAs will not leak privacy, whereas today’s ISAs and hardware provide no such guarantees. Second, writing programs to avoid data-dependent behavior is inherently high performance overhead. This paper tackles both the security and performance aspects of this problem by proposing a Data Oblivious ISA extension. On the security side, we present ISA design principles to block microarchitectural side channels, and embody these ideas in a concrete ISA capable of safely executing existing data oblivious programs. On the performance side, we design our ISA with support for efficient memory oblivious computation, and with safety features that allow modern hardware optimizations, e.g., out-of-order speculative execution, to remain enabled in the common case. We provide a complete hardware prototype of our ideas, built on top of the RISC-V out-of-order, speculative BOOM processor, and prove that the ISA can provide the advertised security through formal analysis of an abstract BOOM-style machine. We evaluate area overhead of hardware mechanisms needed to support our prototype, and provide performance experiments showing how the ISA speeds up a variety of existing data oblivious codes (including “constant time” cryptography and memory oblivious data structures), in addition to improving their security and portability.	cache-oblivious algorithm;code;computation;cryptography;data dependency;data structure;experiment;industry standard architecture;microarchitecture;microprocessor;overhead (computing);prototype;risc-v;software portability;speculative execution;time complexity	Jiyong Yu;V M;Mohamad Hajj;Christopher W. Fletcher	2018	IACR Cryptology ePrint Archive			Arch	-54.36962742623873	56.3881312639924	31938
ec8d7187df8b9066ec6cd55913f6a24694d7a9b6	ada 95 as implementation vehicle for formal specifications	formal specification;ada 95;system requirements;formal specifications;vehicles formal specifications real time systems control systems mathematics informatics mathematical model software systems control system synthesis telephony;transformation steps;implementation vehicle;real time systems ada 95 implementation vehicle formal specifications system requirements transformation steps;real time systems	A main concern in the initial phases of the development of a system to be built is capturing the system requirements and expressing them as an adequate model, either formal or informal. In subsequent phases of the development of the system this model is used as reference for transformation steps. In this paper we describe a reasonably successful experimental strategy for the implementation of real-time systems, starting from a formal specification, resulting in an Ada 95 implementation of the system and we evaluate the approach based upon practical experiences.	ada;formal specification	K. Brink;Jan van Katwijk;Hans Toetenel	1996		10.1109/RTCSA.1996.554966	real-time computing;formal methods;formal verification;computer science;operating system;formal specification	SE	-44.664869138462294	32.7144922783487	32043
6ff68a460cf0c999f6082c2cc2026250cf018ea0	enhancing interoperability in cross-platform enterprise mashups through data aggregation and extraction	mashups data models concrete xml protocols business;service composition;mashups;tool support;runtime environment;heterogeneous data;web service;indexing terms;data aggregation mashups web services runtime environment service composition;professional development;programming model;business data processing;data aggregation;heterogeneous data type handling interoperability enhancement cross platform enterprise mashup data aggregation data extraction service domains complex service composition web services;web services;web services business data processing data handling open systems;data handling;open systems	Enterprise Mashups usually comprise services from different service domains that often differ vastly from each other regarding their protocols, data or programming models. This fact poses a challenge especially for non-professional developers who want to quickly create a complex service composition because they are required to know the technical details of the involved service domains in order to use them in their composite application. In this paper we describe an extensible framework that allows for enhanced interoperability of arbitrary service types by introducing a unified way to interact with disparate Web services and an intuitive model for handling heterogeneous data types within a composition. Our framework forms the basis for building extensive tool support for end-users that allows easy creation of Mashups with diversified services, collaboration with other users and distributed execution of a service composition.	algorithm;composite application;control flow;correctness (computer science);data aggregation;dataflow;entity;executable;graphical user interface;guard (computer science);hoc (programming language);interoperability;language binding;mashup (web application hybrid);model checking;news aggregator;process modeling;prototype;proxy server;serializability;service composability principle;service-oriented architecture;type safety;usability;web application description language;web services description language;web service	Max Tritschler;Robert Kleinfeld;Stephan Steglich	2011	2011 IEEE International Conference on High Performance Computing and Communications	10.1109/HPCC.2011.98	web service;computer science;data mining;database;world wide web	DB	-45.11807614339098	43.60087603861214	32140
6f9abd5ed02fb14a3c05ac3fe4e6f64ce70e8610	towards a train-to-ground and intra-wagon communications solution capable of providing on trip customized digital services for passengers		The widespread adoption of Smartphone by citizens represents a great opportunity to integrate such nomadic devices inside vehicles in order to provide new on trip personalized digital services for passengers. In this paper a proposal of communication architecture to provide the ubiquitous connectivity needed to enhance the concept of smart train is presented and preliminarily tested. It combines an intra-wagon communication system based on nomadic devices connected through a Bluetooth Piconet Network with a highly innovative train-to-ground communication system.		Itziar Salaberria;Asier Perallos;Leyre Azpilicueta;Francisco Falcone;Roberto Carballedo;Ignacio Angulo;Pilar Elejoste;José Javier Astrain;Jesús E. Villadangos	2013		10.1007/978-3-319-03176-7_43	architecture;communications system;computer network;human–computer interaction;computer science;piconet;context-aware services	HCI	-39.12089488632307	52.61917901993816	32164
09baac12dfc225bf7a05081fa34b089e748651ae	autospearman: automatically mitigating correlated metrics for interpreting defect models		The interpretation of defect models heavily relies on software metrics that are used to construct them. However, such software metrics are often correlated in defect models. Prior work often uses feature selection techniques to remove correlated metrics in order to improve the performance of defect models. Yet, the interpretation of defect models may be misleading if feature selection techniques produce subsets of inconsistent and correlated metrics. In this paper, we investigate the consistency and correlation of the subsets of metrics that are produced by nine commonly-used feature selection techniques. Through a case study of 13 publicly-available defect datasets, we find that feature selection techniques produce inconsistent subsets of metrics and do not mitigate correlated metrics, suggesting that feature selection techniques should not be used and correlation analyses must be applied when the goal is model interpretation. Since correlation analyses often involve manual selection of metrics by a domain expert, we introduce AutoSpearman, an automated metric selection approach based on correlation analyses. Our evaluation indicates that AutoSpearman yields the highest consistency of subsets of metrics among training samples and mitigates correlated metrics, while impacting model performance by 1-2%pts. Thus, to automatically mitigate correlated metrics when interpreting defect models, we recommend future studies use AutoSpearman in lieu of commonly-used feature selection techniques.	dimensionality reduction;enlightenment;feature selection;futures studies;interpretation (logic);microelectronics and computer technology corporation;open-source software;r language;software bug;software engineering;software metric;software quality;subject-matter expert;supercomputer;the australian	Jirayus Jiarpakdee;Chakkrit Tantithamthavorn;Christoph Treude	2018	CoRR		computer science;subject-matter expert;software analytics;theoretical computer science;data mining;software metric;feature extraction;correlation;feature selection	SE	-62.7950317510417	33.60937937118367	32351
ce6351af13c9ca447925145150b4f018e0d8f895	using a capability-oriented methodology to build your cloud ecosystem	nist;standards;cloud ecosystem;security of data cloud computing;cloud;cloud ecosystem cloud cloud computing cloud architecture standards security national institute of standards and technology nist;cloud architecture;cloud computing security nist ecosystems computer architecture indexes;cloud consumers capability oriented methodology cloud ecosystem cloud based solution information systems national institute of standards and technology nist cloud computing security reference architecture risk based approach cloud actor security component;security;cloud computing;national institute of standards and technology	Organizations often struggle to capture the necessary functional capabilities for each cloud-based solution adopted for their information systems. Identifying, defining, selecting, and prioritizing these functional capabilities and the security components that implement and enforce them is surprisingly challenging. This article explains recent developments by the National Institute of Standards and Technology (NIST) in addressing these challenges. The article focuses on the capability-oriented methodology for orchestrating a secure cloud ecosystem proposed as part of the NIST Cloud Computing Security Reference Architecture. The methodology recognizes that risk can vary for cloud actors within a single ecosystem, so it takes a risk-based approach to functional capabilities. The result is an assessment of which cloud actor is responsible for implementing each security component and how implementation should be prioritized. Cloud actors, especially cloud consumers, that follow the methodology can more easily make well-informed decisions regarding their cloud ecosystems.	actor model;cloud computing security;ecosystem;information system;reference architecture	Michaela Iorga;Karen Scarfone	2016	IEEE Cloud Computing	10.1109/MCC.2016.38	cloud computing security;cloud computing;computer science;information security;operating system;cloud testing;internet privacy;world wide web;computer security	Metrics	-49.388104821891815	57.26135720460944	32419
a13688dd53c18604662b3c2aa6cd8c650b46d54d	on attribute-based usage control policy ratification for cooperative computing context		In an open information systems paradigm, real-time context -awareness is vital for the success of cooperation, therefore dynamic s ecurity attributes of partners should considered in coalition for avoiding sec urity conflicts. Furthermore, the cross-boundary asset sharing activities and risks associated to loss of governance call for a continuous regulation of par tners’ behavior, paying attention to the resource sharing and consuming acti vities. This paper describes an attribute-based usage control policy shce me compline to this needs. A concise syntax with EBNF is used to summarize th e base policy model. The semantics of negotiation process is disam biguated with abductive constraint logic programming (ACLP) and Event Ca lculus (EC). Then we propose a policy ratification method based on a policy aggregation algebra that elaborate the request space and policy rul e relation. This method ensures that, when policies are aggregated due to res ourc sharing and merging activities, the resulting policy correctly int erprets the original security goals of the providers’ policies.	abductive reasoning;constraint logic programming;context awareness;information system;open system (computing);programming paradigm;real-time transcription	Ziyi Su;Frédérique Biennier	2013	CoRR		knowledge management;data mining;network security policy;computer security	AI	-49.031756162048346	52.334025203967336	32431
99c7d06fc2789e938e586fb923a0e3c23bdced61	user-centric and intelligent service composition in ubiquitous computing environments	service composition;mobile device;service oriented computing;service robot;ubiquitous computing;urban computing;ubiquitous computing environment	The advancement of service-oriented computing and mobile device technologies gives us new challenges to provide intelligent services in ubiquitous computing (ubicomp) environments. User-centricity and dynamism support are the most essential requirements to meet those challenges. In this talk, I will introduce a user-centric and intelligent service composition framework that allows users to create their personalized ubicomp applications that utilize service resources in a highly dynamic ubicomp environments. The main features of our framework include: (1) task-oriented and spontaneous service composition; (2) dynamic service monitoring and reconfiguration; and (3) pervasive service retrieval and management. I will also explain our experiences of applying this framework to urban computing applications and intelligent service robots.	ubiquitous computing	In-Young Ko	2009		10.1007/978-3-642-04595-0_46	context-aware pervasive systems;human–computer interaction;computer science;distributed computing;utility computing;ubiquitous robot;data as a service;world wide web;ubiquitous computing	HCI	-40.83091218676675	45.457224667378206	32444
4086d0912d3b4373d80d5b76e863e7ea80766b3e	using aspects to design a secure system	formal specification;aspect oriented modeling approach aspects complex systems data security uml object oriented programming system availability timeliness;object oriented programming;formal specification security of data object oriented programming specification languages;complex system;specification languages;secure system;aspect oriented;weaving information security unified modeling language availability software systems internet programming software design pattern analysis interference;security of data	Developers of complex systems have to address concerns such as security, availability of services, and timeliness that often are non-orthogonal to traditional design structures, that is, the concerns cross-cut traditional design units. We illustrate how an aspect-oriented approach to modeling allows developers to encapsulate such design concerns so that they can be woven into a design in a systematic and consistent manner. The paper focuses on the use of aspects for modeling and weaving in security concerns.	aspect-oriented programming;aspect-oriented software development;complex systems;prototype;source-to-source compiler	Geri Georg;Indrakshi Ray;Robert B. France	2002	Eighth IEEE International Conference on Engineering of Complex Computer Systems, 2002. Proceedings.	10.1109/ICECCS.2002.1181504	complex systems;aspect-oriented programming;computer science;systems engineering;software engineering;formal specification;programming language;object-oriented programming	Embedded	-47.16162712777718	33.44976420594008	32479
08e851684493787f978afeb5345d84bdb6cc2fba	putting fixed priority scheduling theory into engineering practice for safety critical applications	job shop scheduling aerospace electronics software safety dynamic scheduling timing processor scheduling application software aircraft software standards certification;aircraft computers safety critical software scheduling certification aerospace computing software standards timing real time systems;evidence presentation fixed priority scheduling engineering practice industrial safety critical hard real time systems class a systems civil aircraft software standard do178b evidence gathering technical benefits timing requirements certification authorities;certification;application software;job shop scheduling;processor scheduling;timing requirements;technical benefits;civil aircraft software standard do178b;priority scheduling;hard real time system;aerospace computing;software safety;scheduling;certification authorities;safety critical software;aerospace electronics;safety critical system;evidence presentation;evidence gathering;aircraft computers;software standards;fixed priority scheduling;class a systems;engineering practice;industrial safety critical hard real time systems;aircraft;dynamic scheduling;real time systems;timing	This paper describes the approach proposed by the York University Technology Centre (YUTC) for introducing xed priority scheduling into industrial safety critical hard real-time systems. The work has been performed within the context of a class A (safety-critical) system as deened by civil aircraft software standard DO178B 1]. Traditionally, class A systems have been scheduled by a cyclic executive. However, many such systems can be redesigned using a xed priority sched-uler. This saves time and money, with no signiicant increase in risk. Also, signiicant technical beneets are apparent. This paper describes the timing requirements of the system, provides a potential scheduling approach (including appropriate timing analysis), and outlines an approach for gathering the necessary evidence for presentation to certiication authorities.	cyclic executive;do-178b;dynamic priority scheduling;fixed-priority pre-emptive scheduling;moral hazard;real-time clock;real-time computing;requirement;rework (electronics);scheduling (computing);static timing analysis	Neil C. Audsley;Iain Bate;Alan Burns	1996		10.1109/RTTAS.1996.509517	embedded system;job shop scheduling;application software;real-time computing;simulation;dynamic priority scheduling;computer science;operating system;certification;scheduling	Embedded	-39.0987890524222	34.784818588381015	32509
f71f311bc879cfaa80734b052e5621864e2e18a4	propagation of incremental changes to performance model due to soa design pattern application	model change;software performance;soa pattern;lqn;change propagation;service based systems	Design patterns for Service Oriented Architecture (SOA) provide solutions to architectural, design and implementation problems, involving software models in different layers of a SOA design. For performance analysis, a performance model can be generated from the SOA design and used to predict its performance. The impact of the design patterns is also reflected in the performance model. It is helpful to be able to trace the causality from the design pattern to its predicted performance impact. This paper describes a technique for automatically refactoring a SOA design model by applying a design pattern and for propagating the incremental changes to its LQN performance model. A SOA design model is expressed in UML extended with two standard profiles: SoaML for expressing SOA solutions and MARTE for performance annotations. The SOA design pattern is specified using a Role Based Modeling Language (RBML) and their application is automated using QVT-O. Automated incremental transformations are explored and evaluated for effectiveness on a case study example.	causality;code refactoring;modeling and analysis of real time and embedded systems;profiling (computer programming);qvt;service-oriented architecture;soaml;software design pattern;software propagation;unified modeling language	Nariman Mani;Dorina C. Petriu;C. Murray Woodside	2013		10.1145/2479871.2479887	real-time computing;simulation;software performance testing;computer science;systems engineering;engineering;oasis soa reference model	SE	-43.469987650652726	33.84315721244189	32525
b28c1d25771d590fa7d205a313194b3d693acc3e	regulatory requirements traceability and analysis using semi-formal specifications	requirements specification;legal requirements;traceability;domain specific languages	Information systems are increasingly distributed and pervasive, enabling organizations to deliver remote services and share personal information, worldwide. However, developers face significant challenges in managing the many laws that govern their systems in this multi-jurisdictional environment. In this paper, we report on a computational requirements document expressible using a legal requirements specification language (LRSL). The purpose is to make legal requirements open and available to policy makers, business analysts and software developers, alike. We show how requirements engineers can codify policy and law using the LRSL and design, debug, analyze, trace, and visualize relationships among regulatory requirements. The LRSL provides new constructs for expressing distributed constraints, making regulatory specification patterns visually salient, and enabling metrics to quantitatively measure different styles for writing legal and policy documents. We discovered and validated the LRSL using thirteen U.S. state data breach notification laws.	cyber-security regulation;data breach;information system;personally identifiable information;pervasive informatics;requirement;requirements traceability;security breach notification laws;semiconductor industry;software developer;software requirements specification;specification language	Travis D. Breaux;David G. Gordon	2013		10.1007/978-3-642-37422-7_11	requirements analysis;software requirements specification;traceability;requirements management;business requirements;computer science;systems engineering;domain-specific language;engineering;requirement;software engineering;system requirements specification;data mining;traceability matrix;requirements traceability	SE	-51.48165199513218	50.196073897952004	32598
d57ca30273f4eb7fa149cd1291a6b4ede6910eca	private virtual infrastructure for cloud computing	information owner;pvi datacenter;cloud fabric;locator bot;cloud computing;security property;service provider;security model;cloud locator bot;cloud security;private virtual infrastructure	Cloud computing places an organization’s sensitive data in the control of a third party, introducing a significant level of risk on the privacy and security of the data. We propose a new management and security model for cloud computing called the Private Virtual Infrastructure (PVI) that shares the responsibility of security in cloud computing between the service provider and client, decreasing the risk exposure to both. The PVI datacenter is under control of the information owner while the cloud fabric is under control of the service provider. A cloud Locator Bot pre-measures the cloud for security properties, securely provisions the datacenter in the cloud, and provides situational awareness through continuous monitoring of the cloud security. PVI and Locator Bot provide the tools that organizations require to maintain control of their information in the cloud and realize the benefits of cloud computing.	cloud computing security;data center;fink;information assurance;online locator service;poor posture;privacy;programming paradigm;security controls;synergy;threat (computer);total cost of ownership;virtual private network	F. John Krautheim	2009			cloud computing security;cloud computing;cloud testing;internet privacy;world wide web;computer security	Security	-49.53047137681719	58.01259878860096	32642
7d6036d9818ef931106555114d32cfbb2d9e9521	attack-resilient compliance monitoring for large distributed infrastructure systems	security of data distributed processing;sensors;airports;distributed processing;performance evaluation attack resilient compliance monitoring large distributed infrastructure systems monitoring system security enterprise networks critical infrastructure systems security attack detection distributed architecture distributed servers complex security policy evaluation;computer architecture;servers;monitoring;monitoring servers security aircraft computer architecture sensors airports;security;security of data;aircraft	The security of monitoring systems is critical for maintaining an accurate view of the state of infrastructure systems such as enterprise networks and critical infrastructure systems. A malicious user that controls a monitoring system has the ability of delaying the detection of security attacks and sabotages, and can acquire information about the infrastructure that can enable additional attacks. In this paper we present a distributed architecture that increases the resilient of monitoring systems to attacks against their availability, integrity, and confidentiality. Our approach is based on distributing the knowledge of the state of the infrastructure to a large number of non-dedicated servers, so that the compromise of any limited number of hosts does not cause a compromise of the entire monitoring system. We present an algorithm able to integrate information across the distributed servers to evaluate complex security policies. We analyze the security properties of our approach, and we experimentally evaluate the performance and the resilience of our architecture. We show that, compared to current solutions, our solution increases the resilience of a monitoring system while reducing the load on each monitoring machine.	algorithm;authorization;causal filter;compiler;confidentiality;dedicated hosting service;distributed computing;doraemon;experiment;mathematical optimization;security hacker;server (computing);simulation;systems architecture;tree (data structure)	Mirko Montanari;Roy H. Campbell	2011	2011 5th International Conference on Network and System Security	10.1109/ICNSS.2011.6060000	computer security model;cloud computing security;countermeasure;security information and event management;covert channel;asset;computer science;sensor;information security;security service;distributed computing;distributed system security architecture;security testing;computer security;enterprise information security architecture;server;computer network	Security	-53.19903858870959	57.02764575642664	32710
7ecd5f96a54002a16481b26e5585a03fbcc362cc	a client/intercept based system for optimized wireless access to web services	optimized wireless access;wireless access;ibm webexpress design;web services application software computer architecture service oriented architecture simple object access protocol wireless communication costs mobile communication telecommunication computing mobile computing;ibm webexpress design client intercept based system optimized wireless access web services mobile internet mobile e commerce verbose protocol ws aware software http soap stream;wireless devices;mobile e commerce;e commerce;client intercept based system;client server systems;web service;internet;http soap stream;web services;mobile communication;access protocols;ws aware software;mobile computing access protocols client server systems internet mobile communication;mobile computing;mobile internet;verbose protocol	"""In this paper we discuss the introduction of Web services in the wireless/mobile domain. The use of Web services is gradually expanding into the mobile Internet area where the population of users is rapidly increasing. Web services offer standardized ways of creating, publishing, searching and invoking services and provide a very important platform for the development of mobile e-commerce. We identify problems related to the use of the verbose protocols of Web services (i.e., SOAP/HTTP) over the """"expensive"""" wireless medium. Our architecture assumes the existence of WS-aware software (client or provider) in the wireless device. We try to optimize the HTTP/SOAP stream exchanged over the wireless medium. Our effort is largely based on the IBM WebExpress design. Our measurements indicate substantial benefits for the users"""	client (computing);e-commerce;experiment;hypertext transfer protocol;ibm personal computer;internet;java;mathematical optimization;soap;visual intercept;web service	Irene Kilanioti;Georgia Sotiropoulou;Stathes Hadjiefthymiades	2005	16th International Workshop on Database and Expert Systems Applications (DEXA'05)	10.1109/DEXA.2005.7	web service;web development;mobile web;computer science;ws-policy;ws-addressing;database;services computing;internet privacy;ws-i basic profile;mobile computing;law;world wide web;computer network	Mobile	-37.49278366771565	51.892391582313486	32763
c7e0e7b2382f3c0be21745e313e4b904e52771c0	computer safety, reliability, and security : 26th international conference, safecomp 2007, nuremberg, germany, september 18-21, 2007 : proceedings	computer safety;international conference	Safety Cases.- Establishing Evidence for Safety Cases in Automotive Systems - A Case Study.- Goal-Based Safety Cases for Medical Devices: Opportunities and Challenges.- Impact of Security on Safety.- Electronic Distribution of Airplane Software and the Impact of Information Security on Airplane Safety.- Future Perspectives: The Car and Its IP-Address - A Potential Safety and Security Risk Assessment.- Modelling Interdependencies Between the Electricity and Information Infrastructures.- Poster Session 1.- Handling Malicious Code on Control Systems.- Management of Groups and Group Keys in Multi-level Security Environments.- Application of the XTT Rule-Based Model for Formal Design and Verification of Internet Security Systems.- RAMSS Analysis for a Co-operative Integrated Traffic Management System.- Combining Static/Dynamic Fault Trees and Event Trees Using Bayesian Networks.- Component Fault Tree Analysis Resolves Complexity: Dependability Confirmation for a Railway Brake System.- Compositional Temporal Fault Tree Analysis.- Representing Parameterised Fault Trees Using Bayesian Networks.- Human Error Analysis Based on a Semantically Defined Cognitive Pilot Model.- Safety Analysis of Safety-Critical Software for Nuclear Digital Protection System.- Specification of a Software Common Cause Analysis Method.- Combining Bayesian Belief Networks and the Goal Structuring Notation to Support Architectural Reasoning About Safety.- Application of Interactive Cause and Effect Diagrams to Safety-Related PES in Industrial Automation.- Survival by Deception.- How to Secure Bluetooth-Based Pico Networks.- Learning from Your Elders: A Shortcut to Information Security Management Success.- Intrusion Attack Tactics for the Model Checking of e-Commerce Security Guarantees.- Poster Session 2.- Safety Process Improvement with POSE and Alloy.- Defense-in-Depth and Diverse Qualification of Safety-Critical Software.- Experimental Evaluation of the DECOS Fault-Tolerant Communication Layer.- Achieving Highly Reliable Embedded Software: An Empirical Evaluation of Different Approaches.- Modeling, Analysis and Testing of Safety Issues - An Event-Based Approach and Case Study.- A Concept for a Safe Realization of a State Machine in Embedded Automotive Applications.- Safety Demonstration and Software Development.- Improving Test Coverage for UML State Machines Using Transition Instrumentation.- Verification of Distributed Applications.- Analysis of Combinations of CRC in Industrial Communication.- A Comparison of Partitioning Operating Systems for Integrated Systems.- Software Encoded Processing: Building Dependable Systems with Commodity Hardware.- Reliability Modeling for the Advanced Electric Power Grid.- Case Study on Bayesian Reliability Estimation of Software Design of Motor Protection Relay.- A Reliability Evaluation of a Group Membership Protocol.- Poster Session 3.- Bounds on the Reliability of Fault-Tolerant Software Built by Forcing Diversity.- A Tool for Network Reliability Analysis.- DFT and DRBD in Computing Systems Dependability Analysis.- Development of Model Based Tools to Support the Design of Railway Control Applications.- Formal Specification and Analysis of AFDX Redundancy Management Algorithms.- Modeling and Automatic Failure Analysis of Safety-Critical Systems Using Extended Safecharts.- Using Deductive Cause-Consequence Analysis (DCCA) with SCADE.- Experimental Assessment of Astree on Safety-Critical Avionics Software.- Detection of Runtime Errors in MISRA C Programs: A Deductive Approach.- A Taxonomy for Modelling Safety Related Architectures in Compliance with Functional Safety Requirements.- Controller Architecture for Safe Cognitive Technical Systems.- Improved Availability and Reliability Using Re-configuration Algorithm for Task or Process in a Flight Critical Software.		Francesca Saglietti;Norbert Oster	2007		10.1007/978-3-540-75101-4	software security assurance;reliability engineering;security service;security analysis;computer security	Robotics	-56.111075172217305	46.35943353047496	32768
2d8c5f0065e0b71ada5de45d8701ebcb5d9c74b4	modeling and provisioning iot cloud systems for testing uncertainties		Modern Cyber-Physical Systems (CPS) and Internet of Things (IoT) systems consist of both loosely and tightly interactions among various resources in IoT networks, edge servers and cloud data centers. These elements are being built atop virtualization layers and deployed in both edge and cloud infrastructures. They also deal with a lot of data through the interconnection of different types of networks and services. Therefore, several new types of uncertainties are emerging, such as data, actuation, and elasticity uncertainties. This triggers several challenges for testing uncertainty in such systems. However, there is a lack of novel ways to model and prepare the right infrastructural elements covering requirements for testing emerging uncertainties. In this paper, first we present techniques for modeling CPS/IoT Systems and their uncertainties to be tested. Second, we introduce techniques for determining and generating deployment configuration for testing in different IoT and cloud infrastructures. We illustrate our work with a real-world use case for monitoring and analysis of Base Transceiver Stations.	algorithm;cloud computing;cyber-physical system;data center;elasticity (data store);high- and low-level;interaction;interconnection;internet of things;prototype;provisioning;requirement;software deployment;system under test;transceiver	Hong Linh Truong;Luca Berardinelli;Ivan Pavkovic;Georgiana Copil	2017		10.1145/3144457.3144490	software deployment;computer network;virtualization;computer science;interconnection;cyber-physical system;transceiver;distributed computing;provisioning;cloud computing;server	OS	-44.79044751395641	47.03325210642948	32875
013d702dcf012d413c0c41e5b696cd924dc2dca3	jigsaw: efficient, low-effort mashup isolation	object oriented programming;message passing	A web application often includes content from a variety of origins. Securing such a mashup application is challenging because origins often distrust each other and wish to expose narrow interfaces to their private code and data. Jigsaw is a new framework for isolating these mashup components. Jigsaw is an extension of the JavaScript language that can be run inside standard browsers using a Jigsaw-to-JavaScript compiler. Unlike prior isolation schemes that require developers to specify complex, error-prone policies, Jigsaw leverages the well-understood public/private keywords from traditional object-oriented languages, making it easy for a domain to tag internal data as externally visible. Jigsaw provides strong iframe-like isolation, but unlike previous approaches that use actual iframes as isolation containers, Jigsaw allows mutually distrusting code to run inside the same frame; this allows scripts to share state using synchronous method calls instead of asynchronous message passing. Jigsaw also introduces a novel encapsulation mechanism called surrogates. Surrogates allow domains to safely exchange objects by reference instead of by value. This improves sharing efficiency by eliminating cross-origin marshaling overhead.	client-side;cognitive dimensions of notations;compiler;distrust;encapsulation (networking);html element;java;javascript;mashup (web application hybrid);message passing;overhead (computing);programmer;surrogates;web application;whole earth 'lectronic link	James Mickens;Matthew Finifter	2012			computer science;distributed computing;internet privacy;world wide web	PL	-55.00765155835066	58.88775908146404	32922
ffc60d6ee3567bf6123eac2bf0e587aaa076de5f	the coabs grid	secure socket layer;network protocol;technology development;agent based;heterogeneous agents;application program interface;mobile agent system;remote method invocation;agent communication language;legacy system	The CoABS Grid integrates heterogeneous agent-based systems, object -based applications, and legacy systems. The CoABS Grid does not mandate the network protocol used to deliver messages to an agent or the agent communication language. A CoABS Grid agent registers a proxy object supporting a well-known message-delivery interface in a lookup service. Default proxy implementations are provided that use Java Remote Method Invocation (RMI) and RMI over Secure Socket Layer as the network protocol. Lookup of agents based on agent type and agent capability advertisements is supported. The CoABS Grid provides a simple, agent-specific application programming interface, layered over the JiniTM Network Technology developed by Sun Microsystems. However, it also exposes the underlying JiniTM application programming interface for the advanced developer. The CoABS Grid provides support for mobile agent systems by allowing the creation of CoABS Grid agents that may be transported across the network, resuming operation in a new location.	aaron;agent communications language;agent-based model;application programming interface;brian silverman;communications protocol;dylan;java remote method invocation;kaos;legacy system;lookup table;mobile agent;multi-agent system;proxy pattern;subhash suri;transport layer security	Martha L. Kahn;Cynthia Della Torre Cicalese	2002		10.1007/978-3-540-45173-0_9	embedded system;real-time computing;computer science;distributed computing	HPC	-34.66181397327751	45.51092705794703	32942
0f236230f678776a98e3f75ff00bf1de681c245a	graphical models for security		Protection of enterprise systems from cyber attacks is a challenge. Vulnerabilities are regularly discovered in software systems that are exploited to launch cyber attacks. Security analysts need objective metrics to manage the security risk of an enterprise systems. In this talk, we give an overview of our research on security metrics and challenges for security risk analysis of enterprise systems. A standard model for security metrics will enable us to answer questions such as “are we more secure than yesterday” or “how does the security of one system compare with another?” We present a methodology for security risk analysis that is based on the model of attack graphs and the common vulnerability scoring system (CVSS).	buckingham potential;concept map;data protection directive;diagram;directive (programming);emmet (software);enterprise system;general data protection regulation;graphical model;human–computer interaction (security);lemke's algorithm;many-to-many (data model);michael j. fischer;microsoft outlook for mac;mind map;parsing;personally identifiable information;privacy;proposed directive on the patentability of computer-implemented inventions;reductionism;requirement;scalability;semantics (computer science);semiconductor industry;software system;usability;visual instruction set	Peng Liu Sjouke Mauw Ketil Stølen	2017		10.1007/978-3-319-74860-3	data mining;computer science;graphical model	Security	-61.92013708121473	59.04325026319645	32971
249b23cafe084369bd2a5669f05e320559b79b59	a survey of user-centric identity management technologies	identity management systems technology management pervasive computing privacy usability authentication ambient intelligence cultural differences humans mobile computing;ambient intelligence;identity mechanisms;user centric identity management technologies;identity management;mobile computing;mobile computing user centric identity management technologies identity mechanisms	Computing anytime anywhere is more and more the rule. In this ambient intelligent world, the choice of identity mechanisms will have a large impact on its social, cultural, business and political aspects. Privacy is a human need and the whole of society would suffer from its demise. Moreover, people have a hectic life and cannot spend their time administering their digital identities. In this paper, we survey how the requirements for identity management have evolved, and their associated technologies, with emphasis on the federated approaches and user-centricity.	anytime algorithm;identity management;privacy;requirement	Tewfiq El Maliki;Jean-Marc Seigneur	2007	The International Conference on Emerging Security Information, Systems, and Technologies (SECUREWARE 2007)	10.1109/SECUREWARE.2007.4385303	ambient intelligence;computer science;operating system;internet privacy;mobile computing;world wide web;computer security;identity management	EDA	-40.53338323915896	52.94977583739841	32979
b7f2080d5a0485d9ecfea8d79c7c25aeb1107df7	microcosm++ : the development of a loosely coupled object based architecture for open hypermedia systems			hypermedia;loose coupling;object-based language	Nechemia Daniel Beitner	1995				SE	-34.47496438073057	43.313239430812814	33016
4ba7a8b98808aa88ae0604ce7156f2d9a3e70e64	blinky blocks: a physical ensemble programming platform	programming environments;programming environment;distributed programs;development tool;tangible ui;system design;robots;hardware design;development tools	A major impediment to understanding programmable matter is the lack of an existing system with sufficiently many modules of sufficient capabilities. In this paper we describe the requirements of physically distributed ensembles and discuss the use of the distributed programming language Meld to program ensembles of these units. We demonstrate a new system designed to meet these requirements called Blinky Blocks and discuss the hardware design we used to create 100 of these modules.	distributed computing;led art;meld (software);programmable matter;programming language;requirement	Brian T. Kirby;Michael P. Ashley-Rollman;Seth Copen Goldstein	2011		10.1145/1979742.1979712	robot;real-time computing;simulation;human–computer interaction;computer science;artificial intelligence;systems design	HCI	-38.43386012269351	36.82615928275076	33026
d4243292303f3d0ba819fdd16e54bcd3b2706df9	a protocol for user awareness on the world wide web	client server;awareness;world wide web;cscw;protocol	This paper presents the development of an open awareness protocol for the world wide web. The protocol is intended to convey the presence of users to other web users. To encourage uptake of the systems the protocol adheres to the principles that made the world wide web a success, simplicity and openness. An initial version of the protocol is presented along with servers realising the protocol. The paper concludes by showing how the awareness information can support both 2D and 3D presentations of the World Wide Web.	openness;world wide web	Kevin Palfreyman;Tom Rodden	1996		10.1145/240080.240236	web service;sensor web;hypertext transfer protocol;protocol;awareness;web-based simulation;human–computer interaction;web accessibility initiative;web standards;computer science;web api;computer-supported cooperative work;web navigation;web page;database;internet privacy;web intelligence;web 2.0;world wide web;web server;client–server model	Web+IR	-36.70025280330927	48.572209972565	33050
af1a95d699968c4e73f4082cd7bbc71d80ac97d8	clustering analysis of function call sequence for regression test case reduction	cluster analysis;cost effectiveness;vector;sequence;test case reduction	Regression test case reduction aims at selecting a representative subset from the original test pool, while retaining the largest possible fault detection capability. Cluster analysis has been proposed and applied for selecting an e®ective test case subset in regression testing. It groups test cases into clusters based on the similarity of historical execution pro ̄les. In previous studies, historical execution pro ̄les are represented as binary or numeric function coverage vectors. The vector-based similarity approaches only consider which functions or statements are covered and the number of times they are executed. However, the vector-based approaches do not take the relations and sequential information between function calls into account. In this paper, we propose cluster analysis of function call sequences to attempt to improve the fault detection e®ectiveness of regression testing even further. A test is represented as a function call sequence that includes the relations and sequential information between function calls. The distance between function call sequences is measured not only by the Levenshtein distance but also the Euclidean distance. To assess the e®ectiveness of our approaches, we designed and conducted experimental studies on ̄ve subject programs. The experimental results indicate that our approaches are statistically superior to the approaches based on the similarity of vectors (i.e. binary vectors and numeric vectors), random and greedy function-coverage-based maximization test case reduction techniques in terms of fault detection e®ectiveness. With respective to the cost-e®ectiveness, cluster analysis of sequences measured using the Euclidean distance is more e®ective than using the Levenshtein distance.	cluster analysis;euclidean distance;expectation–maximization algorithm;experiment;fault detection and isolation;greedy algorithm;levenshtein distance;regression testing;sampling (signal processing);similarity learning;subroutine;test case;test suite	Rongcun Wang;Rubing Huang;Yansheng Lu;Binbin Qu	2014	International Journal of Software Engineering and Knowledge Engineering	10.1142/S0218194014500387	cost-effectiveness analysis;vector;computer science;machine learning;data mining;sequence;cluster analysis	SE	-60.76995742020537	35.676825745805665	33098
1919d9da3d01048df634e5a08d3e9bab6c61aabf	lighting control actuator design and development for a zigbee network with a web server mounted on raspberry pi	zigbee actuators control system synthesis file servers home automation lighting control microcontrollers public domain software;zigbee actuators batteries lighting control home automation lighting ports computers;light switch lighting control actuator design lighting control actuator development zigbee network web server raspberry pi zigbee protocol low cost components open source open hardware home automation system environment adaptive web application xbee modules worst case scenario	Competitive solutions to the growing demand for home automation will most likely include the use of low cost components, open source and open hardware. Using the ZigBee protocol, a low cost lighting control actuator was developed that was battery efficient and easy to use in a home automation system environment. Raspberry Pi was used to house a Web server, which provided the user an adaptive Web application to access the lighting actuator control. XBee modules were chosen for their small size, low energy consumption and the large amount of documentation that exists in regard to its use. Considering the worst-case scenario, the lighting control actuator was designed so if its battery runs out of power or the ZigBee network fails, it can work as a normal light switch.	documentation;environment variable;home automation;lighting control system;open-source hardware;open-source software;raspberry pi 3 model b (latest version);server (computing);web application;web server;worst-case scenario	Martin V. Urgiles;Paul E. Arpi;Diego P. Chacón-Troya	2015	2015 IEEE International Conference on Automation Science and Engineering (CASE)	10.1109/CoASE.2015.7294165	embedded system;home automation;real-time computing;computer hardware;engineering;digital addressable lighting interface;lighting control console	Robotics	-38.48395492707433	52.23585791296576	33100
3fc20ac7d0b522310a13f125c37425a425e04b63	ontologies in a service oriented computing environment	scientific application;mathematics;workflows grid service oriented computing;standards organizations;application software;service orientation;pervasive computing;distributed computing;ontologies service oriented architecture pervasive computing distributed computing grid computing computer science semantic web standards organizations mathematics application software;grid services;data mining;system on a chip;ontologies artificial intelligence;computer algebra system;grid;computer architecture;workflows;service oriented computing;ontologies artificial intelligence grid computing;web services;grid service;semantic web;ontologies;computer science;grid infrastructures;service oriented architecture;grid computing;grid infrastructures ontologies service oriented computing grid services	With Service Oriented Computing (SOC), services have a central rolein rapid development and deployment of loosely-coupled, massivelydistributed scientific applications with supporting services spanningfrom Computer Algebra Systems (CAS) to Grid services. Key featuresfrom service orientation can be easily integrated in a SOC-based development. The integration of SOA with Grid infrastructures and Grid Servicesis an open issue for current research. A service-oriented architecturefor the Grid (like OGSA) and its corresponding infrastructure, namelyOGSI, are fundamental in order to develop Service Orientation overa Grid infrastructure. The PEGAF project is developing a generic platformfor the modeling, implementation and execution of workflow-based applicationwith Grid orientation. In this paper we are presenting the generalarchitecture of the PEGAF platform, and an ontology-oriented viewas the core of a flexible architecture for the definition and executionof dynamic workflows.	database;high- and low-level;ontology (information science);open grid services architecture;semantic integration;service discovery;service-oriented architecture;service-oriented device architecture;software deployment;user interface	Teodor-Florin Fortis;Cristina Mindruta	2009	2009 Workshops at the Grid and Pervasive Computing Conference	10.1109/GPC.2009.19	semantic grid;computer science;operating system;service-oriented architecture;database;distributed computing;world wide web;ubiquitous computing;grid computing	HPC	-38.34302384583594	42.68899837580006	33140
0344e81ff9d93a914598ad573dba287abcc61530	a domain-specific language for web apis and services mashups	service composition;user interface;distributed programs;web service;programming model;service model;domain specific language	Distributed programming has shifted from private networks to the public Internet and from using private and controlled services to increasingly using publicly available heterogeneous Web services (e.g., REST, SOAP, RSS, and Atom). This move enables the creation of innovative end-user-oriented composed services with user interfaces. These services  mashups are typically point solutions to specific (specialized) problems; however, what is missing is a programming model that facilitates and accelerates creation and deployment of mashups of  diverse services. In this paper we describe a domain-specific language that unifies the most common service models and facilitates service composition and integration into end-user-oriented Web applications. We demonstrate our approach with an implementation that leverages the Ruby on Rails framework.	domain-specific language	E. Michael Maximilien;Hernán Wilkinson;Nirmit Desai;Stefan Tai	2007		10.1007/978-3-540-74974-5_2	web service;computer science;domain-specific language;service delivery framework;operating system;service-oriented modeling;ws-policy;service-oriented architecture;database;multimedia;programming paradigm;services computing;programming language;ws-i basic profile;user interface;law;world wide web;universal description discovery and integration	NLP	-45.003048340864524	43.668137472786285	33219
026c84df70942697ae850f9097c1676531a49821	secure and efficient application monitoring and replication		Memory corruption vulnerabilities remain a grave threat to systems software written in C/C++. Current best practices dictate compiling programs with exploit mitigations such as stack canaries, address space layout randomization, and control-flow integrity. However, adversaries quickly find ways to circumvent such mitigations, sometimes even before these mitigations are widely deployed. In this paper, we focus on an “orthogonal” defense that amplifies the effectiveness of traditional exploit mitigations. The key idea is to create multiple diversified replicas of a vulnerable program and then execute these replicas in lockstep on identical inputs while simultaneously monitoring their behavior. A malicious input that causes the diversified replicas to diverge in their behavior will be detected by the monitor; this allows discovery of previously unknown attacks such as zero-day exploits. So far, such multi-variant execution environments (MVEEs) have been held back by substantial runtime overheads. This paper presents a new design, ReMon, that is non-intrusive, secure, and highly efficient. Whereas previous schemes either monitor every system call or none at all, our system enforces cross-checking only for security critical system calls while supporting more relaxed monitoring policies for system calls that are not security critical. We achieve this by splitting the monitoring and replication logic into an in-process component and a cross-process component. Our evaluation shows that ReMon offers same level of security as conservative MVEEs and run realistic server benchmarks at near-native speeds.	address space layout randomization;best practice;c++;compiler;control flow;control-flow integrity;critical system;exploit (computer security);interaction;lockstep (computing);memory corruption;network switch;performance evaluation;run time (program lifecycle phase);server (computing);stack buffer overflow;system call	Stijn Volckaert;Bart Coppens;Alexios Voulimeneas;Andrei Homescu;Per Larsen;Bjorn De Sutter;Michael Franz	2016			real-time computing;operating system;distributed computing;computer security	OS	-55.61947705277028	56.85784815722827	33229
aa51d7541c46efcb4f005490a0ce9dcc3062940f	an open and secure terminal infrastructure for hosting personal services	distributed system;naming services;embedded systems internet naming services java distributed object management open systems security of data interactive terminals application program interfaces smart cards personal information systems;java naming and directory interface;user s mobility;architectures frameworks;type of service;embedded systems;distributed objects;internet;personal information systems;smart cards;interactive terminals;object oriented;accessing methods open secure terminal infrastructure personal service hosting internet distributed object technologies distributed system architectures location independent access remote objects java naming and directory interface jndi object oriented view naming services directory services federated services personal naming and directory service pnds embedded component smart card prototype object oriented java card open card framework standardized framework network based services;application program interfaces;distributed object management;lookup services access to distributed services;smart cards and terminals;open systems;security of data;naming directory services;java;directory service;java smart cards web and internet services prototypes	With the emergence of the Internet and distributed object technologies, naming and directory services have become key elements in distributed system architectures. A key benefit of naming services is to enable location-independent access to remote objects. In the Java environment, the Java Naming and Directory Interface (JNDI) is proposed as a common framework to provide an object-oriented view of naming and directory services, and to unify and federate different types of services. In this paper, we present a terminal infrastructure which relies on naming and directory services for accessing distributed objects. This infrastructure is based on the usage of a new component that we have named the Personal Naming and Directory Service (PNDS), which is embedded on a smart card. PNDS has been prototyped using a fully object-oriented Java card, and has been integrated as a new additional component in the JNDI framework. By leveraging the Open Card framework, the standardized framework and interface to access both the smart card reader and applications on the card, and the JNDI framework approach to naming and directory services, PNDS enables access to both smart card- and network-based services. The PNDS smart card provides the proposed terminal infrastructure with knowledge of a user's personal services, and with means for accessing and securing the access.		Alain Macaire	2000		10.1109/TOOLS.2000.848747	lightweight directory access protocol;java card;computer science;operating system;database;world wide web	NLP	-35.7965330135602	47.41521481189771	33346
bb0995f63bb62fd1f44af43dc0c85448162a32f9	model-based planning for state-related changes to infrastructure and software as a service instances in large data centers	databases;web application server;state based constraints it change planning service management application management ai planning;model based planning;load balancers;object oriented model;object oriented methods;web services computer centres constraint handling database management systems object oriented methods resource allocation;fast storing;application management;database management systems;resource allocation;3 tier applications;service management;image restoration;runtime;iaas components;state based constraints;large object oriented configuration management databases;computer centres;data center;saas business services;performance improvement;large data centers;object oriented;clouds;state constraints;web services;constraint handling;object oriented planning approach;planning;it change planning;software as a service;load balance;planning object oriented modeling clouds image restoration runtime databases data models;configuration management;cloud state related constraints crossing infrastructure;object oriented modeling;ai planning;large object oriented configuration management databases model based planning software as a service large data centers 3 tier applications cloud state related constraints crossing infrastructure web application server load balancers saas business services object oriented planning approach iaas components fast storing;large data;data models	To deliver 3-tier applications as a Service in the Cloud state-related constraints crossing Infrastructure- and Software as a Service boundaries need to be managed. By automating the lifecycle of applications like databases, load balancers, and web application servers rich SaaS business services can be provided in the Cloud. We propose an object oriented planning approach based on state constraints to plan for changes of SaaS and IaaS components in the Cloud. We evaluate techniques for fast storing and restoring of large object oriented Configuration Management Databases and show that enforcing constraints in a procedural instead of a declarative way offers huge performance improvements. The advantages of our approach lie within the tight integration of the planning algorithm with object oriented models frequently used for Configuration Management Databases. In addition to that, the algorithm scales to a large number of nodes and preserves its runtime even for large, heavily loaded data centers.	algorithm;application server;automated planning and scheduling;cloud computing;configuration management;data center;database;fastest;multitier architecture;scalability;software as a service;software deployment;virtual appliance;web application	Sebastian Hagen;Alfons Kemper	2010	2010 IEEE 3rd International Conference on Cloud Computing	10.1109/CLOUD.2010.14	automated planning and scheduling;computer science;load balancing;database;distributed computing	DB	-45.650915452560795	40.98800603039986	33349
d66e1b7713748d4b070b709ac09de4fd7f9eca1b	frameself: a generic context-aware autonomic framework for self-management of distributed systems	graph theory;context awareness;distributed processing;smart metering;eda;soa;ontologies artificial intelligence;ubiquitous computing distributed processing graph theory knowledge based systems ontologies artificial intelligence service oriented architecture;graph;logic gates ontologies computer architecture monitoring service oriented architecture sensors actuators;ubiquitous computing;m2m;service oriented architecture;autonomic computing;ontology;m2m smart metering use case frameself generic context aware autonomic framework self management distributed system distributed software generic autonomic architecture machine to machine network mobile machine multimodel representation ontologies graph multilevel knowledge base communication pattern module service oriented communication event driven communication;self management;knowledge based systems;soa autonomic computing self management context awareness ontology graph smart metering m2m eda	The increasing complexity for the management of current distributed software and system needs new solutions. Designing autonomic systems which are self-managing and context aware is a solution and also a challenge. This paper proposes the FRAME SELF framework, a generic autonomic architecture based on context awareness and decision models built. The proposed solution capabilities are illustrated on the self-configuration of machine to machine networks (M2M). M2M needs to connect thousands of various fix and mobile machines that are widely distributed and evolve frequently according to their profile and context changes. FRAME SELF uses multi models representation based on ontologies and graphs to describe the M2M concepts and relationship on a multi-level knowledge base. Two communication patterns modules based on service-oriented and event-driven communications are dynamically selected and configured in deployment plans. A M2M smart metering use case is experimented to illustrate the proposed approach.	autonomic computing;context awareness;control system;design pattern;distributed computing;electronic design automation;event-driven programming;graphml;home automation;knowledge base;layer (electronics);machine to machine;mathematical optimization;osgi;ontology (information science);self-management (computer science);service layer;service-orientation;service-oriented architecture;service-oriented device architecture;smart meter;software deployment;warez;web ontology language	Mahdi Ben Alaya;Thierry Monteil	2012	2012 IEEE 21st International Workshop on Enabling Technologies: Infrastructure for Collaborative Enterprises	10.1109/WETICE.2012.85	real-time computing;computer science;graph theory;theoretical computer science;service-oriented architecture;ontology;database;distributed computing;ubiquitous computing;autonomic computing	SE	-42.20389412791516	43.7938709519224	33399
e3c1cb2ee448883ac8154eea100cb8e74e3ec339	smalltalk and exploratory programming	smalltalk	Using Smalltalk-80, programmers can produce prototypes much faster than with C or Pascal. What techniques do Smalltalk-80 programmers use to produce these prototypes? What is special about Smalltalk-80 that enables them to uses these techniques? Can these techniques be used with conventional languages such as C? In an attempt to answer these questions we interviewed experienced Smalltalk programmers and asked how they approach programming in Smalltalk. Such introspective interviews that are conducted after completion of a project are known to be somewhat unreliable, but not enough is known to use any other methodology. What follows is a summary of the interviews, followed by an explanation of the results. Finally we discuss some of the weaknesses of Smalltalk and some possible solutions.	exploratory programming;pascal;programmer;smalltalk	David W. Sandberg	1988	SIGPLAN Notices	10.1145/51607.51614	computer science;programming language	HCI	-52.88890470247462	37.031552114519826	33402
dcbdb9a2c6477dde41affb1b9398ac8246b2991a	achieving consistency in mobile databases through localization in pro-motion	disconnected operations;data integrity;distributed transactions;mobile databases;mobile computer;client server systems;mobile computing transaction databases computer science batteries computer networks polynomials hazards mobile communication concurrency control protocols;global constraint;mobile database;mobile environment mobile database consistency localization pro motion transaction support mobile computing environment database management systems data consistency global constraints mobile hosts replicated data polynomial inequalities flexible infrastructure transaction processing;mobile environment;replicated databases mobile computing transaction processing data integrity client server systems;replicated data;transaction processing;mobile computing;data consistency;database management system;replicated databases;constraint maintenance	There is great need and potential for traditional transaction support in a mobile computing environment. However, owing to the inherent limitations of mobile computing, we need to augment the well-developed techniques of Database Management Systems with new approaches. In this paper, we focus on the challenge of assuring data consistency. Our approach of localization is to reformulate global constraints so as to enhance the autonomy of the mobile hosts. We show how this approach unifies techniques of maintaining replicated data with methods of enforcing poly nomial inequalities. We also discuss how localization can be implemented in PRO-MOTION, a flexible infrastructure for transaction processing in a mobile environment.	autonomy;cpu cache;management system;mobile computing;polynomial;replication (computing);semiconductor;transaction processing;whole earth 'lectronic link	Subhasish Mazumdar;Panos K. Chrysanthis	1999		10.1109/DEXA.1999.795149	mobile database;computer science;data mining;database;distributed computing;mobile computing	DB	-34.376161085217184	48.87567770852083	33477
1bcfd9fe639d68ec6b73564572265d2ec6184561	hybrid marte		MARTE (Modeling and Analysis of Real-Time and Embedded Systems) is a profile of UML (United Modeling Language). MARTE provides support for specification, design and verification of real-time and embedded systems. Even though MARTE time model offers a support to describe multiform clocks, it lacks the ability to model both discrete and continuous behaviors of a hybrid system. To address the problem of hybrid systems modeling, we propose Hybrid MARTE which is an extension to MARTE for hybrid system modeling and analysis. Compare to MARTE, in Hybrid MARTE, we can construct the logical time and chronometric time in a unified way. Besides, a systemic framework for the modeling the requirements and design of a hybrid system is provided. Hybrid MARTE also provides multiple views modeling, something like UML. Hybrid MARTE Class Diagram can be used for description in static view, Hybrid MARTE Sequence Diagram in interactive view and Hybrid MARTE Statechart in dynamic behavioral view. HybridMARTE is successfully used in modeling and analysis of the Train Position Determination of railway control systems.	class diagram;concurrency (computer science);control system;embedded system;hybrid system;model-driven architecture;modeling and analysis of real time and embedded systems;real-time clock;real-time computing;real-time transcription;requirement;sequence diagram;state diagram;systems modeling;unified modeling language	Lulu Yao;Weiping Li;Yan Zhang;Yuejun Wang	2015	2015 Asia-Pacific Software Engineering Conference (APSEC)	10.1109/APSEC.2015.46		Embedded	-40.18748281418292	32.78804240561816	33510
b3bcaf66914e1126578b8d39b57efd0b65303812	analysis of cybersecurity based on li-fi in green data storage environments		Industrial networking has many issues based on the type of industries, data storage, data centers, and cloud computing, etc. Green data storage improves the scientific, commercial and industrial profile of the networking. Future industries are looking for cybersecurity solution with the low-cost resources in which the energy serving is the main problem in the industrial networking. To improve these problems, green data storage will be the priority because data centers and cloud computing deals with the data storage. In this analysis, we have decided to use solar energy source and different light rays as methodologies include a prism and the Li-Fi techniques. In this approach, light rays sent through the prism which allows us to transmit the data with different frequencies. This approach provides green energy and maximum protection within the data center. As a result, we have illustrated that cloud services within the green data center in industrial networking will achieve better protection with the low-cost energy through this analysis. Finally, we have to conclude that Li-Fi enhances the use of green energy and protection which are advantages to current and future industrial networking.	cloud computing;computer data storage;computer security;cyber security standards;data center;hotspot (wi-fi);ray (optics)	Vijey Thayananthan;Omar A. Abdulkader;Kamal Jambi;Alwi M. Bamahdi	2017	2017 IEEE 4th International Conference on Cyber Security and Cloud Computing (CSCloud)	10.1109/CSCloud.2017.32	computer security;computer science;renewable energy;li-fi;data center;ray;cloud computing;industrial ethernet;solar energy;computer data storage	HPC	-44.18508645352174	52.24811813315755	33563
024097fcf05fe6fb6ce207f577defd44885fcf67	omnisphere: a personal communication environment	home appliances protocols wireless networks ip networks communication networks joining processes automatic control communication system control information retrieval context aware services;protocols;ambient networking;domestic appliances;personal communication networks;wireless network;protocols personal communication networks ubiquitous computing domestic appliances internet meta data;user preferences;future internet;col;internet;community networks;ambient networks;ubiquitous computing;ambient services;meta data;service discovery;data flow;ambient networking omnisphere personal communication environment ubiquitous devices wireless networks internet appliances communication networks wireless appliances ambient services typed data flows channel elements control elements metadata component service discovery network infrastructure user id appliance id user preferences device capabilities discovery protocols slp jini upnp;wireless appliances	Small ubiquitous devices connected by wireless networks will become future Internet appliances. To support them, communication networks must evolve to seamlessly assist appliances and provide advanced functionalities. We present a personal communication environment called Omnisphere that provides a communication and information universe surrounding wireless appliances. It is based on a high level concept called ambient services that allows to construct complex services out of primitive ones by connecting them with typed data flows. A typed data flow is an abstract view of communication between ambient services. It encapsulates three elements: channels, control, and metadata. Omnisphere provides a predefined service for discovery of component services and binding them together with data flows. Our strategy for service discovery is to delegate most of the operations to the network infrastructure and to automate them as much as possible. Based on the User ID and Appliance ID, Omnisphere retrieves the information that restricts the set of possible services: User Preferences, Device Capabilities, and Context. It then makes use of existing discovery protocols such as SLP, Jini, or UPnP to discover relevant services and matches them with the required characteristics. Such a discovery process relieves appliances, which may have limited resources, from the operation that may consume scarce resources and may require the availability of different discovery protocols on the appliance.	central processing unit;dataflow;future internet;high-level programming language;internet appliance;prototype;sensor;service discovery;software appliance;superword level parallelism;telecommunications network;universal plug and play;user (computing);www;jini	Franck Rousseau;Justinian Oprescu;Laurentiu-Sorin Paun;Andrzej Duda	2003		10.1109/HICSS.2003.1174839	data flow diagram;communications protocol;the internet;computer science;operating system;wireless network;service discovery;internet privacy;metadata;world wide web;ubiquitous computing;computer network	Mobile	-39.45471508038533	46.9807970875546	33588
77bb0a05eb36424b39cfd31c66d538a17702498d	platform of rich internet application for wireless sensor network	software;web service internet wireless sensor network web based software;rich internet application;user interface;wsn;web service;wireless sensor network;servers;internet;operating system;ip networks wireless sensor networks middleware application software sensor systems web services control systems software engineering web and internet services hardware;web services;web based software;sensor nodes;open system;middleware;web server;rich internet application middleware platform wsn;middleware platform;wireless sensor networks web services;wireless sensor networks;message service	As Web based software has developed recently, service via web has increased gradually. For this, many research organizations and reports call Web is the platform of software. While Web based technologies have developed, middleware of WSN and its application service has been developed from desktop based operation system. So WSN's technology reached uppermost limit of desktop application. There's difficulty in integrating, distributing and maintaining and repairing for WSN due to a tightly-coupled structure which is related closely to the hardware of sensor node. Unlike this, web has a loosely-coupled structure and an opened system, so Web service or rich Internet application (RIA) is helpful to solve the above limits. Especially, RIA is web application but can be the platform of WSN' application because it gives us various methods to communicate with user interface similar to desktop application. In this study, the author suggest RIA operation environment for WSN, expanding WSN's interconnectedness and accessibility to Internet.	accessibility;desktop computer;interconnectedness;middleware;operating system;rich internet application;sensor node;user interface;web application;web service	Hoon Kim;Young-Jun Jeon;Seung-Ho Shin	2008	2008 International Conference on Computer Science and Software Engineering	10.1109/CSSE.2008.704	web service;web application security;embedded system;web development;web application;web modeling;wireless sensor network;computer science;operating system;world wide web;computer network	SE	-36.54147152287576	48.454833664847186	33674
46b9f6146ad261da744e9ea225da1f4358d8c248	vulnerability analysis in vgbps using prolog	safety problem;vulnerability analysis;prolog;software systems;model checking;access control models;protection system;access control;system safety	Vulnerabilities are now part of all software systems. To handle vulnerabilities, many approaches have been proposed till now. Many of these approaches try to analyze vulnerabilities based on model checking techniques. However, the models used in these approaches handle authorized and unauthorized rules separately. This basically cause in weaker modeling abilities and consequently weaker vulnerability analysis. From authorized and unauthorized rules, we mean those emanated from access control model and those originated from vulnerabilities respectively. Currently, a new general graph-based protection system concentrating on vulnerabilities called VGBPS is proposed to overcome the mentioned problem. VGBPS combines vulnerabilities and their related rules in an access control system, in a way that no extra effort is needed to handle them. In contrast, vulnerability analysis in this model can be done by answering safety problem. Using this model, we propose a new approach for vulnerability analysis based on Prolog inference engine. In this approach, we show how to express modeling graph and rules set of a VGBPS model using Prolog facts and rules. Safety problem is also defined by Prolog rules. Finally, we use Prolog inference engine to answer safety problem which is the base of vulnerability analysis in VGBPS. We provide a case study to show how this approach can help us find possible exploits of a specific configuration in a system. Using Prolog, we can also find all possible scenarios of these exploits which can be used in many security analyses.	access control;authorization;control system;exploit (computer security);inference engine;model checking;prolog;software system;vulnerability (computing)	Mohammad Ebrahim Rafiei;Mohsen Taherian;Hamid Mousavi;Ali Movaghar-Rahimabadi;Rasool Jalili	2007		10.1007/978-3-540-75698-9_28	model checking;computer science;access control;vulnerability assessment;data mining;database;programming language;prolog;system safety;computer security;software system	Security	-60.99489854426729	58.561455022344845	33677
11eab7825ce5031326f0d4b8c273e5ff553432f1	reasoning about adaptive requirements for self-adaptive systems at runtime	automated planning;adaptive reasoning adaptive requirements self adaptive systems mobile applications requirements engineering re automated planning;monitoring and evaluation;requirements engineering;feedback;formal verification;adaptive applications;requirement engineering;adaptive system;mobile computing formal verification;planning;self adaptive systems requirements engineering planning feedback;navigation system;self adaptive systems;cognition runtime planning monitoring context adaptation models humans;mobile computing;mobile application	Increasing proliferation of mobile applications challenge the role of requirements engineering (RE) in developing customizable and adaptive software applications for the end-users. Such adaptive applications need to alter their behavior while monitoring and evaluating the changes in the environment at runtime by being aware of their end-user's needs, context and resources. More specifically, these applications should be able to: (i) reason about their own requirements and refine and validate them at run-time by involving end-users, if necessary; (ii) provide solutions for the refined or changed requirements at runtime, for instance by exploiting available services. In this position paper we focus on the first issue. We propose to extend our previous work on adaptive requirements with preference-based reasoning and automated planning to enable a continuous adaptive reasoning of requirements at runtime. We describe this vision using a navigation system example and highlight challenges.	adaptive system;artificial intelligence;automated planning and scheduling;case-based reasoning;exploit (computer security);global positioning system;mobile app;requirement;requirements engineering;run time (program lifecycle phase);sas;synergy;user interface;whole earth 'lectronic link	Nauman A. Qureshi;Sotirios Liaskos;Anna Perini	2011	2011 2nd International Workshop on Requirements@Run.Time	10.1109/ReRunTime.2011.6046243	real-time computing;simulation;computer science;systems engineering	SE	-42.84119011012675	37.87206513387132	33682
6bef8ac6ad79fbc6df2d8101ac1cf3d1268f223b	integrity levels: a new paradigm for protecting computing systems	trusted computing authorisation data integrity data privacy invasive software operating system kernels;computers;kernel;virtualization;computer architecture virtualization trusted computing computer security policy hypervisor operating system;computer security policy;data access integrity levels computing system protection integrity risk ubiquitous connectivity data storage data accessibility computer platforms operating system kernel os kernel rootkit data vulnerabilities computer system functionality computer system behavior system integrity malware attacks functionality degradation security policy specification security anomalies;hypervisor;computer architecture computers computational modeling kernel hardware malware;trusted computing;computer architecture;computational modeling;operating system;malware;hardware	"""As the field of determined and increasingly sophisticated adversaries multiplies, the risk to integrity of deployed computing devices magnifies. Given the ubiquitous connectivity, substantial storage, and accessibility, the increased reliance on computer platforms make them a significant target for attackers. Over the past decade, malware has transitioned from attacking a single program to subverting the operating system (OS) kernel by means of what is commonly known as a root kit. While computer systems require patches to fix newly discovered vulnerabilities, undiscovered vulnerabilities remain. Furthermore, typical solutions utilize mechanisms that operate within the OS. If the OS becomes compromised, these mechanisms may be vulnerable to being disabled or upon detection of the potential compromise, being """"shut down"""" until patched, or otherwise mitigated. We propose an innovative approach to designing computer systems that allows the behavior or functionality of the computer system to change based on the integrity of the system. Instead of attempting to prevent or detect all malware attacks, our proposed approach allows possible graceful degradation of functionality according to the security policy specification as anomalies of security concern are detected. We believe this innovative paradigm can determine the """"integrity level"""" of the system. Based on the integrity level, the computer system may behave differently or limit access to data."""	accessibility;computer;domain model;elegant degradation;fault tolerance;http 404;malware;mandatory integrity control;operating system;ptc integrity;patch (computing);programming paradigm;prototype;rootkit;smart meter;vulnerability (computing)	Christipher Jenkins;Lyndon Pierson	2014	2014 IEEE 13th International Conference on Trust, Security and Privacy in Computing and Communications	10.1109/TrustCom.2014.68	computer security model;embedded system;attack;covert channel;computer science;operating system;trusted computing base;data integrity;computer security	Security	-53.61088135450659	57.20955061272219	33750
4d6c0af07ecc18aa13c1e686417fc6ded8eee901	towards automated secure web service execution	semantic security;security protocols;web service;web services;ontology;security protocol;automated execution	Existing solutions for authentication and authorization in Web services make use of technologies such as SAML or WS-Security. These provide a static solution by using a set of predefined protocols. We propose a semantic security protocol model from which security protocol specifications are generated and automatically executed by participants. The proposed model consists of a sequential component, implemented as a WSDL-S specification, and an ontology component, implemented as an OWL specification. The correctness of the proposed model is ensured by using a set of rules and algorithms for generating it based on a protocol model given by the user. We validate our approach by generating and implementing several specifications for existing protocols such as ISO9798 or Kerberos protocols.	algorithm;authentication;authorization;correctness (computer science);cryptographic protocol;kerberos;middleware;security assertion markup language;semantic security;ws-security;web ontology language;web services description language;web service	Béla Genge;Piroska Haller	2009		10.1007/978-3-642-01399-7_74	computer security model;web application security;universal composability;computer science;database;security service;world wide web;owl-s;computer security	Security	-50.42005575207432	52.950283675641664	33822
18d9e4fa1641c3be6e76eebbb5c40014e43e30d9	the unbearable lightness of pin cracking	insider attack;phantom withdrawal;hsm;api attack;security api;visa pvv;functional requirement;financial pin processing api;emv;ibm 3624	We describe new attacks on the financial PIN processing API. The attacks apply to switches as well as to verification facilities. The attacks are extremely severe allowing an attacker to expose customer PINs by executing only one or two API calls per exposed PIN. One of the attacks uses only the translate function which is a required function in every switch. The other attacks abuse functions that are used to allow customers to select their PINs online. Some of the attacks can be applied on a switch even though the attacked functions require issuer’s keys which do not exist on a switch. This is particularly disturbing as it was widely believed that functions requiring issuer’s keys cannot do any harm if the respective keys are unavailable.	application programming interface;network switch;password cracking;personal identification number	Omer Berkman;Odelia Moshe Ostrovsky	2007		10.1007/978-3-540-77366-5_20	computer science;internet privacy;computer security;functional requirement;hardware security module	Security	-53.69643386003135	59.65995354893617	33830
c42eac67909040dd543b09fc7155bbd930149e4f	application of formal verification to the lane change module of an autonomous vehicle		For autonomous vehicles correct behavior is of the utmost importance, as unexpected incorrect behavior can have catastrophic outcomes. However, as with any large-scale software development, it is not easy to get the system correct. As the system is made up of multiple sub-modules that interact with each other, unexpected behavior can arise from incorrect interactions when one module may have unfulfilled expectations on the other. This paper describes how formal verification was applied to the lane change module of the decision and control software (under development) for an autonomous vehicle. The module was manually modelled as an extended finite-state machine, as were some of the requirements. When applying the Supremica software to perform the formal verification, some bugs were discovered in the model. Setting up additional unit tests triggering the incorrect behavior showed that this behavior was also present within the actual code. For some of the errors, applied corrections resulted in the absence of the particular error, thus demonstrating the power of true formal verification.	algorithm;autonomous robot;extended finite-state machine;formal methods;formal verification;interaction;requirement;software bug;software development;unit testing	Anton Zita;Sahar Mohajerani;Martin Fabian	2017	2017 13th IEEE Conference on Automation Science and Engineering (CASE)	10.1109/COASE.2017.8256223	model checking;software bug;matlab;software engineering;supervisory control theory;software development;unit testing;software;embedded system;formal verification;computer science	SE	-46.6438486253363	35.4338467998512	33839
39ccb5b973f5e66d652a3e31ffdf0df2b77681a9	domain-based access control for distributed computing systems	domain model;inter organisation interactions;access rights;distributed computing systems;transparent access;computer networks;security of data computer networks distributed processing network operating systems;distributed computing system;security requirements;organisational structure;computer networks distributed computing network operating systems data security;security requirements computer networks distributed computing systems domain model access rights access control organisational structure inter organisation interactions transparent access capability based access rights;access control;capability based access rights	Advances in communications technology allow the construction of very large distributed computing systems (VLDCSs) containing thousands of computers and spanning several organisational boundaries. Existing management tools and approaches are not appropriate to the size and multiple-organisation nature of these VLDCSs. This paper describes a new approach to the management of VLDCSs based on a domain model. While this model is applicable to most aspects of management, the paper describes an implementation of the domain model for management of access rights. Domains provide a flexible means for specifying access control policies, which reflect organisational structure, and permit secure inter-organisation interactions, while giving users transparent access to resources. The paper describes an implementation of domains in terms of capability-based access rights, which meets the flexibility and security requirements for managing VLDCSs. Security is enhanced by physically preventing programs from directly accessing capabilities.	access control;computer;distributed computing;domain model;file spanning;interaction;requirement	D. C. Robinson;M. S. Sloman	1988	Software Engineering Journal	10.1049/sej.1988.0019	computer security model;organizational structure;physical access;computer science;knowledge management;access control;logical security;domain model;role-based access control;distributed computing;distributed system security architecture;network access control;computer security	Security	-46.90539211467318	54.05068327544768	33847
125d3233dba3ebcc09f765b720282205fa416f31	a cooperative approach to support software deployment using the software dock	cooperative approach;interrelated processes;software systems software performance ip networks permission context modeling software engineering laboratories java mobile agents cd roms;configuration management cooperative approach software deployment support software dock framework evolving collection interrelated processes large networks internet software deployment technologies distributed agent based deployment framework software producers software consumers standardized deployment schema deployable software description format software deployment activities software dock infrastructure high level deployment services java mobile agents;deployment;software dock framework;networks;distributed agents;mobile computing distributed programming object oriented programming java;mobile agents;software systems;semantics;software deployment activities;object oriented programming;software producers;software engineering;standardized deployment schema;software deployment support;software performance;computer networks;computer programs;collection;evolving collection;internet;high level deployment services;permission;deployable software description format;consumers;large networks;distributed programming;semantic description;software deployment;ip networks;software consumers;mobile agent;mobile computing;context modeling;software deployment technologies;configuration management;software dock infrastructure;infrastructure;cd roms;distributed agent based deployment framework;java	Software deployment is an evolving collection of interrelated processes such as release, install, adapt, reconfigure, update, activate, deactivate, remove, and retire. The connectivity of large networks, such as the Internet, is affecting how software deployment is being performed. To take full advantage of this connectivity, new software deployment technologies must be introduced in order to support these processes. The Software Dock research project is creating a distributed, agent-based deployment framework to support the ongoing cooperation and negotiation among software producers themselves and among software producers and software consumers. This deployment framework is enabled by the use of a standardized semantic schema for describing software systems, called the Deployable Software Description (DSD) format. The Software Dock employs agents to traverse between software producers and consumers and to perform software deployment activities by interpreting the semantic descriptions of the software systems. The Software Dock infrastructure enables software producers to offer high-level deployment services that were previously not possible to their customers.	agent-based model;declarative programming;document structure description;high- and low-level;internet;software deployment;software system;traverse	Richard S. Hall;Dennis Heimbigner;Alexander L. Wolf	1999		10.1145/302405.302463	personal software process;long-term support;verification and validation;computer science;package development process;backporting;social software engineering;software framework;component-based software engineering;software development;software design description;operating system;software engineering;software construction;database;distributed computing;semantics;software walkthrough;programming language;software analytics;resource-oriented architecture;mobile computing;software deployment;software quality;software system;avionics software	SE	-35.59423773696353	42.21965796310449	33872
9a2383a2eabf529a27fb02350d88aca97b343618	robustness and security in a mobile-agent based network monitoring system	web server;detectors;remote monitoring;intelligent networks;system architecture;system security;robustness;fault detection;network monitoring;mobile agent;multi agent system;system monitoring	We present the mechanisms for self-recovery in Konark, a mobile agent based system for monitoring network computing systems. An important aspect of our design is the use of the monitoring system's inherent capabilities to detect its own component failures. The Konark system is implemented using Ajanta. Our monitoring system achieves robustness by incorporating mechanisms for self-monitoring and self-configuration at different levels of the system architecture. The event detection, correlation, and notification mechanisms are used as the basic building blocks for failure detection. Our design uses the notion of continuous periodic detection and notification of a failure event until the failed components causing it are repaired.	agent-based model;extensibility;mobile agent;systems architecture	Anand R. Tripathi;Muralidhar Koka;Sandeep Karanth;Ivan Osipkov;Harsha Talkad;Tanvir Ahmed;David Johnson;S. Dier	2004	International Conference on Autonomic Computing, 2004. Proceedings.	10.1109/ICAC.2004.51	embedded system;system monitoring;intelligent network;detector;real-time computing;computer science;multi-agent system;mobile agent;computer security;network monitoring;web server;fault detection and isolation;robustness;systems architecture;rmon	SE	-38.14578866622022	43.41406698629716	33882
1a7d5dde0cdf7bbf9ae139c0656d811d557b7e8b	dynamic traceability links supported by a system architecture description	system architecture description;program diagnostics;system comprehension;query processing;maintenance;software maintenance;information collection;information size limitation;system documentation;system feature description information dynamic traceability links information collection information inspection information size limitation mental model predefined types scalable query mechanism software maintenance system architecture description system comprehension system documentation;maintenance efficiency;computer aided software engineering;system understanding;system feature description information;system documentation computer aided software engineering program diagnostics query processing reverse engineering software maintenance;scalable query mechanism;system architecture;traceability;information inspection;dynamic traceability links;mental model;documentation;reverse engineering;predefined types	ted. Abstract To reduce the effort spent on system comprehension during software maintenance, easy access to different type of information describing the system features is necessary. This is used by the maintainer to build a mental model of the software. Links relating different types of information are often implicit, and called traceability links. Extracting all traceability links of possible interest from the system documentation would be an extremely complex operation. Storing and updating a database of such information manually would be an expensive task. We propose that the identification of information about a feature is done dynamically. The maintainer is provided with a powerful query mechanism for identifying a starting point for collecting information. Further information is identified dynamically by automatically expanding several predefined types of traceability links. This avoids the problems of the traditional database approach. To make the query mechanism scalable, an architectural description of the system is used to limit the size of information which must be inspected by a query.	accessibility;information retrieval;mental model;scalability;software documentation;software maintenance;systems architecture;traceability;type system	Eirik Tryggeseth;Øystein Nytrø	1997		10.1109/ICSM.1997.624244	traceability;documentation;computer science;systems engineering;engineering;software engineering;data mining;database;programming language;software maintenance;requirements traceability;reverse engineering;systems architecture	SE	-53.57979213241598	34.640655057962604	33943
6234d224dcead74308b4e1faac266e0b804afe6e	java object behavior modeling and visualization	java visualization logic yarn measurement prototypes software prototyping state space methods large scale systems displays;performance measure;object activity spectrum software visualization object visualization object behavior model;yarn;state space methods;measurement;object interaction;software prototyping;behavior modeling;prototypes;logic;object behavior model;spectrum;object activity spectrum;visualization;displays;object visualization;large scale systems;java;software visualization	Java developers need to know what a specific object did during a program run. Object behavior visualization can fulfill this requirement. This paper presents a novel object behavior model, a Lifetime Behavior Model (LBM) and visualization methods to provide deductive and inductive visualizations of Java object behavior. For the deductive visualization, this paper visualizes the object behavior by three different LBMTrees from thread, object interaction and method invocation view respectively. For the inductive visualization, this paper presents an Activity Spectrum Model (ASM) and a set of performance measurements based on the LBM. The visualization prototype is developed to access object behavior events by JVMPI, construct the models and visualize the models. Experiment shows that the results proposed here can provide comprehensive and clear understanding of Java object behaviors.	behavior model;event (computing);experiment;inductive reasoning;java;lattice boltzmann methods;need to know;program comprehension;prototype;subroutine;time series;tracing (software)	Ji Wu;Xiao-xia Jia;Yong Po Liu;Guo-huan Li	2006	2006 International Conference on Software Engineering Advances (ICSEA'06)	10.1109/ICSEA.2006.50	software visualization;spectrum;computer vision;method;visualization;computer science;data access object;theoretical computer science;programming language;java;logic;measurement;computer graphics (images)	SE	-54.14958946046192	36.38559411469604	33950
48d5cfbf4df664367d0b349bb91595c76204789d	extensible contract broker for performance differentiation	extensible contract broker;contract broker architecture;qos;software architecture;dynamic xml specifications;xml diffserv networks quality of service software architecture;contracts control systems automatic control quality of service xml admission control runtime application software yarn tagging;dynamic xml specifications extensible contract broker performance differentiation quality of service qos contract broker architecture;xml;diffserv networks;quality of service;performance differentiation	We denote as Self-properties the control a system exerts on itself autonomically to achieve and maintain Quality-of-Service (QoS). Our focus is in external, non-intrusive approaches and in this paper we focus mainly timeliness objectives. The self-adaptation process must be guided by the specification of the desired qualities. In this paper we describe the design of a Contract-Broker (C-Broker) architecture for implementing Quality-of-Service features and Selfproperties. We also show how basic QoS features were implemented in C-Broker. Our approach involved dynamic XML specifications for required parameters.	dynamic xml;message broker;quality of service	Pedro Nuno San-Bento Furtado;Celso Rafael Santos	2007	International Workshop on Software Engineering for Adaptive and Self-Managing Systems (SEAMS '07)	10.1109/SEAMS.2007.7	broker pattern;quality of service;computer science;software engineering;database;world wide web;computer network	SE	-39.66623220713642	40.256670834442986	34063
b73b86e33f2ef7a1be510f5d00602b39540d5627	robust insider attacks countermeasure for hadoop: design and implementation		Hadoop is an open source software framework for storage and processing of large-scale datasets. The proliferation of cloud services and its corresponding increasing number of users lead to a larger attack surface, especially for internal threats. Therefore, in corporate data centers, it is essential to ensure the security, authenticity, and integrity of all the entities of Hadoop. The current secure implementations of Hadoop mainly utilize Kerberos, which is known to suffer from many security and performance issues, including the concentration of authentication credentials, single point of failure, and online availability. Most importantly, these Kerberos-based implementations do not guard against insider threats. In this paper, we propose an authentication framework for Hadoop that utilizes trusted platform module technology. The proposed approach provides significant security guarantees against insider threats, which manipulate the execution environment without the consent of legitimate clients. We have conducted extensive experiments to validate the performance and the security properties of our approach. The results demonstrate that the proposed approach alleviates many of the shortcomings of Kerberos-based state-of-the-art protocols and provides unique security guarantees with acceptable overhead. Moreover, we have formally proved the correctness and the security guarantees of our protocol via Burrows–Abadi–Needham logic.	apache hadoop;attack surface;authentication;burrows–abadi–needham logic;cloud computing;correctness (computer science);credential;data center;entity;experiment;insider threat;kerberos;needham–schroeder protocol;open-source software;overhead (computing);reliability engineering;single point of failure;software framework;threat (computer);trusted platform module	Zuochao Dou;Issa M. Khalil;Abdallah Khreishah;Ala I. Al-Fuqaha	2018	IEEE Systems Journal	10.1109/JSYST.2017.2669908	distributed computing;internet privacy;computer security	Security	-52.072587638244826	57.94585476688116	34084
f701d9f95a16338da01aae54662db7ba451f73b1	web access control strategies		In Discretionary Access Control models such as the Harrison, Ruzzo, Ullman model [1], the security policy is specified by using the concepts of subjects, actions and objects. Subjects are the active entities of the system, objects are the passive entities and actions are the direct accesses that subjects can perform on objects. The access control policy consists of a set of triples (subject, action, object). Each triple (s, a, o) reads “subject s has the permission to perform action a on object o” 1 . Such models are called discretionary since permissions address the concrete identity of subjects. Many DAC models support the concept of user group. User groups simplify the management of authorizations since a single permission granted to a group propagates to all group members. Most of the DAC models incorporate the concept of ownership. Subjects may grant or revoke rights for objects they own (basically objects they have created). In other words, subjects administrate the security policy for their objects.	discretionary access control;entity;sethi–ullman algorithm	Alban Gabillon	2011		10.1007/978-1-4419-5906-5_664	web application security;web development;web analytics;web accessibility initiative	Security	-48.50729912140753	53.07256640247533	34115
eea48d962e78b587ebecfccc85c341144de1a8fc	a meta-heuristically optimized fuzzy approach towards multi-metric security risk assessment in heterogeneous system of systems		Security measurement of complex systems is a challenging task since devices deployed over the so-called System of Systems (SoS) are extremely heterogeneous and hence imply an interoperability effort in order to enable a common resilient security measurement language. Moreover, systems demand more features beyond security concept, require to preserve privacy and claim for dependable structures in order to seek a holistic and aggregated security and safety view. This paper addresses this need by capitalizing the availability of multiple security metrics through an hybrid meta-heuristic fuzzy aggregation and composition approach that takes into account the expertise compiled by the security manager, towards the generation of visual dashboards reflecting the SPD (Security, Privacy and Dependability) risk status of the system at hand.	apple sos;compiler;complex systems;dependability;fuzzy logic;heuristic;holism;interoperability;risk assessment;system of systems	Iñaki Eguia;Javier Del Ser	2014		10.5220/0004876802310236	reliability engineering;data mining;risk analysis	Security	-54.90920210664093	47.16473183974322	34204
0f6972b4c365bb10b6583a88b8eb01cb24dc5c85	provenance as first class cloud data	artefacto;anomaly;consumidor;storage system;informatique dans les nuages;metadata;consommateur;indice aptitud;anomalie;date;journal article;artefact;anomalia;indice aptitude;pattern detection;fecha;capability index;consumer;systeme memoire;comportement utilisateur;metadonnee;next generation;metadatos;user behavior;sistema memoria;comportamiento usuario;computacion en nube;cloud computing	Digital provenance is meta-data that describes the ancestry or history of a digital object. Most work on provenance focuses on how provenance increases the value of data to consumers. However, provenance is also valuable to storage providers. For example, provenance can provide hints on access patterns, detect anomalous behavior, and provide enhanced user search capabilities. As the next generation storage providers, cloud vendors are in the unique position to capitalize on this opportunity to incorporate provenance as a fundamental storage system primitive. To date, cloud offerings have not yet done so. We provide motivation for providers to treat provenance as first class data in the cloud and based on our experience with provenance in a local storage system, suggest a set of requirements that make provenance feasible and attractive.	cloud computing;computer data storage;first-class function;next-generation network;requirement;thread-local storage;virtual artifact	Kiran-Kumar Muniswamy-Reddy;Margo I. Seltzer	2009	Operating Systems Review	10.1145/1713254.1713258	simulation;anomaly;consumer;cloud computing;process capability index;computer science;operating system;database;metadata;world wide web;computer security	OS	-41.22931947195903	58.461539338334966	34218
0b118cc206c845eedf7eb568d58d124bd0e6c18c	ontological representation of networks for ids in cyber-physical systems		Cyber-Physical System (CPSs) combine information and communication technologies and means controlling physical objects. Modern infrastructure objects such as electrical grids, smart-cities, etc. represent complex CPSs consisting of multiple interconnected software and hardware complexes. The software contained in them requires development, support, and in case of updates termination can be the target for malicious attacks. To prevent intrusion into networks of cyber-physical objects one can use Intrusion-Detection System (IDS) that are widely used in existing noncyber-physical networks. CPSs are characterized by formalization and determinacy and it allows to apply a specification-based approach for IDS development.	cyber-physical system	Vasily A. Sartakov	2015		10.1007/978-3-319-26123-2_40	determinacy;distributed computing;cyber-physical system;ontology;software;intrusion;intrusion detection system;information and communications technology;computer science	Logic	-57.10647589719725	51.901610635470874	34226
45fb47365dc54e19ca962b270128608b023f8180	tackling architectural complexity with modeling	tackling architectural complexity;diagnose architectural problem;component model	Component models can help diagnose architectural problems in both new and existing systems.		Kevin Montagne	2010	ACM Queue	10.1145/1854039.1862187	simulation;systems modeling;computer science;operating system;component object model;algorithm	Arch	-45.80901577918982	33.434087488254576	34245
afb44ef1d8ecc57a5def0b33d27cc37bb51038f9	non-functional information transmission patterns for distributed real-time java	rt jrmp;drstj;rt rmi;rtsj	Many real-time systems use preemptive priority-based scheduling in their internals to guarantee certain real-time performance. This includes technologies that range from The Real-Time Specification for Java (RTSJ) to middleware like Real-Time Common Object Request Broker Architecture (RT-CORBA), which offers additional models and policies that blend client and server information. This decision eases the integration of real-time admission tests and dispatching policies in these types of infrastructures. In this paper, we analyze different trade-offs that emerge from the definition of different propagation models for distributed real-time Java. The paper covers technological integration aspects as impact on interfaces and other practical issues mainly related to the performance that this model offers to a real-time application and non-functional overhead. The contribution described in the paper may help in the development of The Distributed Specification for Java (DRTSJ). Copyright q 2011 John Wiley & Sons, Ltd.	common object request broker architecture;distributed object communication;function model;interference (communication);john d. wiley;middleware;overhead (computing);preemption (computing);real time java;real-time clock;real-time computing;real-time transcription;remote procedure call;scheduling (computing);serialization;server (computing);software propagation;transmitter	Pablo Basanta-Val;Marisol García-Valls;Iria Estévez-Ayres	2011	Softw., Pract. Exper.	10.1002/spe.1084	real-time computing;computer science;operating system;database;real time java;programming language	Embedded	-37.04554333282084	40.87754048597066	34308
658508c702bf6d8ec6af4b0f3bb7bf17d970e70c	heap protection for java virtual machines	system engineering;cache;javabeans;persistence;java virtual machine;serialization;xml;scripting;ltp;java	Java virtual machine (JVM) crashes are often due to an invalid memory reference to the JVM heap. Before the bug that caused the invalid reference can be fixed, its location must be identified. It can be in either the JVM implementation or the native library written in C language invoked from Java applications. To help system engineers identify the location, we implemented a feature using page protection that prevents threads executing native methods from referring to the JVM heap. This feature protects the JVM heap during native method execution, and when native method execution refers to the JVM heap invalidly, it interrupts the execution by generating a page-fault exception and then reports the location where the page-fault exception was generated. This helps the system engineer to identify the location of the bug in the native library. The runtime overhead for using this feature averaged 4.4% based on an estimation using the SPECjvm98, SPECjbb2000, and JFCMark benchmark suites.	benchmark (computing);interrupt;java virtual machine;library (computing);overhead (computing);page fault;systems engineering	Yuji Chiba	2006		10.1145/1168054.1168069	persistence;parallel computing;xml;cache;serialization;long-term potentiation;computer science;operating system;scripting language;javabeans;programming language;java;java annotation	PL	-56.86324781788514	55.295732051313486	34336
f208db892aa327a106b1d4c1bf8d19ebbc89dd33	semantic view re-creation for the secure monitoring of virtual machines	virtualization;virtual machines;secure monitoring;semantic gap;dissertation;systems security;introspection	The insecurity of modern-day software systems has created the need for security monitoring applications, such as anti-virus tools. These applications conduct passive monitoring of the system's state and active monitoring of the system's events. Two serious deficiencies are commonly found in such applications. First, their lack of isolation from the system being monitored allows malicious software to tamper with or disable them. Second, the lack of secure and reliable monitoring primitives in the operating system compromises their visibility, making them easy to be evaded. #R##N#A technique known as Virtual Machine Introspection attempts to solve these problems by leveraging the strong isolation and mediation properties of full-system virtualization. It isolates the monitoring application in a separate, security virtual machine, from where it can securely monitor a guest virtual machine by leveraging the hypervisor's view of resources. This separation creates, however, a problem known as semantic gap, which can be defined by the loss of a high-level view of the guest's state and events from the part of the monitoring application. It occurs as a result of the low-level separation enforced by the hypervisor between the guest and the security virtual machine. #R##N#This thesis proposes and investigates novel techniques to overcome the semantic gap, advancing the state-of-the-art on the syntactic and semantic guest view re-creation for security applications that conduct passive and active monitoring of virtual machines. #R##N#In the space of passive monitoring, we propose a new technique for reconstructing a syntactic view of the guest OS kernel's heap state. By applying a combination of static code and dynamic memory analysis techniques, we are able to reconstruct a map of the guest OS's dynamic kernel objects. Our key contribution over previous work is the accuracy and completeness of our analysis, which translates into stronger monitoring capabilities for security applications. #R##N#Although sufficient for certain types of integrity checking applications, a syntactic view of the guest state is not enough for others that require access to information at a higher level. With this in mind, we propose a technique that combines the security of out-of-VM monitoring with the semantic awareness of in-VM monitoring. By allowing out-of-VM applications to invoke and securely execute API functions inside the monitored guest's kernel, we eliminate the need for the application to know details of the guest's internal data structures. Our key contribution over previous work is the ability to overcome the semantic gap between the monitoring application and the guest OS in a robust and secure manner, by relying on the guest's own code. #R##N#A security monitoring solution cannot be complete without an active monitoring component that intercepts and evaluates guest events as they happen. In this space, we propose a new virtualization-based event monitoring technique based on the interception of kernel data modifications as opposed to code execution trapping. Our key contribution over previous work is the ability to monitor high-level operating system events without the need for in-guest components and without the same circumvention problems of code execution hooks, and the ability to automatically re-create the syntactic context of guest kernel memory accesses.		Martim Carbone	2012			real-time computing;computer science;distributed computing;computer security	OS	-54.49675882402301	54.64797795783562	34341
376a14c30e714e4f36903c850fa125a406a8bde4	a privacy negotiation mechanism for the internet of things		This paper presents a new privacy negotiation mechanism for an IoT environment that is both efficient and practical to cope with the IoT special need of seamlessness. This mechanism allows IoT users to express and enforce their personal privacy preferences in a seamless manner while interacting with IoT deployments. In addition, the proposed mechanism satisfies the privacy requirements of the IoT deployment owner. Finally, the proposed privacy mechanism is agnostic to the actual IoT architecture and can be used over a user-managed, edge-managed or a cloud-managed IoT architecture. Prototypes of the proposed mechanism have been implemented for each of these three architectures, and the results show the capability of the protocol to negotiate privacy while adding insignificant time overhead.	adobe flash player;interaction;internet of things;overhead (computing);privacy policy;requirement;seamless3d;software deployment	Khaled Alanezi;Shivakant Mishra	2018	2018 IEEE 16th Intl Conf on Dependable, Autonomic and Secure Computing, 16th Intl Conf on Pervasive Intelligence and Computing, 4th Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)	10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.00099	computer security;software deployment;data collection;architecture;negotiation;cloud computing;information privacy;server;computer science;internet of things	Mobile	-43.12546396060018	59.3446445664511	34397
0f591220d6a6bf96ec36c7d23c54d8c86d3f5203	language support for changeable large real time systems	real time systems;distributed system;object oriented	A set of concepts for modeling large real time systems is discussed informally. The concepts support the design of centralized as well as distributed systems. They are object oriented in that they correspond to entities of the 'real world', and they are 'change oriented' in that they support not only the first development stage of a system but also its continuous change and evolution. In particularly, the concepts give a promising solution to 'on the fly' changes of existing, active entities.	centralized computing;distributed computing;entity;on the fly;software release life cycle	Ivar Jacobson	1986		10.1145/28697.28736	real-time computing;simulation;computer science;distributed computing;programming language;object-oriented programming	DB	-39.01027760887869	39.07476159463767	34473
660c4a80bf800f19ad5c992e0ae23e6e1b6dbea1	cloudfit, a paas platform for iot applications over pervasive networks		IoT applications are the next important step towards the establishment of mobiquitous systems, but at the same time these environments raise important challenges when considering data distribution and processing. While most IoT applications today rely on clouds as back-end, critical applications that require fast response or enhanced privacy levels may require proximity services specially tailored to these needs. The deployment of private cloud services on top of pervasive grids represent an interesting alternative to traditional cloud infrastructures. In this work we present CloudFIT, a PaaS middleware that allows the creation of private clouds over pervasive environments. Using a MapReduce application as example, we show how CloudFIT provides both storage and data aggregation/analysis capabilities at the service of IoT networks.	apache hadoop;application programming interface;cloud computing;data aggregation;distributed computing;fits;filter (signal processing);mapreduce;middleware;peer-to-peer;pervasive informatics;platform as a service;programming paradigm;requirement;software deployment	Luiz Angelo Steffenel;Manuele Kirsch-Pinheiro	2015		10.1007/978-3-319-33313-7_2	cloud computing;data aggregator;software deployment;computer network;overlay network;middleware;internet of things;computer science	OS	-42.50198717267717	48.96145074496284	34564
f76e30a8bd5ae08a0aa6cda6d4406c139d0a64c9	modular verification of safe online-reconfiguration for proactive components in mechatronic uml	information technology;cooperative agents;model checking;modular verification;mechatronic systems;time constraint	While traditionally the environment considered by an autonomous mechatronic systems only consists of the measurable, surrounding physical world, today advanced mechatronic systems also include the context established by the information technology. This trend makes mechatronic systems possible which consist of cooperating agents which optimize and reconfigure the system behavior by adjusting their local behavior and cooperation structure to better serve their current goals depending on the experienced mechanical and information environment. The MECHATRONIC UML approach enables the componentwise development of such self-optimizing mechatronic systems by providing a notion for hybrid components and support for modular verification of the safe online-reconfiguration. In this paper, we present an extension to the formerly presented solution which overcomes the restriction that only purely reactive behavior with restricted time constraints can be verified. We present how model checking can be employed to also verify the safe modular reconfiguration for systems which include components with complex time constraints and proactive behavior.	automata theory;autonomous robot;hybrid system;mechatronics;model checking;real-time web;refinement (computing);simulation;state space;unified modeling language;verification and validation	Holger Giese;Martin Hirsch	2005		10.1007/11663430_8	control engineering;model checking;real-time computing;computer science;systems engineering;engineering;information technology	SE	-42.22200232879966	37.547191482519835	34702
0302f35a4766f9f01e217407c8d3139fd3b78f17	samproc - a middleware for highly dynamic and heterogeneous environments		Software in mobile and ubiquitous computing scenarios has to cope with a highly dynamic and heterogeneous environment. For tapping the environment’s full potential, software should be able to dynamically adapt on demand in a platformand language-independent manner (e.g., by migration). Current systems have only limited support for the requested dynamics and heterogeneity. Thus, the development of such adaptive applications is still a highly complex and thus error-prone task. In the context of this thesis, the SAMProc middleware is designed. SAMProc supports software developers in creating such adaptive applications and thus eases development. In the SAMProc approach, applications are able to dynamically change the location and to adapt the provided interface, the current state and the implementation in use to the particular local execution context. Thereby, applications are transparently and permanently addressable. This allows continuous interaction with other applications. For easing application development SAMProc uses a model-driven approach. For this purpose, this thesis introduces the novel concept of a self-adaptive mobile process (SAMProc), which allows an abstract specification of the life cycle as well as distribution aspects of an adaptive application: First, software developers model the SAMProc; then, a code generator automatically maps this description to the SAMProc infrastructure and creates the necessary adaptation code. This thesis presents all parts of the SAMProc middleware in detail. Important contributions are an architectural design pattern for dynamic application adaptation and migration in a heterogeneous environment and novel infrastructure services supporting software in such environments, such as a service for dynamic management of code, a generic context service and a novel particularly lightweight Web service container. Innovative example applications complete this work by demonstrating the potential of the SAMProc middleware.	architectural pattern;code generation (compiler);cognitive dimensions of notations;design pattern;language-independent specification;map;middleware;model-driven architecture;software developer;ubiquitous computing;web service	Holger Schmidt	2009			distributed computing;computer science;middleware	SE	-40.4607421693377	40.219681117233364	34717
74e1b306c5f636a7dae47d39b8cf1bb5543db6cf	on the design, implementation and application of an authorisation architecture for web services	information security;legacy applications;net framework;healthcare technology;web service;computer security;authorisation architecture;web services;middleware;access control;service oriented architecture;multiple access control;information and computer security	This paper proposes an authorisation architecture for web services. It describes the architectural framework, the administration and runtime aspects of our architecture and its components for secure authorisation of web services as well as the support for the management of authorisation information. The paper then describes the implementation aspects of the architecture. The architecture has been implemented and integrated within the .NET framework. The authorisation architecture for web services is demonstrated using a case study in the healthcare domain. The proposed architecture has several benefits. First and foremost, the architecture supports multiple access control models and mechanisms; it supports legacy applications exposed as web services as well as new web service-based applications built to leverage the benefits offered by the Service-Oriented Architecture; it is decentralised and distributed and provides flexible management and administration of web services and related authorisation information. The proposed architecture can be integrated into existing middleware platforms to provide enhanced security to web services deployed on those platforms.	.net framework;access control;authorization;enterprise architecture framework;foremost;middleware;service-oriented architecture;service-oriented device architecture;web service	Sarath Indrakanti;Vijay Varadharajan;Ritesh Agarwal	2007	IJICS	10.1504/IJICS.2007.012245	enterprise architecture framework;web service;web application security;reference architecture;the open group architecture framework;middleware;space-based architecture;website architecture;web development;web modeling;sherwood applied business security architecture;computer science;applications architecture;information security;multitier architecture;ws-policy;service-oriented architecture;database;service;solution architecture;distributed system security architecture;services computing;world wide web;computer security;data architecture	Web+IR	-46.95178782893296	54.90444986204269	34727
ece6cc810224fd924f68cbe4d6aa94477a4135e6	second life information desk system using instant messaging and short messaging service technologies	software;instant messaging;electronic mail;information systems;information desk;second life information desk system;short message service;second life message service avatars facial animation scheduling communication channels feedback organizing telecommunication computing telecommunication services;help desk;telecommunication computing;data mining;companies;information systems avatars electronic messaging;sms;second life avatar;servers;feedback;instant messaging service;organizing;second life avatar second life information desk system instant messaging service short messaging service technology;scheduling;facial animation;electronic messaging;avatars;telecommunication services;second life;organizations;communication channels;user interaction;short messaging service technology;messaging;virtual worlds information desk help desk second life messaging sms;virtual worlds;message service	Organizations with a presence in the Second Life® world typically only provide direct user interaction with staff at specific schedules, or not at all. We present a system that provides organizations with a simple way to enable constant interaction with users of the Second Life world, by simulating staff presence using automated avatars as communication channels to real-life staff by means of instant messaging and short message service technologies. Staff members are assigned to communication with Second Life avatars based on a hierarchy of information desk staffing priorities, and communication is bidirectional.	client (computing);duplex (telecommunications);instant messaging;prototype;real life;sapo (computer);sl (complexity);second life;session (computer science);simulation;virtual reality;virtual world;windows live	Sergio Valerio;Jean Pereira;Leonel Morgado;Pedro Mestre;Carlos Serôdio;Fausto de Carvalho	2009	2009 Conference in Games and Virtual Worlds for Serious Applications	10.1109/VS-GAMES.2009.13	computer science;multimedia;internet privacy;world wide web	HCI	-39.66590455942676	55.154429719231466	34737
0d01c2adf91bc3215249b6479cacc7d5421cb330	the zodiac policy subsystem: a policy-based management system for a high-security manet	authorization policies policy based management system high security manet zero outage dynamic intrinsically assurable communities policy subsystem byzantine faults usability requirements network control system security policies keynote language;control systems;manet;computer network security;information security;authorisation;authentication;authorization policies;mobile ad hoc networks information security containers authorization authentication protection cryptography usability control systems computer network management;system security;system security policies;computer architecture;network control;protection;telecommunication security ad hoc networks authorisation mobile radio telecommunication network management;design and implementation;mobile ad hoc networks;cryptography;policy based management;mobile radio;computer network management;telecommunication security;usability requirements;ad hoc networks;authorization;high security manet;computer science;keynote language;peer to peer computing;byzantine faults;usability;zero outage dynamic intrinsically assurable communities policy subsystem;policy based management system;containers;computer network security policy based management manet;telecommunication network management	Zodiac (Zero Outage Dynamic Intrinsically Assurable Communities) is an implementation of a high-security MANET, resistant to multiple types of attacks, including Byzantine faults. The Zodiac architecture poses a set of unique system security, performance, and usability requirements to its policy-based management system (PBMS). In this paper, we identify theses requirements, and present the design and implementation of the Zodiac Policy Subsystem (ZPS), which allows administrators to securely specify, distribute and evaluate network control and system security policies to customize Zodiac behaviors. ZPS uses the Keynote language for specifying all authorization policies with simple extension to support obligation policies.	authorization;byzantine fault tolerance;management system;requirement;sun outage;usability	Yuu-Heng Cheng;Mariana Raykova;Alexander Poylisher;D. Scott Alexander;Martin Eiger;Steven M. Bellovin	2009	2009 IEEE International Symposium on Policies for Distributed Systems and Networks	10.1109/POLICY.2009.7	business;internet privacy;computer security;computer network	Arch	-49.577748842664576	55.23963079413771	34776
b967aba5bf056532510a5e106504c3a9853f15f0	weather-based road condition estimation in the era of internet-of-vehicles (iov)		Modern high-end vehicles are equipped with advanced embedded computing elements and wireless connectivity capabilities. All these new features can provide new services, introducing the smart car era. This evolution has created a large network that includes cars as preliminary entities, extending the Internet of Things (IoT) to what is often referred as Internet-of-Vehicles (IoV). Thus, smart cars rise the challenge of efficient utilization of the available computational, sensing and connection potential. This paper presents a systematic methodology for combining different information sources in order to estimate the status of the car and perform specific actions. As a use case, the proposed approach considers the estimation of the road condition utilizing weather information. Specifically, it informs the driver about a recommended (dynamic) speed limit that the car should adapt to.	embedded system;entity;institute for operations research and the management sciences;internet of things	Ioannis Galanis;R Ibrahimi;Dona Burkard;Iraklis Anagnostopoulos	2018	2018 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2018.8351582	the internet;electronic engineering;computer network;computer science;speed limit;wireless;internet of things	Embedded	-43.48629804739475	50.65977671695969	34816
5558ea18aa25a5827e296074ffc555427576b656	distributed industrial control systems: a fault-tolerant architecture	distributed system;design process;performance monitoring;hardware software codesign;performance evaluation;fault tolerant;dependability validation;system performance;control system;fault tolerant system;fault tolerance;industrial control;distributed systems;manufacturing system;distributed architecture;design methodology	Nowadays, distributed architectures are the base of many manufacturing systems. Some aspects like fault-tolerance, system validation and design process are very important in the development of these systems. In this paper we study the dependability of three different architectures of a distributed system, and we show the development of both physical and logical fault injectors and the implementation of local performance monitors. We also study the impact of checkpointing mechanisms on the system performance in a control system based on a CAN network. Finally we propose a distributed system design methodology based on codesign.	control system;fault tolerance	José Carlos Campelo;Francisco Rodríguez;Alicia Rubio;Rafael Ors Carot;Pedro J. Gil;Lenin Lemus;José V. Busquets-Mataix;José Albaladejo;Juan José Serrano	1999	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/S0141-9331(99)00019-8	embedded system;fault tolerance;real-time computing;computer science;control system;distributed system security architecture;distributed design patterns	Embedded	-34.90414049696812	37.49387094027586	34833
89ed0f6b5a58e5a37984feb84a04cdb3de1b90b2	design and development of multi-tenant web framework	multi tenancy	Abstract In order to develop a multi-tenant web application, developer has to follow certain standards to enable multi-tenancy. Multi-Tenant Web Framework (MWF) provides standard set of components and utilities that expedites the process of developing multi-tenant functionalities and converts an ordinary web application in to an multi-tenant enabled application. MWF consist of features that include authentication, authorization, tenant tracking, application platform controller and securing tenant data in shared schema.	multitenancy;web framework	Sivakumar Kuppusamy;Devi Thirupathi;Vivekanandan Kaniappan	2018	IJSTM	10.1504/IJSTM.2018.10011467	web application;data mining;control theory;database;computer science;authorization;schema (psychology);multitenancy;web application security;authentication;web application framework	Web+IR	-47.592559750776026	55.416234541747734	34919
b745fd924b5fd7c9e6721788b7bf626ae86a12f6	protecting the web server and applications			web server;world wide web	Iain Franklin	2001	Computers & Security	10.1016/S0167-4048(01)01018-5	web service;web development;clickstream;web api;web page;database;world wide web;web server;application server	Crypto	-35.70712284822238	50.29591824957195	34948
169e4dcd2e3ba96d9dbd18ece8c3612c507702b7	measuring and analyzing emerging properties for autonomic collaboration service adaptation	collaborative work;collaborative environment;emergent properties;emergent behavior	Dynamic collaboration environments in which team member utilize different pervasive collaboration services for their collaborative work pose many challenges for service adaptation. Given a team, the underlying collaboration services must fulfil the team’s goal. Thus, it is not enough to adapt collaboration services to the context of an individual. One needs to understand the behavior of the team and the collaboration services in order to adapt these services. Though many research efforts aim at understanding team behavior at the human level, there is no such a framework that focuses on adapting collaboration services for teamwork. In this paper, we introduce a set of novel metrics that characterizes emergent behavior of teams. We present a team analysis and adaptation framework (TAAF) which monitors diverse collaboration services, analyzes and provides relevant metrics for understanding dynamic teams and for continuous team and service adaptation. This paper also discusses how TAAF can be used to support self-management of collaboration services for collaborative teams.	autonomic computing;autonomic networking;emergence;pervasive informatics;run time (program lifecycle phase);self-management (computer science)	Christoph Dorn;Hong Linh Truong;Schahram Dustdar	2008		10.1007/978-3-540-69295-9_15	simulation;computer science;knowledge management;services computing;emergence	HCI	-43.04468188417849	40.381481001342124	34949
1e8eac548830f42564c7ee82894b47871b306360	reactive security for smart grids using models@run.time-based simulation and reasoning	smart grid;reactive security;models runtime;model driven engineering;meta modeling;reasoning engine	Smart grids leverage modern information and communication technology to offer new perspectives to electricity consumers, producers, and distributors. However, these new possibilities also increase the complexity of the grid and make it more prone to failures. Moreover, new advanced features like remotely disconnecting meters create new vulnerabilities and make smart grids an attractive target for cyber attackers. We claim that, due to the nature of smart grids, unforeseen attacks and failures cannot be effectively countered relying solely on proactive security techniques. We believe that a reactive and corrective approach can offer a long-term solution and is able to both minimize the impact of attacks and to deal with unforeseen failures. In this paper we present a novel approach combining a Models@run.time-based simulation and reasoning engine with reactive security techniques to intelligently monitor and continuously adapt the smart grid to varying conditions in near real-time.	high-level programming language;real-time clock;real-time computing;real-time locating system;semantic reasoner;simulation;smart tv;smart meter	Thomas Hartmann;François Fouquet;Jacques Klein;Grégory Nain;Yves Le Traon	2014		10.1007/978-3-319-10329-7_9	real-time computing;simulation;computer science;computer security	AI	-58.27593771058156	50.741011218182685	34980
4bc2d4c2a69170b81c1929de172a4b484e40e0c5	a java architecture for dynamic object and framework customizations	object oriented language;design pattern	A collection of design patterns was described by Gamma Helm Johnson and Vlissides in Each pattern ensures that a certain aspect can vary over time for example the operations that can be applied to an object or the algorithm of a method The patterns are described by constructs such as the inheritance and reference relations attempting to emulate more dynamic relationships As a result the design patterns demonstrate how awkward it is to program natural concepts of reuse and evolution when using a traditional object oriented language We investigate the generic evolution patterns common among many design patterns and the role that language has in supporting evolution within various software architectures Class based models and languages generally require dynamic behavior to be implemented through explicit delegation using a technique similar to the state strategy and visitor design patterns The use of explicit delegation to achieve dynamic im plementation is awed in two respects From a design point of view the relation between an object s interface and its implementation is not ade quately captured From an implementation point of view additional vir tual invocation is required and the host reference this is not properly maintained Java member classes come close to providing the language construct necessary for clean implementation of the state design pattern however they still require explicit delegation and they do not support delegation to an external object Thus member classes do not support multiple object collaborations such as those described by the visitor pat tern Frameworks elevate encapsulation and reuse to the level of large grained components namely groups of collaborating classes The abstract model de ned in a framework is easily customized to an application speci c model through static subclassing and method overriding How ever it is often necessary for an application to dynamically customize a	algorithm;encapsulation (networking);hierarchical editing language for macromolecules;java;language construct;method overriding;point of view (computer hardware company);software design pattern;software development kit	Linda M. Seiter	1998		10.1007/3-540-49255-0_22	computer architecture;method;real-time computing;object model;computer science;object;common object request broker architecture;design pattern;programming language;object-oriented programming;object definition language	PL	-34.27031059201717	41.489100234544686	35066
71f1d438b0f625ea5071d89757c482a64a10ca32	implementation of dynamic inversion-based control of a pressurizer at the paks npp	safety critical software continuous systems nuclear engineering computing nuclear power stations power control pressure control programmable controllers;qa75 electronic computers computer science szamitastechnika;continuous systems;control systems;nuclear power stations;nuclear engineering computing;programmable controllers;szamitogeptudomany;pressure control;measurement system;plc;supervisor module;usa councils;control systems usa councils conferences;pressure controlling tank;dynamic inversion based control;design and implementation;pressurizer;safety critical software;hungary;nuclear power plant;continuous power controller;safety critical requirement dynamic inversion based control pressure controlling tank paks npp nuclear power plant pressurizer hungary plc measurement system continuous power controller supervisor module;conferences;safety critical requirement;paks npp;power control	The implementation of a dynamic inversion based controller of the pressure controlling tank located in the primary circuit of the Paks Nuclear Power Plant (NPP) in Hungary is presented in this paper The implemented controller has a distributed structure including measurement and control PLCs, a continuous power controller and a special supervisor module. The hardware and software design and implementation meet the safety-critical requirements imposed by the special nature of the controlled plant.	internet protocol suite;requirement;software design;transformer	István Varga;Gábor Szederkényi;Péter Gáspár;Jozsef Bokor	2008	2008 IEEE International Conference on Control Applications	10.1109/CCA.2008.4629677	control engineering;engineering;control theory;mechanical engineering	Embedded	-35.41759253832173	37.73860433604428	35068
43ca58d738e5bde73a2e324b766d66315a9cc30d	temporal analysis of api usage concepts	application program interfaces;data mining;software reusability;api usage concepts;api usage pattern detection;application programming interfaces;client change history;client program;software development;software reuse;temporal api usage pattern mining;temporal analysis;api usability;api usage;mining software repositories;software reuse;usage pattern	Software reuse through Application Programming Interfaces (APIs) is an integral part of software development. The functionality offered by an API is not always accessed uniformly throughout the lifetime of a client program. We propose Temporal API Usage Pattern Mining to detect API usage patterns in terms of their time of introduction into client programs. We detect concepts as distinct groups of API functionality from the change history of a client program. We locate those concepts in the client change history and detect temporal usage patterns, where a pattern contains a set of concepts that were added into the client program in a specific temporal order. We investigated the properties of temporal API usage patterns through a multiple-case study of three APIs and their use in up to 19 client software projects. Our technique was able to detect a number of valuable patterns in two out of three of the APIs investigated. Further investigation showed some patterns to be relatively consistent between clients, produced by multiple developers, and not trivially derivable from program structure or API documentation.	application programming interface;client (computing);code reuse;cognitive dimensions of notations;data mining;documentation;floor and ceiling functions;interaction;java platform, standard edition;open-source software;software development;structured programming	Gias Uddin;Barthélémy Dagenais;Martin P. Robillard	2012	2012 34th International Conference on Software Engineering (ICSE)		application programming interface;computer science;operating system;data mining;database;world wide web	SE	-56.638362826725185	35.46128587643867	35096
f482c2791947be93577363656cb0686389da6553	design and implementation of rfid based air-cargo monitoring system	tracking system;monitoring system;design and implementation;radio frequency identification	This paper deals with the design and implementation of radio-frequency identification (RFID) based cargo monitoring system which supports tracking and tracing in air-cargo operation. In order to apply a proper RFID technology, firstly we studied RF operational environment and tested different RFID frequencies. After finding a right technology (i.e. frequency), we designed and implemented tracking and tracing system applying EPC networks. We believe that our research will bring a guideline for developing RFID based tracking system for cargo operation.		Yoon Seok Chang;Min Gyu Son;Chang Heun Oh	2011	Advanced Engineering Informatics	10.1016/j.aei.2010.05.004	radio-frequency identification;embedded system;electronic engineering;real-time computing;tracking system;computer science;engineering;artificial intelligence	DB	-48.90142771878045	47.163292130514314	35143
a936b6bbb7b7073f7888db2045b8aa8704424a9a	methodology for information management and data assessment in cloud environments	trust;energy;information assessment;data model;monitoring;risk;information management;architectural model;cost;article	The emergence of cloud technologies has affected the service computing ecosystem introducing new roles and relationships as well as new architectural and business models. Along with the increase of capabilities and potentials of the service providers comes the increase of the information available and issues to efficiently manage it. In this paper, an architectural approach is presented that involves a combination of a cloud-enabled data model, the monitoring infrastructure and the establishment of assessment mechanisms which are based on factors such as trust, risk, energy and cost (TREC factors). This architectural model discusses the monitoring features as well as the how the assessment functionalities can work together with other components to produce a self-reliant cloud ecosystem preventing any fails during the service lifecycle. This paper elaborates on how the self-management, by using decision-making processes, can maximise business level objectives of the providers. The results presented in the paper show how the suggested architecture can help develop efficient cloud architecture. Methodology for Information Management and Data Assessment in Cloud Environments	aggregate data;cloud computing;data model;ecosystem;elasticity (cloud computing);emergence;entity;fault tolerance;high- and low-level;high-level programming language;information management;interoperability;proactive parallel suite;provisioning;requirement;scalability;self-information;self-management (computer science);semiconductor consolidation;sensor;services computing;text retrieval conference;user requirements document;verification and validation	Mariam Kiran;Gregory Katsaros;Jordi Guitart;Juan Luis Prieto	2014	IJGHPC	10.4018/IJGHPC.2014100104	energy;data model;computer science;knowledge management;risk;information management;trustworthy computing;computer security;quantum mechanics	DB	-49.10135620986651	45.11210587551736	35181
40d452727cb55989ca99ca6ee1bba031943981a7	lessons learned with the systems security engineering capability maturity model	security engineering;system engineering;capability maturity model process improvement security system engineering improvement infowar assurance;improvement;reference model;system security;infowar;assurance;lessons learned;capability maturity model;secure system;process improvement;systems engineering and theory capability maturity model national security appraisal information security coordinate measuring machines spice history permission aggregates;security;national security agency	This paper describes the principles upon which the SSECMM is based, the structure of the model, and its use in appraisals. The paper discusses ex-periences in developing and piloting the model and method, and comments on the potential for using the model in process-based assurance.	capability maturity model;security engineering	Rick Hefner	1997		10.1145/253228.253454	software security assurance;computer security model;standard of good practice;certified information security manager;cloud computing security;reliability engineering;certified information systems security professional;reference model;united states national security agency;security information and event management;security engineering;leancmmi;computer science;systems engineering;engineering;information security;security service;computer security;capability maturity model	SE	-56.5077169152045	48.450483316864165	35204
d25f7dd2414aeffcb849eeab412d28823111af06	web testing made easy	automated testing;application development;test driven development;web application;automatic acceptance test;open source;change control	In this paper we describe WebTest, an Open Source tool for automated testing of web applications. In particular we will show how to quickly create tests that shine with excellent maintainability and runtime performance as well as perfect integration in the application development cycle.	run time (program lifecycle phase);test automation;web application;web testing	Marc Guillemot;Dierk König	2006		10.1145/1176617.1176677	keyword-driven testing;change control;test-driven development;web application;web modeling;computer science;acceptance testing;database;programming language;rapid application development;world wide web;web testing	SE	-54.12091066861692	38.201638764919714	35232
1b56f0b32768722c54b430513c09ec0494b68241	parameter dependencies for component reliability specifications	prediction method;reliability;management system;component based systems;transition probability;software systems;cbse;markov model;parameter dependencies;software component;usage profile;prediction	Predicting the reliability of a software system at an architectural level during early design stages can help to make systems more dependable and avoid costs for fixing the implementation. Existing reliability prediction methods for component-based systems use Markov models and assume that the software architect can provide the transition probabilities between individual components. This is however not possible if the components are black boxes, only at the design stage, or not available for testing. We propose a new modelling formalism that includes parameter dependencies into software component reliability specifications. It allows the software architect to only model a system-level usage profile (i.e., parameter values and call frequencies), which a tool then propagates to individual components to determine the transition probabilities of the Markov model. We demonstrate the applicability of our approach by modelling the reliability of a retail management system and conduct reliability predictions.		Heiko Koziolek;Franz Brosch	2009	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2009.09.026	reliability engineering;markov chain;real-time computing;prediction;computer science;software reliability testing;component-based software engineering;reliability;management system;markov model;software system	DB	-47.56783495502916	34.6623578958917	35251
e3d2f4df72ea7fae85a8874d8a3170c134f1ee44	a communication architecture for distributed real-time robot control		Due to their continuous development in terms of performance and also reliability, standard PCs offer an adequate platform as basis for the development and implementation of control systems. The efficient and robust control of highly dynamic systems requires the implementation of very short control cycles (fcycle 1 kHz). This requires real-time and high performance communication mechanisms implemented in software with dedicated hardware support providing response times in the range of a couple of s. Such time constraints are typically not covered by commercial off-the-shelf (COTS) products in the full scale. Therefore, we intro- duce a communication architecture for a PC-based control platform that spans the complete chain from software development to integration of sensor and actuator de- vices with respect to real-time and performance requirements for high end control	real-time transcription;robot control	Yannick Dadji;Harald Michalik;Nnamdi Kohn;Jens Steiner;Guido Beckmann;Tobias Möglich;J. Uwe Varchmin	2011		10.1007/978-3-642-16785-0_13	control engineering;embedded system;real-time computing;engineering	Robotics	-34.312415984578145	37.56504902368823	35273
0edea4da0452ce2e929d26da44fa9f3a57d49dd0	research on real-time software development approach	platform specific model;platform independent model;aspect oriented;model driven architecture;real time systems	Model Driven Architecture (MDA) is a development method of which can generate useable so ftware products directly by the model. It includes a serie s of standardized modeling, transformation rules and other relevant standards architecture. Real-Time systems h ave been applied in many areas widely, but they have ma ny nonfunctional requirements, which always crosscut the whole system modules. That may cause the code tangle and scatter, make the systems hard to design, reuse and maintain , and affect performance of systems badly. AOP is a new s oftware development paradigm, which could attain a higher l evel of separation of concerns in both functional and no nfunctional matters by introducing aspect, for the implementation of crosscutting concerns. Different aspects can be designed separately, and woven into systems. This article introduces the technology of MDA, aspect-or iented, real-time systems and UML. This article takes the Asp ectoriented to the MDA modeling by the UML extension mechanisms, and presents a method, which is AspectOriented MDA. In this article, UML profile is utili zed to construct the meta-modal specifications respectivel y for common Aspect-Oriented and AspectJ. So the core bus iness logic and the crosscutting aspects can be modeled a s separate, modular Aspect-Oriented PIM's and PSM's. The authors analysis non-functional requirements of the realtime systems, and then apply aspect-oriented MDA modeling to develop an example of real-time systems , and propose how to model aspects of timer and real-time constraints. Finally, in order to more clearly understand how to complete the MDA in the aspect-oriented mode ling, especially in real-time system. This paper through t he example of a real-time system is discussed. The syst em simulates the operation of self-automatic washer pr ocess.	aspect-oriented software development;aspectj;cross-cutting concern;functional requirement;modal logic;model-driven architecture;non-functional requirement;profile (uml);programming paradigm;real-time clock;real-time computing;real-time transcription;separation of concerns;software development process;timer;unified modeling language;usability	Wei Qiu;Li-Chen Zhang	2012	JSW	10.4304/jsw.7.7.1593-1600	real-time computing;aspect-oriented programming;computer science;operating system;applications of uml;database;programming language;computer security;systems design	Embedded	-41.42424347269485	33.12190022077049	35292
2a9bed35b06bdc09191397f7b7c03a6509cb3c07	loosely coupled service composition for deployment of next generation service overlay networks	service overlay network;service oriented architecture internet;value added services;service composition;context aware;service provider;proof of concept;internet;service creation;next generation;mashup technologies loosely coupled service composition next generation service overlay networks user centric approach user generated contents ieee ngson standard functional architecture value added services context awareness dynamic adaptation self organizing management service oriented architecture user generated services;service oriented architecture;dynamic adaptation;service oriented architecture mashups couplings next generation networking protocols user centered design	In order to provide various attractive services to users, a user-centric approach such as usergenerated contents and services and a platform which can create a service easily and efficiently have been studied and designed during the past two decades. The IEEE NGSON standard has defined a functional architecture that provides advanced overlay functions for value-added services by collaboration of services with features of context awareness, dynamic adaptation and self-organizing management. Its service composition mechanism is comparable with service oriented architecture. However, due to SOA's complexity, it has not become as popular for user-generated services and contents as so-called mashup technologies, which also provide attractive and simple service creation. In this article we propose the adoption of loosely coupled service composition into the NGSON framework that can easily integrate existing communications and data services with little to no dependence on the underlying implementation and service description language details. Furthermore, the NGSON prototype supporting our proposal is presented as a proof of concept and to verify its advantages for operators, service providers, and end users.	context awareness;ieee 754-1985;loose coupling;mashup (web application hybrid);next-generation network;organizing (structure);overlay network;prototype;self-organization;service composability principle;service-oriented architecture;software deployment;user-generated content	Satoshi Komorita;Manabu Ito;Hidetoshi Yokota;Christian Makaya;Benjamin Falchuk;Dana Chee;Subir Das	2012	IEEE Communications Magazine	10.1109/MCOM.2012.6122534	service provider;service level requirement;mobile qos;the internet;service product management;differentiated service;computer science;service delivery framework;service-oriented architecture;service design;database;service;service layer;service discovery;service desk;data as a service;customer service assurance;law;proof of concept;world wide web;computer network;service system	Networks	-36.929875305224286	49.485206352674105	35356
1e9a483bcc26054247eb20699499692d71fed4da	an objective reuse metric: model and methology	source code;software reuse	Software reuse is an eeective way to gain productivity in constructing software systems. In order to continuously monitor the progress of reuse in the context of a project, we need an objective and repeatable way to measure the extent of reuse. This paper proposes a model and methodology to automatically compute a general objective reuse measure from the source code of applications and reusable software repositories. Unlike consumer-oriented reuse measures in the literature, this measure is useful from the viewpoints of both reuse consumers and reuse producers. The former can use it to estimate the extent of reuse in their applications while the latter can use it to determine the impact of certain reusable components. In contrast to other objective code-based measures , our measure is both complete and precise in that it is calculated based on and only on those code entities that are essential to the functioning of an application. While the model and methodology are largely language-independent, we have implemented tools to compute the reuse measure from C code. We report experiences from using the measure on a few large software systems and discuss its use in discovering and encouraging reuse.	bsd;code reuse;complex event processing;computable function;entity;language-independent specification;object file;precision and recall;prototype verification system;software portability;software repository;software system;usability	Yih-Farn Robin Chen;Balachander Krishnamurthy;Kiem-Phong Vo	1995		10.1007/3-540-60406-5_10	systems engineering;software system;reuse;software;dead code;source code;computer science	SE	-58.0867911527682	32.852537245485394	35364
e9e3c2c6531fa37e2e36f2b089fa250400fb0854	information sharing with handheld appliances	information sharing;allgemeine werke;000 informatik;handheld device;user interaction;informationswissenschaft	Handheld appliances such as PDAs, organisers or electronic pens are currently very popular. Handhelds are used to enter and retrieve useful information, e.g., dates, to do lists, memos and addresses. They are viewed as standalone devices and are usually not connected to other handhelds, thus sharing data between two handhelds is very difficult. Rudimentary infrastructures to exchange data between handhelds exist, but they are not designed for a seamless integration into handheld applications. The fundamental different nature of handheld devices to desktop computers leads to a number of issues. In this paper, we first analyse the specific characteristics of handheld devices, the corresponding applications and how users interact with handhelds. We identify three basic requirements for a successful realisation: privacy, awareness and usability. Based on these considerations, we present our own approach.	desktop computer;handheld game console;mobile device;personal digital assistant;privacy;requirement;seamless3d;usability	Jörg Roth	2001		10.1007/3-540-45348-2_23	human–computer interaction;computer science;internet privacy;world wide web	Mobile	-38.17606959015759	49.61065673419226	35383
448785f9858b51886f4069459cf165c04ba8dee1	new design error modeling and metrics for design validation		When simulation is used for design verification, a subset of simulation input patterns is used, since exhaustive simulations usually not practical. In this case, the immediate question is how much of the design has been verified? To provide a measure of the simulation pattern coverage based on design error modeling, a new simulation coverage metric is introduced. This measure is useful for obtaining insight into the actual level of design validation, since it provides more realistic results than those which are presently available.	simulation;software design	Sungho Kang;Stephen A. Szygenda	1992			iterative design;embedded system;computer architecture;verification and validation of computer simulation models;logic synthesis;probabilistic design;simulation software;idef4;computer science;computer-automated design;theoretical computer science;logic simulation;design for testing;functional verification;generative design;computer engineering	EDA	-48.41106970749957	35.22790207877491	35431
b99953c8b86c684aeb3860008527a5f47a27421e	dangers and joys of stock trading on the web: failure characterization of a three-tier web service	databases;multi tier application failure characterization detection;software fault tolerance;web services commerce software fault tolerance;detection;commerce;browsers;containers databases web services servers browsers;servers;multi tier application;application specific checks latent software faults failure characterization three tier web service stock trading generic consistency check;web services;failure characterization;containers	Characterizing latent software faults is crucial to address dependability issues of current three-tier systems. A client should not have a misconception that a transaction succeeded, when in reality, it failed due to a silent error. We present a fault injection-based evaluation to characterize silent and non-silent software failures in a representative three-tier web service, one that mimics a day trading application widely used for benchmarking application servers. For failure characterization, we quantify distribution of silent and non-silent failures, and recommend low cost application-generic and application-specific consistency checks, which improve the reliability of the application. We inject three variants of null-call, where a callee returns null to the caller without executing business logic. Additionally, we inject three types of unchecked exceptions and analyze the reaction of our application. Our results show that 49% of error injections from null-calls result in silent failures, while 34% of unchecked exceptions result in silent failures. Our generic-consistency check can detect silent failures in null-calls with an accuracy as high as 100%. Non-silent failures with unchecked exceptions can be detected with an accuracy of 42% with our application-specific checks.	application server;business logic;client (computing);concurrency (computer science);dependability;enterprise javabeans;eventual consistency;exception handling;experiment;fault injection;machine learning;multitier architecture;server (computing);web service;world wide web	Fahad A. Arshad;Saurabh Bagchi	2011	2011 IEEE 30th International Symposium on Reliable Distributed Systems	10.1109/SRDS.2011.27	web service;reliability engineering;computer science;multitier architecture;operating system;database;distributed computing;world wide web;computer security;software fault tolerance;server;computer network	Arch	-55.805747748717565	59.10103884297138	35446
185d9ad032149734b18041ac658e2680707d8f42	online prediction and improvement of reliability for service oriented systems	software reliability ports computers runtime software data models predictive models;system reconfiguration online data analysis reliability improvement reliability prediction spectrum based localization;component replica online prediction service oriented systems software quality software reliability prediction faulty components service composition reliability online improvement time intervals spectrum fault localization technique sfl technique system reliability;software reliability service oriented architecture software quality	Reliability is an important metric for measuring the quality of software. Many methods have been proposed for online predicting and improving software reliability, but most of them have the following weakness: they are not able to predict software reliability on different time intervals and to locate the faulty components that cause the declining of the reliability either. This paper proposes a new method for online improvement of reliability of service composition. We use monitored failure data at ports of services to predict the reliabilities of service composition on different time intervals. If the predicted reliability is lower than the expected value, then we locate the faulty components that cause the declining of the reliability by using an improved spectrum-fault-localization (SFL) technique. The system can be automatically reconfigured to improve the system reliability by adding a component replica or replacing the faulty component. An Online Shop example is used to demonstrate the effectiveness of our method.	algorithm;real-time computing;reliability engineering;runtime system;service composability principle;software quality;software reliability testing;static program analysis;time complexity;tracing (software)	Zuohua Ding;Ting Xu;Tiantian Ye;Yuan Zhou	2016	IEEE Transactions on Reliability	10.1109/TR.2015.2504720	reliability engineering;availability;verification and validation;real-time computing;computer science;software reliability testing;software construction;database;software deployment;software system	SE	-61.10332147272063	35.82417023476759	35452
f58c60a1f361acf2cccea7008243c6b6115dc748	understanding javascript event-based interactions with clematis	fault localization;event based interactions;program comprehension;web applications;javascript	Web applications have become one of the fastest-growing types of software systems today. Despite their popularity, understanding the behavior of modern web applications is still a challenging endeavor for developers during development and maintenance tasks. The challenges mainly stem from the dynamic, event-driven, and asynchronous nature of the JavaScript language. We propose a generic technique for capturing low-level event-based interactions in a web application and mapping those to a higher-level behavioral model. This model is then transformed into an interactive visualization, representing episodes of triggered causal and temporal events, related JavaScript code executions, and their impact on the dynamic DOM state. Our approach, implemented in a tool called Clematis, allows developers to easily understand the complex dynamic behavior of their application at three different semantic levels of granularity. Furthermore, Clematis helps developers bridge the gap between test cases and program code by localizing the fault related to a test assertion. The results of our industrial controlled experiment show that Clematis is capable of improving the comprehension task accuracy by 157% while reducing the task completion time by 47%. A follow-up experiment reveals that Clematis improves the fault localization accuracy of developers by a factor of two.	assertion (software development);behavioral modeling;causal filter;document object model;event-driven programming;fastest;high- and low-level;interaction;interactive visualization;internationalization and localization;javascript;software system;test assertion;test case;web application	Saba Alimadadi;Sheldon Sequeira;Ali Mesbah;Karthik Pattabiraman	2016	ACM Trans. Softw. Eng. Methodol.	10.1145/2876441	web application;real-time computing;computer science;theoretical computer science;unobtrusive javascript;javascript;programming language;world wide web	SE	-55.57434724917498	37.632463197711594	35496
2243b6ecdb68ebd7f070b586ab3a8094550dfe58	smapreduce: a programming pattern for wireless sensor networks	macro programming;abstractions;sensor network;wireless sensor network;complex data;patterns;article;wireless sensor networks;home automation	Wireless Sensor Networks (WSNs) are increasingly used in various application domains like home-automation, agriculture, industries and infrastructure monitoring. As applications tend to leverage larger geographical deployments of sensor networks, the availability of an intuitive and user friendly programming abstraction becomes a crucial factor in enabling faster and more efficient development, and reprogramming of applications. We propose a programming pattern named sMapReduce, inspired by the Google MapReduce framework, for mapping application behaviors on to a sensor network and enabling complex data aggregation. The proposed pattern requires a user to create a network-level application in two functions: sMap and Reduce, in order to abstract away from the low-level details without sacrificing the control to develop complex logic. Such a two-fold division of programming logic is a natural-fit to typical sensor networking operation which makes sensing and topological modalities accessible to the user.	data aggregation;high- and low-level;home automation;mapreduce;reduce;sensor;software design pattern;usability	Vikram Gupta;Eduardo Tovar;Luís Miguel Pinho;Junsung Kim;Karthik Lakshmanan;Ragunathan Rajkumar	2011		10.1145/1988051.1988059	sensor web;embedded system;real-time computing;wireless sensor network;computer science;distributed computing;key distribution in wireless sensor networks;mobile wireless sensor network;visual sensor network	Mobile	-41.03523469039111	48.830238689452514	35553
22c488a0f09f69c11a81c032fa5499dfd5bb51b0	fmeda-based fault injection and data analysis in compliance with iso-26262		With the growing demand on automotive electronics for the advanced driver assistance systems and autonomous driving, the functional safety becomes one of the most important issues in the hardware development. Thus, the safety standard for automotive E/E system, ISO-26262, becomes state-of-the-art guideline to ensure that the required safety level can be achieved. In this study, we base on ISO-26262 to develop a FMEDA-based fault injection and data analysis framework. The main contribution of this study is to effectively reduce the effort for generating FMEDA report which is used to evaluate hardware's safety level based on ISO-26262 standard.	autonomous car;fail-safe;failure cause;failure modes, effects, and diagnostic analysis;fault injection;simulation	Kuen-Long Lu;Yung-Yuan Chen;Li-Ren Huang	2018	2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W)	10.1109/DSN-W.2018.00075	real-time computing;advanced driver assistance systems;fault injection;automotive engineering;automotive electronics;data modeling;computer science;functional safety;guideline;automotive industry	Robotics	-56.822885906245844	45.54408760334776	35625
0bee387ff5485315e9212c2195c71c8d0e23ea1a	safe kernel extensions without run-time checking	assembly languages;packet filtering;fields computer programs;packet switching;software engineering;systems analysis;executive routines;proof carrying code;computer program reliability;fault isolation;operating systems computers;system safety;computer program verification;binary processors	This paper describes a mechanism by which an oper ating system kernel can determine with certainty that it is safe to execute a binary supplied by an untrusted source The kernel rst de nes a safety policy and makes it public Then using this pol icy an application can provide binaries in a spe cial form called proof carrying code or simply PCC Each PCC binary contains in addition to the native code a formal proof that the code obeys the safety policy The kernel can easily validate the proof with out using cryptography and without consulting any external trusted entities If the validation succeeds the code is guaranteed to respect the safety policy without relying on run time checks The main practical di culty of PCC is in gener ating the safety proofs In order to gain some prelim inary experience with this we have written several network packet lters in hand tuned DEC Alpha as sembly language and then generated PCC binaries for them using a special prototype assembler The PCC binaries can be executed with no run time over head beyond a one time cost of to milliseconds for validating the enclosed proofs The net result is that our packet lters are formally guaranteed to be safe and are faster than packet lters created using Berkeley Packet Filters Software Fault Isolation or safe languages such as Modula This research was sponsored in part by the Advanced Research Projects Agency CSTO under the title The Fox Project Advanced Languages for Systems Software ARPA Or der No C issued by ESC ENS under Contract No F C The views and conclusions contained in this document are those of the authors and should not be interpreted as repre senting the o cial policies either expressed or implied of the Advanced Research Projects Agency or the U S Government Appeared in Proceedings of the Second Symposium on Operating Systems Design and Implementation OSDI Seattle Washington October pp Introduction In this paper we address the problem of how an op erating system kernel or a server can determine with absolute certainty that it is safe to execute code sup plied by an application or other untrusted source We propose a mechanism that allows a kernel or server from now on referred to as the code con sumer to de ne a safety policy and then verify that the policy is respected by native code binaries sup plied to it by an untrusted code producer In contrast to some previous approaches we do not rely on the usual authentication or code editing mechanisms Instead we require that the code pro ducer creates its binaries in a special form which we call proof carrying code or simply PCC A PCC binary contains an encoding of a formal proof that the enclosed native code respects the safety policy The proof is structured in such a way that makes it easy and foolproof for any agent and in particu lar the code consumer to verify its validity without using cryptographic techniques or consulting with external trusted entities there is also no need for any program analysis code editing compilation or interpretation Besides being safe PCC binaries are also extremely fast because the safety check needs to be conducted only once after which the consumer knows it can safely execute the binary without any further run time checking In a PCC binary the proof is linked with the na tive code so that its validity guarantees the code s safety Furthermore proof carrying code is tamper proof the consumer can easily detect most attempts by any malicious agent to forge a proof or modify the code Tampering can go undetected only if the adulterated code is still guaranteed to respect the consumer de ned safety policy Another feature of the PCC method is that the proof checking algo rithm is very simple allowing fast and easy to trust implementations The safety policy is de ned and published by the code consumer and comprises a set of proof formation rules along with a set of preconditions Safety policies can be de ned to stipulate standard requirements such as memory safety as well as more abstract and ne grained guarantees about the in tegrity of data abstraction boundaries To take a simple example consider the abstract type of le descriptors In this case a client is said to preserve the abstraction boundaries if it does not exploit the fact that le descriptors are represented as integers by incrementing a le descriptor for example Although we have worked out many of the theo retical underpinnings for PCC and indeed most of the theory is based on old and well known principles from logic type theory and formal veri ca tion there are many di cult problems that remain to be solved In particular we do not know at this point the most practical way to generate the proofs We have thus set out to gain some prelim inary experience both to measure the bene ts and to identify the practical problems In the experiments reported in this paper we have in fact achieved fully automatic proof genera tion In general however this problem is similar to program veri cation and is not completely automat able Actually the problem is somewhat easier than veri cation because we have the option of inserting extra run time checks as is done in Software Fault Isolation which would have the e ect of simplifying the proving process at the cost of reducing perfor mance By extra we mean run time checks that are not intrinsically a part of the algorithm of the extension code For example SFI will actually edit the code and insert extra checks PCC does not normally do this Fortunately we have not yet had any need or desire to insert extra run time checks in any of our PCC examples Still automation of proof generation remains as one of the most seri ous obstacles to widespread practical application of PCC In our main experiment we implemented several network packet lters in DEC Alpha assem bly language and then used a special prototype assembler to create PCC binaries for them We were motivated to use an unsafe assembly language in or der to place equal emphasis on both performance and safety as well as to demonstrate the generality of the PCC approach In addition to the assem bler we implemented a proof validator that accepts a PCC binary checks its safety proof and if it is found to be valid loads the enclosed native code and sets it up for execution The results of this and other experiments are en couraging For our collection of packet lters we are able to automate completely the generation of the PCC binaries The one time cost of loading and checking the validity of the safety proofs is between and milliseconds Because a safety proof guar antees safety our hand tuned packet lters can be executed safely in the kernel address space without adding any run time checks Predictably they are much faster than safe packet lters produced by any other means with which we are familiar We believe that our early results show that proof carrying code is a new point in the design space that is worthy of further attention and study This pa per presents an overview of the approach We begin with a brief overview of the process of generating and validating the safety proofs Then we make this more concrete by showing how a safety policy can be de ned and proofs created for a generic as sembly language This is followed by a description of our main experiment involving safe network packet lters The benchmark results provide some prelim inary indication that the PCC methodology has the potential to surpass traditional approaches from a safety point of view while maintaining or improv ing performance In particular we show that PCC leads to faster and safer packet lters than previous approaches to code safety in systems software in cluding Berkeley Packet Filters Software Fault Isolation and programming in the safe subset of Modula Finally we conclude with a discussion of the remaining di culties and speculate on what might be necessary to make the approach work on a practical scale CPU CODE PRODUCER USER PROCESS UNTRUSTED CLIENT CODE CONSUMER OS KERNEL NETWORK SERVER	abstract type;abstraction (software engineering);algorithm;assembly language;authentication;automated proof checking;benchmark (computing);binary file;central processing unit;cryptography;dec alpha;data descriptor;esc/java;entity;experiment;forge;formal proof;formation rule;genera;icy;loadable kernel module;lotus improv;machine code;malware;memory safety;modula;naruto shippuden: clash of ninja revolution 3;network packet;numerical aperture;operating system;operational amplifier;portable c compiler;precondition;program analysis;proof-carrying code;prototype;provider-aggregatable address space;requirement;run time (program lifecycle phase);safety engineering;sandbox (computer security);server (computing);tamper resistance;type theory;validator	George C. Necula;Peter Lee	1996		10.1145/238721.238781	systems analysis;real-time computing;computer science;theoretical computer science;operating system;distributed computing;programming language;system safety;computer security;fault detection and isolation;packet switching;assembly language	OS	-56.29575807423608	55.49148385422448	35648
b68953a54bdf10c9a6bcb261b4b9be58e8c887d3	flexible key distribution for scada network using multi-agent system	electric power industry;supervisory control and data acquisition system;multi agent system;network security;energy industry;multiagent systems scada systems power system security communication system control control systems industrial plants communication networks computer security electrical equipment industry gas industry;terrorism gas industry multi agent systems petroleum industry scada systems security of data;cyber security;multi agent systems;cyber attacks;community networks;scada network;oil industry;petroleum industry;scada system;electric power;flexible key distribution concept;scada systems;critical infrastructure;flexible key distribution concept scada network multi agent system supervisory control and data acquisition system cyber attacks cyber security energy industry electric power industry gas industry oil industry terrorism;security of data;gas industry;terrorism;key distribution	SCADA (Supervisory Control and Data Acquisition) system has been used for remote measurement and control on the critical infrastructures as well as modern industrial facilities. As cyber attacks increase on communication networks, SCADA network has been also exposed to cyber security problems. Especially, SCADA systems of energy industry such as electric power, gas and oil are vulnerable to targeted cyber attack and terrorism. Recently, many research efforts to solve the problems have made progress on SCADA network security. In this paper, flexible key distribution concept is proposed for improving the security of SCADA network using multi-agent system (MAS).	computer security;data acquisition;key distribution;multi-agent system;network security;telecommunications network	Hak-Man Kim;Dong-Joo Kang;Tai-Hoon Kim	2007	2007 ECSIS Symposium on Bio-inspired, Learning, and Intelligent Systems for Security (BLISS 2007)	10.1109/BLISS.2007.22	embedded system;dnp3;engineering;operations management;computer security;scada	Security	-57.525387723670896	51.19666715989181	35674
b412a66110edbe13402eb6af23066b882516ef63	cell-phone based user activity recognition, management and utilization	information retrieval;active rfid tags;cellular phones radiofrequency identification active rfid tags transmitters information retrieval semantic web privacy information services web sites internet;information services;semantic web technology;user profile;internet;web sites;transmitters;semantic web;radiofrequency identification;privacy;cellular phones;activity recognition	"""This paper presents a framework for aggregating and updating use activity data (called user profile in this paper) recognized by a cell phone equipped with active RFID transmitter/receiver. Each component of user profile information comes from a different source and is combined each other at the Profile Aggregator (PA), which is user profile management server. In the PA, a piece of information is connected to other related information and this info-linkage facilitates the retrieval of detailed information easily. This function has been implemented based on semantic web technology. The PA has also an important role of privacy control in terms of disseminating a subset of user profile, including a query interface, trigger interface. And we finally describe """"Profile Blog,"""" which is a visualization service of aggregated user profile, and a cell phone application called """"Remembrance Viewer"""" through which the past user activities at each location can be seen."""	activity recognition;blog;cell (microprocessor);linkage (software);mobile app;mobile phone;radio-frequency identification;semantic web;server (computing);transmitter;user profile	Daisuke Morikawa;Masaru Honjo;Akira Yamaguchi;Satoshi Nishiyama;Masayoshi Ohashi	2006	7th International Conference on Mobile Data Management (MDM'06)	10.1109/MDM.2006.45	web service;transmitter;the internet;computer science;semantic web;web navigation;internet privacy;privacy;world wide web;information retrieval;information system;activity recognition	Mobile	-38.706741161944834	50.05058894469171	35710
1662e306fac0007b9462e79b8a379cdec37df2e6	a search-based ocl constraint solver for model-based test data generation	test data;software testing;unified modeling language testing search problems data models context genetic algorithms software engineering;uml;software systems;test data generation;genetic algorithm search based ocl constraint solver model based test data generation systematic testing solutions complex industrial software systems unified modeling language object constraint language white box testing search heuristics;unified modeling language constraint handling formal verification search problems;formal verification;empirical evaluation uml ocl search based testing test data;ocl;search based testing;unified modeling language;industrial application;constraint handling;model based testing;genetic algorithm;search problems;empirical evaluation;object constraint language	Model-based testing (MBT) aims at automated, scalable, and systematic testing solutions for complex industrial software systems. To increase chances of adoption in industrial contexts, software systems should be modeled using well-established standards such as the Unified Modeling Language (UML) and Object Constraint Language (OCL). Given that test data generation is one of the major challenges to automate MBT, this is the topic of this paper with a specific focus on test data generation from OCL constraints. Though search-based software testing (SBST) has been applied to test data generation for white-box testing (e.g., branch coverage), its application to the MBT of industrial software systems has been limited. In this paper, we propose a set of search heuristics based on OCL constraints to guide test data generation and automate MBT in industrial applications. These heuristics are used to develop an OCL solver exclusively based on search, in this particular case genetic algorithm and (1+1) EA. Empirical analyses to evaluate the feasibility of our approach are carried out on one industrial system.	code coverage;genetic algorithm;heuristic (computer science);like button;model-based testing;object constraint language;scalability;software system;software testing;solver;test data generation;unified modeling language;white-box testing	Shaukat Ali;Muhammad Zohaib Z. Iqbal;Andrea Arcuri;Lionel C. Briand	2011	2011 11th International Conference on Quality Software	10.1109/QSIC.2011.17	unified modeling language;test data generation;computer science;theoretical computer science;software engineering;database;programming language;object constraint language	SE	-58.671933579716246	33.97646849105828	35761
688850039d75997ac39e9982598f2e93ad39a7cc	modeling chp descriptions in labeled transitions systems for an efficient formal validation of asynchronous circuit specifications	petri net;extended finite state machine;model checking;asynchronous circuit	This work addresses the analysis and validation of CHP specifications for asynchronous circuits, using property verification tools. CHP semantics, initially given in terms of Petri Nets, are reformulated as labeled transition systems. Circuit specifications are translated into an intermediate format (IF) based on communicating extended finite state machines. They are then validated using the IF environment, which provides model checking and bi-simulation tools. The direct translation must be optimized to delay state explosion. Performance studies are reported.	asynchronous circuit;finite-state machine;model checking;petri net;simulation;verification and validation	Dominique Borrione;Menouer Boubekeur	2003		10.1007/1-4020-7991-5_18	embedded system;real-time computing;computer science;theoretical computer science	EDA	-33.85070994144083	32.358213369097335	35803
60b5417dc49353b627c2453c81b9f3efac22a374	software model checking for mobile security - collusion detection in \mathbb k k		Mobile devices pose a particular security risk because they hold personal details and have capabilities potentially exploitable for eavesdropping. The Android operating system is designed with a number of built-in security features such as application sandboxing and permission-based access control. Unfortunately, these restrictions can be bypassed, without the user noticing, by colluding apps whose combined permissions allow them to carry out attacks that neither app is able to execute by itself. In this paper, we develop a software model-checking approach within the (mathbb {K}) framework that is capable to detect collusion. This involves giving an abstract, formal semantics to Android applications and proving that the applied abstraction principles lead to a finite state space.	mobile security;model checking	Irina Mariuca Asavoae;Nguyen Hoang Nga;Markus Roggenbach	2018		10.1007/978-3-319-94111-0_1	access control;collusion;computer security;model checking;software;android (operating system);mobile device;sandbox (computer security);computer science;eavesdropping	Logic	-54.64395174349118	53.801513516596174	35837
62034c4261bab244b8acd132781be79053c32516	poster abstract: scaling iot device apis and analytics		Many IoT applications consist of two types of actions: interaction with the device, which can be sensors or actuators, and interaction with the data, for example, to reveal insights. In this poster, we introduce a software stack that provides these functionalities in a scalable manner. The API for device interaction is designed with generality in mind so that widest possible array of devices are supported and in large numbers. The analytics framework, called Composer, is designed to allow user code to be easily integrated into data analytics. We present the design, describe the implementation and deployment, and present some evaluation results. We share the performance data from a live deployment with tens of thousands of active users to demonstrate the scalability of the design.	application programming interface;composer;mind;scalability;sensor;software deployment	Omprakash Gnawali;David Moss;Dmitry Shirkalin;Russ Clark;Brian Jones;William Eason	2016	2016 15th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN)		embedded system;computer science;sensor;operating system;data analysis;software analytics;world wide web;server;actuator	Mobile	-42.1456799940163	48.12764651564895	35869
da1e2c19ca4c7a7ce9f48fb77ec1815e82297a2a	omniware: a universal substrate for web programming	fault isolation;web pages;programming language;distributed computing;web development;operating system	This paper describes Omniware, a system for producing and executing mobile code. Next generation Web applications will use mobile code to specify dynamic behavior in Web pages, implement new Web protocols and data formats, and dynamically distribute computation between servers and browsers. Like all mobile code systems, Omniware provides portability and safety. The same compiled Omniware module can be executed transparently on different machines, and a module’s access to host resources can be precisely controlled. In addition to portability and safety, Omniware has two unique features. First, Omniware is open. Omniware uses software fault isolation (SFI) to enforce safe execution of standard programming languages, enabling Web developers to leverage the vast store of existing software and programming expertise. For example, Omniware developers can use C++ to create programs for Web pages. Second, Omniware is fast. We evaluated Omniware under the Solaris 2.4 operating system on a SPARCstation 5 using eight C benchmark programs, including five programs from the C SPEC92 benchmark suite. We evaluated the performance of Omniware in two ways. First, we showed that Omniware modules can be represented compactly, reducing the space consumption compared to SunPro cc shared object files by an average of 38%. Second, we showed that Omniware modules execute at near native speeds. Including the runtime overhead necessary to ensure that Omniware modules are both portable and safe, our benchmark programs ran within 6% of native performance.	benchmark (computing);c++;code mobility;compiler;computation;desktop computer;fault detection and isolation;high- and low-level;internet;library (computing);low-level programming language;object file;operating system;overhead (computing);programmer;sandbox (computer security);software portability;virtual machine;visual basic;web developer;web development;web page	Steven Lucco;Oliver Sharp;Robert Wahbe	1996	World Wide Web Journal		web development;web modeling;web api;computer science;world wide web;reactive programming;programming language;web page;web service;programming domain;client-side scripting	OS	-33.882177388968884	40.13252013704708	35944
17aa9e7f25cc46d6002ce220e4eb8306178a0a80	asset management solution based on profibus-pa profiles	programmable controllers;object oriented methods;asset management;asset management asset management solution profibus pa profiles automation control systems process control plc based network optimisation temporal requirements topological requirements asset management functions object oriented solution profibus pa device proxy objects;controller area networks;automatic evaluation;control system;asset management automatic control automation control systems process control data mining data engineering field buses actuators programmable control;object oriented;process control;object oriented methods controller area networks process control programmable controllers	Effective solutions for asset management become increasingly important in modern automation and control systems. They are especially required in the area of process control. The control systems used here usually consist of PLC-based networks, which are optimised concerning the special temporal and topological requirements of the process. Asset management functions have to be introduced to those systems, without any influence on the existing equipment. The paper shows an object-oriented solution based on PROFIBUS-PA device. By automatically evaluating a system structure and mapping the profile-relevant parameters of devices to proxy objects, different application modules for asset management functions can be implemented acting on these objects.	control system;power-line communication;profibus;proxy server;requirement	Martin Wollschlaeger;Christian Diedrich;Jochen Mueller;Ulrich Epple	2001	ETFA 2001. 8th International Conference on Emerging Technologies and Factory Automation. Proceedings (Cat. No.01TH8597)	10.1109/ETFA.2001.997766	control engineering;embedded system;real-time computing;computer science;engineering;control system;operating system;programmable logic controller;process control;control theory;object-oriented programming	EDA	-35.71441069168668	37.91144448749816	36001
2abf2bd4dbc1a129abef82d2586cd03e68bf046b	support for mobility and replication in the aspectix architecture	distributed computing;open architecture;distributed objects;client server;middleware	CORBA as a standardized object-based middleware for distributed computing still lacks sufficient support for mobility and replication, although there are several proposals to integrate these mechanisms. AspectIXis a more flexible and more open architecture than CORBA, butAspectIX is still fully CORBA compliant. Unlike CORBA with its static client-server relationship, AspectIXuses the concept of distributed objects. Each distributed object is represented by at least one local part, called fragment, that communicates with other fragments to synthesize the desired behaviour. The fragment implementation that is actually used depends on nonfunctional aspects which are specified on the distributed object via a typed interface. Based on this model AspectIXprovides a single mechanism that is especially suited to realize both: mobility and replication.	client–server model;common object request broker architecture;distributed computing;distributed object;fragment (computer graphics);middleware;object-based language;open architecture;server (computing)	Martin Geier;Martin Steckermeier;Ulrich Becker;Franz J. Hauck;Erich Meier;Uwe Rastofer	1998		10.1007/3-540-49255-0_93	shared disk architecture;reference architecture;middleware;space-based architecture;open architecture;computer science;applications architecture;operating system;middleware;database;distributed computing;distributed object;distributed system security architecture;client–server model;replication	PL	-35.26856543189597	43.033577906922766	36076
2f0aa72039cc72a5bf8a4cd66ec468eeb1d3d534	a comparative study of off-line deep learning based network intrusion detection		Network intrusion detection systems (NIDS) are essential security building-blocks for today's organizations to ensure safe and trusted communication of information. In this paper, we study the feasibility of off-line deep learning based NIDSes by constructing the detection engine with multiple advanced deep learning models and conducting a quantitative and comparative evaluation of those models. We first introduce the general deep learning methodology and its potential implication on the network intrusion detection problem. We then review multiple machine learning solutions to two network intrusion detection tasks (NSL-KDD and UNSW-NB15 datasets). We develop a TensorFlow-based deep learning library, called NetLearner, and implement a handful of cutting-edge deep learning models for NIDS. Finally, we conduct a quantitative and comparative performance evaluation of those models using NetLearner.	data mining;deep learning;intrusion detection system;online and offline;open-source software;performance evaluation;sparse matrix	Jiaqi Yan;Dong Jin;Cheol Won Lee;Ping Liu	2018	2018 Tenth International Conference on Ubiquitous and Future Networks (ICUFN)	10.1109/ICUFN.2018.8436774	task analysis;deep learning;feature extraction;artificial neural network;distributed computing;intrusion detection system;machine learning;computer science;data modeling;artificial intelligence	Security	-60.25356573889454	60.43835367798476	36156
811295f8f5c8548af0cf530a757312bf9eaae4c5	iot for telemedicine practices enabled by an android™ application with cloud system integration		Connected medical devices are already paving the way towards the realization of Smart Hospitals where patient care is improved thanks to Internet of Things (IoT) solutions. Following this trend, in this paper we propose an IoT cloud-based network for anesthesia on-line monitoring. This architecture allows the anesthesiologist to remain simultaneously connected to all the sedated patients through an Android app. Moreover, medical data from the patients can be shared on a cloud solution accessible by a web application enabling teleconsulting. Hence, by accessing the cloud, medical specialists can consult the shared data from everywhere and at any time. The flexibility and the portability of our monitoring architecture ensure the possibility to interface with any medical device which can wirelessly send the measured data. Therefore, other medical monitoring applications can also be addressed.	android;cloud computing;duplex (telecommunications);internet of things;online and offline;raspberry pi 3 model b (latest version);smartphone;smartwatch;software bug;software portability;system integration;tablet computer;toad data modeler;web application	Francesca Stradolini;Nadia Tamburrano;Anastasios Spiliotopoulos;Abuduwaili Tuoheti;Danilo Demarchi;Sandro Carrara	2018	2018 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2018.8351871	humanoid robot;system integration;computer engineering;electronic engineering;architecture;web application;android (operating system);computer science;software portability;cloud computing;telemedicine	Arch	-44.968191235433274	48.39250674909841	36188
ff539294abb4e1631d2d0e1340b98d2ec357b83a	refactoring development simplified: demonstration	refactoring;transformation engine;java	In this paper we briefly describe a new Java Refactoring API to simplify development.	application programming interface;code refactoring;java	Ralph Benjamin Ruijs;Jan Lahoda;Jan Becicka	2012		10.1145/2328876.2328887	real-time computing;computer science;software engineering;programming language;code refactoring	SE	-53.09845925958433	33.667746180532724	36235
66ed3cd6f5fbefe5f87bb6cc2eb62b453d39d0dc	exploiting model profiles in requirements verification of cloud systems	formal methods;requirements verification;cloud systems;mde;modelling profiles;model driven engineering;cloud computing	Cloud Systems arose in the last years as a standard de-facto in IT enterprises for offering practically any kind of services to worldwide users. They provide means for realizing and distributing everything-as-a-service, including infrastructures, hardware and software platforms and services. Even if now, Service-centric models and technologies are mature in the IT scenario, composition, analysis and validation of Cloud services are open research challenges. In this work, we describe a Modeling Profile that enables Model Driven Engineering (MDE) analysis of systems and requirements verification of Cloud-based services. The verification process exploits formal methods during the whole life cycle of services. We show the application of the proposed methodology in a simple example.	cloud computing;complex systems;fault tolerance;formal methods;model-driven engineering;open research;requirement;run time (program lifecycle phase);system analysis;verification and validation	Francesco Moscato	2015	IJHPCN	10.1504/IJHPCN.2015.071258	model-driven architecture;real-time computing;simulation;formal methods;cloud computing;computer science;operating system;services computing	SE	-46.179611027466684	38.15583999110525	36271
7e591eef6105cf0858d0769440997a1bfb583c77	safe trans loader: mitigation and prevention of memory corruption attacks for released binaries		A variety of countermeasures against memory corruption attacks have been proposed to implement within compilers, linkers, operating systems, and libraries. However, according to our survey, a certain number of executable binaries in Linux distributions are not protected by the countermeasures, even when the countermeasures are applied to these binaries. Further, the countermeasures have some problems including the way of application, the scope of attacks, and the runtime overhead. For example, some require source code or need to update the kernel or specific libraries. These requirements are not acceptable for everyone. In this paper, we propose an application-level loader called Safe Trans Loader (STL) that mitigates or prevents memory corruption attacks. The STL can be applied to already released executable binaries in an operational phase. Note that the STL replaces vulnerable library functions with safe substitute functions when it loads the protected binary. These safe substitute functions mitigate or prevent stack-based buffer overflow attacks, heap-based buffer overflow attacks, and use-after-free attacks. Since the STL has minimal dependencies on the execution environment, it does not require specific changes to the existing operating system or library. Further, through our evaluation, the runtime overhead of the STL is only 1.24%.	memory corruption	Takamichi Saito;Masahiro Yokoyama;Shota Sugawara;Kuniyasu Suzaki	2018		10.1007/978-3-319-97916-8_5	stack buffer overflow;loader;computer science;real-time computing;computer security;compiler;buffer overflow;memory corruption;source code;executable;heap (data structure)	Security	-56.36469422888968	55.803610516839775	36288
0071c8fb7aa2c8cc8206474262f88227c79a8a79	address space randomization for mobile devices	mobile device;android;smartphones;aslr;operating system;return to libc;control ow hijacking;mobile devices	Address Space Layout Randomization (ASLR) is a defensive technique supported by many desktop and server operating systems. While smartphone vendors wish to make it available on their platforms, there are technical challenges in implementing ASLR on these devices. Pre-linking, limited processing power and restrictive update processes make it difficult to use existing ASLR implementation strategies even on the latest generation of smartphones. In this paper we introduce retouching, a mechanism for executable ASLR that requires no kernel modifications and is suitable for mobile devices. We have implemented ASLR for the Android operating system and evaluated its effectiveness and performance. In addition, we introduce crash stack analysis, a technique that uses crash reports locally on the device, or in aggregate in the cloud to reliably detect attempts to brute-force ASLR protection. We expect that retouching and crash stack analysis will become standard techniques in mobile ASLR implementations.	address space layout randomization;aggregate data;android;booting;cloud computing;desktop computer;executable;kernel (operating system);mobile device;operating system;prelink;randomized algorithm;sensor;server (computing);smartphone	Hristo Bojinov;Dan Boneh;Rich Cannings;Iliyan Malchev	2011		10.1145/1998412.1998434	embedded system;real-time computing;computer science;operating system;mobile device;computer security;computer network	Security	-55.09039018294936	58.72447797799283	36311
819fd2d752010e6f4be2504550e212ea43ec3ec6	a recovery method for the robotic decentralized control system with performance redundancy		The fault of the robotic control system is critical and leads to the general system failure, while autonomous robots have to gain their aims without any maintenance. Contemporary academic studies propose decentralized control systems as prospective from the robustness point of view. On the other hand, a performance redundancy allows to optimize resource utilization and improve the fault-tolerance potential of the control system. This paper is devoted to the recovery method of the robotic decentralized control system with performance redundancy. A reconfiguration problem has been formalized, decentralized method of the solution obtaining is represented. Also some simulation results are given and discussed.	autonomous robot;distributed control system;fault tolerance;prospective search;simulation	Iakov Korovin;Eduard Melnik;Anna Klimenko	2016		10.1007/978-3-319-43955-6_2	control engineering;real-time computing;control theory	Robotics	-36.7660529456807	37.29612771902074	36324
15fbf03aac449ed1e85c7e9c427dffe1e462e1ef	the development of integrated static and dynamic mechanical test-bed	integration testing;mechanical engineering computing;static and dynamic testing mechanical experiments integrated test bed;digital testing system integrated static mechanical test bed integrated dynamic mechanical test bed test equipment;automatic test equipment;test bed;mechanical engineering computing automatic test equipment dynamic testing;testing strain stress strain measurement bridges education software;mechanical testing;dynamic testing	A new generation of integrated static & dynamic mechanical test-bed and supporting comprehensive test equipment are designed. The test-bed contains eight kinds of experimental devices, it could offer beyond twenty experimental items. The test-bed uses digital testing system with the computer, it has both static and dynamic experimental functions.	built-in test equipment;testbed	Zhankui Xie;Rui Xie	2011	Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology	10.1109/EMEIT.2011.6023121	structural engineering;reliability engineering;automatic test equipment;simulation;integration testing;computer science;engineering;dynamic testing;testbed	Robotics	-46.928769742322345	35.15214371792602	36351
40ed41604bd0f6e858bfe6cc248196d4ded1072b	planning with diversified models for fault-tolerant robots	dynamic change;fault tolerant;au tonomous system;autonomic system;redundancy;fault tolerance;dependability;task planning;fault injection;autonomous robot	Planners are central to the notion of complex autonomous systems. They provide the flexibility that autonomous systems need to be able to operate unattended in an unknown and dynamically-changing environment. However, they are notoriously hard to validate. This paper reports an investigation of how redundant, diversified models can be used as a complement to testing, in order to tolerate residual development faults. A fault-tolerant temporal planner has been designed and implemented using diversity, and its effectiveness demonstrated experimentally through fault injection. The paper describes the implementation of the fault-tolerant planner and discusses the results obtained. The results indicate that diversification provides a noticeable improvement in planning reliability with a negligible performance overhead. However, further improvements in reliability will require implementation of an on-line checking mechanism for assessing plan validity before execution. Introduction Planning shows promising success as a central decisional mechanism in complex autonomous systems, both in space exploration and in experimental studies. However, the dependability of planners remains a stumbling block to real life utilization. Indeed, how can we justifiably trust such mechanisms, whose behavior is difficult to predict and validate? Autonomous systems strive to accomplish goals in open environments. The space of possible execution contexts is thus, in essence, infinite, and cannot be exhaustively tested during validation. Testing and other validation techniques are nevertheless necessary in order to obtain planners that are as bug-free as possible. However, we believe that such validation techniques must be complemented by a fault-tolerance approach aimed at making planners resilient to residual bugs. We propose in this paper a fault tolerance approach focused on development faults in planner knowledge. The approach uses redundant diversified planning models. Copyright © 2007, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. First, we introduce basic concepts of dependability and dependability issues relative to planning. Second, we present fault tolerance techniques that may be applied to planning mechanisms and the implementation of a fault tolerant planner for an autonomous system architecture. Third, we introduce the validation framework that we developed to assess performance and efficacy of our fault tolerance approach. Finally, we present our experimental results. Dependability and Fault Tolerance Dependability is a major concern in computing systems controlling critical structures such as railroads, planes and nuclear plants. We introduce here basic dependability concepts extracted from (Avizienis et al. 2005). We then discuss validation and other dependability issues relative to planning. Dependability basic concepts The dependability of a computing system is its ability to deliver service that can justifiably be trusted. Correct service is delivered when the service implements the system function, that is what the system is intended to do. Three concepts further describe this notion: the attributes of dependability, the threats to dependability, and the means by which dependability can be attained (Figure 1). Dependability encompasses numerous attributes. Depending on the application intended for the system, different emphasis may be put on each attribute. We focus particularly on reliability (the continuous deliverance of correct service for a period of time) and safety (the absence of catastrophic consequences on the users and the environment) of a system. The threats to a system’s dependability consist of failures, errors and faults. A system failure is an event that occurs when the delivered service deviates from correct service. An error is that part of the system state that can cause a subsequent failure. An error is detected if its presence is indicated by an error message or error signal. A fault is the adjudged or hypothesized cause of an error. Here, we focus particularly on development faults: faults that are unintentionally caused by man during the development of the system. } } INTEGRITY MAINTAINABILITY	artificial intelligence;autonomous robot;autonomous system (internet);declarative programming;deliverance;dependability;diversification (finance);error detection and correction;error message;experiment;failure detector;fault injection;fault tolerance;online and offline;overhead (computing);plan 9 from bell labs;real life;simulation;software bug;software development;stumbleupon;systems architecture;threat (computer);verification and validation;watchdog timer	Benjamin Lussier;Matthieu Gallien;Jérémie Guiochet;Félix Ingrand;Marc-Olivier Killijian;David Powell	2007			fault tolerance;real-time computing;simulation	AI	-47.96791851958266	38.49779251493724	36370
c84a265ddeb6a9343a13aabbf65e47fc6b2ffa06	coverage analysis for embedded testing and an application	coverage analysis	Dashboards track key development metrics The C++test plugin for Wind River® Workbench provides Workbench users easy access to C++test’s full code analysis, code review, and unit testing capabilities directly within their IDE. This seamless integration enables testing and verification to become a natural and continuous part of the development process. The complete target-based test execution flow, including test case generation, crosscompilation, deployment, execution, and loading results back into the GUI, can be automated within C++test. C++test leverages Wind River CDT to provide contextsensitive pop-up menus and views.	accessibility;eclipse;embedded system;graphical user interface;seamless3d;software deployment;static program analysis;test case;unit testing;workbench	Jinsong Zhu;Son T. Vuong	1999		10.1007/978-0-387-35578-8_21	computer science;theoretical computer science;data mining;algorithm	SE	-53.1345855927041	35.21288542303215	36428
60e0fb12518b0ed10a873c1326a2b0ba95b146ad	authorization-aware hateoas		The architectural style named Representational State Transfer (REST) is nowadays widely established and still enjoys a growing popularity. One of the core principles of REST is referred as ”Hypermedia as the Engine of Application State” (HATEOAS). HATEOAS is one of the foundations of the scalability that RESTful systems provide and enables the decoupling of client and server. But the realization of HATEOAS is challenging, because there is no systematic approach how to enforce the constraint. Therefore, the implementation is mostly up to the developer of a RESTful service. This work describes a new method of how to apply the HATEOAS constraint. We describe a method that systematically enables HATEOAS based on REST API models and the integration of access control mechanisms. In order to avoid unauthorized access attempts and unnecessary network traffic, the resource representations are customized to the requesting subject. References that lead to not accessible resources, are not included in the customized resource representations. Therefore, an attribute based access control mechanism is extended to distinguish between static and dynamic attributes. A 2-phase authorization procedure is introduced that relies on this discrimination and determines the references which must be included in the resource representation. The result is a flexible realization of HATEOAS based on formal models.	access control;application programming interface;authorization;control system;coupling (computer programming);executable;hateoas;hypermedia;industry 4.0;network traffic control;representational state transfer;scalability;server (computing);state (computer science)	Marc Hüffmeyer;Florian Haupt;Frank Leymann;Ulf Schreier	2018		10.5220/0006683700780089	database;computer science;authorization;hateoas	PL	-40.00344736428859	41.341343905988566	36595
028940ac269878273a83bdd20206a07886cca1b2	the design of an open hybrid recommendation system for mobile commerce	open hybrid recommendation system;electronic commerce;mobile device;performance evaluation;multi agent architecture design;multi agen architecturet hybrid recommendation mobile commerce;hybrid recommendation;arrays;multi agent systems;performance evaluation open hybrid recommendation system mobile commerce generic customer profile weighted hybrid recommendation algorithm multi agent architecture design;recommender system;mobile commerce;heuristic algorithms;mobile communication;multi agent systems electronic commerce information filters mobile computing;mobile handsets;multi agent architecture;generic customer profile;weighted hybrid recommendation algorithm;mobile computing;business customer profiles context awareness laboratories data engineering knowledge engineering prototypes xml sun australia;information filters;algorithm design and analysis;customer profiles;multi agen architecturet	To meet more and more complex recommendation needs, it is quite important to implement hybrid recommendations for mobile commerce. In this paper, we propose a design for open hybrid recommendation systems in mobile commerce, which could integrate multiple recommendation algorithms together to improve recommendation performance. First, three solutions for an open hybrid recommendation approach are discussed in detail, which are generic customer profile, weighted hybrid recommendation algorithm, and mobile device profile creation. After that, we give out a multi-agent architecture design to make the three solutions work together. Finally a prototype system based on our proposed architecture is implemented to demonstrate the feasibility of our design and evaluate the performance of the proposed open hybrid recommendation system.	agent architecture;algorithm;mobile commerce;mobile device;multi-agent system;openness;personally identifiable information;prototype;recommender system	Chengzhi Liu;Caihong Sun;Jia Yu	2008	2008 International Conference on Computational Intelligence for Modelling Control & Automation	10.1109/CIMCA.2008.114	algorithm design;mobile commerce;mobile telephony;computer science;mobile device;database;mobile computing;world wide web;recommender system	AI	-36.18370188045362	49.032487265522924	36654
3070a534bdf1169391f97219cbccbc5e0700a6d5	tirt: a traceability information retrieval tool for software product lines projects	software;program diagnostics;software reusability information retrieval product design program diagnostics;measurement;tool;asset management;information retrieval;maintenance engineering;traceability relations tirt traceability information retrieval tool software product lines projects product line engineering maintenance traceability problem;computer architecture;software reusability;software maintenance engineering measurement context asset management information retrieval computer architecture;product design;traceability;context;software product lines;tool traceability software product lines	Software Product Line has proven to be an effective methodology for developing a diversity of software products at lower costs, in shorter time, and with higher quality. However, the adoption and maintenance of traceability in the context of product lines is considered a difficult task, due to the large number and heterogeneity of assets developed during product line engineering. Furthermore, the manual creation and management of traceability relations is difficult, error-prone, time consuming and complex. In this sense, Traceability Information Retrieval Tool (TIRT) was proposed in order to mitigate the maintenance traceability problem. An experimental study was performed in order to identify the viability of the proposed tool and traceability scenarios.	cognitive dimensions of notations;experiment;information retrieval;requirements traceability;software product line	Wylliams Barbosa Santos;Eduardo Santana de Almeida;Silvio Romero de Lemos Meira	2012	2012 38th Euromicro Conference on Software Engineering and Advanced Applications	10.1109/SEAA.2012.40	maintenance engineering;reliability engineering;traceability;systems engineering;engineering;software engineering;reverse semantic traceability;product design;traceability matrix;requirements traceability;measurement	SE	-57.82938143248764	32.53717251906038	36748
3233eaafd68afc325bf050225769d8acae319d20	delayed and controlled failures in tamper-resistant software	tamper resistance	Tamper-resistant software (TRS) consists of two functional components: tamper detection and tamper response. Although both are equally critical to the effectiveness of a TRS system, past research has focused primarily on the former, while giving little thought to the latter. Not surprisingly, many successful breaks of commercial TRS systems found their first breaches at the relatively näıve tamper-response modules. In this paper, we describe a novel tamper-response system that evades hacker detection by introducing delayed, probabilistic failures in a program. This is accomplished by corrupting the program’s internal state at well-chosen locations. Our tamper-response system smoothly blends in with the program and leaves no noticeable traces behind, making it very difficult for a hacker to detect its existence. The paper also presents empirical results to demonstrate the efficacy of our system.	anti-tamper software;smoothing;tamper resistance;tracing (software)	Gang Tan;Yuqun Chen;Mariusz H. Jakubowski	2006		10.1007/978-3-540-74124-4_15	real-time computing;simulation;computer science;computer security;tamper resistance	SE	-58.71772641933964	57.28271291987418	36774
c36d852b9fd8b0896e34a8c14bf9084c7071fda6	research on network architecture with trustworthiness and controllability	network architecture	In this paper, the architecture of trustworthy and controllable networks is discussed to meet arising application requirements. After reviewing the lessons and experiences of success and failure in the Internet and summarizing related work, we analyze the basic targets of providing trustworthiness and controllability. Then, the anticipant architecture is introduced. Based on the resulting design, several trustworthy and controllable mechanisms are also discussed.	experience;internet;network architecture;requirement;trust (emotion);trustworthy computing	Chuang Lin;Xuehai Peng	2006	Journal of Computer Science and Technology	10.1007/s11390-006-0732-2	reference architecture;real-time computing;network architecture;computer science;distributed computing;computer security	Networks	-49.18063144362334	56.0973075747928	36785
ab45fbd848f50bc34c279ba79d3ad22edc6cfd09	a data synchronization framework for personal health systems		This paper illustrates the design of a multi-platform synchronization framework which is particularly useful for speeding up the implementation of Personal Health Systems on mobile devices. Those devices turn out to be of great help since in order to transfer any data available at the patient site to the clinic and vice-versa a solid networking infrastructure and data exchange protocol is needed. The framework we developed extends an open source platform available on the market by empowering it with new features that better decouple domain specific data from the underlying transport logic. In the last part of the paper two prototypes exploiting the framework are described.	data synchronization;microsoft sync framework	Davide Capozzi;Giordano Lanzola	2011		10.1007/978-3-642-29734-2_41	real-time computing;simulation;computer science;data mining	NLP	-42.47554516739702	47.00664366323795	36864
bb8fd94d07c3fe0a5c8f04767bcef5cab189b1a1	infusing trust in indoor tracking: poster	trust;sensors;accuracy;indoor tracking;subset selection	"""An indoor tracking system is inherently an asynchronous and distributed system that contains various types (e.g., detection, selection, and fusion) of events. One of the key challenges with regards to indoor tracking is an efficient selection and arrangement of sensor devices in the environment. Selecting the """"right"""" subset of these sensors for tracking an object as it traverses an indoor environment is the necessary precondition to achieving accurate indoor tracking. With the recent proliferation of mobile devices, specifically those with many onboard sensors, this challenge has increased in both complexity and scale. No longer can one assume that the sensor infrastructure is static, but rather indoor tracking systems must consider and properly plan for a wide variety of sensors, both static and mobile, to be present. In such a dynamic setup, sensors need to be properly selected using an opportunistic approach. This opportunistic tracking allows for a new dimension of indoor tracking that previously was often infeasible or unpractical due to logistic or financial constraints of most entities. In this paper, we are proposing a selection technique that uses trust as manifested by its a quality-of-service (QoS) feature, accuracy, in a sensor selection function. We first outline how classification of sensors is achieved in a dynamic manner and then how the accuracy can be discerned from this classification in an effort to properly identify the trust of a tracking sensor and then use this information to improve the sensor selection process. We conclude this paper with a discussion of results of this implementation on a prototype indoor tracking system in an effort to demonstrate the overall effectiveness of this selection technique."""	application domain;benchmark (computing);build automation;distributed computing;entity;event-driven architecture;mathematical optimization;mobile device;precondition;prototype;quality of service;sensor;software deployment;tracking system	Ryan Rybarczyk;Rajeev R. Raje;Mihran Tuceryan	2016		10.1145/2933267.2933538	real-time computing;simulation;telecommunications;computer science;sensor;accuracy and precision;trustworthy computing	Mobile	-40.84767921786351	49.96212083583578	36891
aa4281abdbe6a8d5253254a6d5c111664e3bda48	a modular and flexible system for activity recognition and smart home control based on nonobtrusive sensors	graphical user interfaces;handicapped aids;home automation;learning (artificial intelligence);pattern recognition;public domain software;sensors;activity model learning;activity model personalization;activity recognition;event recognition;flexible system;modular open source aal framework;nonobtrusive sensors;smart home control;standard compliant interfaces;system architecture	This work describes a modular open source AAL framework for event recognition and smart home control. Various integrated tools simplify the configuration task, the personalization as well as the learning of activity models by a novel approach. Flexibility, standard compliant interfaces as well as the ability to transfer the system into new environments with little efforts have a strong focus. The paper describes the system architecture and the algorithms used.	atm adaptation layer;activity recognition;algorithm;home automation;open-source software;personalization;sensor;systems architecture	Johannes Kropf;Lukas Roedl;Andreas Stainer-Hochgatterer	2012	2012 6th International Conference on Pervasive Computing Technologies for Healthcare (PervasiveHealth) and Workshops		embedded system;home automation;simulation;human–computer interaction;computer science;sensor;operating system;graphical user interface;public domain software	Robotics	-42.02779560293784	45.754555870728915	37001
b2543700beab53f235d736ad7ac05aae85f08959	context awareness over transient clouds		The exponential increase in the number and types of mobile devices, along with their ever-growing sets of capabilities, have enabled the development of new architectures that aim to harness such heterogeneity. Transient Clouds (TCs) are examples of mobile clouds which are created on-the-fly by the devices present in an environment to share their physical resources (e.g., CPU, memory, network) and would disappear as the nodes leave the network. They enable a device to go beyond its own physical limitations through utilizing the capabilities offered by nearby devices over an ad-hoc network. In this paper we present a Transient Context-Aware Cloud (TCAC) in which the nodes of the network care more about providing/learning higher level functionalities rather than lower level capabilities. We make the case for such an architecture in scenarios where it is not feasible for all the nodes to compute the context due to privacy, energy, and delay constraints rather than an unreachable network.We present a prototype implementation of our architecture over Android smartphones connected via WiFi along with the performance metrics (power/energy consumption and accuracy)to show the benefits of context awareness in TCs.	android;central processing unit;computation;context awareness;experiment;hoc (programming language);mobile device;privacy;programming paradigm;prototype;smartphone;time complexity;unreachable memory	Andrea Sciarrone;Igor Bisio;Fabio Lavagetto;Terrence Penner;Mina Guirguis	2014	2015 IEEE Global Communications Conference (GLOBECOM)	10.1109/GLOCOM.2014.7417785	embedded system;real-time computing;simulation;cloud computing;computer science;sensor;operating system;computer network	Mobile	-39.92677924713894	49.04217210144131	37003
395a5a6179fdae01558ed1a03577c0cb88500267	generating test data for both paths coverage and faults detection using genetic algorithms: multi-path case	faults detection;software testing;multi objective optimization;software testing multiple paths coverage faults detection multi objective optimization;multiple paths coverage;yan zhang dunwei gong 故障检测 测试数据 遗传算法 多路径 覆盖率 多目标优化问题 通路 软件测试 generating test data for both paths coverage and faults detection using genetic algorithms multi path case	Generating test data that can expose the faults of the program is an important issue in software testing. Although previous methods of covering path can generate test data to traverse target path, the test data generated by these methods are difficult in detecting some low-probabilistic faults that lie on the covered paths. We present a method of generating test data for covering multiple paths to detect faults in this study. First, we transform the problem of covering multiple paths and detecting faults into a multi-objective optimization problem with constraint, and construct a mathematical model for it. Then, we give a strategy of solving the model based on a weighted genetic algorithm. Finally, we apply our method to several real-world programs, and compare it with several methods. The experimental results confirm that the proposed method can more efficiently generate test data that not only traverse the target paths but also detect faults lying in them than other methods.	bubble sort;code coverage;genetic algorithm;instrumentation (computer programming);mathematical model;mathematical optimization;multi-objective optimization;optimization problem;sensor;software testing;traverse;test data;tree accumulation	Yan Zhang;Dun-Wei Gong	2014	Frontiers of Computer Science	10.1007/s11704-014-3372-7	basis path testing;mathematical optimization;theoretical computer science;multi-objective optimization;distributed computing;software testing;algorithm	SE	-59.88789212192279	35.742042837430375	37106
b96ea93b013b1f06cb50247e0d81ee3d65e45963	trustworthiness calculation based on delegation relationship in e-service	databases;p2p simulation platform;collusion attack;delegation relationship;delegation trustworthiness;service industries data privacy peer to peer computing;collaboration;p2p;presses;trust model;blind delegation;usa councils;trustworthiness calculation;e service;computational modeling;service industries;data privacy;delegatee permission;delegation constraint;access control;trust models;computational modeling access control educational institutions computer science permission hazards collaborative work authorization collaboration quality of service;peer to peer computing;peer to peer platform;delegation depth;trustworthiness calculation delegation trustworthiness delegation relationship e service;peer to peer platform trustworthiness calculation delegation relationship e service trust models delegatee permission delegation depth delegation constraint collusion attack blind delegation p2p simulation platform	In e-service environments, the delegation relationship is rarely considered as one of important factors of trust models. Most of work on delegation relationship focused on the permission of delegatee, delegation depth, delegation constrain and so on. They ignored the potential hazards brought from blind delegation, so easily to form the collusion attack. This paper defines the concept of delegation trustworthiness firstly, and then proposes a method for calculating trustworthiness based on the delegation relationship. The method enables the involved delegator and delegatee to be responsible for their actual behavior in delegation process. The delegate targets can choose a reliable trust entity according to the value of trustworthiness. Finally, a P2P simulation platform is used for experiments, and the simulation results presented verify the validity of the trustworthiness calculation.	algorithm;e-services;experiment;malware;openness;peer-to-peer;simulation;trust (emotion)	Jie Jiang;Dongxian Shi;Deren Chen	2010	2010 International Conference on Service Sciences	10.1109/ICSS.2010.56	business;internet privacy;world wide web;computer security	HPC	-46.596214884545596	55.88035157980274	37114
4dc9121a96ad384b5a3dbdfd6889445abba97e66	automatic property generation for the formal verification of bus bridges	time to market constraints;automatic verification;protocols;hardware systems;intellectual property;ocp ip protocol family;network on chip;clocks;degree of freedom;design flow;automatic property generation;protocols bridges clocks pipeline processing hardware communication channels system on a chip;abstract model;bridges;state machine;system on a chip;automatic generation;chip;system buses;finite state machines;formal verification;state machines;system on chip;verification holes;verification suite;ocp ip protocol family automatic property generation formal verification bus bridges time to market constraints hardware systems protocols automatic generation protocol specification verification suite verification tool verification holes abstract model state machines generic work flow;protocol specification;hardware design;time to market;experimental evaluation;ministry of education;communication channels;system buses finite state machines formal verification protocols;verification tool;pipeline processing;generic work flow;hardware;bus bridges	The automatic verification of designs is a challenging task and of high interest due to increasing time-to-market constraints. In this paper, we focus on the verification of bus bridges which are used in many hardware systems to connect two buses running different protocols. We developed an approach to assist the automatic generation of properties from the protocol specification for the formal verification of bus bridges. The technical contribution is that the final set of the verification suite is functionally complete in respect to the underlying verification tool which shows the absence of any verification holes. The approach uses an abstract model of bus bridges in terms of state machines which enables a generic work flow. In experimental evaluations we applied the approach to bus bridges based on the OCP/IP protocol family.	debugging;failure;formal methods;formal verification;functional completeness;internet protocol suite;open core protocol;protocol stack	Mathias Soeken;Ulrich Kühne;Martin Freibothe;Görschwin Fey;Rolf Drechsler	2011	14th IEEE International Symposium on Design and Diagnostics of Electronic Circuits and Systems	10.1109/DDECS.2011.5783129	system on a chip;embedded system;real-time computing;verification;software verification;computer science;theoretical computer science;operating system;high-level verification;runtime verification;finite-state machine;programming language;intelligent verification;functional verification	EDA	-37.01025118819914	33.29484626040947	37196
60edac5796a7438c51a50f8a0d017f2674a9d3b3	a case for secure virtual append-only storage for virtual machines	virtual machine;kernel;in memory execution state;virtual storage abstraction;storage management;intrusion detection;vas based logging complements;virtual machine monitors;monitoring system;internet;operating system;monitoring;malware;virtual machines;virtual machines internet invasive software storage management;secure virtual append only storage;driver circuits;invasive software;linux;driver circuits kernel monitoring virtual machine monitors linux cloud computing;system activity monitoring;cloud computing;operating systems;in memory execution state secure virtual append only storage virtual machines operating systems system activity monitoring malware virtual storage abstraction vas based logging complements intrusion detection techniques;intrusion detection techniques	Traditional operating systems and applications use logs extensively to monitor system activity and perform intrusion detection. Consequently, logs have also become prime targets for intruders. When a malware or intruder obtains root privileges in a system, one of its first actions is to hide its footprint by deleting or modifying system logs, especially the log entry recording the intrusion activity (such as unauthorized root login). A key weakness of most current logging mechanisms is that logs are stored on a storage device over which the system being logged has complete control, including the ability to delete/modify the logs arbitrarily. Once the root privileges of such a system are compromised, so are the logs. Virtualization offers a unique opportunity to eliminate this point of weakness. In this paper, we propose a new virtual storage abstraction for virtual machines (VMs) called Virtual Append-only Storage (VAS) that secures and preserves all system and/or application logs in a VM and can prevent an intruder from deleting/modifying past logs even after the root privileges of a VM are compromised. Our VAS-based logging complements existing intrusion detection techniques which mainly monitor the in-memory execution state and data, but do not protect the storage device on which logs are stored. Since logs can become voluminous over time, VAS also provides administrators the ability to secure either system-wide or application-specific logs, rather than blindly logging all system activity.	append;authorization;full virtualization;in-memory database;intrusion detection system;login;malware;operating system;overhead (computing);prototype;server (computing);superuser;universal instantiation;virtual machine;z/vm	Zhao Lin;Kartik Gopalan;Ping Yang	2010	2010 39th International Conference on Parallel Processing Workshops	10.1109/ICPPW.2010.15	embedded system;parallel computing;real-time computing;computer science;virtual machine;operating system;computer network	OS	-53.4958730304331	57.526694658915744	37423
25f5405d1777c67902a161e6d68d72177047502e	service-oriented communication for controller area networks	automotive engineering;protocols;automobiles;standards;servers;registers;cloud computing	The appearance of high-performance connectivity (e.g. Automotive Ethernet, C2X Communication) will enable seamless integration of cloud-based services into the vehicle's Electric/Electronic (E/E) architecture. This new development poses several challenges regarding in-vehicle communication such as the need for runtime adaptive communication or the establishment of continuous connectivity between functionality hosted in the cloud and on deeply embedded Electronic Control Units (ECUs). These upcoming challenges can be tackled by the introduction of Service-Oriented Communication (SOC) protocols into the vehicle's communication system. For Ethernet a first standard has been defined with AUTOSAR's SOME/IP. However, today's most used automotive communication bus is still the Controller Area Network (CAN) bus. This paper introduces a concept and a first implementation of a service-oriented communication protocol for CAN.	autosar;can bus;cloud computing;communications protocol;embedded system;end-to-end principle;engine control unit;internet backbone;microsoft outlook for mac;mobile device;programming paradigm;publish–subscribe pattern;quality of service;request–response;seamless3d;service-oriented architecture;service-oriented device architecture	Marco Wagner;Sebastian Schildt;Michael Poehnl	2016	2016 IEEE 84th Vehicular Technology Conference (VTC-Fall)	10.1109/VTCFall.2016.7881222	embedded system;communications protocol;real-time computing;cloud computing;telecommunications;computer science;engineering;operating system;processor register;server;computer network	EDA	-37.29842331603772	46.72393186534055	37478
35aad7908bd4d4a3d32fd888d75fbe3918f0d1b0	a program stability measure	information structure;external routine usages;common file specification;fortran systems;software systems;standard names;structured data interfacing;common files;software reliability	This paper contributes to the understanding of program structures in terms of its stability and reliability in a quantitative sense. Distinctions are made between the logical structure of a program and the information structure of a program.  The general characteristics of a good program will not be discussed in this paper other than citing relevent references. The term stability is defined as the resistance to the amplification of changes that has been made to a given program. The information structure of a program is based on the sharing of information between the components of the program.		Norman L. Soong	1977		10.1145/800179.810197	program analysis;computer science;data mining;database;programming language	SE	-51.29050910846114	35.78418215443015	37504
d02710abd46d069ddacc1c70c557ff30257f75a4	a formal approach for service composition in a cloud resources sharing context	ltl cloud service based business processes composition formal verification;formal approach resource provider service composition correction checking state based ltl formulae event based ltl formulae sog symbolic observation graphs multitenancy elasticity resources management cloud managers user requirements cloud services composition cloud resources sharing;composition;cloud;service based business processes;formal verification;cloud computing petri nets context semantics elasticity contracts stakeholders;ltl;temporal logic cloud computing formal verification graph theory resource allocation	Composition of Cloud services is necessary when a single component is unable to satisfy all the user's requirements. It is a complex task for Cloud managers which involves several operations such as discovery, compatibility checking, selection, and deployment. Similarly to a non Cloud environment, the service composition raises the need for design-time approaches to check the correct interaction between the different components of a composite service. However, for Cloud-based service composition, new specific constraints, such as resources management, elasticity and multitenancy have to be considered. In this work, we use Symbolic Observation Graphs (SOG) in order to abstract Cloud services and to check the correction of their composition with respect to event-and state-based LTL formulae. The violation of such formulae can come either from the stakeholders' interaction or from the shared Cloud resources perspectives. In the former case, the involved services are considered as incompatible while, in the latter case, the problem can be solved by deploying additional resources. The approach we propose in this paper allows then to check whether the resource provider service is able, at run time, to satisfy the users' requests in terms of Cloud resources.	cloud computing;correctness (computer science);elasticity (cloud computing);elasticity (data store);expect;experiment;model checking;multitenancy;platform as a service;requirement;run time (program lifecycle phase);sensor;service composability principle;service-oriented modeling;software deployment;web service	Kais Klai;Hanen Ochi	2016	2016 16th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGrid)	10.1109/CCGrid.2016.74	composition;real-time computing;formal verification;computer science;operating system;database;distributed computing;programming language	SE	-46.96079986773516	42.959372848132425	37553
0047798ed5bbb87084440d31345ae318e1e3a1b4	predicting cyber risks through national vulnerability database	zero day vulnerability;risk assessment	Su Zhang1, Xinming Ou2, and Doina Caragea3 1Cloud Platform Engineering, Symantec Corporation, Mountain View, California, USA 2Department of Computer Science and Engineering, University of South Florida, Tampa, Florida, USA 3Department of Computing and Information Sciences, Kansas State University, Manhattan, Kansas, USA ABSTRACT Software vulnerabilities are the major cause of cyber security problems. The National Vulnerability Database (NVD) is a public data source that maintains standardized information about reported software vulnerabilities. Since its inception in 1997, NVD has published information about more than 43,000 software vulnerabilities affecting more than 17,000 software applications. This information is potentially valuable in understanding trends and patterns in software vulnerabilities so that one can better manage the security of computer systems that are pestered by the ubiquitous software security flaws. In particular, one would like to be able to predict the likelihood that a piece of software contains a yet-to-be-discovered vulnerability, which must be taken into account in security management due to the increasing trend in zero-day attacks. We conducted an empirical study on applying data-mining techniques on NVD data with the objective of predicting the time to next vulnerability for a given software application. We experimented with various features constructed using the information available in NVD and applied various machine learning algorithms to examine the predictive power of the data. Our results show that the data in NVD generally have poor prediction capability, with the exception of a few vendors and software applications. We suggest possible reasons for why the NVD data have not produced a reasonable prediction model for time to next vulnerability with our current approach, and suggest alternative ways in which the data in NVD can be used for the purpose of risk estimation.	algorithm;application security;common vulnerabilities and exposures;computer engineering;computer science;computer security;coupling (computer programming);data mining;information privacy;machine learning;microsoft windows;nl (complexity);national vulnerability database;risk assessment;security management;ubiquitous computing;vulnerability (computing);zero-day (computing)	Su Zhang;Xinming Ou;Doina Caragea	2015	Information Security Journal: A Global Perspective	10.1080/19393555.2015.1111961	vulnerability management;risk assessment;computer science;data mining;secure coding;zero-day attack;computer security	Security	-61.262479083422214	46.7041584994464	37604
25712a0b7f5ac5c07457e40d654ac0c8ef9c5c13	performance of a real-time ethercat master under linux	networked control systems;real time;real time and embedded systems industrial control systems open source operating systems;best effort;public domain software field buses linux local area networks networked control systems;embedded system;open source hardware;field buses;public domain software;embedded systems;operating system;industrial control;ethernet for control automation technology real time ethercat master linux open source operating systems networked control systems domain rt patch;linux;industrial control systems;real time systems operating systems industrial control linux open source hardware embedded systems;real time application;open source operating systems;local area networks;operating systems;open source;real time systems;real time and embedded systems	The adoption of open-source operating systems for the execution of real-time applications is gaining popularity, even in the networked control systems domain, due to cost and flexibility reasons. However, as opposed to their commercial counterparts, the actual performance level to be expected from them is still little known and may often depend on the kind of real-time extension being used, its configuration, and the overall software load of the system, including best-effort components. In this paper, an open-source EtherCAT master supported by a popular real-time extension for Linux, the RT Patch, is thoroughly evaluated with long-term measurements, which build confidence on the suitability of the proposed approach for real-world applications. Special attention is devoted to the unexpected, adverse effect that some best-effort components, for instance, graphics applications, may have on the overall real-time characteristics of the system. For reference, the proposed approach is also compared with RTAI, a more traditional and well-known real-time extension for Linux already in use for demanding applications.	best-effort delivery;control system;graphics;linux;open-source software;operating system;rtai;real-time clock;real-time transcription	Marco Cereia;Ivan Cibrario Bertolotti;Stefano Scanzio	2011	IEEE Transactions on Industrial Informatics	10.1109/TII.2011.2166777	local area network;best-effort delivery;embedded system;industrial control system;real-time computing;computer science;operating system;open source hardware;public domain software;linux kernel	Embedded	-34.05414743268154	37.83222238803097	37724
55689b87951bfbbbb914d72ade794049062d7d53	analyzing and extending mumcut for fault-based testing of general boolean expressions	software testing;disjunctive normal form;information technology;swinburne;null;failure analysis;programming profession;fault detection;fault detection genetic mutations software testing communications technology computer science failure analysis costs programming profession information technology terminology;communications technology;terminology;genetic mutations;computer science	Boolean expressions are widely used to model decisions or conditions of a specification or source program. The MUMCUT, which is designed to detect seven common faults where Boolean expressions under test are assumed to be in Irredundant Disjunctive Normal Form (IDNF), is an efficient fault-based test case selection strategy in terms of the fault-detection capacity and the size of selected test suite. Following up our previous work that reported the fault-detection capacity of the MUMCUT when it is applied to general form Boolean expressions, in this paper we present the characteristic of the types of single faults committed in general Boolean expressions that a MUMCUT test suite fails to detect, analyze the certainty why a MUMCUT test suite fails to detect these types of undetected faults, and provide some extensions to enhance the detection capacity of the MUMCUT for these types of undetected faults.	boolean expression;disjunctive normal form;test case;test suite	Chang-ai Sun;Yunwei Dong;R. Lai;Kwan Yong Sim;Tsong Yueh Chen	2006	The Sixth IEEE International Conference on Computer and Information Technology (CIT'06)	10.1109/CIT.2006.51	information and communications technology;failure analysis;computer science;theoretical computer science;machine learning;database;software testing;terminology;law;information technology;disjunctive normal form;fault detection and isolation;algorithm	SE	-60.0004800060485	33.760888787509195	37732
1b702d421f38d23532bc405f8a0a2539bb764250	fuzzy belief-based supervision	fuzzy set view enhancement fuzzy belief based supervision automatic specification software system failure detection finite state machines;software supervision;automatic detection of failures;specification based detection of failures;formal specification;fuzzy set;fuzzy belief based supervision;specification based detection of failures software reliability software failures automatic detection of failures software supervision;software systems;master thesis;software fault tolerance;software system failure detection;fuzzy set theory;law legal factors automata fuzzy sets availability computer security fuzzy systems software systems concrete application software;harvested from collections canada;finite state machines;software fault tolerance finite state machines formal specification fuzzy set theory program testing;program testing;automatic detection;automatic specification;software failures;fuzzy set view enhancement;experimental evaluation;software reliability;communicating finite state machine	This paper considers automatic, specification based detection of failures (supervision) of software systems. It is applicable to systems specified in a formalism based on communicating finite state machines. The technique enhances the belief-based approach to supervision to allow its continuation after occurrences of failures. The enhancement adopts the fuzzy set view of the membership of hypotheses in the behavior matched set. The paper first overviews the belief-based approach, presents the fuzzy enhancements and describes an experimental evaluation of the technique and summarizes its results.	communicating finite-state machine;continuation;fuzzy set;semantics (computer science);software system	Alexandre Vorobiev;Rudolph E. Seviora	2008	2008 Third International Conference on Availability, Reliability and Security	10.1109/ARES.2008.46	reliability engineering;real-time computing;computer science;data mining;fuzzy set operations	SE	-49.63814079282017	35.857343504522824	37739
e11b9dd1f2b600f755f3941b8327f227a1297a2b	private rendezvous-based calibration of low-cost sensors for participatory environmental sensing	sensor calibration;mobile sensing;air pollution;location privacy;citizen science	Ever-connected smart phones and advanced sensors have lead to new sensing paradigms that promise environmental monitoring in unprecedented spatio-temporal resolution. Especially in air quality sensing with low-cost sensors, regular in-situ device calibration is a helpful approach to ensure data quality. In participatory sensing scenarios, privacy implications arise, as personal sensor data, time and location need to be exchanged. We present a novel privacy-preserving multi-hop sensor calibration scheme that combines Private Proximity Testing and an anonymizing MIX network with cross-sensor calibration based on sensor rendezvous. Our evaluation with simulated ozone measurements and real-world taxicab mobility traces shows that our scheme provides privacy protection while maintaining competitive overall data quality in dense participatory sensing networks.	data quality;mix network;participatory sensing;sensor;smartphone;tracing (software)	Jan-Frederic Markert;Matthias Budde;Gregor Schindler;Markus Klug;Michael Beigl	2016		10.1145/2962735.2962754	engineering;internet privacy;computer security;remote sensing	Mobile	-42.14303396228676	51.915934580044706	37796
506a8abff57a1b87f8d0c0c40e56e5f336f04616	an ontology driven software framework for the healthcare applications based on ant+ protocol	healthcare;sensors;ant;health care application domain ontology driven software framework ant protocol interoperability api application program interfaces abstraction layer ontology concept;ant framework sensors healthcare ontologies;ontologies artificial intelligence health care medical computing;ontologies;ontologies wireless sensor networks protocols sensor phenomena and characterization ad hoc networks wireless communication;framework	Programmers are not flexible in integrating devices and sensors for their products due to lack of interoperability between thousands of drivers, ah-hoc APIs and protocols. How to concentrate on the functionality instead of technical details is a major requirement. We propose a development framework which classifies the devices based on their functionality and provides an abstraction layer to escape physical hardware constraints and connectivity issues. We propose the usage of ontologies in order to expose the devices functionality represented by an ontology concept. The problem is addressed in context to the healthcare application domain while using the ANT+ protocol technology.	abstraction layer;application domain;application programming interface;code generation (compiler);hoc (programming language);interoperability;ontology (information science);programmer;requirement;sensor;software framework	Nadeem Qaisar Mehmood;Rosario Culmone;Leonardo Mostarda	2014	2014 28th International Conference on Advanced Information Networking and Applications Workshops	10.1109/WAINA.2014.48	computer science;sensor;ontology;artificial intelligence;software framework;operating system;data mining;database;programming language;ant;world wide web;process ontology;computer network	EDA	-40.55322446226106	46.867626396250785	37805
954a4366ec75c675722b51dcbec0431803b21ba9	architecture based development with dynacomm: incorporating dynamic reconfiguration and hierarchical design into community	formal specification;dynamic reconfiguration;dynacomm;component;connector;client server systems;refinement;subsystem architecture description language adl component connector dynacomm dynamic reconfiguration interface manager refinement regulative superposition;connectors communities servers semantics synchronization sociology statistics;subsystem;community diagram architecture based development dynacomm dynamic reconfiguration hierarchical design architecture description language adl software structuring software architecture coordination principle categorical framework component specification composition system specification system architectural change specification dynacomm language fault tolerant client server system design dynamic client server system design dynacomm design community design;software architecture;interface manager;architecture description language adl;software architecture client server systems formal specification;regulative superposition	Architecture Description Languages (ADLs) were developed to support the abstract level of software structuring that is the subject matter of software architecture. Community is an ADL built on co-ordination principles and a categorical framework to support the composition of specifications of components to form the system's specification. However, an important shortcoming of Community is the lack of integrated support for specifying the system's architectural changes in both the set of components and the connections between them. This paper presents DynaComm, an extension of Community to support hierarchical design and dynamic reconfiguration of component based systems. The architectural design principles supported by the DynaComm language are illustrated through the design of a fault-tolerant, dynamic client-server system. A state anchored semantics of DynaComm is given by reducing a DynaComm design to a Community design, i.e., a configuration of components, cables and superposition morphisms, and then taking the colimit of the Community diagram. The careful use of extension, refinement and superposition morphisms guarantees that this is always possible.	architecture description language;client–server model;continuous design;diagram;fault tolerance;high-level programming language;microsoft windows 1.0;quantum superposition;refinement (computing);server (computing);software architecture;subject matter expert turing test	Xiang Ling;T. S. E. Maibaum	2012	2012 19th Asia-Pacific Software Engineering Conference	10.1109/APSEC.2012.109	software architecture;real-time computing;computer science;systems engineering;software engineering;formal specification;component;database;system;refinement;programming language	SE	-37.992808841576455	38.57801815662797	37855
cbaad0226a5f7e3586177a2f905268b9dac7f99b	a framework for reliability-aware embedded system design on multiprocessor platforms	reliability;design optimization;model driven development;embedded systems;fault tolerance	This paper presents a model-driven framework that provides a tool-supported design flow for fault-tolerant embedded systems. Its system models comprise abstract descriptions of the application and the underlying execution platform. They provide the input to our analysis and optimization techniques that enable the automated exploration of design alternatives for applications with reliability requirements. The automated generation of source code and platform configuration files speeds up the development process. Our contribution is to advance reliability-aware design further into practice by providing an integrated tool framework and removing unrealistic assumptions in the analyzes. The case studies demonstrate the effectiveness of our approach.	embedded system;multiprocessing;systems design	Jia Huang;Simon Barner;Andreas Raabe;Christian Buckl;Alois Knoll	2014	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/j.micpro.2014.02.007	embedded system;fault tolerance;computer architecture;parallel computing;real-time computing;multidisciplinary design optimization;computer science;operating system;reliability	EDA	-42.38752103240713	34.73825744413045	37859
1c83ce64042f5d28ff63065af8a0ec8ba84a7ecd	a ui-dspl approach for the development of context-adaptable user interfaces		Unlike adaptive interfaces which use sensors to adapt themselves, adaptable interfaces need the intervention of end users to adapt their different aspects according to user requirements. These requirements are commonly expressed according to the context of use. This latter was defined by the triplet <platform, environment, user> where the platform refers to the physical device and the device software, the environment refers to the physical environment in which the application is used and the user element refers to the user preferences and user profile. In this paper, we define a dynamic software product line (DSPL) approach for the development of a family of context-adaptable user interfaces. The DSPL paradigm exploits the knowledge acquired in software product line engineering to develop systems that can be context-aware, or runtime adaptable. Our approach satisfies a set of contributions which will be validated by implementing and evaluating them according to an illustrative case study.	complex adaptive system;peripheral;programming paradigm;requirement;sensor;software product line;triplet state;user (computing);user interface;user profile;user requirements document	Thouraya Sboui;Mounir Ben Ayed;Adel M. Alimi	2018	IEEE Access	10.1109/ACCESS.2017.2782880	end user;computer science;distributed computing;software product line;user requirements document;software;exploit;user profile;user interface	SE	-41.60777089079745	42.76747261229484	37866
44e01656fcf94048177efd51246f8dad12f1347c	security, privacy and anonymity in computation, communication and storage		Due to the fast growth of emerging information technologies such as Internet of Things (IoT), cloud computing, Internet services, and social networking, an increasing interest in big data security and privacy is aroused. An entire lifetime of big data contains four phases: big data collection; transmission; processing and analytics; storage and management. However, the five salient features of big data: volume, variety, velocity, value, and veracity bring great challenges on protecting big data security and privacy during its whole lifetime. In this paper, we survey schemes and techniques that are applied to ensure big data security and privacy. Based on the literature review, we discuss open chal‐ lenges and issues in this research area towards comprehensive protection on big data security and privacy in its lifetime.	big data;cloud computing;computation;data security;internet of things;privacy;velocity (software development);veracity;web service	Guojun Wang;Indrakshi Ray;Jose M. Alcaraz Calero;Sabu M. Thampi	2016		10.1007/978-3-319-49145-5	privacy software;information privacy;internet privacy;world wide web;computer security	Security	-44.41727125645611	51.47490851373284	37939
f0b8ae9cc53f9f1cd88abaa6b4b44f64236b394a	cp-nets based methodology for integrating functional verification and performance analysis of network protocol	computers;analytical models;protocols;functional cp nets models cp nets based methodology functional verification integration network protocol performance analysis current model based protocol engineering project colored petri nets protocol design bittorrent protocol;functional verification;network protocol;measurement;design engineering;distributed computing;protocol design;colored petri nets;satisfiability;software engineering;functional cp nets models;protocols formal verification graph colouring petri nets;bittorrent protocol colored petri nets functional verification performance analysis;formal verification;monitoring;stochastic processes;network protocol performance analysis;bittorrent protocol;bittorrent;performance analysis;performance model;colored petri net;functional model;integral functional;artificial intelligence;functional requirement;petri nets;peer to peer computing;cp nets based methodology;protocol engineering;current model based protocol engineering project;performance analysis protocols petri nets computers analytical models stochastic processes software engineering artificial intelligence distributed computing design engineering;graph colouring;functional verification integration;data models	It is very risky to improve the performance of network protocols without the assurance of its functional correctness, especially for protocols that with complicated and concurrent behaviors. However, in most of current model based protocol engineering projects, two independent models are adopted for individual functional verification and performance analysis, which could not guarantee the performance model satisfying the functionality correctness, and usually cost more in protocol design and maintenance. In this paper, we propose a colored Petri nets (CP-nets) based method to integrate functional verification and performance analysis procedures, and focus on the BitTorrent protocol as a representative example to illustrate the practical effectiveness of our proposed methodology. That is, the functional CP-nets models of BitTorrent protocol are constructed and validated firstly, and then performance related temporal constrains are added into above models to form its performance CP-nets models for corresponding simulation based performance analysis. Because such closely related CP-nets models are utilized where every occurrence sequence in the performance model corresponds to an occurrence sequence in its functional model, it is guaranteed that both models satisfy the functionality requirements of protocol systems. Besides, model maintenance becomes more convenient.	bittorrent;cp/m;communications protocol;correctness (computer science);function model;futures studies;petri net;profiling (computer programming);requirement;simulation;systems design	Jing Liu;Xinming Ye;Jun Li	2010	2010 11th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing	10.1109/SNPD.2010.16	stochastic process;communications protocol;real-time computing;computer science;theoretical computer science;software engineering;distributed computing;functional verification;statistics;computer network	SE	-46.10870757135163	34.30558385980041	38024
765163d4319c369195a20a168a79ec93f683f1f5	a security architecture for microprocessors		The Security Architecture for Microprocessors (SAM ) is a lightweight and high-performance combined hardand software security extension for microprocessors. SAM has been designed to provide a secure remote code execution environment. It can be used to implement effective copy-protection schemes and provides mechanisms to prevent data and algorithm disclosure. SAM provides protection even if an attacker has full access to both the operating system and hardware. SAM uses an enhanced processor core which can be used as a drop in replacement for a standard processor to provide transparent encryption and hashing of memory contents to prevent external tampering and sniffing attacks. Further internal security-related extensions support a secure operating system implementation. Both the hardware and software design are presented in this thesis.	algorithm;application security;arbitrary code execution;computer security;copy protection;encryption;microprocessor;multi-core processor;operating system;software design	Jörg Platte	2008			embedded system;applications architecture;computer engineering	Arch	-53.6722024214327	56.14975874230255	38030
b5160a6c5006745244fd2cae5e11e58bf051b330	decentralized documents authoring system for decentralized teamwork: matching architecture with organizational structure	p2p;collaborative systems;virtual communities	While systems for collaborative distributed works focus on enhancing distributed work group productivity, little attention has been paid to their architecture. In fact, most of these systems rely on centralized ones for both user communications and data hosting. These architectures raise issues about the administrative control, maintenance and management of the central entity. In this paper, we present a new architecture based on peer-to-peer (P2P) model driven by user relationship. In our architecture, users choose the trusted co-workers they are connected with. Thus, only the most trusted users manage to obtain a high number of connections which grant them a relative authority inside the system.	centralized computing;peer-to-peer	Frédéric Merle;Aurélien Bénel;Guillaume Doyen;Dominique Gaïti	2012		10.1145/2389176.2389195	computer science;knowledge management;operating system;peer-to-peer;distributed computing;distributed system security architecture;management;world wide web;collaboration	DB	-34.33394589175847	55.97708690662383	38049
ba4f0e1f376c5811cc543799b7f27a3d54981227	context variability modeling for runtime configuration of service-based dynamic software product lines	context awareness;context variability;process variability;data aware systems	In emerging domains such as Cloud-based Industrial Control Systems (ICSs) and SCADA systems where data-intensive and high performance computing are needed, a higher degree of flexibility is being demanded to meet new stakeholder requirements, context changes and intrinsic complexity. In this light, Dynamic Software Product Lines (DSPLs) provide a way to build self-managing systems exploiting traditional product line engineering concepts at runtime. Although context-awareness is widely perceived to be a first-class concern in such runtime variability mechanisms, existing approaches do not provide the necessary level of formalization to model and enact context variability for DSPLs. This is crucial for operational analytics processes since variant configuration could differ from context to context depending on diverse data values linked to context features and cross-tree constraints in a feature model. In this paper, we propose a context variability modeling approach, demonstrate its applicability and usability via a wind farm use case, and present the fundamental building blocks of a framework for enabling context variability in service-based DSPLs which provide Workflow as a Service (WFaaS).	context awareness;control system;data-intensive computing;feature model;heart rate variability;knowledge-based configuration;requirement;run time (program lifecycle phase);software product line;spatial variability;supercomputer;usability	Aitor Murguzur;Rafael Capilla;Salvador Trujillo;Óscar Ortiz;Roberto Erick Lopez-Herrejon	2014		10.1145/2647908.2655957	reliability engineering;real-time computing;computer science;systems engineering;context model	SE	-42.718150733324755	41.2157137111715	38132
199050627355272413eb64e8f846e3c7ed06dd1f	towards understanding the importance of variables in dependable software	error recovery;instruments;measurement;software maintenance;pervasive computing;error recovery mechanisms;software systems;importance metric dependability fault injection spatial impact temporal impact variable importance;runtime;transient analysis;variable importance;system recovery error detection software reliability;qa76 electronic computers computer science computer software;protection;aircraft manufacture;system recovery;importance metric;software safety;temporal impact;spatial impact;error correction;dependability;erroneous software state;computer science;error detection;software design;fault injection;software reliability;error detection mechanisms;dependable software system;computer errors;aircraft;importance metric dependable software system error detection mechanisms error recovery mechanisms erroneous software state;software systems computer errors computer science protection software maintenance software safety error correction software design pervasive computing runtime;qa76 computer software	A dependable software system contains two important components, namely, error detection mechanisms and error recovery mechanisms. An error detection mechanism attempts to detect the existence of an erroneous software state. If an erroneous state is detected, an error recovery mechanism will attempt to restore a correct state. This is done so that errors are not allowed to propagate throughout a software system, i.e., errors are contained. The design of these software artefacts is known to be very difficult. To detect and correct an erroneous state, the values held by some important variables must be ensured to be suitable. In this paper we develop an approach to capture the importance of variables in dependable software systems. We introduce a novel metric, called importance, which captures the impact a given variable has on the dependability of a software system. The importance metric enables the identification of critical variables whose values must be ensured to be correct.	dependability;error detection and correction;software bug;software system	Matthew Leeke;Arshad Jhumka	2010	2010 European Dependable Computing Conference	10.1109/EDCC.2010.20	reliability engineering;real-time computing;error detection and correction;computer science;engineering;theoretical computer science;operating system;software engineering;ubiquitous computing;software metric	SE	-62.10052673498526	32.4892754650413	38161
d89969416f915734d9cc9fb534e3b20e39dbc6ab	security considerations in a multi-programmed computer system	reasonable step;minimum requirement;security consideration;multi-programmed computer system;possible loss;security system;large multi-programmed system;adequate security level;remote terminal;classified information;absolute sense	Security can not be attained in the absolute sense. Every security system seeks to attain a probability of loss which is commensurate with the value returned by the operation being secured. For each activity which exposes private, valuable, or classified information to possible loss, it is necessary that reasonable steps be taken to reduce the probability of loss. Further, any loss which might occur must be detected. There are several minimum requirements to establish an adequate security level for the software of a large multi-programmed system with remote terminals.	computer;requirement	Bernard Peters	1967		10.1145/1465482.1465524	reliability engineering;telecommunications;engineering;security testing;computer security	Crypto	-61.30681594574681	50.02099980649996	38187
3a85658fe1ddb62d2192eac43d3c32d0fa75738b	portable and efficient continuous data protection for network file servers	loss measurement;file servers;protocols;protection file servers file systems operating systems hardware humans delay computer science protocols loss measurement;disk access interface;protection;security of data file organisation;operating system;design and implementation;file system;network file servers;continuous data protection;user level continuous data protection continuous data protection network file servers file system disk access interface operating systems;humans;computer science;security of data;user level continuous data protection;file systems;operating systems;hardware;file organisation	Continuous data protection, which logs every update to a file system, is an enabling technology to protect file systems against malicious attacks and/or user mistakes, because it allows each file update to be undoable. Existing implementations of continuous data protection work either at disk access interface or within the file system. Despite the implementation complexity, their performance overhead is significant when compared with file systems that do not support continuous data protection. Moreover, such kernel-level file update logging implementation is complex and cannot be easily ported to other operating systems. This paper describes the design and implementation of four user-level continuous data protection implementations for NFS servers, all of which work on top of the NFS protocol and thus can be easily ported to any operating systems that support NFS. Measurements obtained from running standard benchmarks and real-world NFS traces on these user-level continuous data protection systems demonstrate a surprising result: Performance of NFS servers protected by pure user-level continuous data protection schemes is comparable to that of unprotected vanilla NFS servers.	benchmark (computing);byte;central processing unit;context switch;continuous data protection;denial-of-service attack;disk image;disk mirroring;embedded system;file server;ibm notes;information privacy;magnetic storage;malware;operating system;overhead (computing);requirement;server (computing);software deployment;software portability;synthetic intelligence;system image;throughput;tracing (software);undo;user space	Ningning Zhu;Tzi-cker Chiueh	2007	37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)	10.1109/DSN.2007.74	continuous data protection;file server;communications protocol;real-time computing;computer file;network file system;computer science;stub file;versioning file system;operating system;unix file types;database;distributed computing;open;computer security;computer network	OS	-54.17429466939012	57.67857847231085	38235
92e0990b4bd39fd44bcd98d991eec29ef900fd6f	stackfences: a run-time approach for detecting stack overflows	buffer overflows;dependability	This paper describes StackFences, a run-time technique for detecting overflows in local variables in C programs. This technique is different from all others developed so far because it tries to detect explicit overflow occurrences, instead of detecting if a particular stack value, namely a return address, was corrupted because of a stack overflow. Thus, StackFences is useful not only for detecting intrusion attempts but also for checking the run-time robustness of applications. We also conceived different policies for deploying StackFences, allowing a proper balancing between detection accuracy and performance. Effectiveness tests confirmed that all overflows in local variables are detected before causing any severe damage. Performance tests ran with several tools and parameters showed an acceptable performance degradation.	assignment zero;bellard's formula;buffer overflow protection;call stack;computer security;correctness (computer science);elegant degradation;icdcs;intrusion detection system;local variable;overhead (computing);p (complexity);rapid application development;real-time transcription;recovery time objective;sensor;stack overflow;stack buffer overflow;test suite;tiny c compiler	André Zúquete	2004			real-time computing;engineering;operating system;computer security	SE	-56.50355655905504	56.141619025274586	38255
7e99680db4ebc4bc15307bc84c131edc8cc834e0	gui testing for introductory object-oriented programming exercises		Automated testing is necessary in large classrooms where many students learn a programming language. This paper presents a method to test and score student programs with graphical user interfaces written in JavaFX. The method is based on scripts that analyzes the structure of programs under test and simulates user’s interactions. We implemented several utility methods to write the testing scripts easy. No additional software library is required to run the scripts. Preliminary evaluation results are shown on the developing and executing of scripts for real exercises in our introductory programming classrooms.	graphical user interface testing	Ushio Inoue	2018		10.1007/978-3-319-96806-3_1	programming language;object-oriented programming;software;graphical user interface testing;distributed computing;scripting language;computer science;graphical user interface	PL	-53.874695384788204	37.01321298557277	38304
e9978c9bfc6ddc64a97d6fc01206800edc175a86	design and implementation of capability opening engine in paas for internet of vehicles	internet of vehicles;engines vehicles intelligent vehicles platform as a service telecommunications;web service;logistics location service capability opening engine internet of vehicle paas iov industry web service capability opening interface requirement analysis static structure dynamic structure;service capability opening;engines;platform as a service;intelligent vehicles;web service internet of vehicles service capability opening;vehicles;web services application program interfaces cloud computing internet of things systems analysis;telecommunications	In this paper, a capability opening engine for Internet of Vehicles (IOV) PaaS is designed and implemented to solve the problems currently existing in IOV industry such as high threshold of development and industry barriers. Through the analysis of service capability opening technologies, Web Service is preferred to implement the capability opening interfaces. Partial typical interfaces are designed after the requirement analysis of capability opening interfaces. The static and dynamic structure of the system will be introduced as a key point. At last, the application in logistics location service verifies the feasibility of the system.	cloud computing;giove;internet;location-based service;logistics;network convergence;platform as a service;requirements analysis;scalability;single-root input/output virtualization;software deployment;software development kit;web service	Hongman Wang;Xiaohan Xia;Yueming Gao;Hongmin Zhou	2014	2014 Asia-Pacific Services Computing Conference	10.1109/APSCC.2014.37	web service;embedded system;simulation;computer science;law;computer security;computer network	Mobile	-37.57521710329955	53.54529581944728	38335
c1f589626f10a339faf822b9db966ebce2fde01e	control flow ambiguous-type inter-procedural semantic analysis for dynamic language compilation		Abstract: The recent expansion of cloud-based solutions highlights that legacy programming languages and technologies, such as the PHP language, are still in heavy use. Furthermore, it turns out that for their effective integration into modern platforms, it is important to understand the legacy code base to provide modern analysis, testing and eventually compiler tooling. The dynamic language PHP would be the perfect candidate for such a synergy, due to its extensive usage, frequent issues in source codes and the need for a better performance and scalability. In this paper, we describe a solution that combines known static language code analysis techniques and that is enhanced by numerous modifications. As a result, we have a design and implementation of a PHP dynamic language code analysis, compiler and runtime, which improves performance and allows for more advanced high level code analysis and tools, all incorporated into the modern platform of .NET Core while taking advantage of the Microsoft Roslyn Compiler Platform.	control flow	Jakub Mísek;Filip Zavoral	2017		10.1016/j.procs.2017.05.452	programming language;data mining;compiler;dynamic compilation;static program analysis;functional compiler;language code;computer science;source code;code generation;compile time	PL	-54.56328444912749	38.67526931541731	38339
44eb1eb1013a35940fc1ab424dd1aad411d205f0	how type errors were fixed and what students did		Providing better supports for debugging type errors has been an active research area in the last three decades. Numerous approaches from different perspectives have been developed. Most approaches work well under certain conditions only, for example, when type errors are caused by single leaves and when type annotations are correct. However, the research community is still unaware of which conditions hold in practice and what the real debugging situations look like. We address this problem with a study of 3 program data sets, which were written in different years, using different compilers, and were of diverse sizes. They include more than 55,000 programs, among which more than 2,700 are ill typed. We investigated all the ill-typed programs, and our results indicate that current error debugging support is far from sufficient in practice since only about 35% of all type errors were caused by single leaves. In addition, type annotations cannot always be trusted in error debuggers since about 30% of the time type errors were caused by wrong type annotations. Our study also provides many insights about the debugging behaviors of students in functional programming, which could be exploited for developing more effective error debuggers.	compiler;debugger;debugging;effective method;error message;functional programming	Baijun Wu;Sheng Chen	2017	PACMPL	10.1145/3133929	theoretical computer science;debugging;functional programming;computer science	PL	-55.32332280259704	39.931086115525844	38376
637418d9b3a4ef3838320761c04a222619f9bb48	an ontology for interoperability: modeling of composite services in the smart home environment	smart home;composite services;interoperability;ontology	A developed service for smart home environments controls and coordinates a variety of appliances to influence physical environmental parameters selectively and to create a pleasant and activity appropriated room atmosphere. In the meaning of this paper this kind of service is defined as a composite service. There are several approaches to map services in ontologies. But these ontologies are not able to map composite services as they are introduced here. In this paper a possibility for mapping composite services with an ontology is presented. The composite services are defined on a devices landscape, with devices using exclusively DC voltage. The mapping of the composite services is done to promote compatibility to other systems. The paper’s scope is to show recent research activities and their partial results. Because the work is still in process final results are outstanding.	home automation;interoperability	Manja Görner;Thomas Göschel;Stephan Kassel;Sabrina Sander;Thomas Klein	2015		10.1007/978-3-662-47157-9_3	engineering;knowledge management;data mining;services computing;world wide web	HCI	-41.84624770559756	44.61027284403701	38385
8a2f87a85496de60a131190548168f7bc9b87297	hierarchical supervisors for automatic detection of software failures	automatic collection;fault localization;failure data;computational efficiency telephony law legal factors testing software reliability laboratories application software fault detection real time systems;program diagnostics;software supervision;formal specification;specification nondeterminism;application software;random telephone traffic;online failure detection;failure detection;real time;software failure detection;software performance evaluation;software fault tolerance;telecommunication computing;real time reactive systems;call processing software;testing;automatic programming;telephone exchanges;telephony;law;telephone exchanges software performance evaluation software fault tolerance automatic programming program diagnostics formal specification computational complexity telecommunication computing;legal factors;seeded failures;automatic detection;direct single layer supervisor;base supervisor layer;computational complexity;small telephone exchange;fault detection;direct single layer supervisor hierarchical supervisors automatic detection software failure detection software supervision correct behavior requirement specification online failure detection real time reactive systems fault localization automatic collection failure data hierarchical approach detailed behavior checking path detection layer base supervisor layer computational cost specification nondeterminism call processing software small telephone exchange random telephone traffic seeded failures;reactive system;computational cost;correct behavior;detailed behavior checking;hierarchical supervisors;computational efficiency;path detection layer;software reliability;requirement specification;real time systems;hierarchical approach	Software supervision is an approach to the automatic detection of software failures. A supervisor observes the inputs and outputs of a target system. It uses a model of correct behavior, derived from the target system's requirement specification. Discrepancies between specified and observed behaviors are reported as failures. Applications of the supervisor include online failure detection in real time reactive systems, fault localization and automatic collection of failure data. The paper describes a hierarchical approach to supervision. The approach differs from previous approaches in that supervision is split into two sub problems: tracking the behavior of the target system and detailed behavior checking. The architecture of the hierarchical supervisor has two layers: the path detection layer and the base supervisor layer. The hierarchical approach results in a significant reduction in computational cost arising from specification nondeterminism. The approach was evaluated by supervising the call processing software of a small telephone exchange, executed under random telephone traffic at different loads. Several thousand failures were individually seeded into the output generated by the exchange. The supervisor was able to detect the presence of all seeded failures. Reductions in computational cost of several orders of magnitude were measured in comparison with the direct, single layer supervisor.		Tony Savor;Rudolph E. Seviora	1997		10.1109/ISSRE.1997.630847	reliability engineering;application software;real-time computing;telephone exchange;reactive system;computer science;operating system;software engineering;formal specification;distributed computing;software testing;telephony;computational complexity theory;fault detection and isolation;software quality;software fault tolerance	Logic	-48.97396217200158	36.593280937306226	38503
8fbb02ab52abf65da9e98641d6553ed515396fdd	the jdevan tool suite in support of object-oriented evolutionary development	asynchronous api evolution;refactoring detection;object oriented;software development;evolutionary software development;model differencing	An apparatus for use in a firing range, in which transducers located adjacent the target area detect the airborne shock wave from supersonic projectiles. The position at which each projectile passes through the target plane is determined from the relative time of arrival of the associated shock wave at the transducers. A visual display of the target and the projectile position is provided for the use of range personnel.		Zhenchang Xing;Eleni Stroulia	2008		10.1145/1370175.1370203	real-time computing;computer science;systems engineering;software development;software engineering;programming language;object-oriented programming;software development process	Logic	-50.514278860773324	35.2851787810224	38516
c7331220c3e2cd57d4ac039a354e367b29b236a6	validating personal requirements by assisted symbolic behavior browsing	internet;electronic commerce;electronic mail;formal specification;formal verification;gstview tool;openmodel paradigm;web services;anonymity;assisted symbolic behavior browsing;e-commerce;email;high level personal requirements validation;privacy;telecommunications	Risks and hazards abound for users of today's large scale distributed telecommunications and e-commerce systems. Service nodes are documented loosely and incompletely, omitting functional details that can violate stakeholder requirements and thwart high level goals. For example, it is not enough to know that a book finding service locates a book for no more than a set price; does the chosen book vendor use an acceptable delivery mode and service? Does it retain or abuse personal information? The OpenModel paradigm provides the basis for a solution: instead of interface information alone, each node publishes a behavioral model of itself. However, large scale and multi-stakeholder systems rule out the use of traditional validation technologies, because state spaces are far too large and incompletely known to support concrete simulation, exhaustive search, or formal proof. Moreover, high level personal requirements like privacy, anonymity, and task success are impossible to formalize completely. This work describes a new methodology, assisted symbolic behavior browsing, and an implemented tool, GSTVIEW, that embodies it to help the user recognize potential violations of high level requirements. The paper also describes case studies of applying GSTVIEW in the domains of email and Web services.	approximation algorithm;behavioral modeling;brute-force search;display resolution;distributed computing;e-commerce;email;formal proof;high-level programming language;period-doubling bifurcation;personally identifiable information;privacy;programming paradigm;requirement;simulation;tree (data structure);usability;web service	Robert J. Hall;Andrea Zisman	2004	Proceedings. 19th International Conference on Automated Software Engineering, 2004.	10.1109/ASE.2004.10064	e-commerce;behavioral modeling;web service;the internet;formal verification;computer science;state space;engineering;software engineering;brute-force search;formal specification;database;programming language;world wide web;computer security	SE	-53.38074028371055	59.95287291864333	38531
5c2e5464f280ef0fc289b3fddcd2ff431f3040c6	the internet of things: a survey from the data-centric perspective	bepress selected works;pervasive computing;the internet of things pervasive computing ubiquitous computing;ubiquitous computing;the internet of things	Advances in sensor data collection technology, such as pervasive and embedded devices, and RFID Technology have lead to a large number of smart devices which are connected to the net and continuously transmit their data over time. It has been estimated that the number of internet connected devices has overtaken the number of humans on the planet, since 2008. The collection and processing of such data leads to unprecedented challenges in mining and processing such data. Such data needs to be processed in real-time and the processing may be highly distributed in nature. Even in cases, where the data is stored offline, the size of the data is often so large and distributed, that it requires the use of big data analytical tools for processing. In addition, such data is often sensitive, and brings a number of privacy challenges associated 384 MANAGING AND MINING SENSOR DATA with it. This chapter will discuss a data analytics perspective about mining and managing data associated with this phenomenon, which is now known as the internet of things.	big data;embedded system;humans;information privacy;internet of things;online and offline;pipeline (computing);radio-frequency identification;real-time computing;real-time locating system;sensor;smart device;tag (metadata)	Charu C. Aggarwal;Naveen Ashish;Amit P. Sheth	2013		10.1007/978-1-4614-6309-2_12	context-aware pervasive systems;web of things;distributed computing;internet privacy;world wide web;ubiquitous computing	ML	-42.3193002712615	49.64362219072963	38573
9798751aa7ea5ab172535dff5f93e830a0497d70	a computer-algebraic approach to formal verification of data-centric low-level software	computers;analytical models;software;market research;computational modeling;algebra;hardware	Methods of Computer Algebra have shown to be useful when formally verifying data-centric hardware designs. This has been demonstrated especially for cases where complex arithmetic computations are tightly coupled with the system's control structures at the bit level. As a consequence of current design trends, however, more and more functionality that was traditionally implemented in hardware is now shifted into the low-level software of the system. Not only control functions but also more and more arithmetic operations and other data-centric functions are involved in this shift. Motivated by this observation, it is the goal of our work to extend the scope of computer-algebraic methods from hardware to low-level software. The paper develops how hardware-dependent software can be modeled algebraically so that efficient proof procedures are possible. Our results show that also in low-level software a computer-algebraic approach can have substantial advantages over state-of-the-art SMT solving.	algorithm;bit-level parallelism;boolean satisfiability problem;component-based software engineering;control flow;control function (econometrics);correctness (computer science);digital signal processor;embedded software;embedded system;formal verification;global serializability;high- and low-level;linear algebra;low-level programming language;niche blogging;subroutine;symbolic computation;verification and validation	Oliver Marx;Carlos Villarraga;Dominik Stoffel;Wolfgang Kunz	2016	2016 ACM/IEEE International Conference on Formal Methods and Models for System Design (MEMOCODE)	10.1109/MEMCOD.2016.7797743	market research;computing;formal methods;software sizing;architectural pattern;software verification;computer science;backporting;theoretical computer science;software framework;component-based software engineering;software development;software design description;software engineering;software construction;hardware architecture;programming language;computational model;algorithm;software metric;software system	EDA	-48.08048508668201	33.9205261610725	38596
eff9a94e0ad2b8450b299ecc05732d196bdc072e	end-to-end secure and privacy preserving mobile chat application		Since the 1990s, two technologies have reshaped how we see and experience the world around us. These technologies are the Internet and mobile communication, especially smartphones. The Internet provides a cheap and convenient way to explore and communicate with distant people. A multitude of services have converged on the smartphone platform, and potentially the most notable is social networking. With increased interconnectivity and use of online services, concerns about consumers' security and privacy are growing. In this paper, we evaluate the securityand privacy-preserving features provided by existing mobile chat services. This paper also puts forwards a basic framework for an End-to-End (E2E) security and privacy-preserving mobile chat service and associated requirements. We implemented the proposal to provide proof-of-concept and evaluate the technical di culty of satisfying the stipulated security and privacy requirements.	e-services;internet;privacy;requirement;smartphone;social messaging	Raja Naeem Akram;Ryan K. L. Ko	2014		10.1007/978-3-662-43826-8_9	internet privacy;mobile computing;world wide web;computer security	Security	-40.91032667077884	53.00714923416442	38605
e66a575738ce6ef4e60be84e8b935a7a9b184d1d	research on workflow qos	analytical models;reliability;workflow management system workflow qos quality of service;quality of service monitoring costs delay contracts artificial intelligence computer science paper technology workflow management software control systems;service management;workflow management software business data processing quality of service;workflow qos;system performance;computational modeling;time factors;monitoring;business data processing;model quality of service workflow;model;workflow management software;workflow;workflow management system;quality of service;monitoring and control;business process	Abstract—In business processes, buyers and suppliers define a contract between the two parties such as quality of service(QOS) and quality of products. Organizations operating in modern markets require an excellent service management.When services or products are created or managed using workflow processes, the workflow management system should predict, monitor and control the QOS rendered to customers according to specifications of the contract. To achieve these objectives, an appropriate QOS model for workflow processes and methods to compute QOS are proposed.	business process;quality of service	Tao Lv	2009	2009 International Joint Conference on Artificial Intelligence	10.1109/JCAI.2009.110	workflow;mobile qos;quality of service;service management;computer science;knowledge management;reliability;computer performance;business process;computational model;workflow management system;workflow engine;workflow technology	DB	-48.25512858248382	43.00421358232377	38620
1f9716c06fbfbc13c63407a43a5c19109a0114ab	visualising object-oriented source code complexity using xml	complexity metrics;object oriented methods;visualization xml navigation html tagging systems engineering and theory humans programming profession animation feedback;object oriented;computational complexity;program visualisation xml computational complexity object oriented methods;xml;source code;software visualisation source code complexity xml java object oriented source codes complexity visualisation software development;program visualisation	In this paper a tool is described which employs a variety of XML technologies to enable Java object oriented source code applications to be visualised. The tool utilises various complexity metrics which have been rendered visually using effective graphical metaphors. The evaluation of the tool in a commercial setting is described and the results of its use by several development teams are presented. The work demonstrates the feasibility and strengths of using XML in visualising software.	graphical user interface;java;programming complexity;xml	R. M. Marks;F. George Wilkie	2004	Proceedings. Ninth IEEE International Conference on Engineering of Complex Computer Systems	10.1109/ICECCS.2004.1310914	simple api for xml;xml;computer science;theoretical computer science;xml framework;database;programming language;object-oriented programming;computational complexity theory;source code	SE	-54.41578487826519	35.06824574100346	38633
12c4e0a692745a36282f1af52a61ec028b8071f4	4sensing -- decentralized processing for participatory sensing data	masterthesis;distributed processing modeling;roads sensors peer to peer computing mobile communication aggregates pipelines computer architecture;programming language;domain specific programming language;sensors;data stream;distributed processing;smart phone;mobile computer;wide area sensing tasks;mobile computing participatory sensing decentralized processing data streaming;distributed processing development;computer architecture;4sensing;side effect;roads;aggregates;pipelines;sensor capable smart phones;participatory sensing;mobile communication;decentralized processing;experimental evaluation;peer to peer computing;mobile computing;distributed processing prototyping;domain specificity;data streaming;mobile computing 4sensing decentralized processing participatory sensing sensor capable smart phones wide area sensing tasks domain specific programming language distributed processing modeling distributed processing prototyping distributed processing development	Participatory Sensing is an emerging application paradigm that leverages the growing ubiquity of sensor-capable smart phones to allow communities carry out wide-area sensing tasks, as a side-effect of people's everyday lives and movements. This paper proposes a decentralized infrastructure for supporting Participatory Sensing applications. It describes an architecture and a domain specific programming language for modeling, prototyping and developing the distributed processing of participatory sensing data with the goal of allowing faster and easier development of these applications. Moreover, a case-study application is also presented as the basis for an experimental evaluation.	distributed computing;domain-specific language;participatory sensing;programming language;programming paradigm;smartphone	Heitor Ferreira;Sergio N. Duarte;Nuno M. Preguiça	2010	2010 IEEE 16th International Conference on Parallel and Distributed Systems	10.1109/ICPADS.2010.20	embedded system;real-time computing;mobile telephony;computer science;sensor;operating system;participatory gis;database;distributed computing;pipeline transport;programming language;mobile computing;computer security;side effect;computer network	Mobile	-40.49870051076793	49.04768521234938	38694
93679814843694c94795bd6f2726876d9f58e0cc	phoenix-based clone detection using suffix trees	software analysis;abstract syntax tree;suffix tree;software evolution;automatic detection;code clone;crosscutting concerns;source code;clone detection;suffix trees;lexical analysis;code clones	A code clone represents a sequence of statements that are duplicated in multiple locations of a program. Clones often arise in source code as a result of multiple cut/paste operations on the source, or due to the emergence of crosscutting concerns. Programs containing code clones can manifest problems during the maintenance phase. When a fault is found or an update is needed on the original copy of a code section, all similar clones must also be found so that they can be fixed or updated accordingly. The ability to detect clones becomes a necessity when performing maintenance tasks. However, if done manually, clone detection can be a slow and tedious activity that is also error prone. A tool that can automatically detect clones offers a significant advantage during software evolution. With such an automated detection tool, clones can be found and updated in less time. Moreover, restructuring or refactoring of these clones can yield better performance and modularity in the program.This paper describes an investigation into an automatic clone detection technique developed as a plug-in for Microsoft's new Phoenix framework. Our investigation finds function-level clones in a program using abstract syntax trees (ASTs) and suffix trees. An AST provides the structural representation of the code after the lexical analysis process. The AST nodes are used to generate a suffix tree, which allows analysis on the nodes to be performed rapidly. We use the same methods that have been successfully applied to find duplicate sections in biological sequences to search for matches on the suffix tree that is generated, which in turn reveal matches in the code.	abstract syntax tree;code refactoring;cognitive dimensions of notations;cross-cutting concern;duplicate code;emergence;function-level programming;lexical analysis;paste;plug-in (computing);qr code;software evolution;suffix tree	Robert Tairas;Jeffrey G. Gray	2006		10.1145/1185448.1185597	real-time computing;computer science;programming language;algorithm;duplicate code	SE	-57.41716908574493	36.204824234430866	38703
d45f7035e2468850f3c75dd2b57e60010c572faf	institution-governed cross-domain agent service cooperation: a model for trusted and autonomic service cooperation	trusted;multi agent;cooperation facilitation;virtual organization;policy driven;institution governed;autonomic;service cooperation	Constructing Virtual Organizations (VOs) by creating service cooperation (i.e. service-oriented cooperation) has become a mainstream approach for reforming the development of application software systems in Web environments. However, the inherent non-controllability of business services across different management domains has brought on the so-called “trust” crisis that the success and benefit of cooperation cannot be ensured. It is this crisis that cumbers the achievement of autonomic cooperation and thereby the large-scale deployment of VOs. Therefore, this paper proposes a model, called IGTASC, to conquer this crisis and make service cooperation both trusted and autonomic by developing three closely-coupled technologies: institution-governed cooperation, policy-driven self-management, and community facilitation management.	autonomic computing;autonomic networking;autonomy;interaction;real life;self-management (computer science);self-organization;semantics (computer science);service-orientation;service-oriented device architecture;service-oriented infrastructure;software deployment;software system;virtual organization (grid computing);web application	Ji Gao;Hexin Lv	2011	Applied Intelligence	10.1007/s10489-011-0323-y	knowledge management;computer security	Web+IR	-44.663067557522126	45.438619430344886	38765
a2bff3f928f98ec8329251f6a0d96376f1e8ebc5	special section on emerging multimedia technology for smart surveillance system with iot environment		The internet-of-things (IoT) can be defined as the interconnection of uniquely identifiable embedded computing devices within the existing Internet infrastructure. Typically, IoT is expected to offer advanced connectivity of devices, systems, and services that goes beyond machine-to-machine communications (M2M) and covers a variety of protocols, domains, and applications. The interconnection of these embedded devices (including smart objects) is expected to usher in automation covering all major engineering fields, while also enabling advanced applications such as smart grid and smart surveillance. Smart surveillance system is mainly composed of automatic video/audio analysis. Therefore, an emerging surveillance system must consider multimedia information for monitoring activities and extracting meaningful information from the environment. Researchers are working to solve a number of challenging questions that are often encountered during the designing of smart surveillance systems. Some of those interesting questions are tough and fundamental, but unavoidable: what are the applications of smart surveillance? What are the possible system architectures for smart surveillance? What are the core technologies? What are key technical challenges? What are the implications of smart surveillance, both to security and privacy?	embedded system;interconnection;machine to machine;smart tv;smart card;smart objects;technical standard	Brian Kim;Kostas E. Psannis;Harish Bhaskar	2016	The Journal of Supercomputing	10.1007/s11227-016-1939-9	embedded system;multimedia;computer security	EDA	-44.97886066154734	49.76964992463455	38874
a77b63f28f441b1af61673e0286e3b608b818f25	integrating the security aspect into design space exploration of embedded systems	formalization;embedded system design formalization security integration design space exploration;security of data automotive engineering embedded systems;integration;embedded system design;design space exploration;security hardware computer architecture real time systems resource management embedded systems timing;automotive embedded system security design space exploration embedded system design automated dse process;security	Conventionally, the process of design space exploration (DSE) in embedded system design considers performance, energy and cost as important objectives for optimization. However, in many domains such as in modern day cars the security aspect is becoming more and more significant. On the other hand, the inclusion of security aspect adds a new dimension to the existing complexity of large design spaces, thus an automated support for this is highly desired. The goal of this work is to integrate the security constraint in an automated DSE process to obtain an architecture which is both cost-optimized and secure. In specific, for a given system, our approach defines a formal notion of security, which along with other parameters is fed as an input to the DSE process to obtain an architecture satisfying the defined security and real-time requirements. An evaluation of the proposed approach is also performed using an example automotive embedded system.	computer security;design space exploration;embedded system;mathematical optimization;real-time clock;requirement;sociotechnical system;symmetric multiprocessing;systems design;systems engineering;terminator 2: judgment day	Ingo Stierand;Sunil Malipatlolla;Sibylle B. Fröschle;Alexander Stühring;Stefan Henkler	2014	2014 IEEE International Symposium on Software Reliability Engineering Workshops	10.1109/ISSREW.2014.29	software security assurance;computer security model;embedded system;real-time computing;computer science;engineering;information security;operating system;computer engineering	Embedded	-42.66541061714411	34.69798016893209	38890
1789032a8348fccf2a3daf68bc06ed4608dba25b	andromeda: accurate and scalable security analysis of web applications	information flow;integrity;static analysis;abstract interpretation;taint analysis;security	Security auditing of industry-scale software systems mandates automation. Static taint analysis enables deep and exhaustive tracking of suspicious data flows for detection of potential leakage and integrity violations, such as cross-site scripting (XSS), SQL injection (SQLi) and log forging. Research in this area has taken two directions: program slicing and type systems. Both of these approaches suffer from a high rate of false findings, which limits the usability of analysis tools based on these techniques. Attempts to reduce the number of false findings have resulted in analyses that are either (i) unsound, suffering from the dual problem of false negatives, or (ii) too expensive due to their high precision, thereby failing to scale to real-world applications. In this paper, we investigate a novel approach for enabling precise yet scalable static taint analysis. The key observation informing our approach is that taint analysis is a demand-driven problem, which enables lazy computation of vulnerable information flows, instead of eagerly computing a complete data-flow solution, which is the reason for the traditional dichotomy between scalability and precision. We have implemented our approach in ANDROMEDA, an analysis tool that computes data-flow propagations on demand, in an efficient and accurate manner, and additionally features incremental analysis capabilities. ANDROMEDA is currently in use in a commercial product. It supports applications written in Java, .NET and JavaScript. Our extensive evaluation of ANDROMEDA on a suite of 16 production-level benchmarks shows ANDROMEDA to achieve high accuracy and compare favorably to a state-of-the-art tool that trades soundness for precision.	.net framework;computation;cross-site scripting;dataflow;duality (optimization);failure;java;javascript;lazy evaluation;program slicing;sql injection;scalability;software system;spectral leakage;taint checking;type system;usability;web application	Omer Tripp;Marco Pistoia;Patrick Cousot;Radhia Cousot;Salvatore Guarnieri	2013		10.1007/978-3-642-37057-1_15	taint checking;information flow;computer science;information security;data mining;programming language;world wide web;computer security;static analysis	SE	-57.052548321419295	57.63669595957107	38897
a3edfed7a4b23f829ed34855bfa87ceb6b82c14a	a corba adapter to promote reusability of java components	difference operator;programming language;information technology;computer network;software component;middleware;distributed computing environment	As information technologies evolve, computing environments become more heterogeneous and distributed. More and more applications in different programming languages running on different platforms with different operating systems are required to communicate with each other over different computer networks. Conflicts resulting from having heterogeneous and distributed computing environments require strategies for interoperability. The emerging middleware technologies offer an industrial de facto standard communication infrastructure to support the interoperability of heterogeneous applications in components. However, the implementation of a component suffers from high interaction complexities in the component that seriously degrades the application independence. Software components should be built to be independent of the context in which they are used, allowing them to be reused in many different computing environments. In this paper, we are presenting an adapter to support the development of reusable CORBA components in Java. The adapter isolates, encapsulates, and manages a CORBA component's interactions outside the component. The use of adapters increases the reusability of CORBA components in Java and also simplifies the integration of the components to an application.	common object request broker architecture;java	Chia-Chu Chiang;Zhendong Chen	2004		10.1145/1071509.1071559	real-time computing;common component architecture;computer science;operating system;distributed computing	SE	-33.930748049873046	44.38267145369829	39014
8bb3fee503a72ed68d53c16a1deee365cc4d4a3d	a designer/verifiers's assistant	automated programmer assistance;proofs of programs;program specifications;maintenance;program design;question answering automated program verifier automated programmer assistance design of incremental systems effects of incremental changes incremental program design and verification maintenance program design program specifications program verification proofs of programs;program verification;automated program verifier;incremental program design and verification;operating systems file systems software maintenance induction generators prototypes proposals programming profession error correction drain avalanche hot carrier injection contracts;effects of incremental changes;design of incremental systems;program specification;question answering	Since developing and maintaining formally verified programs is an incremental activity, one is not only faced with the problem of constructing specifications, programs, and proofs, but also with the complex problem of determining what previous work remains valid following incremental changes. A system that reasons about changes must build a detailed model of each development and be able to apply its knowledge, the same kind of knowledge an expert would have, to integrate new or changed information into an existing model.		Mark Moriconi	1979	IEEE Trans. Software Eng.	10.1109/TSE.1979.234206	program analysis;question answering;incremental build model;computer science;software engineering;database;program design language;programming language	SE	-51.68630238570622	36.646351210776906	39025
f6d6e632eab27eaf0f98c4e8ea2e85e4894fa850	cybersafety: cybersecurity and safety-critical software engineering		A range of common software components are gradually being integrated into the infrastructures that support safety-critical systems. These include network management tools, operating systems – especially Linux, Voice Over IP (VOIP) communications technologies, and satellite based augmentation systems for navigation/timing data etc. The increasing use of these common components creates concerns that bugs might affect multiple systems across many different safety-related industries. It also raises significant security concerns. Malware has been detected in power distribution, healthcare, military and transportation infrastructures. Most previous attacks do not seem to have deliberately targeted critical applications. However, there is no room for complacency in the face of increasing vulnerability to cyber attacks on safety-related systems. This paper illustrates the threat to air traffic management infrastructures and goes on to present a roadmap to increase our resilience to future CyberSafety attacks. Some components of this proposal are familiar concepts from Security Management Systems (SecMS), including a focus on incident reporting and the need for improved risk assessment tools. Other components of the roadmap focus on structural and organizational problems that have limited the effectiveness of existing SecMS; in particular there is a need to raise awareness amongst regulators and senior management who often lack the technical and engineering background to understand the nature of the threats to safety-critical software.	component-based software engineering;computer security;linux;malware;management system;online predator;operating system;risk assessment;security management;software bug	Chris W. Johnson	2012		10.1007/978-1-4471-2494-8_8	component-based software engineering;network management;air traffic management;malware;computer security;life-critical system;senior management;security management;computer science;vulnerability	SE	-57.39433812349233	50.05236766809908	39026
e0daa1cdb881d102b9301ebbcafc2db817dd4439	romeo: reputation model enhancing openid simulator		OpenID is a standard decentralized initiative aimed at allowing Internet users to use the same personal account to access different services. Since it does not rely on any central authority, it is hard for such users or other entities to validate the trust level of each entity deployed in the system. Some research has been conducted to handle this issue, defining a reputation framework to determine the trust level of a relying party based on past experiences. However, this framework has been proposed in a theoretical way and some deeper analysis and validation is still missing. Our main contribution in this paper consist of a simulation environment able to validate the feasibility of the reputation framework and analyze its behaviour within different scenarios.	access control;authentication;entity;experience;malware;openid;romeo;relying party;simulation	Ginés Dólera Tormo;Félix Gómez Mármol;Gregorio Martínez Pérez	2014		10.1007/978-3-319-11851-2_15	internet privacy;world wide web;computer security	Web+IR	-47.047448038426616	57.946595134101415	39032
bd0d0c509e5efda01a1b0a079630b43e2b13349d	a pushing approach for data synchronization in cloud to reduce energy consumption in mobile devices	databases;synchronisation cloud computing energy consumption mobile computing power aware computing smart phones;current acquisition pushing approach data synchronization cloud computing energy consumption reduction mobile devices smartphone sales application server polling and pushing techniques energy consumption arduino ina219 voltage acquisition;servers;energy measurement;energy consumption;synchronization;games;mobile communication;mobile handsets;servers energy consumption games mobile communication synchronization databases mobile handsets;polling;smartphone;pushing;pushing energy consumption energy measurement smartphone polling	With the increase in smartphone sales and the use of clouds, data synchronization is an essential activity in the mobile applications that aim to keep data up-to-date with information from the application server. This paper proposes an analysis of the main approaches for data synchronization concerning energy, which are the polling and pushing techniques. The results show a substantial gain in terms of energy consumption for the pushing technique when compared to the polling technique in mobile applications. An experimental environment based on Arduino and INA219 that allows current and voltage acquisition of 138 samples per second was used. Results showed a significant increase of 187% in terms of energy consumption when the polling technique was applied, which was more advisable than the use of the polling technique when one or more requests in a forty minute range are not necessary to be made.	application server;arduino;data synchronization;energy drift;mobile app;mobile operating system;polling (computer science);server (computing);smartphone	S. A. L. Carvalho;Rafael Nunes de Lima;Abel G. da Silva Filho	2014	2014 Brazilian Symposium on Computing Systems Engineering	10.1109/SBESC.2014.16	embedded system;real-time computing;computer science;distributed computing	Embedded	-36.23981870018997	55.3270209227853	39042
f40af6167e0592fe22b47ee739121942349630c1	first-class labels: using information flow to debug security holes		We present a system of first-class labels that assists web authors in assessing and diagnosing vulnerabilities in web applications, focusing their attention on flows of information specific to their application. Using first-class labels, web developers can directly manipulate labels and express security policies within JavaScript itself, leveraging their existing knowledge to improve the quality of their applications. Introducing first-class labels incurs no additional overhead over the implementation of information flow in a JavaScript Virtual Machine, making it suitable for use in a security testing environment even for applications that execute large amounts of JavaScript code.	debug;information flow	Eric Hennigan;Christoph Kerschbaumer;Stefan Brunthaler;Per Larsen;Michael Franz	2013		10.1007/978-3-642-38908-5_12	operating system;world wide web;computer security	Crypto	-56.06641203975229	57.942555653208935	39080
3a2e06bd00f6904ffeeb5acef5c274e0ed753fe8	towards dependably detecting geolocation of cloud servers		Every physical data center is located somewhere on the globe. A cloud service can be delivered from a set of data centers in several locations, depending on their workload situation. Responsibilities of the service provider include ensuring that legal and agreed constraints are respected also by its subcontractors, for example, those providing cloud computing resources. Several countries have data protection legislation that restrict sharing copies of sensitive data to locations that do not have compliant legislation. This paper presents ideas to dependably detect location specific information, like the legislation properties, of the current physical host server executing a service.	geolocation	Leo Hippelainen;Ian Oliver;Shankar Lal	2017		10.1007/978-3-319-64701-2_51	computer network;service provider;computer security;cloud computing;computer science;data center;geolocation;server;data integrity;legislation;data protection act 1998	Metrics	-46.97088637836843	58.888223848953494	39098
2bd1df288d3f17817d35df2567b78d3ac47a2365	pcom - a component system for pervasive computing	user mobility;pervasive computing communication system control application software contracts automatic control control systems sensor phenomena and characterization communications technology programming profession computer architecture;pervasive computing;system provided strategies pcom component system pervasive computing user mobility high level programming abstraction automatic adaptation execution environment flexible adaptation control user supplied strategies;adaptive control;object oriented programming;object oriented programming ubiquitous computing middleware;ubiquitous computing;middleware	Applications in the pervasive computing domain are challenged by the dynamism in which their execution environment changes, e.g. due to user mobility. As a result, applications have to adapt to changes regarding their required resources. In this paper we present PCOM, a component system for pervasive computing. PCOM offers application programmers a high-level programming abstraction which captures the dependencies between components using contracts. The resulting application architecture is a tree formed by components and their dependencies. PCOM supports automatic adaptation in cases where the execution environment changes to the better or to the worse. User supplied as well as system provided strategies take users out of the control loop while offering flexible adaptation control.	applications architecture;control system;expect;high- and low-level;high-level programming language;instance (computer science);interdependence;middleware;overhead (computing);programmer;spontaneous order;state (computer science);tree (data structure);ubiquitous computing	Christian Becker;Marcus Handte;Gregor Schiele;Kurt Rothermel	2004	Second IEEE Annual Conference on Pervasive Computing and Communications, 2004. Proceedings of the	10.1109/PERCOM.2004.1276846	embedded system;real-time computing;context-aware pervasive systems;human–computer interaction;computer science;operating system;end-user computing;middleware;distributed computing;object-oriented programming;ubiquitous computing	HPC	-39.47918583665468	40.65594272773714	39124
611141c41a9a65acf00359e3d8685450c239dc6d	corgids: a correlation-based generic intrusion detection system		Cyber-physical systems (CPS) consist of software and physical components which are knitted together and interact with each other continuously. CPS have been targets of security attacks due to their safety-critical nature and relative lack of protection. Specification based intrusion detection systems (IDS) using data, temporal, data temporal and time, and logical correlations have been proposed in the past. But none of the approaches except the ones using logical correlations take into account the main ingredient in the operation of CPS, namely the use of physical properties. On the other hand, IDS that use physical properties either require the developer to define invariants manually, or have designed their IDS for a specific CPS. This paper proposes CORGIDS, a generic IDS capable of detecting security attacks by inferring the logical correlations of the physical properties of a CPS, and checking if they adhere to the predefined framework. We build a CORGIDS-based prototype and demonstrate its use for detecting attacks in the two CPS. We find that CORGIDS achieves a precision of 95.70%, and a recall of 87.90%, with modest memory and performance overheads.	cyber-physical system;intrusion detection system;invariant (computer science);prototype;sensor	Ekta Aggarwal;Mehdi Karimibiuki;Karthik Pattabiraman;André Ivanov	2018		10.1145/3264888.3264893	data mining;software;cyber-physical system;recall;intrusion detection system;correlation;internet of things;computer science	Security	-52.9172055136271	48.90889350504709	39141
36411155410214d0a1fa72b66820ff600689ad6a	keep your friends close: the necessity for updating an anomaly sensor with legitimate environment changes	concept drift;anomaly detection;model update	"""Large-scale distributed systems have dense, complex code-bases that are assumed to perform multiple and inter-dependent tasks while user interaction is present. The way users interact with systems can differ and evolve over time, as can the systems themselves. Consequently, anomaly detection (AD) sensors must be able to cope with updates to their operating environment. Otherwise, the sensor may incorrectly classify new patterns as malicious (a false positive) or assert that old or outdated patterns are normal (a false negative). This problem of """"model drift"""" is an almost universally acknowledged hazard for anomaly sensors. However, relatively little work has been done to understand the process of identifying and seamlessly updating an operational network AD sensor with legal modifications like changes to a file system or back-end database.  In this paper, we highlight some of the challenges of keeping an anomaly sensor updated, an important step toward helping anomaly sensors become a practical intrusion detection tool for real-world network and host environments. Our goal is to eliminate needless false positives arising from the gradual de-synchronization of the sensor from the environment it is monitoring. To that end, we investigate the feasibility of automatically deriving and applying a """"data"""" or """"model patch"""" that describes the changes necessary to update a """"reasonable"""" AD behavioral model (i.e., a model whose structure follows the core design principles of existing anomaly models). We propose an update procedure that is holistic in nature: specifically, we present preliminary results on how to update a sensor that monitors the request and response messages for non-dynamic HTTP requests and software patches. In addition, we propose extensions for dynamic, database-driven requests and responses."""	anomaly detection;behavioral modeling;distributed computing;holism;intrusion detection system;malware;operating environment;patch (computing);sensor	Angelos Stavrou;Gabriela F. Cretu-Ciocarlie;Michael E. Locasto;Salvatore J. Stolfo	2009		10.1145/1654988.1655000	engineering;data mining;world wide web;computer security	Security	-55.60697265240529	57.65075731373546	39267
816a4a551c02f994d28fcd9944880dc3e049e26a	vehicular cloud computing	intelligent transportation vehicular cloud computing mobile cloud computing mobile agents environment modeling content discovery data collection data dissemination internet cloud models mobile computing mobile vehicular cloud urban sensing;mobile agents;traffic engineering computing automated highways cloud computing data handling mobile agents mobile computing;automated highways;vehicles cloud computing mobile communication sensors navigation surveillance;traffic engineering computing;data handling;mobile computing;cloud computing	Mobile Cloud Computing is a new field of research that aims to study mobile agents (people, vehicles, robots) as they interact and collaborate to sense the environment, process the data, propagate the results and more generally share resources. Mobile agents collectively operate as Mobile Clouds that enable environment modeling, content discovery, data collection and dissemination and other mobile applications in a way not possible, or not efficient, with conventional Internet Cloud models and mobile computing approaches. In this paper, we discuss design principles and research issues in mobile cloud computing. We then focus on the Mobile Vehicular Cloud and review cloud applications ranging from urban sensing to intelligent transportation.	access control;content discovery platform;device driver;distributed computing;mathematical optimization;mobile agent;mobile app;mobile cloud computing;mobile computing;personal cloud;ramp simulation software for modelling reliability, availability and maintainability;robot;routing;software propagation;vehicle-to-vehicle	Mario Gerla	2012	2012 The 11th Annual Mediterranean Ad Hoc Networking Workshop (Med-Hoc-Net)	10.1109/MedHocNet.2012.6257116	cloud computing security;mobile search;mobile web;cloud computing;computer science;mobile technology;cloud testing;utility computing;internet privacy;mobile computing;computer security;provisioning;computer network;autonomic computing	Mobile	-41.655518674697156	51.42841645555941	39274
5b63e3bd15c9a0365bd8c89da5f7716548394e22	symbolic side-channel analysis for probabilistic programs		In this paper we describe symbolic side-channel analysis techniques for detecting and quantifying information leakage, given in terms of Shannon and min-entropy. Measuring the precise leakage is challenging due to the randomness and noise often present in program executions and side-channel observations. We account for this noise by introducing additional (symbolic) program inputs which are interpreted probabilistically, using symbolic execution with parametrized model counting. We also explore a sampling approach for increased scalability. In contrast to typical Monte Carlo techniques, our approach works by sampling symbolic paths, representing multiple concrete paths, and uses pruning to accelerate computation and guarantee convergence to the optimal results. A key novelty of our approach is to provide bounds on the leakage that are provably under- and over-approximating the exact leakage. We implemented the techniques in the Symbolic PathFinder tool and demonstrate them on Java programs.	computation;cryptographic protocol;information leakage;java;maxima and minima;monte carlo method;nonlinear system;randomized algorithm;randomness;sampling (signal processing);scalability;sensor;shannon (unit);side-channel attack;spectral leakage;symbolic execution	Pasquale Malacaria;M. H. R. Khouzani;Corina S. Pasareanu;Quoc-Sang Phan;Kasper Søe Luckow	2018	2018 IEEE 31st Computer Security Foundations Symposium (CSF)	10.1109/CSF.2018.00030	randomness;theoretical computer science;scalability;noise measurement;monte carlo method;probabilistic logic;the symbolic;computer science;symbolic execution;side channel attack	Logic	-55.38087122980759	52.56601804127727	39465
36c3e673bd346120039a749803c7fb0ce51fdb6b	cvsscan: visualization of code evolution	life cycle;user study;software systems;software evolution;source code;use case;software visualization	During the life cycle of a software system, the source code is changed many times. We study how developers can be enabled to get insight in these changes, in order to understand the status, history and structure better, as well as for instance the roles played by various contributors. We present CVSscan, an integrated multiview environment for this. Central is a line-oriented display of the changing code, where each version is represented by a column, and where the horizontal direction is used for time, Separate linked displays show various metrics, as well as the source code itself. A large variety of options is provided to visualize a number of different aspects. Informal user studies demonstrate the efficiency of this approach for real world use cases.	software system;usability testing	Lucian Voinea;Alexandru Telea;Jarke J. van Wijk	2005		10.1145/1056018.1056025	use case;kpi-driven code analysis;software visualization;biological life cycle;codebase;simulation;code review;human–computer interaction;computer science;software evolution;software framework;software development;software engineering;software construction;internal documentation;software quality;static program analysis;software system;source code	SE	-55.56117017372482	34.930812222656876	39536
d60305a20f064c90652fd720801ac6bf495ed301	software architecture of an x.400 electronic mail system	electronic mail;software architecture	Abstract   This paper describes the development of a prototype electronic mail system. An outline of the systems' architecture is followed by an account of design process. Special attention is given to a number of aspects (services) that are not part of the CCITT X.400 recommendations, such as online configuration management, system recovery, message recovery and reliability. Important characteristics of the architecture are described, including its modularity, flexibility and the ability to interface to non-OSI networks and services.	email;software architecture	Dieter K. Hammer	1988	Computer Communications	10.1016/0140-3664(88)90060-6	enterprise architecture framework;reference architecture;embedded system;software architecture;space-based architecture;database-centric architecture;computer science;applications architecture;operating system;solution architecture;software architecture description;computer security;data architecture;computer network;systems architecture	Networks	-38.2282423275827	38.07328760125873	39553
940346d1fcdc5c6cdb95e0be964f1d65a8bd2a83	a new software testing approach based on domain analysis of specifications and programs	software testing;program diagnostics;formal specification;code coverage;software fault tolerance;data type;software testing fault detection computer science chemical technology prototypes input variables software systems system testing software quality costs;test case generation;program testing;fault detection;program diagnostics program testing software fault tolerance formal specification;test case generation software testing input domain analysis software specification software program partition testing fault detection boundary test case selection functional domain operational domain adsod system nonnumeric data types code coverage;domain analysis	Partition testing is a well-known software testing technique. This paper shows that partition testing strategies are relatively ineffective in detecting faults related to small shifts in input domain boundary. We present an innovative software testing approach based on input domain analysis of specifications and programs, and propose the principle and procedure of boundary test case selection in functional domain and operational domain. The differences of the two domains are examined by analyzing the set of their boundary test cases. To automatically determine the operational domain of a program, the ADSOD system is prototyped. The system supports not only the determination of input domain of integer and real data types, but also non-numeric data types such as characters and enumerated types. It consists of several modules in finding illegal values of input variables with respect to specific expressions. We apply the new testing approach to some example studies. A preliminary evaluation on fault detection effectiveness and code coverage illustrates that the approach is highly effective in detecting faults due to small shifts in the input domain boundary, and is more economical in test case generation than the partition testing strategies.	boundary-value analysis;code coverage;domain analysis;domain testing;enumerated type;fault detection and isolation;level of measurement;sensor;software testing;test case;turing completeness;verification and validation;white-box testing	Ruilian Zhao;Michael R. Lyu;Yinghua Min	2003		10.1109/ISSRE.2003.1251031	domain analysis;non-regression testing;domain testing;reliability engineering;development testing;regression testing;real-time computing;fuzz testing;orthogonal array testing;software performance testing;white-box testing;manual testing;data type;system integration testing;integration testing;computer science;systems engineering;software reliability testing;software engineering;domain engineering;functional testing;software construction;formal specification;software testing;code coverage;programming language;data-driven testing;test management approach;fault detection and isolation;software fault tolerance	SE	-59.414974822919504	34.140499116505936	39571
2054a0f00133d08501901d31a1a491a08762d3f6	mauth: a fine-grained and user-centric permission delegation framework for multi-mashup web services	social network services;google;session management;protocols;authorization problems;multimashup web services;mashups;authorisation;information retrieval;permission delegation;mashups authorization protocols google social network services;web service;unauthorized access;social network;software architecture;interactive system;social networks;user centric permission delegation framework;web services;permission delegation mashup security access control;mashup;mauth;authorization;session management protocol mauth user centric permission delegation framework multimashup web services social networks business processes unauthorized access authorization problems extensible reference architecture;access control;web services authorisation information retrieval software architecture;security;data retrieval;reference architecture;extensible reference architecture;business process;session management protocol;business processes	Mashups are a new breed of interactive web applications that aggregate and stitch together data retrieved from one or more sources to create an entirely new and innovative set of services. The paradigm is not limited to social networks and many enterprises are redesigning their business processes to create interactive systems in the form of mashups. However, protecting users' private data from unauthorized access in mashups is a challenging security problem. Existing solutions for addressing the various authorization problems are limited due to all-or-nothing policy, third party dependence and scalability issues. In this paper, we present a general permission delegation model for mashups that is fine-grained, user centric and scalable. This contribution has the following objectives: We formally specify the dependency relationships among multiple web applications. Dependency relationships are categorized on the basis of specific data items. We present an extensible reference architecture for configuring multiple web applications and a session management protocol.	aggregate data;authorization;business process;categorization;image stitching;information privacy;mashup (web application hybrid);programming paradigm;reference architecture;scalability;session (computer science);social network;web application;web service	Masoom Alam;Xinwen Zhang;Mohammad Nauman;Sohail Khan;Quratulain Alam	2010	2010 6th World Congress on Services	10.1109/SERVICES.2010.112	database;business;internet privacy;world wide web	Security	-48.30070965628012	51.24316656451743	39588
9dd7a83a9442c7e77da6d267737a998971ad2874	iot4s: a new architecture to exploit sensing capabilities in smart cities	sensing;contiki;iot;sensor web enablement;smart cities;internet of things;big data;sensor networks;swe	Smart cities offer a challenging way to design future networks, optimising services, costs and management activities. This requires ICT technologies able to interact with the environment. In particular, sensing activities become essential to support new and advanced services for the public good. The drawback is the explosion of the amount of heterogeneous information that has to be stored and processed. Thus, specific solutions for big data management become crucial. In this paper, we present a new architecture, called IoT4S, for the fine-grained storage of environmental information collected through several heterogeneous sensing devices. It provides a high abstraction layer for the description of both sensing infrastructures and sensed data, which can be exported as 'things' to the internet. Also, we propose a new two-layer system for data storage to support IoT4S in the data dissemination service, which is a hybrid solution based on both SQL-like and NoSQL technologies. The paper provides an in-depth description of the IoT4S design, implementation details and a preliminary analysis of its behaviour in real and emulated scenarios.	smart city	Maria Fazio;Antonio Puliafito;Massimo Villari	2014	IJWGS	10.1504/IJWGS.2014.060255	big data;human–computer interaction;computer science;internet privacy;world wide web;computer security;internet of things	Robotics	-42.42274371851802	48.556213155334376	39600
fabefe93b526de2fd3ed36d7124c153137ad1569	knowledge for a longer life: development impetus for energy-efficient smartphone applications	smart phones energy conservation mobile computing power aware computing;smartphone energy consumption application;energy consumption ieee 802 11 standard global positioning system batteries mobile communication wireless communication synchronization;energy consumption;smartphone;application;energy consumption reduction energy efficient smartphone applications mobile applications battery lifetime	In recent years, there has been a rapid growth in the spread of smartphones and thus in the utilization of mobile applications. Such applications require a substantial portion of the available energy. Since a short battery lifetime has a very negative impact for the user experience, application developers should have the skills and the knowledge to avoid energy-inefficient applications. In this paper, we present a comprehensive survey of approaches and methods to reduce the energy consumption and thus help software developers to improve their applications.	algorithm;cloud computing;color;data compression;global positioning system;graphical user interface;internationalization and localization;mobile app;oled;scheduling (computing);scientific literature;sensor web;smartphone;software developer;user experience	Ronny Hans;Daniel Burgstahler;Alexander Müller;Manuel Zahn;Dominik Stingl	2015	2015 IEEE International Conference on Mobile Services	10.1109/MobServ.2015.27	embedded system;simulation;engineering;computer security	Mobile	-41.247948605278744	50.777576267543914	39604
5f010ee8523150e724675a7de02a55ac770325fb	understanding probabilistic software leaks	software security;program analysis;noninterference	Probabilistic security leaks in multi-threaded programs exploit nondeterminism and interleaving. Probabilistic leaks does not leak secret values directly, but secret values influence the probability of public events. The article explains probabilistic leaks, and discusses various methods for checking probabilistic noninterference.		Gregor Snelting	2015	Sci. Comput. Program.	10.1016/j.scico.2013.11.008	program analysis;software security assurance;computer science;database;programming language;computer security	Logic	-55.086593895641464	53.39323014168201	39660
45ddbb5f835cc929148ed179e92be7fec03b65c0	iterative analysis to improve key properties of critical human-intensive processes: an election security example	engineering;elections;model checking;process modeling;iterative analysis;fault tree analysis	In this article, we present an approach for systematically improving complex processes, especially those involving human agents, hardware devices, and software systems. We illustrate the utility of this approach by applying it to part of an election process and show how it can improve the security and correctness of that subprocess. We use the Little-JIL process definition language to create a precise and detailed definition of the process. Given this process definition, we use two forms of automated analysis to explore whether specified key properties, such as security and safety policies, can be undermined. First, we use model checking to identify process execution sequences that fail to conform to event-sequence properties. After these are addressed, we apply fault tree analysis to identify when the misperformance of steps might allow undesirable outcomes, such as security breaches. The results of these analyses can provide assurance about the process; suggest areas for improvement; and, when applied to a modified process definition, evaluate proposed changes.	child process;correctness (computer science);fault tree analysis;iterative method;key (cryptography);model checking;requirement;software system;vulnerability (computing)	Leon J. Osterweil;Matt Bishop;Heather M. Conboy;Huong Phan;Borislava I. Simidchieva;George S. Avrunin;Lori A. Clarke;Sean Peisert	2017	ACM Trans. Priv. Secur.	10.1145/3041041	reliability engineering;computer science;data mining;computer security	Security	-54.285539068125395	48.4076149233642	39681
43c33e0e3ff1daa3105e7c6a7dde777262990f49	human-centered methods for improving api usability		Application programming interfaces (APIs) are the way that developers reuse functionality supplied in libraries, software development kits (SDKs), toolkits, frameworks, etc. By adapting a variety of user-centered methods from human-computer interaction (HCI), we have studied usability problems both for API users and for API designers. These studies revealed barriers both at a low level (such as using the factory pattern in an API) and at a high level (such as the lack of example code in the documentation). In lab studies, we have shown that some patterns can slow programmers down by a factor of 10, and in the field, we have seen problematic APIs block programmers for up to a week while they waited for an answer from the API designer. The implications of our results can guide the design of the API, and, when APIs cannot be changed, inspire novel documentation and tools to help use the APIs. Our collaboration with SAP resulted in significant improvements to their APIs, documentation and tools. This talk will summarize results presented in our recent paper on Improving API Usability, along with our newly proposed work on studying the needs of API designers. I will also cover a wide variety of HCI methods we have found to be effective for better understanding and meeting the needs of API users of all levels: novice, professional, and end-user programmers (EUPs). We have applied these methods across all activities of API development: requirements and problem analysis, design, development, testing, and deployment. Since programming is a human process, we have found that many of these HCI methods can be used without change to answer many useful questions, but for other questions, we have needed to create new human-centered methods.	application programming interface;decade (log scale);documentation;factory method pattern;high-level programming language;human–computer interaction;library (computing);list of toolkits;problem solving;programmer;requirement;software deployment;software development kit;software framework;usability;user-centered design	Brad A. Myers	2017	2017 IEEE/ACM 1st International Workshop on API Usage and Evolution (WAPI)	10.1109/WAPI.2017.2	application programming interface;factory method pattern;software deployment;documentation;look and feel;empirical process (process control model);software development;human–computer interaction;usability;systems engineering;computer science	HCI	-53.46405776569385	34.237896240227585	39692
2339fefafdac72a01aa0eb5bad9f2bd9a0b68476	the trust problem in modern network infrastructures		SDN and NFV are modern techniques to implement networking infrastructures and can be used also to implement other advanced functionalities, such as the protection architecture designed by the SECURED project. This paper discusses a couple of techniques – trustworthy network infrastructure monitoring and remote attestation of virtual machines – useful towards a trusted and secure usage of SDN and NFV.	component-based software engineering;cryptography;data integrity;network function virtualization;network switch;software-defined networking;trust (emotion);trusted computing;virtual machine	Ludovic Jacquin;Antonio Lioy;Diego R. López;Adrian L. Shaw;Tao Su	2015		10.1007/978-3-319-25360-2_10	engineering;distributed computing;world wide web;computer security	Security	-51.18645546655322	57.64260112591622	39714
8aa4a1ef913565a2084da51ca782acd08bab6e99	i4copter: an adaptable and modular quadrotor platform	nand flash memory;distributed system;reliability;control algorithm;ssd solid state drive;raid;open architecture;ftl flash translation layer;vertical take off and landing;trajectory tracking;mechanism design;use case;micro air vehicle;real time and embedded systems	Quadrotor helicopters are micro air vehicles with vertical take-off and landing capabilities controlled by varying the rotation speed of four fixed pitch propellers. Due to their rather simple mechanical design they have grown to popularity as platform for various research projects. Despite most of them being individually highly successful, they are typically tailored to a specific purpose making it hard to utilise them for further research and education.  In this paper, we present the novel design of the I4Copter quadrotor. It has been developed to provide a stable demonstration quadrotor platform for various kinds of research and education projects targeting cross-field challenges in real-time and embedded systems, distributed systems, robotics and cybernetics. The modular and open architecture of our platform allows an application-specific, fine-grained extension, adaption and replacement of software and hardware components. The safe extensibility is supported by strict temporal and spatial isolation between the software modules. We validated our approach by two distinct cross-field use cases: an evaluation platform for modularised control algorithms enabling trajectory tracking and an implementation that is resilient to transient hardware errors.	algorithm;cybernetics;distributed computing;embedded system;extensibility;open architecture;real-time transcription;robotics	Peter Ulbrich;Rüdiger Kapitza;Christian Harkort;Reiner Schmid;Wolfgang Schröder-Preikschat	2011		10.1145/1982185.1982267	use case;mechanism design;embedded system;real-time computing;simulation;open architecture;computer science;operating system;reliability;database;computer security;raid	Robotics	-42.907645509946796	38.8663545514855	39716
97f10dbddf8f6a732d7356c016a2926809506582	modeling of post-incident root cause analysis	root cause analysis	Because digital incidents are not always from an external source, the focus often is upon the internal network and the people who use it. Frauds, abuses and other insider threats are, by most accounts, more common than externally caused events. In any event, such root cause investigations usually center upon the internal network, the entry point (especially in large-scale virus and worm infections) and the damage done during the incident.	entry point;insider threat;internet;intranet;petri net;refinement (computing);set theory	Peter Stephenson	2003	IJDE		data mining;computer science;root cause analysis	Security	-62.81212150579565	60.10514953270531	39786
67a2283d83575590c1e3074ed824e34d02dfc8d1	towards automatic inference of kernel object semantics from binary code		This paper presents ARGOS, the first system that can automatically uncover the semantics of kernel objects directly from a kernel binary. Based on the principle of data use reveals data semantics, it starts from the execution of system calls (i.e., the user level application interface) and exported kernel APIs (i.e., the kernel module development interface), and automatically tracks how an instruction accesses the kernel object and assigns a bit-vector for each observed kernel object. This bit-vector encodes which system call accesses the object and how the object is accessed (e.g., read, write, create, destroy), from which we derive the meaning of the kernel object based on a set of rules developed according to the general understanding of OS kernels. The experimental results with Linux kernels show that ARGOS is able to recognize the semantics of kernel objects of our interest, and can even directly pinpoint the important kernel data structures such as the process descriptor and memory descriptor across different kernels. We have applied ARGOS to recognize internal kernel functions by using the kernel objects we inferred, and we demonstrate that with ARGOS we can build a more precise kernel event tracking system by hooking these internal functions.	binary code;bit array;data structure;hooking;kernel (operating system);linux;loadable kernel module;object-based language;operating system;system call;tracking system	Junyuan Zeng;Zhiqiang Lin	2015		10.1007/978-3-319-26362-5_25	sysfs;kernel method;procfs;real-time computing;string kernel;kernel embedding of distributions;computer science;theoretical computer science;kernel virtual address space;graph kernel;epoll;kernel preemption;tree kernel;programming language;polynomial kernel	Security	-58.13172076361193	53.83160949443373	39798
05a715eb69fc28be46a7ba37d0a99e5fb72e4973	proving multilevel security of a system design	verifiable systems;generic model;multics;security proof;protection;security kernel;system design;type extension;multiple model;security;supervisors;multilevel security;operating systems	Two nearly equivalent models of multilevel security are presented. The use of multiple models permits the utilization of each model for purposes where that model is particularly advantageous. In this case, the more general model is simple and easily comprehensible, being more abstract, and is useful for exposition of the meaning of multilevel security. The less general model relates well to design specifications and permits straightforward proof of the security of a system design. The correspondence between the two models is easily demonstrated. The two models when applied appropriately are more useful for defining and proving the multilevel security of systems than existing models. The utility of the two models and their relationship to existing models is discussed and the proof of the security of one particular system design is illustrated. The technique for accomplishing the security proof is straightforward and can be extensively automated.	multilevel security;norm (social);provable security;systems design	Richard J. Feiertag;Karl N. Levitt;Lawrence Robinson	1977		10.1145/800214.806547	computer security model;computer science;information security;operating system;concrete security;security testing;computer security;algorithm;systems design	OS	-51.871744286478155	50.69881071111706	39837
dfc610a197ee52714cde7ecac378a0d7a8d08f8a	staccato: a bug finder for dynamic configuration updates (artifact)	004;dynamic configuration updates dynamic analysis software configuration	This artifact is based on Staccato, a tool for finding errors in dynamic configuration update (DCU) implementations. Dynamic configuration update refers to configuration changes that occur at runtime without program restart. Errors in DCU implementations occur when stale data— computed from old configurations—or inconsistent data—computed from different configurations—are used. Staccato uses a dynamic analysis in the style of taint analysis to detect these errors. Staccato supports concurrent programs running on commodity JVMs. We evaluated Staccato on three open-source applications and found errors in all of them. 1998 ACM Subject Classification D.2.9; Software configuration management	open-source software;run time (program lifecycle phase);software configuration management;taint checking	John Toman;Dan Grossman	2016	DARTS	10.4230/DARTS.2.1.14	real-time computing;computer science;database;distributed computing	SE	-57.97788297406292	39.55818382778583	39863
519fc3c6222ad0e8cf1a81521610daa5aba0b213	javascript patterns - build better applications with coding and design patterns	design pattern		design pattern;javascript	Stoyan Stefanov	2010			computer architecture;computer science;operating system;world wide web	HCI	-51.748943451916105	33.13826089174028	39968
fc483c8e8c26e7dbd5f3718c19758dc27da7daed	an agent-managed ad-hoc social network to facilitate f2f networking at paams 2014		The world of social networks could greatly profit from us- ing an approach based on multi-agent systems, in which personal agents running on the users' personal devices autonomously access information from different data sources (managed and combined by context agents) in order to bring physically together users that may have common in- terests. In this demonstration, we will present the first example of such a system, which has been specifically created to facilitate networking among PAAMS 2014 participants.	hoc (programming language);social network	Ludo Stellingwerff;Giovanni Egidio Pazienza	2014		10.1007/978-3-319-07551-8_45	simulation;engineering;world wide web;computer security	Mobile	-39.13777607978486	45.054350412982245	39983
8ba97aafbed253103ffe7558779ec425d222986f	plcloud: comprehensive power grid plc security monitoring with zero safety disruption	trusted computing industrial control monitoring optimal control programmable controllers scada systems;programmable logic controllers plcloud comprehensive power grid plc security monitoring zero safety disruption security threats cyber physical critical power grid infrastructures complex interdependencies optimal plant control infrastructural safety topics architectural solutions networked industrial control systems smart grid sensing processing actuation embedded nodes control decisions optimal actions global safety maintenance transient stability decision making modules cyber security induced safety incidents safety handling modules global trusted computing base attack surface combined safety control system architecture optimal design paradigm plcloud cloud based safety preserving architecture minimal trusted safety verifier layer cyber based supervisory control data acquisition scada infrastructure;computer architecture;smart grids;monitoring;malware;safety;smart grids safety computer architecture monitoring real time systems malware;real time systems	Recent security threats against cyber-physical critical power grid infrastructures have further distinguished the differences and complex interdependencies between optimal plant control and infrastructural safety topics. In this paper, we reflect upon few real-world scenarios and threats to understand how those two topics meet. We then propose a practical architectural solutions to address the corresponding concerns. As a first concrete step, we focus on networked industrial control systems in smart grid where several sensing-processing-actuation embedded nodes receive information, make control decisions, and carry out optimal actions. Traditionally, global safety maintenance, e.g., transient stability, is embedded into control and taken into account by the decision making modules. With recent cyber security-induced safety incidents, we believe that the safety-handling modules should also be considered as a part of global trusted computing base (attack surface) for security purposes. Generally, maximizing the system's overall security requires the designers to minimize its trusted computing base. Consequently, we argue that the traditional combined safety-control system architecture is not anymore the optimal design paradigm to follow given existing threats. Instead, we propose PLCLOUD, a new cloud-based safety-preserving architecture that places a minimal trusted safety verifier layer between the physical world and the cyber-based supervisory control and data acquisition (SCADA) infrastructure, specifically programmable logic controllers (PLCs). PLCLOUD's main objective is to take care of infrastructural safety and separate it from optimal plant control that SCADA is responsible for.	attack surface;care-of address;cloud computing;computer security;control system security;data acquisition;denial-of-service attack;embedded system;interdependence;optimal design;programming paradigm;systems architecture;threat (computer);trusted computing base;trusted operating system	Henry Senyondo;Pengfei Sun;Robin Berthier;Saman A. Zonouz	2015	2015 IEEE International Conference on Smart Grid Communications (SmartGridComm)	10.1109/SmartGridComm.2015.7436401	reliability engineering;embedded system;engineering;computer security	Embedded	-56.91240280569743	50.981944848491665	39987
7adc1fd104902e02b545f5eefa90c469c0ecc027	the evolution of software and its impact on complex system design in robotic spacecraft embedded systems	mars reconnaissance orbiter;software;anomalies;aerospace engineering;central processing units;adaptive control;systems engineering;versatility;autonomy;complexity;software engineering;requirements;robot control;computer systems programs;complex systems;algorithms;unmanned spacecraft;systems simulation;hardware	The growth in computer hardware performance, coupled with reduced energy requirements, has led to a rapid expansion of the resources available to software systems, driving them towards greater logical abstraction, flexibility, and complexity. This shift in focus from compacting functionality into a limited field towards developing layered, multi-state architectures in a grand field has both driven and been driven by the history of embedded processor design in the robotic spacecraft industry. The combinatorial growth of interprocess conditions is accompanied by benefits (concurrent development, situational autonomy, and evolution of goals) and drawbacks (late integration, nondeterministic interactions, and multifaceted anomalies) in achieving mission success, as illustrated by the case of the Mars Reconnaissance Orbiter. Approaches to optimizing the benefits while mitigating the drawbacks have taken the form of the formalization of requirements, modular design practices, extensive system simulation, and spacecraft data trend analysis. The growth of hardware capability and software complexity can be expected to continue, with future directions including stackable commodity subsystems, computer-generated algorithms, runtime reconfigurable processors, and greater autonomy.	algorithm;autonomy;central processing unit;complex system;computer hardware;computer-generated holography;embedded system;interaction;mechatronics;modular design;processor design;programming complexity;requirement;robot;robotic spacecraft;simulation;software system;stackable switch;systems engineering	Roy Butler;Michael Pennotti	2013		10.1016/j.procs.2013.01.078	requirements analysis;complex systems;complexity;real-time computing;simulation;adaptive control;computer science;artificial intelligence;operating system;systems simulation;robot control;autonomy;algorithm;unmanned spacecraft	Robotics	-45.026106723136024	37.713458695687635	40027
4630e43fa4da896b9d92a28acbac5859b93524bb	rde: replay debugging for diagnosing production site failures	record and replay;failure analysis;symbolic execution	Online service failures in production computing environments are notoriously difficult to debug. One of the key challenges is to allow the developer to replay the failure execution within an interactive debugging tool such as GDB. Previous work has proposed in-situ approaches to inferring the production-run failure path within the production environment. However, those tools may sometimes suggest failure execution paths that are infeasible to reach by any program inputs. Moreover, production site often does not record or provide failure-triggering inputs due to the user privacy concern. In this paper, we present RDE, a Replay DEbug system that can replay a production-site failure at the development site within an interactive debugging environment without requiring user inputs. RDE takes an inferred production failure path as input and performs execution synthesis using a new guided symbolic execution technique. RDE can tolerate imprecise or inaccurate failure path information by navigating the symbolic execution along a set of selected paths. RDE synthesizes an input from the selected symbolic execution path which can be fed to a debugging tool to replay the failure. We have implemented an initial prototype of RDE and tested it with a set of coreutils bugs. The results show that RDE can successfully replay all the tested bugs within GDB.	debugging;deployment environment;experiment;gnu debugger;gnu octave;ibm notes;online service provider;prototype;software bug;symbolic execution;xojo	Peipei Wang;Hiep Nguyen;Xiaohui Gu;Shan Lu	2016	2016 IEEE 35th Symposium on Reliable Distributed Systems (SRDS)	10.1109/SRDS.2016.050	reliability engineering;embedded system;failure analysis;parallel computing;real-time computing;computer science;operating system;database;distributed computing;programming language	OS	-60.722862345408615	37.26822106430688	40111
08ba1efc7cd8932d50738c0fed2314d86b33bcdd	from the internet of things to the web of things: resource-oriented architecture and best practices		Creating networks of “smart things” found in the physical world (e.g., with RFID, wireless sensor and actuator networks, embedded devices) on a large scale has become the goal of a variety of recent research activities. Rather than exposing real-world data and functionality through vertical system designs, we propose to make them an integral part of the Web. As a result, smart things become easier to build upon. In such an architecture, popular Web technologies (e.g., HTML, JavaScript, Ajax, PHP, Ruby) can be used to build applications involving smart things, and users can leverage well-known Web mechanisms (e.g., browsing, searching, bookmarking, caching, linking) to interact with and share these devices. In this chapter, we describe the Web of Things (WoT) architecture and best practices based on the RESTful principles that have already contributed to the popular success, scalability, and evolvability of the Web. We discuss several prototypes using these principles, which connect environmental sensor nodes, energy monitoring systems, and RFID-tagged objects to the Web. We also show how Web-enabled smart things can be used in lightweight ad-hoc applications, called “physical mashups”, and discuss some of the remaining challenges towards the global World Wide Web of Things. 5.1 From the Internet of Things to the Web of Things As more and more devices are getting connected to the Internet, the next logical step is to use the World Wide Web and its associated technologies as a platform for smart things (i.e., sensor and actuator networks, embedded devices, electronic appliances and digitally enhanced everyday objects). Several years ago, in the 1 The original publication is available at www.springerlink.com published in the book: “Architecting the Internet of Things”, edited by M. Harrison, F. Michahelles and D. Uck-	ajax (programming);best practice;embedded system;floor and ceiling functions;html;hoc (programming language);hypertext transfer protocol;interaction;internet of things;javascript;library (computing);linker (computing);loose coupling;mobile device;openness;overhead (computing);php;pervasive informatics;radio-frequency identification;representational state transfer;resource-oriented architecture;ruby;scalability;sensor web;smart tv;user experience;web developer;web of things;web standards;web syndication;world wide web	Dominique Guinard;Vlad Trifa;Friedemann Mattern;Erik Wilde	2011		10.1007/978-3-642-19157-2_5	web of things;internet privacy;world wide web	Web+IR	-41.37400746725671	47.503795051076665	40266
7caa959fad363afba6171c9305e9486d81f346a0	qos-aware dynamic service composition in ambient intelligence environments	semantic web service;owl s;ambient intelligence;wireless network;web service composition;semantic web services;web services composition;on the fly;handheld device;automata theory;qos awareness;network services;dynamic service composition	Due to the large success of wireless networks and handheld devices, the ambient intelligence (AmI) paradigm is becoming a reality. One of the most challenging objectives to achieve in AmI environments is to enable a user to perform a task by composing on the fly networked services available at a specific time and place. Towards this goal, we propose a solution based on semantic Web services, and we show how service capabilities described as conversations can be integrated to perform a user task that is also described as a conversation, further meeting the QoS requirements of the user task. Experimental results show that the runtime overhead of our algorithm is reasonable, and further, that QoS-awareness improves its performance.	algorithm;ambient intelligence;mobile device;on the fly;overhead (computing);programming paradigm;quality of service;requirement;semantic web service;service composability principle	Sonia Ben Mokhtar;Jinshan Liu;Nikolaos Georgantas;Valérie Issarny	2005		10.1145/1101908.1101959	web service;ambient intelligence;computer science;operating system;wireless network;ws-policy;social semantic web;automata theory;mobile device;semantic web stack;database;multimedia;web intelligence;world wide web;owl-s	Mobile	-40.589038942630644	45.38418030932398	40304
26aa39a38b421cf7f4e05f3c5aa3b7d472adb2e8	intelligent web crawler for file safety inspection	google;inspection crawler;web pages;security of data indexing internet;html;indexes;servers;crawlers internet web pages servers indexes google html;internet;malicious codes intelligent web crawler file safety inspection internet web indexing;crawlers	The Internet has always been growing with all the contents and information added by different types of users. Without proper storage and indexing, these contents can easily be lost in the sea of information housed by the Internet. Hence, an automated program, known as the web crawler is used to index all the contents added to the Internet. With proper configurations and settings, a web crawler can be used for other purposes besides web indexing, which include downloading files from the web. Millions or billions of files are uploaded on the Internet and for most of the sites which host these files, there are no direct indication of whether the file is safe and free of malicious codes. Therefore, this paper aims to provide a construction of a web crawler which crawls all the pages in a given website domain, and download all the possible downloadable files linked to those pages, for the purpose of file safety inspection.	ajax (programming);code;data structure;download;internet;parsing;programming language;python;robots exclusion standard;web crawler;web indexing;web page	Ling Cong Xiang;Shih Yin Ooi;Pang Ying Han	2015	2015 IEEE International Conference on Signal and Image Processing Applications (ICSIPA)	10.1109/ICSIPA.2015.7412210	ajax;web application security;database index;spider trap;static web page;web development;site map;the internet;html;web search engine;web standards;computer science;web crawler;web page;database;focused crawler;internet privacy;world wide web;website parse template;web server;server;page hijacking	DB	-58.05027995226762	59.3112339840669	40317
30732de1198d98c5918bba38d2c287dfef03d798	satem: trusted service code execution across transactions	service commitment protocol trusted service code execution web services service oriented architecture internet computing client service transaction satem architecture execution monitor trust evaluator;trusted platform module;service provider;web service;trusted computing;software architecture;internet computing;operating system;web services;transaction processing;service oriented architecture;web services security of data software architecture transaction processing;security of data;monitoring web services service oriented architecture kernel protocols web and internet services operating systems linux testing protection	Web services and service oriented architectures are becoming the de facto standard for Internet computing. A main problem faced by users of such services is how to ensure that the service code is trusted. While methods that guarantee trusted service code execution before starting a client-service transaction exist, there is no solution for extending this assurance to the entire lifetime of the transaction. This paper presents Satem, a Service-aware trusted execution monitor that guarantees the trustworthiness of the service code across a whole transaction. The Satem architecture consists of an execution monitor residing in the operating system kernel on the service provider platform, a trust evaluator on the client platform, and a service commitment protocol. During this protocol, executed before every transaction, the client requests and verifies against its local policy a commitment from the service platform that promises trusted code execution. Subsequently, the monitor enforces this commitment for the duration of the transaction. To initialize the trust on the monitor, we use the Trusted Platform Module specified by the Trusted Computing Group. We implemented Satem under the Linux 2.6.12 kernel and tested it for a Web service and DNS. The experimental results demonstrate that Satem does not incur significant overhead to the protected services and does not impact the unprotected services	interpreter (computing);kernel (operating system);linux;operating system;overhead (computing);service-oriented architecture;trust (emotion);trusted computing;trusted execution technology;trusted platform module;web service	Gang Xu;Cristian Borcea;Liviu Iftode	2006	2006 25th IEEE Symposium on Reliable Distributed Systems (SRDS'06)	10.1109/SRDS.2006.42	web service;direct anonymous attestation;differentiated service;computer science;service delivery framework;operating system;trusted network connect;world wide web;computer security;trusted client;computer network	Security	-49.65615980935348	54.887003294054665	40356
88697e1dea96178e89e0694cae6928ea3cbbfc80	model-based debugging of embedded software systems	sequence diagram	Model Driven Development (MDD) has been slowly superseding traditional ways of developing embedded software in the recent decade. In line with the MDD, debugging Real-Time Embedded Software Systems (RTESS) and visualizing their behavior using models such as UML diagrams is becoming a reality. However, the existing MDD based debugging tools for RTESS are not applicable (require significant source code instrumentation, sophisticated debug interfaces, etc) for memorysize constrained RTESS. To address this, we discuss a modelbased debugging methodology for RTESS which aims at overcoming the aforementioned limitations. Using our approach, the target behavior can be visualized in real-time using UML sequence and timing diagrams. We illustrate our approach with a prototype and examples. Performance metrics such as the target monitor size and the instrumentation overhead are discussed.	complexity;debugger;debugging;desktop computer;digital timing diagram;embedded software;embedded system;graphical user interface;instrumentation (computer programming);memory footprint;model-driven engineering;overhead (computing);prototype;real-time clock;real-time computing;real-time transcription;software system;systems design;uml state machine;unified modeling language	Padma Iyenghar;Elke Pulvermüller;Clemens Westerkamp;Michael Uelschen;Juergen Wuebbelmann	2011	Softwaretechnik-Trends		software construction;debugging;algorithmic program debugging;real-time computing;computer science;systems engineering;background debug mode interface;flash memory emulator;avionics software;nexus (standard);tracing	Embedded	-43.64346480295593	34.46572003308375	40387
1677bf5c635ef0e81b6c6cdfce30727f83959132	secure dynamic code generation against spraying	just in time compilation;dep;insert;jit spraying;just in time compiler;operating system;compiler optimization;dynamic code generation	"""DCG (Dynamic Code Generation) technologies have found widely applications in the Web 2.0 era, Dion Blazakis recently presented a Flash JIT-Spraying attack against Adobe Flash Player that easily circumvented DEP and ASLR protection mechanisms built in modern operating systems. We have generalized and extended JIT Spraying into DCG Spraying. Based our analyses on this abstract model of DCG Spraying, we have found that all mainstream DCG implementations (Java/ JavaScript/ Flash/ .Net/ SilverLight) are vulnerable against DCG Spraying attack, and none of the existing ad hoc defenses such as compilation optimization, random NOP padding and constant splitting provides effective protection. Furthermore, we propose a new protection method, INSeRT, which combines randomization of intrinsic elements of machine instructions and randomly planted special trapping snippets. INSeRT practically renders the """"sprayed code"""" ineffective, while alerts the host program of ongoing attacking attempts. We implemented a prototype of INSeRT on the V8 JavaScript engine, and the performance overhead is less than 5%, which should be acceptable in practical application."""	address space layout randomization;adobe flash player;code generation (compiler);compiler;definite clause grammar;executable space protection;heap spraying;hoc (programming language);insert (sql);jit spraying;java;javascript engine;just-in-time compilation;mathematical optimization;microsoft silverlight;modern operating systems;nop;operating system;overhead (computing);padding oracle attack;protection mechanism;prototype;randomness;rendering (computer graphics);self-modifying code;web 2.0;world wide web	Tao Wei;Tielei Wang;Lei Duan;Jing Luo	2010		10.1145/1866307.1866415	embedded system;real-time computing;jit spraying;computer science;operating system;just-in-time compilation;computer security	Security	-56.06849093312522	57.10374376736723	40400
3ffc9671a054c8661179bb1ac09e31a5bf18be1e	architecture for wsn nodes integration in context aware systems using semantic messages	semantic representation;semantic technologies;wireless sensor network;semantic information;standard definition;information need;context aware systems	Wireless sensor networks (WSN) are becoming extremely popular in the development of context aware systems. Traditionally WSN have been focused on capturing data, which was later analyzed and interpreted in a server with more computational power. In this kind of scenario the problem of representing the sensor information needs to be addressed. Every node in the network might have different sensors attached; therefore their correspondent packet structures will be different. The server has to be aware of the meaning of every single structure and data in order to be able to interpret them. Multiple sensors, multiple nodes, multiple packet structures (and not following a standard format) is neither scalable nor interoperable. Context aware systems have solved this problem with the use of semantic technologies. They provide a common framework to achieve a standard definition of any domain. Nevertheless, these representations are computationally expensive, so a WSN cannot afford them. The work presented in this paper tries to bridge the gap between the sensor information and its semantic representation, by defining a simple architecture that enables the definition of this information natively in a semantic way, achieving the integration of the semantic information in the network packets. This will have several benefits, the most important being the possibility of promoting every WSN node to a real semantic information source.	activity recognition;analysis of algorithms;communications protocol;computation;context-aware pervasive systems;cooperative breeding;habitat;information needs;information source;internet of things;interoperability;network packet;parsing;real life;scalability;sensor node;server (computing);smart environment;standard-definition television	Iker Larizgoitia;Leire Muguira;Juan Ignacio Vázquez	2009		10.1007/978-3-642-11723-7_50	standard-definition television;embedded system;information needs;semantic computing;semantic integration;wireless sensor network;semantic grid;computer science;theoretical computer science;data mining;semantic technology;world wide web;computer network	AI	-41.15278512833644	47.426458783799795	40428
04cf092d3a78b85f390d77b969b5b7c5cf9bec55	generating simpler ast edit scripts by considering copy-and-paste		In software development, there are many situations in which developers need to understand given source code changes in detail. Until now, a variety of techniques have been proposed to support understanding source code changes. Tree-based differencing techniques are expected to have better understandability than text-based ones, which are widely used nowadays (e.g., diff in Unix). In this paper, we propose to consider copy-and-paste as a kind of editing action forming tree-based edit script, which is an editing sequence that transforms a tree to another one. Software developers often perform copy-and-paste when they are writing source code. Introducing copy-and-paste action into edit script contributes to not only making simpler (more easily understandable) edit scripts but also making edit scripts closer to developersu0027 actual editing sequences. We conducted experiments on an open dataset. As a result, we confirmed that our technique made edit scripts shorter for 18% of the code changes with a little more computational time. For the other 82% code changes, our technique generated the same edit scripts as an existing technique. We also confirmed that our technique provided more helpful visualizations. Keywords: Understanding code changes, Edit scripts, Copy-and-paste	autoregressive integrated moving average;cut, copy, and paste;diff utility;experiment;software development;text-based (computing);time complexity;unix	Yoshiki Higo;Akio Ohtani;Shinji Kusumoto	2017	2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)		software bug;theoretical computer science;programming language;computer science;software;source code;software development;visualization;unix;scripting language	SE	-55.84017648232827	36.39201830251031	40507
29afd79956f82142ec42e89bf9dfee36dad10ef8	corba application tailored manager for quality of service support	application process monitoring corba application tailored manager quality of service micro qos manager end to end qos management object resource managers admission control real time scheduling video on demand application qos renegotiation;end to end qos;electrical capacitance tomography;application tailored manager;identity based encryption;video on demand application;resource allocation;high speed networks;micro qos manager;resource manager;resource management;client server systems;corba;qos renegotiation;quality management quality of service resource management electrical capacitance tomography monitoring identity based encryption context video on demand high speed networks workstations;qos;process monitoring;monitoring;scheduling;real time scheduling;workstations;video on demand;distributed object management;qos management;resources management;dynamic qos management;end to end qos management;quality of service;application process monitoring;object resource managers;client server systems distributed object management quality of service resource allocation real time systems scheduling video on demand;context;quality management;admission control;real time systems	We describe a component for supporting quality of service (QoS) in object environments. This component, called the micro QoS manager (/spl mu/QoS-Mngr), is associated to an application and handles all aspects of static and dynamic QoS management related to that application. End to end QoS management is provided through the cooperation of the /spl mu/QoS-Mngr with a set of object resource managers (OR-Mngr) each of which is responsible for the QoS management of a specific resource, including, admission control, real-time scheduling and monitoring functionalities. QoS renegotiation is achieved through application process monitoring and communication, with the OR-Mngr. We focus on the micro QoS manager and give an overview of its implementation in a CORBA platform in the context of a video on demand application.	common object request broker architecture;quality of service	Didier Le Tien;Olivier Villin;Christian Bac	2000		10.1109/ISORC.2000.839511	real-time computing;mobile qos;quality of service;computer science;resource management;operating system;distributed computing;computer network	DB	-36.67611905198014	42.220250131379956	40521
d5b0146859646497703258404f10bf3d73ad3048	helping johnny encrypt: toward semantic interfaces for cryptographic frameworks	functionality engineers;semantic api;regulator pattern;security engineers;cryptography	Several mature cryptographic frameworks are available, and they have been utilized for building complex applications. However, developers often use these frameworks incorrectly and introduce security vulnerabilities. This is because current cryptographic frameworks erode abstraction boundaries, as they do not encapsulate all the framework-specific knowledge and expect developers to understand security attacks and defenses. Starting from the documented misuse cases of cryptographic APIs, we infer five developer needs and we show that a good API design would address these needs only partially. Building on this observation, we propose APIs that are semantically meaningful for developers, we show how these interfaces can be implemented consistently on top of existing frameworks using novel and known design patterns, and we propose build management hooks for isolating security workarounds needed during the development and test phases. Through two case studies, we show that our APIs can be utilized to implement non-trivial client-server protocols and that they provide a better separation of concerns than existing frameworks. We also discuss the challenges and potential approaches for evaluating our solution. Our semantic interfaces represent a first step toward preventing misuses of cryptographic APIs.	application programming interface;client–server model;cryptography;design pattern;encryption;misuse case;separation of concerns;server (computing);software build;vulnerability (computing)	Soumya Indela;Mukul Kulkarni;Kartik Nayak;Tudor Dumitras	2016		10.1145/2986012.2986024	computer science;internet privacy;world wide web;computer security	Security	-54.58058598468941	57.83620730075135	40631
40b18908aeff0ed7f5c1538384d79fd37e9dc873	secure resource sharing for embedded protected module architectures	embedded file system;trusted computing;internet of things;protected module architecture;resource sharing;access control	Low-end embedded devices and the Internet of Things IoT are becoming increasingly important for our lives. They are being used in domains such as infrastructure management, and medical and healthcare systems, where business interests and our security and privacy are at stake. Yet, security mechanisms have been appallingly neglected on many IoT platforms. In this paper we present a secure access control mechanism for extremely lightweight embedded microcontrollers. Being based on Sancus, a hardware-only Trusted Computing Base and Protected Module Architecture for the embedded domain, our mechanism allows for multiple software modules on an IoT-node to securely share resources. We implement and evaluate our approach for two application scenarios, a shared memory system and a shared flash drive. Our implementation is based on a Sancus-enabled TI MSP430 microcontroller. We show that our mechanism can give high security guarantees at small runtime overheads and a moderately increased size of the Trusted Computing Base.		Jo Van Bulck;Job Noorman;Jan Tobias Mühlberg;Frank Piessens	2015		10.1007/978-3-319-24018-3_5	shared resource;embedded system;computer science;access control;operating system;distributed computing;trustworthy computing;computer security;internet of things	EDA	-51.3590663177331	56.525799899894984	40649
35a73d221aa9d23b03d19f181fa29b750d1820cb	community-based secure information and resource sharing in azure cloud iaas	iaas;formal models;information sharing;azure	To efficiently collaborate in cyber security defense and response, organizations must be able to securely share information and resources. A community in a cloud IaaS, which refers to a group of organizations with common business interests, will utilize cloud IaaS to realize their infrastructure deployments. Communities establish a mechanism to prevent, detect and respond to cyber attacks, and help member organizations in the community recover expeditiously. In this paper, we present an access control model for secure information and resource sharing between organizations in a community-based isolated environment in Microsoft Azure IaaS cloud platform, one of dominant commercial cloud platforms. The model facilitates organizations to share their IT resources with each other in a controlled and secure manner. We formally specify the administrative model and discuss enforcement techniques in the Azure cloud platform.	access control;amazon web services;cloud computing;computer security;data security;microsoft azure	Yun Zhang;Farhan Patwa;Ravi S. Sandhu	2016		10.1145/2898445.2898455	internet privacy;world wide web;computer security	Security	-48.03893742465258	55.25358031545819	40658
c14e53912eedc97b63c026fcc6b4c5e431a30f25	the cloud design of the cognitive virus based on saas	databases;collaboration;trojan horses;computational modeling;security;knowledge based systems;cloud computing	This article makes full use of the features of worm virus and Trojans, and constructs intelligent cognitive virus model based on Agent. The process of viral population genetics and evolution is presented in detail. The virus cloud is described with SaaS architecture approach. The ideas in this paper provides a guideline for using the virus for better attacks in cyberspace confrontation areas.	client (computing);cyberspace;software agent;software as a service;trojan horse (computing)	Yang Sun;Wei Xiong	2016	2016 IEEE First International Conference on Data Science in Cyberspace (DSC)	10.1109/DSC.2016.8	simulation;engineering;world wide web;computer security	Robotics	-46.8619216664633	56.97912929060136	40683
7e112c13a5f063b2af0bd6cd58a2fa83ef1da3bd	proteus: a system for dynamically composing and intelligently executing web services	system design;remote procedure call;web service	Many organizations envision web services as an enabling component of Internet-scale computing. A final vision of web services is to realize a dynamic environment that identifies, composes and executes web services in response to a query. This vision shapes the design and implementation of Proteus. In addition to describing Proteus’ novel components, this paper outlines its initial system design.	proteus;systems design;web service	Shahram Ghandeharizadeh;Craig A. Knoblock;Christos Papadopoulos;Cyrus Shahabi;Esam Alwagait;José Luis Ambite;Min Cai;Ching-Chien Chen;Parikshit Pol;Rolfe R. Schmidt;Saihong Song;Snehal Thakkar;Runfang Zhou	2003				Networks	-39.57869923989552	42.15161612372278	40697
1819fcc3a5a77bcb6e54b8cd703e27749ffe1b5e	design and implement of push notification server in mobile iot environment	mobile applications;push notification;mobile terminals	In this poster, we propose a hybrid framework of push notification for mobile terminals. We put forward the framework of our push notification server with the hybrid trait, since there exists less open architecture of prevalent push server at present on the one side. Moreover, it adopts a hybrid mode to push different types of data, for instance, real-time messages or events, and non-real-time messages. Afterwards, it chooses relevant operations to satisfy corresponding demands. Relative to existing implementation, we believe that it can be applied into mobile Internet of Things(IoT) system likewise, to make the server more effective, well-organized, and trustworthy.	html5 in mobile devices;open architecture;push technology;real-time locating system;server (computing);trustworthy computing	Xuan Liu;Bo Cheng;Zhongyi Zhai;Junliang Chen	2016	2016 IEEE International Conference on Mobile Services (MS)	10.1109/MobServ.2016.40	embedded system;engineering;world wide web;computer network	Robotics	-38.32487590425139	52.22378337862543	40767
3040750c9289bffb91fa9e2483decd2f1a7bb1fe	from uml models to software performance results: an spe process based on xml interchange formats	uml;plug and play;performance tool;automated model building;tool interoperability;system performance;software performance;proof of concept;methods and tools;software performance engineering;model building;queueing network model;performance model interchange format;performance model;xml;interchange format;software design;meta model;performance assessment;modeling tool;spe process	The SPE process uses multiple performance assessment tools depending on the state of the software and the amount of performance data available. This paper describes two XML based interchange formats that facilitate using a variety of performance tools in a plug-and-play manner thus enabling the use of the tool best suited to the analysis. The Software Performance Model Interchange Format (S-PMIF) is a common representation that is used to exchange information between (UML-based) software design tools and software performance engineering tools. On the other hand, the performance model interchange format (PMIF 2.0) is a common representation for system performance model data that can be used to move models among system performance modeling tools that use a queueing network model paradigm. This paper first defines an XML based S-PMIF based on an updated SPE meta-model Then it demonstrates the feasibility of using both the S-PMIF and the PMIF 2.0 to automatically translate an architecture description in UML into both a software performance model and a system performance model to study the performance characteristics of the architecture. This required the implementation of some extensions to the XPRIT software in order to export UML models into the S-PMIF and a new function in the SPEED software to import S-PMIF models, which are also described. The SPE process and an experimental proof of concept are presented.	metamodeling;network model;performance engineering;plug and play;profiling (computer programming);programming paradigm;queueing theory;software design;unified modeling language;xml	Connie U. Smith;Catalina M. Lladó;Vittorio Cortellessa;Antinisca Di Marco;Lloyd G. Williams	2005		10.1145/1071021.1071030	metamodeling;unified modeling language;model-driven architecture;xml;model building;software performance testing;uml tool;computer science;systems engineering;software design;operating system;software engineering;applications of uml;database;computer performance;proof of concept	SE	-37.746420130846744	36.491407078246425	40795
8900dff9e69f1887f9ef61aff0eed8662ff19bd1	poster: designing bug detection rules for fewer false alarms		One of the challenging issues of the existing static analysis tools is the high false alarm rate. To address the false alarm issue, we design bug detection rules by learning from a large number of real bugs from open-source projects from GitHub. Specifically, we build a framework that learns and refines bug detection rules for fewer false positives. Based on the framework, we implemented ten patterns, six of which are new ones to existing tools. To evaluate the framework, we implemented a static analysis tool, FeeFin, based on the framework with the ten bug detection rules and applied the tool for 1,800 open-source projects in GitHub. The 57 detected bugs by FeeFin has been confirmed by developers as true positives and 44 bugs out of the detected bugs were actually fixed.	open-source software;software bug;static program analysis	Jaechang Nam;Song Wang;Yuan Xi;Lin Tan	2018	2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)	10.1145/3183440.3194987	real-time computing;false alarm;software bug;computer science;static program analysis;constant false alarm rate;static analysis;false positive paradox	SE	-58.78805917278388	37.86517485424731	40801
d233172568d19e257a0783290b4309812e4f3f24	adding a deliberative layer to an autonomic system	abduction;oscillators;home appliances;heating;dialogue;grid monitoring level autonomic system deliberative layer smart microgrid self management capability;smart power grids distributed power generation fault tolerant computing power engineering computing;smart micro grid;abduction autonomic computing smart micro grid dialogue artificial intelligence;autonomic systems;cognition;artificial intelligence;temperature measurement;atmospheric modeling;autonomic computing;autonomic systems temperature measurement home appliances heating cognition oscillators atmospheric modeling	Autonomic systems appear as closed because their internal logic cannot be communicated to their users. This work presents a method to solve this communication problem. The objective is to let the user specify, negotiate and observe the high-level objectives imposed to the autonomic system without affecting the latter's self-management capabilities, but also to enable relevant communication from the system towards the user. A smart micro-grid is the autonomic system used. A procedure that generates relevant arguments is connected at the monitoring level of the grid as a deliberative layer.	autonomic computing;high- and low-level;self-management (computer science)	Marius Pol	2015	2015 IEEE International Conference on Autonomic Computing	10.1109/ICAC.2015.32	embedded system;atmospheric model;real-time computing;simulation;cognition;temperature measurement;computer science;artificial intelligence;distributed computing;oscillation;computer security;autonomic computing	Robotics	-41.73408178616177	39.80820200443518	40814
6a7936177dfaa1ee45937cb2ad08c52f6b4f5f3d	behavior computation for smart grid software analysis	software analysis	Smart grid embedded software is subject to intrusion and compromise with potentially serious consequences. Oak Ridge National Laboratory (ORNL) is conducting research and development in the new technology of software behavior computation to help address this problem. This technology applies mathematical foundations of denotational semantics to compute the behavior of software in all circumstances of use. Behavior computation is implemented in Function eXtraction (FX) systems that perform static semantic analysis on input programs. Research has shown how to make the effects of theoretical limitations on this process, as expressed in the halting problem, arbitrarily small. Behavior computation operates on the functional semantics of programs, and is not subject to the limitations of syntactic recognition or testing. ORNL is applying FX technology to help evaluate cyber security properties in smart grid systems, with initial focus on detecting vulnerabilities in embedded software that controls smart meters.	computation;computer security;denotational semantics;embedded software;halting problem;sensor;smart meter	Richard C. Linger;Mark G. Pleszkoch;Stacy J. Prowell;Kirk Sayre	2011		10.1145/2179298.2179354	software visualization;computational science;software sizing;software verification;theoretical computer science;component-based software engineering;software development;software engineering;software construction;resource-oriented architecture;software deployment;software system	Embedded	-49.72510915246385	34.20939699310167	40815
dfdaf24a7a6eda742678b1a57b0cea8e568881d7	naming as a fundamental concept of open hypermedia systems	component based open hypermedia system cb ohs;management system;naming system;open hypermedia systems;reference architecture	Names play a key role in distributed hypertext systems, for two main reasons: Firstly, because accessing and managing system services require finding and locating the relevant components. Secondly, because managing structures between hypertext resources, such as nodes, anchors and links, requires that these resources are named and addressed. We argue that naming services are endemic to hypertext systems and therefore, form a core part of any hypertext system’s infrastructure. In particular, the current move towards interoperable component-based Open Hypermedia Systems (CB-OHS) demonstrates the need for naming components.	component-based software engineering;hypermedia;hypertext;interoperability;oracle http server	Manolis Tzagarakis;Nikos Karousos;Dimitris Christodoulakis;Siegfried H Reich	2000		10.1145/336296.336338	reference architecture;computer science;management system;database;multimedia;world wide web	Web+IR	-36.4130871048974	43.20444181126099	40941
11f7876aa83d79c90c7ddb49b01186c80f6777b5	enhancing symbolic execution with veritesting	symbolic execution;veritesting;verification;testing and debugging;semantics of programming languages	We present MergePoint, a new binary-only symbolic execution system for large-scale and fully unassisted testing of commodity off-the-shelf (COTS) software. MergePoint introduces veritesting, a new technique that employs static symbolic execution to amplify the effect of dynamic symbolic execution. Veritesting allows MergePoint to find twice as many bugs, explore orders of magnitude more paths, and achieve higher code coverage than previous dynamic symbolic execution systems. MergePoint is currently running daily on a 100 node cluster analyzing 33,248 Linux binaries; has generated more than 15 billion SMT queries, 200 million test cases, 2,347,420 crashes, and found 11,687 bugs in 4,379 distinct applications.	code coverage;commodity computing;linux;software bug;symbolic execution;test case	Thanassis Avgerinos;Alexandre Rebert;Sang Kil Cha;David Brumley	2014	Commun. ACM	10.1145/2927924	parallel computing;real-time computing;verification;computer science;operating system;distributed computing;programming language;management;concolic testing	SE	-58.3363319216128	39.04958560139856	40976
65b916e96815671bb6abaf63228b3af65980de97	simulation of let models in simulink and ptolemy	simulink;ptolemy;timing definition language;timing definition language tdl;real time;logical execution time let;logical execution time;simulation of real time behavior;platform independent model;real time modeling;simulation environment;embedded software;open source	This paper describes two different approaches of simulating embedded control software whose real-time requirements are explicitly specified by means of the Logical Execution Time (LET) abstraction introduced in the Giotto project. As simulation environments we chose the black-box MATLAB/Simulink product and the open-source project Ptolemy II. The paper first sketches the modeling of LET-based components with the Timing Definition Language (TDL). As the LET abstraction allows the platform-independent modeling of the timing behavior of embedded software, a correct simulation of TDL components is equivalent to the behavior on a specific platform. We integrated TDL with both MATLAB/Simulink and Ptolemy and highlight the differences and similarities of the particular TDL simulation.	black box;content-control software;dataflow;embedded software;embedded system;event-driven programming;matlab;model transformation;open-source software;overhead (computing);pdf/a;program counter;ptolemy ii;real-time clock;real-time computing;requirement;run time (program lifecycle phase);simulation;simulink	Patricia Derler;Andreas Naderlinger;Wolfgang Pree;Stefan Resmerita;Josef Templ	2008		10.1007/978-3-642-12566-9_5	real-time computing;simulation;computer science;programming language	Embedded	-39.81534339799099	32.78712308274754	41008
ec2bba2df44f494b5b9a0d33915edbc9f34a8968	trusted launch of virtual machine instances in public iaas environments	openstack;iaas;trusted virtual machine launch;trusted computing;engineering and technology;teknik och teknologier;security	Cloud computing and Infrastructure-as-a-Service (IaaS) are emerging and promising technologies, however their adoption is hampered by data security concerns. At the same time, Trusted Computing (TC) is experiencing an increasing interest as a security mechanism for IaaS. In this paper we present a protocol to ensure the launch of a virtual machine (VM) instance on a trusted remote compute host. Relying on Trusted Platform Module operations such as binding and sealing to provide integrity guarantees for clients that require a trusted VM launch, we have designed a trusted launch protocol for VM instances in public IaaS environments. We also present a proof-of-concept implementation of the protocol based on OpenStack, an open-source IaaS platform. The results provide a basis for the use of TC mechanisms within IaaS platforms and pave the way for a wider applicability of TC to IaaS security.	booting;chain of trust;cloud computing;communications security;computer data storage;data security;freedom of information laws by country;library (computing);open-source software;prototype;record sealing;trusted computing;trusted platform module;trusted third party;virtual machine;xiii	Nicolae Paladi;Christian Gehrmann;Mudassar Aslam;Fredric Morenius	2012		10.1007/978-3-642-37682-5_22	direct anonymous attestation;computer science;information security;operating system;trusted network connect;distributed computing;trustworthy computing;computer security	Security	-50.65277566902546	57.85270975295664	41040
eedc49b46d5f4e6cd17381f4b78886c85e73ca1d	a hw/sw co-verification framework for systemc	test automation;model checking;hw sw co verification;systemc	SystemC is widely used for modeling and simulation in hardware/software co-design. However, existing verification techniques are mostly ad-hoc and non-systematic. In this article, we present a systematic, comprehensive, and formally founded co-verification framework for digital HW/SW systems that are modeled in SystemC. The framework is based on a formal semantics of SystemC and uses a combination of model checking and testing, whereby testing includes both the automated generation of timed inputs and automated conformance evaluation. We demonstrate its performance and its error detecting capability with two case studies, namely a packet switch and an anti-slip regulation and anti-lock braking system.	conformance testing;hoc (programming language);model checking;network packet;packet switching;semantics (computer science);sensor;shattered world;simulation;systemc	Paula Herber;Sabine Glesner	2013	ACM Trans. Embedded Comput. Syst.	10.1145/2435227.2435257	model checking;embedded system;computer architecture;real-time computing;computer science	Embedded	-38.82129213891195	32.87574080230549	41077
11585a9bdd942ef38030ca676d316bdef7c56b78	the web as the ubiquitous computer	internet web;ubiquitous computer internet web web technologies;ubiquitous computing cloud computing convergence mobile computing pervasive computing;mobile smart device;mobile computing internet;internet;web technologies;web sites;mobile communication;ubiquitous computing;mobile computing;ubiquitous computer;ubiquitous computer mobile smart device cloud computing;cloud computing	With the convergence of mobile smart devices, cloud computing, and software as a service, the Web is enabling anywhere, anytime computing.	anytime algorithm;cloud computing;smart device;software as a service;ubiquitous computing;world wide web	Vishnu S. Pendyala;Simon S. Y. Shim	2009	Computer	10.1109/MC.2009.302	web service;mobile search;the internet;context-aware pervasive systems;mobile web;mobile telephony;cloud computing;ubiquitous commerce;computer science;operating system;mobile technology;end-user computing;distributed web crawling;cloud testing;distributed computing;utility computing;internet privacy;smart environment;ubiquitous robot;mobile computing;world wide web;internet of things;ubiquitous computing;ubiquitous communicator;autonomic computing	HCI	-38.65113730914258	50.956369935365124	41126
8d0aa411a6362f2d8a3d50846a0e82e013089804	non-homogenous network, control hub and smart controller (ncs) approach to incremental smart homes	smart home;digital home;universal control hub;user interface;network control;remote user interfaces;accessibility;universal remote console;task based user interfaces;usability;smart environment	"""The rapid increase in memory and processing power of even simple devices is opening up new opportunities for intelligent devices and environments. However, major barriers exist to practical limitations. Many """"smart environments"""" are currently more complex to either set up or operate than their predecessors. Environments which are simpler to use are often very complex to set up. They also often require wholesale re engineering of the environment. Proposed is a model for using a mixture of non homogeneous network technologies, a control hub and a smart controller to provide a way for users to slowly transition both themselves and their houses from current technologies to smart technologies and environments."""	network computing system	Gregg C. Vanderheiden;Gottfried Zimmermann	2007		10.1007/978-3-540-73281-5_25	embedded system;simulation;engineering;smart environment;computer security;internet of things	Robotics	-38.27125462669229	48.00938119737359	41127
9758ba60a643fab43ec5e84c5760c03844b30577	using network analysis for recommendation of central software classes	measurement;program comprehension;software systems;dependency graph;software development management network theory graphs public domain software reverse engineering;network analysis;public domain software;indexes;software algorithms;markov processes;algorithm design and analysis software systems measurement software algorithms indexes markov processes;network theory graphs;algorithm design and analysis;software development management;dependency graph network analysis program comprehension;reverse engineering;central software class retrieval software system software developement reverse engineering network analysis metrics dependency graph open source project	As a new developer, getting to know a large unknown software system is a challenging task. If experienced developers are available, they can suggest which classes to read first, helping new developers to quickly grasp the system's most fundamental concepts. In practice, however, experienced developers often are no longer available. In these cases, the set of most important classes must be reverse engineered. This paper presents a thorough analysis of using different network analysis metrics on dependency graphs to retrieve central classes. An empirical study on four open source projects evaluates the results based on a survey among the systems' core developers. It demonstrates that the algorithmic results can compete with the suggestions of experienced developers.	algorithm;open-source software;reverse engineering;social network analysis;software system	Daniela Steidl;Benjamin Hummel;Elmar Jürgens	2012	2012 19th Working Conference on Reverse Engineering	10.1109/WCRE.2012.19	database index;algorithm design;dependency graph;network analysis;computer science;systems engineering;operating system;software engineering;database;markov process;programming language;public domain software;reverse engineering;measurement;software system	SE	-61.84388452246244	34.56559984978288	41172
dadc2e909f2bb7dce28df9a6f6765fd399b25bed	an automated tool for regression testing in web applications	html dom;web application testing;regression testing;web crawlers	Regression testing of web applications is a costly activity as it tends to generate more test cases than the previous stages of software testing. This cost can be reduced significantly by identifying and testing only the modified parts of a web application. This will require locating the changes that have been introduced in the web application from the previous version that was tested.  In this paper we have introduced an automated tool for locating the changes in the web application which will thereby aid in effective regression testing of the application. This tool compromise of 3 parts, a) a web crawler that crawls the web application, b) an HTML DOM tree generator that generates the DOM tree for a specified web page, c) an comparator that compares the new DOM tree with a previous version of the DOM tree stored in our system.	comparator;document object model;emoticon;html;regression testing;software testing;test case;web application;web crawler;web page	Shikha Raina;Arun Prakash Agarwal	2013	ACM SIGSOFT Software Engineering Notes	10.1145/2492248.2492272	document object model;regression testing;computer science;web crawler;data mining;database;world wide web;web testing	SE	-59.387089477774545	38.98383805308645	41218
35c6b40b1be762e66965b9fb05230889ea609813	trust modeling for peer-to-peer based computing systems	trust;peer to peer computing large scale systems computer networks quality of service scalability costs robustness computational modeling control systems authorization;resource allocation;distributed processing;large scale system;trust model;accuracy;large scale;data privacy;security of data distributed processing data privacy resource allocation quality of service;cost of ownership;peer to peer computing;quality of service;network computing systems trust modeling peer to peer based computing systems peer to peer approach large scale systems site autonomy trust issues trust model peer to peer structured large scale network computing system recommender network;peer to peer;security;security of data;network computing	The peer-to-peer approach to design large-scale systems has significant benefits including scalability, low cost of ownership, robustness, and ability to provide site autonomy. However, this approach has several drawbacks as well including trust issues and lack of coordination and control among the peers. We present a trust model for a peer-to-peer structured large-scale network computing system and completely define the trust model and describe the schemes used in it. Central to the model is the idea of maintaining a recommender network that can be used to obtain references about a target domain. Simulation results indicate that the trust model is capable of building and maintaining trust and also identifying the bad domains.	peer-to-peer	Farag Azzedin;Muthucumaru Maheswaran	2003		10.1109/IPDPS.2003.1213203	quality of service;information privacy;resource allocation;computer science;information security;distributed computing;accuracy and precision;internet privacy;trustworthy computing;world wide web;computational trust	EDA	-43.845343674472474	56.66075295829953	41242
54e866157ef3d9cbc67cd5a34737ca54b9cc22fc	qos-aware service selection considering potential service failures	000 allgemeines;service failures;qos aware service selection;000 allgemeines wissenschaft;it services;ddc 000;wissenschaft	Nowadays, service compositions are increasingly used to execute business processes. During the execution of a service composition, a service failure leads to a necessary re-planning. Due to such runtime events, the ex-post realized Quality of Service (QoS) values and thus the realized utility of an executed service composition may be significantly lower than the ex-ante computed one. The presented paper examines how the consideration of the effects of potential service failures can be modeled for an ex-ante QoS-aware service selection using expected utilities. Furthermore, we analytically evaluate our approach and demonstrate its applicability by an example. By doing so, we show that considering the effects of potential service failures leads to substantial better decisions about the QoS-aware service selection.	business process;expected utility hypothesis;heuristic (computer science);internet;mathematical optimization;money;quality of service;service composability principle	Bernd Heinrich;Lars Lewerenz	2013			service level requirement;service level objective;mobile qos;differentiated service;artificial intelligence;data mining;data as a service;management;world wide web;computer security	Metrics	-47.883110676568805	42.90578924899514	41248
0ff5e362315be1ec789463f1ec1d757abd7645f4	efficient software verification: statistical testing using automated search	test data;statistical analysis software testing automatic testing probability distribution software engineering fault detection sampling methods software algorithms application software flow graphs;near optimal probability distribution;code optimization;software verification;test coverage of code;software fault detection;statistical test;testing;random testing;program verification;indexing terms;automated search;statistical distributions;structural testing;statistical analysis;program testing;test coverage;probability distribution;software program verification;testing strategies;optimization;search problems;statistical testing;optimization software program verification testing strategies test coverage of code;statistical testing program testing program verification statistical distributions;dynamic testing;near optimal probability distribution software verification statistical testing automated search software fault detection dynamic testing random testing structural testing test data	Statistical testing has been shown to be more efficient at detecting faults in software than other methods of dynamic testing such as random and structural testing. Test data are generated by sampling from a probability distribution chosen so that each element of the software's structure is exercised with a high probability. However, deriving a suitable distribution is difficult for all but the simplest of programs. This paper demonstrates that automated search is a practical method of finding near-optimal probability distributions for real-world programs, and that test sets generated from these distributions continue to show superior efficiency in detecting faults in the software.	dynamic testing;sampling (signal processing);sensor;software verification;test data;white-box testing	Simon M. Poulding;John A. Clark	2010	IEEE Transactions on Software Engineering	10.1109/TSE.2010.24	domain testing;probability distribution;keyword-driven testing;reliability engineering;statistical hypothesis testing;regression testing;test data generation;orthogonal array testing;software performance testing;white-box testing;manual testing;software verification;computer science;software reliability testing;theoretical computer science;dynamic testing;risk-based testing;data-driven testing;statistics	SE	-60.74008252175879	34.962748791901866	41301
db69785af38197c4db68a2780e5c7a0f5ba4db00	resource classification from version control system logs	social network services;software;control systems;data mining;business;computer bugs;algorithm design and analysis	Collaboration in business processes and projects requires a division of responsibilities among the participants. Version control systems allow us to collect profiles of the participants that hint at participants' roles in the collaborative work. The goal of this paper is to automatically classify participants into the roles they fulfill in the collaboration. Two approaches are proposed and compared in this paper. The first approach finds classes of users by applying k-means clustering to users based on attributes calculated for them. The classes identified by the clustering are then used to build a decision tree classification model. The second approach classifies individual commits based on commit messages and file types. The distribution of commit types is used for creating a decision tree classification model. The two approaches are implemented and tested against three real datasets, one from academia and two from industry. Our classification covers 86% percent of the total commits. The results are evaluated with actual role information that was manually collected from the teams responsible for the analyzed repositories.	business process;cluster analysis;commitment scheme;control system;data logger;decision tree;k-means clustering;machine learning;statistical classification;veritas cluster server;version control	Kushal Agrawal;Michael Aschauer;Thomas Thonhofer;Saimir Bala;Andreas Rogge-Solti;Nico Tomsich	2016	2016 IEEE 20th International Enterprise Distributed Object Computing Workshop (EDOCW)	10.1109/EDOCW.2016.7584383	algorithm design;software bug;computer science;systems engineering;control system;artificial intelligence;operating system;software engineering;data mining;database;world wide web;computer security	SE	-61.97283825166576	40.306317373120216	41310
b58fc4dd8a3e9a1c9d7fabd156babc8f37993189	the elektra railway signalling-system: field experience with an actively replicated system with diversity	railways;high availability;railway interlocking systems;rail transportation;signalling;fault tolerant;design faults;availability;two channel system;active replication;alcatel austria;physical faults elektra railway signalling system field experience actively replicated system alcatel austria electronic interlocking system stringent safety requirements two channel system design diversity high availability high reliability actively triplicated redundancy on line recovery fault tolerance mechanisms design faults;physical faults;on line recovery;field experiment;computer architecture;safety critical software signalling railways redundancy safety traffic engineering computing;actively triplicated redundancy;redundancy;software safety;fault tolerant systems;high reliability;safety critical software;fault tolerance;field experience;safety;railway safety;fault tolerance mechanisms;traffic engineering computing;actively replicated system;design diversity;stringent safety requirements;elektra railway signalling system;system safety;rail transportation railway safety availability fault tolerance hardware software safety computer architecture redundancy costs fault tolerant systems;hardware;electronic interlocking system	Since the beginning of the century, Alcatel Austria has been the main supplier of railway signalling products in Austria. In 1985, Alcatel Austria began developing the electronic interlocking system ELEKTRA. In order to meet the stringent safety requirements for railway interlocking applications, a two channel system based on design diversity has been developed. High availability and reliability are achieved by using actively triplicated redundancy with on-line recovery. In 1989, the first system was put into operation. About 15 railway interlocking systems are in operation and further installations are ongoing. The paper presents the fault tolerance mechanisms used for design faults as well as physical faults. The experience gained with these concepts is also discussed. >		Heinz Kantz;Christian Koza	1995		10.1109/FTCS.1995.466954	reliability engineering;embedded system;engineering;transport engineering	HCI	-35.42019205478386	36.54416712229743	41326
2ec96a7294a3ee16227adbd4185c05a4027670cf	research on a behavior-based active controllable defense model and its application	databases;control systems;information systems;security model;computer network security;information security;helium;information technology;trojan network information security active controllable defense behavior based prediction detection response model bpdr key information systems defense architecture;three dimensional;bpdr model;power engineering and energy;information security information systems databases predictive models safety power system protection helium power engineering and energy information technology delay;computational modeling;behavior defense;security model active controllable defense behavior defense bpdr model;security requirements;safety;active controllable defense;invasive software;predictive models;power system protection;information system;invasive software computer network security	For facing the new challenges of the network information security, the theory of active controllable defense is introduced in this paper, and then a Behavior-based Prediction Detection Response model of active controllable defense called BPDR is proposed. Firstly, it is in order to reduce and eliminate the security delay brought by the traditional passive protection. Secondly, it is for resolving the problems of zero-day break produced by Trojan. Finally, it is with the purpose of meeting the high-level security requirements when the key information systems are under the specific safety situations. At the end of this paper, a structural security application model which is combined with the technologies of active controllable defense such as active prediction, dynamic detection and instant response is designed to prevent various known and unknown attacks effectively for building the hierarchical, three-dimensional and integrated defense architecture of the network information security.	high- and low-level;information security;information system;requirement;trojan horse (computing)	Zhitao Guan;Jietao He;Kehe Wu;Shuai Yuan	2010	2010 IEEE International Conference on Wireless Communications, Networking and Information Security	10.1109/WCINS.2010.5541867	simulation;computer science;information security;internet privacy;information technology;computer security;information system;computer network	Mobile	-61.26853846217072	59.69753728934727	41330
90512d9a1380a4fe1e56deb0d219b82894a7d26a	synthesis of a software security system	software engineering;computer security;software security;design and implementation;security systems;secure system;access control;privacy	This paper describes an ongoing Air Force sponsored project at The MITRE Corporation to develop provably effective security (access) controls for computer systems. Because of the stringent requirements imposed by the need for algorithmic security control, an innovative software engineering technique was developed for the design and implementation of the security kernel, the software portion of the controls. This paper touches briefly on these controls and then describes in some detail the components of the software engineering technique and the methodology for proving the correctness of the system. An example, taken from the security kernel for a PDP-11/45 based system, is used to demonstrate the components and techniques used.	application security;correctness (computer science);pdp-11;requirement;software engineering	Edmund L. Burke	1974		10.1145/1408800.1408873	software security assurance;computer security model;cloud computing security;security through obscurity;security information and event management;security engineering;covert channel;computer science;information security;backporting;social software engineering;software development;logical security;software engineering;trusted computing base;software construction;security service;security testing;network access control;computer security;software system;computer engineering	SE	-55.73116016710208	50.373902834012114	41339
4e074ca35cd9fae37be4fff614843735d09a55d7	a traceability technique for specifications	standard vector space model traceability technique specifications software life cycle information retrieval techniques documentation dimensionality reduction methods information theoretic approaches probabilistic approaches;software;manuals;formal specification;dimensionality reduction methods;vector space model;software maintenance;specifications;information retrieval;information retrieval techniques;system documentation;testing;computer architecture;large scale integration;information theoretic approaches;probabilistic approaches;standard vector space model;entropy;software life cycle;traceability technique;specifications traceability information retrieval software link;software specification;traceability;information theoretic;link;dimensional reduction;large scale integration documentation information retrieval code standards functional analysis information analysis performance analysis manuals software systems programming;system documentation formal specification information retrieval software maintenance;documentation	Traceability in software involves discovering links between different artifacts, and is useful for a myriad of tasks in the software life cycle. We compare several different Information Retrieval techniques for this task, across two datasets involving real-world software with the accompanying specifications and documentation. The techniques compared include dimensionality reduction methods, probabilistic and information theoretic approaches, and the standard vector space model.	dimensionality reduction;documentation;information retrieval;information theory;software release life cycle;traceability	Ahron Abadi;Mordechai Nisenson;Yahalomit Simionovici	2008	2008 16th IEEE International Conference on Program Comprehension	10.1109/ICPC.2008.30	entropy;traceability;verification and validation;link;documentation;computer science;software engineering;reverse semantic traceability;data mining;formal specification;database;software maintenance;vector space model;information retrieval;requirements traceability	SE	-57.66651517692207	33.229234064409226	41342
a64b9797b179e56df2adfa454a22a3fd6a33318a	bayesian reliability assessment of legacy safety-critical systems upgraded with fault-tolerant off-the-shelf software	bayesian reliability assessment;qa75 electronic computers computer science;fault tolerance;prediction accuracy;ha statistics	This paper presents a new way of applying Bayesian assessment to systems, which consist of many components. Full Bayesian inference with such systems is problematic, because it is computationally hard and, far more seriously, one needs to specify a multivariate prior distribution with many counterintuitive dependencies between the probabilities of component failures. The approach taken here is one of decomposition. The system is decomposed into partial views of the systems or part thereof with different degrees of detail and then a mechanism of propagating the knowledge obtained with the more refined views back to the coarser views is applied (recalibration of coarse models). The paper describes the recalibration technique and then evaluates the accuracy of recalibrated models numerically on contrived examples using two techniques: u-plot and prequential likelihood, developed by others for software reliability growth models. The results indicate that the recalibrated predictions are often more accurate than the predictions obtained with the less detailed models, although this is not guaranteed. The techniques used to assess the accuracy of the predictions are accurate enough for one to be able to choose the model giving the most accurate prediction.	algorithm;bayesian network;black box;cloud computing;complex system;component-based software engineering;computation;computational complexity theory;consistency model;fault tolerance;numerical analysis;online and offline;predictive modelling;public lending right;regular expression;software reliability testing	Peter T. Popov	2013	Rel. Eng. & Sys. Safety	10.1016/j.ress.2013.03.017	reliability engineering;fault tolerance;computer science;engineering;data mining;statistics	SE	-48.95228885565701	40.52085752151239	41363
1756b6ea3b00d2bc93942eb3613e59edea1c67b5	detecting stubborn permission requests in android applications		The Android permission mechanism is designed to protect the privacy of Android users. An Android application must request permissions when it needs to access sensitive data at runtime. If users do not grant the application requested permissions, the application would not provide functionalities related to these permissions. However, some applications violate this purpose in that they request permissions at initialization. If the user does not grant the requested permissions, these applications would simply exit, refusing to provide any functionalities, including the ones that do not require sensitive data. This behavior of stubbornly requesting permissions damages the right of users in utilizing non-sensitive functionalities. To address this problem, we propose an approach to detect this kind of permission requests. First, we model the key features of stubborn permission requests. Then, we identify the stubborn permission requests by statically analyzing Android applications. We evaluate our approach with real-world market applications and the experimental result shows that our app roach can effectively detect stubborn permission requests in Android applications.	android;app store;code;decompiler;privacy;run time (program lifecycle phase);sensor;static program analysis	Jianmeng Huang;Wenchao Huang;Fuyou Miao;Yan Xiong	2018	2018 4th International Conference on Big Data Computing and Communications (BIGCOM)	10.1109/BIGCOM.2018.00020	computer security;data mining;initialization;android (operating system);computer science;information privacy;permission	SE	-55.23856261676399	58.45711526704525	41375
cba7b962ddd3e8bb9fc05f1e7849b116c56c64ed	analysis of types and importance of sensors in smart home services	sensor systems;magnetic sensors;temperature sensors;smart cities;intelligent sensors;smart homes	In the paradigm shift of smart home to smart city, more sensors and smart objects will be deployed for user-centric service. To design the advanced smart city, understanding and analysis of duplicated sensors in current smart home services is important. In this paper, we categorize the smart home services and analyze the types of sensors that used in the smart home, and then we estimate the importance of sensors using the sensor tree. As a result, the motion sensor has high importance score and it is used in 7 smart home services.	categorization;home automation;motion detector;programming paradigm;sensor;smart tv;smart city;smart objects	Byeongkwan Kang;Seunghwan Kim;Myeong-in Choi;Keon-hee Cho;Seongman Jang;Sehyun Park	2016	2016 IEEE 18th International Conference on High Performance Computing and Communications; IEEE 14th International Conference on Smart City; IEEE 2nd International Conference on Data Science and Systems (HPCC/SmartCity/DSS)	10.1109/HPCC-SmartCity-DSS.2016.0196	embedded system;smart system;computer security;internet of things;intelligent sensor	Robotics	-42.22397625137243	50.92452804162579	41398
17d24cb36625d5aaa1370c43f877b5ede2c908ea	mining exception-handling rules as sequence association rules	databases;silicon;degradation;computer languages;programming language;application software;probability density function;function calls mining exception handling rules sequence association rules programming languages java c opened database connections;association rules;data mining;sequence association rules;function calls;mining exception handling rules;receivers;c language;real world application;opened database connections;association rule;lead;data mining association rules databases computer languages application software computer science java degradation lead;software algorithms;programming languages c language data mining exception handling java;exception handling;computer science;c;programming languages;java	Programming languages such as Java and C++ provide exception-handling constructs to handle exception conditions. Applications are expected to handle these exception conditions and take necessary recovery actions such as releasing opened database connections. However, exception-handling rules that describe these necessary recovery actions are often not available in practice. To address this issue, we develop a novel approach that mines exception-handling rules as sequence association rules of the form “(FC1<inf>c</inf><sup>1</sup>…FC<inf>c</inf><sup>n</sup>) ∧ FC<inf>a</inf> ⇒ (FC<inf>e</inf><sup>1</sup>…FC<inf>e</inf><sup>m</sup>)”. This rule describes that function call FCa should be followed by a sequence of function calls (FC<inf>e</inf><sup>1</sup>…FC<inf>e</inf><sup>m</sup>) when FC<inf>a</inf> is preceded by a sequence of function calls (FC<inf>e</inf><sup>1</sup>…FC<inf>c</inf><sup>n</sup>). Such form of rules is required to characterize common exception-handling rules. We show the usefulness of these mined rules by applying them on five real-world applications (including 285 KLOC) to detect violations in our evaluation. Our empirical results show that our approach mines 294 real exception-handling rules in these five applications and also detects 160 defects, where 87 defects are new defects that are not found by a previous related approach.	algorithm;association rule learning;c++;exception handling;http 404;java;mined;open-source software;requirement;software engineering	Suresh Thummalapenta;Tao Xie	2009	2009 IEEE 31st International Conference on Software Engineering	10.1109/ICSE.2009.5070548	association rule learning;computer science;operating system;data mining;database;programming language	SE	-58.126411503700446	40.02862767364079	41408
91fe9f903b23e4fd014bf948b5592ac567b2c498	"""enforcing """"sticky"""" security policies throughout a distributed application"""	distributed application;application independent pep;privacy policy enforcement;policy enforcement;obligation enforcement;sticky policies;policy decision point;channel model;privacy policy;security policy;pdp	"""Existing policy enforcement points (PEPs) typically call a local policy decision point (PDP) running at the local site, either embedded in the application, or running as a local stand alone service. In distributed applications, the PDPs at each site do not usually coordinate decision making amongst themselves, and do not pass policies between themselves. Thus it becomes very difficult to enforce """"sticky"""" policies such as privacy policies and obligations at all the sites in a distributed application. This paper looks at different ways in which the PEPs and PDPs of a distributed application may share policies between themselves so as to enforce """"sticky"""" policies throughout a distributed application. Three alternative models are described, the Application Protocol Enhancement Model, the Encapsulating Security Layer Model and the Back Channel Model. The strengths and weaknesses of the three models are evaluated, and we compare them to prior research in the field."""	channel (communications);common open policy service;distributed computing;embedded system;privacy policy;sticky bit;xacml	David W. Chadwick;Stijn F. Lievens	2008		10.1145/1463342.1463343	operations management;information security standards;business;internet privacy;computer security	ML	-49.3744771163262	55.023223028854424	41421
63f10460c812d02cb69a766ea6d9bce4ca84db82	meet hans, the heath & safety autonomous inspector		We present here one of the demonstrators we implemented as part of our research on the integration of robots in smart cities, where an autonomous mobile platform is employed for monitoring and assessing the Health&Safety rules in a smart office environment in combination with a centralised infrastructure for data integration, processing and reasoning. Monitoring of the status of infrastructures and environments still presents a number of challenges in terms of knowledge acquisition and representation, as data need to constantly be re-evaluated due to their high dynamism. Common solutions, ranging from human monitoring to sensor deployment, fail in flexibility, costs and, in the case of large scale scenarios, scalability. We focus on the idea that autonomous mobile agents can be used as moving sensors deployed by a larger, knowledgebased infrastructure, where the central unit collects and reasons over the information produced by the agents. In particular, the paper presents HanS, the Health&Safety inspector, with the goal of showing that applications integrating robots as data consumers and collectors can be deployed thanks to a combination of state-of-the-art semantic and robotics technologies.	autonomous robot;centralisation;knowledge acquisition;mobile agent;mobile operating system;robotics;scalability;sensor;smart city;software deployment	Emanuele Bastianelli;Gianluca Bardaro;Ilaria Tiddi;Enrico Motta	2018				AI	-41.66591867018954	48.32382515590433	41489
166755a844e0a4c3ae1340fc8a981ea4139a0c81	ghids: defending computational grids against misusing of shared resources	computational grid;security of data grid computing operating system kernels;grid user behavior analysis computational grid shared resource protection grid specific attack grid specific host based intrusion detection system bottleneck verification operating system kernel integer comparison;false alarm rate;shared resource protection;grid specific host based intrusion detection system;grid user behavior analysis;operating system;indexation;bottleneck verification;detection rate;grid specific attack;user behavior;operating system kernels;integer comparison;operating system kernel;grid computing;high performance;high efficiency;security of data;intrusion detection system;grid computing intrusion detection operating systems data security military computing protection kernel resource management mesh generation middleware	Detecting intrusions at host level is vital to protecting shared resources in grid, but traditional host-based intrusion detecting system (HIDS) is not suitable for grid environment. Grid-specific attacks are different from traditional ones, and traditional HIDS can not recognize a grid user and always with high performance overhead. This paper proposes a grid-specific host-based intrusion detection system (GHIDS) which employs bottleneck verification approach to detect intrusions with low false alarm rate and high detection rate. Working within operating system kernel and performing bottleneck verification by integer comparison, GHIDS achieves high efficiency and accuracy. Security reports generated by GHIDS are indexed not only by local user ID, but also by grid user ID. That is more useful for analyzing grid user behaviors globally by both host administrators and high level grid-based IDS	computation;grid systems corporation;high-level programming language;host-based intrusion detection system;kernel (operating system);operating system;overhead (computing);sensor;services computing	Guofu Feng;Xiaoshe Dong;Weizhe Liu;Ying Chu;Junyang Li	2006	2006 IEEE Asia-Pacific Conference on Services Computing (APSCC'06)	10.1109/APSCC.2006.60	intrusion detection system;computer science;operating system;constant false alarm rate;distributed computing;computer security;grid computing	HPC	-46.52662160630654	56.30661600689907	41499
20818c1fa4dfc772d14d899b4be9e355bef3ba5b	web app security: a comparison and categorization of testing frameworks	manuals;high performance computing;authentication;forgery;testing;software engineering;web apps;software security;software development;authorization;security testing	Web app developers often face challenges in using the many available security-testing frameworks, owing to those frameworks' inherent complexity and the lack of proper documentation. No up-to-date criteria exist that can help practitioners and organizations select an appropriate framework. Consequently, numerous vulnerabilities go undetected in the final product, creating a potential for major attacks. To help practitioners select the right framework, researchers classified 26 frameworks, using 27 criteria.	categorization;documentation;web application	Satish M. Srinivasan;Raghvinder S. Sangwan	2017	IEEE Software	10.1109/MS.2017.21	software security assurance;supercomputer;web application;computer science;software development;software engineering;authentication;software testing;authorization;internet privacy;security testing;world wide web;computer security	SE	-60.719527064703485	55.97809997424717	41507
4daecda9c07ced90b5dca1cd2d8275acb855ad08	modeling unknown values in test and verification		With increasing complexities and a component-based design style the handling of unknown values (e. g., at the interface of components) becomes more and more important in electronic design automation (EDA) and production processes. Tools are required that allow an accurate modeling of unknowns in combination with algorithms balancing exactness of representation and efficiency of calculation. In the following, state-ofthe-art approaches are described that enable an efficient and successful handling of unknown values using formal techniques in the areas of Test and Verification.	algorithm;black box;circuit design;component-based software engineering;digital electronics;electronic design automation;formal methods;semantics (computer science)	Bernd Becker;Matthias Sauer;Christoph Scholl;Ralf Wimmer	2015		10.1007/978-3-658-09994-7_5	theoretical computer science;conjunctive normal form;true quantified boolean formula;automatic test pattern generation;electronic design automation;computer science	EDA	-36.426424014676705	33.73485314514644	41545
d23d407c7f0fee654f904ee41b37a9df7f9b5070	analysis and a case study of transparent computing implementation with uefi	unified extensible firmware interface;transparent computing;software as a service;uefi;virtualisation;saas	Transparent computing (TC) can be thought as a special kind of cloud computing that regards storage as a service. TC logically splits the software stack from the underlying hardware platform, and separates the computing unit from storage for the purpose of making the same software run on different hardware and different software run on the same hardware. TC requires a unified software-hardware interface to abstract the underlying platform details. As the new generation BIOS interface, Unified Extensible Firmware Interface (UEFI) defines the interface between the software stack and hardware platform, which aligns TC well. UEFI’s modular design also allows for BIOS customisation. An ideal vision of TC implementation with UEFI is proposed in this paper as the forecast. Several models are also provided to illustrate the constraints of current hardware and software stack in terms of performance, portability, and hardware adaptability. Finally, a case study of a wireless TC tablet is provided together with the analysis.	bios;cloud computing;hardware interface design;modular design;personalization;software portability;tablet computer;transparency (human–computer interaction);unified extensible firmware interface	Wu Ming	2012	IJCC	10.1504/IJCC.2012.049765	embedded system;real-time computing;computer science;operating system;software as a service	Arch	-34.637182814087225	52.662372108554415	41568
95c57dd96a6dde03acaaa448dc9e916d200d2f15	qos prediction of web service based on us-aws	web service;qos prediction;the combined model	In parallel with the rapid growth of the number of research works, there exist some disadvantages and advantages of each model, how to combine the strengths of each model to create a new model for more effective. This is an important issue that should be studied in the context of increasing the number of new models. In this paper, we proposed a new combined model for predicting the QoS values of Web service. We first construct the AWS model based on an AutoenCoder with two predictions: User-based AWS (U-AWS) and Service-based AWS (S-AWS). Then we combine two predictions (U-AWS and S-AWS) to produce a more accurate one. To train the combined model, we generate the level one data by using the J-fold cross-validation data and using regression models to combine predictive modeling. Experimental results showed that the combined method has better results than single methods.	amazon web services;quality of service;web service	Le Van Thinh;HongBing Wang;Nguyen Xuan Hau	2016	JoSSR	10.1007/s12927-016-0010-y	simulation;computer science;data mining;world wide web	HPC	-50.291505000520004	43.45570943761401	41604
861c820a26aa90ad44e3e1e178b6f483bc567231	codesurfer/path inspector	program diagnostics;c language;reasoning about programs;reasoning about programs program diagnostics c language;programming flaws codesurfer path inspector source code analysis source code navigation tool c c language x86 machine code;navigation software maintenance tree graphs information analysis computer bugs specification languages costs educational institutions;source code analysis	CodeSurfer is a powerful source code analysis and navigation tool for a range of languages, including C/C++ and x86 machine code. The Path Inspector is an add-on to CodeSurfer that allows a user to reason about paths through the program, and which can be used to find programming flaws.	add-ons for firefox;c++;machine code;static program analysis;x86	Paul Anderson	2004	20th IEEE International Conference on Software Maintenance, 2004. Proceedings.	10.1109/ICSM.2004.1357853	kpi-driven code analysis;intentional programming;computer science;theoretical computer science;redundant code;computer programming;database;code coverage;programming language;source lines of code;managed code;code generation;assembly language;static program analysis;unreachable code;source code	SE	-58.16817511523522	36.378591882329474	41637
38892fa8cc84bde5ee637d87a17b95d3b44ec4e4	flexible service composition based on bundle communication in osgi	ubiquitous computing;m2m;osgi;self adaptive systems	Service provision and consumption platforms are more and more used to enable communication between sensors, actuators and intelligent devices, since they provide mechanisms that make possible the combination of components to create composed services. However, these kinds of platforms have limitations to adapt themselves to new and unknown devices. In this work we analyze the challenge of component or bundle communication by using the Open Services Gateway Initiative (OSGi) technology and we propose three mechanisms with the aim of contributing to flexible component communication: Common Service, Specific Service and WSIF Web Service Invocation. We provide these solutions with some architectural models and validate them through different example services. Finally we compare them regarding performance, flexibility and application complexity.	osgi	Ramón Alcarria;Tomás Robles;Augusto Morales Dominguez;Sergio Gonzalez-Miranda	2012	TIIS	10.3837/tiis.2012.01.007	embedded system;computer science;operating system;machine to machine;world wide web;computer security;ubiquitous computing;devices profile for web services;computer network	HPC	-40.35785774116787	45.943684536082905	41676
66dfdff9fd66b83f4b7e58f6d42c31a6663b0e98	a survey on energy-aware security mechanisms	energy aware security;security;mobile devices;green networking	The increasing adoption of mobile devices as the preferred tool to access the Internet imposes to deepen the investigation of security aspects. In parallel, their power constrained nature must be explicitly considered in order to analyze security in an effective and comprehensive manner. This aspect, which is often neglected in the literature, allows investigating two important behaviors of mobile devices: i) evaluate if all the layers accounting for privacy and security can be re-engineered or optimized to save power, and ii) understand the effectiveness of draining energy to conduct attacks. In this perspective, this paper surveys and highlights the most recent work on energy-awareness and security. Also, it summarizes the current state of the art on general techniques to save energy, as well as tools to perform measurements. The major contributions of this survey are, thus, a review of past work aimed at minimizing the energy footprint of security mechanisms, and the identification of promising research trends, such as detecting attacks via anomalous power consumption.	mobile device;sensor	Alessio Merlo;Mauro Migliardi;Luca Caviglione	2015	Pervasive and Mobile Computing	10.1016/j.pmcj.2015.05.005	computer security model;cloud computing security;simulation;security through obscurity;security information and event management;security engineering;asset;computer science;information security;operating system;mobile device;human-computer interaction in information security;security service;internet privacy;security testing;computer security;computer network	Security	-47.150435028744305	59.72453545704748	41685
42fd80b3b88bd8ec39bbe010755af1f75edeed9c	towards the governance of open distributed systems: a case study in wireless mobile grids	000 informatik informationswissenschaft allgemeine werke;330 wirtschaft;000 allgemeines wissenschaft	New networking technologies such as wireless mobile grids and peer-to-peer middleware are examples of a growing class of open distributed systems whose strength is the absence of a central controlling instance and which function through the cooperation of autonomous entities that voluntarily commit resources to a common pool. The social dilemma in such systems is that it is advantageous for rational users to access the common pool resources without making any commitment of their own. This is commonly known as free-riding. However, if a substantial number of users followed this selfish strategy, the system itself would fail, depriving all users of its benefits. In this dissertation, we demonstrate how governance decisions can induce cooperation in such systems and how normative frameworks in combination with multi-agent system simulations can be successfully employed to analyse their effects, even at an early development stage. We show that our approach is not only practical and powerful, but also easily accessible. We demonstrate its unctionality by implementing a prototype to explore the impact of enforcement mechanisms on wireless mobile grids, a concept which has been proposed to address the energy issues arising in the next generation of mobile phones and the networks that connect them. We also infer lessons from this example for open distributed systems in general. Simulation experiments quantify the benefits of enforcement mechanisms for wireless mobile grids. We analyse these results with respect to the costs of enforcement as well as further criteria that reflect the interests of the multiple stakeholders in the system. We conclude with some observations on how the lessons learned from both process and outcomes may be applicable to the broader context of open distributed systems. In particular, we highlight (i) the use of simulation using intelligent agents and a normative framework as a means for in silico exploration of complex systems for both business and technological objectives, and (ii) the insight offered into a range of enforcement mechanisms and a better understanding of the conditions and constraints under which they are applicable.	distributed computing	Tina Balke	2011			simulation;engineering;management science;computer security	Mobile	-36.37676485850582	43.901488453934704	41718
cfbad76fb3750b1d49917536d12b77f7b69b75e8	a component-based approach to adaptive user-centric pervasive applications	context aware;real time;rule based;adaptive control;data processing;data analysis;adaptive system;sensors and actuators;domain specific language;self organization;man machine interface;system architecture;affective computing	In the last years computing has become omnipresent and even devices that do not look like computers have computing capabilities. Seamless man-machine interfaces and ad-hoc communication allow for pervasive adaptive control and computer support in everyday activities. So-called pervasive-adaptive environments are becoming able to monitor, diagnose and respond to the cognitive, emotional and physical states of persons in real time. In this talk we present a new approach for designing and realising adaptive systems that provide assistance to humans in a discrete and personalized manner. The approach is based on a strict component-based framework for controlling pervasive adaptive systems including real-time sensor and actuator control, user and context-awareness, affective computing, self-organization and adaptation. A rule-based domain-specific language simplifies the dynamic creation and modification of system architectures; mechanisms for the transparent distribution of applications, flexible on-line data processing, and early experimentation with data analysis algorithms facilitate the construction of user-centric adaptive systems while a modular assume/guarantee framework allows to compute formal representation of such systems and to verify them against given system requirements. We illustrate our approach by two case studies for detecting cognitive overload and influencing the mood of a user in the way he desires.		Martin Wirsing	2010		10.1007/978-3-642-13821-8_2	human–machine interface;control engineering;real-time computing;self-organization;data processing;adaptive control;computer science;domain-specific language;theoretical computer science;adaptive system;affective computing;data analysis	HCI	-40.80908701956629	42.4982446066339	41809
523d2d909788ae4f18fa905af5868efa14ca9abe	reverse engineering convolutional neural networks through side-channel information leaks		A convolutional neural network (CNN) model represents a crucial piece of intellectual property in many applications. Revealing its structure or weights would leak confidential information. In this paper we present novel reverse-engineering attacks on CNNs running on a hardware accelerator, where an adversary can feed inputs to the accelerator and observe the resulting off-chip memory accesses. Our study shows that even with data encryption, the adversary can infer the underlying network structure by exploiting the memory and timing side-channels. We further identify the information leakage on the values of weights when a CNN accelerator performs dynamic zero pruning for off-chip memory accesses. Overall, this work reveals the importance of hiding off-chip memory access pattern to truly protect confidential CNN models.	adversary (cryptography);artificial neural network;computer memory;confidentiality;convolutional neural network;encryption;hardware acceleration;information leakage;input/output;mathematical optimization;memory access pattern;performance tuning;reverse engineering;spectral leakage;vulnerability (computing)	Weizhe Hua;Zhiru Zhang;G. Edward Suh	2018	2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC)	10.1109/DAC.2018.8465773	real-time computing;convolutional neural network;reverse engineering;side channel attack;encryption;adversary;information leakage;data modeling;computer science;hardware acceleration	EDA	-54.585751223506506	55.364037266786134	41851
272f127cb3cb18cf6b656ae53b4d223ebb80d9ad	fault detection in safety-critical embedded systems	fault detection;embedded system	In the paper, a proposition for a systematic approach to fault detection in building a dependable and fault-tolerant control system is presented. A network of simple monitoring cells that monitor and evaluate functioning of critical sub-processes of the system is proposed. Further, different approaches for the implementation of the monitoring cells are observed.	embedded system;fault tolerance	Domen Verber;Matjaz Colnaric;Wolfgang A. Halang	2002			fault;fault indicator;fault detection and isolation	EDA	-36.57382844743359	36.86520014755392	41961
07a712348a1a8fa914231efe182d4258f22690e6	automatic segmentation of method code into meaningful blocks: design and evaluation	program understanding;software tool;readability;automatic formatting;article	Good programming practice and guidelines suggest that programmers use both vertical and horizontal spacing to visibly delineate between code segments that represent different algorithmic steps or high level actions. Unfortunately, programmers do not always follow these guidelines. Editors and IDEs can easily indent code based on syntax, but they do not currently support automatic blank line insertion, which presents more significant challenges involving the semantics. This paper presents and evaluates a heuristic solution to the automatic blank line insertion problem, by leveraging both program structure and naming information to identify “meaningful blocks”, consecutive statements that logically implement a high level action. Our tool, SEGMENT, takes as input a Java method, and outputs a segmented version that separates meaningful blocks by vertical spacing. We report on several studies involving human judgements to evaluate the effectiveness of the automatic blank line insertion algorithm, for different size methods and for different levels of programmer expertise. The results indicate strong positive overall opinion of SEGMENT’s effectiveness in comparison with both developer-written blank lines and blank lines inserted by newcomers to the code. The results vary only slightly among short and long methods, and among novice and advanced programmers. SEGMENT assists in making users obtain an overall picture of a method’s actions and comprehend it quicker as well as provides hints for internal documentation placement. Copyright c © 2012 John Wiley & Sons, Ltd.	algorithm;code segment;computer programming;contour line;documentation;eclipse;gnu indent;heuristic;high-level programming language;integrated development environment;java;john d. wiley;programmer;software design pattern;software maintenance;structured programming	Xiaoran Wang;Lori L. Pollock;K. Vijay-Shanker	2014	Journal of Software: Evolution and Process	10.1002/smr.1581	computer science;engineering;software engineering;management;engineering drawing;algorithm	SE	-55.424801479269	36.236165943481964	42020
a2a79b27e49943ebb3bc73ec7cf2987f0ee2450f	research on the collaborative analysis technology for source code and binary executable based upon the unified defect mode set	source code defect mode;collaborative analysis technology binary executable vulnerability detection technique source code vulnerability detection technique test case generation dynamic analysis binary program vulnerability detection method test software security vulnerability detection static analysis method security defect detection technology unified defect mode set;security binary codes software buffer overflows collaboration data mining context;system monitoring automatic test software program diagnostics security of data source code software;collaborative analysis;unified defect mode set;binary executable defect mode;collaborative analysis source code defect mode binary executable defect mode unified defect mode set	The security defect detection technology based on source code usually makes use of static analysis methods to detect security vulnerabilities in the test software, which does not consider the runtime information of program execution and the interaction information with the program runtime surrounding environment, so the security defect detection technology based on source code usually results in higher false positive rate. The binary program vulnerability detection methods based upon dynamic analysis usually have lower false positive rate, but their effectiveness depends entirely on test case generation, the detection efficiency of dynamic analysis based on binary program is lower. Moreover, the research of source code vulnerability detection technique and binary executable vulnerability detection technique are essentially fragmented, without considering the relationship between the two classes of methods. In order to improve the efficiency for source code and binary executables vulnerability detection, we presents a collaborative analysis method for vulnerability detection of source code and binary executables based upon the unified defect mode set, meanwhile, we verify the effectiveness of our method using a concrete example in this paper.	executable;interaction information;malware;run time (program lifecycle phase);software bug;static program analysis;test case;vulnerability (computing)	Xiaobing Liang;Baojiang Cui;Yingjie Lv;Yilun Fu	2015	2015 9th International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing	10.1109/IMIS.2015.40	kpi-driven code analysis;real-time computing;computer science;theoretical computer science;computer security;static program analysis	SE	-57.726945543430055	41.16926332106719	42066
529bc3c351655dd22df050b376af9b7c7efdd6f8	tv-centric gaming applications for android os: architecture and a framework	television receivers;internet;television receivers computer games internet mobile computing operating systems computers;concept usability tv centric gaming application android os operating systems consumer electronic device set top box tv receiver television receiver mobile device internet innovative gameplay tv centric gaming concept user survey;games tv smart phones consumer electronics androids humanoid robots;computer games;mobile computing;operating systems computers	The proliferation of devices with Android OS recently facilitated the integration of various consumer electronic devices running Android for novel applications. In this paper the architecture and a framework for the development of TV-centric games is presented. These games involve set-top boxes or TV receivers, mobile devices and the Internet towards the creation of an innovative gameplay. The paper discusses the benefits of TV-centric gaming concept, presents several developed game prototypes and gives the first results of user survey with regard to the usability of the concept.	android;internet;mobile device;operating system;set-top box;usability	Milan Z. Bjelica;Vladan Zdravkovic;Marija Punt;Nikola Teslic	2013	2013 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2013.6487064	the internet;simulation;computer science;operating system;multimedia;mobile computing	Mobile	-38.50527737418059	53.200200264322376	42084
5fd88f89bd8738816e62808a1b7fb12d3ab14a2f	nix: a safe and policy-free system for software deployment	software deployment;scientific;wiskunde en informatica wiin	Existing systems for software deployment are neither safe nor sufficiently flexible. Primary safety issues are the inability to enforce reliable specification of component dependencies, and the lack of support for multiple versions or variants of a component. This renders deployment operations such as upgrading or deleting components dangerous and unpredictable. A deployment system must also be flexible (i.e., policyfree) enough to support both centralised and local package management, and to allow a variety of mechanisms for transferring components. In this paper we present Nix, a deployment system that addresses these issues through a simple technique of using cryptographic hashes to compute unique paths for component instances.	apple lisa;automatic programming;capacitor plague;centralisation;code reuse;compile farm;component-based software engineering;computer science;cryptographic hash function;cryptography;heart rate variability;high- and low-level;make;nix package manager;operating system;program transformation;rendering (computer graphics);rewriting;server (computing);software build;software configuration management;software deployment;software development;software system;system deployment;transformation language	Eelco Dolstra;Merijn de Jonge;Eelco Visser	2004			deployment diagram;software deployment;computer security	OS	-53.24144097969568	45.23325247983113	42169
906c8c9fe1b6c793f5ae1e933d1b5bb5fc8359fe	data cleaning technique for security logs based on fellegi-sunter theory		Information security is one of the most important aspects an organization should consider. Due to this matter and the variety of existing vulnerabilities, there are specialized groups known as Computer Security Incident Response Team (CSIRT), that are responsible for event monitoring and for providing proactive and reactive support related to incidents. Using as a case study a CSIRT of a university with 10,000 users, and considering the high volume of events to be analyzed on a daily basis, it is proposed to implement a Big Data ecosystem. One of the most important activities for the information processing is the data cleaning phase, it will remove useless data and help to overcome storage limitations, since CSIRT is actually limited to a small time-frame, usually a few days and cannot analyze historical security events. Focusing on this cleaning phase, this article analyzes an intuitive technique and proposes a comparative technique based on the Fellegi-Sunter theory. The main conclusion of our research is that some data could be safely ignored helping to reduce storage size requirements. Moreover, increasing the data retention will enable to detect some events from historical data.	plasma cleaning	Diana Martínez-Mosquera;Sergio Luján-Mora;Gabriel López Millán;Lauro Santos	2017		10.1007/978-3-319-66996-0_1	information security;information processing;data retention;data mining;big data;event monitoring;computer science;vulnerability	Crypto	-61.30586824369664	53.79505849385386	42233
4bd26447c3dc75eb1b59466b0658b54f3dc07c9a	challenges and trends in probabilistic programming (dagstuhl seminar 15181)	bayesian networks differential privacy machine learning probabilistic programs security semantics static analysis verification;004	"""This report documents the program and the outcomes of Dagstuhl Seminar 15181 """"Challenges and Trends in Probabilistic Programming"""". Probabilistic programming is at the heart of machine learning for describing distribution functions; Bayesian inference is pivotal in their analysis. Probabilistic programs are used in security for describing both cryptographic constructions (such as randomised encryption) and security experiments. In addition, probabilistic models are an active research topic in quantitative information now. Quantum programs are inherently probabilistic due to the random outcomes of quantum measurements. Finally, there is a rapidly growing interest in program analysis of probabilistic programs, whether it be using model checking, theorem proving, static analysis, or similar. Dagstuhl Seminar 15181 brought researchers from these various research communities together so as to exploit synergies and realize cross-fertilisation."""		Gilles Barthe;Andrew D. Gordon;Joost-Pieter Katoen;Annabelle McIver	2015	Dagstuhl Reports	10.4230/DagRep.5.4.123	computer science;theoretical computer science;machine learning;data mining;probabilistic logic	DB	-62.555016249064884	48.257010130245824	42236
5f08341e26204ba63c0fcd4ecb48c812a0e0b21a	multi-metrics approach for security, privacy and dependability in embedded systems	sensor systems;multi metrics;internet of things;embedded systems;dependability;security;privacy	Embedded Systems have become highly interconnected devices, being the key elements of the Internet of Things. Their main function is to capture, store, manipulate and access data of a sensitive nature. Moreover, being connected to Internet, expose them to all kind of attacks, which could cause serious consequences. Traditionally, during the design process, security, privacy and dependability (SPD) have been set aside, including them as an add-on feature. This paper provides a methodology together with a Multi-Metrics approach to evaluate the system SPD level during both the design and running processes. The simplicity, based on a single process during the whole system evaluation, and scalability, simple and complex systems are evaluated equally, are the main advantages. The applicability of the presented methodology is demonstrated by the evaluation of a smart vehicle use case.	add-ons for firefox;complex systems;dependability;embedded system;entry point;internet of things;privacy;requirement;scalability;self-organized criticality	Iñaki Garitano;Seraj Fayyad;Josef Noll	2015	Wireless Personal Communications	10.1007/s11277-015-2478-z	embedded system;computer science;information security;operating system;dependability;privacy;computer security;internet of things;computer network	Security	-50.814484864165124	56.419030132711015	42312
5e938a83767e97c849ce3760589d0a5b30e58545	on cost function synthesis for multi-objective design decisions in complex real-time systems	directed graphs;design decisions;modern complex real time systems engineering;cost function;life cycle;costing;resource allocation;cost function synthesis;information technology;resource allocation real time systems systems analysis directed graphs costing;attribute cost functions;government;resource management;multi objective design decisions;technology management;equational form;multiple objectives;systems analysis;engineering management;information management;system cost function synthesis;cost function real time systems resource management timing government computer science information management technology management engineering management information technology;non functional objectives;dimensional units;computer science;multi objective problems cost function synthesis multi objective design decisions modern complex real time systems engineering design objectives dimensional units design decisions system life cycle multiple objectives non functional objectives attribute cost functions hierarchical model equational form system cost function synthesis;design objectives;hierarchical model;system life cycle;multi objective problems;real time systems;timing	Engineering of modern complex real-time systems is driven by a host of design objectives, often di ering not only in nature, importance, and form, but also in dimensional units and range, and themselves interacting in complex ways. These objectives in uence design decisions throughout the entire system life cycle. We consider the problem of formulating the cost of a design decision driven by multiple objectives. We start by discussing issues and relationships among non-functional objectives, their decomposition, and attribute cost functions used to quantify them. We then describe a hierarchical model, along with an equational form, for system cost function synthesis. Finally, examples are given to show how the model applies to actual multi-objective problems in the design of complex real-time systems.	approximation;graphical user interface;hierarchical database model;image scaling;interaction;interactivity;interdependence;loss function;prototype;real-time clock;real-time computing;real-time transcription;system lifecycle;systems design	Carlos C. Amaro;Roman Nossal-Tüyeni;Alexander D. Stoyen	1999		10.1109/ICECCS.1999.802853	systems analysis;resource allocation;computer science;systems engineering;engineering;technology management;resource management;management science;information management;management;information technology;government	Embedded	-49.26062286183567	42.78206947818708	42356
bd64958ba4fdbf65809a23c4b2ff2deaebd3f028	domain-independent monitoring and visualization of sla metrics in multi-provider environments - (short paper)		Over the past decade IT services have become increasingly multi-sourced, from IT outsourcing of functions such as network management and server provisioning to the widespread use of micro-services from different vendors in Cloud applications. Managing end-to-end service quality is becoming an increasing challenge. One of the key issues lies in the exchange of quality-related data between service participants in order to analyze not only SLA compliance but also root causes of failure. Formal SLA specification and management approaches today do not facilitate the specification of metric data in conjunction with a convenient way to supply this data to business analytics tools for detailed analysis. In this industry paper, we propose an extension of the rSLA DSL defining data items to be monitored and a specification-driven generator producing relational, denormalized time series tables accordingly. The resulting time series can be ingested by commonly used Business Intelligence tools such as Watson Analytics or Tableau in a straightforward and efficient manner. Using this approach, service integrators quickly identify problem areas in federated service delivery and consequently improve operational efficiency and avoid penalties related to SLA violations.	service-level agreement	Robert Engel;Bryant Chen;Shashank Rajamoni;Heiko Ludwig;Alexander Keller;Mohamed Mohamed;Samir Tata	2017		10.1007/978-3-319-69462-7_39	business analytics;data mining;service delivery framework;business intelligence;cloud computing;network management;operational efficiency;provisioning;analytics;computer science	HPC	-53.346820817671905	43.444787006469156	42463
4c4dd4eeb28aca3b4294ad67957de90b23b52890	automated performance maintenance for service compositions	transparent runtime monitoring automated performance maintenance web service compositions bpel processes service level agreements web service monitoring runtime adaptability;detectors;degradation;service composition;runtime adaptability;automated performance maintenance;web services software maintenance specification languages system monitoring;bpel processes;software maintenance;perforation;service level agreements;system monitoring;maintenance engineering;web service;runtime;web service composition;time factors;monitoring;specification languages;web services;maintenance engineering monitoring detectors time factors web services runtime degradation;runtime monitoring;transparent runtime monitoring;service level agreement;web service monitoring;web service compositions	Web service compositions, usually defined as BPEL processes, need to adapt to changes in their constituent web services, in order to maintain functionality and performance. Therefore, BPEL processes must be able to detect web service failure and performance degradation resulting in the violation of service-level agreements. Automated diagnosis and repair are equally important. However, BPEL lacks constructs for web service monitoring and runtime adaptability, which are pre-requisites for diagnosis and repair. We present a solution for transparent runtime monitoring, as well as automated performance degradation detection, diagnosis, and repair for BPEL processes. Our solution uses lightweight monitoring techniques, supports customizable diagnosis and repair strategies, and is compatible with any standards-compliant BPEL engine.	business process execution language;elegant degradation;overhead (computing);sampling (signal processing);service-level agreement;standards-compliant;traceability;web service	Adina D. Mosincat;Walter Binder	2009	2009 11th IEEE International Symposium on Web Systems Evolution	10.1109/WSE.2009.5631246	maintenance engineering;web service;real-time computing;database;law;world wide web	Embedded	-46.85002927469261	40.83645551590454	42467
665607f1f968b82015d568baccb3f672e5158b3b	towards training set reduction for bug triage	software;machine learning algorithms;bug data bug fixing bug triage training set reduction iterative case filter feature selection algorithm instance selection algorithm;training set reduction;instance selection;software maintenance;instance selection algorithm;training;software quality bug triage training set reduction feature selection instance selection;software maintenance iterative methods program debugging;iterative methods;bug fixing;accuracy;machine learning;feature selection algorithm;iterative case filter;bug triage;training accuracy computer bugs text categorization machine learning algorithms software educational institutions;feature selection;program debugging;computer bugs;text categorization;software quality;bug data	Bug triage is an important step in the process of bug fixing. The goal of bug triage is to assign a new-coming bug to the correct potential developer. The existing bug triage approaches are based on machine learning algorithms, which build classifiers from the training sets of bug reports. In practice, these approaches suffer from the large-scale and low-quality training sets. In this paper, we propose the training set reduction with both feature selection and instance selection techniques for bug triage. We combine feature selection with instance selection to improve the accuracy of bug triage. The feature selection algorithm ¦Ö2-test, instance selection algorithm Iterative Case Filter, and their combinations are studied in this paper. We evaluate the training set reduction on the bug data of Eclipse. For the training set, 70% words and 50% bug reports are removed after the training set reduction. The experimental results show that the new and small training sets can provide better accuracy than the original one.	chi;eclipse;feature selection;hotfix;iterative method;machine learning;selection algorithm;software bug;software engineering;software quality;test set;windows firewall	Weiqin Zou;Yan Hu;Jifeng Xuan;He Jiang	2011	2011 IEEE 35th Annual Computer Software and Applications Conference	10.1109/COMPSAC.2011.80	software bug;computer science;software engineering;machine learning;pattern recognition;data mining;accuracy and precision;iterative method;software maintenance;feature selection;software regression;software quality	SE	-62.37498765372256	39.1174752273828	42486
01bc478a9bd5693603b662f515bedb7393c11fa3	resilient design and operation of cyber physical systems with emphasis on unmanned autonomous systems	resilience;self-organization;reconfigurable control;complex adaptive systems	Autonomy and autonomous systems are occupying central stage in the research community, as autonomous vehicles are proliferating and their utility in all aspects of the military and civilian domains are increasing exponentially from one year to the next. The development and application of resiliency and safety technologies to autonomous systems is, unfortunately, not keeping pace with their growth rate. Several factors impede the deployment and adoption of autonomous systems. Among them is the absence of an adequately high level of autonomy that can be relied upon, significant challenges in the area of human-machine interface requiring significant human intervention to operate and interpret sensor data, the need for emerging machine learning technologies and, most importantly, the resilient design and operation of complex systems to assure their safety, reliability and availability when executing missions in unstructured and cluttered environments. Recent advances in resiliency and safety of complex engineered systems have focused on methods/tools to tradeoff system performance for increased time to failure aiming at mission completion or trial and error methods to arrive at a suboptimal policy for system self-organization in the presence of a failure mode. This paper introduces a novel framework for the resilient design and operation of such complex systems via self-organization and control reconfiguration strategies that avoid empirical trial and error techniques and may be implemented and perform in real time on-platform. The main theme is summarized as: “a healthy and resilient system is a safe system”. To accomplish this objective, we introduce an integrated and rigorous approach to resilient design while safety considerations ascertain that the targeted system is contained within a safe envelope. A resilient system is robustly and flexibly monitoring its internal and external environment, it can detect and anticipate disturbances that may affect its operational integrity and take appropriate action to compensate for the disturbance. Resilience enhances safety while improving risk factors and assures that vehicles subjected to extreme disturbances remain within their safe envelope. The enabling technologies begin with graph spectral and epidemic spreading modeling tools to represent the system behaviors under normal and faulty conditions; a Markov Decision Process is the basic self-organization module. We are introducing a novel approach to fault-tolerance by considering the impacts of severe fault modes on system performance as inputs to a Reinforcement Learning (RL) strategy that trades off system performance with control activity in order to extend the Remaining Useful Life (RUL) of the unmanned system. Performance metrics are defined and assist in the algorithmic developments and their validation. We pursue an integrated and verifiable methodology to safety assurance that enables the evaluation of the effectiveness of risk management strategies. Several unmanned autonomous systems are used for demonstration purposes.	autonomous system (internet);unmanned aerial vehicle	George J. Vachtsevanos;Benjamin Lee;Sehwan Oh;Michael Balchanos	2018	Journal of Intelligent and Robotic Systems	10.1007/s10846-018-0881-x	reliability engineering;control engineering;risk management;software deployment;cyber-physical system;failure mode and effects analysis;control reconfiguration;reinforcement learning;complex adaptive system;engineering;safety assurance	Robotics	-44.490673078189744	38.14484711553931	42563
71a5944e6e034399dd857952792f6a58044f03fc	validating security policy conformance with ws-security requirements	static checking;ws security;best practice;data exchange;conformance validation;basic security profile;security requirements;ws securitypolicy;security policy;web services security	Web Services Security (WS-Security) is a technology to secure the data exchanges in SOA applications. The security requirements for WS-Security are specified as a security policy expressed in Web Services Security Policy (WS-SecurityPolicy). The WS-I Basic Security Profile (BSP) describes the best-practices security practices for addressing the security concerns of WS-Security. It is important to prepare BSP-conformant security policies, but it is quite hard for developers to create valid security polices because the security policy representations are complex and difficult to fully understand. In this paper, we present a validation technology for security policy conformance with WS-Security messages. We introduce an Internal Representation (IR) representing a security policy and its validation rules, and a security policy is known to be valid if it conforms to the rules after the policy is transformed into the IR. We demonstrate the effectiveness of our validation technology and evaluate its performance on a prototype implementation. Our technology makes it possible for a developer without deep knowledge of WS-Security and WS-SecurityPolicy to statically check if a policy specifies appropriate security requirements.	conformance testing;requirement;ws-security	Fumiko Satoh;Naohiko Uramoto	2010		10.1007/978-3-642-16825-3_10	software security assurance;computer security model;data exchange;standard of good practice;cloud computing security;web application security;security through obscurity;security information and event management;security engineering;security convergence;covert channel;asset;computer science;security policy;information security;information security standards;database;security service;security analysis;security testing;network security policy;world wide web;computer security;best practice	Crypto	-51.69109720968576	52.27223967686228	42593
fe51ee9db58b64b517e26f0ff8a050d2e7a374f0	context-sensitive access in industrial internet of things (iiot) healthcare applications		Industrial Internet of Things (IIoTs) is the fast growing network of interconnected things that collects and exchange data using embedded sensors planted everywhere. Several IIoT applications such as the ones related to healthcare systems are expected to widely utilize the evolving 5G technology. This 5G-inspired IIoT paradigm in healthcare applications enables the users to interact with various types of sensors via secure wireless medical sensor networks (WMSNs). Users of 5G networks should interact with each other in a seamless secure manner. And thus, security richness is highly coveted for the real time wireless sensor network systems. Asking users to verify themselves before every interaction is a tedious, time-consuming process that disrupts inhabitants’ activities, and degrades the overall healthcare system performance. To avoid such problems, we propose a context-sensitive seamless identity provisioning (CSIP) framework for the IIoT. CSIP proposes a secure mutual authentication approach using hash  and global assertion value to prove that the proposed mechanism can achieve the major security goals of the WMSN in a short time period.	assertion (software development);context-sensitive grammar;embedded system;interaction;internet of things;mutual authentication;programming paradigm;provisioning;seamless3d;sensor	Fadi M. Al-Turjman;Sinem Alturjman	2018	IEEE Transactions on Industrial Informatics	10.1109/TII.2018.2808190	real-time computing;computer science;wireless sensor network;industrial internet;mutual authentication;computer network;hash function;wireless;provisioning	Mobile	-44.67861312454527	48.706320338545716	42633
f4390406f2b4fcb89a24ea541b6dfc9134721614	superstring: a scalable service discovery protocol for the wide-area pervasive environment	service discovery protocol;protocols;resource discovery;pervasive computing;information technology;distributed computing;web service;indexing terms;power supply;mobile phone;planets;batteries;bandwidth;service discovery;protocols costs pervasive computing sensor arrays bandwidth batteries distributed computing computational efficiency information technology planets;computational efficiency;peer to peer;sensor arrays;hand held computer	Arguably, the world has become one large pervasive computing environment. Our planet is growing a digital skin of a wide array of sensors, hand-held computers, mobile phones, laptops, web services and publicly accessible web-cams. Often, these devices and services are deployed in groups, forming small communities of interacting devices. Service discovery protocols allow processes executing on each device to discover services offered by other devices within the community. These communities can be linked together to form a wide-area pervasive environment, allowing processes in one group to interact with services in another. However, the costs of communication and the protocols by which this communication is mediated in the wide-area differ from those of intra-group, or local-area, communication. Communication is an expensive operation for small, battery powered devices, but it is less expensive for servers and workstations, which have a constant power supply and are connected to high bandwidth networks. This paper introduces Superstring, a peer-to-peer service discovery protocol optimised for use in the wide-area. Its goals are to minimise computation and memory overhead in the face of large numbers of resources. It achieves this memory and computation scalability by distributing the storage cost of service descriptions and the computation cost of queries over multiple resolvers.	computation;computer;interaction;laptop;mobile device;mobile phone;overhead (computing);peer-to-peer;pervasive informatics;power supply;scalability;sensor;server (computing);service discovery;ubiquitous computing;web service;workstation	Ricky Robinson;Jadwiga Indulska	2003		10.1109/ICON.2003.1266272	telecommunications;computer science;theoretical computer science;operating system;distributed computing;service discovery;information technology;world wide web;computer security;computer network	Mobile	-38.98677721865772	47.95023293658196	42671
526834c67126b2d21ba29c5b3083f18e0df9ce56	a penalty-based approach for qos dissatisfaction using fuzzy rules	qos;penalty;fuzzy logic;service level agreement	Quality of Service (QoS) guarantees are commonly defined in Service Level Agreements (SLAs) between provider and consumer of services. Such guarantees are often violated due to various reasons. QoS violation requires a service adaptation and penalties have to be associated when promises are not met. However, there is a lack of research in defining and assessing penalties according to the degree of violation. In this paper, we provide an approach based on fuzzy logic for modelling and measuring penalties with respect to the extent of QoS violation. Penalties are assigned by means of fuzzy rules.	quality of service	Barbara Pernici;Seyed Hossein Siadat;Salima Benbernou;Mourad Ouziri	2011		10.1007/978-3-642-25535-9_43	fuzzy logic;quality of service;computer science;data mining;computer security	Robotics	-47.82685222292806	43.23607824999728	42677
ceea02111423f9583c0a763bb92289e440e0404f	an approach for developing adaptive, mobile applications with separation of concerns	distributed application;mobile computing distributed computing personal digital assistants application software adaptive systems pervasive computing computer science collaboration logic collaborative work;separation of concern;mobile computer;adaptive behavior;distributed mobile application mobile computing;distributed mobile application;mobile computing;dynamic adaptation;mobile application;mobile user	Modern mobile computing paradigms have set new challenges for the development of distributed mobile applications and services. Because of the variability which characterizes the context of such environments, it is important that mobile applications are developed so that they can dynamically adapt their extra-functional behavior, in order to optimize the experience perceived by their users. This paper proposes an approach for developing adaptive, mobile applications. It is argued that this approach eases the development effort by clearly separating the work required for the development of the application logic from that required for enabling its adaptive behavior. It is argued that in addition to mitigating the development complexity, this approach also enables a new generation of distributed applications. The novelty in the latter is that the applications can dynamically and collaboratively adapt in an ad-hoc manner to improve the quality of the services offered to mobile users	adaptive behavior;business logic;distributed computing;hoc (programming language);mobile app;mobile computing;separation of concerns;spatial variability	Nearchos Paspallis;George Angelos Papadopoulos	2006	30th Annual International Computer Software and Applications Conference (COMPSAC'06)	10.1109/COMPSAC.2006.22	mobile search;simulation;mobile web;human–computer interaction;separation of concerns;computer science;operating system;adaptive behavior;mobile technology;mobile agent;distributed computing;mobile business development;mobile computing;autonomic computing	SE	-40.99761576087756	42.831942318499465	42708
7280366814c330a18aa9300989f7c55553fc1bde	burden of compliance and burden of violation		It this paper we address the issue of what it means to comply with or violate norms, and we propose a computationally oriented approach to reason about such notions.	defeasible logic;electronic paper;real life;smart contract	Guido Governatori	2015		10.3233/978-1-61499-609-5-31	computer science;data mining	AI	-49.14839604523445	52.42782588842817	42769
8c0802aa7d08a314be1019ec3d94e18848979ded	towards self-healing web services composition	system modeling;recovery;web service composition;statistical learning theory;aspect oriented programming;web services composition;runtime monitoring;ws bpel;diagnosis;self healing	To achieve self-healing web services composition, much work has been studied in the area of web services composition recently. However, most work addresses the problem of runtime monitoring, diagnosis and recovery in isolation. What is missing, however, is a unified solution that can be used to tackle this challenge in a principled manner. This paper presents a fresh view on self-healing web services composition. In particular, rather than building baseline system model a priori, we advocate using statistical learning theory(SLT) technique to extract it by observing the behavior of web services composition and locate the potential anomaly.	anomaly detection;baseline (configuration management);machine learning;web service	Guoquan Wu;Jun Wei;Tao Huang	2009		10.1145/1640206.1640221	web service;web modeling;simulation;business process execution language;web analytics;web standards;engineering;ws-policy;service-oriented architecture;social semantic web;ws-addressing;database;services computing;web intelligence;ws-i basic profile;world wide web	Web+IR	-46.73154743712761	41.174440359265326	42973
150db062026468de98bcf3bf99fd62aab515db20	an alternative approach to blockchain mining work for making blockchain technologies fit to ubiquitous and mobile computing environments		The mining work concentration problem, whereby machine power is monopolized when supporting its operation, is becoming a serious problem in virtual currency using the blockchain technology that has drawn significant attention in recent years. The paper presents a new solution to solve the problem by using a simple virtual currency service that allows a user to operate the service by giving the user a new incentive based on gamification, not traditional economic incentives. We conducted experiments that show the feasibility of adopting the alternative incentive.	bitcoin;experiment;gamification;informatics;mobile computing;virtual currency	Yuki Kano;Tatsuo Nakajima	2017	2017 Tenth International Conference on Mobile Computing and Ubiquitous Network (ICMU)	10.23919/ICMU.2017.8330097	virtual currency;incentive;mobile computing;distributed computing;blockchain	EDA	-41.48499029250601	51.71479390596932	42977
8660e6f34c40c2e3fc7f2b5eeb8188a84e5e379a	generic terminal support	front end	"""Modern computing trends attempt to raise applications programming to a level where the programmer need know very little, if anything, about the hardware being used. This trend has been quite successful in the area of I/O support. Applications programmers rarely need be concerned with the details of differences about the hardware they use. However, with the advent and growing popularity of """"smart"""" CRT terminals, programmers that desire to use the features of these smart terminals must learn the protocols that invoke the desired functions. This problem is compounded by the fact that [almost] every terminal vendor has a different protocol for each function. For example, to clear the screen on the IBM 3101 [1] the sequence ESC """"L"""" is used, while on the Lear Siegler ADM-3A [2] a Control/Z is used. The purpose of this paper is to propose a solution to this problem and describe a partial implementation of the ideas presented. The solution consists of three levels: (i) the level visible from application, (ii) the host system support level, and (iii) the front-end support level."""	adm-3a;cathode ray tube;input/output;programmer	Bruce Light Hillsberg	1981	Operating Systems Review	10.1145/1041459.1041460	real-time computing;simulation;computer science;front and back ends;operating system;computer security	HPC	-35.763937267770025	44.405646758958014	43014
ee98badae6e90c5fd44c40a7cbc3ef7235ea29a4	challenges in generating qos-constrained software implementations	production function;code generation;software systems;model based approach;satisfiability;modeling language;automatic generation;model driven development;uml profile;object management group;quality of service;software implementation;embedded software;generic programming;modeling and analysis;real time and embedded systems	The limited processing power and memory capacities of early computing technologies often forced designers of real-time and embedded software to devise relatively sophisticated and complex architectural and programming patterns. The primary motivation, of course, was to eke out as much capability as the underlying implementation technologies could provide. Unfortunately, this tended to add complexity to the software, often making it very difficult to evolve, maintain, or port to different (e.g., more advanced) technologies. One of the primary sources of complexity in such systems is the intricate interleaving of application-specific concerns with implementation concerns that prevents easy distinction of one from the other.  As applications get more sophisticated, these technology sensitive development methods are proving to be increasingly greater barriers to the higher levels of productivity, functionality, and reliability required of modern systems. In many cases, these traditional methods are no longer deemed appropriate.  So-called model-driven development (MDD) methods address this issue by exploiting two fundamental techniques: First, they allow more direct specification of application solutions by providing language constructs that very closely reflect application concepts while abstracting out implementation- and technology-specific concerns. This not only simplifies design but also reduces the likelihood of design errors. Second, MDD calls for the use of computer-based automation in the process of implementation. This involves, notably, techniques of automatically generating code from the high-level application design. In many cases, this process can be fully automated, analogous to the way that third-generation programming language compilers produced machine-executable code.  However, when it comes to software systems with stringent quality of service (QoS) constraints, such as encountered in realtime and embedded software, the automation process becomes significantly more challenging. This is not only because such constraints are more stringent and, therefore, are more difficult to satisfy, but also because the auto-generation of implementations is heavily dependent on the underlying platform on which the implementation is to execute. In such cases, it may be necessary to utilize every opportunity offered by the underlying platform technology. Therefore, the ideal implementation generator would evaluate the platform technology and generate code that makes optimal use of its capabilities. Clearly, this requires an accurate and relatively detailed representation of the target platform. It also requires a detailed and precise specification of the QoS requirements of the application (deadlines, throughput, reliability, security, etc.).  In the first part of this presentation, we describe a promising model-based approach to specifying both platforms and the QoS requirements of applications. It takes advantage of two modeling language standards issued by the Object Management Group (OMG): the general-purpose modeling language UML 2 and the recently adopted UML profile for Modeling and Analysis of Real-Time and Embedded Systems (MARTE). This combination provides the necessary conditions for creating powerful automatic implementation generators that first determine the feasibility of a given application-platform combination and, if the combination appears feasible, generate an optimal implementation that fully utilizes the platform capabilities.  The realization of such generators is, at present, an unsolved problem that goes beyond just the basic code generation issue and involves research in a number of diverse areas. In the second part of the presentation, we analyze the key elements of this problem and explore some potential research directions.	code generation (compiler);compiler;embedded software;embedded system;encrypted key exchange;executable;forward error correction;general-purpose modeling;high- and low-level;model-driven architecture;model-driven engineering;modeling and analysis of real time and embedded systems;primary source;profile (uml);quality of service;real-time transcription;requirement;software system;third-generation programming language;throughput;unified modeling language	Bran Selic	2007		10.1145/1289971.1289986	real-time computing;quality of service;embedded software;computer science;theoretical computer science;production function;modeling language;programming language;generic programming;code generation;software system;satisfiability	Embedded	-42.45816204358443	35.2752288546835	43154
2509867f48c5d590a1c4528a9496a722e1c26217	adding roles to corba objects	protocols;object oriented methods;irrigation;idls;application software;testing;interface definition languages;runtime;software components;computer architecture;software architecture;corba objects;software components corba objects idls protocol interoperability automated checking protocols component based software development;software reusability;application software computer architecture protocols software architecture programming proposals testing software reusability irrigation runtime;distributed object management;software component;object oriented methods distributed object management;interface definition language;component based software development;compatibility and substitutability of components;proposals;programming;automated checking;protocol interoperability	Traditional IDLs were defined for describing the services that objects offer, but not those services they require from other objects, nor the relative order in which they expect their methods to be called. Some of the existing proposals try to add protocol information to object interfaces, but most of them fail to do so in a modular way. In this paper we propose an extension of the CORBA IDL that uses a sugared subset of the polyadic π-calculus for describing object service protocols, based on the concept of roles. Roles allow the modular specification of the observable behavior of CORBA objects, reducing the complexity of the compatibility tests. Our main aim is the automated checking of protocol interoperability between CORBA objects in open component-based environments, using similar techniques to those used in software architecture description and analysis. In addition, our proposal permits the study of substitutability between CORBA objets, as well as the realization of dynamic compatibility tests during their run-time execution.	common object request broker architecture;component-based software engineering;interoperability;observable;software architecture description	Carlos Canal;Lidia Fuentes;Ernesto Pimentel;José M. Troya;Antonio Vallecillo	2003	IEEE Trans. Software Eng.	10.1109/TSE.2003.1183935	interoperable object reference;real-time computing;computer science;component-based software engineering;operating system;database;programming language	SE	-37.61069122735128	39.62292277902152	43175
572e14f818137e64a059bf818b44acda3cfd17c1	towards a software product line-based approach to adapt iaas cloud configurations	manuals;runtime;self adaptation;adaptation models;user interfaces;software product lines;cloud computing	Cloud computing is nowadays one of the most promising IT technologies, since it provides seemingly unlimited resources on demand at low costs. Hence, different types of applications have been migrated to IaaS environments, e.g. multi-tier (distributed) applications. However, in order to benefit from such characteristics, cloud configurations (i.e. virtual resource configurations) should be designed accordingly to the necessities of the applications. Furthermore, such configurations have to provide the required resources not only at the application deployment-time, but also during the whole application execution time. Hence, adaptive paradigms are required when designing solutions to cloud applications with dynamic resource requirements. Software Product Lines (SPLs) provide great flexibility and a high level of abstraction to describe complete system configurations. Even though SPLs are not commonly used to describe changes after an initial product (configuration) has been created, their inherent characteristics can enable producing the required virtual resource configuration to adapt applications after their initial deployment, i.e., at runtime. In this paper, we present an approach to create and adapt cloud configurations at the IaaS level by using SPLs. We focus on the architectural design of our solution as well as on the possible implementation challenges we could face.	adaptive grammar;cloud computing;high-level programming language;multitier architecture;requirement;run time (program lifecycle phase);software deployment;software product line	Carlos Ruiz;Hector A. Duran-Limon;Nikos Parlavantzas	2016	2016 IEEE/ACM 9th International Conference on Utility and Cloud Computing (UCC)	10.1145/2996890.3007893	real-time computing;simulation;cloud computing;computer science;operating system;user interface	HPC	-44.80876588582985	41.44559016467741	43280
6d20ec0e901860c660f2eed00458e2e5b7223fae	alphanumeric shellcode generator for arm architecture		Shellcode usually refers to a piece of code that is injected into a program in order to perform some malicious actions. For any processor, the set of instructions that consist only of alphanumeric characters is generally limited is size. Therefore it is non-trivial to construct shellcode that consists of only alphanumeric bytes. There exist a number of exploit tools that automatically translate non-alphanumeric shellcode into semantically equivalent alphanumeric shellcode for x86 architecture. To the best of our knowledge, there are no such tools available for ARM architecture. We report on our progress in developing a tool for automated generation of alphanumeric shellcode for ARM architecture.	arm architecture;alphanumeric shellcode	Pratik Kumar;Nagendra Chowdary;Anish Mathuria	2013		10.1007/978-3-642-41224-0_3	embedded system;computer hardware;operating system	Arch	-56.921536635890625	55.52471877981119	43303
d4473150b75bc77ac8924d0ee27494c0167944db	an effective and secure buyer-seller watermarking protocol	legislation;access control mechanism ontology driven xacml framework normative laws software system regulations enterprise privacy;authorisation;access control mechanism;330 wirtschaft;software systems;ddc 330;xml authorisation data privacy legislation ontologies artificial intelligence;ontologies artificial intelligence;ontologies data privacy law protection application software access control data security information security software systems computer architecture;access control policy;software system regulations;normative laws;data privacy;xml;software framework;access control;ontology driven xacml framework;enterprise privacy	Different buyer-seller watermarking protocols have been proposed to address preserving the digital rights of both the buyer and the seller. The previously published protocols have faced one or more of the following major common problems which are customer's rights, copy deterrence, unbinding, conspiracy, buyer's participation in the dispute resolution, protocol's practice applicability and man in the middle attack problems. In this paper an effective and secure buyer-seller watermarking protocol is proposed that encapsulates flexible and yet convenient solution to all of the previously mentioned problems. The security of the proposed protocol is based on the security of the public key infrastructure (PKI). The proposed protocol exploits the existence of the trusted certification authority (CA) to solve the conspiracy problem. Furthermore, the proposed protocol solves the unbinding problem based on a novel idea of generating buyer's dual signature of both the purchase order and the buyer's associated unique watermark.	certificate authority;digital watermarking;man-in-the-middle attack;public key infrastructure;public-key cryptography	Ibrahim M. Ibrahim;Sherif Hazem Nour El-Din;Abdel Fatah A. Hegazy	2007	Third International Symposium on Information Assurance and Security	10.1109/IAS.2007.39	privacy software;privacy by design;computer science;data mining;database;computer security	Security	-45.57689190048539	57.55106741828875	43313
ba3fe711523ac8c4f09403a9cd455738030a1e28	an integrated wsn and mobile robot system for agriculture and environment applications		Agriculture and environment issues are becoming increasingly important and are facing some new challenges. It is believed that wireless Sensor Networks (WSNs) and machine automation are among the key enabling technologies to address these challenging issues. Although extensive research has been conducted on individual technologies, their seamless integration to solve complex environmental problems has not been done before. This paper provides a design concept and some preliminary results for an integrated autonomous monitoring system. The integrated system will provide a powerful and cost-efficient tool for optimal, profitable, and sustainable management of environment and agriculture and thus bring significant social and economic benefits.	autonomous robot;cost efficiency;mobile robot;seamless3d	Hong Zhou;Haixia Qi;Thomas M. Banhazi;Tobias Low	2013		10.1007/978-3-319-11569-6_3	simulation	AI	-45.70854285189771	47.679681477366124	43325
bed91128af74fd2edba67f10ec579db2ee64c000	green communication with geolocation		Green communications is the practice of selecting energy efficient communications, networking technologies and products. This process is followed by minimizing resource use whenever possible in all branches of communications. In this day and age, green communication is vital to the footprint we leave on this planet as we move into a completely digital age. One such communication tool is Message Queue Transport Telemetry or MQTT which is an open source publisher/subscriber standard for M2M (Machine to Machine) communication. It is well known for its low energy and bandwidth footprint and thus makes it highly suitable for Green Internet of Things (IoT) messaging situations where power usage is at a premium or in mobile devices such as phones, embedded computers or microcontrollers. It is a perfect tool for the green communication age upon us and more specifically Green IoT. One problem however with the original MQTT protocol is that it is lacking the ability to broadcast geolocation. In today’s age of IoT however, it has become more pertinent to have geolocation as part of the protocol. In this paper, we add geolocation to the MQTT protocol and offer a revised version, which we call MQTTg. We describe the protocol here and show where we are able to embed geolocation successfully. We also offer a glimpse into an Android OS application we are developing for Open Source use.	android;computer;embedded system;geolocation;internet of things;java;m2m (eclipse);mqtt;machine to machine;message queue;microcontroller;mobile device;open-source software;operating system;publish–subscribe pattern;relevance;smart city	Gautam Srivastava;Andrew Fisher;Robert Bryce;Jorge Crichigno	2018	CoRR			Security	-39.94020192454227	48.65301307916734	43446
359f20d26d35c44563de25f3a76ca5a4b586e4c8	using graphviz as a low-cost option to facilitate the understanding of unix process system calls	program visualization;unix operating system;visual programming;operating system;graphical representation;software specification;open source;process system calls	Unix system calls to create and execute processes are usually hard to understand for novice students. Using graphics for visualizing the behaviour of these system calls can be useful both for the teacher to explain and for the students to understand them. The problem here is that there is no software specifically addressed to generate graphical representations of these kind of programs, and to develop it would be costly. Instead of developing a complete system to visualize programs that use Unix system calls, we turned to a ”cheaper” alternative solution. In this paper we show how we have used the open source graphviz tool to develop a simple way of generating graphical representations of the behaviour of these system calls, thus facilitating the comprehension of this important part in the learning of the Unix operating system.	graphical user interface;graphics;graphviz;open-source software;operating system;posix;process architecture;simulation;system call;unix;visualization software	Miguel Riesco;Marián D. Fondón;Darío Álvarez Gutiérrez	2009	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2008.12.052	embedded system;software requirements specification;real-time computing;computer science;operating system;visual programming language;programming language	Security	-54.16612444128109	36.70376785793716	43467
f216ff452e834a81d4964b6913176c8c8ca0f1b5	a privacy-level model of user-centric cyber-physical systems	qa75 electronic computers computer science	In an interconnected cyber-world, Cyber-Physical Systems (CPSs) appear to play an increasingly important role in smart ecosystems. A variety of resource-constrained thin clients, such as sensors, RFIDs, actuators and smart devices, are included in the list of CPS. These devices can be used in a number of medical, vehicular, aviation, military and smart cities applications. A plethora of sensitive data is transmitted in insecure wireless or wired environments whilst adversaries are eager to eavesdrop, modify or destroy sensed data invading the privacy of user-centric CPSs. This work presents an overview and analysis of the most e↵ective attacks, privacy challenges and mitigation techniques for preserving the privacy of users and their interconnected devices. In order to preserve privacy, a privacy-level model is proposed in which users have the capability of assigning di↵erent privacy levels based on the variety and severity of privacy challenges and devices’ capabilities. Finally, we evaluate the performance of specific CPSs at di↵erent privacy-levels in terms of time and consumed energy in an experimental test-bed that we have developed.	computation;cyber-physical system;ecosystem;operating system;privacy;sensor;smart city;smart device;testbed;thin client;while	Nikos Petroulakis;Ioannis G. Askoxylakis;Apostolos Traganitis;George Spanoudakis	2013		10.1007/978-3-642-39345-7_36	privacy software;telecommunications;engineering;internet privacy;computer security	Security	-46.99241125956158	60.087554940980844	43621
0adc9fb8abe298622682791ba55a00175c46985a	on the behavioral drift estimation of ubiquitous computing systems in partially known environments	service composition;hidden markov model;cyber physical systems;uncertainties;drift estimation;ubiquitous computing	Background. With the recent advent of the so-called connected objects, today largely present in our surroundings, software applications have an open door to the physical world through sensors and actuators. However, although it offers huge opportunities in many areas (e.g., smart-home, smart-cities, etc...), it poses a serious methodological challenge. Indeed, while classical software applications operate in the well known and delimited digital world, the so-called ambient applications operate in and through the physical world, open and subject to uncertainties that cannot be modeled accurately and entirely. These uncertainties lead the behavior of the ambient applications to potentially drift over time against requirements. In this paper, we propose a framework to estimate the behavioral drift of the ambient applications against requirements at runtime.  Methodology. We rely on the Moore Finite State Machines (FSM) modeling framework to specify the ideal behavior an ambient application is supposed to meet, irrespective of the operating environment and the underlying software infrastructure. We then appeal on the control theory and propose a framework to transform the Moore FSM to its associated Continuous Density Hidden Markov Model (CD-HMM) state observer. By accounting for uncertainties through probabilities, it extends Moore FSM with viability zones, i.e. zones where the behavioral requirements of the ambient applications are acceptable. The observation of the execution of a concrete ambient application together with the statistical modeling framework underlying its associated state observer allow to compute the likelihood of an observation sequence to have been produced by the application. The likelihood then gives direct insight into the behavioral drift of the concrete application against requirements.  Results. We validate our approach through a concrete use-case in the field of school lighting. The results demonstrate the soundness and efficiency of the proposed approach for estimating the behavioral drift of the ambient applications at runtime. In light of these results, one can envision using this estimation to support a decision-making algorithm (e.g., within a self-adaptive system).	as-interface;adaptive system;algorithm;control theory;delimiter;finite-state machine;hidden markov model;home automation;markov chain;operating environment;requirement;run time (program lifecycle phase);statistical model;ubiquitous computing;whole earth 'lectronic link	Gérald Rocher;Jean-Yves Tigli;Stéphane Lavirotte	2016		10.1145/2994374.2994398	embedded system;real-time computing;simulation;human–computer interaction;telecommunications;computer science;operating system;cyber-physical system;computer security;ubiquitous computing;hidden markov model	SE	-45.234105060609885	39.26052886339521	43646
7a530d92c0f60a0780137ba94095f80efbe193e3	python deserialization denial of services attacks and their mitigations		In recent years, many vulnerabilities in deserialization mechanisms are reported. Serialization is converting an object to a byte string, and deserialization is converting the byte string to the object. Pickle is a serialization/deserialization module in Python standard library. In the pickle module, specially-crafted data consumes huge memory in deserialization. There is a possibility that the memory consumption leads to deniable of services attacks. This paper precisely describes the DoS attacks and their mitigations.	python;serialization	Kousei Tanaka;Taiichi Saito	2018		10.1007/978-3-319-96806-3_2	serialization;operating system;byte;distributed computing;python (programming language);computer science;denial-of-service attack	Crypto	-58.003759188759524	59.03292857029284	43698
423c6d2d7b644e231e7f1758abe7c8461aa630c4	from wireless sensor networks to wireless body area networks: formal modeling and verification on security using pat		Model checking has successfully been applied on verification of security protocols, but the modeling process is always tedious and proficient knowledge of formal method is also needed although the final verification could be automatic depending on specific tools. At the same time, due to the appearance of novel kind of networks, such as wireless sensor networks (WSN) and wireless body area networks (WBAN), formal modeling and verification for these domain-specific systems are quite challenging. In this paper, a specific and novel formal modeling and verification method is proposed and implemented using an expandable tool called PAT to do WSN-specific security verification. At first, an abstract modeling data structure for CSP#, which is built in PAT, is developed to support the node mobility related specification for modeling location-based node activity. Then, the traditional Dolev-Yao model is redefined to facilitate modeling of location-specific attack behaviors on security mechanism. A throughout formal verification application on a location-based security protocol in WSN is described in detail to show the usability and effectiveness of the proposed methodology. Furthermore, also a novel location-based authentication security protocol in WBAN can be successfully modeled and verified directly using our method, which is, to the best of our knowledge, the first effort on employing model checking for automatic analysis of authentication protocol for WBAN.		Tieming Chen;Zhenbo Yu;Shijian Li;Bo Chen	2016	J. Sensors	10.1155/2016/8797568	embedded system;real-time computing;computer science;runtime verification;computer security;functional verification	Mobile	-51.15912805352188	47.870203996567824	43721
0b128476bab8964b674a126bb74411580704f146	implementing an adaptive higher level observer in trusted desktop grid to control norms	trust;multi agent systems;desktop grid system adaptive control loop multi agent systems trust norms;desktop grid system;observers open systems measurement security adaptive control control systems runtime;norms;adaptive control loop	Grid Computing Systems are examples for open systems with heterogeneous and potentially malicious entities. Such systems can be controlled by system-wide intelligent control mechanisms working on trust relationships between these entities. Trust relationships are based on ratings among individual entities and represent system-wide information. In this paper, we propose to utilise a normative approach for the system-level control loop working on basis of these trust values. Thereby, a normative approach does not interfere with the entities' autonomy and handles each system as black box. Implicit rules already existing in the system are turned into explicit norms - which in turn are becoming mandatory for all entities. This allows the distributed systems to derive the desired behaviour and cooperate in reaction to disturbed situations such as attacks.	algorithm;autonomy;black box;cluster analysis;control system;desktop computer;distributed computing;entity;grid computing;intelligent control;open system (computing);software agent	Jan Kantert;Hannes Scharf;Sarah Edenhofer;Sven Tomforde;Jörg Hähner;Christian Müller-Schloer	2014	2014 11th International Conference on Informatics in Control, Automation and Robotics (ICINCO)	10.5220/0005057902880295	simulation;computer science;artificial intelligence;multi-agent system;distributed computing;trustworthy computing;computer security;norm	Robotics	-48.43873142715063	43.91868266005596	43789
5a894ec0f6acb01da24b9ce698ce529670035770	model-based safety approach for early validation of integrated and modular avionics architectures		Increasing complexity of avionics systems leads to reconsider methods that are used today to analyze them from a safety point of view. This paper presents how the Model-based techniques can be used for safety assessment in early validation to support flexible and rapid prototyping of integrated systems (such as Integrated Modular Avionics and Cockpit Display), in order to evaluate and compare several envisaged architectures with their compliance to the safety objectives (under nominal and dispatch conditions).	dynamic dispatch;integrated modular avionics;rapid prototyping	Marion Morel	2014		10.1007/978-3-319-12214-4_5	reliability engineering;embedded system;integrated modular avionics;systems engineering	SE	-44.851783719456094	33.607810194424694	43821
60f42dbe7dbb3716d5a2f80fa37b71f8f1e13ecd	transforming code to drop dead privileges		To help programmers write programs that follow Saltzer and Schroeder's Principle of Least Privilege, modern operating systems divide the power of the administrative user into separate privileges which applications can enable on demand and permanently discard when no longer needed. However, using such privileges requires tedious temporal reasoning of program behavior. This paper describes a compiler, named AutoPriv, that helps programmers use privileges more easily. AutoPriv uses whole-program analysis during link-time optimization to determine where programs use privileges; it then transforms programs to remove unnecessary privileges during their execution. We tested AutoPriv on several privileged open-source programs that typically run as root. Our results show that AutoPriv increases optimization time by 19% on average but that transformed programs exhibit practically no overhead.	call graph;control flow;feedback;ibm notes;interprocedural optimization;mathematical optimization;modern operating systems;needham–schroeder protocol;open-source software;operating system;optimizing compiler;overhead (computing);principle of least privilege;privilege separation;program analysis;programmer;system call	Xiaoyu Hu;Jie Zhou;Spyridoula Gravani;John Criswell	2018	2018 IEEE Cybersecurity Development (SecDev)	10.1109/SecDev.2018.00014	program analysis;privilege escalation;compiler;real-time computing;principle of least privilege;computer science	Arch	-57.380472157785626	39.61448084222817	43978
77298d98bae5f70f2f6b52e7faa1f2cc1a5b130a	model-based testing of nasa's osal api — an experience report	coverage model based testing state machines;program testing application program interfaces finite state machines operating systems computers;finite state machines;bug detection model based testing osal api operating system ion layer hierarchical finite state machines fsm flight software missions nasa mbt architecture test cases;program testing;application program interfaces;testing documentation computer bugs file systems computer architecture abstracts;operating systems computers	We present a case study that evaluates the applicability and effectiveness of model-based testing in detecting bugs in real-world, mission-critical systems. NASA's Operating System ion Layer (OSAL) is the subject system of this paper. The OSAL is a reusable framework that wraps several operating systems (OS) and is used extensively in NASA's flight software missions. We developed a suite of behavioral models, represented as hierarchical finite state machines (FSMs), of the core file system API and generated a large number of test cases automatically. We then automatically executed these test cases against the OSAL. The results show that the OSAL is a high quality product. Naturally, due to the systematic and rigorous nature of MBT, we detected a few previously unknown “corner-case” bugs and issues, which escaped traditional manual testing and code reviews. We discuss the MBT architecture, the detected bugs, the code coverage of generated tests, as well as threats to validity of the study.	api testing;application programming interface;code coverage;core dump;corner case;display resolution;embedded software;file system api;finite-state machine;manual testing;mission critical;model-based testing;operating system abstraction layer;sensor;software bug;test case;threat (computer)	Christoph Schulze;Dharmalingam Ganesan;Mikael Lindvall;Dave Mcf Omas;Alan Cudmore	2013	2013 IEEE 24th International Symposium on Software Reliability Engineering (ISSRE)	10.1109/ISSRE.2013.6698883	reliability engineering;embedded system;real-time computing;simulation;computer science;engineering;operating system;software engineering;finite-state machine;programming language	SE	-57.16131554336866	36.73825443492307	43993
77d39b0eb8525cfb1e100581a44132124b182de3	bio-inspired enterprise security	scada;mobile agents;ant colony;cyber security;computer security;social network;supervisory control and data acquisition;bio inspired approaches;defense mechanism;scada infrastructure bio inspired enterprise security networking infrastructure diverse system current static oriented defense mechanism security paradigm social networking bioinformatics complex adaptive security computer system supervisory control data acquisition;complex adaptive;mobile agent;bioinformatics collaboration computer security conferences scada systems humans;scada systems;critical infrastructure;security policy;security of data scada systems;security of data;mobile agents computer security bio inspired approaches complex adaptive scada	Providing security for enterprises is difficult due to the size and complexity of their computing and networking infrastructures. These environments consist of a large number of diverse systems and services that continually change, thus they are difficult to defend using current static-oriented defense mechanisms. This paper introduces a new security paradigm that mimics designs in nature to ensure the safety and soundness of infrastructures that are potentially very diverse and dynamic. Primary inspiration has come from ant colonies, social networking, and bioinformatics. The proposed framework combines these ideas to provide complex-adaptive security for computer systems, telecommunications, and critical Supervisory Control and Data Acquisition (SCADA) infrastructures.	ant colony;bioinformatics;data acquisition;programming paradigm	Glenn A. Fink;Christopher S. Oehmen;Jereme Haack;A. David McKinnon;Errin W. Fulp;Michael B. Crouse	2011	2011 IEEE Fifth International Conference on Self-Adaptive and Self-Organizing Systems	10.1109/SASO.2011.33	computer science;distributed computing;computer security;computer network;scada	HPC	-57.44882635621669	51.271782095143166	43997
03eaed94e565a5b8c1f65883088944e65cc07ca6	the use of development history in software refactoring using a multi-objective evolutionary algorithm	refactoring;code change;semantics;search based software engineering;design defects	One of the widely used techniques for evolving software systems is refactoring, a maintenance activity that improves design structure while preserving the external behavior. Exploring past maintenance and development history can be an effective way of finding refactoring opportunities. Code elements which undergo changes in the past, at approximately the same time, bear a good probability for being semantically related. Moreover, these elements that experienced a huge number of refactoring in the past have a good chance for refactoring in the future. In addition, the development history can be used to propose new refactoring solutions in similar contexts. In this paper, we propose a multi-objective optimization-based approach to find the best sequence of refactorings that minimizes the number of bad-smells, and maximizes the use of development history and semantic coherence. To this end, we use the non-dominated sorting genetic algorithm (NSGA-II) to find the best trade-off between these three objectives. We report the results of our experiments using different large open source projects.	cache coherence;code refactoring;evolutionary algorithm;experiment;genetic algorithm;mathematical optimization;multi-objective optimization;open-source software;software system;sorting	Ali Ouni;Marouane Kessentini;Houari A. Sahraoui;Mohamed Salah Hamdi	2013		10.1145/2463372.2463554	search-based software engineering;computer science;continuous design;semantics;programming language;software maintenance;code refactoring;algorithm	SE	-58.212432788336244	34.45841102023853	44013
638297d9b5c8e0e83ca5acfbf1325196ea0bbb3c	isomeron: code randomization resilient to (just-in-time) return-oriented programming		Until recently, it was widely believed that code randomization (such as fine-grained ASLR) can effectively mitigate code reuse attacks. However, a recent attack strategy, dubbed just-in-time return oriented programming (JIT-ROP), circumvents code randomization by disclosing the (randomized) content of many memory pages at runtime. In order to remedy this situation, new and improved code randomization defenses have been proposed. The contribution of this paper is twofold: first, we conduct a security analysis of a recently proposed fine-grained ASLR scheme that aims at mitigating JIT-ROP based on hiding direct code references in branch instructions. In particular, we demonstrate its weaknesses by constructing a novel JIT-ROP attack that is solely based on exploiting code references residing on the stack and heap. Our attack stresses that designing code randomization schemes resilient to memory disclosure is highly challenging. Second, we present a new and hybrid defense approach, dubbed Isomeron, that combines code randomization with executionpath randomization to mitigate conventional ROP and JITROP attacks. Our reference implementation of Isomeron neither requires source code nor a static analysis phase. We evaluated its efficiency based on SPEC benchmarks and discuss its effectiveness against various kinds of code reuse attacks.	address space layout randomization;code reuse;compiler;instrumentation (computer programming);just-in-time compilation;mathematical optimization;memory management;microsoft windows;qr code;randomized algorithm;reference implementation;return-oriented programming;run time (program lifecycle phase);static program analysis	Lucas Davi;Christopher Liebchen;Ahmad-Reza Sadeghi;Kevin Z. Snow;Fabian Monrose	2015			computer security;address space layout randomization;real-time computing;computer science;return-oriented programming;source code	Security	-55.695231539416774	56.413223121360495	44031
a656cf971d7e484029f6619e607f0611050b1508	improving the testability of object oriented software through software contracts	software testing;fuzziness;object oriented software;fuzzy logic;effort estimation;contraction method;software development life cycle;cocomo;kloc	Software testing is one of the most expensive phases of the software development life cycle. Testing object oriented software is more expensive due to various features like abstraction, inheritance etc. The cost of testing can be reduced by improving the software testability. Software testability of a class is generally measured in terms of the testing effort which is equal to the number of test cases required to test a class. Hence testability can be improved if the test cases can be reduced. Software contracts (method preconditions, method postcondtions, and class invariant) can be used in improving the testability of the software. This paper demonstartes how software contracts can be used to reduce the number of test cases and hence improve the testability of an object oriented software. To accomplish this, software contracts are instrumented in a class and test cases are designed for this class using the path testing technique and then it is compared with the class without instrumenting the software contracts. We found that the instrumentation of software contracts reduces the number of test cases and hence improves the testability.	class invariant;design by contract;instrumentation (computer programming);precondition;software development process;software testability;software testing;test case	Yogesh Singh;Anju Saha	2010	ACM SIGSOFT Software Engineering Notes	10.1145/1668862.1668869	fuzzy logic;reliability engineering;verification and validation;regression testing;test data generation;real-time computing;software sizing;software verification;computer science;systems engineering;package development process;backporting;software reliability testing;component-based software engineering;software development;software design description;software engineering;cocomo;software construction;software testing;systems development life cycle;management;software regression;software quality;software metric;software quality analyst	SE	-60.71223715551301	33.26733897386576	44043
22329ac6aeae7027671b43322a6f391345f734a0	an approach of creative application evolution on cloud computing platform	creative computing;conference;software evolution;legacy system;creative application;cloud computing	Cloud computing is a paradigm that focuses on sharing data and computing resources over a scalable network of nodes, so it is becoming a preferred environment for those applications with large scalability, dynamic collaboration and elastic resource requirements. Creative computing is an emerging research field in these applications, which can be considered as the study of computer science and related technologies and how they are applied to support creativity, take part in creative processes, and solve creativity related problems. However, it is a very hard work to develop such applications from the very beginning under new environment, while it is a big waste for legacy systems under existing environment. Now software evolution plays an important role. In this paper, we introduced creative computing firstly, including definition, properties and requirements. Then the advantages of cloud computing platform for supporting creative computing were analysed. Next, a private cloud as experimental environment was built. Finally, the process of creative application evolution was illustrated. Our work is about research and application of software evolution methodology, also is an exploratory try to do creative computing research under cloud environment.	cloud computing;computer science;legacy system;programming paradigm;requirement;scalability;software evolution	Jiantao Zhou;Shang Zheng;Delin Jing;Hongji Yang	2011		10.1145/1982185.1982199	simulation;human–computer interaction;cloud computing;computer science;software evolution;operating system;software engineering;end-user computing;utility computing;legacy system	HPC	-46.18666958693838	45.033520250283736	44107
2d51252d2ceb8bd353eb6c40f0a6bc0ceac3b134	a naming system for feature-based service specification in distributed operating systems	name registration;key words:;underlying naming model;service feature;naming system;name server function;service specific function;data-driven communication paradigm;name resolution;high level name;name server;feature-based service specification	The paper describes a naming system that allows a service to evolve or reconfigure in functionality by adding and removing features and still co-exist with its previous versions. The underlying naming model has two aspects: (1) (Attribute_name, Attribute_value) pair based characterization of service features which allows the meta information on a service to be represented as a collection of such pairs (as in X.500 and Universal Naming Protocol). At low level, the name server provides parse and match operations on the (attribute, value) pairs using which high level name binding operations, viz., name registration and name resolution, are constructed. (2) Data-driven communication paradigm which enables different versions of a client and server to communicate with one another. In this paradigm, a server matches the attributes requested by a client with those it supports, and invokes service specific functions named by the attributes. Since attributes refer to orthogonal features, client and server can evolve independently by adding or removing attributes and still communicate. With this model of specifying services, name server functions may be factorized from service specific functions and implemented in a generic fashion in terms of parse and match operations and function invocations. The paper also describes language support for the naming system and implementation issues.		K. Ravindran;K. K. Ramakrishnan	1991		10.1145/111048.111059	real-time computing;systems engineering	OS	-35.7429596120528	42.403334037920686	44127
20618b485ffb830a1c88c99cb1e61e1625c4ceb7	corbaview: a visualisation tool to aid in the understanding of corba-based distributed applications		Middlewares provide the fundamental technology needed to create distributed applications, allowing developers to concentrate on their own specific needs. The developer may end up creating a very small part of the overall system themselves. They are able to make use of tried and tested middleware to aid in their application development. The tools provided by the CORBA middleware mean that much of the code used to create the distributed application is automatically generated for the developer. For a student learning about these middlewares, much of the development of the application that they are creating is done by pre-compiled, third party software. Understanding how the distributed application works can be difficult when you have only developed a very small part of the overall system. This paper describes CORBAview, a visualisation tool that gathers and displays the network communication and inner workings of the objects in a CORBA-based distributed application.	common object request broker architecture;compiler;distributed computing;middleware;psychology of programming;third-party software component	Declan Ryan;Christopher Exton	2004				HCI	-51.825260499521484	34.696050363169576	44136
96c32bfe453518f3e11837af001231d1dc4573a1	electronic commerce recommendation mobile crowd system based on cooperative data collection and embedded control	signal image and speech processing;circuits and systems;control structures and microprogramming;embedded control;electronic circuits and devices;cooperative data collection;mobile crowd;electronic commerce recommendation	It is known that the collection of the specific needs of mobile users and location management in an electronic commerce recommendation system are important indicators used to evaluate user satisfaction and system execution efficiency. In order to improve the low accuracy of recommendation systems and ensure real-time location management, we proposed cooperative embedded data collection and control of the electronic commerce recommendation system for mobile crowd system. First of all, we would assign every user with a mobile crowd terminal. The terminal could collect the user’s demand and personalized data. An embedded terminal could provide users with a real-time, accurate, personalized shopping experience. The terminal could collect the user demand dynamic evolution function. Then, the electronic commerce recommendation application platform would be formed through the deployment of a mobile crowd terminal. The platform collects and forwards the signal acquisition through a single port, dual port and port more adaptive network structure. Experimental results show that the performance of the proposed scheme is superior to the electronic commerce recommendation system based on user personalization drive, such as the data collection of the embedded crowd terminal delay, precision and accuracy of electronic commerce recommendation, as well as the number of iterations. The proposed scheme is more suitable for an electronic commerce recommendation system.	e-commerce;embedded system;iteration;personalization;real-time clock;real-time transcription;recommender system;software deployment	Juan Wang;Wen-Min Deng;Xing-Yue Yin	2016	EURASIP J. Emb. Sys.	10.1186/s13639-016-0024-z	embedded system;operating system;multimedia;internet privacy;world wide web	Mobile	-40.60786505939718	58.26142534684295	44141
4c9f9f228390d1370e0df91d2565a2559444431d	simrt: an automated framework to support regression testing for data races	kernels;processes;testing;concurrency;data races	Concurrent programs are prone to various classes of difficult-to-detect faults, of which data races are particularly prevalent. Prior work has attempted to increase the cost-effectiveness of approaches for testing for data races by employing race detection techniques, but to date, no work has considered cost-effective approaches for re-testing for races as programs evolve. In this paper we present SimRT, an automated regression testing framework for use in detecting races introduced by code modifications. SimRT employs a regression test selection technique, focused on sets of program elements related to race detection, to reduce the number of test cases that must be run on a changed program to detect races that occur due to code modifications, and it employs a test case prioritization technique to improve the rate at which such races are detected. Our empirical study of SimRT reveals that it is more efficient and effective for revealing races than other approaches, and that its constituent test selection and prioritization components each contribute to its performance.	race condition;regression testing;sensor;test case	Tingting Yu;Witawas Srisa-an;Gregg Rothermel	2014		10.1145/2568225.2568294	real-time computing;concurrency;computer science;software engineering;distributed computing;software testing;algorithm	SE	-61.4876026775294	36.406472942619814	44144
09c7c99a71d4440c75506450e54b7caae55aee8d	position paper: safety and security monitoring in ics/scada systems		Supervisory control and Data Acquisition (SCADA) systems play a core role in a nation’s critical infrastructure, overseeing the monitoring and control of systems in electricity, gas supply, logistics services, banks and hospitals. Monitoring safety and security properties in industrial control system (ICS) and SCADA environments faces unique challenges not found in typical enterprise networks. Novel monitoring solutions are desirable that take into account these differences. This paper presents a new approach for monitoring safety and security properties in industrial control systems. The approach is based on verifying a formal specification of ICS/SCADA components during runtime that is capable of detecting abnormal system behaviours. The solution is miniaturised and can be deployed at various points throughout the SCADA network, making masquerading and man-in-the-middle attacks more difficult to execute successfully.	computer security;control system;data acquisition;experiment;formal specification;http 404;interval temporal logic;logistics;man-in-the-middle attack;microcontroller;sensor;verification and validation;workstation;x86	Andrew Nicholson;Helge Janicke;Antonio Cau	2014			reliability engineering;embedded system;engineering;supervisory control;computer security;scada	Security	-57.079451870118014	52.09830920687466	44169
261fe811192d165e59b5a1b0d2b5d27693d71406	mesovirtualization: lightweight virtualization technique for embedded systems	virtual machine monitor;multiprocessor systems;embedded system;operating system;multiprocessor architecture;source code;virtual environment	These days, embedded and ubiquitous devices are becoming feature rich, and multiprocessor architectures for those devices are on the horizon. In order to utilize the resources of multiprocessor systems efficiently and securely, virtual machine monitors (VMMs) have been common among servers and desktop systems. The same can be applied if the cost of virtualization becomes much less expensive. In this paper, we introduce mesovirtualization, a new lightweight virtualization technique. Mesovirtualization makes VMMs smaller and requires only a few modifications for the guest operating system (OS) source code. We designed and implemented a VMM named Gandalf according to mesovirtualization. Our experimental results show that Linux on Gandalf performs better than XenLinux. Therefore, mesovirtualization makes virtualization environments suitable for embedded and ubiquitous devices.	central processing unit;desktop computer;embedded system;full virtualization;gandalf (theorem prover);linux;multiprocessing;operating system;virtual machine manager;x86 virtualization	Megumi Ito;Shuichi Oikawa	2007		10.1007/978-3-540-75664-4_52	embedded system;full virtualization;real-time computing;virtualization;computer science;virtual machine;operating system;hypervisor;source code	Embedded	-34.591752442895604	53.90262891187903	44179
1da2d0e2fea5f51497540b012008f0b13f7ff3fb	models for the consistent interaction of adaptations in self-adaptive systems	institutional repositories;fedora;vital;vtls;ils	Self-adaptive systems enable the run-time modification, or dynamic adaptation, of a software system in order to offer the most appropriate behavior of the system according to its context of execution and the situations of its surrounding environment. Depending on the situations currently at hand, multiple and varied adaptations may affect the original behavior of a software system simultaneously. This may lead to accidental behavioral inconsistencies if not all possible interactions with other adaptations were anticipated. The behavioral inconsistencies problem becomes even more acute if adaptations are unknown beforehand, for example, when new adaptations are incorporated to the system on the fly. Self-adaptive systems must therefore provide a means to arbitrate interactions between adaptions at run time, to ensure that there will be no inconsistencies in the system’s behavior as adaptations are dynamically composed into or withdrawn from the system. This chapter surveys existing approaches that allow software systems to manage behavioral inconsistencies that may appear due to the interaction of adaptations at run time. The approaches are classified into four categories: formal, architectural modeling, rule-based, and transition system approaches. Each of these approaches is evaluated with respect to the assurances they provide for the run-time consistency of the system, in the light of dynamic behavior adaptations.	adaptive system	Nicolás Cardozo;Kim Mens;Siobhán Clarke	2013		10.1007/978-3-319-74183-3_11	real-time computing;simulation;engineering;communication	Logic	-42.26469653241114	36.9039008902574	44209
fa88a0258a50dfd648d6c5e4f6ee7f67c6688ff0	model-based testing for asynchronous systems		Model-based testing is a prominent validation technique, integrating well with other formal approaches to verification, such as model checking. Automated test derivation and execution approaches often struggle with asynchrony in communication between the implementation under test (IUT) and tester, a phenomenon present in most networked systems. Earlier attacks on this problem came with different restrictions on the specification model side. This paper presents a new and effective approach to model-based testing under asynchrony. By waiving the need to guess the possible output state of the IUT, we reduce the computational effort of the test generation algorithm while preserving soundness and conceptual completeness of the testing procedures. In addition, no restrictions on the specification model need to be imposed. We define a suitable conformance relation and we report on empirical results obtained from an industrial case study from the domain of electric mobility.	algorithm;asynchronous circuit;asynchrony (computer programming);computation;conformance testing;energybus;model checking;model-based testing;pseudocode;test suite	Alexander Graf-Brill;Holger Hermanns	2017		10.1007/978-3-319-67113-0_5	real-time computing;completeness (statistics);asynchrony;model-based testing;model checking;soundness;asynchronous communication;computer science;synchronizer;phenomenon	SE	-36.299100964004914	33.1068652376344	44274
1d0186c4cd7088c32563a914067dd4ad71aa66c6	comprehensive survey of the iot open-source oss		The Internet of things (IoT) has attracted a great deal of research and industry attention recently and is envisaged to support diverse emerging domains including smart cities, health informatics, and smart sensory platforms. Operating system (OS) support for IoT plays a pivotal role in developing scalable and interoperable applications that are reliable and efficient. IoT is implemented by both high-end and low-end devices that require OSs. Recently, the authors have witnessed a diversity of OSs emerging into the IoT environment to facilitate IoT deployments and developments. In this study, they present a comprehensive overview of the common and existing open-source OSs for IoT. Each OS is described in detail based on a set of designing and developmental aspects that they established. These aspects include architecture and kernel, programming model, scheduling, memory management, networking protocols support, simulator support, security, power consumption, and support for multimedia. They present a taxonomy of the current IoT open-source OSs. The objective of this survey is to provide a well-structured guide to developers and researchers to determine the most appropriate OS for each specific IoT devices/applications based on their functional and non-functional requirements. They remark that this is the first such tutorial style paper on IoT OSs.		Mahmoud H. Qutqut;Aya Al-Sakran;Fadi Almasalha;Hossam S. Hassanein	2018	IET Wireless Sensor Systems	10.1049/iet-wss.2018.5033	systems engineering;memory management;non-functional requirement;scalability;programming paradigm;architecture;interoperability;functional requirement;health informatics;computer science	Embedded	-45.2387011957712	45.77240553606767	44306
796982f889d0111219ed749c605f631eab5ff29b	learning useful system call attributes for anomaly detection.	system modeling;anomaly detection	Traditional host-based anomaly detection systems model normal behavior of applications by analyzing system call sequences. Current sequence is then examined (using the model) for anomalous behavior, which could correspond to attacks. Though these techniques have been shown to be quite effective, a key element seems to be missing – the inclusion and utilization of the system call arguments. Recent research shows that sequence-based systems are prone to evasion. We propose an idea of learning different representations for system call arguments. Results indicate that this information can be effectively used for detecting more attacks with reasonable space and time overhead.	anomaly detection;evasion (network security);overhead (computing);sensor;system call	Gaurav Tandon;Philip K. Chan	2005			anomaly-based intrusion detection system;anomaly detection;systems modeling;computer science;machine learning	Security	-60.12165505911585	60.29507740747299	44314
1ebfb954ca4ecf168d40363c10cb6832b0975ccd	binary code extraction and interface identification for security applications	interfaces;cryptography;identification;coding;data processing security;extraction	Binary code reutilization is the process of automatically i dentifying the interface and extracting the instructions and data dependencies of a code fragment from a n executable program, so that it is selfcontained and can be reused by external code. Binary code reu tilization is useful for a number of security applications, including reusing the proprietary c ptographic or unpacking functions from a malware sample and for rewriting a network dialog. In this pa per we conduct the first systematic study of automated binary code reutilization and its security app lications. The main challenge in binary code reutilization is understa nding the code fragment’s interface. We propose a novel technique to identify the prototype of an und ocumented code fragment directly from the program’s binary, without access to source code or symbo l information. Further, we must also extract the code itself from the binary so that it is self-con tai ed and can be easily reused in another program. We design and implement a tool that uses a combinati on of dynamic and static analysis to automatically identify the prototype and extract the instr uc ions of an assembly function into a form that can be reused by other C code. The extracted function can be run independently of the rest of the program’s functionality and shared with other users. We apply our approach to scenarios that include extracting t he encryption and decryption routines from malware samples, and show that these routines can be reu s d by a network proxy to decrypt encrypted traffic on the network. This allows the network proxy to rewrite the malware’s encrypted traffic by combining the extracted encryption and decryption funct io s with the session keys and the protocol grammar. We also show that we can reuse a code fragment from an unp cking function for the unpacking routine for a different sample of the same family, even if the code fragment is not a complete function.	algorithm;binary code;binding corporate rules;botnet;carnegie mellon cylab;cryptography;data dependency;encryption;executable;malware;naruto shippuden: clash of ninja revolution 3;prototype;qr code;rewrite (programming);rewriting;session key;spamming;trojan horse (computing);undocumented feature;dialog	Juan Caballero;Noah M. Johnson;Stephen McCamant;Dawn Xiaodong Song	2010			code word;identification;dead code;extraction;code access security;computer science;cryptography;theoretical computer science;redundant code;coding;locally testable code;world wide web;computer security;code generation;unreachable code;source code	Security	-57.238777774208984	58.475206839465784	44410
2922ec988b605adee8a93d20dd20c949757380ab	designing cps/iot applications for smart buildings and cities	intelligent green house;smart home;intelligent transportation system;smart buildings;cyber physical systems;energy consumption reduction;smart cities;internet of things;cps iot design;power distribution grid	Internet of Things (IoT) and cyber-physical systems (CPS) technologies can be applied to many application domains. Examples include intelligent green house, intelligent transportation system, power distribution grid, smart home, smart building, and smart city. Among these application domains, some of them have been extensively studied, e.g., smart home and intelligent transportation systems. In the meantime, smart buildings and smart cities attract researchers and industries to investigate these two use scenarios. Well-designed IoT/CPS can reduce energy consumption, enhance safety in buildings and cities, or can increase the comfortability in the building. In the last few years, the research communities and industrial partners started to study and investigate these two use scenarios to develop prototype or commercial services for these two scenarios. Although many works have been conducted on these two scenarios, many challenges remain open. In this study, the authors study the development and challenges in five topics. They are middleware, computation model, fault tolerance, quality of data, and virtual run-time environment.		Chi-Sheng Shih;Jyun-Jhe Chou;Niels Reijers;Tei-Wei Kuo	2016	IET Cyper-Phys. Syst.: Theory & Appl.	10.1049/iet-cps.2016.0025	embedded system;architectural engineering;engineering;computer security;internet of things	Theory	-44.23991317719134	50.91102603182275	44495
748e3eb3b64d936baadb9437be9b0f4243b5a069	frameworks for a taxonomy of fault-tolerance attributes in computer systems	fault tolerant;conceptual framework;fault tolerant system;design methodology	A conceptual framework is presented that relates various aspects of fault-tolerance in the context of system structure and architecture. Such a framework is an essential first step for the construction of a taxonomy of fault-tolerance.  A design methodology for fault-tolerant systems is used as the means to identify and classify the major aspects of fault-tolerance: system pathology, fault detection and recovery algorithms, and methods of modeling and evaluation.  A computing system is described in terms of four universes of observation and interpretation, ordered in the following sequence: physical, logic, information, and interface, or user's. The description is used to present a classification of faults, i.e., the causes of undesired behavior of computing systems.	algorithm;fault detection and isolation;fault tolerance;hardware description language;taxonomy (general)	Algirdas Avizienis	1983		10.1145/800046.801633	fault tolerance;computer science;theoretical computer science	DB	-36.63576278239845	36.51869477694035	44566
4f160f814772b696f2038a3fad0a86b300d75fc3	an adaptive fault-tolerance agent running on situation-aware environment	adaptive fault tolerance;situation awareness;ubiquitous computing;middleware;education system;simulation model	The focus of situation-aware ubiquitous computing has increased lately. An example of situation-aware applications is a multimedia education system. Since ubiquitous applications need situation-aware middleware services and computing environment keeps changing as the applications change, it is challenging to detect errors and recover them in order to provide seamless ser- vices and avoid a single point of failure. This paper proposes an Adaptive Fault Tolerance Agent (AFTA) in situation-aware middleware framework and pre- sents its simulation model of AFT-based agents. The strong point of this system is to detect and recover error automatically in case that the session's process comes to an end through a software error.	fault tolerance	SoonGohn Kim;Eung Nam Ko	2008		10.1007/978-3-540-85930-7_40	embedded system;situation awareness;real-time computing;simulation;computer science;simulation modeling;middleware;ubiquitous computing	Robotics	-39.805991151191016	43.479733196382426	44579
e01bb37ffaf1cabcfc44468a22070259ac48e340	the distarnet approach to reliable autonomic long-term digital preservation	distributed system;timed systems;long period;data format;digital preservation;open system	The rapidly growing production of digital data, together with their increasing importance and essential demands for their longevity, urgently require systems that provide reliable long-term preservation of digital objects. Most importantly, these systems have to ensure guaranteed availability over a long period of time, as well as integrity and authenticity of the preserved data and their metadata. This means that all kinds of technical problems need to be reliably handled and that the evolution of data formats is supported. At the same time, systems need to scale with the volume of data to be archived. In this paper, we present DISTARNET, a fully distributed system that reliably executes pre-defined workflows for long-term preservation. Moreover, DISTARNET is designed as an open system that allows the curators of digital objects to specify new processes to cope with additional challenges.	archive;autonomic computing;case preservation;data model;digital data;distributed computing;fault tolerance;scalability;simulation;test case	Ivan Subotic;Heiko Schuldt;Lukas Rosenthaler	2011		10.1007/978-3-642-20152-3_8	real-time computing;computer science;database;open system;world wide web	Graphics	-45.48999398457885	40.62878283797922	44646
a6cafc6e581777809af5e22e07f69d0c78dda7d2	untrustworthiness: a trust-based security metric	information security;untrustworthiness benchmark trust based security metric low cost high reward trust based metric database configurations;information security power system security data security databases computer hacking current measurement charge measurement informatics proposals uncertainty;best practices;database systems;security metric;proposals;security of data;benchmark testing	Quantifying security is very hard and, although there are many proposals of security metrics in the literature, no consensual quantitative security metric has been proposed so far. A key difficulty is that security is, usually, more influenced by what is unknown about a system than by what is known about it. In this paper we present the idea of trust-based metrics, which are based on the idea of quantifying and exposing the trustworthiness relationship between a system and its owner. We defend that they represent a powerful alternative to traditional security metrics and are much easier to obtain. As an instantiation, we propose minimum untrustworthiness as a low-cost high-reward trust-based metric that can be easily used to assess and compare security aspects. We discuss what does it express, show how it can be computed and what are its advantages. Finally, we present preliminary work on the definition of an untrustworthiness benchmark for database configurations.	benchmark (computing);database security;distrust;information security;system administrator;trust (emotion);universal instantiation	Afonso Araújo Neto;Marco Vieira	2009	2009 Fourth International Conference on Risks and Security of Internet and Systems (CRiSIS 2009)	10.1109/CRISIS.2009.5411967	software security assurance;computer security model;cloud computing security;benchmark;security through obscurity;security information and event management;security engineering;security convergence;covert channel;computer science;information security;concrete security;data mining;security service;internet privacy;security testing;computer security;computational trust;best practice	EDA	-50.57160399973072	57.92420777890851	44667
b43d2cdbd56f67aafe1e58ab449290e1042f1cc4	textual and behavioral views of function changes	structure formation;behavior change;program slice encoding;program slicing;version control	In this paper, we describe an approach that automatically computes function change information between consecutive revisions along the revision history of C language projects. Function changes are computed at two abstract levels. First, we compute the textual changes between two function revisions. Computed results include function additions and deletions, and the quantity and the ratio of textual change in changed functions across two revisions. Second, we compute the behavioral changes of functions using program slicing techniques. We use an XML-formatted document to represent computed function change information. The function change information, together with the SCM change log, helps maintainers understand code changes between two revisions. The structured format of the function change information also helps create traceability links between the changes and other artifacts. We describe our prototype implementation for computing function changes, and we evaluate our approach through a case study on the Sed project.	algorithm;c standard library;cryptographic hash function;diff utility;function-level programming;principle of abstraction;program slicing;prototype;sed;subversion;traceability;xml	Kai Pan;E. James Whitehead;Guozheng Ge	2005		10.1145/1107656.1107659	computer science;theoretical computer science;data mining;database	SE	-53.94159855630543	35.71278985192679	44720
1d9f8c792c1cf91a9a66d168cd07e09d2297d602	symcrash: selective recording for reproducing crashes	capture and replay;program instrumentation;articulo;symbolic execution;error handling;symcrash selective recording for reproducing crashes;crash reproduction	"""Software often crashes despite tremendous effort on software quality assurance. Once developers receive a crash report, they need to reproduce the crash in order to understand the problem and locate the fault. However, limited information from crash reports often makes crash reproduction difficult. Many """"capture-and-replay"""" techniques have been proposed to automatically capture program execution data from the failing code, and help developers replay the crash scenarios based on the captured data. However, such techniques often suffer from heavy overhead and introduce privacy concerns. Recently, methods such as BugRedux were proposed to generate test input that leads to crash through symbolic execution. However, such methods have inherent limitations because they rely on conventional symbolic execution techniques. In this paper, we propose a dynamic symbolic execution method called SymCon, which addresses the limitation of conventional symbolic execution by selecting functions that are hard to be resolved by a constraint solver and using their concrete runtime values to replace the symbols. We then propose SymCrash, a selective recording approach that only instruments and monitors the hard-to-solve functions. SymCrash can generate test input for crashes through SymCon. We have applied our approach to successfully reproduce 13 failures of 6 real-world programs. Our results confirm that the proposed approach is suitable for reproducing crashes, in terms of effectiveness, overhead, and privacy. It also outperforms the related methods."""	failure;overhead (computing);privacy;software quality assurance;solver;symbolic execution	Yu Cao;Hongyu Zhang;Sun Ding	2014		10.1145/2642937.2642993	exception handling;real-time computing;simulation;computer science;crash;programming language;computer security	SE	-60.41139099122246	37.4915216100524	44728
b701037ee6892a8ef49992f2e4d1f3b68fe3d7d6	managing access to service providers in federated identity environments: a case study in a cloud storage service	federated identity environments;access control federated identity environments cloud computing;hierarchical authorization system access management service providers federated identity environments cloud storage service resource access control access control mechanism identity provider financial agreement;access control;information storage authorisation cloud computing;cloud computing servers authentication authorization computer numerical control context;cloud computing	Currently the diversity of services, which are adhering to Identity Federation, has raised new challenges in the area. Increasingly, service providers need to control the access to their resources by users from the federation as, even though the user is authenticated by the federation, its access to resources cannot be taken for granted. Each Service Provider (SP) of a federation implements their own access control mechanism. Moreover, SPs might need to allow different access control granularity. For instance, all users from a particular Identity Provider (IdP) may access the resources due to some financial agreement. On the other hand, it might be the case that only specific users, or groups of users, have access to the resources. This paper proposes a solution to this problem through a hierarchical authorization system. Our approach, which can be customized to different SPs, allows the SP administrator to manage which IdPs, or users, have access to the provided resources. In order to demonstrate the feasibility of our approach, we present a case study in the context of a cloud storage solution.	access control;authentication;authorization;cloud storage;federated identity;federation (information technology);identity provider	Thomas Diniz;Andre Castro de Felippe;Tainá Medeiros;Carlos Eduardo da Silva;Roberto Araújo	2015	2015 XXXIII Brazilian Symposium on Computer Networks and Distributed Systems	10.1109/SBRC.2015.32	computer access control;role-based access control;business;internet privacy;world wide web;computer security	Security	-47.21282115539525	55.0540066428763	44819
655b14ce1846256ca5c45f30c737e2ea800249c1	an adaptive fault-tolerance qos for whiteboard errors based on rcsm for ubiquitous computing	adaptive fault tolerance;ubiquitous computing	A QoS resource error detection-recovery model called “AFQ_WE” was proposed for situation-aware middleware as RCSM. AFQ_WE model was used to detect and recover the QoS resource errors among actions. An example of situation-aware applications is a multimedia education system. Education system for distributed multimedia holds the promise of greatly improving all forms of remote education and training. The model aims at guaranteeing it through application QoS. The AFQ_WE model is proposed for supporting QoS resource errors detection-recovery in the situation-aware middleware.	fault tolerance;quality of service;ubiquitous computing	Eung Ko;SoonGohn Kim	2007		10.1007/978-3-540-74282-1_64	embedded system;real-time computing;computer science;distributed computing	HCI	-39.90716301460178	43.447410837492775	44836
ab82ccffa35ba8b6419770b644018ae9b1899787	sequoll: a framework for model checking binaries	high assurance protected microkernel sequoll model checking binaries multicriticality real time systems protected mode operating systems bounded interrupt latencies guaranteed isolation wcet analysis trustworthy information loop bounds infeasible path annotations binary code loop counts malardalen wcet benchmarks;program control structures;formal verification;software verification and validation;model checking semantics manuals kernel real time systems computer architecture optimization;operating system kernels;software verification and validation real time systems operating system kernels;security of data formal verification operating system kernels program control structures real time systems;security of data;real time systems	Multi-criticality real-time systems require protected-mode operating systems with bounded interrupt latencies and guaranteed isolation between components. A tight WCET analysis of such systems requires trustworthy information about loop bounds and infeasible paths. We propose sequoll, a framework for employing model checking of binary code to determine loop counts and infeasible paths, as well as validating manual infeasible path annotations which are often error-prone. We show that sequoll automatically determines many of the loop counts in the Malardalen WCET benchmarks. We also show that sequoll computes loop bounds and validates several infeasible path annotations used to reduce the computed WCET bound of seL4, a high-assurance protected microkernel for multi-criticality systems.	arm architecture;alias analysis;aliasing;automated theorem proving;benchmark (computing);binary code;cognitive dimensions of notations;computation;correctness (computer science);criticality matrix;domain-specific language;human error;interrupt;l4 microkernel family;mixed criticality;model checking;operating system;preemption (computing);protected mode;real-time clock;real-time computing;self-organized criticality;static timing analysis;the australian;time complexity;trust (emotion);verification and validation;worst-case execution time	Bernard Blackham;Gernot Heiser	2013	2013 IEEE 19th Real-Time and Embedded Technology and Applications Symposium (RTAS)	10.1109/RTAS.2013.6531083	embedded system;verification and validation;parallel computing;real-time computing;formal verification;computer science;operating system;programming language	Embedded	-55.36506404548528	53.744880598894696	44947
fc9d2c83d2909755004b5f25d220f8853fb37c34	a middleware architecture for enhancing web services infrastructure for distributed coordination of workflows	web service coordination management middleware;distributed coordination;web service coordination management middleware web services middleware;collaborative work;wscmm;web services infrastructure;logic;tellurium;wscmm middleware architecture web services infrastructure workflow software architecture proof of concept prototype web service coordination management middleware;usa councils;web service;proof of concept;computer architecture;software architecture;proof of concept prototype;middleware architecture;workflow software architecture;web services;workflow management software;workflow management software middleware software architecture web services;middleware;service oriented architecture;energy management	This paper discusses a layered workflow software architecture for distributed coordination of workflows over Web services and proposes fundamental enhancements to the web services infrastructure that facilitate the layered workflow software architecture. We verify our layered approach using a detailed simulation and a proof-of-concept prototype implementation. Our proposed architecture for Web service coordination management middleware (WSCMM) is a simple but powerful enhancement to the web service infrastructure enabling the services to locally manage their dependencies and to handle messages resulting from multiple workflows. We have carried out a detailed simulation to identify key components of our middleware architecture. We also compare and contrast our architecture with the current web service technologies. Our experiments demonstrate that we can develop both centralized and distributed workflows over the architecturally enhanced web services with relative simplicity. In addition, our lightweight coordination components with footprint no larger than 150 KB allow these workflows to be executed even on a handheld.	centralized computing;defense in depth (computing);experiment;handheld game console;middleware;prototype;simulation;software architecture;web service	Janaka Balasooriya;Sushil K. Prasad;Shamkant B. Navathe	2008	2008 IEEE International Conference on Services Computing	10.1109/SCC.2008.127	web service;middleware;computer science;ws-policy;database;distributed computing;world wide web	HPC	-35.97183343414642	47.1527613513372	45037
b8765fd63abe756ccd40936779834a71a7e5b2fd	edge computing enabling the internet of things	cloud computing mobile communication mobile computing computer architecture bandwidth mobile handsets;iot;nfv;nfv mec sdn iot fog computing cloud computing;mec;mobile computing cloud computing internet of things;sdn;fog computing;sdn mobile edge computing internet of things it worlds telecom worlds telecom market virtualization revolution cloud computing iaas infrastructure as a service cloud capabilities mobile phones usage mobile applications world of things wearables home automation systems sensors rfid tags iot architecture iot applications e healthcare real time monitoring mec architecture vehicular ad hoc networks software defined network;cloud computing	Mobile Edge Computing (MEC), a new concept that emerged about a year ago, integrating the IT and the Telecom worlds will have a great impact on the openness of the Telecom market. Furthermore, the virtualization revolution that has enabled the Cloud computing success will benefit the Telecom domain, which in turn will be able to support the IaaS (Infrastructure as a Service). The main objective of MEC solution is the export of some Cloud capabilities to the user's proximity decreasing the latency, augmenting the available bandwidth and decreasing the load on the core network. On the other hand, the Internet of Things (IoT), the Internet of the future, has benefited from the proliferation in the mobile phones' usage. Many mobile applications have been developed to connect a world of things (wearables, home automation systems, sensors, RFID tags etc.) to the Internet. Even if it is not a complete solution for a scalable IoT architecture but the time sensitive IoT applications (e-healthcare, real time monitoring, etc.) will profit from the MEC architecture. Furthermore, IoT can extend this paradigm to other areas (e.g. Vehicular Ad-hoc NETworks) with the use of Software Defined Network (SDN) orchestration to cope with the challenges hindering the IoT real deployment, as we will illustrate in this paper.	cloud computing;edge computing;fog computing;hoc (programming language);home automation;internet of things;many-worlds interpretation;mobile app;mobile phone;network function virtualization;openness;programming paradigm;radio-frequency identification;scalability;sensor;serial digital video out;software deployment;software-defined networking;wearable computer	Ola Salman;Imad H. Elhajj;Ayman I. Kayssi;Ali Chehab	2015	2015 IEEE 2nd World Forum on Internet of Things (WF-IoT)	10.1109/WF-IoT.2015.7389122	embedded system;network functions virtualization;human–computer interaction;cloud computing;computer science;operating system;software-defined networking;mobile computing;computer security;internet of things;computer network	Mobile	-40.93818698519255	53.004387555844275	45063
a33cb64bc1da1b3540e1feb8ba56e83ecef4fe0d	a process toward total dependability - airbus fly-by-wire paradigm	manufacturing defect;developpement logiciel;systeme avec perte;sistema mecanico;seguridad funcionamiento;fiabilidad;reliability;surete fonctionnement;securite;redundancia;availability;surveillance;disponibilidad;mando numerico;safety systems;actionneur;flight;aeronef;control electronico;systeme mecanique;commande numerique;regulacion de la posicion;aeronave;actuator;vol;commande position;vigilancia;redundancy;monitoring;position control;electronic control;desarrollo logicial;loss system;fiabilite;dependability;mechanical system;airplane;software development;safety;defaut fabrication;avion;systeme securite;accionador;monitorage;digital control;monitoreo;seguridad;vuelo;disponibilite;flight control;system safety;redondance;aircraft;defecto fabricacion;commande electronique;sistema con perdida	The presentation deals with digital electrical flight control system of the Airbus airplanes. This system is built to very stringent dependability requirements both in term of safety (the system must not output erroneous signals) and availability. System safety and availability principles along with assessment process are presented with an emphasis on their evolution and on future challenges. The purpose of a flight control system is to link the pilot to the control surfaces of the airplane. A fly-by-wire system replaces mechanical transmission of signal to the actuators by a set of computers and electrical wires. Main impairments to such a system are erroneous positioning of control surfaces (safety) and loss of control (system availability). With respect to physical faults, safety is ensured basically by the use of command and monitoring computers, such that in case of failure, the outputs are forced in a safe state. Redundancy provides the needed availability. With respect to design and manufacturing errors, error avoidance and removal are applied with a stringent development process. Error tolerance is used as well. Particular risks are a concern in the sense that they can be single events that could affect several redundancies. Segregation between redundant elements when they are installed is a key precaution. Airbus flight control system offers piloting aids such as flight envelope protections, some of them are available on non fly-by-wire aircraft while others are specific, along with maintainability helping devices. Safety assessment process allows for both qualitative and quantitative assessment, proceedings from aircraft top-level events.	dependability;fly-by-wire;paradigm	Pascal Traverse;Isabelle Lacaze;Jean Souyris	2005		10.1007/11408901_1	reliability engineering;embedded system;simulation;engineering;system safety	Theory	-35.45249192043787	36.4856119355591	45096
6fda472c1040e9b163f2958eb0a6e9b5ddc9f1d5	talk on activities of security and grid computing service project	project management;multi agent system;authorisation;trust model;project management grid computing multi agent systems message authentication authorisation graphical user interfaces;grid computing multi agent systems message authentication authorisation graphical user interfaces project management;multi agent systems;graphical user interfaces;grid computing service project grid environment software technology multiagent system authorization based trust model graphic based authentication interface;message authentication;grid computing authentication authorization graphics cellular phones multiagent systems skin artificial intelligence environmental management service robots;user authentication;grid computing;grid environment software technology multiagent system authorization based trust model graphic based authentication interface grid computing service project	Summary form only given. With the rapid expansion of the GRID environment and the maturing of software technology such as multiagent system, systems that provide commercial services are faced with conventional commercial concerns, namely how to build trust between service owners and users. Authentication and authorization must be enforced on the use and provision of services. Based on our experiences, we introduce an authorization-based trust model (ABTM) (W. Wen et al., 2000) for such an environment, and then introduce a graphic-based authentication interface.	agent-based model;authentication;authorization;grid computing;multi-agent system	Fumio Mizoguchi	2004	13th IEEE International Workshops on Enabling Technologies: Infrastructure for Collaborative Enterprises	10.1109/ENABL.2004.63	message authentication code;computer science;artificial intelligence;multi-agent system;graphical user interface;authorization;internet privacy;world wide web;computer security;grid computing	SE	-45.256954769633495	54.70631375872246	45179
321351a26f97e358e3b9f060e698352d6c654ebc	toward a model-driven access-control enforcement mechanism for pervasive systems	model driven security;rbac;models runtime;access control enforcement mechanism;access control reasoner	Pervasive systems typically involve heterogeneous users, devices and networks to provide services seamlessly interacting with the physical world. In order to be flexible, these systems must be both dynamically adaptive to handle and still open to the ability of receiving new elements. Characteristics of these systems can have a major impact on the enforcement of role-based access control policies. Enforcement mechanism for RBAC policies need to be tailored to distributed and adaptive software architectures. It must be capable of handling architectural changes (e.g., a resource hosted by a node is moved to another node) in order to maintain the enforced policy. In this paper we describe an approach of policy enforcement that leverages on a mapping between RBAC and a component-based architecture to reason on architectural changes and maintain the enforced policy. Models@runtime paradigm provides elementary bricks to reason on adaptive architecture. Relying on it and on runtime adaptation and monitoring mechanisms we propose a design for a model-driven RBAC enforcement mechanism.	adaptive architecture;component-based software engineering;interaction;model-driven integration;programming paradigm;role-based access control;software architecture;ubiquitous computing	Ben David Olivier-Nathanaël;Benoit Baudry	2012		10.1145/2422498.2422504	real-time computing;engineering;distributed computing;computer security	Arch	-40.136958006050556	41.282040012189306	45212
5fdc818fa00e5f4c3a84ac282ac49d672f797c9a	sqlpil: sql injection prevention by input labeling	web applications;flow tracking;program analysis;data labeling tainting;sql injection attacks;prepared parameterized statements	SQL injection attacks SQLIAs aim at exploiting vulnerabilities in web applications in order to execute malicious SQL commands. It is established that prepared statements are resilient to SQLIAs, and thus, developers are advised to use them when constructing SQL queries as opposed to applying string concatenation operations. Unfortunately, this recommended programming practice is not as pervasive as it should be. This paper addresses this shortcoming by presenting SQL injection Prevention by Input LabelingSQLPIL, an effective, light, and fully automated tool that leverages prepared statements to prevent SQLIAs at runtime. Given a Java program in which SQL queries are built as strings, SQLPIL dynamically transforms the strings into secure prepared statements right before their execution, thus guaranteeing that malicious input will always be treated as data and never as SQL commands. We empirically evaluated our Java implementation of SQLPIL using a benchmark that includes five JSP commercial applications, a number of legitimate queries, and a number of attacks of representative types. The results were promising as all attacks were prevented, and all legitimate runs executed successfully; in other words, the technique exhibited no false alarms when applied on typical applications. Also, the runtime cost was acceptable, assuming typical settings. Copyright © 2015 John Wiley & Sons, Ltd.	sql injection	Wes Masri;Sam Sleiman	2015	Security and Communication Networks	10.1002/sec.1199	program analysis;data transformation services;web application;sql injection;stored procedure;computer science;query by example;user-defined function;autocommit;database;sql/psm;world wide web;computer security;null	Crypto	-56.72184342866752	57.86975880735508	45237
8573187d4c8ed4d638ec043569fb5a08a715264f	real-time adaptive resource management	resource management mission critical systems quality of service application software periodic structures pipelines computer industry hardware middleware costs;resource allocation;client server systems;software development costs real time adaptive resource management distributed mission critical environments quality of service distributed system commercial off the shelf middleware;software reusability;distributed object management;quality of service;client server systems real time systems resource allocation quality of service software reusability distributed object management;real time systems	"""D istributed mission-critical environments employ a mixture of hard and soft real-time applications that usually expect a guaranteed range of quality of service. These applications have different levels of criticality and varied structures ranging from periodic independent tasks to distributed pipelines or event-driven modules. The underlying distributed system must evolve and adapt to the high variability in resource demands that competing applications impose. The current industry trend is to use commercial off-the-shelf (COTS) hardware and software components to build distributed environments for mission-critical applications. Adding a middleware layer above the COTS components facilitates consistent management of system resources, decreases system complexity, and reduces development costs. The real-time adaptive resource management system consists of a middleware layer that provides integrated services for real-time mission-critical distributed applications. RTARM includes a number of features that facilitate distributed resource management: • scalable end-to-end criticality-based QoS contract negotiation, which allows distributed applications to share common resources while maximizing their use and execution quality; • end-to-end QoS adaptation that dynamically adjusts resource use according to availability; • integrated services for CPU, network , and other resources with end-to-end QoS guarantees; • real-time application QoS monitoring for integrated services, and • plug-and-play components for easy extensibility for new services. As Figure 1 shows, RTARM's hierarchical architecture includes service managers , recursive structural entities that encapsulate a set of resources and their management mechanism. A higher-level service manager may receive a request for an integrated service that requires resources from lower-level service managers. Lower-level service managers directly control resources such as the CPU, network, or memory. The entire hierarchy treats resources and negotiation requests uniformly. This hierarchy allows dynamic configuration, since new service managers can join the system at any time. The admission protocol builds a virtual spanning tree over the service manager hierarchy that remains valid throughout the application's lifetime. The service manager hierarchy forms a directed acyclic graph, with service managers as nodes and edges represented by the """" uses-services-from """" relation. The virtual spanning tree built over the service manager hierarchy allows the service managers to conduct QoS translation and reverse translation. A hierarchical, recursive resource management architecture facilitates implementing QoS representations on top of basic services in complex distributed applications. A richer QoS representation simplifies application design and facilitates consistent resource management for incompatible applications. Regardless of the complexity of the application architecture and the QoS semantics at the top of the service manager …"""	applications architecture;central processing unit;component-based software engineering;criticality matrix;directed acyclic graph;distributed computing;end-to-end encryption;end-to-end principle;entity;event-driven architecture;event-driven programming;extensibility;file spanning;heart rate variability;integrated services;job scheduler;middleware;mission critical;operating system service management;pipeline (computing);plug and play;quality of service;real-time clock;real-time computing;real-time transcription;recursion;scalability;self-organized criticality;spanning tree	Allalaghatta Pavan;Rakesh Jha;Lee Graba;Saul Cooper;Ionut Cardei;Vipin Gopal;Sanjay Parthasarathy;Saad Bedros	2001	IEEE Computer	10.1109/2.933517	distributed algorithm;middleware;real-time computing;quality of service;resource allocation;computer science;operating system;middleware;software as a service;distributed computing;distributed design patterns;management;human resource management system	OS	-36.693240748939026	42.58862830080225	45246
a1e34c041a29ee4288792066eaccc17a98761433	automating periodic role-checks - a tool-based approach	use case;identity management;it security	The use of roles in Identity Management has proven to be a solution for reorganising and securing the access structures of organisations. One critical challenge companies face after they implemented roles is the maintenance of the role system itself. This includes sophisticated duties like periodically verifying the valid roles. We argue that due to the high complexity, periodic rolechecks need to be automated. However, as a result of lacking theoretical foundation, no approaches to leverage the level automation have been published so far. In this work we develop a catalogue of use cases that affect the role definitions within an organisation. We propose checkROLE, a tool for automated role-checking on basis of the defined use case catalogue.	algorithm;assistive technology;data mining;identity management;scenario testing;seamless3d;sensor;simpletext;text-based (computing);verification and validation	Ludwig Fuchs;Christian Müller	2009				AI	-49.3988070652032	55.88097663924207	45278
6fa3b5c2438dd10845f8ed7a7a9b66292aac6468	source code annotated memory leak detection for soft real time embedded systems with resource constraints	soft real time system;software;memory leak;memory management;hybrid leak detection;resource management;source code annotation;leak detection;virtual memory model;leak detection software memory management resource management real time systems pattern matching;hybrid leak detection memory leak soft real time system virtual memory model source code annotation;pattern matching;software development source code annotated memory leak detection resource constraint delivery time phase detection hybrid automated source code annotated memory leak detection approach soft real time embedded system software garbage collection source code annotation tool memory leak candidate hybrid leak detection approach control flow graph source code generation static phase dynamic phase filters false positives execution environment executable software simulation environment virtual platform target architecture virtual memory model platform systemc abstract level transaction level modeling software under test architecture specific feature;virtual storage c language program testing software architecture source code software transaction processing;real time systems	With ever increasing complexity of software and increased competition in the market, it is very important to minimize the production to market delivery time. Early phase detection of serious errors in the design and program can be done with automated approaches which save time. Memory leaks are the most important memory related problems which are severe threats to software applications. We propose a novel hybrid automated source code annotated memory leak detection approach for soft real time embedded system software with no garbage collection. We utilize our source code annotation tool to detect the potential memory leak candidates, which are further analyzed in our hybrid leak detection approach. The control flow graph of the source code generated with our annotation tool provides information of all execution paths, where the basic blocks are annotated with specific marks. Our hybrid method combines static and dynamic approaches. Static phase of the hybrid approach detect the potential leak candidates from the control flow graph. Dynamic phase filters false positives from the static phase results and provide results on the actual leaks. In early design stages where a real execution environment and complete executable software are unavailable, a simulation environment and a virtual platform are necessary. Virtual platforms provide flexibility to change the target architecture to be tested. Our proposed dynamic phase of the approach runs on the virtual memory model platform we implemented in SystemC at an abstract level using Transaction Level Modeling. The software under test is run on top of this model. This makes our approach faster and provides early results. Our approach can be easily deployed across a variety of architectures as it is compiler independent and does not implement any architecture specific features. Moreover the automated approach provides an efficient methodology to determine potential leaks in early phases of software development.	algorithm;assembly language;basic block;code coverage;compiler;control flow graph;data dependency;data-flow analysis;dataflow architecture;dominator (graph theory);embedded system;executable;fragmentation (computing);garbage collection (computer science);memory corruption;memory leak;overhead (computing);pattern matching;simulation;software development;source-code annotation;systemc;transaction-level modeling;virtual machine;virtual reality	Mabel M. Joy;Wolfgang Müller;Franz-Josef Rammig	2014	2014 IEEE 12th International Conference on Dependable, Autonomic and Secure Computing	10.1109/DASC.2014.38	embedded system;real-time computing;computer science;operating system;overlay;memory leak;static program analysis	SE	-56.56189612462056	42.22246337201684	45353
278087a2c3219882364505fd149bd208cd07ca59	serecon: a trusted environment for sopc design	field programmable gate array;ip protection;systems blur;field programmable gate array reconfigurable computing trusted computing security run time reconfiguration ip protection trust management;partial reconfiguration;reconfigurable computing;reconfigurable architectures;bezpieczenstwo;trust management;trusted environment;run time reconfiguration;fpga;serecon;intellectual property protection;time management;trusted computing;run time reconfigurable;systemy rekonfigurowalne;digital communication;embedded system design;system on chip;security and privacy;system on chip field programmable gate arrays reconfigurable architectures security of data;trusted computing base;ochrona wartości intelektualnych;field programmable gate arrays;security;security of data;serecon trusted environment reconfigurable computing systems blur embedded system design fpga field programmable gate arrays	Problems of fraud, theft, impersonation and counterfeiting have migrated into computing and digital communication technology. Reconfigurable computing (RC) (e.g., FPGA) systems blur the boundary between hardware and software. As reconfigurable computing systems become more popular, concerns arise about their security and privacy. Run-time partial reconfiguration provides the flexibility of hardware, but at the same time may compromise security and integrity of the embedded system design. This paper discusses potential threats to such systems and describes SeReCon, a secure reconfiguration controller, as a countermeasure. SeReCon supports intellectual property protection within the FPGA and provides secure run-time management of designs within FPGA. The fundamentals of the SeReCon trusted computing base are described. Various IP Block processing scenarios are proposed. Early implementation results are reported.	embedded system;field-programmable gate array;ip address blocking;privacy;reconfigurable computing;systems design;trusted computing base	Krzysztof Kepa;Fearghal Morgan;Krzysztof Kosciuszkiewicz;Tomasz R. Surmacz	2008	2008 Third International Conference on Dependability of Computer Systems DepCoS-RELCOMEX	10.1109/DepCoS-RELCOMEX.2008.56	embedded system;real-time computing;reconfigurable computing;engineering;computer security	EDA	-53.157362286591145	55.88124556658939	45400
380486007668fd15a118f84d9cbb9e50de77d51c	fltl-mc: online high level program analysis for web services	online debugging liveness and saftey web services;web services finite element analysis high level languages temporal logic;high level languages;distributed protocol;ws reliable messaging protocol;online debugging;fltl mc tool;temporal logic;hidden bug detection distributed web services fltl mc tool high level program analysis finite linear temporal logic online model checking binary instrumentation ws reliable messaging protocol;construction industry;web service;satisfiability;data mining;engines;model checking;linear temporal logic;liveness and saftey;online model checking;safety;web services;finite element analysis;program analysis;binary instrumentation;simple object access protocol;finite linear temporal logic;distributed web services;computer bugs;high level program analysis;web services simple object access protocol safety computer bugs logic java computer science laboratories instruments engines;hidden bug detection	Although Java- or .NET- centric technologies are the most commonly used in Web services, they are by no means the only ones in practice. This paper proposes an online finite model checking tool FLTL-MC to check the high level safety and liveness properties in complex distributed web service systems, which can offer both a richer and more natural way to search errors. Liveness properties can specify desirable system behaviors which must be satisfied eventually, but are not always satisfied. Existing software model checkers cannot verify liveness in real code because doing so requires finding an infinite execution that does not satisfy aliveness property. In our proposed model, we adopt the finite linear temporal logic to specify the semantics of the online model checking, use binary instrumentation to obtain the distribute states and apply the FLTL-MC engine to dynamically verify the finite linear temporal logic properties in Web service systems. At last, we investigate the well-known distributed protocol WS-Reliable Messaging to demonstrate its applications and detect some hidden bugs with our prototype system.	c++;distributed computing;forward error correction;heuristic;high-level programming language;interaction;linear temporal logic;liveness;model checking;program analysis;prototype;reliable messaging;software bug;ws-reliablemessaging;ws-security;web service;whole earth 'lectronic link	Zhengwei Qi;Liang Liu;Fuyuan Zhang;Haibing Guan;Hao Wang;Ying Chen	2009	2009 Congress on Services - I	10.1109/SERVICES-I.2009.61	real-time computing;computation tree logic;computer science;database;distributed computing;liveness	SE	-55.49743660744594	39.22750438573379	45409
c38a6bbbd764fbbf6d238eeba883f246f0e220b7	sm@rt offloader: supporting adaptive computation offloading for android applications	runtime;offloading management;android application	Computation offloading is a promising technique to help improve the performance as well as reduce the battery consumption of a smartphone app (short for application) by executing some parts of the app on a remote server. However, to make a favorable offloading decision, one has to consider lots of factors including the program structure of the app, the hardware configuration of the smartphone, the current resources usages (e.g., battery, CPU, and memory), and the volatile network conditions. It is almost impossible to implement a program logically built-in in the app to support adaptive offloading along with the ever-changing app contexts. In this paper, we present a middleware, SM@RT Offloader, to support adaptive computation offloading for Android mobile apps. Our experiment shows the feasibility and effectiveness of SM@RT Offloader.	android;canonical account;central processing unit;computation offloading;middleware;mobile app;server (computing);smartphone;structured programming	Huaqian Cai;Wei Zhang;Ying Zhang;Gang Huang	2013		10.1145/2541614.2541617	embedded system;real-time computing;computer science;operating system	Mobile	-35.19172883116816	54.43999656706224	45425
d13368064f104735977754e40fd9b49a1d6c55d5	an approach to ga-driven automatic refactoring based on design patterns	refactoring;java programming;software maintenance;software maintenance genetic algorithms object oriented programming;java program;object oriented programming;ga driven automatic refactoring;discriminant analysis;syntax analysis;syntax analysis design patterns discriminant analysis genetic algorithm refactoring;design pattern;genetic algorithm;genetic algorithms;design patterns;java program ga driven automatic refactoring design pattern genetic algorithm	Refactoring is a process of applying behavior-preserving transformations to improve the design, readability, structure, performance, abstraction, and maintainability of existing code. This paper presents an approach to genetic algorithm-driven refactoring for Java programs to automatically judge the qualities of programs based on design patterns. If a program is judged to be bad, refactoring will be further recommended so that the program can be transformed using an appropriate design pattern.	code refactoring;genetic algorithm;java;natural language processing;software design pattern;software release life cycle	Takao Shimomura;Kenji Ikeda;Muneo Takahashi	2010	2010 Fifth International Conference on Software Engineering Advances	10.1109/ICSEA.2010.39	genetic algorithm;computer science;software engineering;linear discriminant analysis;programming language;code refactoring;algorithm	SE	-57.50974983282461	35.40226828673573	45442
b1acfe3c7fa9b9b0915089bd3dd9b31bb8ab1be4	efficient tagged memory		We characterize the cache behavior of an in-memory tag table and demonstrate that an optimized implementation can typically achieve a near-zero memory traffic overhead. Both industry and academia have repeatedly demonstrated tagged memory as a key mechanism to enable enforcement of powerful security invariants, including capabilities, pointer integrity, watchpoints, and information-flow tracking. A single-bit tag shadowspace is the most commonly proposed requirement, as one bit is the minimum metadata needed to distinguish between an untyped data word and any number of new hardware-enforced types. We survey various tag shadowspace approaches and identify their common requirements and positive features of their implementations. To avoid non-standard memory widths, we identify the most practical implementation for tag storage to be an in-memory table managed next to the DRAM controller. We characterize the caching performance of such a tag table and demonstrate a DRAM traffic overhead below 5% for the vast majority of applications. We identify spatial locality on a page scale as the primary factor that enables surprisingly high table cache-ability. We then demonstrate tag-table compression for a set of common applications. A hierarchical structure with elegantly simple optimizations reduces DRAM traffic overhead to below 1% for most applications. These insights and optimizations pave the way for commercial applications making use of single-bit tags stored in commodity memory.		Alexandre Joannou;Jonathan Woodruff;Robert Kovacsics;Simon W. Moore;Alex Bradbury;Hongyan Xia;Robert N. M. Watson;David Chisnall;Michael Roe;Brooks Davis;Edward Napierala;John Baldwin;Khilan Gudka;Peter G. Neumann;Alfredo Mazzinghi;Alex Richardson;Stacey D. Son;A. Theodore Markettos	2017	2017 IEEE International Conference on Computer Design (ICCD)	10.1109/ICCD.2017.112	pointer (computer programming);real-time computing;parallel computing;locality;implementation;cache;word (computer architecture);metadata;control theory;computer science;enforcement	Arch	-55.59224793136355	55.80426159937838	45450
6a5b52bbe5be23b73f3874c448de17163e09bd16	a platform for secure static binary instrumentation	security policy enforcement;cots binary hardening;software security;binary translation;control flow integrity;binary instrumentation	Program instrumentation techniques form the basis of many recent software security defenses, including defenses against common exploits and security policy enforcement. As compared to source-code instrumentation, binary instrumentation is easier to use and more broadly applicable due to the ready availability of binary code. Two key features needed for security instrumentations are (a) it should be applied to all application code, including code contained in various system and application libraries, and (b) it should be non-bypassable. So far, dynamic binary instrumentation (DBI) techniques have provided these features, whereas static binary instrumentation (SBI) techniques have lacked them. These features, combined with ease of use, have made DBI the de facto choice for security instrumentations. However, DBI techniques can incur high overheads in several common usage scenarios, such as application startups, system-calls, and many real-world applications. We therefore develop a new platform for secure static binary instrumentation (PSI) that overcomes these drawbacks of DBI techniques, while retaining the security, robustness and ease-of-use features. We illustrate the versatility of PSI by developing several instrumentation applications: basic block counting, shadow stack defense against control-flow hijack and return-oriented programming attacks, and system call and library policy enforcement. While being competitive with the best DBI tools on CPU-intensive SPEC 2006 benchmark, PSI provides an order of magnitude reduction in overheads on a collection of real-world applications.	application security;basic block;benchmark (computing);binary code;central processing unit;control flow;emoticon;exploit (computer security);library (computing);perl dbi;return-oriented programming;shadow stack;static library;synchronous backplane interconnect;system call;usability	Mingwei Zhang;Rui Qiao;Niranjan Hasabnis;R. Sekar	2014		10.1145/2576195.2576208	software security assurance;embedded system;instrumentation;real-time computing;computer science;computer security	Security	-55.502813075439136	56.45533704308626	45505
04ecd541a45c253933487cc69a05543b280ad980	integration and uses of rf memory tags with smart space semantic web middleware	space based computing;human computer interaction;semantic web computerised instrumentation human computer interaction middleware mobile computing;mobile device;radio frequency semantic web middleware space technology pervasive computing mobile handsets mobile computing web and internet services marketing and sales ieee news;rf memory tags;resource description framework;data mining;internet of things;smart space semantic web middleware;radio frequency;user interaction rf memory tags smart space semantic web middleware space based computing mobile devices sub internet of things;mobile handsets;semantic web;smart spaces;middleware;space technology;computerised instrumentation;sub internet of things;mobile computing;user interaction;mobile devices;radiofrequency identification	As devices become more ubiquitous and information more pervasive, technologies such as semantic Web and space-based computing come to the fore as a platform for user-interaction with their environment. Integrating technologies such as advanced RF memory tag with mobile devices and combining this with computation platforms such as those supporting semantic Web principles provides innovative and novel applications for the user. In a way, information stored in smart spaces on RF memory tags could even create a sub-Internet of Things where users are the main information transport mechanism rather than the Internet.	computation;information management;interaction;internet of things;middleware;mobile device;pervasive informatics;physical access;radio frequency;requirement;semantic web;service implementation bean;service-oriented infrastructure;smart tv;spaces;tag system;virtual reality;wireless access point	Ian Oliver;Kary Främling;Joni Jantunen;Sergey Boldyrev;Jukka Honkola	2009	2009 IEEE Conference on Emerging Technologies & Factory Automation	10.1109/ETFA.2009.5347169	computer science;operating system;social semantic web;mobile device;database;internet privacy;mobile computing;world wide web	HCI	-39.216490939858296	50.278164972251325	45559
03b8dd3156dd19090a0e9749d12e814b076b967b	hierarchy-driven approach for attack patterns in software security education	hierarchy attack trees attack patterns refinement;hierarchy;security of data computer science education;network security;active learning;attack patterns;higher education;refinement;software engineering;attack trees;computer network;hierarchy driven approach;computer science education;software security;computer security education;computer security education hierarchy driven approach attack patterns software security education student learning network security;software security education;student learning;information security computer security software computer networks computer science education educational institutions information systems solid modeling programming payloads;security of data;approaches to learning	"""We propose a hierarchy-driven approach to facilitate student learning and foster a deeper understanding of the importance of attack patterns in computer, network, and software security. This is a fundamental point in computer and software security education because the """"patch and pray"""" mentality of software security is insufficient. The importance and significance of our approach is justified by accentuating the deficiencies in previous ad-hoc approaches to teaching attack patterns. Because of the vast amount of information in attack pattern repositories, it is unrealistic to expect students to fully comprehend attack pattern fundamentals and its place in computer, network, and software security."""	application security;attack patterns;common criteria;database schema;hoc (programming language);information needs	Joshua J. Pauli;Patrick Henry Engebretson	2008	Fifth International Conference on Information Technology: New Generations (itng 2008)	10.1109/ITNG.2008.15	software security assurance;computer security model;attack;computer science;network security;software engineering;refinement;active learning;higher education;world wide web;computer security;hierarchy	Security	-59.9403309925215	54.45008309553275	45585
104c98e09d2623839e1c58b0348c5dcc4b937aa6	behavioural skeletons for component autonomic management on grids.	components;grid;code adaptivity;inf 01 informatica;design pattern;component model;autonomic computing;component based design;management policy;skeletons	We present behavioural skeletons for the CoreGrid Component Model, which are an abstraction aimed at simplifying the development of GCMbased self-management applications. Behavioural skeletons abstract component self-man-agent in component-based design as design patterns abstract class design in classic OO development. As here we just want to introduce the behavioural skeleton framework, emphasis is placed on general skeleton structure rather than on their autonomic management	abstract type;autonomic computing;autonomic networking;component-based software engineering;design pattern;google cloud messaging	Marco Aldinucci;Sonia Campa;Marco Danelutto;Patrizio Dazzi;Domenico Laforenza;Nicola Tonellotto;Peter Kilpatrick	2007		10.1007/978-0-387-78448-9_1	real-time computing;simulation;computer science;algorithm	SE	-36.407770831335625	40.80216426923362	45617
034e056f123bc0a399190ed1d8e0076ba2a2285d	automated dynamic firmware analysis at scale: a case study on embedded web interfaces	iot;emulation;firmware;vulnerability discovery;eurecom ecole d ingenieur telecommunication centre de recherche graduate school research center communication systems;dynamic analysis	Embedded devices are becoming more widespread, interconnected, and web-enabled than ever. However, recent studies showed that embedded devices are far from being secure. Moreover, many embedded systems rely on web interfaces for user interaction or administration. Web security is still difficult and therefore the web interfaces of embedded systems represent a considerable attack surface.  In this paper, we present the first fully automated framework that applies dynamic firmware analysis techniques to achieve, in a scalable manner, automated vulnerability discovery within embedded firmware images. We apply our framework to study the security of embedded web interfaces running in Commercial Off-The-Shelf (COTS) embedded devices, such as routers, DSL/cable modems, VoIP phones, IP/CCTV cameras. We introduce a methodology and implement a scalable framework for discovery of vulnerabilities in embedded web interfaces regardless of the devices' vendor, type, or architecture. To reach this goal, we perform full system emulation to achieve the execution of firmware images in a software-only environment, i.e., without involving any physical embedded devices. Then, we automatically analyze the web interfaces within the firmware using both static and dynamic analysis tools. We also present some interesting case-studies and discuss the main challenges associated with the dynamic analysis of firmware images and their web interfaces and network services. The observations we make in this paper shed light on an important aspect of embedded devices which was not previously studied at a large scale.	attack surface;cable modem;closed-circuit television;digital subscriber line;embedded system;emulator;firmware;ip camera;internet security;router (computing);scalability;user interface	Andrei Costin;Apostolis Zarras;Aurélien Francillon	2016		10.1145/2897845.2897900	embedded system;firmware;emulation;real-time computing;embedded software;computer science;operating system;dynamic program analysis;common firmware environment;world wide web;computer security;internet of things	Security	-52.98817528384693	59.380045735032354	45655
cde8626845b5fc148e10e8af80f9e852e17bfaf6	adidos - adaptive and intelligent fully-automatic detection of denial-of-service weaknesses in web services		Denial-of-Service (DoS) attacks aim to affect availability of applications. They can be executed using several techniques. Most of them are based upon a huge computing power that is used to send a large amount of messages to attacked applications, e.g. web services. Web services apply parsing technologies to process incoming XML messages. This enlarges the amount of attack vectors since attackers get new possibilities to abuse specific parser features and complex parsing techniques. Therefore, web service applications apply various countermeasures, including message length or XML element restrictions. These countermeasures make validations of web service robustness against DoS attacks complex and error prone. In this paper, we present a novel adaptive and intelligent approach for testing web services. Our algorithm systematically increases the attack strength and evaluates its impact on a given web serice, using a blackbox approach based on server response times. This allows one to automatically detect message size limits or element count restrictions. We prove the practicability of our approach by implementing a new WS-Attacker plugin and detecting new DoS vulnerabilities in widely used web service implementations.	algorithm;cognitive dimensions of notations;countermeasure (computer);cryptography;data compression;denial-of-service attack;internet;parsing;penetration test;plug-in (computing);recursion (computer science);soap;security assertion markup language;sensor;server (computing);soapui;sourceforge;web service;xml signature	Christian Altmeier;Christian Mainka;Juraj Somorovsky;Jörg Schwenk	2015		10.1007/978-3-319-29883-2_5	web service;web application security;web modeling;computer science;ws-policy;ws-addressing;database;world wide web;computer security;mashup	Security	-57.8777971972697	59.09475846529347	45830
326d41eae31931acf244998626aa03e7cb107811	data protection aspects in an integrated hospital information system	hospital information system;edp auditing;datacommunication software;recovery;logging;hospital information systems;integration;operating system;access control;data protection;database management system	Data protection aspects in a large. integrated Hospital Information System are described. A special feature in this case is the interaction between the operatmg system BOS and a large number of application subsystems. (Both system and application software has been developed in-house by BAZIS). Integration aspects are shown lo contribute to data protection. The mam topic in the present article is access control; however, several measures with regard to accuracy. consistency and availability of the information (data integrity) will also be mentioned.	access control;data integrity;f-spot;fallout 3;information privacy;information system;password	C. P. Louwerse;J. M. L. Kouwenberg	1984	Computers & Security	10.1016/0167-4048(84)90007-5	recovery;computer science;access control;data mining;database;data protection act 1998;computer security;logging	DB	-47.54797159170421	59.95583337531634	45832
1e38d7b67856cbd91e4a675cf2cba3009e4fe53b	semantically enriched iot gateway for wearable devices.		With the advance of wearable devices, an IoT (Internet of Things) gateway should support the efficient forwarding, processing, and provisioning of the streaming data obtained from them. We propose a semantically enriched IoT gateway working on a smartphone for wearable devices. The proposed gateway supports (a) an efficient consolidation and compression of heterogeneous streaming sensor data, (b) privacy preserving data processing by semantic reasoning, and (c) a selective data sharing through their social networks according to a degree of sensitivity. Experimental results with the prototype implementation show the potential of the proposed IoT gateway for novel user experiences based on semantic web technologies.	emoticon;internet of things;prototype;provisioning;semantic web;semiconductor consolidation;smartphone;social network;streaming media;wearable computer;wearable technology	Kangho Hur;Sangjin Shin;Sungkwang Eom;Minjae Song;Kyong-Ho Lee	2015			multimedia	Mobile	-41.82641313144411	48.57017855522192	45839
21acf0bd10b6117a498b34b01c3e510a39cb6e9d	s.s. iyengar on the paradigm shift in computing	paradigm shift		paradigm	John Gehl	2002	Ubiquity	10.1145/512517.512518	paradigm shift;computer science	Crypto	-47.05679167391535	46.6585949822211	45874
78d404539e9f54a27c24379d55e623de2370cdfc	50k-c: a dataset of compilable, and compiled, java projects		We provide a repository of 50,000 compilable Java projects. Each project in this dataset comes with references to all the dependencies required to compile it, the resulting bytecode, and the scripts with which the projects were built.  The dependencies and the build scripts provide a mechanism to re-create compilation of the projects, if needed (to instruct source code for bytecode analysis, for example). The bytecode is ready for testing, execution, and dynamic analysis tools.		Pedro Henrique Martins;Rohan Achar;Cristina V. Lopes	2018	2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)	10.1145/3196398.3196450	database;computer science;compiler;bytecode;software;scripting language;source code;software mining;java	SE	-53.63764995796359	35.86578018532645	45911
85f2e85fb3e9ee9692498a245906d7918ff20881	software theory change for resilient near-complete specifications	research outputs;research publications	Software evolution and its laws are essential for antifragile system design and development. In this paper we model early-stage perfective and corrective changes to software system architecture in terms of logical operations of expansion and safe contraction on a theory. As a result, we formulate an inference-based notion of property specification resilience for computational systems, intended as resistance to change. The individuated resilient core of a software system is used to characterize adaptability properties. c © 2015 The Authors. Published by Elsevier B.V. Peer-review under responsibility of the Conference Program Chairs.	belief revision;interpretation (logic);kelly criterion;logical connective;postcondition;programming paradigm;software evolution;software system;systems architecture;systems design;theory	Giuseppe Primiero;Franco Raimondi	2015		10.1016/j.procs.2015.05.091	computer science;artificial intelligence;software design;software development;software design description;software construction;algorithm	SE	-42.61347037568195	36.588390254663466	45915
889140c057007774fc9cbffd9a1e697eff127fc0	inter-device sensor-fusion for action authorization on industrial mobile robots		Usage of mobile robots in industry increased significantly in recent years. However, mobile robots introduce additional safety issues for human workforce and pose a higher risk of failures in production due to possible abnormal robot behavior. Such abnormal behavior could, among other things, be caused by security weaknesses that entail attacks. These problems lead to a need for action authorization mechanisms to protect humans and mitigate possible costly failures. In this paper, we propose an authorization mechanism for critical actuator actions on industrial mobile robots. The mechanism relies on security principles that prevent adversaries from unauthorized action execution. To the best knowledge of the authors, no similar concept for secured action authorization for industrial mobile robots is currently known in research. Our evaluation shows more than 80% of additional safety hazard causes introduced by the lack of security can be mitigated with the proposed authorization mechanism.	authorization;mobile robot	Sarah Haas;Andrea Höller;Thomas Ulz;Christian Steger	2018		10.1007/978-3-319-99130-6_19	systems engineering;computer security;computer science;actuator;behavior-based robotics;device sensor;authorization;mobile robot	Robotics	-56.30280549639554	51.04006916893322	45965
636f56ff6bd207adec4c626ff7f0edb3e0c0dfec	position paper: advances in reconfigurable distributed real time embedded systems	design model;model driven engineering mde component based architectures distributed real time embedded dre systems dynamic reconfiguration;component based software engineering;dynamic reconfiguration;distributed real time embedded;system dynamics;reconfigurable architectures;development process;software engineering;embedded system;formal method;embedded systems;distributed real time embedded dre systems;real time systems embedded system software engineering model driven engineering runtime computer architecture hardware embedded computing systems engineering and theory buildings;system architecture position paper reconfigurable distributed real time embedded systems computational complexity runtime support level formal methods software engineering hardware design;computational complexity;execution environment;model driven engineering;user requirements;hardware design;model driven engineering mde;system architecture;component based architectures;software engineering computational complexity embedded systems reconfigurable architectures	A system is reconfigurable when it can modify its behavior or its architecture at runtime. The constant growth of the complexity in embedded systems, the variation of execution environment constraints and the evolution of user requirements make the reconfiguration more important and more difficult to achieve. The challenges concern as much the design model level as the runtime support level. These area involve widely diverse core expertise ranging from formal methods, system architecture, hardware design and software engineering. However, there is still a gap between levels involved in the development of these systems. This observation yield the following: we present the state of the art for the modeling and development of these systems through four classes of tools: component-based software engineering, technologies used to specify and configure embedded systems, frameworks and development processes.	component-based software engineering;embedded system;formal methods;requirement;run time (program lifecycle phase);systems architecture;user requirements document	Fatma Krichen	2010	2010 10th Annual International Conference on New Technologies of Distributed Systems (NOTERE)	10.1109/NOTERE.2010.5536646	computer architecture;real-time computing;system of systems;computer science;systems development life cycle;computer engineering;systems design	EDA	-43.69412993666312	37.14148213775253	46017
85d5cdf98430a6c929517cb3783af2c882b321a8	automated pattern-based service deployment in programmable networks	virtual private network;resource discovery;building block;on demand service deployment;service description;distribution pattern;service deployment;programmable networks	This paper presents a flexible service deployment architecture for the automated, on-demand deployment of distributed services in programmable networks. The novelty of our approach is (a) the customization of the deployment protocol by utilizing modular building blocks, namely navigation patterns, aggregation patterns, and capability functions, and (b) the definition of a corresponding service descriptor. A customizable deployment protocol has several important advantages: It supports a multitude of services, and it allows for an ad hoc optimization of the protocol according to the specific needs of a service and the current network conditions. Moreover, our architecture provides an environment for studying new patterns which aim at reducing deployment latency and bandwidth for certain services. We demonstrate how the developed architecture can be used to setup a virtual private network, and we present measurements conducted with our prototype in the PlanetLab test network. Furthermore, a comparison of a distributed pattern with a centralized pattern illustrates the performance trade-off for different deployment strategies.	centralized computing;emoticon;hoc (programming language);mathematical optimization;multitier architecture;planetlab;prototype;requirement;software deployment;virtual private network;xml	Daniela Brauckhoff;Matthias Bossardt;Bernhard Plattner	2005	Journal of Network and Systems Management	10.1007/s10922-005-9014-5	deployment diagram;service delivery framework;distributed computing;software deployment;computer security;computer network	Networks	-38.059726181578874	45.73868048340795	46033
1d7b03c5a95b772685685c791319f0d122946e69	a collaborative framework for generating probabilistic contracts	groupware;repackaging collaborative framework probabilistic contract android malware;smart phones;malware collaborative framework probabilistic contract generation android smartphone application server application usage profile clustered probabilistic automata pearson chi squared test;contracts probabilistic logic probability distribution automata collaboration androids humanoid robots;invasive software;statistical testing;mobile computing;operating systems computers;statistical testing groupware invasive software mobile computing operating systems computers smart phones	We propose a collaborative framework for generating probabilistic contracts for Android smartphones aimed at detecting repackaged applications. To this end, a network of users sends to the application server the sequences of actions that represent the usage profile of the application. Then, the application server generates a contract from this set of traces. Contracts are represented through clustered probabilistic automata. At run-time, a monitoring system on the smartphone verifies the compliance of the running application against the contract through the Pearson's Chi Squared test. In the preliminary tests, the proposed framework has been able to detect repackaged applications whose behavior is strongly similar to the original application but hide malware.	android;application server;automata theory;chi;experiment;malware;mobile app;performance;probabilistic automaton;prototype;reputation system;sensor;server (computing);smartphone;system call;tracing (software);usability	Fabio Martinelli;Andrea Saracino;Daniele Sgandurra;Alessandro Aldini	2013	2013 International Conference on Collaboration Technologies and Systems (CTS)	10.1109/CTS.2013.6567219	statistical hypothesis testing;real-time computing;computer science;operating system;mobile computing;world wide web;computer security;collaborative software	SE	-59.64488293704303	53.27299274047016	46109
79f4fb42e277b33d84bac7dd575c6a67d2b08a2e	codebender: remote software protection using orthogonal replacement	developpement logiciel;security and protection;replacement;remplacement;hardware software protection;securite informatique;program transformation;layout;reverse engineering industrial property program testing;software engineering;computer security;development tool;reverse engineering codebender remote software protection orthogonal replacement client replacement strategy malicious host problem client code security malicious host problem client code validity program code;program testing;synchronization;data structures;desarrollo logicial;seguridad informatica;indexation;software development;ofdm;ofdm remote monitoring data structures encoding synchronization layout computer security;software software engineering;reemplazo;development tools;industrial property;remote monitoring;encoding;data structure;software protection;reverse engineering;security and protection hardware software protection development tools software software engineering program transformation	CodeBender implements a novel client replacement strategy to counter the malicious host problem and address the problem of guaranteeing client-code security. CodeBender is a tool that implements a novel client-replacement strategy to counter the malicious host problem. It works by limiting the client code's validity and, when the code expires, by having the server provide a new client that replaces the former one. The complexity of analyzing frequently changing, always different (orthogonal) program code deters an adversary's reverse engineering efforts. We've implemented CodeBender and tested its practicability in two case studies.	adversary (cryptography);reverse engineering;server (computing)	Mariano Ceccato;Paolo Tonella	2011	IEEE Software	10.1109/MS.2010.158	layout;synchronization;orthogonal frequency-division multiplexing;data structure;computer science;engineering;software development;operating system;software engineering;database;programming language;computer security;reverse engineering;encoding;rmon;remote evaluation	SE	-58.529414708522616	42.422435398815956	46141
0aa4ee795c79934ccafd8e0239db386e8228d77f	target-sensitive construction of diagnostic programs for procedure calling sequence generators	automated testing;semantics directed compiler generation;formal specification;code generation;partial evaluation;specification tests;compilation of higher order functional languages	Building compilers that generate correct code is difficult. In this paper we present a compiler testing technique that closes the gap between actual compiler implementations and correct compilers. Using formal specifications of procedure calling conventions, we have built a target-sensitive test suite generator that builds test cases for a specific aspect of compiler code generators the procedure calling sequence generator. By exercising compilers with these target-specific test suites, our automated testing tool has exposed bugs in every compiler tested. These compilers include ones that have been in heavy use for many years. The detected bugs cause more than 14,000 test cases to fail.	calling convention;compiler;software bug;test automation;test case;test suite	Mark W. Bailey;Jack W. Davidson	1996		10.1145/231379.231431	computer architecture;compiler;compiler correctness;interprocedural optimization;computer science;compiler construction;formal specification;programming language;partial evaluation;functional compiler;algorithm;code generation	PL	-57.10274412060546	38.348945626106236	46174
c1857c04ffb1b2fc30347b178790625fdde3f5ad	approaches towards dealing with complex systems configuration	complex system	This paper describes some challenges facing architects who have to develop solutions operating on distributed virtualized infrastructures. It suggests the use of model-driven deployment and end-to-end monitoring for alleviating some of these problems.	complex systems	Nirmal K. Mukhi	2010		10.1007/978-3-642-16961-8_10	simulation;systems engineering;engineering;management science	OS	-45.65886126783884	37.95244552206843	46186
bc05d530b44b58c5442157881416913b54d0eef3	an innovative internet service for backing up data on personal computer and mobile devices	home computing;computers;software;backward web feed based framework;backup service;really simple syndication;backup restoration;mobile device;security of data back up procedures internet;information security;personal computer;data management podcast really simple syndication web feed backup restoration;web and internet services;web feed;feeds;podcast;data management;internet service;data mining;media;back up procedures;servers;web and internet services microcomputers mobile computing digital audio broadcasting feeds home computing redundancy information security subscriptions competitive intelligence;internet;redundancy;automatic file backup;remote backup servers internet service personal computer mobile devices digital data safe keeping backward web feed based framework automatic file backup automatic file restoration web technology podcast technology backup service;digital audio broadcasting;automatic file restoration;subscriptions;internet services;competitive intelligence;service design;linux;digital data;safe keeping;mobile computing;podcast technology;web technology;security of data;mobile devices;microcomputers;remote backup servers	Nowadays, it is highly important for us to well organize, access, and store digital data in a way which is efficient and secure. Since there are many incidents which may cause the loss of important data, backing up and safe-keeping important data into some reliable and flexibly accessible media is an extremely important practice. In this paper we propose a Backward Web-feed Based Framework (BWBF) for supporting the automatic backup and restoration of important files by using an emerging web technology – the podcast technology. A sample BWBF based backup service designed and implemented to automatically and asynchronously backup files to (and restore files from) BWBF-based remote backup servers was described in this article.	circuit restoration;digital data;mobile operating system;personal computer;podcast;remote backup service;web feed	Shuchih Ernest Chang	2009	2009 International Conference on Complex, Intelligent and Software Intensive Systems	10.1109/CISIS.2009.52	backup software;continuous data protection;computer science;database;internet privacy;world wide web	HPC	-39.72577741284784	57.58364670016309	46222
73517359399549c55f8343306bedb7166ca592cc	design and implementation of a task manager prototype for the operation room		An operation room is a stressful work environment. Nevertheless, all involved persons have to work safely as there is no space for making mistakes. To ensure a high level of concentration and seamless interaction, all involved persons have to know their own tasks and tasks of their colleagues. The entire team must work synchronously at all times. However, the operation room (OR) is a noisy environment and the actors have to set their focus on their work. To optimize the overall workflow, a task manager supporting the team was developed. Each actor is equipped with a client terminal showing a summary of their own tasks. Moreover, a big screen displays all tasks of all actors. The architecture is a distributed system based on a communication framework that supports the interaction of all clients with the task manager. A prototype of the task manager and several clients have been developed and implemented. The system represents a proof-of-concept for further development. This paper describes the concept of the task manager.	prototype;task manager	Markus Wiemuth;Oliver Burgert	2014			architecture;task manager;simulation;real-time computing;workflow;computer science	OS	-37.383437629834326	41.013561730583824	46266
fc8cee1bd73686c21361a9b37b5e61489bcec4c7	an architecture for privacy-abcs		One of the main objectives of the ABC4Trust project was to define a common, unified architecture for Privacy-ABC systems to allow comparing their respective features and combining them into common platforms. The chapter presents an overview of features and concepts of Privacy-ABCs and introduces the architecture proposed by ABC4Trust, describing the layers and components as well as the highlevel APIs. We also present the language framework of ABC4Trust through an example scenario. Furthermore, this chapter investigates integration of Privacy-ABCs with the existing Identity Management protocols and also analyses the required trust relationships in the ecosystem of Privacy-ABCs. As we mentioned in the previous chapter, there are several implementations of Privacy-ABCs, based on different cryptographic primitives. Even though these schemes have similar features, they are realized with different cryptographic mechanisms and many times they are even called differently, making these technologies hard to understand and compare. Their differences and complexity also makes it difficult for application developers to use them in practice and it is almost impossible to switch between them once the application has been deployed. The ABC4Trust architecture presented in this chapter aims to overcome these problems by defining an abstract interface to Privacy-ABCs, in such a way that they are independent from the concrete algorithms or cryptographic components used Patrik Bichsel, Jan Camenisch, Maria Dubovitskaya, Robert R. Enderlein, Stephan Krenn, Anja Lehmann, Gregory Neven, and Franz-Stefan Preiss IBM Research – Zurich, Switzerland, e-mail: {pbi,jca,mdu,enr,skr,anj,nev,frp}@	abstraction layer;algorithm;cryptographic primitive;cryptography;ecosystem;email;franz lisp;hartmut neven;ibm research;identity management;jan bergstra;privacy;switzerland	Patrik Bichsel;Jan Camenisch;Maria Dubovitskaya;Robert R. Enderlein;Stephan Krenn;Ioannis Krontiris;Anja Lehmann;Gregory Neven;Christian Paquin;Franz-Stefan Preiss;Kai Rannenberg;Ahmad Sabouri	2015		10.1007/978-3-319-14439-9_2	multilayered architecture;enterprise architecture framework;reference architecture;software architecture;the open group architecture framework;space-based architecture;database-centric architecture;architecture domain;applications architecture;cellular architecture;service-oriented modeling;enterprise architecture management;solution architecture;software architecture description;architecture framework;data architecture;systems architecture		-46.74742887271318	52.88772698437385	46351
37eb514a83159697b0209fb24df3d73a2334b13b	augmenting test suites automatically	program testing;augment test suites;automatic generated integration test cases;complex object interactions;execution sequences;software testing;automatic test generation;software testing;unit and integration testing	We present an approach to augment test suites with automatically generated integration test cases. Our approach utilizes existing test cases to direct generation towards producing complex object interactions and execution sequences that have not been observed before.	integration testing;interaction;test case;test suite	Konstantin Rubinov;Jochen Wuttke	2012	2012 34th International Conference on Software Engineering (ICSE)		computer science;engineering;software engineering;software testing;programming language;test case;computer engineering	SE	-56.25229768692131	32.32911000703614	46419
b3b8cf7f9c202d1da7f64256e4668c89c3d7967b	solving problems of optimization of cellular networks for mobile telephony: towards a dynamic architecture based on agents	cellular network;mobile telephony	In recent years, wireless communication systems have experienced tremendous growth, especially in broadcasting, television or handheld computers to the new generation with Web access (UMTS, GPRS). Meanwhile, the techniques of radio are now crucial to a growing number of services. As a result, the size and complexity of applications increases. Current technology can not protect us from design problems. That is why we need a high-level modeling, which we will analyze the organization between the different elements of the system and the interactions between them. The interest of multi-agent systems stems from the collective behavior produced by the interaction of several autonomous entities called agents and flexible, these interactions that revolve around cooperation, competition and coexistence between these agents. However, techniques from the field, focus more on the expression of inter-agents. The expression of the mobility point of view of distributed systems is not described. A property once acquired, will allow the process to choose for themselves to move on the sites of a network to work locally on the resources and make their exchange interactions. Why we propose in this paper a formalization that uses a process algebra, which is the π-calculus to develop self-adaptive systems that can respond to the form of the problem.	adaptive system;autonomous robot;coexist (image);computer;distributed computing;entity;high- and low-level;interaction;internet access;mathematical optimization;mobile device;multi-agent system;process calculus	Chaker Mezioud;Mohamed-Khireddine Kholladi	2009		10.3233/978-1-60750-034-6-447	computer science;architecture;computer network;mobile computing;mobile telephony;cellular network	AI	-43.55643442741948	54.296514298005974	46465
30fcb926673ac31e9779ff534d470299727fcab0	virtual smartphone over ip	virtual smartphone over ip system;virtualization;image processing;virtual smartphone images;prototypes;cloud;smart phones;android;pc like functionality;data center;servers;cloud smartphone android virtualization;mobile cloud;image customization;image customization mobile application pc like functionality virtual smartphone over ip system virtual smartphone images mobile cloud;batteries;mobile computing image processing ip networks;mobile communication;ip networks;smart phones mobile communication batteries servers prototypes security;smartphone;mobile computing;security;mobile application	The number of smartphone users and mobile application offerings are growing rapidly. A smartphone is often expected to offer PC-like functionality. In this paper, we present Virtual Smartphone over IP system that allows users to create virtual smartphone images in the mobile cloud and to customize each image to meet different needs. Users can easily and freely tap into the power of the data center by installing the desired mobile applications remotely in one of these images. Because the mobile applications are controlled remotely, they are not constrained by the limit of processing power, memory and battery life of a physical smartphone.	data center;mobile app;mobile cloud computing;smartphone	Eric Y. Chen;Mitsutaka Itoh	2010	"""2010 IEEE International Symposium on """"A World of Wireless, Mobile and Multimedia Networks"""" (WoWMoM)"""	10.1109/WOWMOM.2010.5534992	embedded system;data center;virtualization;image processing;computer science;operating system;internet privacy;mobile computing;android	Mobile	-37.57150659652702	52.56279126915321	46545
8e724aca1b6a615770979f3e3d3f60d9b7cb27b0	applying end-to-end path delay analysis to multi-rate automotive systems developed using legacy tools	datorsystem;computer systems;delays real time systems automotive engineering time factors optimization clocks;software maintenance automotive engineering embedded systems optimisation;pessimism minimization end to end path delay analysis multirate automotive embedded system legacy tool optimization multirate task chain single rate subchain	The end-to-end path delay analysis is used to predict timing behavior of multi-rate automotive embedded systems. Some of the assumptions used by the existing analysis may not be strictly followed by some legacy tools due to optimizations applied during the development of these systems. As a result, the existing analysis may not be applicable in some cases. In this paper we identify one such case. That is, the case in which all the tasks in a multi-rate task chain have equal priorities despite the fact that they have different periods. Furthermore, the chain contains at least one single-rate sub-chain. We also propose a preliminary solution that makes the existing analysis applicable to this case. However, the proposed solution is pessimistic. Currently, we are working on minimizing the pessimism.		Saad Mubeen;Thomas Nolte	2015	2015 IEEE World Conference on Factory Communication Systems (WFCS)	10.1109/WFCS.2015.7160585	real-time computing;simulation;engineering;operations management	Embedded	-39.46175660925678	34.684365908563066	46574
bd911c659b1027079afa14b4bcc7a1ad4b87d92a	improved system-access control using complementary technologies	complementary technologies;authentication;re authentication;it security;identification;access control;verified authentication	Although there are many different aspects to consider when looking at IT security, one of the most tried and trusted methods of ensuring the safety of systems and data is to control people’s access to them. In this article the various complementary system-access control mechanisms will be discussed. In addition, this article is aimed at demonstrating that in order to tighten up control and security, it is important to think in terms of combining mechanisms by using various complementary technologies. This article, therefore, does not necessarily make a new contribution to the domain of systemaccess control, but attempts rather to integrate and consolidate current approaches to improve it. Emphasis is, therefore, placed on the inherent nature of the mechanisms rather than on specific technologies such as biometrics.	access control;biometrics;control system;mind;network security;trusted operating system	Les Labuschagne;Jan H. P. Eloff	1997	Computers & Security	10.1016/S0167-4048(97)00007-2	identification;computer science;access control;authentication;internet privacy;world wide web;computer security	Security	-46.35054153331274	58.51394101048225	46583
d3b138678934513667a0f08b0c311a79c67e93dc	towards the secure modelling of olap users' behaviour	healthcare;users behaviour;information security;olap;query evolution;user behaviour;information system;data warehouses;data warehouse;security;state models;inference;on line analytical processing	Information Security is a crucial aspect for organizations, and must be considered during the development of Information Systems. The data in Data Warehouses (DWs) are highly sensitive since they manage historical information which is used to make strategic decisions, and security constraints should therefore be included in DW modelling within its structural aspects. However, another dynamic security component is also related to the sequences of OLAP (On-Line Analytical Processing) operations, and could be used to access (or infer) unauthorized information. This paper complements the modelling of DWs with state models, which permit the modelling of these dynamic situations in which sensitive information could be inferred. That is, it models queries that include security issues, and controls that their evolution through the application of OLAP operations always leads to authorized states. Finally, our proposal has been applied to a healthcare case study in which a DW manages admissions information with various security constraints.	authorization;dreamwidth;information security;information sensitivity;information system;online analytical processing	Carlos Blanco;Eduardo Fernández-Medina;Juan Trujillo;Jan Jürjens	2010		10.1007/978-3-642-15546-8_8	security information and event management;online analytical processing;computer science;information security;data warehouse;data mining;database;computer security;information system	DB	-49.5970820260271	53.083609504981986	46603
dd09a400b31807ee0569542b7d6cbb2eb87d8a16	reliable implementation of hybrid control systems for advanced avionics	advanced avionics;hybrid control systems;reliable implementation	The avionics systems of modern aircraft must manage a complex array of continuous and discrete dynamics that govern aircraft operations. Producing the software that implements such systems and provably satisfies stringent safety requirements is a difficult problem. Expanded cockpit automation, addition of data links to bring more information into the cockpit, and planned upgrades to the air traffic management system will increase the system complexity and the magnitude of this problem. This paper describes avionics automation from a hybrid systems perspective and presents initial results of our research in providing methods for reliable implementation of these systems.	avionics	Darren D. Cofer	1996		10.1007/BFb0031557	reliability engineering;embedded system;real-time computing;integrated modular avionics	Embedded	-45.98268738592567	36.79890810793895	46642
648cad1a11cd3564dc2353ef6b01c3ba748ab56b	resource contention analysis of cloud-based system through fuml-driven model execution		Model-driven software engineering not only enables the efficient development of software but also facilitates the analysis of non-functional properties (NFPs). As UML, the most adopted modeling language for designing software, lacks in formal execution semantics, current approaches translate UML models into dedicated analysis models, before NFPs can be computed. However, such transformations introduce additional complexity for the users and developers of analysis tools. To avoid this additional complexity, we show how the analysis of certain NFPs can be performed solely based on UML models without translating them into, e.g., queuing networks. Therefore, we leverage the execution semantics of fUML, a recent OMG standard, to gain execution traces from UML models and, based on these traces, compute indices, such as response time, taking into account the contention of resources, as well as different resource management configurations, such as balancing and scaling strategies.	cloud computing;component-based software engineering;executable uml;experiment;image scaling;model-driven architecture;model-driven engineering;non-functional requirement;profiling (computer programming);programming language;provisioning;resource contention;response time (technology);scalability;throughput;tracing (software);unified modeling language;usability;whole earth 'lectronic link	Martin Fleck;Luca Berardinelli;Philip Langer;Tanja Mayerhofer;Vittorio Cortellessa	2013			cloud computing;modeling language;resource management;software;semantics;real-time computing;response time;unified modeling language;queueing theory;computer science	SE	-45.901508800110086	39.96739644731583	46664
de2f2098678a72dac5ab96cfeb14b62d51647080	towards a (de)composable workflow architecture to define data collection policies	sensor network;software architecture;software reuse	"""Sensor networks are classically used in the Internet of Things to collect data, typically supporting Smart Cities or Smart Homes use cases. However, a deep knowledge of these networks is needed to properly develop applications over the deployed systems. This leads to a target mismatch: developers know how to exploit the collected data to develop large-scale """"smart"""" systems, but do not have enough knowledge to technically enact and compose such behaviors on a given sensor network. In this paper, we envision a tooled approach that supports data collection policies management at a higher level of abstraction, fostering reuse. We discuss an architectural abstraction based on workflow concepts assisting developers in expressing data collection policies. The resulting architectures are then composable to be enacted on the same sensor network, but they can also be partially reused through a selection operator."""	internet of things;smart city	Cyril Cecchinel;Sébastien Mosser;Philippe Collet	2016		10.1145/2851613.2851962	software architecture;real-time computing;wireless sensor network;computer science;operating system;software engineering;data mining;database;world wide web;computer security	AI	-43.01535136988284	45.86315560773489	46733
a3dacf78a81949625c717bacfbbe448acc589d17	enabling rtos simulation modeling in a system level design language	enabling rtos simulation modeling;real time aspect;system level design language;rtos simulation model;complete embedded system simulation;new process definition;execution semantics;existing systemc simulation engine;sim_api library;simulation model;fpga;embedded system;real time;embedded systems	In this paper, we propose a new process definition (T-THREAD) and an extension to the existing SystemC simulation engine (SIM/spl I.bar/API library) to capture the real time aspects of RTOS simulation models in an SLDL like SystemC. We describe the execution semantics of this process and show how it works in a complete embedded system simulation model.	application programming interface;embedded system;level design;programming language;simulation;systemc	M. Abdelsalam Hassan;Keishi Sakanushi;Yoshinori Takeuchi;Masaharu Imai	2005	Proceedings of the ASP-DAC 2005. Asia and South Pacific Design Automation Conference, 2005.	10.1109/ASPDAC.2005.1466493	embedded system;embedded operating system;computer architecture;real-time computing;computer science;operating system;simulation modeling;field-programmable gate array;hardware-in-the-loop simulation	EDA	-39.63157308259718	33.15935257754179	46768
c979655bce60f6db6556653a3a661223f9d2811f	wireless sensor network technologies for condition monitoring of industrial assets	wireless sensor network;condition monitoring	Systematic and robust condition monitoring of crucial equipment is the cornerstone of any successful preventive maintenance policy in the industrial environment. Recent advances in low-cost wireless sensor network (WSN) technologies and products indicate a promising future for a cost-effective, wider and more permanent deployment of a distributed sensing and processing infrastructure. This paper aims to provide a comprehensive assessment of main WSN technology alternatives available today, based on a qualitative and quantitative analysis of the typical range of requirements in the specific application domain of industrial machine condition monitoring.		Spilios Giannoulis;Christos Koulamas;Christos Emmanouilidis;Petros Pistofidis;Dimitris Karampatzakis	2012		10.1007/978-3-642-40361-3_5	wireless sensor network;engineering;operations management;key distribution in wireless sensor networks;forensic engineering;computer security	Mobile	-45.808646665145666	47.39228992374282	46816
9a39e0b7870f21236cf76eda54f3d0625d70f387	towards component orientation in embedded web service environments	protocols;service provider;service oriented system;service orientation;component based approach;devices profile for web services technology;web services embedded systems object oriented programming software architecture software reusability;object oriented programming;web service;function block;embedded systems;software architecture;protocols business service oriented architecture subscriptions simple object access protocol containers;system design;software reusability;business;web services;subscriptions;features service orientation;devices profile for web services;features service orientation embedded web service environment service oriented architecture service oriented system component based approach devices profile for web services technology;service oriented architecture;simple object access protocol;embedded web service environment;component based design;containers	Service-oriented architectures and systems designs are powerful concerning reusability of functional blocks and hiding implementation details from functional interfaces. But to compose a complete application, often central entities and engines are required for processing a specific sequence of service. In component-based designs the component itself is capable of describing both not only their offered services and interfaces but also dependencies on other services and interfaces to fulfill a complete task or application logic. This paper investigates on how to transfer and enhance exiting component-based approaches, already known from business applications, into the domain of embedded web services environments. Special focus is on the Devices Profile for Web Services (DPWS) technology which features service orientation also in device centric applications. This paper introduces a new approach to create applications, based on services provided by devices deployed with DPWS, in an abstract and dynamic way.	business logic;centralized computing;component-based software engineering;devices profile for web services;embedded system;entity;interoperability;service-oriented architecture;service-oriented device architecture;service-oriented software engineering;web service;gsoap	Elmar Zeeb;Guido Moritz;Dirk Timmermann;Frank Golatowski	2010	2010 IEEE 15th Conference on Emerging Technologies & Factory Automation (ETFA 2010)	10.1109/ETFA.2010.5641333	web service;computer science;operating system;database;distributed computing;services computing;world wide web	Embedded	-38.987802519479835	41.95038616944186	46908
459fbe175f849187174beea4a0d99bf7079ab7ed	supporting context awareness in smart environments: a scalable approach to information interoperability	context aware;rdf graphs;semantic technologies;information space;pervasive system;context aware middleware;quality requirement;middleware;interoperability;legacy system;smart environments;smart environment	A still open issue in pervasive systems is how to effectively support the development and run-time execution of smart applications in wide-scale and open deployment scenarios. That calls for i) easy and low-cost interoperability with legacy systems/services and ii) scalability in terms of both overhead and performance results for different classes of applications with different quality requirements. In this position paper, we report the research work we are doing within an ongoing project on the exploitation of context awareness for scalability in smart environments, where interoperability is achieved at the information level via semantic technologies. In particular, the paper shows how context-aware middleware facilities can help in dynamically determining personalized views on a shared information space, implemented as coordinated repositories of Resource Description Format triples. The advantages are relevant in terms of usability (automatic discarding of unsuitable resource/service components) and overhead reduction (limitation of quality monitoring space).	context awareness;interoperability;legacy system;middleware;overhead (computing);personalization;requirement;resource description framework;scalability;smart environment;software deployment;ubiquitous computing;usability	Alessandra Toninelli;Susanna Pantsar-Syväniemi;Paolo Bellavista;Eila Ovaska	2009		10.1145/1657127.1657134	semantic interoperability;computer science;database;distributed computing;world wide web	SE	-40.68170981962978	44.348536461806596	46976
93d3677b75d535955acce6869ebf401fad1c3de7	identity management in platforms offering iot as a service			identity management	Juan D. Parra Rodriguez;Daniel Schreckling;Joachim Posegga	2014		10.1007/978-3-319-19656-5_40	computer network	EDA	-45.92536890875401	49.437369166302915	46982
823aa2adf1002e4ccaedd7d1c036bb6e53ef3466	moses: a framework for qos driven runtime adaptation of service-oriented systems	semiconductor optical amplifiers;service oriented system moses qos driven runtime adaptation service oriented system software system architecture service oriented paradigm runtime self adaptable system software engineering self adaptation qos driven adaptation;software systems;runtime;settore ing inf 05 sistemi di elaborazione delle informazioni;runtime adaptation;quality of service service oriented architecture runtime adaptation;quality of service;service oriented architecture;adaptation models;service oriented architecture quality of service runtime concrete semiconductor optical amplifiers adaptation models software systems;concrete	Architecting software systems according to the service-oriented paradigm and designing runtime self-adaptable systems are two relevant research areas in today's software engineering. In this paper, we address issues that lie at the intersection of these two important fields. First, we present a characterization of the problem space of self-adaptation for service-oriented systems, thus providing a frame of reference where our and other approaches can be classified. Then, we present MOSES, a methodology and a software tool implementing it to support QoS-driven adaptation of a service-oriented system. It works in a specific region of the identified problem space, corresponding to the scenario where a service-oriented system architected as a composite service needs to sustain a traffic of requests generated by several users. MOSES integrates within a unified framework different adaptation mechanisms. In this way it achieves greater flexibility in facing various operating environments and the possibly conflicting QoS requirements of several concurrent users. Experimental results obtained with a prototype implementation of MOSES show the effectiveness of the proposed approach.	moses;problem domain;programming paradigm;programming tool;prototype;quality of service;requirement;service-oriented architecture;service-oriented device architecture;service-oriented modeling;software engineering;software system;unified framework	Valeria Cardellini;Emiliano Casalicchio;Vincenzo Grassi;Stefano Iannucci;Francesco Lo Presti;Raffaela Mirandola	2012	IEEE Transactions on Software Engineering	10.1109/TSE.2011.68	real-time computing;simulation;quality of service;concrete;computer science;systems engineering;engineering;operating system;software engineering;service-oriented architecture;software system	SE	-42.845936120527895	39.262345532776834	47009
19694ab35316e5cd020864bc378dd793db77dd74	an early risk detection and management system for the cloud with log parser		Abstract In a software-defined data center (SDDC), detecting potential risks from structured or unstructured data in real-time is challenging and proves to be of vital significance to deliver zero downtime services. It is essential for the cloud administrators to be aware of any operation or sequence of operations that could cause critical failures resulting in the loss of Business/Mission Critical Systems (BCS/MCS). This demands for a solution such as an Early Risk Detection and Management System (ERDMS). ERDMS provides insights on the operations that can put the system in peril, and recommends the suitable steps to reduce or eliminate the risks involved. In this work, we present our implementation of an Early Risk Detection and Management System (ERDMS) for the cloud using data analytics, association rule learning and machine learning techniques. The ERDMS continually monitors various system parameters by processing the sequence of operations performed on the system to detect potential risk(s) and recommend the probable solution(s). Initially, it parses the log bundles to learn rules, known as “association rules”, for risk detection using apriori algorithm. Each of these association rules consists of premise − sequence of operations and inference − potential risk(s). While constantly monitoring a system, if ERDMS detects a pattern it has learnt, it classifies the pattern into a set of potential risk(s) using decision tree algorithm. It computes a probability for each potential risk to gauge the impact; it also generates a summary of its learning. Once the potential risk(s) is detected, it searches the relevant sites to recommend a set of probable solution(s). Furthermore, it offers “auto-resolution”, where the recommended steps are automatically executed. Consequently, upon appropriate action, the system might not encounter the issue and will continue to work seamlessly.	management system;parser	N. Nagashree;Ravi Tejasvi;C. SwathiK.	2018	Computers in Industry	10.1016/j.compind.2018.01.018	data mining;apriori algorithm;systems engineering;engineering;mission critical;management system;downtime;association rule learning;cloud computing;decision tree learning;unstructured data	ML	-53.10444282113348	42.43212262957707	47082
308f78fab69c9185d5329f7ca2034eecec3f3804	preventing drive-by download via inter-module communication monitoring	drive by download;intrusion detection;inter module communication;distributed denial of service;activex;false positive;malicious script	Drive-by download attack is one of the most severe threats to Internet users. Typically, only visiting a malicious page will result in compromise of the client and infection of malware. By the end of 2008, drive-by download had already become the number one infection vector of malware [5]. The downloaded malware may steal the users' personal identification and password. They may also join botnet to send spams, host phishing site or launch distributed denial of service attacks.  Generally, these attacks rely on successful exploits of the vulnerabilities in web browsers or their plug-ins. Therefore, we proposed an inter-module communication monitoring based technique to detect malicious exploitation of vulnerable components thus preventing the vulnerability being exploited. We have implemented a prototype system that was integrated into the most popular web browser Microsoft Internet Explorer. Experimental results demonstrate that, on our test set, by using vulnerability-based signature, our system could accurately detect all attacks targeting at vulnerabilities in our definitions and produced no false positive. The evaluation also shows the performance penalty is kept low.	botnet;denial-of-service attack;drive-by download;internet explorer;malware;password;phishing;plug-in (computing);prototype;test set;vector (malware);vulnerability (computing)	Chengyu Song;Jianwei Zhuge;Xinhui Han;Zhiyuan Ye	2010		10.1145/1755688.1755705	intrusion detection system;type i and type ii errors;computer science;internet privacy;world wide web;computer security;denial-of-service attack	Security	-56.51306024472708	60.10703554467914	47104
6a74b674c322069c45ca5a7eb6b4795378f3721d	operation and utilization of the racal-datacom cms ® for tivoli tme 10 netview		The optional graphics-oriented, highly intuitive CMS user interface enables a single operator to monitor and control a large multi-technology network. Racal’s integration with NetView gives users a complete network management application.		Jack Hillhouse	1996	Int. Journal of Network Management	10.1002/(SICI)1099-1190(199609/10)6:5%3C284::AID-NEM219%3E3.0.CO;2-Z	real-time computing;computer science;database;world wide web	DB	-33.95617902109798	52.635115671394544	47171
284bc67a4ae731150a32571eba36dd17ff3e4a8b	an immuno-based autonomic computing system for iaas security in public clouds		Cloud Computing is the new way for computing infrastructures exploitation. These infrastructures, offered as a service by the cloud IaaS service model are being very appealing to the new industry and business. However, surveys reveal that security issues are still the major barrier facing the migration from Infrastructure in premise to public clouds. On the other hand, Autonomic Computing Systems have been used so far to enable the cloud, and in this work, we will investigate these systems capabilities to enable security management for IaaS in	amazon elastic compute cloud (ec2);amazon web services;artificial immune system;autonomic computing;cloud computing;exploit (computer security);prototype;rule-based system;security management;software deployment;systems architecture	Abdelwahhab Satta;Sihem Mostefai;Imane Boussebough	2017		10.5220/0006352405460553	simulation;internet privacy;computer security	Security	-48.841835564865114	56.9271908406901	47206
0c496ac46cd1ce17794e6a35a32e4edafdee23b8	mobile computing at the edge (keynote)	mobility;software engineering;software architecture;mobile computing;edge computing	As sales of mobile devices grow and smartphones and tablets become for many the preferred way of interacting with the Internet, social media and the enterprise, organizations are striving to push content and functionality out to mobile users. However, mobile devices still do not have the computing power and battery life that will allow them to perform effectively. This keynote explores current and future options for mobile devices to leverage edge networks and servers to extend their computing power and battery life, along with the software engineering challenges and opportunities that this movement brings.	interaction;internet;mobile computing;mobile device;smartphone;social media;software engineering	Grace A. Lewis	2014		10.1145/2593902.2593920	embedded system;real-time computing;mobile search;simulation;mobile web;engineering;mobile technology;mobile computing;ubiquitous computing	HCI	-39.359334938085986	52.71051925865009	47242
c656fea4db744c215b63ea2ec46e0f024524ea0c	comparing different functional allocations in automated air traffic control design		In the early phases of the design of safety-critical systems, we need the ability to analyze the safety of different design solutions, comparing how different functional allocations impact the overall reliability of the system. To achieve this goal, we can apply formal techniques ranging from model checking to model-based fault-tree analysis. Using the results of the verification and safety analysis, we can compare different solutions and provide the domain experts with information on the strengths and weaknesses of each solution. In this paper, we consider NASA's early designs and functional allocation hypotheses for the next air traffic control system for the United States. In particular, we consider how the allocation of separation assurance capabilities and the required communication between agents affects the safety of the overall system. Due to the high level of details, we need to abstract the domain while retaining all of the key properties of NASA's designs. We present the modeling approach and verification process that we adopted. Finally, we discuss the results of the analysis when comparing different configurations including both new, self-separating and traditional, ground-separated aircraft.	control system;fault tree analysis;formal methods;full scale;functional dependency;high-level programming language;interaction;max-flow min-cut theorem;model checking;scalability;separation kernel	Cristian Mattarei;Alessandro Cimatti;Marco Gario;Stefano Tonetta;Kristin Y. Rozier	2015	2015 Formal Methods in Computer-Aided Design (FMCAD)		atmospheric model;simulation;resource management	AI	-44.6404006898613	34.25382100823101	47243
fd950fe9e9940a00db63b6975ef93401866ef6f0	a system for collecting activity annotations for home energy management	annotations;energy consumption;mobile application;home automation	Home energy management is becoming increasingly important and, though there are a plethora of tools for accessing energy consumption data, few provide concrete insights that can directly help users manage demand. Mechanisms that enable a user to draw connections between activities and energy consumption by attaching contextual labels to energy events are a promising step; however solutions for collecting annotations from users can be error prone or intrusive. This work presents a system for collecting in situ annotations using a smartphone application coupled with an off-the-shelf home energy measurement infrastructure. We use a novel power profiling approach to identify important energy consumption events and solicit contextual annotations from the user via a push notification sent to a smartphone. Using a five-week study performed in five homes, we show that our power profiling approach can identify a significant percentage of important energy consumption events using a very small number of monitored devices. We were able to collect an average of over 2 annotations per day and while users provided a wide range of annotations, the motivation to provide annotations varied across subjects.		Sami Rollins;Nilanjan Banerjee;Lazeeb Choudhury;David Lachut	2014	Pervasive and Mobile Computing	10.1016/j.pmcj.2014.05.008	embedded system;home automation;computer science;operating system;data mining;internet privacy;world wide web	HCI	-35.799977031713084	55.69573872930844	47309
34a703e93d034f30e6801b1636cbb38a0363c451	message from the chairs	documentation;miscellaneous	The International Workshop on Wireless and Sensor Networks (WSNET) has established itself as an integral part of the International Conference on Parallel Processing (ICPP). Wireless networks, and in particular wireless sensor networks, have become an increasingly significant area of distributed computing research, with tough challenges like topology control, scalable routing, or efficient power management. The concept of ubiquitous computing is gradually becoming a reality, although some technical obstacles are still precluding mass adoption of underlying technologies.	distributed computing;floor and ceiling functions;power management;routing;scalability;topology control;ubiquitous computing	Min-Te Sun;Gilles Thonet	2005		10.1109/ICPPW.2005.52		Visualization	-33.870160124020714	50.57339059636076	47329
660c2b90694ce0c528d8d021d3dc4f36a4c0058d	understanding software architectures by visualization--an experiment with graphical elements	visualization architecture experiment graphical elements program comprehension save;software maintenance;program comprehension;software systems;large scale;software architecture;visualization;architecture visualization tool software architectures software evolution software maintenance graphical element configuration program comprehension;software maintenance program visualisation reverse engineering software architecture;graphical elements;software architecture visualization computer architecture large scale systems software systems software engineering software maintenance performance analysis monitoring information retrieval;save;experiment;architecture;program visualisation;reverse engineering	The evolution and maintenance of large-scale software systems requires first an understanding of its architecture before delving into lower level details. Tools facilitating the architecture comprehension tasks by visualization provide different sets of graphical elements. We conducted a controlled experiment that exemplifies the critical role of such graphical elements when aiming at understanding the architecture. The results show that a different configuration of graphical elements influences program comprehension tasks significantly. In particular, a gain of effectiveness by 63% in basic architectural analysis tasks was achieved simply by choosing a different set of graphical elements. Based on the results we claim that significant effort should be spent on the configuration of architecture visualization tools	computer graphics;graphical user interface;program comprehension;software system	Jens Knodel;Dirk Muthig;Matthias Naab	2006	2006 13th Working Conference on Reverse Engineering	10.1109/WCRE.2006.54	experiment;reference architecture;software architecture;visualization;computer science;systems engineering;engineering;architecture;operating system;software engineering;programming language;software maintenance;reverse engineering;computer engineering	SE	-55.287388103575104	35.20015216607665	47373
5f4c990c75271afce21f1409e2752485e0bd18fb	the web of things vision: things as a service and interaction patterns	cloud services interact;things vision;wiley periodicals;traditional web application;novel navigation;real world object;interaction paradigm;common physical object;interaction pattern;new stakeholders	This paper presents our vision of the Web of Things, where real world objects and cloud services interact through the web. In this prospect, mechanisms to embody object-as-a-service, along with novel navigation and interaction paradigms are needed to enable common physical objects in the surroundings to be considered as new stakeholders in traditional web applications.	cloud computing;interaction technique;web application;web of things;world wide web	Benoit Christophe;Mathieu Boussard;Monique Lu;Alain Pastor;Vincent Toubiana	2011	Bell Labs Technical Journal	10.1002/bltj.20485	web modeling;web of things;engineering;web navigation;multimedia;internet privacy;web intelligence;web engineering;world wide web	Web+IR	-41.574188480499416	45.79283600903048	47467
1d26d405ddc1c72e3ffd76506a1286071ad67197	yet another microarchitectural attack: : exploiting i-cache	instruction cache;side channel analysis;microarchitectural analysis;branch prediction;rsa;montgomery multiplication;modular exponentiation	MicroArchitectural Attacks (MA), which can be considered as a special form of Side-Channel Analysis, exploit microarchitectural functionalities of processor implementations and can compromise the security of computational environments even in the presence of sophisticated protection mechanisms like virtualization and sandboxing. This newly evolving research area has attracted significant interest due to the broad application range and the potentials of these attacks. Cache Analysis and Branch Prediction Analysis were the only types of MA that had been known publicly. In this paper, we introduce Instruction Cache (I-Cache) as yet another source of MA and present our experimental results which clearly prove the practicality and danger of I-Cache Attacks.	cpu cache;cache (computing);microarchitecture;protection mechanism;rsa (cryptosystem);sandbox (computer security);yet another	Onur Aciiçmez	2007	IACR Cryptology ePrint Archive	10.1145/1314466.1314469	computer architecture;parallel computing;montgomery reduction;computer science;computer security;modular exponentiation;branch predictor	Arch	-54.03357989617874	56.448899597142464	47468
6b5917100cac46bc06cfa0f0d74e84d3f9a59363	middleware challenges for next generation networks and services	next generation network;middleware		middleware;next-generation network	George Kormentzas;Thomas Magedanz	2007	Computer Networks	10.1016/j.comnet.2007.06.005	middleware;next-generation network;computer science;middleware;computer security;computer network	ML	-34.44058905372966	47.19667394872696	47477
6ccc1e253554093acfd24d62da21e494c0f00908	a framework for engineering reusable self-adaptive systems		The increasing complexity and size of information systems result in an increasing effort for maintenance. Additionally, miniaturization of devices leads to higher mobility and the need for context-adaptation, especially in new types of systems, such as Cyber-physical Systems or Internet of Things. Self-adaptive Systems (SASs) has the ability to adapt to changes in their environment or the system resources and address the aforementioned challenges. So far, however, development of these systems is frequently tailored towards the requirements of use cases. The research for frameworks and reusable elements — for implementation as well as design processes — is often neglected. Integrating reusable process and implementation artifacts into a framework and offering a tool suite to developers would make development of SASs faster and less error-prone. This thesis presents the Framework for Engineering Self-adaptive Systems (FESAS). It offers a reusable implementation of a reference system, tools for implementation and design as well as a middleware for controlling system deployment. Due to distribution of systems and an increase of available information, the complexity for adaptation reasoning increases. This can lead to uncertainty at runtime resulting in incompleteness or obsolescence in adaptation goals, models or rules. Therefore, the need for changing the adaptation reasoning arises. As a second contribution, this thesis introduces a new approach for self-improvement of SASs. It complements the SAS with an additional module for meta-adaptation. Unlike existing approaches, the approach is not limited to a specific type of adaptation nor to specific implementation frameworks. The thesis describes the integration of the module for self-improvement with the FESAS Framework. Following a design science approach, this thesis explains the design of the artifacts based on requirements derived from an analysis of related work. For evaluation, prototypes of the artifacts are implemented in a proof by prototyping approach and discussed regarding their usability, applicability, and performance.	adaptive system;cognitive dimensions of notations;cyber-physical system;information system;internet of things;middleware;requirement;run time (program lifecycle phase);sas;sass;sentient computing;software deployment;software framework;system deployment;usability	Christian Krupitzer	2018			adaptive system;miniaturization;systems engineering;information system;system deployment;use case;sass;computer science;middleware	SE	-47.222871626016435	38.315496001924785	47493
8a0d610a0d616cafce8cfd1a10faec3debd65432	towards a diversification framework for operating system protection	malware protection;code diversification;operating system security	In order to use resources of a computer, malware has to know the interfaces provided by the operating system. If we make these critical interfaces unique by diversifying the operating system and user applications, a piece of malware can no longer successfully interact with its environment. Diversification can be considered as a computer-specific secret. This paper discusses how this API diversification could be performed. We also study how much work would be needed to diversify the Linux kernel in order to hide the system call interface from malware.	application programming interface;diversification (finance);linux;malware;operating system;system call	Sampsa Rauti;Johannes Holvitie;Ville Leppänen	2014		10.1145/2659532.2659642	operating system;cryptovirology;world wide web;computer security	OS	-55.73027020592198	59.48984317999143	47528
f98e170af275c1bcc4563ff7ba1c30044b03add1	mass-computer interaction for thousands of users and beyond		We introduce Mass-Computer Interaction (MCI) as a natural evolution of Crowd-Computer Interaction (CCI) fostered by recent technical innovations and advances in large-scale sensing, processing, and interactive systems. MCI represents a sensible combination of (1) a very large number of end-users, usually in the order of hundreds or thousands, (2) very large physical settings, such as theaters and auditoriums, and (3) large-scale infrastructure, including distributed systems. We outline design challenges posed by the new Mass-Computer Interaction paradigm, elaborate on its defining characteristics, and provide a general-purpose model for MCI applications. These contributions are exemplified with SKEMMI, our general-purpose platform specifically designed for developing and deploying Mass-Computer Interaction applications.	contactless smart card;distributed computing;general-purpose modeling;human–computer interaction;interactive computing;programming paradigm;requirement	Jean-Yves Lionel Lawson;Jean Vanderdonckt;Radu-Daniel Vatavu	2018		10.1145/3170427.3188465	human–computer interaction;computer science	HCI	-41.13395823890404	46.322907491501816	47573
acd5824f788a6393e1d80e41105139b6becde5d0	hbbtv goes cloud: decoupling application signaling and application execution in hybrid tv	cloud based hbbtv;ui cloud execution;hbbtv;red button signaling;hybrid tv	The cloud-based execution of the User Interface has already begun to disrupt the TV domain. Indeed, in European Hybrid TV Standard - Hybrid Broadcast Broadband (HbbTV) - the signaling of applications is terminated by special libraries on the client. Therefore, the cloud-based UI execution does directly affect the HbbTV. This work presents an architecture that enables the shift of HbbTV functionality into the Cloud. This is based on the decoupling of HbbTV application signaling and application execution on the client side. The shift is executed by defining new interfaces for HbbTV-to-cloud and cloud-to-device. This work describes possible approaches for such architectures, relevant open issues and corresponding challenges.	client-side;cloud computing;coupling (computer programming);library (computing);user interface	Alexandra Mikityuk;Oliver Friedrich;Randolph Nikutta	2015		10.1145/2745197.2755523	embedded system;real-time computing;engineering;operating system	Mobile	-34.39022402051091	59.25726271330705	47581
b2101e6c90cb448345905a828c7490adeb8d1fee	implementing web access control system for the multiple web servers in the same domain using rbac concept	information resources;web documents;access control web server permission environmental management resource management computer science sparks error correction network servers authentication;authorisation;web accessibility;role based access control;control system;internet;internet web access control system multiple web servers rbac hyper links role based access control access permission rights document content access;authorisation internet information resources;access control	As the Web server based system is being used more and more, having separate Web servers for each task to distribute the Web server's load are gaining much more popularity over having one main Web server to process all the tasks. When the user tries to access each Web server that contains a number of Web documents that are linked to each other via hyper-links within the domain, each Web server asks the user to follow the verification process even though the user is identical, and this prohibits the user from using the system efficiently. The role based access control method, which is the most suitable access control concept available now for the distributed Web server based system within the domain, is used in this paper. Additionally the method for controlling the level of Web document contents available to the user based on the user's access permission rights is introduced to reduce the granularity of the document content access.	control system;role-based access control	Won Bo Shim;Seog Park	2001		10.1109/ICPADS.2001.934896	web service;web application security;static web page;web development;the internet;data web;web mapping;clickstream;web design;web accessibility initiative;web standards;computer science;control system;access control;web api;web navigation;web log analysis software;web accessibility;web page;role-based access control;database;authorization;internet privacy;web 2.0;world wide web;computer security;web server;application server;server	Security	-36.5110874569966	57.932420418076426	47618
83649bf4222a6afb0927a2cc5b518677f9e44283	designing components for e-services	web application development.;wrapper;component;cooperation;e-service;legacy system;e-application;component model;distributed application;web application development;object oriented	Component based approaches are becoming more and more popular to support distributed application development. The concept of component itself, however, is not generally agreed upon and several definitions can be found. Moreover, different approaches to object oriented component modeling obtain different abstraction levels (conceptual vs. operational). In this paper, we discuss the concept of component in the framework of e-Service and e-Application design, where these services are based on legacy systems. We give a precise definition of stateful and stateless components, and we discuss their characteristics and their applicability in different stages of web application development.	distributed computing;e-services;legacy system;state (computer science);stateless protocol;web application development	Barbara Pernici;Massimo Mecella	2000			web application development;e-services;database;legacy system;object-oriented programming;computer science	SE	-37.58499939928279	39.49058111012725	47623
da2c2f88db72caa095212ade9258934b803bef1f	infection size as a measure of bug severity	bug severity;infection size;slicing;program trace	A simple bug in a program can influence a large part of the program execution by spreading throughout the state at runtime. This is known as program infection. The seriousness of bugs is usually measured by studying their external effects. However, such effects essentially derive from internal factors of a program. Our idea is to focus on internal factors, in particular the infection chain, to measure how serious a bug was. This allows reasoning about bugs from a new and potentially insightful perspective.	run time (program lifecycle phase);software bug	Mohammad R. Azadmanesh;Matthias Hauswirth	2015		10.1145/2823363.2823370	real-time computing;simulation;computer science;communication	PL	-59.691935839272226	42.409258997887164	47633
24b2f987c6a1b633df3a8ca970ce59a7259fd482	nozzle: a defense against heap-spraying code injection attacks	web pages;individual object;detection threshold;security and privacy;software development;runtime monitoring;success rate;code injection attack;false positive;static analysis	Heap spraying is a security attack that increases the exploitability of memory corruption errors in type-unsafe applications. In a heap-spraying attack, an attacker coerces an application to allocate many objects containing malicious code in the heap, increasing the success rate of an exploit that jumps to a location within the heap. Because heap layout randomization necessitates new forms of attack, spraying has been used in many recent security exploits. Spraying is especially effective in web browsers, where the attacker can easily allocate the malicious objects using JavaScript embedded in a web page. In this paper, we describe NOZZLE, a runtime heap-spraying detector. NOZZLE examines individual objects in the heap, interpreting them as code and performing a static analysis on that code to detect malicious intent. To reduce false positives, we aggregate measurements across all heap objects and define a global heap health metric. We measure the effectiveness of NOZZLE by demonstrating that it successfully detects 12 published and 2,000 synthetically generated heap-spraying exploits. We also show that even with a detection threshold set six times lower than is required to detect published malicious attacks, NOZZLE reports no false positives when run over 150 popular Internet sites. Using sampling and concurrent scanning to reduce overhead, we show that the performance overhead of NOZZLE is less than 7% on average. While NOZZLE currently targets heap-based spraying attacks, its techniques can be applied to any attack that attempts to fill the address space with malicious code objects (e.g., stack spraying [42]).	address space;aggregate data;code injection;embedded system;heap spraying;javascript;malware;memory corruption;overhead (computing);sampling (signal processing);static program analysis;web page	Paruj Ratanaworabhan;Benjamin Livshits;Benjamin G. Zorn	2009			type i and type ii errors;computer science;software development;operating system;web page;internet privacy;world wide web;computer security;static analysis	Security	-56.95441211852753	57.62030121916259	47757
19cabb9df2c4443fe08273d6c70bd75b34516030	personal safety triggering system on android mobile platform		Introduction of Smart phones redefined the usage of mobile phones in the communication world. Smart phones are equipped with various sophisticated features such as Wi-Fi, GPS navigation, high resolution camera, touch screen with broadband access which helps the mobile phone users to keep in touch with the modern world. Many of these features are primarily integrated with the mobile operating system which is out of reach to public, by which the users can’t manipulate those features. Google came up with an innovative operation system termed as ANDROID, which is open system architecture with customizable third party development and debugging environment which helps the user’s to manipulate the features and to create their own customizable applications. In this paper, ‘Emergency Based Remote Collateral Tracking System’ application using Google’s Android Mobile Platform is addressed. Emergency is divided into three categories: heart beat based emergency, security threats like personal safety and road accidents. This application is targeted to a person who is driving a vehicle. Heart rate monitoring device is integrated with our application to sense the heart beat of a person driving the vehicle and if there is any abnormalities in the heart beat, then our application performs a dual role. One in which, application uses a GPS to track the location information of the user and send those location information as a message via SMS, email and post it on Facebook wall Simultaneously, an emergency signal is sent to Arduino Microcontroller. Road accidents are quite common, this application is also designed to detect the accident using the sensors in the Android Mobile. Security threat can occur anywhere, our application also answers for personal safety, when the user interacts with the application by pressing the button, then automatically the application generates the geographical information and sends that location information via SMS and email to a pre-stored emergency contact and the same information will be posted on user’s Facebook wall. This application is written in JAVA programming language which runs on Eclipse Integrated Development Kit.	android;arduino;debugging;definition;eclipse;email;gps navigation device;global positioning system;image resolution;internet access;java;microcontroller;mobile operating system;mobile phone;programming language;sensor;systems architecture;touchscreen	Ashokkumar Ramalingam;Prabhu Dorairaj;Saranya Ramamoorthy	2012	CoRR		embedded system;operating system;internet privacy;computer security;computer network	Mobile	-40.98382821483699	54.606082923946126	47770
fcad02f244b6e833b6441a3e6f21bfffbbd92adc	cybersecurity and resilience modelling for software-defined networks-based manufacturing applications		In addition to productivity and quality output, for many years, manufacturing systems were also designed with reliability and safety requirements in mind. In the recent decade or so, that approach seems not to be adequate anymore. The current manufacturing global operations ask for more stringent requirements than ever before, which include privacy and security of transactions, among others. Manufacturing control is not new, but the use of cloud environments to integrate distributed manufacturing facilities and entirely control the production processes across those facilities is an active research area denoted in terms such as: virtual factory, cloud manufacturing, Industry 4.0, and more recently, software-defined networking-based manufacturing. Software-defined networking is a relatively new networking architecture that decouples the network data and control mechanisms and assigns the entire data control to a logically centralized control plane that can be software-programmed based on specific application needs. From the security point of view, this translates in the fact that anyone with access to the controllers that run the network control software could potentially control the entire network. This paper proposes an integrated modelling environment that addresses the manufacturing system assurance through cybersecurity and resilience mechanisms for software-defined networks-based (SDN-based) manufacturing applications. First, the paper presents the proposed combined cybersecurity-resilience ontology to be used in the requirements capture of the manufacturing network design stages. Then, the paper presents the framework for SDN-based cybersecurity-resilience mechanisms for manufacturing applications, and ends with a future research section concerning the proposed cybersecurity-resilience modelling environment.	computer security;cyber security standards	Radu F. Babiceanu;Remzi Seker	2016		10.1007/978-3-319-51100-9_15	reliability engineering;construction engineering;systems engineering;engineering	Robotics	-59.35272735277188	49.1231386290251	47804
69897c5b261a891047020f127df556ee0672211b	on technical security issues in cloud computing	ws security;internet economics;cloud computing security;security of data internet;cloud computing data security web and internet services application software national electric code europe environmental economics hardware costs licenses;cloud malware injection cloud computing security attacks tls ws security;servers;technical security issue cloud computing internet capital expenditure reduction operational expenditure reduction;internet;cryptography;attacks;xml;tls;cloud malware injection;technical security issue;capital expenditure reduction;simple object access protocol;meteorology;security of data;operational expenditure reduction;cloud computing	The Cloud Computing concept offers dynamically scalable resources provisioned as a service over the Internet. Economic benefits are the main driver for the Cloud, since it promises the reduction of capital expenditure (CapEx) and operational expenditure (OpEx). In order for this to become reality, however, there are still some challenges to be solved. Amongst these are security and trust issues, since the user's data has to be released to the Cloud and thus leaves the protection-sphere of the data owner. Most of the discussions on this topics are mainly driven by arguments related to organizational means. This paper focuses on technical security issues arising from the usage of Cloud services and especially by the underlying technologies used to build these cross-domain Internet-connected collaborations.	browser security;cloud computing security;end-user license agreement;platform as a service;relevance;scalability;social capital;software as a service;ws-security;web service;xml signature	Meiko Jensen;Jörg Schwenk;Nils Gruschka;Luigi Lo Iacono	2009	2009 IEEE International Conference on Cloud Computing	10.1109/CLOUD.2009.60	cloud computing security;xml;cloud computing;computer science;cryptography;operating system;internet privacy;world wide web;computer security	HPC	-49.30594178869125	57.98953694318443	47856
5bdbdfb787b3a87b2dd1f734eb45ee2574d59cdf	nash equilibrium-based semantic cache in mobile sensor grid database systems	semantics mobile communication database systems computer architecture mobile handsets optimization microprocessors;sensor grid database system game theory location dependent data ldd query nash equilibrium semantic cache	Mobile applications are being increasingly deployed on a massive scale in various mobile sensor grid database systems. With limited resources from the mobile devices, how to process the huge number of queries from mobile users with distributed sensor grid databases becomes a critical problem for such mobile systems. While the fundamental semantic cache technique has been investigated for query optimization in sensor grid database systems, the problem is still difficult due to the fact that more realistic multidimensional constraints have not been considered in existing methods. To solve the problem, a new semantic cache scheme is presented in this paper for location-dependent data queries in distributed sensor grid database systems. It considers multidimensional constraints or factors in a unified cost model architecture, determines the parameters of the cost model in the scheme by using the concept of Nash equilibrium from game theory, and makes semantic cache decisions from the established cost model. The scenarios of three factors of semantic, time, and locations are investigated as special cases, which improve existing methods. Experiments are conducted to demonstrate the semantic cache scheme presented in this paper for distributed sensor grid database systems.	algorithm;analysis of algorithms;database;experiment;game theory;hit (internet);lego digital designer;mathematical optimization;mobile device;nash equilibrium;query optimization;response time (technology);simulation	Qingfeng Fan;Karine Zeitouni;Naixue N. Xiong;Qiongli Wu;Seyit Camtepe;Yu-Chu Tian	2017	IEEE Transactions on Systems, Man, and Cybernetics: Systems	10.1109/TSMC.2016.2523949	sensor grid;mobile database;mobile search;architecture;database;cache;semantic grid;query optimization;computer science;cache algorithms;distributed computing	DB	-34.406861105489625	49.06583841504571	47925
4a281fadfd18e808ebc2ca740f032177491fe9da	on conditions for self-healing in distributed software systems	electronic commerce;software systems;software fault tolerance;satisfiability;software engineering;software systems fires infrared sensors conferences space heating computer science biological systems optical computing propulsion terminology;complex system;biological systems;enterprise system self healing distributed software system self repair system heterogeneous distributed software system configuration failure invariant system biological system software behavior electronic purchasing;enterprise system;configuration management;software fault tolerance configuration management electronic commerce software engineering	This paper attempts to identify one of the necessary conditions for self-healing, or self-repair, in complex systems, and to propose means for satisfying this condition in heterogeneous distributed software. The condition identified here is the following: For a system with a wide and open range of possible configurations to be self healing, it must possess suitable regularities, which can be relied upon to be satisfied by all possible configurations of the system, and which must be invariant of its failures. We observe that self healing in physical artifacts, as well as in biological systems, are largely based on regularities engendered by the laws of nature. But since laws of nature have no effective sway over the behavior of software, we propose means for imposing artificial laws over a given distributed system, which are designed to induce desired regularities in them. We demonstrate the efficacy of the proposed approach by applying it to a simple example of electronic purchasing in enterprise systems.	artifact (software development);biological system;complex systems;distributed computing;enterprise system;purchasing;software system;three laws of robotics	Naftaly H. Minsky	2003		10.1109/ACW.2003.1210208	reliability engineering;verification and validation;software sizing;software configuration management;systems engineering;engineering;social software engineering;software framework;component-based software engineering;software development;software design description;middleware;software construction;systems development life cycle;resource-oriented architecture;software deployment;software requirements;software metric;software system;computer engineering	SE	-49.36711674881566	37.62103186503765	48001
1aca24bfe2890fb5601dbf6f5c2a05c6222f27b5	the performance estimation of the situation awareness rfid system from ubiquitous environment scenario	utilisation information;evaluation performance;uso informacion;architecture systeme;informatique mobile;calculateur embarque;performance evaluation;information use;performance estimation;pervasive computing;evaluacion prestacion;informatica difusa;captador medida;measurement sensor;capteur mesure;identification radiofrequence;informatique diffuse;inferencia;boarded computer;situation awareness;radio frequency identification;arquitectura sistema;system architecture;mobile computing;calculador embarque;inference	Many sensors providing situation data will be in everywhere under the ubiquitous environment. It requires the current RFID system should be extended to recognize and use situation information from the sensors. We already proposed a new RFID system architecture that is suitable for the coming ubiquitous environment, and basically consists of the key components such as the inference engine, use policy, and definition language. It has four alternatives (types) depending on the role distributions to the components. This paper shows the performance evaluations of the alternatives. Using the research results, we can select a proper RFID system architecture for corresponding ubiquitous application. The contributions of this paper are as follow: providing a new RFID system architecture to recognize, analyze, and utilize the situation information; defining the four alternatives for ubiquitous applications.	inference engine;radio-frequency identification;sensor;systems architecture	Dongwon Jeong;Heeseo Chae;Hoh Peter In	2005		10.1007/11596356_98	radio-frequency identification;embedded system;situation awareness;simulation;telecommunications;computer science;operating system;mobile computing;ubiquitous computing	HCI	-37.39051565826117	47.29804128135027	48006
e55046cbe05d0e9c86835e48427df5b85cb82c24	economic incentives on dnssec deployment: time to move from quantity to quality		The security extensions to the DNS (DNSSEC) currently cover approximately 3% of all domains worldwide. In response to the low deployment of DNSSEC, a few top-level domains started offering u0027per-domainu0027 economic incentives to encourage adoption of the protocol by offering a yearly discount on each signed domain. However, it remains unclear whether these incentives are well-balanced and foster the overall security of the infrastructure as well as its deployment at scale. In this paper we argue that, in the presence of fixed costs of deployment, misaligned u0027per-domainu0027 incentives may have the collateral effect of encouraging large operators to massively deploy unsecure implementations of DNSSEC, whereas smaller operators, for which the effect of the economic incentive is negli­gible, may not significantly benefit from it. To investigate this, we study the security of DNSSEC deployment at scale, particularly in TLDs that offer economic incentives. We find that the security of DNSSEC implementations in the wild poorly reflects standard recommendations, particularly for tasks that cannot be solved by triggering a flag in the DNS software service (e.g. key rollover). Further, we find that, on average, large operators deploy weak DNSSEC security more frequently than small DNSSEC operators, suggesting that current incentives are ineffective in promoting a secure adoption and in deterring insecure implementations. We conclude the paper with actionable recommendations for TLD registry operators to improve the alignment of economic incentives with secure DNSSEC requirements.	best practice;domain name system security extensions;internet;requirement;rollover (key);software as a service;software deployment;speeded up robust features	Tho Le;Roland van Rijswijk-Deij;Luca Allodi;Nicola Zannone	2018	NOMS 2018 - 2018 IEEE/IFIP Network Operations and Management Symposium	10.1109/NOMS.2018.8406223	fixed cost;collateral;implementation;computer science;computer network;computer security;cryptography;software deployment;network security;incentive;service (systems architecture)	Security	-50.20894565650941	59.339917122860626	48010
ae54bcdb0b18e392b77356308b13b71deaa2440d	detection and classification of malicious javascript via attack behavior modelling	malicious javascript;l;behavior modelling;malware detection	Existing malicious JavaScript (JS) detection tools and commercial anti-virus tools mostly use feature-based or signature-based approaches to detect JS malware. These tools are weak in resistance to obfuscation and JS malware variants, not mentioning about providing detailed information of attack behaviors. Such limitations root in the incapability of capturing attack behaviors in these approches. In this paper, we propose to use Deterministic Finite Automaton (DFA) to abstract and summarize common behaviors of malicious JS of the same attack type. We propose an automatic behavior learning framework, named JS*, to learn DFAs from dynamic execution traces of JS malware, where we implement an effective online teacher by combining data dependency analysis, defense rules and trace replay mechanism. We evaluate JS* using real world data of 10000 benign and 276 malicious JS samples to cover 8 most-infectious attack types. The results demonstrate the scalability and effectiveness of our approach in the malware detection and classification, compared with commercial anti-virus tools. We also show how to use our DFAs to detect variants and new attacks.	antivirus software;attack model;behavior model;data dependency;dependence analysis;deterministic finite automaton;javascript;malware;obfuscation (software);out-of-order execution;scalability;tracing (software)	Yinxing Xue;Junjie Wang;Yang Liu;Hao Xiao;Jun Sun;Mahinthan Chandramohan	2015		10.1145/2771783.2771814	l;computer science;cryptovirology;internet privacy;world wide web;computer security	SE	-57.85195064441506	58.581776029655536	48031
3f80ce5efa0dc64666416bda7b1860a15f8337c6	ubiquitous information provision in the vehicle domain	computers;software;vehicle domain;protocols;context aware;software prototyping;intelligent transport system;intelligent transportation systems;cellular radio;cellular networks;telecommunication traffic intelligent transportation systems intelligent vehicles protocols ad hoc networks monitoring advertising cellular networks context software prototyping;ubiquitous information provision;information sharing;software requirements;location based notifications;indium tin oxide;telecommunication traffic;servers;monitoring;hardware software requirements ubiquitous information provision vehicle domain information sharing intelligent transportation systems vehicular ad hoc networks location based notifications peer to peer networks cellular networks context aware information;roads;hardware software requirements;vehicular ad hoc networks;intelligent vehicles;context aware information;communication protocol;ad hoc networks;vehicular ad hoc network;ubiquitous computing;vehicles;p2p networks;peer to peer computing;ubiquitous computing ad hoc networks cellular radio peer to peer computing;peer to peer;peer to peer networks;context;hardware;advertising	Information sharing and provision in the vehicular field is nowadays a growing research topic in Intelligent Transportation Systems (ITS). Although a lot of work is being carried out in these terms, the most of the approximations are based on local solutions which solve a particular problem. Communication protocols are proposed to exchange information among vehicles in vehicular ad-hoc networks (VANETs), monitoring centres provide information about traffic, advertising mechanisms send location-based notifications about local shops. The system presented in this paper proposes an integrated infrastructure for information sharing in both local and global schemes. Using a novel communication paradigm based on cellular and peer to peer (P2P) networks, traffic information is shared among close vehicles and sent to a core infrastructure for a global processing. This centralized system is able to provide not only traffic events, but also general context-aware information adapted to the userpsilas preferences. The complete design has been developed over a whole prototype, considering both hardware and software requirements.	approximation;centralized computing;hoc (programming language);peer-to-peer;programming paradigm;prototype;requirement;software requirements	José Santa;Antonio Moragón;Antonio F. Gómez-Skarmeta	2008	2008 3rd International Symposium on Wireless Pervasive Computing	10.1109/ISWPC.2008.4556255	computer science;distributed computing;vehicular communication systems;computer security;computer network	Mobile	-38.9468805539091	50.11787804439789	48074
acab5362fd665c8bacda800d2291be27a13da00b	an architecture for runtime customization of smart devices	town and country planning;intelligent actuators;software computer architecture sensors hardware androids humanoid robots libraries;software engineering;embedded systems;town and country planning embedded systems intelligent actuators intelligent sensors software engineering;intelligent sensors;device repurposing smart sensors heterogeneous platforms;data management smart device runtime customization architecture relatively uncharted ict territory actuator devices sensor devices value added services smart cities high degree device heterogeneity software design innovative architecture plug in based design modular design low level duties high level duties device discovery code compilation binary deployment service composition	Smart environments represent a relatively uncharted ICT territory where plenty of sensor and actuator devices can be enrolled on-demand in order to realize high value-added services. A few application scenarios, such as Smart Cities, have already been explored. However, in order to finally enable such a paradigm, several issues have to be dealt with. In particular, from a developer perspective the high degree of heterogeneity for devices (ranging from cheap sensors to smart phones) could represent a hurdle for software design. In this paper, we present an innovative architecture that aims at providing a common reference platform for repurposing of devices i.e., reshaping their operational behavior for emergent and unforeseen requirements. Thanks to its modular and plugin based design, the proposed architecture is poised to ease implementation of both low-level (e.g., device discovery, code compilation, binary deployment) and high-level (e.g., service composition, data management) duties. We present the general architecture, then focusing on device-side aspects, while also providing two simple use cases that demonstrate the suitability of the proposed approach.	compiler;emergence;high- and low-level;programming paradigm;requirement;sensor;service composability principle;smart city;smart device;smart environment;smartphone;software deployment;software design	Maria Fazio;Giovanni Merlino;Dario Bruneo;Antonio Puliafito	2013	2013 IEEE 12th International Symposium on Network Computing and Applications	10.1109/NCA.2013.39	reference architecture;embedded system;simulation;computer science;operating system;internet of things;intelligent sensor	Mobile	-42.540436765754826	46.50525444263828	48133
f7e33255ca10e633ab9d443004113800b1c3fdfb	iot mashup as a service: cloud-based mashup service for the internet of things	mashups computational modeling computer architecture unified modeling language adaptation models;vacant room detection application iot mashup as a service internet of things environment cloud based iot mashup service model web mashup technology web service heterogeneous devices computation scalability model driven architecture principles cloud computing paradigm;web services cloud computing internet of things software architecture;internet of things;software architecture;model driven architecture internet of things service modeling cloud computing;web services;service modeling;model driven architecture;cloud computing	Mashup, a way to compose a new service from existing services, is expected to play a great role in internet of things (IoT) environment. Many recent researches suggest that mashup in IoT environment is possible with existing web mashup technology if each thing exposes its functionalities as a web service. However, this approach may have limitations in dealing with many heterogeneous devices and computation scalability in the presence of large number of things involved. In this paper, we propose a cloud-based IoT mashup service model, called IoT Mashup as a Service (IoTMaaS), to overcome heterogeneity of devices by following the model driven architecture principles and computational scalability based on cloud computing paradigm. We also design a cloud platform on which IoTMaaS be executed in harmony with stakeholders such as end users, device manufacturers, and cloud computing providers. We proved the concept of the architecture by implementing a prototype platform with an vacant room detection application.	cloud computing;computation;constraint logic programming;ecosystem;internet of things;mashup (web application hybrid);model-driven architecture;programming paradigm;prototype;run time (program lifecycle phase);scalability;stream processing;web service	Janggwan Im;Seong Hoon Kim;Daeyoung Kim	2013	2013 IEEE International Conference on Services Computing	10.1109/SCC.2013.68	web service;computer science;database;internet privacy;world wide web;mashup	HPC	-43.69101882265475	44.02295475394527	48173
f689676bc2e4f609b551753eaa5d4d8e1875f4e0	leveraging short-lived social networks in vehicular environments	leveraging short lived social networks mobile devices ad hoc connections physical environment virtual world internet web 2 0 vehicular environments;social networking online internet mobile computing;internet;social networking online;mobile computing;vehicles social network services ad hoc networks mobile communication mobile handsets cloud computing	The interactions enabled by the popular sites of the Web 2.0 are largely confined to the virtual world of the Internet, thus failing to engage people in relevant interactions with people, contents or resources in their physical environment. In this paper, we motivate the potential of automatically establishing sporadic social networks among people (acquaintances or strangers) who happen to be physically close to one another at a certain moment. We present the design of one platform intended to provide solutions from the lowest level of establishing ad-hoc connections among nearby mobile devices, up to the highest level of automatically identifying the most relevant pieces of information to deliver at any time. Specific scenarios are discussed considering vehicular environments, as well as the results from pilot experiments with a representative application.	experiment;failure;hoc (programming language);interaction;internet;mathematical optimization;mobile device;social network;virtual world;web 2.0;world wide web	Jack Fernando Bravo-Torres;Martín López Nores;Yolanda Blanco-Fernández;José Juan Pazos-Arias	2013	Second International Conference on Future Generation Communication Technologies (FGCT 2013)	10.1109/FGCT.2013.6767199	mobile search;the internet;mobile web;engineering;delay-tolerant networking;internet privacy;mobile computing;world wide web;computer network	HCI	-40.0693889789086	53.307798850807394	48179
1b3589c70186d3492f1277c63a3380c54ba23799	test suite reduction in conformance testing	conformance testing	Conformance testing is based on a test suite. Standardization committees release standard test suites, which consist of hundreds of test cases. The main problem of conformance testing is that we do not have enough time to execute them all. Therefore, test selection is required to maximize the test coverage. In our earlier papers [6,7] we outlined a new method of selecting an optimal test suite which can detect the errors with better probability and reduce the time of testing. In this paper we will expound the mathematical optimization method for test suite optimization based on cost and test coverage, and we will apply this method to an ISDN protocol.	conformance testing;fault coverage;integrated services digital network;mathematical optimization;test case;test suite	Tibor Csöndes;Sarolta Dibuz;Balázs Kotnyek	1999	Acta Cybern.		computer science;conformance testing	SE	-59.57717929088458	33.08235942336019	48207
925b8ca5720dd56693dca8512e280892ade4f3f7	predictive algorithms for mobility and device lifecycle management in cyber-physical systems	signal image and speech processing;information systems applications incl internet;telecomunicaciones;communications engineering networks	Cyber-Physical Systems (CPS) are often composed of a great number of mobile, wireless networked devices. In order to guarantee the system performing, management policies focused on becoming transparent to high-level applications, the changes in the hardware platform have to be implemented. However, traditional reactive methodologies and basic proposed predictive solutions are not valid either due to the extremely dynamical behavior of CPS or because the high number of involved devices prevents fulfill the timing requirements. Therefore, in this paper, we present an advance predictive solution for managing the mobility and device lifecycle, being able to meet all requirements of CPS. The solution is based on an infinite loop, which calculates, in each iteration, a sequence of future system states using a CPS simulator and interpolation algorithms. Furthermore, an experimental validation is provided in order to determine the performing of the proposed solution.	algorithm;cyber-physical system;dynamical system;high- and low-level;infinite loop;interpolation;iteration;requirement;simulation	Borja Bordel Sánchez;Ramón Alcarria;Diego Sánchez de Rivera;Álvaro Sánchez-Picot	2016	EURASIP J. Wireless Comm. and Networking	10.1186/s13638-016-0731-0	real-time computing;simulation;telecommunications;computer science;operating system;cyber-physical system;computer network	Embedded	-39.75668682446081	36.87879532574998	48243
a0912a620f1cdaf0ba8919e0d68a6113df6af10a	a novel method makes concolic system more effective		Fuzzing is attractive for finding vulnerabilities in binary programs. However, when the application's input space is huge, fuzzing cannot deal with it well. For discovering vulnerabilities more effective, researchers came up concolic testing, and there are much researches on it recently. A common limitation of concolic systems designed to create inputs is that they often concentrate on path-coverage and struggle to exercise deeper paths in the executable under test, but ignore to find those test cases which can trigger the vulnerabilities. In this paper, we present TSM, a novel method for finding potential vulnerabilities in concolic systems, which can help concolic systems more effective for hunting vulnerabilities. We implemented TSM method on a wide-used concolic testing tool-Fuzzgrind, and the evaluation experiments show that TSM can make Fuzzgrind hunt bugs quickly in real-world software, which are hardly found ever before.	code coverage;concolic testing;dangling pointer;executable;experiment;ibm spectrum protect (tivoli storage manager);shadow mapping;software bug;test automation;test case;vulnerability (computing)	Hongliang Liang;Zhengyu Li;Minhuan Huang;Xiaoxiao Pei	2017	2017 IEEE 4th International Conference on Cyber Security and Cloud Computing (CSCloud)	10.1109/CSCloud.2017.43	computer security;computer science;software bug;software;concolic testing;executable;test case;fuzz testing	SE	-57.779897759426355	56.19880235469367	48262
bb4f9b57aa6dc14686579b104319423585196681	required information release	algorithmic knowledge;information release;declassification;information flow	Many computer systems have a functional requirement to release information. Such requirements are an important part of a system's information security requirements. Current information-flow control techniques are able to reason about permitted information flows, but not required information flows.In this paper, we introduce and explore the specification and enforcement of required information release in a language-based setting. We define semantic security conditions that express both what information a program is required to release, and how an observer is able to learn this information. We also consider the relationship between permitted and required information release, and define bounded release, which provides upper-and lower-bounds on the information a program releases. We show that both required information release and bounded release can be enforced using a security-type system.		Stephen Chong	2012	Journal of Computer Security	10.3233/JCS-2012-0442	simulation;information flow;computer science;data mining;computer security	Crypto	-53.67399809804176	52.06617661222053	48274
a7d7a069e9d18094ddfbf733260236613acdb14e	virtual platforms and timing analysis: status, challenges and future directions	verification;automotive engineering;distributed system;constraint optimization;timing computer architecture performance analysis automotive engineering constraint optimization delay software standards standards development predictive models application software;application software;real time;complex distributed systems virtual platforms timing analysis automotive architecture osek autosar software architecture;virtual platforms;osek;real time embedded system;software architecture automotive engineering large scale systems;embedded system;computer architecture;embedded systems;software architecture;standards development;complex distributed systems;performance analysis;embedded systems design verification design methodologies real time;model based development;timing analysis;design;predictive models;software standards;design methodologies;autosar;automotive architecture;quantitative evaluation;large scale systems;design methodology;timing	This paper outlines a methodology based on virtual platforms and timing analysis to perform the exploration and selection of automotive architecture solutions. The analysis provides a quantitative evaluation of architecture options with respect to para-functional metrics. The satisfaction of deadline constraints and the optimization with respect to latency on end-to-end computations is considered. In addition, we discuss to what degree existing standards, including OSEK and AUTOSAR and model-based development practices can support the development of time predictable software and what is required to move toward the desirable goal of timing isolation when integrating multiple applications into the same execution platform. Finally, the paper provides a quick glance at recent results in the the optimization of the software architecture for complex distributed systems with respect to worst case timing behavior.	autosar;best, worst and average case;computation;distributed computing;end-to-end principle;mathematical optimization;model-driven engineering;osek;software architecture;static timing analysis;virtual machine	Marco Di Natale	2007		10.1145/1278480.1278620	embedded system;software architecture;design;constrained optimization;computer architecture;application software;real-time computing;verification;computer science;engineering;operating system;model-based design	Embedded	-39.96504061748966	34.72597206717168	48302
895fe0669d6eefc725aae1760e4abf505649573d	toward systematic design of fault-tolerant systems	fault tolerant;fault tolerant systems nasa computer errors space vehicles concurrent computing error correction codes reliability engineering laboratories fault tolerance rails;chip;fault tolerant system;fault tolerant computing;technological forecasting fault tolerant computing microprocessor chips;chip manufacturers systematic design fault tolerant systems high confidence computing critical applications ubiquitous computing computing speed quality of life fault tolerant microprocessors competitive price;microprocessor chips;technological forecasting	"""A s computing and communications become irreplaceable tools of modern society, one fundamental principle emerges: The greater the benefits these systems bring to our well-being and quality of life, the greater the potential for harm when they fail to perform their functions or perform them incorrectly. Consider air, rail, and automobile traffic control; emergency response systems; airline flight controls; nuclear power plant safety systems; and most of all, our rapidly growing dependence on health care delivery via high-performance computing and communications. When these systems fail, lives and fortunes may be lost. At the same time, threats to dependable operation are growing in scope and severity. Leftover design faults (bugs and glitches) cause system crashes during peak demand, resulting in service disruptions and financial losses. Complex systems suffer stability problems due to unforeseen interactions of overlapping fault events and mismatched defense mechanisms. Hackers and criminally minded individuals invade systems, causing disruptions, misuse, and damage. Accidents result in severed communication links, affecting entire regions. Finally, we face the possibility of systems damage by """" info-terrorists. """" Fault tolerance is our best guarantee that high-confidence systems will not betray the intentions of their builders and the trust of their users by succumbing to physical, design, or human-machine interaction faults, or by allowing viruses and malicious acts to disrupt essential services. I originally formulated the concept of fault tolerance in 1967: """" We say that a system is fault-tolerant if its programs can be properly executed despite the occurrence of logic faults.'' 1 The fault tolerance concept resulted from three converging developments. First, the earliest use of computers made it apparent that even with careful design and good components, physical defects and design errors were unavoidable. Thus, designers of early computers used practical techniques to increase reliability: They used redundant structures to mask failed components; error-control codes and duplication or triplication with voting to detect or correct information errors; diagnostic techniques to locate failed components; and automatic switchovers to replace failed subsystems. 2 Second, in parallel with these evolving engineering techniques, computing pioneers such as John von Neumann 3 and Edward F. Moore and Claude E. Shannon 4 addressed the general problem of building reliable systems from unreliable components. William H. Pierce unified their theories of masking redundancy and incorporated some others. 5 Third, in 1958 NASA challenged Caltech's Jet Propulsion Laboratory to build unmanned spacecraft for interplanetary exploration. These missions would last 10 …"""	causality;code;complex systems;computer;edward f. moore;emergence;emergency response systems;error detection and correction;fault tolerance;glitch;human–computer interaction;malware;pierce oscillator;shannon (unit);software bug;supercomputer;unmanned spacecraft	Algirdas Avizienis	1997	IEEE Computer	10.1109/2.585154	embedded system;technology forecasting;fault tolerance;real-time computing;telecommunications;computer science;operating system;software engineering;computer security;software fault tolerance	HPC	-59.76245686553483	48.6103514977017	48391
42079779485e7213adac2d9c9a3e1ed5d34a532f	dynamic symbolic database application testing	dynamic symbolic execution;test case generation	A database application differs form regular applications in that some of its inputs may be database queries. The program will execute the queries on a database and may use any result values in its subsequent program logic. This means that a user-supplied query may determine the values that the application will use in subsequent branching conditions. At the same time, a new database application is often required to work well on a body of existing data stored in some large database. For systematic testing of database applications, recent techniques replace the existing database with carefully crafted mock databases. Mock databases return values that will trigger as many execution paths in the application as possible and thereby maximize overall code coverage of the database application.  In this paper we offer an alternative approach to database application testing. Our goal is to support software engineers in focusing testing on the existing body of data the application is required to work well on. For that, we propose to side-step mock database generation and instead generate queries for the existing database. Our key insight is that we can use the information collected during previous program executions to systematically generate new queries that will maximize the coverage of the application under test, while guaranteeing that the generated test cases focus on the existing data.	code coverage;database;mock object;software engineer;software testing;system under test;test case	Chengkai Li;Christoph Csallner	2010		10.1145/1838126.1838133	cost database;database theory;intelligent database;rollback;database tuning;computer science;theoretical computer science;database model;data mining;database;view;database schema;consistency;physical data model;alias;database testing;database design	DB	-57.156398061603156	39.941220136050674	48498
9f79b05b8555960ac9e960d38e6afe6d3c0f3bf6	a 7-layer model for modernizing the world: a step towards a hi-tech world	databases;neodymium;e services;e governance;7 layer model;information retrieval;mobile agents;government;mobile agent technology;information management peer to peer computing management information systems mobile agents environmental management quality management technology management engineering management web and internet services government;hexadecimal id;type of service;fault tolerant system 7 layer model mobile agent hexadecimal id e governance pmade hi tech world information retrieval;servers;fault tolerant system;fault tolerant computing;graphical user interfaces;information management;e services hi tech world e governance information management pmade mobile agent technology;pmade;hi tech world;cities and towns;mobile agent;mobile agents fault tolerant computing government data processing information retrieval;government data processing	In this paper we present a 7-layer model which consists of peer systems arranged logically in a hierarchical fashion. Every layer of the system is integrated with PMADE[9] and mobile agents [8]. This system is integrated with at least seven agents one at each layer. This system allocates 17 digit hexadecimal ID to every individual who is the citizen of the country which is a member of this system. It provides two types of services to every citizen. First type of service is shared between the citizens of the entire system provided at country layer and other type of service is shared between the citizens of a single country provided at state layer.	email;fault tolerance;hexadecimal;information retrieval;internet;mobile agent;osi model;scalability;type of service	Rohit Vaid;R. B. Patel	2009	2009 International Conference on Advances in Recent Technologies in Communication and Computing	10.1109/ARTCom.2009.103	computer science;knowledge management;operating system;type of service;mobile agent;neodymium;information management;world wide web;computer security;government;computer network	Robotics	-50.733945887902955	45.461875794113524	48516
0aeb656deaf19cf3130dad60cf234d851a00849b	modeling timing requirements in problem frames using ccsl	timing requirements;process integration;specification language;embedded system;embedded systems requirements engineering timing requirements problem frames approach ccsl;requirements engineering;embedded systems;formal verification;requirement engineering;clock constraint specification language timing requirements modeling problem frames ccsl requirements engineering pf modeling and analysis of real time and embedded systems marte;clocks timing unified modeling language integrated circuits sensors embedded systems;ccsl;problem frames approach;problem frame;modeling and analysis;formal verification embedded systems;real time and embedded systems;time constraint	As the embedded systems are becoming more and more complex, requirements engineering approaches are needed for modeling requirements, especially the timing requirements. Among various requirements engineering approaches, the Problem Frames(PF) approach is particularly useful in requirements modeling for the embedded systems due to the characteristic that the PF pays special attention to the environment entities that will interact with the to-be software. However, no concern is given on timing requirements of the PF at present. This paper studies how to add timing constraints on problem domains in the PF. Our approach is to integrate the problem representation frame in the PF with the timing representation mechanism of MARTE(Modeling and Analysis of Real Time and Embedded systems). A unified problem frame modeling process integrated with timing constraints is provided, and problem frame requirements with timing constraints expressed by MARTE/CCSL(Clock Constraint Specification Language) and clock construction operators are obtained.	cyber-physical system;diagram;embedded system;entity;frame language;frame problem;functional requirement;instant messaging;modeling and analysis of real time and embedded systems;problem domain;problem frames approach;requirements analysis;requirements engineering;scalability;specification language	Xiaohong Chen;Weiping Li;Frédéric Mallet;Zhi Jin	2011	2011 18th Asia-Pacific Software Engineering Conference	10.1109/APSEC.2011.30	embedded system;real-time computing;specification language;formal verification;computer science;systems engineering;requirements engineering;programming language;process integration	Embedded	-40.49970156114365	32.8232230121131	48540
418ebbf01444bfad5d50d3b6e72969d62ceaf92c	an object-oriented architecture for sensorless cutting force feedback for cnc milling process monitoring and control	intelligent manufacturing system;cycle time;fault tolerant;object oriented software design;intelligent manufacturing systems;object oriented software;data representation;sensorless feedback;software architecture;control system;process monitoring;setup time;object oriented;unified modeling language;cutting force	Intelligent Manufacturing Systems require fault-tolerant components capable of preventing and detecting failures, and optimizing their own performance. Although there are a number of process monitoring and control systems (PMCS) available for these tasks, most of them require special equipment that increases cost and setup time. This work proposes a standard software architecture and methodology for implementing a sensorless PMCS which separates data representation from its algorithmic processing. Two different case studies are introduced showing the feasibility and applicability of the proposed design, one for cycle time optimization and another one for online tool condition detection. A good performance was obtained in both case studies based on the proposed PMCS architecture, which can be implemented using different distributed software technologies. 2009 Elsevier Ltd. All rights reserved.	algorithm;common object request broker architecture;control system;data (computing);distributed component object model;distributed computing;fault tolerance;flip-flop (electronics);haptic technology;hoc (programming language);mathematical optimization;sensor;software architecture;software portability	Roberto Augusto Gómez Loenzo;Pedro Daniel Alaniz Lumbreras;René de Jesús Romero-Troncoso;Gilberto Herrera Ruiz	2010	Advances in Engineering Software	10.1016/j.advengsoft.2009.12.016	control engineering;unified modeling language;reference architecture;embedded system;software architecture;fault tolerance;real-time computing;cycle time variation;computer science;engineering;control system;operating system;external data representation;programming language;object-oriented programming	SE	-35.00780033049181	37.29514905538651	48554
1af84cbf91b5d45dfd3c370898d135f7ef70b797	detection and modeling of cyber attacks with petri nets	behavioral analysis;cyber attack;malware;colored petri net;malware detection	The aim of this article is to present an approach to develop and verify a method of formal modeling of cyber threats directed at computer systems. Moreover, the goal is to prove that the method enables one to create models resembling the behavior of malware that support the detection process of selected cyber attacks and facilitate the application of countermeasures. The most common cyber threats targeting end users and terminals are caused by malicious software, called malware. The malware detection process can be performed either by matching their digital signatures or analyzing their behavioral models. As the obfuscation techniques make the malware almost undetectable, the classic signature-based anti-virus tools must be supported with behavioral analysis. The proposed approach to modeling of malware behavior is based on colored Petri nets. This article is addressed to cyber defense researchers, security architects and developers solving up-to-date problems regarding the detection and prevention of advanced persistent threats.	antivirus software;digital signature;electronic signature;malware;petri net	Bartosz Jasiul;Marcin Szpyrka;Joanna Sliwa	2014	Entropy	10.3390/e16126602	computer science;cryptovirology;malware;internet privacy;cyberwarfare;world wide web;computer security	Security	-60.96799277095588	60.31104086777016	48560
5366c5e74bca353344b8ea27bff9132f4cf5d238	validation of soc firmware-hardware flows: challenges and solution directions	debug;ip networks system on chip protocols emulation field programmable gate arrays security silicon;simulation;emulation;intel soc firmware hardware flows infrastructure backbone flows ip firmware hardware interaction power management hardware validation techniques time to market requirements mobile devices;fpga;system on chip firmware hardware software codesign mobile handsets;formal analysis validation simulation emulation fpga virtual platform hardware software co validation soc flows debug;flows;soc;validation;virtual platform;formal analysis;hardware software co validation	In SoC, key infrastructure/backbone flows are distributed across many IPs and involve tight firmware and hardware interaction. Examples include resets, power management, security, and more. Traditional hardware validation techniques are no-longer adequate for such flows, due to the short time-to-market requirements, in particular, for mobile devices. In this paper, we articulate the challenges and discuss a few solution directions that are being pursued in this space at Intel.	aspect-oriented programming;firmware;formal system;global serializability;internet backbone;mobile device;power management;requirement;semantics (computer science);software framework;transform, clipping, and lighting	Yael Abarbanel;Eli Singerman;Moshe Y. Vardi	2014	2014 51st ACM/EDAC/IEEE Design Automation Conference (DAC)	10.1145/2593069.2596692	system on a chip;embedded system;emulation;computer architecture;real-time computing;computer science;operating system;debugging;post-silicon validation;fpga prototype;field-programmable gate array;hardware emulation	EDA	-37.58481067456336	46.76189554568191	48591
7127d3ffa3512e11ffb8dd71c05d7615f99064b8	improving fault localization for simulink models using search-based testing and prediction models	fault localization;debugging;testing;computational modeling;simulink models;search based testing;ranking statistics;predictive models;adaptation models;software packages	One promising way to improve the accuracy of fault localization based on statistical debugging is to increase diversity among test cases in the underlying test suite. In many practical situations, adding test cases is not a cost-free option because test oracles are developed manually or running test cases is expensive. Hence, we require to have test suites that are both diverse and small to improve debugging. In this paper, we focus on improving fault localization of Simulink models by generating test cases. We identify three test objectives that aim to increase test suite diversity. We use these objectives in a search-based algorithm to generate diversified but small test suites. To further minimize test suite sizes, we develop a prediction model to stop test generation when adding test cases is unlikely to improve fault localization. We evaluate our approach using three industrial subjects. Our results show (1) the three selected test objectives are able to significantly improve the accuracy of fault localization for small test suite sizes, and (2) our prediction model is able to maintain almost the same fault localization accuracy while reducing the average number of newly generated test cases by more than half.	debugging;embedded system;kerrison predictor;overhead (computing);requirement;search algorithm;simulink;test case;test suite	Bing Liu;Lucia;Shiva Nejati;Lionel C. Briand	2017	2017 IEEE 24th International Conference on Software Analysis, Evolution and Reengineering (SANER)	10.1109/SANER.2017.7884636	reliability engineering;real-time computing;model-based testing;simulation;fault coverage;engineering;automatic test pattern generation;test case;test management approach	SE	-60.46127114310747	35.82350320053854	48603
b2eb7f9911aa5095d85360b462b4faee98924051	characterization of mobile applications according to their energy consumptions	software;androids;energy profiler android programming mobile computing;internet;humanoid robots;global positioning system;mobile communication;global positioning system internet conferences mobile communication androids humanoid robots software;conferences	"""The use of smart phones and similar devices are increasing day by day and mobile devices are moving towards becoming the mainstream computing hardware. In parallel, the number of mobile applications is increasing exponentially. Besides the many advantages of being mobile, in these applications limited battery power is a fundamental constraint. With """"Energy Aware"""" programming techniques, the new generation of software developers are willing to spread the battery life time as long as possible. In this study, trends of energy consumption are examined for various types of mobile software in terms of software component. For this purpose, The energy profile PowerTutor software is used and without the use of additional hardware, details of energy consumption figures are reported. For selected types of mobile software, energy consumption ratio of GSM radio, processor, screen, sound system and GPS components is examined and a general characterization is sought about these applications."""	component-based software engineering;computer hardware;energy profile (chemistry);global positioning system;mobile app;mobile device;smartphone;software developer	Ozgun Pinarer;Atay Ozgovde	2014	2014 22nd Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2014.6830649	embedded system;real-time computing;mobile search;the internet;simulation;mobile web;mobile telephony;global positioning system;mobile processor;computer science;humanoid robot;mobile technology;mobile station;mobile computing	SE	-37.86342834923973	54.12447210967897	48651
9577902fca7ed4e21e0af34e89d1021298f43e71	agentbase--a framework for handling multiple agents	platform;group communication;computer networks;limited communications agentbase multiple agent handling autonomous mobile agents electronic sales agents network management agents computer networks application programs multiple autonomous agents unlimited communications;computer network;multi agent systems;autonomous agent;computer network management application software mobile agents marketing and sales autonomous agents;computer networks mobile computing multi agent systems;network management;mobile agent;mobile computing;protocol	Recently, autonomous mobile agents, such as electronic sales agents and network management agents, which move around computer networks and perform tasks autonomously, are being used in various elds. There are also application programs which collect information held by multiple agents and make use of it e ciently. In these cases, it is necessary for application programs to handle multiple agents together, but it is very complicated and di cult for applications to implement this. In this paper, we propose Agentbase, a framework for handling multiple autonomous agents both for limited and unlimited communications.	autonomous robot;mobile agent	Chihiro Ono;Satoshi Nishiyama;Sadao Obana	1999		10.1109/ASAMA.1999.805414	network management;agent architecture;protocol;communication in small groups;autonomous agent;mobile agent;platform;mobile computing;intelligent agent;monitoring and surveillance agents	AI	-34.55425092535793	46.033256558309084	48676
56fee770f9bd3b85360210d6b9fb0c9db8da4311	checking running and dormant virtual machines for the necessity of security updates in cloud environments	databases;libraries;virtual machining databases software security libraries maintenance engineering computer architecture;software;virtual machine;security cloud computing virtualization;virtualization;virtual machining;maintenance engineering;virtual machines cloud computing formal verification grid computing security of data software packages software reliability;computer architecture;formal verification;software security;virtual machines;infrastructure as a service;software package;experimental evaluation;security;grid computing;software reliability;security of data;experimental evaluation virtual machines security updates cloud environments infrastructure as a service clouds virtualized grid computing customer software security issues software vulnerability software repository update checker software packages;software packages;cloud computing	A common approach in Infrastructure-as-a-Service Clouds or virtualized Grid computing is to provide virtual machines to customers to execute their software remotely. While giving full super user permissions eases the installation and use of a customer's software, it may lead to security issues. Providers usually delegate the task of keeping virtual machines up to date to the customer, while the customer expects the provider to perform this task. Consequently, a large number of virtual machines (either running or dormant) are not patched against the latest software vulnerabilities. The approach presented in this paper deals with this problem by helping users as well as providers to keep virtual machines up to date. Prior to the update step, it is crucial to know which software is actually outdated. While this task seems trivial, developing a solution that takes care of multiple, different software repositories and identifies the correct packages is a challenging task. The Update Checker presented in this paper identifies outdated software packages in virtual machines, even if the virtual machines are installed with different repositories. The paper presents the design, the implementation and an experimental evaluation of the approach.	binary file;care-of address;categorization;cloud computing;compiler;d-grid;download;grid computing;hessian;hooking;incredibuild;linux;package manager;parsing;plug-in (computing);repository (version control);software project management;software repository;speedup;superuser;vii;virtual machine;vulnerability (computing);yast	Roland Schwarzkopf;Matthias Schmidt;Christian Strack;Bernd Freisleben	2011	2011 IEEE Third International Conference on Cloud Computing Technology and Science	10.1109/CloudCom.2011.40	maintenance engineering;cloud computing;computer science;virtual machine;information security;operating system;database;world wide web	HPC	-57.277438165365226	54.73979133049869	48706
9582cb623479549e72cfff23a61caaa7a194ffce	cyber-physical systems design for electric vehicles	energy conservation;power transmission mechanical;safety battery powered vehicles electric motors energy conservation energy management systems power grids power transmission mechanical;electric motors;energy management systems;batteries electric vehicles mechanical power transmission multicore processing safety;cyber physical systems;embedded systems;power grid cyber physical system design cps design electric powertrain electrical and electronic architectures e e architecture battery electric motor distributed energy management system time triggered system innovative driver assistance system x by wire control electric vehicle energy efficiency electric vehicle safety;safety;electric vehicles;power grids;electric vehicles embedded systems cyber physical systems;battery powered vehicles	Electric vehicles are emerging as a solution to environmental changes and transportation challenges in growing mega-cities. Compared to combustion engine vehicles, electric vehicles bring along new challenges in the CPS design. This paper gives an overview of several of these challenges and presents initial and potential solutions for the design of the electric powertrain and E/E architectures for electric vehicles. The powertrain consists of multiple complex CPS such as the battery, the electric motor, and a distributed energy management system. These components require a complex monitoring and control in order to guarantee safety and maintain a high efficiency. For this purpose, novel E/E architectures become necessary that facilitate a predictable distributed computation and communication, requiring a paradigm shift towards fully time-triggered systems. These E/E architectures will also enable novel CPS such as innovative driver assistance systems, x-by-wire control to further increase the safety and energy-efficiency of electric vehicles, and a pervasive interaction of the vehicle and the grid. Instead of focusing on the specific applications, this paper describes the prerequisite architectural changes that are necessary to implement these novel functions.	computation;cyber-physical system;distributed computing;programming paradigm;systems design	Martin Lukasiewycz;Sebastian Steinhorst;Florian Sagstetter;Wanli Chang;Peter Waszecki;Matthias Kauer;Samarjit Chakraborty	2012	2012 15th Euromicro Conference on Digital System Design	10.1109/DSD.2012.39	electric motor;embedded system;energy conservation;computer science;cyber-physical system	EDA	-45.13879971069585	46.76683017510066	48723
741de23eba3649e86c231f548ab6a2e3dc6e7217	an opportunistic platform for android-based mobile devices	android os;mobile device;opportunistic networks;mobile computer;cellular network;mobile computing	This paper describes a novel Android-based opportunistic platform for mobile computing applications. It has the aim to incentive the growth of practical experiences that should give an answer to the following question: can Opportunistic Networks actually compete with cellular networks to support urban-wide mobile computing applications?	android;mobile computing;mobile device	Paolo Meroni;Elena Pagani;Gian Paolo Rossi;Lorenzo Valerio	2010		10.1145/1755743.1755783	embedded system;mobile search;mobile web;computer science;mobile technology;internet privacy;mobile station;mobile computing;computer network	Mobile	-39.905092863846086	52.85641758344986	48777
0422edaea3367315610ec86c1b6cf89c3c01b368	a quantitative and qualitative investigation of performance-related commits in android apps	google;androids;sorting;smart phones;humanoid robots;mobile communication;context	Performance is nowadays becoming a crucial issue for mobile apps, as they are often implementing computational-intensive features, are being used for mission-critical tasks, and, last but not least, a pleasant user experience often is a key factor to determine the success of an app. This paper reports a study aimed at preliminarily investigating to what extent developers take care of performance issues in their commits, and explicitly document that. The study has been conducted on commits of 2,443 open source Android apps, of which 180 turned out to contain a total of 457 documented performance problems. We classified performance-related commits using a card sorting approach, and found that the most predominant kinds of performance-related changes include GUI-related changes, fixing code smells, network-related code, and memory management.	android;card sorting;care-of address;code smell;graphical user interface;memory management;mission critical;mobile app;open-source software;performance tuning;user experience	Teerath Das;Massimiliano Di Penta;Ivano Malavolta	2016	2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)	10.1109/ICSME.2016.49	mobile telephony;computer science;sorting;engineering;humanoid robot;internet privacy;world wide web;computer security	SE	-55.198083656777214	43.29248993459396	48854
5a2b03505e74ba5b1bccafe2f758293d17bf374b	danger-system: exploring new ways to manage occupants safety in smart building	danger system building environment users feedback mobile devices mobile crowd sensing building management systems smart buildings context dependent notification private buildings commercial buildings automated solutions complex buildings occupants safety;safety building management systems buildings structures mobile computing;architecture emergency services smart buildings mobile handsets security	Safety has always been a main concern in complex buildings: automated solutions to respond to crisis events limiting damages and victims have been researched and adopted in commercial and private buildings since late 19th century. However, the methods available have some limitations: for example the impossibility to promptly detect false alarms and to provide context-dependent notification, which are fundamental to persuade tenants to a fast egress. In this paper we present the architecture of a system for the detection and management of emergencies in smart buildings. The designed architecture shows some novelties and key features which distinguish it from already existing solutions. Indeed, the proposed architecture exploits the existing building management systems leveraging mobile crowd sensing to collect valuable information during a crisis. The new generation of mobile devices are used to dispatch notification and collect users feedback. In order to test and validate the system, we have built a simulation framework which simulates the building environment.	context-sensitive language;cube;dynamic dispatch;egress filtering;high-level programming language;logic programming;mobile device;open innovation;simulation;smart tv;usability	Andrea Piscitello;Francesco Paduano;A. A. Nacci;Danny Noferi;Marco D. Santambrogio;Donatella Sciuto	2015	2015 IEEE 2nd World Forum on Internet of Things (WF-IoT)	10.1109/WF-IoT.2015.7389135	embedded system;simulation;computer security	Mobile	-42.155927034801145	53.74875129651219	48910
543104b3fef902ae76f8e8dbecc1d1668b752e4b	a new approach and a related tool for dependability measurements on distributed systems	unifi;verification;analytical models;distributed algorithms;distributed system;verification and validation v v dependability measurements measurement uncertainty measurements on distributed systems resilience measurements;reliability;instrumentation;measurement;verification and validation v v;nekostat software;distributed algorithms distributed systems instrumentation dependability measurements metrology nekostat software java;measurement uncertainty;metrology;firenze;dependable systems;bonding;resilience;affidabili;monitoring;ricerca;resilient computing lab;measurement computerised instrumentation distributed algorithms java;instrumentation and measurement;dependability;validation;analytical models bonding measurement uncertainty resilience monitoring metrology quality of service instrumentation and measurement java algorithm design and analysis;rcl;computerised instrumentation;affidabilita;quality of service;florence;distributed systems;dependability measurements;sistemi;resilience measurements;experimental measurement;verification and validation;algorithm design and analysis;measurements on distributed systems;assessment;java;uncertainty of measurement	In recent years, experts in the field of dependability are recognizing experimental measurements as an attractive option for assessing distributed systems; contrary to simulation, measurement allows monitoring the real execution of a system in its real usage environment. However, the results of a recent survey have highlighted that the way measurements are carried out and measurement results are expressed is far from being in line with the approach commonly adopted by metrology. The scope of this paper is twofold. The first goal is to extend the discussion on the increasing role that measurements play in dependability and on the importance of cross-fertilization between the dependability and the instrumentation and measurement communities. The second objective is to present a different approach to dependability measurements, in line with the common practices in metrology. With regard to this, the paper presents a tool for dependability measurements in distributed systems that allows evaluating the uncertainty of measurement results. The tool is an enhancement of NekoStat, which is a powerful highly portable Java framework that allows analyzing distributed systems and algorithms. Together with the description of the tool and its innovative features, two experimental case studies are presented.	algorithm;dependability;distributed computing;java;quantum metrology;simulation	Andrea Bondavalli;Andrea Ceccarelli;Lorenzo Falai;Michele Vadursi	2010	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2009.2023815	reliability engineering;distributed algorithm;real-time computing;computer science;systems engineering;engineering;dependability;psychological resilience;statistics;measurement uncertainty	Metrics	-49.55287753663178	37.52148596328388	48915
675fd18ebe9d77a3f019346b3359b466f403e35a	architectures for an efficient sms gateway service	gsm computer architecture modems simple object access protocol databases;software architecture middleware;sms reception smsgateway service architecture middleware service sms sending	This paper presents four alternative architectures that allows the implementation of a SMSGateway service, by pointing out the pro and cons of each solution. SMSGateway is a middleware service that allows the SMS sending and reception from an application point of view, ideal to let any software service able to automatically communicate to final users by means of the SMS channel, irrespectively from the GSM telecommunication provider that provide the SMS service.	middleware;software as a service	Chiara Taddia;Gianluca Mazzini	2015	2015 23rd International Conference on Software, Telecommunications and Computer Networks (SoftCOM)	10.1109/SOFTCOM.2015.7314071	embedded system;concatenated sms;computer science;sms banking;world wide web;computer network	SE	-36.23957444591148	51.79762681777312	48994
17a77fc3dda8a5d7f7c9dcf300ecf52a81ea3aba	enforcing opacity of regular predicates on modal transition systems	partial observation;opacity;modal automata;supervisory control	Abstract   Given a labelled transition system  LTS  partially observed by an attacker, and a regular predicate  Sec  over the runs of  LTS , enforcing opacity of the secret  Sec  in  LTS  means computing a supervisory controller  K  such that an attacker who observes a run of  K/LTS  cannot ascertain that the trace of this run belongs to  Sec  based on the knowledge of  LTS  and  K.  We lift the problem from a single labelled transition system  LTS  to the class of all labelled transition systems specified by a modal transition system  MTS.  The lifted problem is to compute the maximally permissive controller  K  such that  Sec  is opaque in  K/LTS  for every labelled transition system  LTS  which is a model of  MTS.  The situations of the attacker and of the controller are dissymmetric: at run time, the attacker may fully know  LTS  and  K  whereas the controller knows only  MTS  and the sequence of actions executed so far by the unknown  LTS.  We address the problem in two cases. Let S  a   denote the set of actions that can be observed by the attacker, and let S  c   and S  o   denote the sets of actions that can be controlled and observed by the controller, respectively. We provide optimal and regular controllers that enforce the opacity of regular secrets when S  c   ⊆ S  o   ⊆ S  a   = S. We provide optimal and regular controllers that enforce the opacity of regular upper-closed secrets ( Sec  =  Sec. S * ) when S  a   ⊆ S  c   ⊆ S  o   = S.	modal logic	Philippe Darondeau	2012		10.3182/20121003-3-MX-4033.00053	real-time computing;computer science;distributed computing;algorithm	Logic	-54.75317026458583	52.71375061756221	49026
302c6d30a33a7be515ff2bb9263dc2527befd608	cell phones as mobile computing devices	operating systems computers mobile computing mobile handsets;cell phone;cellular phones mobile computing software libraries smart phones fingers bandwidth downlink computer networks portable computers multicore processing;software libraries;mobile broadband;information technology;cell phones;smart phones;mobile computer;computer networks;mobile computing devices mobile processors mobile operating systems mobile broadband cell phones;operating system;mobile processors;downlink;portable computers;multicore processing;fingers;mobile handsets;mobile wireless;bandwidth;mobile computing devices;mobile computing;mobile operating systems;mobile wireless mobile computing cell phone mobile application information technology;mobile application;operating systems computers;cellular phones	Increases in the computational power of mobile processors, improvements in mobile operating systems, and the popularity of mobile broadband make cell phones the best candidate for sophisticated mobile computing devices. Learn about potential applications and their corresponding challenges.	central processing unit;mobile computing;mobile operating system;mobile phone	Kin Yeung Wong	2010	IT Professional	10.1109/MITP.2010.46	multi-core processor;mobile broadband;embedded system;real-time computing;mobile search;mobile web;telecommunications link;imt advanced;mobile processor;computer science;operating system;mobile technology;mobile station;mobile computing;mobile communications over ip;bandwidth;computer network	Mobile	-37.79507525624485	53.900710306401294	49064
1cec327294de84eeff77aa9f9a772d967643a609	flexible probabilistic qos management of orchestrations	monitoring;probabilistic slas;qos management;contract composition	In this paper, the authors develop a comprehensive framework for QoS management based on soft probabilistic contracts. The authors approach also encompasses general QoS parameters, with “response time” as a particular case. In addition, the authors support composite QoS parameters, for example, combining timing aspects with “quality of data” or security level. They also study contract composition (how to derive QoS contracts for an orchestration from the QoS contracts with its called services), and contract monitoring.	orchestration (computing);quality of service;response time (technology)	Sidney Rosario;Albert Benveniste;Claude Jard	2010	Int. J. Web Service Res.	10.4018/jwsr.2010040102	database;computer security	ECom	-46.765608219362626	42.87374299910048	49074
00e2ef184b419cecedb77d5753454ac0ca59dfd1	a layered graphical model for mission attack impact analysis		Business or military missions are supported by hardware and software systems. Unanticipated cyber activities occurring in supporting systems can impact such missions. In order to quantify such impact, we describe a layered graphical model as an extension of forensic investigation. Our model has three layers: the upper layer models operational tasks that constitute the mission and their inter-dependencies. The middle layer reconstructs attack scenarios from available evidence to reconstruct their inter-relationships. In cases where not all evidence is available, the lower level reconstructs potentially missing attack steps. Using the three levels of graphs constructed in these steps, we present a method to compute the impacts of attack activities on missions. We use NIST National Vulnerability Database's (NVD)-Common Vulnerability Scoring System (CVSS) scores or forensic investigators' estimates in our impact computations. We present a case study to show the utility of our model.	business process;computable function;computation;graphical model;graphical user interface;national vulnerability database;software system;system call	Changwei Liu;Anoop Singhal;Duminda Wijesekera	2017	2017 IEEE Conference on Communications and Network Security (CNS)	10.1109/CNS.2017.8228706	national vulnerability database;computer security;cvss;computer science;software system;data mining;nist;computation;cloud computing;vulnerability;graphical model	Security	-60.704218788053474	46.43918913852581	49120
d451211b3bba68859812840719b392dd1171174b	utilizing the ip multimedia subsystem to create an extensible service-oriented architecture	context aware architecture;service delivery platform;social networks;smart spaces;ip multimedia subsystem ims;next generation networks	Smart Spaces provide very promising means of creating context-aware environments. Unfortunately, a lack of information about users within Smart Spaces limits their usefulness. We propose a novel solution that involves integrating Smart Spaces with social networks through the IP Multimedia Subsystem. to create truly context-aware and adaptive spaces. By utilizing the wealth of user information present within social networks, smarter and more adaptive spaces can be created. We therefore propose the design and implementation of “SocioSpace” a Smart Spaces framework that utilizes the social context. We design and implement all components of SocioSpace, including the central server, the location management system, social network interfacing components, service delivery server and user agents. We then run various scenarios to test the reliability of the system. The results show the effectiveness of our framework in successfully creating Smart Spaces that can truly utilize social networks to deliver adaptive services that enhance the users’ experiences and make the environment more beneficial to them.	ip multimedia subsystem;service-oriented architecture	Ahmed Hasswa;Hossam S. Hassanein	2013	J. Comput. Science	10.1016/j.jocs.2012.02.002	embedded system;next-generation network;operating system;world wide web;computer network;social network	Theory	-38.13634742778061	50.79051199364907	49124
b570fe08f283d837a52b04bdcc68b241d221c5b2	raina: reliability and adaptability in android for fog computing	raina reliability adaptability android fog computing smartphones cloud software stack mobile os software architecture;edge computing androids humanoid robots software reliability mobile communication hardware smart phones;androids;smart phones;android operating system cloud computing mobile computing smart phones software architecture software reliability;humanoid robots;mobile communication;software reliability;hardware	The ubiquity and universality of smartphones make them ideal fog devices to bridge edge devices and the cloud. However, to support a wide range of applications, as well as adhere to the resource constraints presented, the software stack on smart phones needs to be reliable and adaptable. We propose RAINA, an architecture to enable reliability and adaptability in Android. While our work is on Android, our ideas can easily be adapted to other mobile OSs. This article describes our software architecture, systems challenges, application challenges, and methods to address these challenges. We also discuss future work to allow smartphones to truly be at the center of the fog.	android;fog computing;input device;mobile operating system;smartphone;software architecture;universality probability	Karthik Dantu;Steven Y. Ko;Lukasz Ziarek	2017	IEEE Communications Magazine	10.1109/MCOM.2017.1600901	embedded system;real-time computing;mobile telephony;computer science;humanoid robot;operating system;software quality	Mobile	-43.01127222038029	47.02095828795283	49177
c5486a800a2d047ddf6abab9c31ea8658e9bb0af	seda: scalable embedded device attestation	remote attestation;device swarms;security	Today, large numbers of smart interconnected devices provide safety and security critical services for energy grids, industrial control systems, gas and oil search robots, home/office automation, transportation, and critical infrastructure. These devices often operate in swarms -- large, dynamic, and self-organizing networks. Software integrity verification of device swarms is necessary to ensure their correct and safe operation as well as to protect them against attacks. However, current device attestation schemes assume a single prover device and do not scale to swarms. We present SEDA, the first attestation scheme for device swarms. We introduce a formal security model for swarm attestation and show security of our approach in this model. We demonstrate two proof-of-concept implementations based on two recent (remote) attestation architectures for embedded systems, including an Intel research platform. We assess performance of SEDA based on these implementations and simulations of large swarms. SEDA can efficiently attest swarms with dynamic and static topologies common in automotive, avionic, industrial control and critical infrastructures settings.	control system;embedded system;organizing (structure);robot;self-organization;simulation;swarm robotics;trusted computing	N. Asokan;Franz Ferdinand Brasser;Ahmad Ibrahim;Ahmad-Reza Sadeghi;Matthias Schunter;Gene Tsudik;Christian Wachsmann	2015		10.1145/2810103.2813670	embedded system;real-time computing;computer science;information security;computer security	Security	-57.99132312393887	51.94005277159822	49189
60ec16ff8c3413f22a4ca570bf7aff5adde67b83	from theory to practice in distributed component systems	asynchrony;distributed components;component model;determinism;grid computing;open source	This talk will start by presenting theoretical results on determinism for asynchronous distributed components. It will then shows how to apply those results in a practical implementation, available as Open Source within the ObjectWeb Open Source community. Further, current work aiming at defining a joint European component model for Grid computing (GCM) will be summarized. Finally, it will conclude with challenges at hand with component systems, especially work related to capturing behavioral properties. Current work aiming at defining behavioural models and techniques for hierarchical components will be introduced.	component-based software engineering;google cloud messaging;grid computing	Denis Caromel	2007	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2006.12.042	asynchrony;real-time computing;computer science;theoretical computer science;component object model;distributed computing;programming language;grid computing;determinism	SE	-35.98427735357868	40.07368745416845	49200
cfe9460eb621737e3826bef47db6d39e0c3f66ef	a hazard analysis technique for additive manufacturing		The promise of Additive Manufacturing (AM) includes reduced transportation and warehousing costs, reduction of source material waste, and reduced environmental impact. AM is extremely useful for making prototypes and has demonstrated the ability to manufacture complex parts not possible (or prohibitively expensive) with conventional machining. Scientists and manufactures are finding increased uses for AM in creation of all types of finished products including those built from polymers, biological material, and metals. Although companies such as GE have been using 3D printing for Additive Manufacturing for over thirty years to make mandrels for light bulb manufacturing, application areas of Additive Manufacturing have increased substantially in recent years, particularly due to the reduction in cost of 3D printers. Like most emergent technologies, there are bound to be growing pains with AM. This paper looks at the software that supports AM and 3D printing and their vulnerability to cyber-attacks, intellectual property theft, defect rates of AM software (which can cause undesired consequences themselves and also create vulnerabilities that a hacker may exploit), part reliability and safety of devices incorporating 3D printed parts (when making mission critical parts), and security/throughput issues of computer networks. Literature searches, consulting with technical experts and a relatively new hazard analysis technique will be used, one especially developed for software intensive systems called Systemic Theoretic Process Analysis (STPA). The purpose of this white paper is to identify risks (or hazards for mission critical parts) for AM in this emergent stage so that mitigations can be applied before accidents occur. A second purpose of this white paper is to evaluate the effectiveness of STPA as a hazard analysis technique in a field that is still relatively new.	3d printing;additive model;emergence;hazard analysis;mission critical;software bug;software prototyping;throughput	Gregory Pope;Mark Yampolskiy	2016	CoRR		data mining;reliability engineering;white paper;machining;mission critical;software;hazard analysis;3d printing;operations management;computer science;exploit;vulnerability	HCI	-59.570687418989074	48.9540183357586	49203
87502a4a8b900b013a341e2ce828bd4a1fb3d5ef	grammar-based test generation for software product line feature models	feature models;test generation;software product line	Product lines are often employed for the facilitation of software re-use, rapid application development and increase in productivity. Despite the numerous advantages of software product lines, the task of testing them is a cumbersome process due to the fact that the number of applications that need to be tested is exponential to the number of features represented in the product line. In this paper, we attempt to reduce the number of required tests for testing a software product line while at the same time preserving an acceptable fault coverage. For this purpose, we introduce eight coverage criteria based on the transformation of software product line feature models into formal context-free grammars. The theoretical foundation for the proposed coverage criteria is based on the development of equivalence partitions on the software product line configuration space and the use of boundary value analysis for test suite generation. We have performed experiments on several SPLOT feature models, the results of which show that the test suite generation strategies based on the proposed coverage criteria are effective in significantly reducing the number of required tests and at the same time maintaining a high fault coverage ratio.	boundary-value analysis;context-free grammar;context-free language;experiment;fault coverage;rapid application development;software product line;test suite;time complexity;turing completeness	Ebrahim Bagheri;Faezeh Ensan;Dragan Gasevic	2012			test data generation;fault coverage;computer science;software testing;code coverage;feature model	SE	-58.87719896900843	32.88829966801148	49270
a56445f1deabc117ef626b75dfcaafff0c09b707	sequencing of refactoring techniques by greedy algorithm for maximizing maintainability	software metrics;refactoring sequence;quality;greedy algorithm;software maintainability	Software maintainability is the ease with which a software system can be modified to correct faults, improve performance or other attributes of the source code. Bad smells are symptoms of deeper problem that indicates the need for refactoring which is the process of changing internal structure of the software without affecting its external attributes. Applying different refactoring techniques in different parts of a code results in changed maintainability value every time. Therefore, sequence in which refactoring should be applied is important so that optimal results can be obtained. In this study, we have proposed an approach for evaluating sequence of refactoring by with the help of greedy algorithm. The algorithm selects locally optimal solution at each stage with the hope of finding global optimal solution. Different sequences are generated and applied to the source code to calculate sum of software maintainability values. Greedy algorithm helps in finding the optimal sequence out of all the search space. We have evaluated the approach with source code having god class, long method, feature envy, long parameter list, data clumps, data class, class hierarchy problem, empty catch block, exception thrown in finally block and nested try statement bad smells which are detected manually. Hence our approach is able to identify sequence for refactoring as well as best refactoring which will increase maintainability and enhance software quality.	class hierarchy;code refactoring;code smell;god object;greedy algorithm;local optimum;optimization problem;software quality;software system	Sandhya Tarwani;Anuradha Chug	2016	2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2016.7732243	reliability engineering;mathematical optimization;greedy algorithm;real-time computing;computer science;systems engineering;code refactoring;maintainability;software metric	SE	-59.229429244881764	35.74618731870897	49304
9ea4eccadf4b467d708dd12ef19aa081ab9444c2	industrial evaluation of a log file analysis methodology	software testing;industrial evaluation;computer languages;instruments;commercial software industrial evaluation log file analysis methodology;log files;application software;computer industry;log file analysis;commercial software;program testing;information analysis software testing writing computer industry instruments application software laboratories computer science pattern analysis computer languages;writing;pattern analysis;computer science;log file analysis methodology;information analysis	Test result evaluation programs often take the form of log file analyzers, which analyze text logs of events that have happened during testing. Previously, we proposed a methodology for deriving logging instrumentation and state-based log file analyzer programs from requirements. In this paper, we report on an industrial evaluation in which the methodology was carried out on two pieces of commercial software in a commercial setting. We report on our quantitative and qualitative observations and on recommendations for improving the methodology and the tools used.	commercial software;log-structured file system;requirement	Donald J. Yantzi;James H. Andrews	2007	Fifth International Workshop on Dynamic Analysis (WODA '07)	10.1109/WODA.2007.7	application software;computer science;software engineering;database;software testing;data analysis;programming language;writing;computer engineering	SE	-61.51238591595878	33.27491071475206	49351
20a1ac0c7bccc7818c8ebb98c368720dc732b286	elicitation of security requirements for e-health system by applying model oriented security requirements engineering (mosre) framework	threats;functional requirements;security issues;security requirements;e health;security requirements engineering	E-health is a health care system which is supported by electronic process and communication. The information that is kept in the system must be accurate. In case of false information, it may cause harm to human life. So this system needs more security to protect the credential information. E-Health system is the most security sensitive process handled electronically. The highest achievable security is never too much for an E-Health system. So when system is being built, tasks such as Security Requirements Elicitation, Specification and Validation are essential to assure the Quality of the resulting secure E-Health system. By considering the Security requirements as functional requirements in the Requirement phase, the completeness of Security Requirements for E-Health system can be developed. In this paper we propose Model Oriented Security Requirements Engineering (MOSRE) Framework in the early phases of E-Health system development, to identify assets, threats and vulnerabilities. This helps in standardizing the Security Requirements for secure E-Health system without any security issues.	credential;functional requirement;health level 7;requirements analysis;requirements elicitation;requirements engineering	P. Salini;S. Kanmani	2012		10.1145/2393216.2393238	software security assurance;computer security model;standard of good practice;cloud computing security;itil security management;countermeasure;requirements management;security through obscurity;security information and event management;security engineering;security convergence;covert channel;asset;computer science;information security;system requirements specification;requirements elicitation;security service;security analysis;security testing;computer security;functional requirement;non-functional requirement	Security	-55.99379511431124	48.5030860360615	49384
d145a8cce8dba3ecae7529d7abd6468087076b42	analysis of license inconsistency in large collections of open source projects	license inconsistency;code clone;software license	Free and open source software (FOSS) plays an important role in source code reuse practice. They usually come with one or more software licenses written in the header part of source files, stating the requirements and conditions which should be followed when been reused. Removing or modifying the license statement by re-distributors will result in the inconsistency of license with its ancestor, and may potentially cause license infringement. In this paper, we describe and categorize different types of license inconsistencies and propose a method to detect them. Then we applied this method to Debian 7.5 and a collection of 10,514 Java projects on GitHub and present the license inconsistency cases found in these systems. With a manual analysis, we summarized various reasons behind these license inconsistency cases, some of which imply potential license infringement and require attention from the developers. This analysis also exposes the difficulty to discover license infringements, highlighting the usefulness of finding and maintaining source code provenance.	categorization;code reuse;debian;java;open-source software;requirement;software license	Yuhao Wu;Yuki Manabe;Tetsuya Kanda;Daniel M. Germán;Katsuro Inoue	2016	Empirical Software Engineering	10.1007/s10664-016-9487-8	engineering;database;world wide web;computer security	SE	-60.500777052289855	40.43994905940786	49390
8153449f13bfc900db24c49e5b14ca5750a68333	designing highly flexible virtual machines: the jnjvm experience	dynamic adaptation;aspect weaving;java virtual machine	Dynamic flexibility is a major challenge in modern system design to react to context or applicative requirements evolutions. Adapting behaviors may impose substantial code modification across the whole system, in the field, without service interruption, and without state loss. This paper presents the JnJVM, a full Java virtual machine (JVM) that satisfies these needs by using dynamic aspect weaving techniques and a component architecture. It supports adding or replacing its own code, while it is running, with no overhead on unmodified code execution. Our measurements reveal similar performance when compared to the monolithic JVM Kaffe. Three illustrative examples show different extension scenarios: (i) modifying the JVMs behavior; (ii) adding capabilities to the JVM; and (iii) modifying applications behavior.	applicative programming language;component-based software engineering;interrupt;java virtual machine;kaffe;overhead (computing);requirement;systems design	Gaël Thomas;Nicolas Geoffray;Charles R Clement;Bertil Folliot	2008	Softw., Pract. Exper.	10.1002/spe.887	real-time computing;simulation;computer science;engineering;virtual machine;operating system;adaptive behavior;programming language;satisfiability;systems design	SE	-40.273803411866936	39.453273538381325	49475
919a3477fe47668ac4b25fa273b4329b4a3915f8	architecture distribuée dédiée aux applications de réalité augmentée mobile. (distributed architecture dedicated to mobile augmented reality applications)		Mobile Augmented Reality (AR) consists in achieving to make co-existing virtual and real worlds in real time. Mobility is made easier by the use of new devices such as smartphones and wearable computers or smart objects (ex. Glasses) involving sensors (inertial, visual...). These devices have lower computation capabilities that can be critical to some Augmented Reality applications. One of the solutions is to use distribution mechanisms to distribute processing on several and heterogeneous machines. The goal of this thesis is to design a software architecture dedicated to distributed Augmented Reality and more precisely to distributed applications that can run on ad-hoc networks including heterogeneous terminals deployed in a network. The second part of the thesis is to prove the feasibility and the efficiency of the proposed architecture on real AR applications and explore different original uses of distribution for AR applications. Focusing on the added value in terms of features and possible opérations compared to non-distributed AR applications and compared to existing frameworks offering distributed AR components.	augmented reality;computation;distributed computing;heterogeneous computing;hoc (programming language);sensor;smart objects;smartphone;software architecture;wearable computer	Mehdi Chouiten	2013				Mobile	-38.812135619299134	46.724262350566306	49503
5551e91523f624493a2eac3609847aec87498189	fault localization with nearest neighbor queries	fault localization;nearest neighbor queries;program diagnostics;siemens suite fault localization nearest neighbor queries similar program spectra distance criterion whither block profiling spectra;general methods;software quality fault diagnosis program diagnostics software tools;experimental validation;software tools;nearest neighbor searches computer bugs testing programming profession computer science electronic mail navigation debugging runtime software engineering;software quality;fault diagnosis	We present a method for performing fault localization using similar program spectra. Our method assumes the existence of a faulty run and a larger number of correct runs. It then selects according to a distance criterion the correct run that most resembles the faulty run, compares the spectra corresponding to these two runs, and produces a report of “suspicious” parts of the program. Our method is widely applicable because it does not require any knowledge of the program input and no more information from the user than a classification of the runs as either “correct” or “faulty”. To experimentally validate the viability of the method, we implemented it in a tool, WHITHER using basic block profiling spectra. We experimented with two different similarity measures and the Siemens suite of 132 programs with injected bugs. To measure the success of the tool, we developed a generic method for establishing the quality of a report. The method is based on the way an “ideal user” would navigate the program using the report to save effort during debugging. The best results we obtained were, on average, above 50%, meaning that our ideal user would avoid looking at half of the program.	basic block;debugging;experiment;software bug	Manos Renieris;Steven P. Reiss	2003		10.1109/ASE.2003.1240292	computer science;theoretical computer science;software engineering;data mining;database;programming language;software quality	SE	-59.901447963072734	37.51427789492959	49529
e94e3ced539e35af7fed05b906b8499fd8334a83	introduction to the special section on rigorous embedded systems design	cache;persistence;wcet	Embedded systems design brakes with traditional system design as it must jointly take into account extra-functional conflicting requirements such as performance, low energy consumption, autonomy, dependability, and short time-to-market. It raises important research challenges including modeling interaction between a system and its physical environment, component-based construction from heterogeneous components, resource-aware compilation, adaptive QoS management, automatic code generation, and deployment. This special section, organized by the European Network of Excellence ArtistDesign, focuses on design approaches guaranteeing essential system properties and determining trade-offs between conflicting design requirements. The call for papers was open for researchers outside of ArtistDesign. TECS received 18 submissions, of which four were accepted. The rest were rejected or withdrawn by the authors. The submissions covered a broad range of topics: timing analysis, programming and specification languages, real-time scheduling, networks and operating systems, performance prediction, memory management, and memory hierarchies. The first article, “Sensitivity of Cache Replacement Policies” by Jan Reineke and Daniel Grund, describes the dependence of the cache performance on the initial cache contents. They show that this sensitivity is vastly different for different cachereplacement strategies. LRU-replacement caches are not sensitive to the initial cache contents while most others are. These results have strong implications for timinganalysis methods using nonexhaustive exploration of the cache-state space for timing analysis. The second article, “Rigorous Rental Memory Management for Embedded Systems”, by Jinkyu Jeong, Hwangju Kim, Jeaho Hwang, Joonwon Lee, and Seungryoul Maeng, describes how to improve memory utilization in embedded systems. A fixed reservation of memory for particular applications is often used as a means to provide physically contiguous memory to the application. This is a waste of resources if the application is not active. In the proposed scheme, a nonactive application would rent its memory for other purposes. Several optimization techniques are described to minimize the latency of memory reallocation. The authors Vasileios Vasilikos, Georgios Smaragdos, Christos Strydis, and Ioannis Sourdis, develop new methods to maintain high reliability in the presence of defects in multiprocessor arrays. In their article entitled “Heuristic Search for Adaptive, DefectTolerant Multiprocessor Arrays”, they formulate defect-tolerant multiprocessor array design as a computational problem, apply offline heuristic methods to solve it, and address the architectural issues that arise. Finally, Maria-Cristina Marinescu and Cesar Sanchez describe in their article “Fusing Statecharts and Java” a new approach for modeling and implementing embedded software components. They propose a unified programming environment which, not only preserves some of the major advantages of Statecharts of having a formal foundation, but also directly supports advanced programming language concepts related to object-orientation, concurrency, and strong typing.	automatic programming;autonomy;code generation (compiler);compiler;component-based software engineering;computational problem;concurrency (computer science);dependability;embedded software;embedded system;heuristic;integrated development environment;java;mathematical optimization;memory hierarchy;memory management;multiprocessing;online and offline;operating system;performance prediction;programming language;real-time clock;requirement;scheduling (computing);software bug;software deployment;specification language;state space;static timing analysis;systems design	Joseph Sifakis;Lothar Thiele;Reinhard Wilhelm	2013	ACM Trans. Embedded Comput. Syst.	10.1145/2435227.2435237	persistence;embedded system;real-time computing;cache;computer science;distributed computing;programming language	Embedded	-39.25606636569639	35.40332525499203	49625
fa4b4a2541eb437ec87d1f064ea9e07d948750ba	guest editorial: security and privacy of p2p networks in emerging smart city		Recently, the smart city has been introduced as a promising concept due to its potential benefits including low-carbon economy, intelligent traffic management, ubiquitous information sharing, and etc. In the smart city, there are many key components, such as smart grid, smart vehicle, smart cloud, and mobile social network. Thanks to the good salability and low processing cost on content delivery and distributed search engine in these components, P2P technologies are expected to play an essential role in accelerating the implementation of the smart city. Although we have witnessed the major and remarkable development in the field of smart city in the recent years, the security and privacy issues of the smart city have not been well studied. Thus, there is a crucial need for security and privacy research to achieve secure and privacy-preserving smart city. This special issue has gained overwhelming attention and received 32 submissions from researchers and practitioners working on security and privacy of P2P Networks in	digital distribution;distributed web crawling;mobile social network;peer-to-peer;privacy;smart city;web search engine	Hongwei Li;Haojin Zhu;Bong Jun Choi	2015	Peer-to-Peer Networking and Applications	10.1007/s12083-015-0393-4	internet privacy;computer security;computer network	Security	-43.880147261532336	51.427642730770714	49701
2ad58cb9567f541ee8ed13aba3579dd34169d533	evaluation of test-driven development: an academic case study	test driven development	This paper investigates the effects of test driven development in the academic environment by conducting a case study with  eight students who were assigned to implement a simple application either with test driven approach or with traditional approach.  The results of the case study showed that the first group of students completed their tasks faster and with higher quality  than the second group. The programs written by the first group pass relatively more test cases than those developed by the  second group. The first group of programmers also wrote cleaner code with higher cohesion by creating more reasonable number  of methods. Therefore, test-driven approach could be used in the academic environment such as assignments and projects.  	test-driven development	Shaochun Xu;Tingting Li	2009		10.1007/978-3-642-05441-9_20	test-driven development;computer science	HCI	-61.7298447765695	35.72528302223123	49765
ed2e33c200584dcd5bce0c833d961b32d5a42145	smart fuzzing method for detecting stack-based buffer overflow in binary codes	valgrind;benchmark programs;vulnerability constraint calculation;feasible execution paths;binary codes;test data generation;computer systems;smart fuzzing method;concrete symbolic execution;stack based buffer overflow vulnerability detection;path constraint calculation;concolic execution	During the past decades several methods have been proposed to detect the stack-based buffer overflow vulnerability, though it is still a serious threat to the computer systems. Among the suggested methods, various fuzzers have been proposed to detect this vulnerability. However, many of them are not smart enough to have high code-coverage and detect vulnerabilities in feasible execution paths of the program. The authors present a new smart fuzzing method for detecting stack-based buffer overflows in binary codes. In the proposed method, concolic (concrete + symbolic) execution is used to calculate the path and vulnerability constraints for each execution path in the program. The vulnerability constraints determine which parts of input data and to what length should be extended to cause buffer overflow in an execution path. Based on the calculated constraints, the authors generate test data that detect buffer overflows in feasible execution paths of the program. The authors have implemented the proposed method as a plug-in for Valgrind and tested it on three groups of benchmark programs. The results demonstrate that the calculated vulnerability constraints are accurate and the fuzzer is able to detect the vulnerabilities in these programs. The authors have also compared the implemented fuzzer with three other fuzzers and demonstrated how calculating the path and vulnerability constraints in the method helps to fuzz a program more efficiently.	binary code;sensor;stack buffer overflow;stack-oriented programming language	Maryam Mouzarani;Babak Sadeghiyan;Mohammad Zolfaghari	2016	IET Software	10.1049/iet-sen.2015.0039	binary code;parallel computing;test data generation;real-time computing;computer science;distributed computing	Security	-57.85985142507124	55.82769166241698	49772
5e47cb2781f392df4bd005fd3be288ab58cd2d35	a survey on data security issues in cloud computing: from single to multi-clouds	cloud computing;data integrity;data confidentiality	Cloud computing usage has increased rapidly in many companies. Cloud computing offers many benefits in terms of low cost and accessibility of data. Ensuring the security of cloud computing plays a major role in the cloud computing, as customers often store important information with cloud storage providers but these providers may be unsafe. Customers are wondering about attacks on the integrity and the availability of their data in the cloud from malicious insiders and outsiders, and from any collateral damage of cloud services. These issues are extremely significant but there is still much room for security research in cloud computing. Dealing with “single cloud” providers is predicted to become less popular with customers due to risks of service availability failure and the possibility of malicious insiders in the single cloud. A movement towards “multi-clouds”, or in other words, “interclouds” or “cloudof-clouds” has increased recently. The purpose of this paper is to survey recent research related to single and multi-clouds security and to address possible solutions. It is found that the research into the use of multi-cloud providers to maintain security has received less attention from the research community than has the use of single clouds. This work aspires to promote the use of multi-clouds due to its ability to reduce security risks that affect the cloud computing consumer.	accessibility;cloud computing security;cloud storage;computer security;data security;malware;personally identifiable information	Mohammed Abdullatif Alzain;Ben Soh;Eric Pardede	2013	JSW		cloud computing security;confidentiality;cloud computing;computer science;operating system;data integrity;database;utility computing;internet privacy;world wide web;computer security	Security	-49.42795158635404	58.71957033998269	49794
0afe760dd71f0509848a29953abd8a36d12f5275	url-driven automated testing	automated testing;information resources;software testing;web based applications;it professional;automated software testing process;maintenance;application software;availability;e commerce;global e commerce;web based test process;url driven automated testing;automatic testing;url dnt;critical challenge;internet computing systems;automatic programming;url dat url driven automated testing web based applications software testing challenges world wide web global e commerce web based test process internet computing systems it professionals automated software testing process critical challenge cost effective maintainable ast system testing methodologies url driven navigation testing case url dnt url driven automated testing methodology;surges;testing methodologies;url driven navigation testing case;internet computing;internet;program testing;url dat;distributed programming;web sites;system testing;cost effectiveness;world wide web;software testing challenges;distributed programming program testing automatic programming information resources;automatic testing application software internet system testing software testing automation maintenance web sites surges availability;url driven automated testing methodology;it professionals;cost effective maintainable ast system;automation	Robert B. Wen A P Technology Corporation, Atlanta, Georgia, USA r.wen@computer.org Web-based applications pose new and unique software testing challenges as the World Wide Web suddenly surges in the global e-Commerce. The Webbased test process becomes more challenging than non-web applications testing. It is no longer sufJicient to obtain accurate analysis and measurements of the Internet computing systems with manual testing alone. IT professionals have repeatedly and convincingly produced the case for the Automated Software Testing (AST) process. The most critical challenge to enterprises is the ability to build a friendly, costeffective and maintainable AST system. This paper contributes a new concept of eflcient testing methodologies to support the rapid deployment of new Web-based applications and technologies. An UULDriven Navigation Testing (URL-DNT) case will be used as an example to introduce the UUL-Driven Automated Testing (UUL-DA r) methodology.	do not track;e-commerce;manual testing;software deployment;software testing;test automation;web application;world wide web	R. Wen	2001		10.1109/APAQS.2001.990029	test strategy;black-box testing;software performance testing;white-box testing;system integration testing;computer science;software reliability testing;cloud testing;data mining;database;software testing;system testing;world wide web	SE	-62.280332396172284	32.93113805567855	49851
79e44ea4c60c8568474a20f6c43459f6e74d505b	generating code summaries using the power of the crowd		One of the first steps to perform most of the software maintenance activities, such as updating features or fixing bugs, is to have a relatively good understanding of the program’s source code which is often written by other developers. A code summary is a description about a program’s entities (e.g., its methods) which helps developers have a better comprehension of the code in a shorter period of time. However, generating code summaries can be a challenging task. To mitigate this problem, in this article, we introduce CrowdSummarizer, a code summarization platform that benefits from the concepts of crowdsourcing, gamification, and natural language processing to automatically generate a high level summary for the methods of a Java program. We have implemented CrowdSummarizer as an Eclipse plugin together with a web-based code summarization game that can be played by the crowd. The results of two empirical studies that evaluate the applicability of the approach and the quality of generated summaries indicate that CrowdSummarizer is effective in generating quality results.	crowdsourcing;eclipse;entity;gamification;high-level programming language;java;natural language processing;qr code;software bug;software maintenance;web application	Sahar Badihi;Abbas Heydarnoori	2016	CoRR		kpi-driven code analysis;computer science;theoretical computer science;data mining;code coverage;programming language;world wide web;code generation;static program analysis;source code	SE	-57.26930818407367	37.725465345141075	49867
124d696cb3cf1d13d33c2fc1f8be5fafd62d4f46	spatial coordination of pervasive systems through chemical-inspired tuple spaces	chemicals;biochemical system spatial coordination pervasive system chemical inspired tuple space distributed infrastructure chemical substance;context aware;distributed infrastructure;pervasive computing;biological system modeling;semantics;tuple space;pervasive system;chemical substance;computational modeling;spatial pattern;chemicals computational modeling biological system modeling semantics pervasive computing noise;ubiquitous computing;functional data;semantic matching;coordination model;biochemical system;spatial coordination;chemical inspired tuple space;noise	Pervasive computing calls for developing distributed infrastructures featuring large-scale distribution, opennes, context-awareness, self-organisation and self-adaptation. There, it is quite natural to see services (software functionality, data, knowledge, signals) as spatial concepts: they are naturally diffused in the network, and in each location they are sensitive to the context and compete with each other—as such, they can be active in one or multiple regions (niches) of the network. To support and engineer this scenario, we propose a natureinspired coordination model of chemical-inspired tuple spaces. They extend standard tuple spaces with the ability of evolving the “weight” of a tuple just as it represented the concentration of a chemical substance in a biochemical system, namely, in terms of reaction and diffusion rules that adaptively apply to tuples modulo semantic match. We show that this model can be used to enact self-* properties in pervasive systems, through typical spatial patterns involving computational fields, paths, and segregation.	context awareness;ecosystem;modulo operation;pervasive informatics;self-organization;spaces;stigmergy;tuple space;ubiquitous computing;virtual machine	Mirko Viroli;Matteo Casadei;Sara Montagna;Franco Zambonelli	2010	2010 Fourth IEEE International Conference on Self-Adaptive and Self-Organizing Systems Workshop	10.1109/SASOW.2010.75	chemical industry;common spatial pattern;computer science;noise;tuple space;theoretical computer science;data mining;distributed computing;computational model;ubiquitous computing	SE	-38.66822967499946	43.87378027498054	49869
59986c890272d649d3cbbe203798de8e10c07155	execution hijacking: improving dynamic analysis by flying off course	program behavior;filtering;software testing;program diagnostics;instruments;prototypes;runtime;force;monitoring;performance analysis runtime instruments force filtering prototypes monitoring;memory error detection execution hijacking dynamic analysis program behavior;performance analysis;technical report;memory error detection;dynamic analysis;execution hijacking	Typically, dynamic-analysis techniques operate on a small subset of all possible program behaviors, which limits their effectiveness and the representativeness of the computed results. To address this issue, a new paradigm is emerging: execution hijacking, consisting of techniques that explore a larger set of program behaviors by forcing executions along specific paths. Although hijacked executions are infeasible for the given inputs, they can still produce feasible behaviors that could be observed under other inputs. In such cases, execution hijacking can improve the effectiveness of dynamic analysis without requiring the (expensive) generation of additional inputs. To evaluate the usefulness of execution hijacking, we defined, implemented, and evaluated several variants of it. Specifically, we performed an empirical study where we assessed whether execution hijacking could improve the effectiveness of a common dynamic analysis: memory error detection. The results of the study show that execution hijacking, if suitably performed, can indeed improve dynamic analysis.	c++;ddos mitigation;error detection and correction;memory debugger;programming paradigm;prototype;ram parity;run time (program lifecycle phase);universal instantiation	Petar Tsankov;Wei Jin;Alessandro Orso;Saurabh Sinha	2011	2011 Fourth IEEE International Conference on Software Testing, Verification and Validation	10.1109/ICST.2011.45	filter;embedded system;parallel computing;real-time computing;computer science;technical report;operating system;software engineering;prototype;dynamic program analysis;software testing;force	SE	-60.78049480347689	37.408246819987276	49973
2e484a5ce2708b03bf846da89cf246ec33ca9ab2	research and application of j2ee and ajax technologies in industry report	ajax;j2ee;industry report	The traditional system of industry report is highly influenced by the speed of Internet and has low efficiency on report. In order to solve these problems, this paper studies J2EE and AJAX technologies, combine them and propose an industry report system which based on J2EE and AJAX technologies. The system which makes full advantages of both technologies, has solved the problems such as easily impacted by the bandwidth, reported in low efficiency, also increased the server’s load capacity. It obtains a good result in the practical application.	ajax (programming);internet;java platform, enterprise edition;server (computing)	Min Hu;Ding-ding Pan;Pei-en Zhou	2011	JCP	10.4304/jcp.6.9.1847-1851	ajax;computer science;data mining;world wide web;computer security	HCI	-33.89927104006286	54.92854680706949	50011
05f89f0719b3925216791ef10ed793624d90cd46	automated refactoring to the strategy design pattern	refactoring;polymorphism;total replacement of conditional logic;design patterns;strategy pattern	Context: The automated identification of code fragments characterized by common design flaws (or ‘‘code smells’’) that can be handled through refactoring, fosters refactoring activities, especially in large code bases where multiple developers are engaged without a detailed view on the whole system. Automated refactoring to design patterns enables significant contributions to design quality even from developers with little experience on the use of the required patterns. Objective: This work targets the automated identification of refactoring opportunities to the Strategy design pattern and the elimination through polymorphism of respective ‘‘code smells’’ that are related to extensive use of complex conditional statements. Method: An algorithm is introduced for the automated identification of refactoring opportunities to the Strategy design pattern. Suggested refactorings comprise conditional statements that are characterized by analogies to the Strategy design pattern, in terms of the purpose and selection mode of strategies. Moreover, this work specifies the procedure for refactoring to Strategy the identified conditional statements. For special cases of these statements, a technique is proposed for total replacement of conditional logic with method calls of appropriate concrete Strategy instances. The identification algorithm and the refactoring procedure are implemented and integrated in the JDeodorant Eclipse plug-in. The method is evaluated on a set of Java projects, in terms of quality of the suggested refactorings and run-time efficiency. The relevance of the identified refactoring opportunities is verified by expert software engineers. Results: The identification algorithm recalled, from the projects used during evaluation, many of the refactoring candidates that were identified by the expert software engineers. Its execution time on projects of varying size confirmed the run-time efficiency of this method. Conclusion: The proposed method for automated refactoring to Strategy contributes to simplification of conditional statements. Moreover, it enhances system extensibility through the Strategy design pattern. 2012 Elsevier B.V. All rights reserved.	algorithm;benchmark (computing);code refactoring;code smell;computation;conditional (computer programming);cyclomatic complexity;data-flow analysis;dataflow;design pattern;eclipse;emoticon;extensibility;java;kolmogorov complexity;level of detail;local variable;machine learning;maxima and minima;plug-in (computing);program transformation;rejection sampling;relevance;run time (program lifecycle phase);software engineer;strategy pattern;subroutine;text simplification	Aikaterini Christopoulou;Emmanouel A. Giakoumakis;Vassilis Zafeiris;Vasiliki Soukara	2012	Information & Software Technology	10.1016/j.infsof.2012.05.004	reliability engineering;polymorphism;software design pattern;computer science;systems engineering;strategy pattern;software engineering;continuous design;database;programming language;code refactoring	SE	-58.33384140529466	34.44691690172335	50058
edbfb838cdf488990025a2aaba427c228301f1c8	a mobile-cloud collaborative traffic lights detector for blind navigation	detectors;context awareness;mobile;context aware;mobile device;context awareness assistive technology visually impaired mobile cloud navigation;real time;ubiquitous computing internet mobile computing traffic engineering computing;cloud;navigation;servers;internet;global positioning system;traffic light detector mobile cloud collaborative traffic lights detector blind navigation context awareness unfamiliar environments context aware navigation cloud computing providers;mobile communication;mobile handsets;assistive technology;visual impairment;visually impaired;ubiquitous computing;traffic engineering computing;resource availability;system architecture;mobile computing;meteorology;cloud computing;navigation safety mobile computing cloud computing internet usability conference management international collaboration computer architecture performance evaluation	Context-awareness is a critical aspect of safe navigation especially for the blind and visually impaired in unfamiliar environments. Existing mobile devices for context-aware navigation fall short in many cases due to their dependence on specific infrastructure requirements as well as having limited access to resources that could provide a wealth of contextual clues. In this paper, we propose a mobile-cloud collaborative approach for context-aware navigation by exploiting the computational power of resources made available by Cloud Computing providers as well as the wealth of location-specific resources available on the Internet. We propose an extensible system architecture that minimizes reliance on infrastructure, thus allowing for wide usability. We present a traffic light detector that we developed as an initial application component of the proposed system. We present preliminary results of experiments performed to test the appropriateness for the real-time nature of the application.	autonomous robot;cloud computing;computational resource;context awareness;display resolution;experiment;mobile device;norm (social);pixel;ramp simulation software for modelling reliability, availability and maintainability;real-time clock;real-time computing;requirement;systems architecture;usability;world wide web	Pelin Angin;Bharat K. Bhargava;Abdelsalam Helal	2010	2010 Eleventh International Conference on Mobile Data Management	10.1109/MDM.2010.71	simulation;cloud computing;computer science;operating system;multimedia;mobile computing;computer security;ubiquitous computing	Robotics	-40.65294503227249	51.51502727783916	50115
17877da72511bc4a05e7a895a929b7c0ac19bd85	a taxonomy of biometric system vulnerabilities and defences	pedestrian safety;poison control;injury prevention;authentication;biometrics;safety literature;traffic safety;injury control;home safety;injury research;safety abstracts;human factors;vulnerabilities;template protection;occupational safety;safety;vitality measures;safety research;accident prevention;violence prevention;bicycle safety;defences;poisoning prevention;security;falls;ergonomics;suicide prevention	The interest in biometric technology is received much attention in the recent years. However, the security issue still persists the main challenge for the reliable functioning of biometric-based authentication systems. Much has been reported on the vulnerabilities of biometric systems that breach the security and user privacy. We present a high-level classification of biometric system vulnerabilities and discuss the defence techniques of these vulnerabilities. We present a multidimensional threat environment of the biometric systems that includes faults, failures and security attacks. A framework of biometric security attacks on man-machine model is presented and the system vulnerabilities are represented using Ishikawa's diagram. The provable defence techniques such as biometric vitality detection and biometric template protection are critically evaluated, in particular, a classification of current state-of-the-art of vitality detection techniques of commonly used biometrics is proposed. Our main contributions include: 1 propose a taxonomy of biometric system vulnerabilities; 2 present a framework of biometric security attacks using man-machine model; 3 representation of vulnerabilities using Ishikawa's diagram; 4 an evaluation of defence techniques of these vulnerabilities.	biometrics;taxonomy (general)	Yogendra Narain Singh;Sanjay Kumar Singh	2013	IJBM	10.1504/IJBM.2013.052964	vulnerability;computer science;suicide prevention;human factors and ergonomics;injury prevention;authentication;internet privacy;secure coding;computer security;biometrics	Security	-62.1562137043652	56.18333750275849	50139
724a4199f21cf7f947d7feec6395b7463bcba412	energy-efficient location and activity-aware on-demand mobile distributed sensing platform for sensing as a service in iot clouds	context aware selective sensing activity aware on demand mobile distributed sensing platform sensing as a service iot clouds internet of things energy efficient location aware mobile sensing platform context aware mobile sensor data engine c mosden selective sensing;sensors;data collection;selective sensing activity awareness cloud sensing middlware platforms context awareness data filtering distributed sensing internet of things iot location awareness;power aware computing cloud computing distributed sensors energy conservation internet of things mobile computing;mobile communication;sensors cloud computing mobile communication data collection context awareness internet of things middleware;context;cloud computing;data models	The Internet of Things (IoT) envisions billions of sensors deployed around us and connected to the Internet, where the mobile crowd sensing technologies are widely used to collect data in different contexts of the IoT paradigm. Due to the popularity of Big Data technologies, processing and storing large volumes of data have become easier than ever. However, large-scale data management tasks still require significant amounts of resources that can be expensive regardless of whether they are purchased or rented (e.g., pay-as-you-go infrastructure). Further, not everyone is interested in such large-scale data collection and analysis. More importantly, not everyone has the financial and computational resources to deal with such large volumes of data. Therefore, a timely need exists for a cloud-integrated mobile crowd sensing platform that is capable of capturing sensors data, on-demand, based on conditions enforced by the data consumers. In this paper, we propose a context-aware, specifically, location and activity-aware mobile sensing platform called context-aware mobile sensor data engine (C-MOSDEN) for the IoT domain. We evaluated the proposed platform using three real-world scenarios that highlight the importance of selective sensing. The computational effectiveness and efficiency of the proposed platform are investigated and are used to highlight the advantages of context-aware selective sensing.	big data;channel (communications);computation;computational resource;computer data storage;context awareness;internet of things;location awareness;overhead (computing);programming paradigm;requirement;sensor	Charith Perera;Dumidu S. Talagala;Chi Harold Liu;Júlio Cezar Estrella	2015	IEEE Transactions on Computational Social Systems	10.1109/TCSS.2016.2515844	data modeling;mobile search;mobile telephony;cloud computing;computer science;sensor;operating system;internet privacy;world wide web;computer security;statistics;data collection	Mobile	-41.915841410261116	50.18915671578927	50220
deb4af26903cbd12bb96ee5b2965d2f49b83168a	vulseeker: a semantic learning based vulnerability seeker for cross-platform binary		Code reuse improves software development efficiency, however, vulnerabilities can be introduced inadvertently. Many existing works compute the code similarity based on CFGs to determine whether a binary function contains a known vulnerability. Unfortunately, their performance in cross-platform binary search is challenged.   This paper presents VulSeeker, a semantic learning based vulnerability seeker for cross-platform binary. Given a target function and a vulnerable function, VulSeeker first constructs the labeled semantic flow graphs and extracts basic block features as numerical vectors for both of them. Then the embedding vector of the whole binary function is generated by feeding the numerical vectors of basic blocks to the customized semantics aware DNN model. Finally, the similarity of the two binary functions is measured based on the Cosine distance. The experimental results show that VulSeeker outperforms the state-of-the-art approaches in terms of accuracy. For example, compared to the most recent and related work Gemini, VulSeeker finds 50.00% more vulnerabilities in the top-10 candidates and 13.89% more in the top-50 candidates, and improves the values of AUC and ACC for 8.23% and 12.14% respectively. The video is presented at https://youtu.be/Mw0mr84gpI8.	basic block;binary search algorithm;code reuse;cosine similarity;numerical analysis;software development	Jian Gao;Xin Yang;Ying Fu;Yu Jiang;Jia-Guang Sun	2018		10.1145/3238147.3240480	computer science;theoretical computer science;binary search algorithm;semantics;code reuse;basic block;binary number;embedding;software development;binary function	SE	-59.88804916975991	40.20946151797567	50263
e25e17b268755baa4e36eefc278e4bedde5df2e4	recovering test-to-code traceability via slicing and conceptual coupling	software maintenance;unit testing;empirical studies unit testing slicing conceptual coupling traceability;semantics;testing;accuracy;slicing;large scale integration;conceptual coupling;couplings accuracy testing semantics programming conferences large scale integration;empirical studies;couplings;traceability;programming;traceability links maintenance test to code traceability recovery dynamic slicing conceptual coupling software system development software system evolution;conferences	Maintaining trace ability links between application code and unit test cases plays an important role for effectively managing the development and evolution of software systems. Unfortunately, the support in the contemporary development environment to identify such links is still inadequate. This research presents an automated solution to recover trace ability links between test cases and classes under test. The approach is based on dynamic slicing and conceptual coupling. A preliminary evaluation indicates that the proposed approach identifies trace ability links between unit test cases and tested classes with a high accuracy and greater stability than existing techniques.	software system;test case;traceability;unit testing	Abdallah Qusef	2011	2011 18th Working Conference on Reverse Engineering	10.1109/WCRE.2011.59	reliability engineering;programming;traceability;computer science;systems engineering;engineering;software engineering;semantics;accuracy and precision;software testing;unit testing;coupling;empirical research;software maintenance;engineering drawing	SE	-61.725067781454804	33.328913172772424	50277
0303489ae30cd330b2d33c48e57ae484c0dd009e	compliance monitor for early warning risk determination	early warning	The paper reports on a reference monitor for early warning risk determination for privacy violations in the context of business compliance and demonstrates its applicability in the particular case of anonymity. To this end, the monitor detects system executions that potentially lead to incompliant states before the actual violation by determining the risk they pose to compliance goals and warning officers responsible for compliance about risky executions. In doing so, the presented monitor is a novel technique to automate some of the tasks involved in guaranteeing compliance.		Rafael Accorsi;Yoshinori Sato;Satoshi Kai	2008	Wirtschaftsinformatik	10.1007/s11576-008-0079-0	simulation;computer science;data mining;privacy;computer security	NLP	-57.43722910475061	50.043959091967665	50357
7575023f6432439babcceb599d6d6af1b4200ea5	sensing wifi packets in the air: practicality and implications in urban mobility monitoring	crowdsensing;wifi monitor mode;smartphone sensing	Mobile sensing systems employ various sensors in smartphones to extract human-related information. As the demand for sensing systems increases, a more effective mechanism is required to sense information about human life. In this paper, we present a systematic study on the feasibility and gaining properties of a crowdsensing system that primarily concerns sensing WiFi packets in the air. We propose that this method is effective for estimating urban mobility by using only a small number of participants. During a seven-week deployment, we collected smartphone sensor data, including approximately four million WiFi packets from more than 130,000 unique devices in a city. Our analysis of this dataset examines core issues in urban mobility monitoring, including feasibility, spatio-temporal coverage, scalability, and threats to privacy. Collectively, our findings provide valuable insights to guide the development of new mobile sensing systems for urban life monitoring.	crowdsensing;scalability;sensor;smartphone;software deployment	Yohan Chon;Suyeon Kim;Seungwoo Lee;Dongwon Kim;Yungeun Kim;Hojung Cha	2014		10.1145/2632048.2636066	simulation;telecommunications;computer security;computer network	Mobile	-42.0278160590051	51.427689201382876	50431
1f4e3b2055aa347fcb5836135d9bff61523557f9	engineering ambient intelligence services by means of mabs	location based service;hidden markov model;ambient intelligence;contextual information;multi agent based simulation	  In this work, the methodology AmISim to test and to deployment of Ambient Intelligence (AmI) system is presented. The development  of AmI systems is a complex task because this technology must adapt to users and contextual information as well as unpredictable  and changeable behaviours. So, we focused in how the methodology AmISim can help to the engineering of adaptative services  for users. In this case, we propose a predictor of location based on Hidden Markov Models (HMMs). So, the system can offer  Location-Based Services(LBS) that adapt to the users. To this end, we propose a methodology based on a previous social multi-agent  based simulation (MABS) and a following deployment of the service in a real environment.    	ambient intelligence	Teresa García-Valverde;Alberto García-Sola;Francisco Lopez-Marmol;Juan A. Botía Blaya	2010		10.1007/978-3-642-12433-4_5	simulation;engineering;machine learning;data mining	AI	-41.24559628751115	42.697034217988204	50441
61133d547f08b180d996df64aa60a8f2015e90c2	how hard is assessing and measuring resilience? panel	computers;software;resilience bonding current measurement costs measurement standards coordinate measuring machines security monitoring blades sonos devices;industries;measurement uncertainty;bonding;current measurement;resilience;monitoring;blades;sonos devices;measurement standards;coordinate measuring machines;security;benchmark testing	Cost pressure, short time to market, and increased complexity are responsible for an evident increase of the failure rate of computing systems, while the cost of failures is growing rapidly, as a result of an unprecedented degree of dependence of our society on computing systems. The combination of these factors has created a dependability and security gap that is often perceived by users as a lack of trustworthiness in computer applications, but that is in fact undermining the network and service infrastructures that constitute the very core of the knowledge-based society.	complexity;computer;dependability;failure rate;knowledge-based systems;trust (emotion)	Andrea Bondavalli	2008	2008 Seventh European Dependable Computing Conference	10.1109/EDCC-7.2008.25	reliability engineering;benchmark;computer science;engineering;information security;forensic engineering;computer security;psychological resilience;statistics;measurement uncertainty	HPC	-59.81875197384203	48.35901189319955	50465
25659749d914ea4d76ae19763b22815c6aac8de7	automatic generation of executable assertions for runtime checking temporal requirements	resource restricted embedded environment automatic generation runtime checking temporal requirement safety critical system model checking approach runtime verification code generation linear temporal logic program execution;formal specification;runtime verification;runtime software testing system testing automata logic devices protocols hardware logic programming information systems environmental economics;temporal logic;code generation;automatic generation;formal verification;model checking;program testing;linear temporal logic;safety critical software;program testing safety critical software formal verification formal specification temporal logic program compilers;safety critical system;program compilers	Checking various temporal requirements is a key dependability concern in safety-critical systems. As model-checking approaches do not scale well to systems of high complexity the runtime verification of temporal requirements has received a growing attention recently. This paper presents a code-generation based method for runtime evaluation of linear temporal logic formulae over program execution traces. The processing-power requirements of our solution are much lower than in case of previous approaches enabling its application even in resource-restricted embedded environments.	dependability;effective method;embedded system;executable;experiment;linear temporal logic;mathematical optimization;model checking;programming language;requirement;runtime verification;tracing (software)	Gergely Pintér;István Majzik	2005	Ninth IEEE International Symposium on High-Assurance Systems Engineering (HASE'05)	10.1109/HASE.2005.6	model checking;computer architecture;linear temporal logic;real-time computing;temporal logic;formal verification;interval temporal logic;computation tree logic;computer science;formal specification;runtime verification;programming language;code generation	Embedded	-44.867489789368236	32.52062884889196	50529
560a6b54389863091ac47b5f6294b489e9b0633b	comparing finite state machine test	transition state;finite state machine test coverage criteria;state coverage;software fault tolerance finite state machines greedy algorithms program testing;test cubierta;software behaviour;algorithme glouton;maquina estado finito;software fault tolerance;greedy algorithms;probabilistic approach;finite state machines;program testing;estado transitorio;enfoque probabilista;approche probabiliste;test coverage;software behaviour finite state machine test coverage criteria transition coverage state coverage initialisation fault transition fault coverage greedy algorithm;initialisation fault;greedy algorithm;algoritmo gloton;transition coverage;machine etat fini;couverture test;finite state machine;transition fault coverage;etat transition	To plan testing activities, testers face the challenge of determining a strategy, including a test coverage criterion that offers an acceptable compromise between the available resources and test goals. Known theoretical properties of coverage criteria do not always help and, thus, empirical data are needed. The results of an experimental evaluation of several coverage criteria for finite state machines (FSMs) are presented, namely, state and transition coverage; initialisation fault and transition fault coverage. The first two criteria focus on FSM structure, whereas the other two on potential faults in FSM implementations. The authors elaborate a comparison approach that includes random generation of FSM, construction of an adequate test suite and test minimisation for each criterion to ensure that tests are obtained in a uniform way. The last step uses an improved greedy algorithm.	finite-state machine	Adenilso da Silva Simão;Alexandre Petrenko;José Carlos Maldonado	2009	IET Software	10.1049/iet-sen.2008.0018	modified condition/decision coverage;reliability engineering;greedy algorithm;simulation;fault coverage;computer science;engineering;finite-state machine;algorithm	SE	-60.019855417894114	33.160922989578935	50576
83b8bcd90cb8ce9cb3659dd3aec044180ccf1950	vehicle automation in cooperation with v2i and nomadic devices communication	mobile communication systems;wireless communication systems;intelligent transportation systems;automated highways;vehicles automation sensor systems humans actuators trajectory;vehicle to infrastructure communications;automotive systeme;intelligent vehicles;traffic infrastructure vehicle automation v2i nomadic devices communication wireless vehicular communication driving automation functions complex test scenario automated driving environment perception communication with the infrastructure vehicle to infrastructure automated its integrated its design assembling vehicle technology and communication technology;radiocommunication;radiocommunication automated highways	Introduction of wireless vehicular communication allows for cooperation between vehicles and infrastructure, thus enabling a variety of new ITS use cases [1]. Furthermore, modern vehicles feature an increasing number of driving automation functions. In this paper a complex test scenario including automated driving, environment perception, communication with the infrastructure (Vehicle to Infrastructure, V2I) and with nomadic devices is described. Based on this, the requirements for a cooperative and automated ITS are identified. In order to demonstrate the test scenario, an integrated ITS design is proposed assembling Vehicle Technology and Communication Technologies with the Traffic Infrastructure and Nomadic Devices. This has been implemented in a prototype for evaluation in February 2011.	aim alliance;automation;autonomous car;ccir system a;dynamic language runtime;microsoft outlook for mac;prototype;requirement;scenario testing;testbed	Christian Loper;Alvaro Catala-Prat;Jan Gacnik;Jan Schomerus;Frank Köster	2011	2011 14th International IEEE Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2011.6082897	embedded system;simulation;engineering;transport engineering	Robotics	-37.18697424552986	44.509627989347656	50612
645904316abade31d06320e1388019538571a149	revamping javascript static analysis via localization and remediation of root causes of imprecision	program analysis;javascript	Static analysis is challenged by the dynamic language constructs of JavaScript which often lead to unacceptable performance and/or precision results. We describe an approach that focuses on improving the practicality and accuracy of points-to analysis and call graph construction for JavaScript programs. The approach first identifies program constructs which are sources of imprecision (i.e., root causes) through monitoring the static analysis process. We then examine and suggest specific context-sensitive analyses to apply. Our technique is able to to find that the root causes comprise less than 2% of the functions in JavaScript library applications. Moreover, the specialized analysis derived by our approach finishes within a few seconds, even on programs which can not complete within 10 minutes with the original analysis.	call graph;context-sensitive grammar;javascript library;pointer analysis;static program analysis	Shiyi Wei;Omer Tripp;Barbara G. Ryder;Julian Dolby	2016		10.1145/2950290.2950338	program analysis;computer science;theoretical computer science;javascript;programming language;world wide web	SE	-57.442296475405776	38.81769203003129	50625
89f69bbcd2a5e1515b0fafbd42f37b2a243cf9a2	testing protocols in internet of things by a formal passive technique		In recent years, Extensible Messaging and Presence Protocol (XMPP) is gaining momentum in Internet of Things (IoT). It has been widely used in chatting, message exchanging and unique addressing. As a matter of course, it raises an interesting issue: how to formally test the conformance and performance of XMPP in IoT environment. While conformance testing of communicating protocols is a functional test that verifies whether the behaviors of the protocol satisfy defined requirements, performance testing is a qualitative and quantitative test that aims at checking whether the performance requirements of the protocol are satisfied under certain conditions. In this paper, we present a logic-based passive testing approach that can test both the conformance and the performance of XMPP protocol through real execution traces and formally specified properties. To evaluate and assess our methodology, we present a developed prototype and the experiments with a set of XMPP properties. Finally, the relevant verdicts and conclusions are provided.	benchmark (computing);communications protocol;complex network;conformance testing;experiment;functional testing;internet of things;online chat;prototype;requirement;software performance testing;tracing (software)	Xiaoping Che;Stéphane Maag	2014	Science China Information Sciences	10.1007/s11432-014-5068-x	real-time computing;computer science;distributed computing;computer security	SE	-38.257157922565554	35.57351370399538	50629
346afe0396c1f6ef4cc80371ab2523e2371af818	towards privacy-preserving data sharing in smart environments	anonymous credential systems;cryptography context internet data privacy cities and towns privacy;attribute based cryptography;internet of things;internet of things cryptography data privacy internet;internet;data privacy;cryptography;cities and towns;context;privacy;anonymous credential systems privacy internet of things attribute based cryptography;future internet privacy preserving data sharing smart environments iot internet of things attribute based cryptography anonymous credential systems	The increasing development of IoT is dramatically changing the way people share information and communicate with their surrounding environment, enabling a constant, invisible and sometimes unintended data exchange, between things and people. This extension of the current Internet to everyday life scenarios leads identity management has to face new challenges due to the inherent nature and requirements of this paradigm. This work gives an overview of recent privacy enhanced technologies such as attribute-based cryptography and anonymous credential systems, as well as their application on IoT scenarios. Additionally, based on such mechanisms, we offer a framework which is designed to enable a secure and privacy-aware data sharing on the Future Internet.	cryptography;digital credential;future internet;identity management;internet of things;privacy;programming paradigm;requirement;smart environment;smart objects	José Luis Hernández Ramos;Jorge Bernal Bernabé;Antonio F. Gómez-Skarmeta	2014	2014 Eighth International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing	10.1109/IMIS.2014.44	privacy software;the internet;information privacy;privacy by design;computer science;cryptography;internet privacy;privacy;world wide web;computer security;internet of things	HCI	-44.509530644448546	59.82947317404539	50708
1c3c5d39c9bb70597ca5a688e2d4471f8c1e1c5c	combining social authentication and untrusted clouds for private location sharing	key management;location tracking;presence systems;security;privacy	Recently, many location-sharing services (LSSs) have emerged that share data collected using mobile devices. However, research has shown that many users are uncomfortable with LSS operators managing their location histories, and that the ease with which contextual data can be shared with unintended audiences can lead to regrets that sometimes outweigh the benefits of these systems. In an effort to address these issues, we have developed SLS: a secure location sharing system that combines location-limited channels, multi-channel key establishment, and untrusted cloud storage to hide user locations from LSS operators while also limiting unintended audience sharing. In addition to describing the key agreement and location-sharing protocols used by SLS, we discuss an iOS implementation of SLS that enables location sharing at tunable granularity through an intuitive policy interface on the user's mobile device.	authentication;cloud storage;key exchange;mobile device;standard sea level;ios	Andrew K. Adams;Adam J. Lee	2013		10.1145/2462410.2462421	computer science;information security;key management;internet privacy;privacy;world wide web;computer security	HCI	-41.218115389612	59.98846631916422	50793
28e4460320a3f690805e1c0501ce27f5decf41d7	the digihome service-oriented platform	context awareness;sca;smart homes	Nowadays, the computational devices are everywhere. In malls, offices, streets, cars, and even homes, we can find devices providing and consuming functionality to improve the user satisfaction. These devices include sensors that provide information about the environment state (e.g., temperature, occupancy, light levels), service providers (e.g., Internet TVs, GPS), smartphones (that contain user preferences), and actuators that act on the environment (e.g., closing the blinds, activating the alarm, changing the temperature). Although these devices exhibit communication capabilities, their integration into a larger monitoring system remains a challenging task, partly because of the strong heterogeneity of technologies and protocols. Therefore, in this article, we focus on home environments and propose a middleware solution, called DigiHome, that applies the Service Component Architecture (SCA) component model to integrate data and events generated by heterogeneous devices in this kind of environments. DigiHome exploits the SCA extensibility to incorporate the REpresentational State Transfer (REST) architectural style and, in this way, leverages on the integration of multiscale systems-of-systems (from wireless sensor networks to the Internet). Additionally, the platform applies Complex Event Processing technology that detects application-specific situations. We claim that the modularization of concerns fostered by DigiHome and materialized in a service-oriented architecture, makes it easier to incorporate new services and devices in smart home environments. The benefits of the DigiHome platform are demonstrated on smart home scenarios covering home automation, emergency detection, and energy saving situations. Copyright © 2011 John Wiley & Sons, Ltd.	closing (morphology);complex event processing;component-based software engineering;extensibility;global positioning system;home automation;internet explorer;middleware;representational state transfer;sensor;service component architecture;service-oriented architecture;service-oriented device architecture;smartphone;system of systems;user (computing);window blind	Daniel Romero;Gabriel Hermosillo;Amirhosein Taherkordi;Russel Nzekwa;Romain Rouvoy;Frank Eliassen	2013	Softw., Pract. Exper.	10.1002/spe.1125	simulation;engineering;operating system;world wide web;computer security	Mobile	-40.964941678446095	45.94122057186976	50935
42fbd41de73056708b9c948b185830c6c062b8f3	towards automatic construction of reusable prediction models for component-based performance engineering	software component;reverse engineering;static analysis;prediction model;software architecture	Performance predictions for software architectures can reveal performance bottlenecks and quantitatively support design decisions for different architectural alternatives. As software architects aim at reusing existing software components, their performance properties should be included into performance predictions without the need for manual modelling. However, most prediction approaches do not include automated support for modelling implemented components. Therefore, we propose a new reverse engineering approach, which generates Palladio performance models from Java code. In this paper, we focus on the static analysis of Java code, which we have implemented as an Eclipse plugin called Java2PCM. We evaluated our approach on a larger component-based software architecture, and show that a similar prediction accuracy can be achieved with generated models compared to completely manually specified ones.	component-based software engineering;dynamic program analysis;eclipse;embedded system;java;performance engineering;reverse engineering;software architect;software architecture;static program analysis;toolchain	Thomas Kappler;Heiko Koziolek;Klaus Krogmann;Ralf H. Reussner	2008			performance prediction;component-based software engineering;resource-oriented architecture;software construction;computer architecture;performance engineering;reference architecture;software verification and validation;computer science;software sizing	SE	-56.471596340122005	32.943534468446195	50941
34d41aba9621ed1c565f1e47416b7486857dabf6	learning-based testing for autonomous systems using spatial and temporal requirements		Cooperating cyber-physical systems-of-systems (CO-CPS) such as vehicle platoons, robot teams or drone swarms usually have strict safety requirements on both spatial and temporal behavior. Learning-based testing is a combination of machine learning and model checking that has been successfully used for black-box requirements testing of cyber-physical systems-of-systems. We present an overview of research in progress to apply learning-based testing to evaluate spatio-temporal requirements on autonomous systems-of-systems through modeling and simulation.	autonomous system (internet);black box;cyber-physical system;machine learning;model checking;requirement;simulation	Hojat Khosrowjerdi;Karl Meinke	2018		10.1145/3243127.3243129	machine learning;model checking;artificial intelligence;white-box testing;automotive software;robot;model-based testing;autonomous system (internet);computer science;modeling and simulation	SE	-45.401975542180054	36.13641986900416	50948
306d80ac58718b4d13bf45add0724017e5c2ed9c	cloudedge: a content delivery system for storage service in cloud environment	distributed system;content manipulation;web information system;caching;content delivery networks;distributed computing;edge network;web information systems;web applications;storage service;content delivery;web services;simple storage service;ubiquitous computing;cdn;distributed file system;access control;content delivery network;amazon web service;article;distributed file systems;cloud computing	With the trend of cloud computing, more and more web applications move their content to external storage services to reduce the cost of hardware and maintenance. In this paper, a novel architecture called CloudEdge for Content Delivery Network (CDN) with the storage service in cloud computing is introduced. This architecture could not only keep the loosely coupled integration between storage service and web applications but also provide better content manipulation features such as edge network content delivery, caching, secured access control, the variations of content objects generated on demand, and post-process on content objects.		Chi-Huang Chiu;Hsien Tang Lin;Shyan-Ming Yuan	2010	IJAHUC	10.1504/IJAHUC.2010.035536	web service;web application;edge;cloud computing;content management;computer science;access control;operating system;database;internet privacy;distributed file system;world wide web;ubiquitous computing	Web+IR	-36.619692088922754	57.57305628430853	50999
9b704e2d41b7d023420ec2ac3c65439441702043	kam1n0: mapreduce-based assembly clone search for reverse engineering	information retrieval;assembly clone search;mining software repositorie	Assembly code analysis is one of the critical processes for detecting and proving software plagiarism and software patent infringements when the source code is unavailable. It is also a common practice to discover exploits and vulnerabilities in existing software. However, it is a manually intensive and time-consuming process even for experienced reverse engineers. An effective and efficient assembly code clone search engine can greatly reduce the effort of this process, since it can identify the cloned parts that have been previously analyzed. The assembly code clone search problem belongs to the field of software engineering. However, it strongly depends on practical nearest neighbor search techniques in data mining and databases. By closely collaborating with reverse engineers and Defence Research and Development Canada (DRDC), we study the concerns and challenges that make existing assembly code clone approaches not practically applicable from the perspective of data mining. We propose a new variant of LSH scheme and incorporate it with graph matching to address these challenges. We implement an integrated assembly clone search engine called Kam1n0. It is the first clone search engine that can efficiently identify the given query assembly function's subgraph clones from a large assembly code repository. Kam1n0 is built upon the Apache Spark computation framework and Cassandra-like key-value distributed storage. A deployed demo system is publicly available. Extensive experimental results suggest that Kam1n0 is accurate, efficient, and scalable for handling large volume of assembly code.	apache cassandra;apache spark;assembly language;attribute–value pair;clustered file system;computation;data mining;database;duplicate code;exploit (computer security);mapreduce;matching (graph theory);nearest neighbor search;reverse engineering;scalability;search problem;sensor;software engineering;software patent;static program analysis;vulnerability (computing);web search engine;lsh	Steven H. H. Ding;Benjamin C. M. Fung;Philippe Charland	2016		10.1145/2939672.2939719	computer science;bioinformatics;data mining;database;world wide web	SE	-60.51568266361517	39.93490225856178	51011
2488323bfd0a355f4353946803bfea1069e352bf	an embedded software architecture for robot with variable structures	robot sensing systems;layered architecture;variable structure;feature oriented analysis;hardware sensor;robotic system;software engineering;embedded system;pob robot module;pob robot module embedded software architecture robotic system variable structures hardware sensor hardware assembling structure software engineering feature oriented analysis xml configuration file architecture mapping layer application layer lego robot module;computer architecture;embedded systems;software architecture;xml configuration file;robots;application layer;xml;architecture mapping layer;embedded system embedded software software architecture robot variable structures;hardware assembling structure;embedded software computer architecture hardware robot sensing systems application software robotic assembly atomic layer deposition xml software architecture software engineering;software product line;robot;variable structures;embedded software architecture;xml embedded systems robots software architecture;embedded software;lego robot module;hardware	The application of robotic system has nowadays been vastly incorporated in many domains. To support customized demands, robotic embedded software oftentimes may face the variation challenges on hardware sensor, hardware assembling structure, and hardware platform. Basing on the perspective of software engineering, the paper endeavors to explore the embedded software architecture. We have concluded 2 commonly adopted embedded software architectures, and thus proposed a new 7 layered architecture. The concept of software product line was integrated to engage Feature-Oriented Analysis on a specific domain in designing features of atomic operation interface layer. To describe dynamic function composition of the system, XML configuration file was employed on Architecture Mapping layer. The intended basic functions on the Application layer were composed by Atomic operation. When hardware or software function demand is changed, we only need to modify the XML configuration file. Thus, embedded software portability and software/hardware variability can be enhanced and thereby reduces the impacts of the aforementioned. Basing on the 3 software architectures proposed, we have conducted implementation on LEGO robot module and POB robot module. The results of the experiments indicated that execution efficiency was not distinguishably reduced, and that nearly no impact was performed on robotic actions, which manifested the feasibility of our software architecture.	embedded software;embedded system;experiment;heart rate variability;linearizability;robot;software architecture;software engineering;software portability;software product line;xml	Chorng-Shiuh Koong;Hung-Jui Lai;Kuan-Chou Lai	2009	2009 Fourth International Conference on Frontier of Computer Science and Technology	10.1109/FCST.2009.114	multilayered architecture;robot;reference architecture;embedded system;software architecture;real-time computing;software sizing;computer science;package development process;backporting;software framework;component-based software engineering;software development;software design description;operating system;software construction;hardware architecture;software architecture description;resource-oriented architecture;software deployment;software system;avionics software	Robotics	-38.31741055034623	39.03033755005247	51077
dc8ebebd2a6396f45fc767b22f3c35f660ac2f5f	rfid sensor-tags feeding a context-aware rule-based healthcare monitoring system	context awareness;pervasive computing;sensor integration;healthcare monitoring system;uhf rfid;ontology	Along with the growing of the aging population and the necessity of efficient wellness systems, there is a mounting demand for new technological solutions able to support remote and proactive healthcare. An answer to this need could be provided by the joint use of the emerging Radio Frequency Identification (RFID) technologies and advanced software choices. This paper presents a proposal for a context-aware infrastructure for ubiquitous and pervasive monitoring of heterogeneous healthcare-related scenarios, fed by RFID-based wireless sensors nodes. The software framework is based on a general purpose architecture exploiting three key implementation choices: ontology representation, multi-agent paradigm and rule-based logic. From the hardware point of view, the sensing and gathering of context-data is demanded to a new Enhanced RFID Sensor-Tag. This new device, de facto, makes possible the easy integration between RFID and generic sensors, guaranteeing flexibility and preserving the benefits in terms of simplicity of use and low cost of UHF RFID technology. The system is very efficient and versatile and its customization to new scenarios requires a very reduced effort, substantially limited to the update/extension of the ontology codification. Its effectiveness is demonstrated by reporting both customization effort and performance results obtained from validation in two different healthcare monitoring contexts.	antenna device component;applicative programming language;choice behavior;customize;declarative programming;document completion status - documented;entity name part qualifier - adopted;gene regulatory network;generic drugs;genetic heterogeneity;greater than;how true feel alert right now;inference engine;information access;logic programming;mandatory - hl7definedroseproperty;microwave;middleware;multi-agent system;multiple organ failure;numerical analysis;numerous;ontology;ontology (information science);patients;pervasive informatics;point of view (computer hardware company);programming paradigm;prototype;radio frequency identification device;radio-frequency identification;real life;reconfigurability;report;request - action;scheduling (computing);scheduling - hl7 publishing domain;skin tag;software agent;software framework;solutions;tag cloud;tracer;ultra high frequency;usability;benefit;sensor (device);standards characteristics	Luca Catarinucci;Riccardo Colella;Alessandra Esposito;Luciano Tarricone;Marco Zappatore	2011	Journal of Medical Systems	10.1007/s10916-011-9794-y	embedded system;real-time computing;computer science;ontology;computer security	HCI	-42.13202016631246	46.83293244443915	51083
0f084f2e975c3f54bbf5740b6d5d261066bfab16	isolated execution on many-core architectures		We explore how many-core platforms can be used to enhance the security of future systems and to support important security properties such as runtime isolation using a small Trusted Computing Base (TCB). We focus on the Intel Single-chip Cloud Computer (SCC) to show that such properties can be implemented in current systems. We design a system called SEMA which offers strong security properties while maintaining high performance and flexibility enabled by a small centralized security kernel. We further implement and evaluate the feasibility of our design. Currently, our prototype security kernel is able to execute applications in isolation and accommodate dynamic resource requests from them. We show that, with minor modifications, many-core architectures can offer some unique security properties, not supported by existing singleand multi-core architectures, such as application context awareness. Context awareness, a new security property that we define and explore in this work, allows each application to discover, without any interaction with the security kernel, which other parts of the system are allowed to interact with it and access its resources. We also discuss how an application can use context awareness to defend itself from an unlikely, yet potentially compromised security kernel.	centralized computing;context (computing);context awareness;kernel (operating system);linux;malware;manycore processor;mobile computing;multi-core processor;operating system;prototype;single-chip cloud computer;trusted computing base	Ramya Jayaram Masti;Devendra Rai;Claudio Marforio;Srdjan Capkun	2014	IACR Cryptology ePrint Archive			Security	-52.69668793388974	56.68261831192521	51164
6eb066c92122bca08e5ddb0d7d32409dd37a8183	a deliberative model for self-adaptation middleware using architectural dependency	deliberative model;architectural dependency;self-adaptation middleware;subsequent architectural reconfigurations;instrumentation service;reference point;extensible beliefs;architectural model;externalized adaptation;crucial prerequisite;reconfiguration change;dependency meta-model;conflict resolution strategy;meta model;software architecture;middleware;conflict resolution;software maintenance	A crucial prerequisite to externalized adaptation is an understanding of how components are interconnected, or more particularly how and why they depend on one another. Such dependencies can be used to provide an architectural model, which provides a reference point for externalized adaptation. In this paper, it is described how dependencies are used as a basis to systems' self-understanding and subsequent architectural reconfigurations. The approach is based on the combination of: instrumentation services, a dependency meta-model and a system controller. In particular, the latter uses self-healing repair rules (or conflict resolution strategies), based on extensible beliefs, desires and intention (EBDI) model, to reflect reconfiguration changes back to a target application under examination.	authentication;authorization;distributed computing;metamodeling;middleware;self-reflection	N. Badr;A. Taleb-Bendiab;Martin Randles;David Reilly	2004	Proceedings. 15th International Workshop on Database and Expert Systems Applications, 2004.	10.1109/DEXA.2004.1333565	metamodeling;software architecture;middleware;real-time computing;simulation;computer science;message oriented middleware;conflict resolution;middleware;database;distributed computing;software maintenance	SE	-41.649941081599884	39.37196188030108	51174
6f7dc21719bd959c49b3d296b7352e15fcfa4502	relationship-based access control for an open-source medical records system	medical records system;relationship based access control;administrative model;authorization principal;authorization graph	Inspired by the access control models of social network systems, Relationship-Based Access Control (ReBAC) was recently proposed as a general-purpose access control paradigm for application domains in which authorization must take into account the relationship between the access requestor and the resource owner. The healthcare domain is envisioned to be an archetypical application domain in which ReBAC is sorely needed: e.g., my patient record should be accessible only by my family doctor, but not by all doctors.  In this work, we demonstrate for the first time that ReBAC can be incorporated into a production-scale medical records system, OpenMRS, with backward compatibility to the legacy RBAC mechanism. Specifically, we extend the access control mechanism of OpenMRS to enforce ReBAC policies. Our extensions incorporate and extend advanced ReBAC features recently proposed by Crampton and Sellwood. In addition, we designed and implemented the first administrative model for ReBAC. In this paper, we describe our ReBAC implementation, discuss the system engineering lessons learnt as a result, and evaluate the experimental work we have undertaken. In particular, we compare the performance of the various authorization schemes we implemented, thereby demonstrating the feasibility of ReBAC.	application domain;authorization;backward compatibility;general-purpose markup language;programming paradigm;role-based access control;social network;systems engineering	Syed Zain R. Rizvi;Philip W. L. Fong;Jason Crampton;James Sellwood	2015		10.1145/2752952.2752962	data mining;database;computer security	Security	-47.07062156856454	54.93341400550848	51329
bd0e07cbe8307af45a340336cb12baea5f253f32	mining temporal specifications from object usage	unique defect;string xml;parameter xml;tikanga prototype;program code;object usage;automatic defect detection · mining specifications · temporal logic · computation tree logic · program analysis;unique code;model checking;fair computation tree logic;temporal specification;operational precondition;ctl f	A caller must satisfy the callee’s precondition—that is, reach a state in which the callee may be called. Preconditions describe the state that needs to be reached, but not how to reach it. We combine static analysis with model checking to mine Fair Computation Tree Logic (CTL F ) formulas that describe the operations a parameter goes through: “In parseProperties(String xml), the parameter xml normally stems from getProperties().” Such operational preconditions can be learned from program code, and the code can be checked for their violations. Applied to AspectJ, our Tikanga prototype found 169 violations of operational preconditions, uncovering 7 unique defects and 27 unique code smells—with 52% true positives in the 25% top-ranked violations.	adaptive server enterprise;aspectj;atomic sentence;c++;code smell;computation tree logic;documentation;formal language;integrated development environment;interprocedural optimization;java;mined;model checking;precondition;programmer;programming language;prototype;sensitivity and specificity;simple set;software bug;static program analysis;subroutine;xml	Andrzej Wasylkowski;Andreas Zeller	2011	Automated Software Engineering	10.1007/s10515-011-0084-1	program analysis;model checking;labeling theory;temporal logic;computation tree logic;computer science;theoretical computer science;formal specification;database;programming language;object-oriented programming;java;computational model;logic;static analysis;satisfiability	SE	-56.90382606874224	40.182487454465075	51368
8986e651e7beecd3cba77d19a9b7d241fc91ff23	toward large-scale situation awareness applications on camera networks	mobility management mobile radio;quality of service cameras mobile computing mobility management mobile radio;cameras programming target tracking runtime sensors surveillance quality of service;cloud computing programming model runtime system situation awareness camera network;quality of service large scale situation awareness applications camera networks ubiquitous deployment video analytics surveillance traffic monitoring programming complexity;quality of service;mobile computing;cameras	Ubiquitous deployment of cameras and recent advances in video analytics enable a new class of applications, situation awareness using camera networks. The application class includes surveillance, traffic monitoring, and assisted living that autonomously generate actionable knowledge from a large number of camera streams. Despite technological advances, developing a large-scale situation awareness application still remains a challenge due to the programming complexity, highly dynamic workloads, and latency-sensitive quality of service. To solve the problem, my research topic concerns developing a programming model and a runtime system to support large-scale situation awareness applications on camera networks. The programming model requires a minimal set of domain-specific handlers from the domain experts, allowing them to focus on the algorithmic aspect of situation awareness applications rather than the details of distributed programming. The runtime system provides automatic resource management using smart cameras and the cloud for handling dynamic workloads and ensuring latency-sensitive quality of services.	distributed computing;domain driven data mining;programming complexity;programming model;quality of service;runtime system;software deployment;video content analysis;website monitoring	Kirak Hong	2013	2013 IEEE International Conference on Pervasive Computing and Communications Workshops (PERCOM Workshops)	10.1109/PerComW.2013.6529530	embedded system;real-time computing;simulation;quality of service;computer science;operating system;mobile computing;computer security	Mobile	-40.86293878377558	49.53935425929674	51373
1f9c32c2837171276618faa436f2ec2c314a6caa	policy exchange and management for policy compliance and change detection system in managed service in data networks	risk management auditing compliance control computer network management network servers;servers lead authentication terminology;network interruptions policy exchange policy management policy compliance change detection system managed service data networks network devices configuration configuration errors policy violations policy inefficiencies vulnerable states security threats faulty configurations noncompliant configurations network management landscape automated process manage risks audit configurations centralized reporting runtime compliance manager rcm common policy language organizational policy policy decision control server policy clients;policy client common policy language policy decision control server policy management	Greater frequencies of change to network devices configuration is a potentially disruptive factor in an already complex and challenging networked universe. Changes may lead to potential configuration errors, policy violations, inefficiencies, vulnerable states, and security threats that are “allowed in” through faulty or non-compliant configurations. The current Network Management landscape is in dire need for an automated process to prioritize and manage risks, audit configurations against internal policies or external best practices, and provide centralized reporting for monitoring and regulatory purposes in real time. Our proposed Policy Compliance and Change Detection System is implemented using two modules: The Runtime Compliance Manager (RCM), and the Common Policy Language (CPL). The CPL is used to express device and organizational policies, and was detailed. This paper describes the design requirements for policy exchange and management between the Policy Decision Control Server and the Policy Clients, and describes the methods for keeping the policy between the Policy Client and the Policy Decision Control Server in sync despite network interruptions.	best practice;cpl;centralized computing;reliability-centered maintenance;requirement	Saeed M. Agbariah	2014	The 2014 International Symposium on Networks, Computers and Communications	10.1109/SNCC.2014.6866524	environmental resource management;policy and charging rules function;business;network security policy;computer security	Security	-49.993443310250136	52.73506422574272	51374
154a4d438ae2271d4d924df651070ddf8e20856c	using relational topic models to capture coupling among classes in object-oriented software systems	software metrics;generative probabilistic model;object oriented methods;relational topic models;software measurement;source coding object oriented methods software metrics;relational topic based coupling;bismuth;couplings software systems object oriented modeling bismuth software measurement strontium;software systems;object oriented software;strontium;open source software system;object oriented software system;probabilistic model;source code element;impact analysis;generative probabilistic model relational topic model object oriented software system coupling metrics software metrics source code element relational topic based coupling open source software system relational topic models;source code;couplings;coupling metrics;relational topic model;object oriented modeling;open source software;source coding	Coupling metrics capture the degree of interaction and relationships among source code elements in software systems. A vast majority of existing coupling metrics rely on structural information, which captures interactions such as usage relations between classes and methods or execute after associations. However, these metrics lack the ability to identify conceptual dependencies, which, for instance, specify underlying relationships encoded by developers in identifiers and comments of source code classes. We propose a new coupling metric for object-oriented software systems, namely Relational Topic based Coupling (RTC) of classes, which uses Relational Topic Models (RTM), generative probabilistic model, to capture latent topics in source code classes and relationships among them. A case study on thirteen open source software systems is performed to compare the new measure with existing structural and conceptual coupling metrics. The case study demonstrates that proposed metric not only captures new dimensions of coupling, which are not covered by the existing coupling metrics, but also can be used to effectively support impact analysis.	coupling (computer programming);identifier;interaction;loose coupling;open-source software;software system;statistical model	Malcom Gethers;Denys Poshyvanyk	2010	2010 IEEE International Conference on Software Maintenance	10.1109/ICSM.2010.5609687	computer science;engineering;theoretical computer science;software engineering;data mining;database;source code	SE	-57.23139363227796	33.69644367970287	51377
4b2689a1e93d4246b13c7066d26c441cb166ab27	architecture of an industrial analog input designed to meet safety requirements		A remote module designed for reading analog inputs in a distributed safety-related system must strictly follow IEC 61508 standard. This standard forces the choice of an appropriated degree of fault tolerance and a high coverage of dangerous failures. This paper presents the hardware architecture we developed and the necessary error detection strategies to claim a high safety integrity level (SIL 3). The module will be part of a remote station that is under production by our partner company, a Brazilian manufacturer of industrial control equipment.	error detection and correction;fault tolerance;mandatory integrity control;requirement	Joao de Moraes;Taisy Weber;Guilherme Muller;Tiago Dall'Agnol;Rafael Macedo;Elaine P. L. Scartezzini;Roque E. Dapper;Sérgio Cechin;Joao Netto	2018	2018 IEEE 19th Latin-American Test Symposium (LATS)	10.1109/LATW.2018.8349673	architecture;error detection and correction;iec 61508;fault tolerance;real-time computing;safety integrity level;hardware architecture;computer science	Embedded	-35.412371820272035	36.58418630221299	51415
3a466b4f10016aa3d80e98d76be53592e5524197	recent-secure authentication: enforcing revocation in distributed systems	authentication revocation;public key cryptography;authentication public key cryptography authorization delay network servers electronic commerce information retrieval protection public key business;distributed system;electronic commerce;formal specification;authenticating entities;information retrieval;immediacy;distributed processing;authentication;credentials;revocation enforcement;protection;network servers;certificates;public key;general methods;highly available revocation service;fresh statements;trusted intermediaries;distributed processing message authentication formal specification;business;secure revocation service;initial policy assumptions;authenticated statements;highly available revocation service formal specification reasoning distributed systems revocation enforcement immediacy authentication revocation authenticating entities freshness constraints credentials authenticated statements trusted intermediaries recent secure authentication fresh statements initial policy assumptions certificates secure revocation service;authorization;freshness constraints;message authentication;recent secure authentication;reasoning;distributed systems	A general method is described for formally specifying and reasoning about distributed systems with any desired degree of immediacy for revoking authentica-tion. To effect revocation, 'authenticating entities' impose freshness constraints on credentials or authenticated statements made by trusted intermediaries. If fresh statements are not presented, then the authentica-tion is questionable. Freshness constraints are derived from initial policy assumptions and authentic statements made by trusted intermediaries. By adjusting freshness constraints, the delay for certain revocation can be arbitrarily bounded. We illustrate how the inclusion of freshness policies within certificates enables the design of a secure and highly available revocation service. We illustrate the application of the method and new techniques in an example.	authentication;credential;distributed computing;entity;formal specification;public key certificate;replay attack	Stuart G. Stubblebine	1995		10.1109/SECPRI.1995.398935	e-commerce;computer science;internet privacy;public-key cryptography;computer security;computer network	Security	-44.664652629032325	56.88128698500941	51422
903616bd4a04ddeffa3dd6c2dd1dc07c9cb488eb	a framework for qos-aware binding and re-binding of composite web services	recovery actions;composite web service;service composition;service provider;service oriented architectures;web service;functional equivalence;dynamic binding;composite web services;quality of service;service oriented architecture;ge netic algorithm	QoS-aware dynamic binding of composite services provides t h capability of binding each service invocation in a composition to a service chosen among a set of functionally equivalent ones to achieve a QoS goal, for example minimizing the response time while li miting the price under a maximum value. This paper proposes a QoS-aware binding approach based on Ge netic Algorithms. The approach includes a feature for early run-time re-binding whenever t he actual QoS deviates from initial estimates, or when a service is not available. The approach has been impl e ented in a framework and empirically assessed through three different service compositions.	late binding;name binding;quality of service;response time (technology);web service	Gerardo Canfora;Massimiliano Di Penta;Raffaele Esposito;Maria Luisa Villani	2008	Journal of Systems and Software	10.1016/j.jss.2007.12.792	web service;reliability engineering;service level requirement;service level objective;mobile qos;application service provider;business service provider;differentiated service;computer science;basic service;service delivery framework;service-oriented architecture;database;data as a service;customer service assurance;world wide web	Web+IR	-47.621355062191576	42.986396202489715	51427
9807ce91c15a878fa3d6d386bf41d3fbcd97cce5	a decentralized authorization mechanism for e-business applications	financial losses decentralized authorization mechanism e business applications security critical action authorization software agents trust relations;electronic commerce;authorisation;software agent;trust management;electronic commerce authorisation;authorization application software public key humans software agents robustness authentication access control public key cryptography contracts;assurance;authorization	E-businessapplicationsneedrobustandpowerfulmechanismsto authorizesecurity-criticalactions. This actions canbeverycomplex, sincethey canbeinitiatednot onlyby humanusers but alsobyapplicationsor software agents. Existingauthorizationmechanismsdonotscalefor large numberof users if thetrustrelationsaredynamicandfail to providereliableauthorizationamongstrangers. Our mechanismusesauthorizationrelevant attributes to definethe policy. Theattributesareassignedto principalsin a decentralizedmanner . We also presenta methodto reducethe financial losses which mayariseif theauthorizationmechanismfails. We concludethe paper with our plans for future research.	authorization;entity;formal proof;software agent	Zoltán Miklós	2002		10.1109/DEXA.2002.1045938	e-commerce;computer science;artificial intelligence;authorization certificate;authorization;internet privacy;computer security	ML	-45.031373118071215	56.92514451574249	51439
a37043eb1b84273006e8c795a2491a88e0a8918f	moflex transaction model for mobile heterogeneous multidatabase systems	low power capability;multidatabase system;mobile host;mobile computing environment;communication failure possibility;mobile databases;mobile hosts;mobile computer;information access;computer networks;distributed databases transaction processing mobile computing;wireless communication;moflex transaction model;low power;communication failure possibility moflex transaction model mobile heterogeneous multidatabase systems mobile hosts information access mobile transaction model wireless communication mobile computing environment flexibility low power capability low bandwidth;mobile heterogeneous multidatabase systems;mobile transaction model;mobile computing wireless communication database systems computer networks delay bandwidth;database systems;distributed databases;bandwidth;low bandwidth;transaction processing;mobile computing;location dependent operations;flexibility	Users on mobile hosts can access information from heterogeneous multidatabase systems by mobile transactions via wireless communication. Hence, a mobile transaction model should include not only features for heterogeneous multidatabase systems but also those for a mobile computing environment. In this paper, we propose a mobile transaction model to support the requirements of mobile heterogeneous multidatabase systems. The proposed Moflex (Mobile flexible) transaction model is able to support the management of mobility, heterogeniety and flexibility in the definition and execution of mobile transactions. Hence, the Moflex transaction model can be suitable for mobile heterogeneous multidatabase systems that have low power capability, low bandwidth and a high possibility of communication failure.		Kyong-I Ku;Yoo-Sung Kim	2000		10.1109/RIDE.2000.836498	transaction processing;distributed transaction;mobile database;computer science;database;distributed computing;mobile computing;bandwidth;wireless;computer network	DB	-34.733481637584305	48.80513658890265	51496
0796adfbf75d6f62cf23b0967ba67d43439a02ee	a semantic self-management approach for service platforms	service level management semantic services ontologies model driven architecture autonomic computing;grounding;semantics;computational modeling;monitoring;ontologies;quality of service;quality of service semantics ontologies monitoring grounding computational modeling knowledge based systems;knowledge based systems	Future personal living environments feature an increasing number of convenience-, health- and security-related applications provided by distributed services, which do not only support users but require tasks such as installation, configuration and continuous administration. These tasks are becoming tiresome, complex and error-prone. One way to escape this situation is to enable service platforms to configure and manage themselves. The approach presented here extends services with semantic descriptions to enable platform-independent autonomous service level management using model driven architecture and autonomic computing concepts. It has been implemented as a OSGi-based semantic autonomic manager, whose concept, prototypical implementation and evaluation are presented.	autonomic computing;autonomic networking;autonomous robot;cognitive dimensions of notations;itil;model-driven architecture;osgi;self-management (computer science)	Jan Schäfer	2014	2014 IEEE Network Operations and Management Symposium (NOMS)	10.1109/NOMS.2014.6838329	ground;quality of service;computer science;knowledge management;ontology;data mining;database;semantics;computational model	OS	-42.347174824800796	43.98450725332699	51543
51b659231585d52d76e495c09992b68874a685a2	scanning the internet for ros: a view of security in robotics research		Security is particularly important in robotics platforms. A robot can sense the physical world using sensors, or directly change the physical world with its actuators. Thus, a robot can leak sensitive information about its environment if accessed by an unauthorized party, or even cause physical harm if operated unsafely. As robots become more common in our daily lives, these concerns become more important. Security issues in robotic platforms have been studied in existing work, particularly in home and industrial environments. Vulnerabilities have been identified in various robotic platforms for use in home [1, 3] and research [7] environments, including weak or non-existent authentication procedures that allow an attacker to control robots, perform firmware updates, or access sensor data. Broadly, these issues seem to stem from lack of consideration in the platform design. Quarta et al. [8] and Maggi et al. [6] surveyed domain experts from both academia and industry, and found that 30% had robots accessible from the Internet, while 76% had never performed a professional cybersecurity assessment. They also used Internet search engines like Shodan [11] to identify industrial robotic devices exposed to the public Internet, identifying 28 industrial robots and thousands of “industrial routers” that enable remote access to devices. Our goal is to add to this conversation by investigating the state of security in robotics research platforms deployed in practice, which have not been measured by previous works. Specifically, we conducted several Internet-wide scans to identify hosts using the Robot Operating System (ROS) [9], a widely-used platform in robotics research. ROS operates as a publish-subscribe service to distribute data among nodes in a system. Nodes publish or subscribe to topics by advertising or querying a central master node to send or receive data. Like many research platforms, ROS was not designed for security: the ROS master node trusts all nodes that connect to it, and thus should not be exposed to the public Internet. While several emerging approaches can provide authentication and authorization mechanisms to ROS, including SROS [12], Rosbridge [2], and ROS2 [10], none appear to be widely adopted in practice at this time. We searched for ROS masters connected to the IPv4 Internet address space by performing a scan of all public addresses ������ �������� ���������������	address space;authentication;authorization;computer security;firmware;industrial robot;information sensitivity;internet;publish–subscribe pattern;robot operating system;robotics;shodan;sensor;web search engine	Nicholas DeMarinis;Stefanie Tellex;Vasileios Kemerlis;George Konidaris;Rodrigo Fonseca	2010	CoRR		computer security;proof of concept;computer engineering;best practice;image sensor;robot;the internet;computer science;ipv4 address exhaustion;robotics;artificial intelligence	Robotics	-52.14637666668257	60.246731540609275	51562
57513d2145d69febc71c3ed7ae86e627bd9335d4	green move: a platform for highly configurable, heterogeneous electric vehicle sharing	environmental impacts;green move;sustainable transportation;prototypes;urban areas;environmental factors electric vehicles;electric vehicles;green move project heterogeneous electric vehicle sharing configurable electric vehicle sharing;vehicle sharing;vehicle dynamics smart phones batteries electric vehicles bluetooth operating systems green design	Vehicle sharing in urban areas has the potential to be the answer to some of the main issues that hinder the spreading of electric vehicles, in particular for what concerns the high upfront costs of the vehicles, combined with their still limited range, which can induce phenomena such as range anxiety. For its potential to be realized, vehicle sharing must be tailored to the multiform needs of its users by offering a wide range of support services that can be selected based on the user preferences. In this paper we present the platform for vehicle sharing developed in the Green Move project, which allows services to be dynamically loaded and unloaded on vehicles, and describe a pair of prototype applications to illustrate its benefits.	prototype;user (computing)	Andrea G. Bianchessi;Gianpaolo Cugola;Simone Formentin;Angelo Morzenti;Carlo Ongini;Emanuele Panigati;Matteo Rossi;Sergio M. Savaresi;Fabio A. Schreiber;Letizia Tanca;Edoardo G. Vannutelli Depoli	2014	IEEE Intelligent Transportation Systems Magazine	10.1109/MITS.2014.2323421	sustainable transport;embedded system;simulation;engineering;prototype;transport engineering	Robotics	-41.673777729381044	51.98139264086517	51636
e64bbb1f966260361e563be41f4addf832f4b15f	formal design of cache memory protocols in ibm	murphi verifier;protocol verification;cache memory;formal method;cache memory protocol;design technique;model checking;formal design of hardware;hardware design;high performance;formal specification and verification	We describe the formal design techniques currently used in IBM to develop cache protocol controllers for high-end servers. In our approach to formal design, formal specification and verification methods are incorporated into the hardware design process, starting from the earliest stages of a hardware project. We describe collaborations between a formal methods expert and hardware designers on two high performance server projects. Properties of the design are verified using both manual proof techniques and model checking. We discuss the modelling and model checking techniques we have developed and indicate future directions.	cpu cache;formal methods;formal specification;formal verification;model checking;server (computing)	Steven M. German	2003	Formal Methods in System Design	10.1023/A:1022921522163	model checking;computer architecture;parallel computing;formal methods;cpu cache;formal verification;computer science;formal specification;formal equivalence checking;programming language;intelligent verification	EDA	-36.74470594769687	33.04989440140723	51660
56c412df2ba1f078665f1ce351a1a751fb5049d8	an architecture for trusted paas cloud computing for personal data		Cloud computing (CC) has gained much popularity. Large amounts of data, many of them personal, are consumed by CC services. Yet, data security and, derived from that, privacy are topics that are not satisfyingly covered. Especially usage control and data leakage prevention are open problems. We propose the development of a trusted Platform as a Service CC architecture that addresses selected Data security and privacy threats (Data breaches, Insecure interfaces and APIs,Malicious insiders of service providers and Shared technology vulnerabilities). Services that consume personal data and are hosted in the proposed architecture are guaranteed to handle these data according to users' requirements. Our proof of concept shows the feasibility of implementing the presented approach.	access control;application programming interface;cloud computing;component-based software engineering;data breach;data loss prevention software;data security;graph coloring;instrumentation (computer programming);naruto shippuden: clash of ninja revolution 3;personally identifiable information;platform as a service;prototype;requirement;run time (program lifecycle phase);spectral leakage;trust (emotion);trusted computing base;upload;von neumann architecture	Lorena González-Manzano;Gerd Brost;Matthias Aumüller	2014		10.1007/978-3-319-12718-7_15	direct anonymous attestation;cloud computing;distributed computing;internet privacy;world wide web	Security	-52.521761118752124	58.57216786517144	51685
e1d6952c6a828dedc557a54d6eaa6f812efda0de	risk-based auto-delegation for probabilistic availability	availability;risk;access control;auto delegation	Dynamic and evolving systems might require flexible access control mechanisms, in order to make sure that the unavailability of some users does not prevent the system to be functional, in particular for emergency-prone environments, such as healthcare, natural disaster response teams, or military systems. The auto-delegation mechanism, which combines the strengths of delegation systems and “break-theglass” policies, was recently introduced to handle such situations, by stating that the most qualified available user for a resource can access this resource. In this work we extend this mechanism by considering availability as a quantitative measure, such that each user is associated with a probability of availability. The decision to allow or deny an access is based on the utility of each outcome and on a risk strategy. We describe a generic framework allowing a system designer to define these different concepts. We also illustrate our framework with two specific use cases inspired from healthcare systems and resource management systems.	access control;authorization;control system;emergence;fits;systems design;unavailability	Leanid Krautsevich;Fabio Martinelli;Charles Morisset;Artsiom Yautsiukhin	2011		10.1007/978-3-642-28879-1_14	reliability engineering;engineering;knowledge management;computer security	Security	-54.83295343591407	46.65355326935487	51688
2c15ff353fa58b0df91be0babadce635b1061874	cafise: an approach to enabling adaptive configuration of service grid applications	application development;application framework;adaptive application framework;dynamic reconfiguration;grid applications;on demand service configuration;dynamic service reconfiguration;reference model;model adaptation;software systems;application integration;modeling language;large scale;convergent modeling;service grid;open standard	Aiming at building up more powerful, open-standard-based and generic infrastructures for application integration, service grids address the challenges in large-scale coordinated sharing and on-demand composition of network-based application services. The related endeavors have opened up new ways of application development, deployment and integration. In connection with the new level of scale, openness and dynamism brought forward by service grids, adaptive service configuration is of essential importance to applications. This paper proposes an approach called CAFISE, which tries to better facilitate on-demand configuration and dynamic reconfiguration of service grid applications. In CAFISE, a business design and its supporting software system are considered in a coherent way, and a convergent relation, which helps to map business-level configurations to software-level configurations, is highlighted. The paper is particularly devoted to presenting and discussing the principles, reference model, modeling language and supporting application framework of CAFISE. Since practical usefulness is highly valued in the development of CAFISE, the application of the approach to a real-world scenario is also presented in the paper.	application framework;coherence (physics);modeling language;openness;reference model;software deployment;software system	Yanbo Han;Zhuofeng Zhao;Gang Li;Dongshan Xing;Qingzhong Lu;Jianwu Wang;Jinhua Xiong;Hao Liu	2003	Journal of Computer Science and Technology	10.1007/BF02948923	real-time computing;reference model;open standard;computer science;operating system;database;distributed computing;modeling language;rapid application development;software system	HPC	-44.698431834665875	40.60508907653502	51805
f159446fc3be79975a6f9db83e7a95169e509046	a virtualized usage control bus system		Usage control is an extension of access control that additionally defines what must and must not happen to data after access has been granted. The process of enforcing usage control requirements on data must take into account all the different representations that the data may assume at different levels of abstraction (e.g. file, window content, network packet). Therefore, multiple data flow tracking and usage control enforcement monitors are likely to exist, one at each relevant layer. Whenever data flows from a representation at one layer to a representation at another layer (e.g. a file is loaded and interpreted by an application), then the monitor for the initiating layer (in the example, the operating system) must notify the monitor for the receiving layer (in this example, an application, like a browser) about the data being transfered. This is required in order to associate both representations to the same data. In this paper, we present a bus system to support system-wide usage control enforcement that, for security and performance reasons, is implemented in a hypervisor. We provide an example application for enforcing usage control across layers of abstraction in the context of social networks. We evaluate security and performance of our bus system.	abstraction layer;access control;authentication;control bus;d-bus;data general nova;dataflow;discrete logarithm;emoticon;forge;hypervisor;inter-process communication;microsoft windows;network packet;non-functional requirement;open-source software;operating system;overhead (computing);principle of abstraction;requirement;shared memory;social network;software deployment;trusted platform module;user space;virtual machine;web page	Cornelius Moucha;Enrico Lovat;Alexander Pretschner	2011	JoWUA		access control;human–computer interaction;real-time computing;computer science;control bus;system bus;back-side bus;data flow diagram;local bus;network packet;abstraction layer	Mobile	-52.55095600146179	54.27145918142198	51837
bcebac9534876de44268c9ed6be701458c3008c0	an investigation of the impact of double bit-flip error variants on program execution		The objective of this paper is to investigate two existing variants of the double bit-flip error models and their impact on program execution. The two variants are (i) flipping two bits in a given word or register and (ii) flipping one bit in two different words or registers. The goal of the study is to determine whether there is relevance for considera- tion for both variants during program validation. Specifically, we seek to determine if the profile failures induced on software by the variants are different. This then motivates that both are needed for validation. We conduct a large-scale experiment on five different software systems from two target systems. Our results show that each variant induces a differ- ent failure profile in software. Hence, we conclude that both variants are important during validation.		Fatimah Adamu-Fika;Arshad Jhumka	2015		10.1007/978-3-319-27140-8_55	real-time computing;simulation;computer science	SE	-62.210076786494376	36.22435909758906	51849
b4074a1b276afb37d103b934773071ff176a1b9d	green mining: energy consumption of advertisement blocking methods	software energy consumption;advertising	Extending battery life on mobile devices has become an important topic recently due to the increasing frequency of smartphone adoption. A primary component of smart phone energy consumption is the apps that run on these devices. Many apps have embedded advertising and web browser apps will show ads that are embedded on webpages. Other researchers have found that advertising libraries and advertisements tend to increase power usage. But is the converse true? If we use advertisement blocking software will we consume less energy, or will the overhead of ad-blocking consume more energy?   This study seeks to determine the effects of advertisements on energy consumption, and the effects of attempts to block the advertisements. We compared different methods of blocking advertisements on an Android mobile phone platform and compared the power efficiency of these methods. We found many cases where ad-blocking software or methods resulted in increased power use.	android;blocking (computing);embedded system;library (computing);mobile device;mobile phone;online advertising;overhead (computing);performance per watt;smartphone;web application;web banner	Kent Rasmussen;Alex Wilson;Abram Hindle	2014		10.1145/2593743.2593749	engineering;advertising;internet privacy;world wide web	Mobile	-36.64757729593834	55.69515589335902	51878
a354d16324b76ef6aa579688f0954ab5624b3478	reliability analysis based on three-dimensional stochastic differential equation for big data on cloud computing	software fault reliability analysis 3d stochastic differential equation model big data cloud computing open source software software development network traffic software reliability assessment;three dimensional modeling;stochastic differential equation;cloud computing software reliability big data computational modeling mathematical model;big data;stochastic processes big data cloud computing differential equations public domain software software reliability;reliability analysis;cloud computing;stochastic differential equation cloud computing big data reliability analysis three dimensional modeling	The big data and cloud computing are attracting attention as a network service to share the computing resources such as networks, servers, storage, applications, and services. Also, many open source software are used in various software development because of the low cost, quick delivery, and standardization. In order to consider the interesting aspect of the big data and network traffic, a new approach to software reliability assessment based on three dimensional stochastic differential equation model is presented in this paper. Also, this paper analyzes actual data to show numerical examples of software reliability assessment considering such characteristics of big data on cloud computing. Moreover, performance examples of the proposed model for the big data on cloud computing are shown. Then, the performance of our model in terms of the big data, network traffic, and software fault is discussed in this paper.	big data;cloud computing;network packet;network traffic control;numerical analysis;open-source software;reliability engineering;software development;software quality;software reliability testing	Yoshinobu Tamura;Kenta Miyaoka;Shigeru Yamada	2014	2014 IEEE International Conference on Industrial Engineering and Engineering Management	10.1109/IEEM.2014.7058761	programming with big data in r;stochastic differential equation;big data;cloud computing;computer science;data science;cloud testing;data mining;database;utility computing;statistics	DB	-49.143159741505286	41.26501947443329	51905
a147edd005e5a2639c769615dcf1bf7c1ba0bed4	secure e-commerce protection profile	common criteria cc;e-commerce;multicast;security;unicast;protection profile pp;satisfiability;security policy;e commerce	We present a Secure E-commerce Protection Profile (SEPP) that captures security requirements for securing sessions in the e-commerce operational environment. The SEPP is prepared in accordance with the Common Criteria (CC), Version 2.1, as specified by the ISO 15408 standard. The SEPP states the requirements that sessions must satisfy in order to respond to the needs of e-commerce. The Target of Evaluation (TOE) security environment, which is composed of threat agents, vulnerabilities, attacks and threats, is described in detail. It is followed by describing the administrative security policies that are necessary to safeguard the TOE or its operating environment. The risks to the TOE are identified. The security objectives for the TOE are stated.	common criteria;e-commerce;operating environment;protection profile;requirement;software documentation;vulnerability (computing)	Anil Kumar Venkataiahgari;Mourad Debbabi;J. William Atwood	2006			e-commerce;computer security;computer science;security through obscurity;information security;security convergence;common criteria;security service;protection profile;security information and event management	Security	-49.18868492251595	54.7998029139349	51941
80cdce1fafae8f01391de269c43dd08791fbb990	social network profile and policy	social network services;protocols;isolation technology;social attributes social network profile social network policy incompatible connectivity protocols data representation;policy;social network profile;social network services standards development xml protocols ecosystems conferences information technology isolation technology australia industrial relations;information technology;distributed processing;specification;swinburne;incompatible connectivity protocols;data representation;social network;standards development;monitoring;social network policy;ecosystems;specification social network policy;web services;xml;industrial relations;social attributes;communities;conferences;australia;automation	A social network describes an entity's relationship with other entities for interacting. The digital world currently consists of many isolated communities or domains that enable entities to form social networks within each community but not across communities because of the undisclosed, incomplete or incompatible connectivity protocols and data representation. A specification for the social network profile and policy is presented in this paper with the aim to address these issues by providing a common representation for defining the entity's relations and their social attributes, and its connectivity expectations and preferences. Furthermore, this profile and policy specification paves the path for supporting entities in automating their actions and managing their social networks.	data (computing);entity;human-readable medium;interaction;linkage (software);scalability;social network;the australian;web service;xml	Juliana Mitchell-Wong;Ryszard Kowalczyk;Quoc Bao Vo	2008	2008 IEEE Workshop on Policies for Distributed Systems and Networks	10.1109/POLICY.2008.41	computer science;knowledge management;data mining;world wide web	Web+IR	-47.07660814147222	45.217355315032925	51987
7a4329fd991c9a76ecc6d8a8c9b06aa39ee3b110	mqtt in aal systems for home monitoring of people with dementia		The paradigmatic shift brought by the Internet of Things has already revolutionized many key sectors, like environmental monitoring, grid and energy management, manufacturing, and it can be seen as a promising solution to address challenging societal issues, like the ability to provide significant enhancement to quality of life for the elderly and, in general, people in need. As a consequence, Internet of Things emerges also as the possible next evolution for the Ambient Assisted Living domain. One major issue to address in this context is the identification of a suitable middleware able to leverage the potentialities offered by the Internet of Things and, at the same time, ensure the necessary support to services and functions related to healthcare and personal assistance. This paper provides an overview of middleware solutions designed for Internet of Things in health and wellness domains, and presents a case study related to assistive technology for the home monitoring of people with dementia. It is illustrated how a specific middleware designed for telemetry applications, the MQTT, can be effectively applied in assistive scenarios too, with different architectural options and communication technologies.	atm adaptation layer;assistive technology;internet of things;mqtt;middleware;software deployment	Antonio Del Campo;Ennio Gambi;Laura Montanini;Davide Perla;Laura Raffaeli;Susanna Spinsante	2016	2016 IEEE 27th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)	10.1109/PIMRC.2016.7794566	simulation;telecommunications;computer security;computer network	Embedded	-43.97013955619382	48.78624483598102	52034
dadf1deb342646aeb3e52a506905ab9021550d5f	smart crowds in smart cities: real life, city scale deployments of a smartphone based participatory crowd management platform	computer communication networks;information systems and communication service;computer applications;it in business;computer systems organization and communication networks;processor architectures	We describe a platform for smart, city-wide crowd management based on participatory mobile phone sensing and location/situation specific information delivery. The platform supports quick and flexible deployments of end-to-end applications for specific events or spaces that include four key functionalities: (1) Mobile phone based delivery of event/space specific information to the users, (2) participatory sensor data collection (from app users) and flexible analysis, (3) location and situation specific message multicast instructing people in different areas to act differently in case of an emergency and (4) post mortem event analysis. This paper describes the requirements that were derived through a series of test deployments, the system architecture, the implementation and the experiences made during real life, large scale deployments. Thus, until today it has been deployed at 14 events in three European countries (UK, Netherlands, Switzerland) and was used by well over 100,000 people.	end-to-end principle;mobile phone;multicast;real life;requirement;smart city;smartphone;switzerland;systems architecture	Tobias Franke;Paul Lukowicz;Ulf Blanke	2015	Journal of Internet Services and Applications	10.1186/s13174-015-0040-6	simulation;operating system;computer applications;world wide web;computer security;computer network	Mobile	-40.83629616596155	54.431504025408444	52060
24a78c335e79288b36723a4e12061cc3030de4dd	making the internet of things fly			internet of things	Michael Baentsch;et al.	2015	ERCIM News		computer security;internet of things;computer science	ML	-46.00603587188572	49.64139145601156	52130
39b26ce5c5e494fe87c30b897b09bbe4a1dc670c	distributed intelligence in the electric smart grid	smart power grids power engineering computing;cyber physical systems;smart grids;human interface;cyberphysical system;power engineering computing;smart power grids;human machine interface;electric smart grid;human interface distributed intelligence electric smart grid cyberphysical system energy management;artificial intelligence;distributed intelligence;usability;security;smart grids artificial intelligence cyber physical systems conferences energy management security privacy;human machine interface physical security usability;privacy;conferences;energy management;physical	Cyber-physical systems are physical infrastructures with deeply embedded computation, As these systems move beyond traditional control and response into more sophisticated planning and interaction, particularly with human elements, the need for intelligent cyber-physical systems emerges. This talk uses advanced electric smart grid technology as an example of distributed intelligence in a cyber-physical system from the point of view of energy management, human interfaces, and security and privacy.	computation;cyber-physical system;embedded system	Bruce M. McMillin	2015	2015 IEEE 39th Annual Computer Software and Applications Conference	10.1109/COMPSAC.2015.338	human–machine interface;embedded system;usability;computer science;engineering;information security;smart grid;cyber-physical system;privacy;computer security;internet of things;human interface device;computer engineering;energy management	Embedded	-45.42260384891191	50.828506322827096	52182
41e15c5b538306c029e0288a35259bf70587154b	a publish/subscribe system using distributed broker for soa based manet applications	manet;publish subscribe system;soa;ns2;service discovery	Due to advances of network enabled wireless devices, there is a growing trend of the use of networked and distributed mobile devices. As a result, deployment of day to day applications in MANET for information services is also rapidly evolving in ad hoc mobile environments, with the growing number of mobile users. The application running on different devices can be accessed as a service by different users on an on demand basis. These scenario demands deployment of Service Oriented Architecture SOA in MANET for efficient access and usage of applications. Since nodes in MANET does not have any prior knowledge about other nodes, in many cases, the publish/subscribe communication paradigm that has the decoupling and asynchrony properties can be useful to share services between nodes. Unfortunately, none of the currently available content-based publish-subscribe system fit the requirements of such dynamic environments. In this paper several distributed publish/subscribe mechanisms for improving the reliability of the service discovery process is analyzed. Nodes in MANET are constrained by limited processing power, battery power, memory etc, so there lays a need to find appropriate nodes for service brokering. To select a broker for publishing services, parameters like available battery power, CPU power, memory capacity, nodes mobility etc. needs to be considered. In this paper a broker selection model based on the above mentioned parameters is proposed. Unlike other mobility prediction algorithms in terms of speed which require devices to be equipped with GPS to predict the mobility, mobility in the proposed algorithm is detected using frequent change of neighborhood.		Prasenjit Choudhury;Subrata Nandi;Narayan C. Debnath	2012	J. Comput. Meth. in Science and Engineering	10.3233/JCM-2012-0443	real-time computing;mobile ad hoc network;computer science;engineering;operating system;service-oriented architecture;service discovery;world wide web;computer network	Logic	-39.09645775563071	47.581913611656155	52217
6183ae4f7e153b291c87f6062ae752c62aad6bfb	aiding evolution with concern-oriented guides	aspect oriented software development;separation of concern;user guidance;software evolution;separation of concerns;documentation;concern modeling	Program documentation is often incomplete and out of date due to its tediousness and perceived low value. This requires evolution tasks to be preceded by time-consuming exploration. In this paper, we explore a concern-oriented approach to documentation that focuses on the code artifacts and their relationships to make the process of creating and using program documentation more efficient. As opposed to traditional documents or tutorials, guides created using this approach are interactive, almost wordless and automatically maintain implementation examples. We also present the rationale and the architecture of Mismar, a toolset tightly integrated in the Eclipse environment and implementing this approach. Moreover, since program documentation involves different artifact types, Mismar was build from the ground up to be extensible, and to support artifacts written in multiple languages or modeling approaches.	design rationale;documentation generator;eclipse	Barthélémy Dagenais;Harold Ossher	2007		10.1145/1275672.1275676	simulation;computer science;systems engineering;technical documentation;internal documentation;world wide web	SE	-53.624459014551476	35.02686537866559	52260
b573a97fa7ccdc9c190c68f197a6cf3c12b9e882	architecture-driven smart grid security management	risks;security management;smart grid;security	The introduction of smart grids goes along with an extensive use of ICT technologies in order to support the integration of renewable energy sources. However, the use of ICT technologies bears risks in terms of cyber security attacks which could negatively affect the electrical power grid. These risks need to be assessed, mitigated and managed in a proper way to ensure the security of both current and future energy networks. Existing approaches have been either restricted to very specific components of the smart grid (e.g., smart meters), or provide a high-level view only. We therefore propose an architecture-driven security management approach for smart grids which goes beyond a mere abstract view without focusing too much on technical details. Our approach covers architecture modeling, risk identification and assessment as well as risk mitigation and compliance checking. We have proven the practical usability of this process together with leading manufacturers and utilities.	computer security;high- and low-level;security management;smart tv;smart card;smart meter;usability	Markus Kammerstetter;Lucie Langer;Florian Skopik;Wolfgang Kastner	2014		10.1145/2600918.2600937	security management;security information and event management;computer science;information security;risk;smart grid;computer security	Security	-56.15604910905621	49.35866082609255	52371
ca76a1053a94070e53cd55b4c723709f43fa4247	modelling and mitigation of cross-origin request attacks on federated identity management using cross origin request policy		Cross origin request attacks (CORA) such as Cross site request forgery (CSRF), cross site timing, etc. continue to pose a threat on the modern day web. Current browser security policies inadequately mitigate these attacks. Additionally, third party authentication services are now the preferred way to carry out identity management between multiple enterprises and web applications. This scenario, called Federated Identity Management (FIM) separates the problem of identity management from the core functionality of an application. In this paper, we construct formally checkable models and design laboratory simulations to show that FIM is susceptible to cross origin attacks. Further, we employ the Cross Origin Request Policy (CORP) to mitigate such attacks.	android;authentication;browser security;chromium (web browser);clickjacking;cross-site cooking;cross-site request forgery;denial-of-service attack;experiment;federated identity;firefox;identity management;login;open-source software;opera (web browser);simulation;web application;web standards;ios	Akash Agrawall;Shubh Maheshwari;Projit Bandyopadhyay;Venkatesh Choppella	2017		10.1007/978-3-319-72598-7_16	federated identity;computer security;computer science;browser security;web application;authentication;identity management;cross-site request forgery	Security	-54.20979224059665	59.76220603344246	52435
66c3e65beacd0d573c2645504661dca0850547fb	an adaptive n-variant software architecture for multi-core platforms: models and performance analysis	performance analysis;probabilistic model checking;general reliability model;adaptive n-variant software architecture;behavioral modeling;formal requirement model;multi-core platform;probabilistic automaton-based model;adaptive software architecture;multiple level;reconfiguration mechanism;parallel computing power	This paper discusses the models and performance analysis for an adaptive software architecture, which supports multiple levels of fault detection, masking, and recovery through reconfiguration. The architecture starts with a formal requirement model defining multiple levels of functional capability and information assurance. The architecture includes a multi-layer design to implement the requirements using Nvariant techniques. It also integrates a reconfiguration mechanism that uses lower layers to monitor higher layers, and if a fault is detected, it reconfigures a system to maintain essential services. We first provide a general reliability model (based on generalized stochastic Petri nets) for such a system with cross-monitoring for reconfiguration. Next, we define a probabilistic automaton-based model for behavioral modeling of the system. This model is especially suitable for modeling security problems induced by value faults. Whereas the Petri net allows for reliability modeling and reconfiguration, the performance analysis of the system is given via probabilistic model checking. The models are experimentally evaluated and compared. With the current widespread deployment of multi-core processors, one question in software engineering is how to effectively harness the parallel computing power provided by these processors. The architecture presented here allows us to explore the parallel computing power that otherwise may be wasted, and uses it to improve the dependability and survivability of a system, which is validated by our performance analysis.	behavioral modeling;central processing unit;coefficient;computation;dependability;experiment;fault detection and isolation;information assurance;layer (electronics);model checking;multi-core processor;parallel computing;petri net;probabilistic automaton;profiling (computer programming);reliability engineering;requirement;simulation;software architecture;software deployment;software engineering;statistical model;systems design;theory	Li Tan;Axel W. Krings	2011		10.1007/978-3-642-21887-3_38	reference architecture;embedded system;real-time computing;computer science;operating system;database;computer security;computer network	SE	-43.64513402775248	36.113029200312184	52472
3cfd78793d5cf8a5f6e1b19465eddca35620f713	scalable program comprehension for analyzing complex defects	computers;software;complex defects scalable program comprehension;program diagnostics;complexity theory;query processing;business rule analysis;program comprehension;complex defects;safety critical control software;query processing program debugging program diagnostics;humans software tools information retrieval application software software safety information analysis artificial intelligence aerospace electronics space technology operating systems;scalable;scalable program comprehension;atlas scalable program comprehension query model refine approach problem solving strategy interactive automatic parallelization business rule analysis safety critical control software;data structures;problem solving strategy;atlas;driver circuits;artificial intelligence;program debugging;interactive automatic parallelization;query model refine approach;business rules;automatic parallelization;problem solving;operating systems	We describe the query-model-refine (QMR) approach for retrieving and modeling information for program comprehension. The QMR approach allows the flexibility to design and execute application-specific problem-solving strategies to suit particular program comprehension goals. The QMR approach has been used for building a number of program comprehension tools for different applications: interactive automatic parallelization, business rule analysis, auditing safety-critical control software, and defect analysis. This presentation will be about a program comprehension tool called Atlas and we will show its use for analyzing complex defects.	content-control software;list comprehension;parallel computing;problem solving;program comprehension;software bug	Suraj C. Kothari	2008	2008 16th IEEE International Conference on Program Comprehension	10.1109/ICPC.2008.44	scalability;data structure;computer science;theoretical computer science;operating system;atlas;software engineering;database;programming language;business rule;automatic parallelization	SE	-54.72824441644346	35.466970707165814	52492
36b2acaaa1a1b545f8bc10eda306d3fa77de5826	a methodology for the generation of efficient error detection mechanisms	analytical models;detectors;fault injection analysis;specification based detector design;measurement;data mining techniques;specification based detector design error detection mechanisms dependable software system error recovery mechanisms software components fault injection analysis data mining techniques;error recovery mechanisms;software reliability data mining error detection software fault tolerance;software systems;software fault tolerance;data mining;software components;detectors software systems data mining data models measurement analytical models;qa76 electronic computers computer science computer software;software dependability;error detection;decision tree induction software dependability fault injection data mining error detection mechanisms;decision tree induction;fault injection;software reliability;error detection mechanisms;dependable software system;data models;qa76 computer software	A dependable software system must contain error detection mechanisms and error recovery mechanisms. Software components for the detection of errors are typically designed based on a system specification or the experience of software engineers, with their efficiency typically being measured using fault injection and metrics such as coverage and latency. In this paper, we introduce a methodology for the design of highly efficient error detection mechanisms. The proposed methodology combines fault injection analysis and data mining techniques in order to generate predicates for efficient error detection mechanisms. The results presented demonstrate the viability of the methodology as an approach for the development of efficient error detection mechanisms, as the predicates generated yield a true positive rate of almost 100% and a false positive rate very close to 0% for the detection of failure-inducing states. The main advantage of the proposed methodology over current state-of-the-art approaches is that efficient detectors are obtained by design, rather than by using specification-based detector design or the experience of software engineers.	component-based software engineering;data mining;dependability;error detection and correction;fault injection;sensitivity and specificity;sensor;software engineer;software system	Matthew Leeke;Saima Arif;Arshad Jhumka;Sarabjot S. Anand	2011	2011 IEEE/IFIP 41st International Conference on Dependable Systems & Networks (DSN)	10.1109/DSN.2011.5958204	reliability engineering;data modeling;detector;real-time computing;error detection and correction;computer science;theoretical computer science;component-based software engineering;programming language;software quality;software fault tolerance;measurement;software system	SE	-62.0908924676848	37.73962553333914	52497
0436a991bd16b02d414dd4fde1a03ac79acdf4ae	compose -- a journey from the internet of things to the internet of services	heterogeniety;compose project;composition;semantics;composition internet of things internet of services semantics heterogeniety scalability security;runtime;internet of things;data visualisation;service industry;service industries;internet;engines;monitoring;europe internet of things internet of services compose project marketplace infrastructure virtual world service industry;internet of services;marketplace infrastructure;scalability;service industries data visualisation internet internet of things security of data;europe;security;security of data;virtual world;security semantics runtime monitoring data models engines internet of things;data models	The COMPOSE project aims to unleash the full potential harbored by the Internet of Things by creating a complete ecosystem around it to enable the flourishing of a resulting Internet of Services, seamlessly integrating the real and virtual worlds. COMPOSE will achieve this through the provisioning of an open and scalable marketplace infrastructure, in which smart objects are associated to services that can be combined, managed, and integrated in a standardized way to easily build innovative applications. The resulting platform is expected to significantly strengthen the service industry in Europe.	ecosystem;internet of things;internet protocol suite;provisioning;scalability;smart objects;virtual world	Benny Mandler;Fabio Antonelli;Robert Kleinfeld;Carlos Pedrinaci;David Carrera;Alessio Gugliotta;Daniel Schreckling;Iacopo Carreras;Dave Raggett;Marc Pous;Carmen Vicente Villares;Vlad Trifa	2013	2013 27th International Conference on Advanced Information Networking and Applications Workshops	10.1109/WAINA.2013.116	tertiary sector of the economy;computer science;database;semantics;internet privacy;world wide web;computer security;computer network	HPC	-46.73201132547369	45.10141524077772	52540
d7a50c41baadeb6a41dfa26e419db1de35feba84	mobile phone computing and the internet of things: a survey	sensors;sensors mobile handsets internet of things mobile communication mobile computing mobile applications monitoring;internet of things;mobile applications;monitoring;mobile communication;mobile handsets;survey mobile computing mobile phonewe applications internet of things web of things sensors;mobile computing	As the Internet of Things (IoT) and the Web of Things (WoT) are becoming a reality, their interconnection with mobile phone computing is increasing. Mobile phone integrated sensors offer advanced services, which when combined with Web-enabled real-world devices located near the mobile user (e.g., body area networks, radio-frequency identification tags, energy monitors, environmental sensors, etc.), have the potential of enhancing the overall user knowledge, perception and experience, encouraging more informed choices and better decisions. This paper serves as a survey of the most significant work performed in the area of mobile phone computing combined with the IoT/WoT. A selection of over 100 papers is presented, which constitute the most significant work in the field up to date, categorizing these papers into ten different domains, according to the area of application (i.e., health, sports, gaming, transportation, and agriculture), the nature of interaction (i.e., participatory sensing, eco-feedback, actuation, and control), or the communicating actors involved (i.e., things and people). Open issues and research challenges are identified, analyzed and discussed.	big data;categorization;critical graph;interaction;interconnection;internet of things;mobile app;mobile computing;mobile phone;on the fly;openness;participatory sensing;personalization;radio frequency;real life;seamless3d;sensor;world wide web	Andreas Kamilaris;Andreas Pitsillides	2016	IEEE Internet of Things Journal	10.1109/JIOT.2016.2600569	mobile broadband;radio access network;mobile identification number;mobile search;mobile web;mobile telephony;imt advanced;human–computer interaction;public land mobile network;gsm services;mobile database;computer science;sensor;operating system;mobile technology;mobile business development;internet privacy;mobile station;mobile computing;world wide web;mobile communications over ip;computer security;internet of things;mobile payment	HCI	-41.59922265799902	51.1881904526654	52542
9f2cf8e43ca869084109bd092cdd0f39286cd605	formal reliability analysis of combinational circuits using theorem proving	computer program;theorem proving;theorem prover;formal verification;probability theory;reliability analysis;combinational circuit;fault model;hol;logic gate;higher order logic	Reliability analysis of combinational circuits has become imperative these days due to the extensive usage of nanotechnologies in their fabrication. Traditionally, reliability analysis of combinational circuits is done using simulation or paper-and-pencil proof methods. But, these techniques do not ensure accurate results and thus may lead to disastrous consequences when dealing with safety-critical applications. In this paper, we mainly tackle the accuracy problem of these traditional reliability analysis approaches by presenting a formal reliability analysis framework based on higher-order-logic theorem proving. We present the higher-order-logic formalization of the notions of fault and reliability for combinational circuits and formally verify the von-Neumann fault models for most of the commonly used logic gates, such as, AND, NOT, OR, etc. This formal infrastructure is then used along with a computer program, written in C++, to automatically reason about the reliability of any combinational circuit within a higher-order-logic theorem prover (HOL). For illustration purposes, we utilize the proposed framework to analyze the reliability of a few benchmark combinational circuits.	adder (electronics);automated theorem proving;c++;combinational logic;correctness (computer science);formal verification;hol (proof assistant);interactivity;logic gate;reliability engineering;scalability;undecidable problem	Osman Hasan;Jigar Patel;Sofiène Tahar	2011	J. Applied Logic	10.1016/j.jal.2011.01.002	discrete mathematics;computer science;theoretical computer science;mathematics;automated theorem proving;programming language;algorithm	Logic	-36.47052442746443	33.69717358284004	52544
320cc0a1d3af84e7617444829197a9ac88ec0265	new architectures, protocols, and middleware for ad hoc collaborative computing	internet architecture;middleware;collaborative computing	The existing Internet architecture and protocols are not suitable for the newly emerging ad hoc collaborative computing systems. New architectures, protocols, and middleware specifically designed for the new ad hoc collaborative computing systems are needed. While work is currently being done to adapt the Internet architecture and protocols so the new systems can be developed and deployed, the fundamental characteristics that make the Internet unsuitable for the new systems — address-based messaging, one-to-one communication, and central servers — still remain. We envision alternative architectures, protocols, and middleware with the opposite characteristics — no central servers, broadcast communication, and content-based messaging. Many-to-Many Invocation (M2MI) and the Many-to-Many Protocol (M2MP) are an initial realization of this vision. “No one pours new wine into old wineskins. If he does, the new wine will burst the skins, the wine will run out and the wineskins will be ruined. No, new wine must be poured into new wineskins. And no one after drinking old wine wants the new, for he says, ‘The old is better.’ ” — Jesus of Nazareth (Luke 5:37–39)	hoc (programming language);internet;many-to-many;middleware;one-to-one (data model);skin (computing)	Alan Kaminsky;Hans-Peter Bischof	2003			middleware;middleware;database;distributed computing;computer network	Arch	-33.8184291133918	47.881267980338	52575
61a1f4c1b1ff910d0faf84649e3259c8c931a862	an integrated state- and event-based framework for verifying liveness in supervised systems	high tech oce printer state and event based system engineering supervised system discrete event supervisory controller supremica supervisor synthesis mcrl2 verification method;formal verification;discrete event systems;formal verification control engineering computing discrete event systems;control engineering computing;maintenance engineering printing modeling radiation detectors supervisory control process control system recovery	Supervisory control theory deals with synthesis of discrete-event supervisory controllers that ensure safe (and nonblocking) behavior of the supervised system. However, the synthesized supervisor comes with no guarantees regarding desired functionality beyond nonblocking behavior. This typically occurs when the control requirements imposed on the system are too strict, or the model of the system needs to be refined. To provide concise and useful feedback to the modeler, we propose an integrated state- and event-based systems engineering framework using state-of-the-art tools: Supremica for supervisor synthesis and mCRL2 for verification. Stating properties in terms of both states and transitions is important in the domain of supervisor synthesis as many control and liveness requirements involve combined state- and event-based specifications. However, many of the available verification tools either focus on state-based or event-based properties. We seek to remedy this situation by providing verification patterns that typically occur in industrial application of supervisory control. We illustrate the framework by revisiting an industrial case study of coordinating maintenance procedures of a high-tech Oce printer, for which we verify the functionality of the solution.	control theory;liveness;mcrl2;printer (computing);requirement;systems engineering;verification and validation	Jasen Markovski;Michel A. Reniers	2012	2012 12th International Conference on Control Automation Robotics & Vision (ICARCV)	10.1109/ICARCV.2012.6485166	control engineering;real-time computing;simulation;formal verification;computer science;engineering	Robotics	-39.09582474832228	34.48986998966966	52733
1fe22197dc8d48ff6dfc5206e5598d8ccd80bf99	an improved methodology on information distillation by mining program source code	industrial case study;software maintenance;program comprehension;data mining;knowledge acquisition methods;association rule mining;object oriented;knowledge acquisition;source code;data code mining;k means clustering;software maintenance issues	This paper presents a methodology for knowledge acquisition from source code. We use data mining to support semiautomated software maintenance and comprehension and provide practical insights into systems specifics, assuming one has limited prior familiarity with these systems. We propose a methodology and an associated model for extracting information from object oriented code by applying clustering and association rules mining. K-means clustering produces system overviews and deductions, which support further employment of an improved version of MMS Apriori that identifies hidden relationships between classes, methods and member data. The methodology is evaluated on an industrial case study, results are discussed and conclusions are drawn. 2006 Elsevier B.V. All rights reserved.	apriori algorithm;association rule learning;cluster analysis;code refactoring;data mining;entity;k-means clustering;knowledge acquisition;maintenance engineering;software maintenance;software system	Yiannis Kanellopoulos;Christos Makris;Christos Tjortjis	2007	Data Knowl. Eng.	10.1016/j.datak.2006.06.002	kpi-driven code analysis;association rule learning;computer science;data science;data mining;database;data stream mining;programming language;object-oriented programming;software maintenance;static program analysis;k-means clustering;source code	SE	-56.76830625537305	34.117137730847936	52764
8720aa0db366b4280683234b002f1608c775b3ee	a comparison of two programming models for pervasive computing	service oriented model programming model comparison pervasive computing interoperability context driven model;service orientation;pervasive computing;pervasive system;open systems ubiquitous computing distributed programming;pervasive computing context modeling programming profession actuators atomic measurements productivity context aware services educational programs logic programming laboratories;programming model;distributed programming;ubiquitous computing;open systems	Establishing suitable programming models for pervasive spaces is essential in improving the productivity, enhancing the quality of pervasive systems, and creating an open platform for interoperability. Two different models, namely, the context-driven model and the service-oriented model, have been proposed and studied for their feasibilities as the foundation for implementing programmable pervasive spaces. We present these two alternatives and contrast their advantages and disadvantages.	interoperability;open platform;pervasive informatics;programming model;service-oriented architecture;service-oriented device architecture;service-oriented infrastructure;service-oriented modeling;software testing controversies;ubiquitous computing	Hen-I Yang;Erwin Jansen;Abdelsalam Helal	2006	International Symposium on Applications and the Internet Workshops (SAINTW'06)	10.1109/SAINT-W.2006.1	context-aware pervasive systems;reactive programming;functional reactive programming;computer science;theoretical computer science;end-user computing;database;distributed computing;programming paradigm;inductive programming;open system;ubiquitous computing	Arch	-39.18616058274348	42.50786759839925	52788
b261dd9aafab9ce470652c0dfca20192313d86a4	report of hotmobile 2010	sensing;context awareness;wireless networks;hotmobile;context aware;wireless network;mobile computer;mobile applications;connectivity;urban computing;mobile computing;security;mobile application;privacy	HotMobile is SIGMOBILE's highly selective, interactive workshop focused on mobile applications, systems, and environments, as well as their underlying state-of-the-art technologies. Themes from this year's workshop were mobile and urban computing, security, privacy, and sensing.		Bo Han;Ahmad Rahmati;Bhojan Anand	2010	IEEE Pervasive Computing	10.1109/MPRV.2010.64	mobile search;mobile web;computer science;operating system;wireless network;mobile technology;internet privacy;mobile computing;computer security;computer network	Vision	-40.13485493547097	50.84299749911475	52866
40c07628aaff9fc4b1e882803bc22cd8ba224efd	towards a taxonomy for simulink model mutations	simulink;detectors;model mutations;simulation languages program testing;model clones;software packages cloning mathematical model detectors taxonomy layout testing;testing;layout;cloning;model clones simulink model mutations taxonomy mutation;taxonomy;mathematical model;mutant injection simulink model mutations mutation analysis modeling language model clone detector testing framework realistic simulink edit operations mutation operator characteristics simulink projects industrial project;mutation;software packages	A relatively new and important branch of Mutation Analysis involves model mutations. In our attempts to realize model-clone detector testing, we found that there was little mutation research on Simulink, which is a fairly prevalent modeling language, especially in embedded domains. Because Simulink model mutations are the crux of our model-clone detector testing framework, we want to ensure that we are selecting the appropriate mutations. In this paper, we propose a taxonomy of Simulink model mutations, which is based on our experiences thus far with Simulink, that aims to inject model clones of various types and is fairly representative of realistic Simulink edit operations. We organize the mutations by categories based on the types of model clones they will inject, and further break them down into mutation classes. For each class, we define the characteristics of mutation operators belonging to that class and demonstrate an example operator. Lastly, in an attempt to validate our taxonomy, we perform a case study on multiple versions of three Simulink projects, including an industrial project, to ascertain if the actual subsystem edit operations observed across versions can be classified using our taxonomy and present any interesting cases. While we developed the taxonomy with the specific goal of facilitating and guiding the injection of mutants for model clones, we believe it is fairly general and a solid foundation for future Simulink model mutation work.	data flow diagram;dataflow;documentation;duplicate code;embedded system;evolutionary taxonomy;matlab;modeling language;mutation testing;simulink;taxonomy (general);tensor operator	Matthew Stephan;Manar H. Alalfi;James R. Cordy	2014	2014 IEEE Seventh International Conference on Software Testing, Verification and Validation Workshops	10.1109/ICSTW.2014.17	mutation;layout;detector;simulation;computer science;bioinformatics;mathematical model;cloning;software testing;algorithm;taxonomy	SE	-56.341796499678	34.28080766031562	52871
a0ce5a18f38287e892857268bdb059d67707955c	fix me up: smart pathways status detection		Pedestrian pathways are a crucial networking components of our cities, and yet they are somehow seen as subordinate to streets in many areas that are not central. As a result, detection of their actual status, and corresponding maintenance, is often far from optimal. In this work we look for solutions to the problem of pathways status detection, and look for novel solutions that allow us to assess the status of a pedestrian pathway. We develop a system that enables to collect data from the pathways in a very economical and flexible way, and then experiment to see whether the data allow to actually produce a reliable status prediction. The results, tested on the field, show that the obtained system is both convenient to use and precise, paving the way for better ways of handling pedestrian pathways networking within a smarter city environment.	approximation algorithm;gene regulatory network;low-power broadcasting;peripheral;real-time clock;scalability;smart card	Massimo Marchiori	2018	2018 International Symposium on Networks, Computers and Communications (ISNCC)	10.1109/ISNCC.2018.8531009	risk analysis (engineering);maintenance engineering;pedestrian;city environment;computer science	Arch	-42.15073864119393	51.95978141857345	52931
bb5c076b3090d2e22b53f0cbdc0b62020fc17354	finite model generation and formal specification development	formal specification;model generation;formal specifications object oriented modeling programming profession logic testing laboratories computer science software systems runtime computer bugs program processors;object oriented programming;formal method;theorem prover;object oriented programming finite model generation formal specification development software tools formal methods automated theorem provers model generators software components completeness consistency;object oriented programming formal specification software tools;software component;software tools	Ef ic ien t supporting tools are very important to the adoption of formal methods. Conventionally such tools are mainly automated theorem provers. I n this paper, we show that model generators can also be very helpful in obtaining formal specifications of software components. These tools help u s to know more about certain properties of specifications, such as completeness and consistency.	automated theorem proving;component-based software engineering;formal methods;formal specification	Jian Zhang	1997		10.1109/TOOLS.1997.713562	b-method;verification and validation;formal methods;object language;formal verification;computer science;design by contract;theoretical computer science;component-based software engineering;software development;software construction;formal specification;refinement;programming language;algorithm	SE	-50.17125814581342	33.11307078373624	53011
5895fa34605757bb40a5a07bcd8a997290ad100e	a testbed and process for analyzing attack vectors and vulnerabilities in hybrid mobile apps connected to restful web services	hybrid mobile application;web development test bed platform attack vector analysis vulnerability analysis hybrid mobile app restful web service mobile device;attack vectors hybrid mobile application vulnerabilities web services web browser thin native containers;thin native containers;smart phones;web services mobile computing program testing security of data software engineering;browsers;attack vectors;mobile applications;vulnerabilities;web browser;web services;mobile communication;security;accelerometers;cameras;mobile communication security browsers accelerometers cameras mobile applications smart phones	Web traffic is increasingly trending towards mobile devices driving developers to tailor web content to small screens and customize web apps using mobile-only capabilities such as geo-location, accelerometers, offline storage, and camera features. Hybrid apps provide a cross-platform, device independent, means for developers to utilize these features. They work by wrapping web-based code, i.e., HTML5, CSS, and JavaScript, in thin native containers that expose device features. This design pattern encourages re-use of existing code, reduces development time, and leverages existing web development talent that doesn't depend on platform specific languages. Despite these advantages, the newness of hybrid apps raises new security challenges associated with integrating code designed for a web browser with features native to a mobile device. This paper explores these security concerns and defines three forms of attack that can specifically target and exploit hybrid apps connected to web services. Contributions of the paper include a high level process for discovering hybrid app attacks and vulnerabilities, definitions of emerging hybrid attack vectors, and a test bed platform for analyzing vulnerabilities. As an evaluation, hybrid attacks are analyzed in the test bed showing that it provides insight into vulnerabilities and helps assess risk.	cascading style sheets;computer data storage;device independence;geolocation;html5;high-level programming language;javascript;mobile device;representational state transfer;software design pattern;testbed;vulnerability (computing);web application;web content;web development;web service;web traffic;wrapping (graphics)	Matthew L. Hale;Seth Hanson	2015	2015 IEEE World Congress on Services	10.1109/SERVICES.2015.35	web service;web application security;web development;web modeling;engineering;internet privacy;world wide web;computer security	Security	-38.04437193834993	52.68129036839685	53035
f00b2146d7170a2695ba78199ab2b2ef30437a71	predicting higher order structural feature interactions in variable systems		Robust and effective support for the detection and management of software features and their interactions is crucial for many development tasks but has proven to be an elusive goal despite extensive research on the subject. This is especially challenging for variable systems where multiple variants of a system and their features must be collectively considered. Here an important issue is the typically large number of feature interactions that can occur in variable systems. We propose a method that computes, from a set of known source code level interactions of n features, the relevant interactions involving n+1 features. Our method is based on the insight that, if a set of features interact, it is much more likely that these features also interact with additional features, as opposed to completely different features interacting. This key insight enables us to drastically prune the space of potential feature interactions to those that will have a true impact at source code level. This substantial space reduction can be leveraged by analysis techniques that are based on feature interactions (e.g Combinatorial Interaction Testing). Our observation is based on eight variable systems, implemented in Java and C, totaling over nine million LoC, with over seven thousand feature interactions.	cit program tumor identity cards;control flow analysis;data-flow analysis;dataflow;heart rate variability;interaction;java;programming language;real life;robustness (computer science);software system;test effort	Stefan Fischer;Lukas Linsbauer;Alexander Egyed;Roberto Erick Lopez-Herrejon	2018	2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)	10.1109/ICSME.2018.00035	software system;systems engineering;software;feature extraction;computer science;nine million;theoretical computer science;source code;java	SE	-56.948985884986314	38.47552152366539	53042
677efba6aafb199acb7ce49a08645853e5de89fc	enhancing security in the memory management unit	institutional repositories;ciphered program execution;processor architecture;virtual memory;memory management unit;electronic commerce;fedora;memory management;hardware security support;block cipher;storage management;security management;access control memory management unit security problems multitasking virtual memory support hardware security support security management unit ciphered program execution;virtual memory support;security memory management hardware computer architecture licenses electronic commerce privacy multitasking access control logic devices;vital;computer architecture;security problems;security management unit;licenses;multitasking;access control;vtls;smu;security;security of data;privacy;ils;logic devices;computer architecture storage management security of data;hardware	We propose an hardware solution to several security problems that are difficult to solve on classical processor architectures, like licensing, electronic commerce, or software privacy. The memory management unit which provides multitasking and v irtual memory support is extended and given a third purpose: to supply strong hardware security support for the software layer. The principle of this enhanced device, that we call a Security Management Unit (or SMU), is based on ciphered program execution and access control. It is composed of a pipelined block ciphering/deciphering unit, an internal permanent memory and logic control, whose interaction is explained in this paper.	access control;cipher;computer multitasking;e-commerce;logic control;memory management unit;non-volatile memory;pipeline (computing);security management;system management unit	Tanguy Gilmont;Jean-Didier Legat;Jean-Jacques Quisquater	1999		10.1109/EURMIC.1999.794507	software security assurance;embedded system;execution unit;computer science;virtual memory;operating system;extended memory;computer security	Arch	-53.07207235035602	55.758985975947986	53049
f792e8a6d6baf845a303e0486b1ba5e8f61568bc	5g visions of user privacy	5g network security;user privacy trusted third party ecosystem digital interaction 5g network security;digital interaction;5g networks;computer crime;privacy visualization;privacy bargain;user privacy;5g mobile communication;internet;data privacy;ecosystems;telecommunication security 5g mobile communication data privacy;trusted third party;mobile communication;telecommunication security;facebook;privacy mobile communication internet computer crime ecosystems facebook;privacy;ecosystem	Currently the discussions are going on the elements and definition of 5G networks. One of the elements in this discussion is how to provide for user controlled privacy for securing users' digital interaction. The purpose of this paper is to present elements of user controlled privacy needed for the future 5G networks. The paper concludes that an ecosystem consisting of a Trusted Third Party between the end user and the service providers as a distributed system could be integrated to secure the perspective of user controlled privacy for future services.	distributed computing;ecosystem;information privacy;randomness;requirement;trusted third party	Lene Tolstrup Sørensen;Samant Khajuria;Knud Erik Skouby	2015	2015 IEEE 81st Vehicular Technology Conference (VTC Spring)	10.1109/VTCSpring.2015.7145587	privacy software;privacy policy;ecosystem;user modeling;information privacy;privacy by design;computer science;internet privacy;world wide web;computer security	Networks	-46.63458689914022	60.36586665754313	53061
6c81fc13a338fb084e84e19fff55aef28a0d0fbe	a proactive event-driven approach for dynamic qos compliance in cloud of things		Cloud-of-things service providers use various descriptions languages to describe Quality of Service (QoS) attributes. However, existing modelling approaches provide support for modelling static QoS attributes only and lack features to model and reason with dynamic QoS attributes such as response time and availability. This paper presents an event-based approach for monitoring dynamic QoS values and their compliance by modelling the behavior of QoS attributes using an Event Calculus (EC) based framework. The logic based reasoning is then performed to proactively identify the possible QoS violations in future.	event calculus;event-driven programming;quality of service;response time (technology);service-level agreement	Falak Nawaz;Omar Khadeer Hussain;Naeem Khalid Janjua;Elizabeth Chang	2017		10.1145/3106426.3109431	quality of service;service provider;mobile qos;real-time computing;telecommunications service;event calculus;cloud computing;service-level agreement;business;reliability engineering;logic programming	SE	-46.68451375703796	42.71844559194918	53103
df60e620b6ae35b5b4896c75e3fac2d7ea0a533c	a clustering-based approach for tracing object-oriented design to requirement	requirement traceability;object oriented design;information retrieval;resource manager;vector space;object oriented software;software requirements;clustering;class;object oriented software development;text clustering;use case	Capturing the traceability relationship between software requirement and design allows developers to check whether the design meets the requirement and to analyze the impact of requirement changes on the design. This paper presents an approach for identifying the classes in object-oriented software design that realizes a given use case, which leverages ideas and technologies from Information Retrieval (IR) and Text Clustering area. First, we represent the use case and all classes as vectors in a vector space constructed with the keywords coming from them. Then, the classes are clustered based on their semantic relevance and the cluster most related to the use case is identified. Finally, we supplement the raw cluster by analyzing structural relationships among classes. We conduct an experiment by using this clustering-based approach to a system - Resource Management Software. We calculate and compare the precision and recall of our approach and nonclustering approaches, and get promising results.		Xin Zhou;Huiye Qiu	2007		10.1007/978-3-540-71289-3_31	use case;document clustering;vector space;computer science;systems engineering;resource management;object-oriented design;software engineering;software construction;data mining;database;class;cluster analysis;programming language;software requirements;requirements traceability	PL	-57.41213416158583	33.516376128676086	53115
970d3521c4f419b04033191f30ca79380924a96a	automated learning of probabilistic assumptions for compositional reasoning	probabilistic system;compositional verification;automatic generation;communication protocol;compositional reasoning	Probabilistic verification techniques have been applied to the formal modelling and analysis of a wide range of systems, from communication protocols such as Bluetooth, to nanoscale computing devices, to biological cellular processes. In order to tackle the inherent challenge of scalability, compositional approaches to verification are sorely needed. An example is assume-guarantee reasoning, where each component of a system is analysed independently, using assumptions about the other components that it interacts with. We discuss recent developments in the area of automated compositional verification techniques for probabilistic systems. In particular, we describe techniques to automatically generate probabilistic assumptions that can be used as the basis for compositional reasoning. We do so using algorithmic learning techniques, which have already proved to be successful for the generation of assumptions for compositional verification of non-probabilistic systems. We also present some recent improvements and extensions to this work and survey some of the promising potential directions for further research in this area.	algorithm;algorithmic learning theory;automata theory;bluetooth;formal verification;machine learning;omega-regular language;probabilistic turing machine;probabilistic automaton;scalability;verification and validation	Lu Feng;Marta Z. Kwiatkowska;David Parker	2011		10.1007/978-3-642-19811-3_2	communications protocol;computer science;theoretical computer science;data mining;algorithm	Logic	-36.32272838902916	33.010919129663115	53192
7df145e3b6fe21dcc734825fd670cc09fc27aeb2	secure design of engineering software tools in industrial automation and control systems	engineering tool;embedded device security;performance evaluation;critical infrastructures;authentication;mitigation techniques;trust boundaries;software tools control engineering computing critical infrastructures embedded systems industrial control security of data;embedded systems;engineering framework security;security risk identification;industrial automation and control systems;industrial control;engineering software tool security design;software tools;control engineering computing;authorization;critical infrastructure;authentication authorization object oriented modeling software tools performance evaluation;security of data;object oriented modeling;secure engineering frameworks;iacs;industrial automation and control systems critical infrastructure embedded device security engineering framework security trust boundaries engineering tool iacs security risk identification mitigation techniques secure engineering frameworks engineering software tool security design	Industrial Automation and Control Systems (IACS) used in critical infrastructure typically perform their tasks using embedded devices. While the security of the embedded devices during the operation of the system is naturally the focus of security considerations, the security of the engineering framework is often overlooked. In this paper, we model the trust boundaries of a typical engineering tool used in an IACS, identify security risks in this context, suggest mitigation techniques for end users, and finally propose an architecture that allows to implement secure engineering frameworks.	automation;best, worst and average case;control system;critical infrastructure protection;embedded system;encryption;entity;internet;mind;multi-user;operating system;programming tool;research data archiving;secure communication;user interface;vulnerability (computing)	Ana Hristova;Sebastian Obermeier;Roman Schlegel	2013	2013 11th IEEE International Conference on Industrial Informatics (INDIN)	10.1109/INDIN.2013.6622968	control system security;security engineering;systems engineering;engineering;software engineering;computer security	EDA	-55.053622012997124	49.29582642432354	53211
f7ebb1b1f9a9262c14247738905f53c7fb1c05d7	autonomic healing for service specific overlay networks		Service Specific Overlay Networks (SSONs) have recently attracted a great interest, and have been extensively investigated in the context of multimedia delivery over the internet. SSONs are virtual networks constructed on top of the underlying network and they have been proposed to provide and improve services not provided by other traditional networks to the end users. The increased complexity and heterogeneity of these networks in addition to ever changing conditions in the network and the different types of fault that may occur make their control and management by human administrators more difficult. Therefore, self-healing concept was introduced to handle these changes and assuring highly reliable and dependable network system performance. Self-healing aims at ensuring that the service will continue to work regardless of defects that might occur in the network. This paper introduces literature in the area of self-healing overlay networks, presents their basic concepts, requirements, and architectures. In addition to that the authors present a proposed self-healing architecture for multimedia delivery services. Their proposed solution is oriented to discover new approaches for monitoring, diagnosing, and recovering of services thus achieving self-healing.	autonomic computing;overlay network	Ibrahim Al-Oqily;Bassam Subaih;Saad Bani-Mohammad;Jawdat Jamil Alshaer;Mohammed Refai	2012	IJITWE	10.4018/jitwe.2012040104	multi-frequency network;overlay network;data mining;database;world wide web;computer security;computer network	Networks	-43.779387696777384	41.99623716118073	53251
787e90da2b7374289a0f11f6d6c0896d21ccfa89	efficient, secure, and isolated execution of cryptographic algorithms on a cryptographic unit	public key cryptography;cryptographic algorithms;instruction set extensions;aes;computer architecture;instruction set extension;cryptographic algorithm;execution environment;tk7885 7895 computer engineering computer hardware;security;embedded processor	Cryptographic algorithms handle sensitive information and their safe execution plays an essential role in many security applications. When implemented in software on general-purpose computers, cryptographic algorithms are vulnerable to a variety of attacks such as side-channel and cold-boot attacks since they either share hardware resources with other simultaneously executing processes or store sensitive information in easily accessible places (e.g. main memory). In this paper, we demonstrate that secure and isolated execution of cryptographic algorithms is possible on a cryptographic unit that can easily be integrated to all RISC processors. The cryptographic unit is capable of physically isolating the execution of cryptographic algorithms from all other simultaneously executing processes. By specifically providing an AES implementation running in this isolated execution environment we demonstrate that it is possible to provide physical process isolation for cryptographic algorithms without any significant overhead in execution time. Furthermore, the proposed technique protects the cryptographic applications against cold-boot and cache attacks as well as any other threats originated from other processes since the sensitive material never leave the cryptographic unit. We realized a RISC-based embedded processor with five-stage pipeline featuring the cryptographic unit on an FPGA device. We included the implementation results both for FPGA and ASIC realizations.	algorithm;application-specific integrated circuit;central processing unit;cold boot attack;computer data storage;crypto++;cryptography;embedded system;field-programmable gate array;general-purpose markup language;information sensitivity;overhead (computing);process isolation;reboot (computing);run time (program lifecycle phase);side-channel attack	Kazim Yumbul;Erkay Savas	2009		10.1145/1626195.1626233	cryptographic primitive;advanced encryption standard;security of cryptographic hash functions;parallel computing;real-time computing;key exchange;computer science;information security;random number generator attack;static key;key management;hash-based message authentication code;cryptographic protocol;distributed computing;cryptographic key types;public-key cryptography;controlled cryptographic item;computer security;key encapsulation	Security	-53.27288654819032	56.02147584738782	53278
a74f6939e90f93ca9ff7b9c9c68fd37a5eb9cf96	improving data integrity with a java mutability analysis	object oriented programming languages data integrity java mutability analysis security threats software components secdetector security analysis tool potential modification access violations;security analysis;data integrity;java information analysis data security information security object oriented programming software engineering information science computer security accidents packaging;object oriented programming languages;software component;experimental evaluation;false positive;security of data;security of data data integrity java;java	This paper presents a static mutability analysis approach relying on escape information for Java components and uses the techniques to detect the security threats to data integrity before software components are deployed. In order to increase the precision of our analysis, we make a couple of significant modifications to mutability definitions based on previous work in the context of components. We extended our security analysis tool SecDetector with proposed mutability analysis, and used it to find potential threats to data integrity in Java components and lead developers to fix the security flaws. On the benchmarks in our experimental evaluation, we show that our tool can correctly find potential modification access violations with few false positives and provide evidence of the effectiveness of our techniques. While the analysis techniques are in the context of Java code, the basic concepts are applicable to other object-oriented programming languages as well.	benchmark (computing);component-based software engineering;data integrity;ibm common user access;immutable object;java;programming language	Aiwu Shi;Gleb Naumovich	2007	14th Asia-Pacific Software Engineering Conference (APSEC'07)	10.1109/APSEC.2007.62	software security assurance;computer security model;security through obscurity;security information and event management;java concurrency;type i and type ii errors;computer science;component-based software engineering;software engineering;strictfp;data integrity;data mining;database;real time java;security analysis;programming language;object-oriented programming;security testing;java;principal;computer security;java annotation	SE	-57.4724834719689	56.68962633122227	53285
4f329b1ac5daa84854668096f134e86e7c981a93	an evolutionary approach for optimisation of state-based test suites for software systems	satisfiability;test coverage;test data generation;software systems;software testing		mathematical optimization;software system	Huaizhong Li;Chiou Peng Lam	2003			test management approach;non-regression testing;keyword-driven testing;machine learning;software construction;artificial intelligence;software system;system integration testing;classification tree method;software reliability testing;computer science	SE	-59.01639410865342	32.8674638061837	53307
282829b67aba4da7e7f87c59945212668d4c7a3a	a stepwise approach towards an interoperable and flexible logging principle for audit trails	software;distributed system;log files;standards;interoperable logging principle;xml technology flexible logging principle interoperable logging principle audit trails computer system grid computing cloud computing distributed systems security breaches;system monitoring;audit trails;computer system;security breaches;automatic evaluation;logging;internet;cloud computing grid computing xml data security power system security authorization event detection information technology interactive systems computer security;xml logging audit trail grid computing cloud computing;taxonomy;xml technology;xml;ip networks;flexible logging principle;correlation;distributed systems;security;grid computing;security of data;audit trail;xml grid computing internet security of data system monitoring;cloud computing	Although event recording on a computer system (also known as logging) is of utmost importance for reconstructing and detecting security relevant events, currently no adequate and sophisticated solution for complex environments, such as Grid and Cloud Computing, exist. Current LOG file formats lack of several important factors, hindering automatic evaluation needed for distributed systems to comply with laws and regulations or hindering detection of security breaches. In this paper we present a new concept utilizing XML technology to solve mentioned problems of current LOG file formats and describe the benefits of this new idea resulting in a flexible, interoperable LOG environment enabling automatic evaluation even in locally dispersed computing systems. As a result reliable, robust and consistent LOG files are produced allowing for automatic evaluation.	cloud computing;comparison of tls implementations;computer;data logger;distributed computing;interoperability;sensor;stepwise regression;xml	David Huemer;A Min Tjoa	2010	2010 Seventh International Conference on Information Technology: New Generations	10.1109/ITNG.2010.33	computer science;operating system;audit trail;database;world wide web;computer security;taxonomy;computer network	HPC	-46.23955963703074	56.36245897072439	53319
74c5df18362856bebe87c9190b3e0bf6e4bd3b73	a survey of platforms for mobile networks research	mobile network	The use of smartphones is growing at an unprecedented rate and is projected to soon pass laptops as consumers' mobile platform of choice. The proliferation of these devices has created new opportunities for mobile researchers; however, when faced with hundreds of devices across nearly a dozen development platforms, selecting the ideal platform is often met with unanswered questions. In this paper I consider desirable characteristics of mobile platforms necessary for mobile networks research. Based on these characteristics, I assess five smartphone platforms: Android (Linux), BlackBerry, iPhone (Mac OS X), Symbian, and Windows Mobile. This survey is current as of December 2008. A living version of this survey is available at: http://blizzard.cs.uwaterloo.ca/eaoliver/platforms/.	android;blackberry;laptop;linux;microsoft windows;mobile device;smartphone;symbian;windows mobile	Earl Oliver	2008	Mobile Computing and Communications Review	10.1145/1508285.1508292	embedded system;cellular network;mobile search;simulation;mobile web;telecommunications;computer science;operating system;mobile technology;mobile computing;world wide web	Mobile	-38.03538247797094	54.282991268625466	53321
ca0a6da80d5c0b6f48b4b4f9f5e1e6a9e3c2cf66	leveraging user-privilege classification to customize usage-based statistical models of web applications	user sessions;general relativity;analytical models;user privilege classification;web application testing;electronic mail;navigation data models testing browsers analytical models educational institutions electronic mail;testing;statistical analysis internet program testing;browsers;statistical model;data model;navigation;test case generation;internet;statistical analysis;program testing;web application;usage based statistical models web application testing;specification tests;usage based statistical model;in conference proceedings;user sessions user privilege classification usage based statistical model web application abstract test cases user privilege specific navigation model user privilege specific test cases;user privilege specific navigation model;abstract test cases;analytical model;data models;user privilege specific test cases;web application testing usage based stastical models;usage based stastical models	Automatically creating test cases from statistical models of web application usage is an effective approach to generating test cases that represent actual usage. The models are typically generated from all collected user sessions. In this paper, we consider how grouping the user sessions -- specifically by the user's privilege -- creates different statistical models and the testing implications of those differences. We performed a study of user-privilege-specific navigation models and the resulting abstract test cases generated from over 19,000 user sessions to four deployed web applications. Our results suggest that grouping user sessions by the users' privileges results in smaller navigation models, which yield realistic test cases that represent users with that privilege well while also exploring navigations not seen in the input user sessions. In some cases, the user-privilege-specific models are significantly smaller, which allows the tester to either (a) generate relatively few test cases and still represent the user type well or (b) create test cases from a less abstract model -- without exorbitant model space costs or the need for additional models to generate executable test cases. However, the benefits are not universal for all applications, thus, we present guidance to testers on metrics to determine whether creating user-privilege-specific test cases will be advantageous.	emoticon;executable;privilege (computing);statistical model;test case;test suite;web application	Sara Sprenkle;Camille Cobb;Lori L. Pollock	2012	2012 IEEE Fifth International Conference on Software Testing, Verification and Validation	10.1109/ICST.2012.96	statistical model;data modeling;navigation;web application;the internet;data model;computer science;operating system;general relativity;data mining;database;software testing;world wide web;statistics;web testing	SE	-58.669063250173465	42.14280495253673	53332
116a3cd5d8260890681643826980f23e34a8d5b0	bio-inspired cyber security for the smart grid	intelligent sensor;biomedical monitoring;power meters;home appliances;complex adaptive system;smart grid;cyber security;computer security;smart grids;agents;digital pheromones smart grid bio inspired cyber security unidirectional power flow bi directional flows smart meters smart appliances long term cyber security attributes social insects digital ants framework mobile light weight agents;smart power grids;monitoring;load flow;bio inspired materials;smart power grids bio inspired materials load flow power meters power system security security of data;computer security smart grids home appliances intelligent sensors monitoring biomedical monitoring;bio inspired;power flow;social insect;smart grid agents bio inspired computer security cyber security;security of data;intelligent sensors;power system security	Smart grid technologies are dramatically transforming the grid's traditional unidirectional power flow into a grid with bi-directional flows of both power and information. The addition of many millions of smart meters and smart appliances will add significant complexity to today's grid. Additionally, these smart meters and smart appliances are relatively new classes of devices and therefore as yet unproven in the field, especially with respect to their long-term cyber security attributes. Both of these issues, scaleability and relatively unknown device behaviors, will impair the effectiveness of traditional cyber security techniques. Therefore, a new bio-inspired approach to cyber security is proposed. The study of social insects, such as ants and bees, has shown that complex-adaptive systems can emerge from the collective application of simple, light-weight behaviors. The Digital Ants framework is a bio-inspired framework that uses mobile light-weight agents to monitor the smart grid. Sensors within the framework communicate with each other via digital pheromones thereby alerting each other to possible cyber security issues. Communication and coordination is all localized and decentralized thereby allowing the framework to scale across the large numbers of devices. The light-weight nature of the sensors also allows the framework to be implemented on devices with limited computational resources. Digital Ants research will enable a new bio-inspired cyber security approach for securing the smart grid.	british informatics olympiad;complex adaptive system;complex systems;computation;computational resource;computer security;eusociality;internationalization and localization;sensor;smart meter	A. David McKinnon	2012	2012 IEEE PES Innovative Smart Grid Technologies (ISGT)	10.1109/ISGT.2012.6175799	embedded system;simulation;engineering;computer security	EDA	-45.27972146354737	51.004958089953846	53340
7f83a16019027bd04b64ec18f3d8bd923a71ef0e	heaptherapy: an efficient end-to-end solution against heap buffer overflows	context encoding resource management monitoring hardware computer bugs security;resource management;safety critical software benchmark testing buffer storage operating systems computers resource allocation;monitoring;spec cpu2006 benchmarks heaptherapy end to end solution heap buffer overflows software vulnerabilities exploit detection defense generation overflow prevention program execution on the fly lightweight trace collection over read attacks heartbleed attack polymorphic exploits operating systems allocation algorithms;security;encoding;computer bugs;context;hardware	For decades buffer overflows have been one of the most prevalent and dangerous software vulnerabilities. Although many techniques have been proposed to address the problem, they mostly introduce a very high overhead while others assume the availability of a separate system to pinpoint attacks or provide detailed traces for defense generation, which is very slow in itself and requires considerable extra resources. We propose an efficient solution against heap buffer overflows that integrates exploit detection, defense generation, and overflow prevention in a single system, named Heap Therapy. During program execution it conducts on-the-fly lightweight trace collection and exploit detection, and initiates automated diagnosis upon detection to generate defenses in real-time. It can handle both over-write and over-read attacks, such as the recent Heartbleed attack. The system has no false positives, and keeps effective under polymorphic exploits.%as the generated defense captures semantic characteristics of exploits. It is compliant with mainstream hardware and operating systems, and does not rely on specific allocation algorithms. We evaluated Heap Therapy on a variety of services (database, web, and ftp) and benchmarks (SPEC CPU2006), it incurs a very low average overhead in terms of both speed (6.2%) and memory (7.7%).	algorithm;buffer overflow;end-to-end principle;enterprise integration;exploit (computer security);heap overflow;heartbleed;memory management;operating system;overhead (computing);real-time clock;software bug;tracing (software);uninitialized variable;vulnerability (computing)	Qiang Zeng;Mingyi Zhao;Peng Liu	2015	2015 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks	10.1109/DSN.2015.54	real-time computing;software bug;computer science;information security;resource management;operating system;heap overflow;distributed computing;computer security;encoding;computer network	Arch	-56.739010451765445	56.42982147448878	53362
1025cae718c83d7049712d1e8db032493fd08b7d	empirical analysis of usage and acceptance of software distribution methods on mobile devices	mobile device;empirical analysis;acceptance;software engineering mobile computing mobile radio;software engineering;acceptance empirical analysis software distribution mobile end device mobile applications usage;mobile applications;mobile radio;mobile communication software mobile computing mobile handsets internet data communication wireless lan;mobile computing;mobile application;mobile internet empirical analysis software distribution method mobile end device smart phone;mobile end device;usage;software distribution	Mobile end devices, such as smart phones, are widely distributed and their technical capacities are increasing. End devices have surpassed the original idea of enabling users to talk to each other. Users are now able to install application software on their devices. This is of special interest to companies, as it portrays a new source of income for stakeholders of the value chain in the mobile internet. However, it is unclear how such applications should be technically transferred to the mobile devices. Different technologies are available for this task. This paper examines the previous use and acceptance of software distribution methods. User preferences for technical transmission ways were furthermore determined with surveys. It was not possible to identify a dominant mode of transmission. Acceptance of software distribution methods especially depends on technical affinity and the user's gender. Thus, operating systems and producers of end devices should enable usage of multiple ways of transferring data.	affinity analysis;bluetooth;discrepancy function;memory card;mobile app;mobile device;modulation;near field communication;personal computer;processor affinity;smartphone;software distribution;technical standard	Saskia Geisler;Martin Zelazny;Stefan Christmann;Svenja Hagenhoff	2011	2011 10th International Conference on Mobile Business	10.1109/ICMB.2011.10	mobile broadband;radio access network;embedded system;mobile identification number;mobile search;mobile web;imt advanced;human–computer interaction;public land mobile network;gsm services;mobile processor;mobile database;computer science;mobile technology;mobile agent;mobile business development;mobile station;mobile collaboration;mobile computing;world wide web;mobile communications over ip;mobile payment	HCI	-38.4797008854071	53.35367721281098	53393
037aaacd97b11288d6620f63d9bbb06cbab66add	building event-based services for awareness in p2p groupware systems	groupware;event condition action rules;peer to peer computing collaborative work collaborative software resource description framework indexing availability;collaboration;super peer network p2p groupware systems collaborative work environments web based applications p2p collaborative systems event based awareness services structured p2p network model p2p middleware infrastructure cloud platform;awareness;middleware;cloud computing collaboration p2p systems awareness services event condition action rules;services;peer to peer computing cloud computing groupware middleware;peer to peer computing;p2p systems;cloud computing	Monitoring and awareness are essential in collaborative work environments, in order to support team members' interaction and coordination processes. There has been much work in implementing awareness in web-based applications, but less so for P2P collaborative systems. P2P systems have the potential to foster more group interaction than server-based approaches, as group members can interact directly with each other and can share their knowledge, skills and resources in order to provide mutual support in the accomplishment of group tasks. Several challenges arise due to the large-scale, dynamic and heterogenous nature of P2P collaborative systems. In this paper we discuss the provision of event-based awareness services in P2P groupware systems. We first review the major requirements for such awareness functionality, and propose a structured P2P network model for meeting these requirements. We then identify a set of low level awareness services and show how these can interoperate over the P2P network in order to provide awareness as part of the P2P middleware infrastructure. Finally, we envision the use of service composition to provide more complex awareness services and we discuss the implementation of a super peer network on a Cloud platform and the provision of reliable awareness services from the Cloud.	cloud computing;collaborative software;interoperability;middleware;network model;peer-to-peer;performance evaluation;principle of abstraction;provisioning;requirement;scalability;server (computing);service composability principle;web application	Alexandra Poulovassilis;Fatos Xhafa	2013	2013 Eighth International Conference on P2P, Parallel, Grid, Cloud and Internet Computing	10.1109/3PGCIC.2013.36	computer science;knowledge management;distributed computing;services computing;world wide web	DB	-44.02239342841304	43.29072910874195	53437
5e3556bcd54492105f7889cd77b0371a4a081cee	simpa-ws: an agent-oriented computing technology for ws-based soa applications.	design and development;computer model;software systems;web service;object oriented;middleware;service oriented architecture	This document briefly describessimpA-WS, a Javabased agent-oriented computing technology to flexibly and effectively implement WS-I compliant SOA/WS applications—i.e. Web-Service applications with a Service-Oriented Architecture— both on the user side and the service side.	service-oriented architecture;service-oriented device architecture;ws-federation;web service	Alessandro Ricci;Claudio Buda;Nicola Zaghini;Antonio Natali;Mirko Viroli;Andrea Omicini	2006			middleware;software engineering;middleware;database;distributed computing;oasis soa reference model	HPC	-38.73662447726619	42.1794565682745	53440
258299c9acaaf0fbe5ae3a5befd6597c17078677	enhancing security testing via automated replication of it-asset topologies	virtualization;io security testing automated it asset topology replication it infrastructure production environment business processes small and medium enterprises physical testbed cost intensive task infrastructure replication process irp virtual environment open standards scap oval xccdf interconnected asset ontology;small to medium enterprises;security testing topology network topology virtualization production;small to medium enterprises business data processing ontologies artificial intelligence security of data;ontologies artificial intelligence;testbed;security automation;business data processing;ontologies;security testing;security automation security testing virtualization testbed ontologies;security of data	Security testing of IT-infrastructure in a production environment can have a negative impact on business processes supported by IT-assets. A test bed can be used to provide an alternate testing environment in order to mitigate this impact. Unfortunately, for small and medium enterprises, maintaining a physical test bed and its consistency with the production environment is a cost-intensive task. In this paper, we present the Infrastructure Replication Process (IRP) and a corresponding Topology Editor, to provide a cost-efficient method that makes security testing in small and medium enterprises more feasible. We utilize a virtual environment as a test bed and provide a structured approach that takes into account the differences between a physical and a virtual environment. Open standards, such as SCAP, OVAL or XCCDF, and the utilization the Interconnected-asset Ontology-IO-support the integration of the IRP into existing (automated) processes. We use the implementation of a prototype to present a proof-of-concept that shows how typical challenges regarding security testing can be successfully mitigated via the IRP.	business process;cost efficiency;deployment environment;disk image;documentation;extensible configuration checklist description format;i/o request packet;network topology;open vulnerability and assessment language;operating system;oracle database;prototype;security content automation protocol;security testing;semiconductor industry;software deployment;test plan;testbed;virtual reality	Henk Birkholz;Ingo Sieverdingbeck;Nicolai Kuntze;Carsten Rudolph	2013	2013 International Conference on Availability, Reliability and Security	10.1109/ARES.2013.46	computer security model;cloud computing security;real-time computing;engineering;security service;distributed computing;computer security	SE	-48.366926976634346	44.55882203759138	53452
48c075e3734c35f3b12752d6ed6c3655632afce8	drama: exploiting dram addressing for cross-cpu attacks		In cloud computing environments, multiple tenants are often co-located on the same multi-processor system. Thus, preventing information leakage between tenants is crucial. While the hypervisor enforces software isolation, shared hardware, such as the CPU cache or memory bus, can leak sensitive information. For security reasons, shared memory between tenants is typically disabled. Furthermore, tenants often do not share a physical CPU. In this setting, cache attacks do not work and only a slow cross-CPU covert channel over the memory bus is known. In contrast, we demonstrate a high-speed covert channel as well as the first side-channel attack working across processors and without any shared memory. To build these attacks, we use the undocumented DRAM address mappings. We present two methods to reverse engineer the mapping of memory addresses to DRAM channels, ranks, and banks. One uses physical probing of the memory bus, the other runs entirely in software and is fully automated. Using this mapping, we introduce DRAMA attacks, a novel class of attacks that exploit the DRAM row buffer that is shared, even in multi-processor systems. Thus, our attacks work in the most restrictive environments. First, we build a covert channel with a capacity of up to 2 Mbps, which is three to four orders of magnitude faster than memory-bus-based channels. Second, we build a side-channel template attack that can automatically locate and monitor memory accesses. Third, we show how using the DRAM mappings improves existing attacks and in particular enables practical Rowhammer attacks on DDR4.	cpu cache;central processing unit;cloud computing;computer data storage;covert channel;data rate units;desktop computer;dynamic random-access memory;hypervisor;information leakage;information sensitivity;memory bus;microarchitecture;mobile device;multiprocessing;operating system;reverse engineering;row hammer;server (computing);shared memory;side-channel attack;spectral leakage;undocumented feature	Peter Pessl;Daniel Gruss;Clémentine Maurice;Michael Schwarz;Stefan Mangard	2016			distributed shared memory;parallel computing;real-time computing;computer security	Security	-53.72602392525604	56.15675148766291	53472
c206c9b3e1b07be55f6f0070f112a7b176a9b42b	system security, platform security and usability	trusted computing;normal human user;security and user experience;systems span;software installation;user experience study;system security;platform security;usability;human user;ordinary user;software platform;immutable design constraint;user study;additional layer;large-scale system;spectrum;user experience;secure communication	Scalable trusted computing seeks to apply and extend the fundamental technologies of trusted computing to large-scale systems. To provide the functionality demanded by users, bootstrapping a trusted platform is but the first of many steps in a complex, evolving mesh of components. The bigger picture involves building up many additional layers to allow computing and communication across large-scale systems, while delivering a system retaining some hint of the original trust goal. Not to be lost in the shuffle is the most important element: the system’s human users. Unlike 40 years ago, they cannot all be assumed to be computer experts, under the employ of government agencies which provide rigorous and regular training, always on tightly controlled hardware and software platforms. It seems obvious that the design of scalable trusted computing systems necessarily must involve, as an immutable design constraint, realistic expectations of the actions and capabilities of normal human users. Experience shows otherwise. The security community does not have a strong track record of learning from user studies, nor of acknowledging that it is generally impossible to predict the actions of ordinary users other than by observing (e.g., through user experience studies) the actions such users actually take in the precise target conditions. We assert that because the design of scalable trusted computing systems spans the full spectrum from hardware to software to human users, experts in all these areas are essential to the end-goal of scalable trusted computing.	bootstrapping (compilers);design closure;high availability;immutable object;scalability;trusted computing;usability testing;user experience	Paul C. van Oorschot	2010			direct anonymous attestation;computer science;trusted computing base;distributed computing;world wide web;computer security;trusted client	OS	-50.42485124948288	56.632779146073325	53493
3b443009d9d926cc57a3dedcaf54c75f6fd167de	reserving immutable services through web service implementation versioning	web service;conference proceeding	Widespread adoption of a Web services-based paradigm for software applications will imply that applications will typically have potentially many dependencies upon Web services that they invoke or consume. These invoked services might typically be available from a remote site and be under the administration of third parties. This scenario implies a significant vulnerability of a Web services-based application: one or more of the services which it consumes may become altered, hence potentially “breaking” the application. Such alterations might be such as those that alter the WSDL signature of the service or could be changes to the underlying service implementation that do not change the WSDL signature. In this paper, we will focus on the second of these two cases and will introduce a versioning system that can detect changes to service implementations and that can avoid the breaking of applications that call services in the face of changes to the implementations of those called services.	immutable object;programming paradigm;version control;web services description language;web service	Robert Steele;Takahiro Tsubono	2005			web service;computer science;ws-policy;web 2.0;law	Networks	-51.77947004868881	59.11615851773711	53517
cf090d52b49ae187c901bddaed60f834f9a331cb	instant matchmaking: simple and secure integrated ubiquitous computing environments	distributed system;controle acces;systeme reparti;informatique mobile;pervasive computing;securite informatique;resource management;informatica difusa;computer security;gestion recursos;sistema repartido;informatique diffuse;seguridad informatica;user experience;gestion ressources;local computation;ubiquitous computing;access control;mobile computing;ubiquitous computing environment	Effective ubiquitous computing applications need to integrate users’ personal devices and data with the devices and resources they encounter around them. Previous work addressed this problem by simply enabling the user to take all of their data with them wherever they go. In this paper, we present a more flexible approach: the “instant matchmaker”, a personal device that allows a user to seamlessly and securely connect his local computing environment with his other personal resources, wherever they are. The matchmaker provides an intuitive user experience, while simultaneously enabling extremely fine-grained control over access to resources. We have implemented a cellphone-based matchmaker and explored its use in a secure media sharing application. The matchmaker concept, however, is general, and can be used to enable a range of appealing and secure ubicomp applications.	instant messaging;mobile phone;ubiquitous computing;user experience	Diana K. Smetters;Dirk Balfanz;Glenn Durfee;Trevor F. Smith;Kyung-Hee Lee	2006		10.1007/11853565_28	embedded system;human–computer interaction;computer science;access control;operating system;database;world wide web;computer security;ubiquitous computing	HCI	-37.89635697834181	49.199492916035254	53659
20a00ff025ae98608f5b6e565c3752d6a681afb6	a variation aware composition model for dynamic web service environments		Contemporary approaches for automated web service composition mostly deal with static web services. The underlying assumption here is that the web services participating in resolving a query are static and thereby, their functional and non-functional parameters change very infrequently or do not change at all. However, in reality, this assumption does not hold. New services are added to the repository, existing unpopular services are removed from the repository, service interfaces change due to changes in the specification. Classical service composition approaches therefore fall short to handle the dynamic behavior of a web service during composition. In this paper, we present a stochastic model of the web service composition problem to capture the dynamic behavior of web services from the functional perspective. We present experimental results on the ICEBE-2005 benchmarks to show the effectiveness of our proposed methods.	web service	Soumi Chattopadhyay;Ansuman Banerjee	2018		10.1007/978-3-030-03596-9_50	data mining;stochastic modelling;web service;dynamic web page;computer science;composition (visual arts)	Web+IR	-40.69431407268939	37.329135079401105	53664
ffc4e86276a5489ff07013d15ccdc2866c1305aa	it's duck (typing) season!		Duck typing provides a way to reuse code and allow a developer to write more extensible code. At the same time, it scatters the implementation of a functionality over multiple classes and causes difficulties in program comprehension. The extent to which duck typing is used in real programs is not very well understood. We report on a preliminary study of the prevalence of duck typing in more than a thousand dynamically-typed open source software systems developed in Smalltalk. Although a small portion of the call sites in these systems is duck-typed, in half of the analysed systems at least 20% of methods are duck-typed.	antivirus software;duck, duck, goose;list comprehension;open-source software;program comprehension;smalltalk;software system;type system;typing	Nevena Milojkovic;Mohammad Ghafari;Oscar Nierstrasz	2017	2017 IEEE/ACM 25th International Conference on Program Comprehension (ICPC)		programming language;software system;smalltalk;pharo;reuse;extensibility;program comprehension;computer science;duck typing	SE	-55.87469619053874	37.44618660644966	53763
6a27daf6f66688e3ce104db76e5a35ca5442b380	formal verification of aadl models using uppaal		Cyber-Physical Systems (CPS) are known to be highly complex systems which can be applied to a variety of different environments, covering both civil and military application domains. As CPS are typically complex systems, its design process requires strong guarantees that the specified functional and non-functional properties are satisfied on the designed application. Model-Driven Engineering (MDE) and high-level specification languages are a valuable asset to help the design and evaluation of such complex systems. However, when looking at the existing MDE tool-support, it is observed that there is still little support for the automated integration of formal verification techniques in these tools. Given that formal verification is necessary to ensure the levels of reliability required by safety critical CPS, this paper presents an approach that aims to integrate the Model Checking technique in the CPS design process for the purpose of correctly analyzing temporal and safety characteristics. A tool named ECPS Verifier was designed to support the model checking integration into the design process, providing the generation of timed automata models from high-levels specifications in AADL. The proposed method is illustrated by means of the design of an Unmanned Aerial Vehicle, from where we derive the timed automata models to be analyzed in the UPPAAL tool.	aerial photography;architecture analysis & design language;automata theory;complex systems;consistency model;cyber-physical system;formal verification;high- and low-level;high-level programming language;model checking;model transformation;model-driven engineering;model-driven integration;thread (computing);timed automaton;uppaal;unmanned aerial vehicle	Fernando Silvano Goncalves;David Pereira;Eduardo Tovar;Leandro Buss Becker	2017	2017 VII Brazilian Symposium on Computing Systems Engineering (SBESC)	10.1109/SBESC.2017.22	complex system;systems engineering;model checking;engineering design process;formal verification;design process;computer science	Embedded	-43.531448113051454	32.43167517814686	53764
1e9c4ac799631166454cab994030078524da3614	superdistribution: testability, security and management of digital applications	application development;testing public key software computer architecture servers;software;application framework;digital signature superdistribution x509;storage management;distributed storage;testing;superdistribution;computer architecture;servers;public key;digital media;digital content;digital signature;cryptography;security requirements;storage management cloud computing cryptography mobile computing;mobile computing;x509 certificate superdistribution digital application security digital application management digital application testability mobility distributive storage application distribution update cyber infrastructure application content framework cyber architecture;cloud computing	Application frameworks have transitioned from isolated computing units to mobile, distributed and connected cyber infrastructure. To meet the needs of mobility and distributive storage and execution application frameworks need to support application distribution and updated mechanisms that can be managed effectively and securely. An application distribution update and execution schemes for consumers such that each and every consumer gets the similar application experience is termed as superdistribution. Superdistribution classically was accomplished by using digital media which required special hardware and software for consumption. Digital content in today's cyber infrastructure have transitioned from basic consume only content to interactive updatable content. Hence controlling superdistribution to guarantee security requirements of application infrastructure is critical for the success of the application content framework. Also important is the ability of the framework to provide a testing framework for application development. With increased mobility and perpetual connectivity the problems of distribution and management digital content in a secure way for different cyber architectures are significant. In this paper we show how the constructs of X509 certificate can be used to manage superdistribution frameworks. We also show the usage of X509 certificate constructs to allow local distribution for application development and its limitations.	digital media;digital recording;requirement;software testability;superdistribution;x.509	Vijay Anand;Jafar Saniie	2012	2012 IEEE International Conference on Electro/Information Technology	10.1109/EIT.2012.6220764	embedded system;digital signature;cloud computing;computer science;cryptography;digital media;operating system;database;distributed computing;mobile computing;computer security;computer network	Mobile	-42.688880246963116	58.24000482630126	53780
d1f4b64b4de0f4814fc93a9cf268dcc93fdb287b	discovering neglected conditions in software by mining dependence graphs	databases;neglected conditions;graph theory;frequent subgraph mining;methods for sqa and v v;program diagnostics;empirical study;computer society;heuristic maximal frequent subgraph mining algorithm;automatic defect detection;mining software repositories;data mining techniques;software defects;dependence graph;indexing terms;data mining;graph matching;program diagnostics data mining graph theory;graph minor;ray yaung;frequent item set mining;data analysis;buffer overflow;data dependence;heuristic algorithms;program dependence graphs;pre and post conditions;methods for sqa and v methods for sqa and v pre and post conditions;linux;enhanced procedure dependence graphs;dependence graph mining;data mining computer bugs buffer overflow databases computer society data analysis heuristic algorithms open source software operating systems linux;methods for sqa and v pre and post conditions;pre and post conditions methods for sqa and v x0026 v;computer bugs;methods for sqa and v x0026;heuristic maximal frequent subgraph mining algorithm dependence graph mining neglected conditions software defects static program analysis data mining techniques enhanced procedure dependence graphs;v;static program analysis;computer engineering discovering neglected conditions in software by mining program dependence graphs case western reserve university h andy podgurski chang;defect detection;open source software;operating systems;open source;program dependence graph	Neglected conditions are an important but difficult-to-find class of software defects. This paper presents a novel approach to revealing neglected conditions that integrates static program analysis and advanced data mining techniques to discover implicit conditional rules in a code base and to discover rule violations that indicate neglected conditions. The approach requires the user to indicate minimal constraints on the context of the rules to be sought, rather than specific rule templates. To permit this generality, rules are modeled as graph minors of enhanced procedure dependence graphs (EPDGs), in which control and data dependence edges are augmented by edges representing shared data dependences. A heuristic maximal frequent subgraph mining algorithm is used to extract candidate rules from EPDGs, and a heuristic graph matching algorithm is used to identify rule violations. We also report the results of an empirical study in which the approach was applied to four open source projects (openssl, make, procmail, amaya). These results indicate that the approach is effective and reasonably efficient.	algorithm;amaya;data dependency;data mining;graph minor;heuristic;matching (graph theory);maximal set;open-source software;openssl;software bug;static program analysis	Ray-Yaung Chang;Andy Podgurski;Jiong Yang	2008	IEEE Transactions on Software Engineering	10.1109/TSE.2008.24	software bug;index term;buffer overflow;computer science;graph theory;theoretical computer science;data mining;database;data analysis;programming language;empirical research;graph minor;linux kernel;matching;static program analysis	SE	-58.79748879090088	39.7035700570875	53798
01ad1cf982b59663b1d6b896c729375d368bbc22	cfaar: control flow alteration to assist repair		We present CFAAR, a program repair assistance technique that operates by selectively altering the outcome of suspicious predicates in order to yield expected behavior. CFAAR is applicable to defects that are repairable by negating predicates under specific conditions. CFAAR proceeds as follows: 1) It identifies predicates such that negating them at given instances would make the failing tests exhibit correct behavior. 2) For each candidate predicate, it uses the program's state information to build a classifier that dictates when the predicate should be negated. 3) For each classifier, it leverages a Decision Tree to synthesize a patch to be presented to the developer. We evaluated our toolset using 149 defects from the IntroClass and Siemens benchmarks. CFAAR identified 91 potential candidate defects and generated plausible patches for 41 of them. Twelve of the patches are believed to be correct, whereas the rest provide repair assistance to the developer.	control flow;debugging;decision tree;experiment;failure;statistical classification	Chadi Trad;Rawad Abou Assi;Wes Masri;Fadi A. Zaraket	2018	2018 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)	10.1109/ISSREW.2018.00008	real-time computing;control flow;maintenance engineering;debugging;decision tree;predicate (grammar);computer science;benchmark (computing);training set	SE	-60.07160767319883	37.86924664616453	53812
34f3c5cb17cf0453452f818ab4bcf585e68b07d9	the detection of fault-prone programs	fault detection software quality software measurement computer errors predictive models software metrics time measurement computer science error analysis linear regression;developpement logiciel;software metrics;quality assurance;principal components procedure;detection erreur;deteccion error;complexity metrics;program modules;group determination;taux erreur;software complexity;sistema informatico;linear regression model;complejidad programa;computer system;ingenieria logiciel;fault prone programs;program verification;indexing terms;software engineering;discriminant analysis;analyse discriminante;linear regression models;aseguracion calidad;analisis discriminante;uncorrelated measures;verificacion programa;software reliability computational complexity program testing quality control software metrics;program modules statistical technique discriminant analysis fault prone programs principal components procedure simple multicollinear complexity metrics uncorrelated measures orthogonal complexity domains group determination quality measure large commercial systems deliberately biased data metric values relatively low error rate linear regression models;program testing;computational complexity;desarrollo logicial;software development;genie logiciel;error rate;systeme informatique;statistical technique;large commercial systems;statistical techniques;program complexity;quality measures;fiabilite logiciel;error detection;metric values;quality measure;fiabilidad logicial;alternating group;orthogonal complexity domains;quality control;verification programme;indice error;software reliability;assurance qualite;simple multicollinear complexity metrics;software quality;discriminative model;relatively low error rate;complexite programme;deliberately biased data;principal component	The use of the statistical technique of discriminant analysis as a tool for the detection of fault-prone programs is explored. A principal-components procedure was employed to reduce simple multicollinear complexity metrics to uncorrelated measures on orthogonal complexity domains. These uncorrelated measures were then used to classify programs into alternate groups, depending on the metric values of the program. The criterion variable for group determination was a quality measure of faults or changes made to the programs. The discriminant analysis was conducted on two distinct data sets from large commercial systems. The basic discriminant model was constructed from deliberately biased data to magnify differences in metric values between the discriminant groups. The technique was successful in classifying programs with a relatively low error rate. While the use of linear regression models has produced models of limited value, this procedure shows great promise for use in the detection of program modules with potential for faults. >		John C. Munson;Taghi M. Khoshgoftaar	1992	IEEE Trans. Software Eng.	10.1109/32.135775	reliability engineering;quality assurance;computer science;linear regression;software engineering;data mining;optimal discriminant analysis;software quality;statistics	SE	-62.40657496238227	33.75538397520964	53813
3206a8f8423ecae13032b6ac9ae6ed67dacb2d34	mutation-based fault localization for real-world multilingual programs (t)	libraries;fault localization;debugging;mutation based fault localization multilingual programs debugging fault localization mutation analysis;multilingual programs;mutation analysis;testing;program testing program debugging;safety;computer bugs debugging java safety testing libraries programming;mutation based fault localization;data abstraction programming languages control abstraction legacy libraries programming language ecosystem fault localization multilingual bug debugging mutation based fault localization technique real world multilingual programs multilingual bug location mutation operators empirical evaluation nontrivial real world multilingual bugs buggy statement identification suspicious statement identification;computer bugs;programming;java	Programmers maintain and evolve their software in a variety of programming languages to take advantage of various control/data abstractions and legacy libraries. The programming language ecosystem has diversified over the last few decades, and non-trivial programs are likely to be written in more than a single language. Unfortunately, language interfaces such as Java Native Interface and Python/C are difficult to use correctly and the scope of fault localization goes beyond language boundaries, which makes debugging multilingual bugs challenging. To overcome the aforementioned limitations, we propose a mutation-based fault localization technique for real-world multilingual programs. To improve the accuracy of locating multilingual bugs, we have developed and applied new mutation operators as well as conventional mutation operators. The results of the empirical evaluation for six non-trivial real-world multilingual bugs are promising in that the proposed technique identifies the buggy statements as the most suspicious statements for all six bugs.	blink;debugger;debugging;ecosystem;experiment;foreign function interface;java;language-independent specification;library (computing);mutation testing;program analysis;programmer;programming language;python;real-time operating system;software bug;test automation	Shin Hong;Byeongcheol Lee;Taehoon Kwak;Yiru Jeon;Bongsuk Ko;Yunho Kim;Moonzoo Kim	2015	2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)	10.1109/ASE.2015.14	programming;shotgun debugging;real-time computing;software bug;computer science;theoretical computer science;mutation testing;software testing;programming language;debugging;java	SE	-58.26680297189784	38.578298563651806	53817
7daff4776dfa3d93b77c8f443f77cb427a070c70	flexible quality of service management of web services orchestrations. (gestion flexible de la qualité de service dans les orchestrations de services web)		We study QoS-aware management of service orchestrations, specifically for orchestrations having a data-dependent workflow. Our study supports multi-dimensional QoS. To capture uncertainty in performance and QoS, we provide support for probabilistic QoS. Under the above assumptions, orchestrations may be non-monotonic with respect to QoS, meaning that improving the QoS of a service may decrease the end-to-end QoS of the orchestration, an embarrassing feature for QoS-aware management. We study monotonicity and provide sufficient conditions for it. We then propose a comprehensive theory and methodology for monotonic orchestrations. Generic QoS composition rules are developed via a QoS Calculus, also capturing best service binding—service discovery, however, is not within the scope of this work. Monotonicity provides the rationale for a contract-based approach to QoS-aware management. Although function and QoS cannot be separated in the design of complex orchestrations, we show that our framework supports separation of concerns by allowing the development of function and QoS separately and then“weaving”them together to derive the QoS-enhanced orchestration. Our approach is implemented on top of the Orc script language for specifying service orchestrations.		Ajay Kattepur	2012				Embedded	-46.00176906169083	41.45680499934887	53856
bd149fb90ee497cd2625f1fecf278a28281226ec	automated fault localization for c programs	fault localization;debugging;model checking;source code;counterexample	If a program does not fulfill a given specification, a model checker delivers a counterexample, a run which demonstrates the wrong behavior. Even with a counterexample, locating the actual fault in the source code is often a difficult task for the verification engineer. We present an automatic approach for fault localization in C programs. The method is based on model checking and reports only components that can be changed such that the difference between actual and intended behavior of the example is removed. To identify these components, we use the bounded model checker CBMC on an instrumented version of the program. We present experimental data that supports the applicability of our approach.	blocking (computing);boolean satisfiability problem;loop unrolling;model checking;parsing;run time (program lifecycle phase);solver;time complexity;user error	Andreas Griesmayer;Stefan Staber;Roderick Bloem	2007	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2006.12.032	model checking;real-time computing;computer science;counterexample;theoretical computer science;fault model;programming language;debugging;source code	Logic	-58.44021068660412	37.75648169617988	53868
4a234e6f40941681db04b3eea8744ab0210ad596	not-so-random numbers in virtualized linux and the whirlwind rng	kernel;random number generator;entropy linux kernel cryptography hardware instruments;virtualization;instruments;virtualization random number generator;virtual machines linux;cryptographic secrets not so random numbers virtualized linux whirlwind rng virtualized environments software based random number generators virtual machine vm snapshots entropy sources linux rng systems vm boots;cryptography;linux;entropy;hardware	Virtualized environments are widely thought to cause problems for software-based random number generators (RNGs), due to use of virtual machine (VM) snapshots as well as fewer and believed-to-be lower quality entropy sources. Despite this, we are unaware of any published analysis of the security of critical RNGs when running in VMs. We fill this gap, using measurements of Linux's RNG systems (without the aid of hardware RNGs, the most common use case today) on Xen, VMware, and Amazon EC2. Despite CPU cycle counters providing a significant source of entropy, various deficiencies in the design of the Linux RNG makes its first output vulnerable during VM boots and, more critically, makes it suffer from catastrophic reset vulnerabilities. We show cases in which the RNG will output the exact same sequence of bits each time it is resumed from the same snapshot. This can compromise, for example, cryptographic secrets generated after resumption. We explore legacy-compatible countermeasures, as well as a clean-slate solution. The latter is a new RNG called Whirlwind that provides a simpler, more-secure solution for providing system randomness.	algorithm;amazon elastic compute cloud (ec2);catastrophic interference;cryptography;instruction cycle;linux;openvms;random number generation;randomness;snapshot (computer storage);virtual machine	Adam Everspaugh;Yan Zhai;Robert Jellinek;Thomas Ristenpart;Michael M. Swift	2014	2014 IEEE Symposium on Security and Privacy	10.1109/SP.2014.42	embedded system;entropy;kernel;real-time computing;virtualization;computer science;cryptography;operating system;computer security;linux kernel	Security	-54.5194652116231	56.81164125559317	54003
68a33f3f182c30f3dc67acac1a4bcdcf76447098	analysis of communicating authorization policies		We present a formal language for specifying distributed authorization policies that communicate through insecure asynchronous media. The language allows us to write declarative authorization policies; the interface between policy decisions and communication events can be specified using guards and policy updates. The attacker, who controls the communication media, is modeled as a message deduction engine. We give trace semantics to communicating authorization policies, and formulate a generic reachability problem. We show that the reachability problem is decidable for a large class of practically-relevant policies specified in our formal language.	authorization;formal language;natural deduction;reachability problem	Simone Frau;Muhammad Torabi Dashti	2012		10.1007/978-3-642-38004-4_7	computer science;database;distributed computing;computer security	PL	-52.77863964752585	53.18401570554667	54006
2fbe9338d99cc72e61a025834ed906e304f3480e	decentralized service composition in pervasive computing environments	service composition;pervasive computing;localized interactions;community networks;decentralized control;local interaction;pervasive computing environment	In a pervasive computing environment, the devices are embedded in the physical world, providing services and interconnected by a communication network. Composition of these services is important issue of pervasive applications which integrate the physical and cyber worlds. Most existing research on service composition in pervasive computing relies on the existence of one or more entities that maintain the global service information. However, such an approach is not always practical due to dynamicity of the environment. In this paper, we propose a fully decentralized approach to service composition. We first model the service composition problem as finding an overlay of the communication network that matches the composition graph. The problem is proved to be NP-complete. We propose an algorithm for the devices to cooperatively construct the requested services through localized interactions. For the purpose of reducing redundant broadcast we propose the service composition backbone built in a fully localized way. We have carried out extensive simulations. to evaluate the performance of our algorithm. Compared with existing pull-based centralized techniques our decentralized service composition algorithm on the service composition backbone is more efficient in terms of response delay and message overhead, while achieving similar quality of composed service.	algorithm;centralized computing;embedded system;entity;interaction;internet backbone;karp's 21 np-complete problems;overhead (computing);pervasive informatics;service composability principle;simulation;telecommunications network;ubiquitous computing	Joanna Izabela Siebert;Jiannong Cao;Long Cheng;Edwin Wei;Canfeng Chen;Jian Ma	2010		10.1145/1815396.1815684	context-aware pervasive systems;decentralised system;computer science;distributed computing;world wide web;computer security;ubiquitous computing;computer network	HPC	-44.041797380166514	53.95769436751682	54033
94f3130f55216d84c810a0e8e7615ec6cc6e5c71	abstraction techniques for modeling real-world interface chips	generic model;chip;requirement specification	We describe techniques for specifying the requirements of real-world interface chips, using as a test target a processor interface unit (PIU). The PIU modeling problem is explained, and current interpreter specification practices are shown to be inadequate for the PIU requirements. General modeling techniques for handling difficult aspects of the PIU are explained, and a new approach that implements these techniques is presented and applied to the PIU requirements specification.		David A. Fura;Phillip J. Windley;Arun K. Somani	1993		10.1007/3-540-57826-9_141	chip;embedded system;computer architecture;real-time computing;computer science	EDA	-39.14141464881406	33.58340646407998	54069
4fb146193c63005226517d60ad524d58524372e2	specification and verification of normative texts using c-o diagrams	normative documents;representation;electronic contracts;legal implications;visual models;c o diagrams normative documents electronic contracts deontic logic formal verification visual models timed automata;clocks;temporal logic;specification;formal methods;semantics;formal languages;contracts;automata clocks contracts semantics cost accounting synchronization formal languages;deontic logic;automata;cost accounting;modal logic;formal verification;model checking;methodologies;synchronization;c o diagrams;timed automata;specification techniques;deontic modalities normative texts formal specification formal verification c o diagrams visual representation electronic contracts timing constraints formal semantics timed automata;specifying and verifying and reasoning about programs;text analysis automata theory formal specification formal verification	C-O diagrams have been introduced as a means to have a more visual representation of normative texts and electronic contracts, where it is possible to represent the obligations, permissions and prohibitions of the different signatories, as well as the penalties resulting from non-fulfillment of their obligations and prohibitions. In such diagrams we are also able to represent absolute and relative timing constraints. In this paper we present a formal semantics for C-O diagrams based on timed automata extended with information regarding the satisfaction and violation of clauses in order to represent different deontic modalities. As a proof of concept, we apply our approach to two different case studies, where the method presented here has successfully identified problems in the specification.	automata theory;diagram;grammatical framework;natural language;semantics (computer science);signature;timed automaton;uppaal	Gregorio Díaz;María-Emilia Cambronero;Enrique Martínez;Gerardo Schneider	2014	IEEE Transactions on Software Engineering	10.1109/TSE.2013.54	modal logic;model checking;synchronization;formal language;formal methods;temporal logic;formal verification;computer science;theoretical computer science;methodology;deontic logic;semantics;automaton;programming language;representation;specification;algorithm;cost accounting	SE	-34.61225730676303	32.48921876195442	54071
3b6aa1072ee3da6392b58196ed08cf230c86d3b1	towards automatic synthesis of a class of application-specific sensor networks	design environments;energy efficiency;formal specification;system modeling;modeling and simulation;energy efficient;plug and play;coordination mechanisms;sensor network;sensor networks;design environment;performance model;sensor nodes;software framework;design space exploration;automatic synthesis;system simulation;graphic design	Automatic synthesis of sensor network-based systems can be described as the process of translating a formal specification of application functionality into a particular task mapping, settings of available hardware knobs, and communication and coordination mechanisms among the sensor nodes, so as to meet the performance requirements and constraints. We propose a general methodology to tackle a specific class of this problem, based on analytical performance modeling, multigranularity system simulation, and automatic refinement of model parameters. To demonstrate the utility and feasibility of our proposed methodology, we define a system model for a class of sensor networks, and implement a software framework for its modeling and simulation. Our graphical design environment supports plug-and-play integration of different performance models, simulation and visualization suites, and even automatic design space exploration and optimization tools.	analytical performance modeling;automatic control;design space exploration;formal specification;graphical user interface;mathematical optimization;performance prediction;plug and play;refinement (computing);requirement;simulation;software framework	Amol Bakshi;Jingzhao Ou;Viktor K. Prasanna	2002		10.1145/581630.581639	embedded system;parallel computing;real-time computing;simulation;wireless sensor network;computer science;operating system;modeling and simulation;efficient energy use;programming language	Embedded	-41.70530879959063	35.09643419349819	54098
bb9f8de6a7dab3505b08f96eae73184c2dc42154	from the cloud to edge and iot: a smart orchestration architecture for enabling osmotic computing		The latest technological and conceptual developments have destroyed the centralized Cloud Computing model, moving Cloud services in emerging ICT infrastructures such as Edge, Fog and Internet of Things (IoT) that are closer to end users. Specifically, current Cloud computing programming models and resource orchestration techniques are challenged by the recent evolution of the IoT phenomenon because smart devices are becoming more and more pervasive, powerful and inexpensive. Therefore, services need to be place near such devices. In this regard, the Osmotic Computing aims to provide a new computing paradigm based on the deployment and migration strategies related to the infrastructures and applications requirements across Cloud, Edge, Fog an IoT layers. In this scientific paper, we investigate the Smart Orchestration of a new software abstraction called MicroELement (MEL), that encapsulates resources, services and data necessary to run IoT applications. Several use cases are presented for describing the Artificial Intelligence processes that enables the MELs deployment.	algorithm;artificial intelligence;centralized computing;cloud computing;computation;internet of things;multitenancy;pervasive informatics;programming paradigm;requirement;scientific literature;smart device;software deployment	Lorenzo Carnevale;Antonio Celesti;Antonino Galletta;Schahram Dustdar;Massimo Villari	2018	2018 32nd International Conference on Advanced Information Networking and Applications Workshops (WAINA)	10.1109/WAINA.2018.00122	end user;architecture;quality of service;computer science;orchestration (computing);distributed computing;software deployment;cloud computing;phenomenon;information and communications technology	HPC	-43.897593631166735	47.41766269495366	54149
3a6032f5aa177ef7362d346644426688beaf3a9a	modeling a distributed intrusion detection system using collaborative building blocks	distributed system;formal specification;dynamic reconfiguration;building block;uml;formal specifications;formal semantics;collaboration oriented model;distribution function;complex system;reusable models;intrusion detection system	Developing complex distributed systems is a non-trivial task. It is even more difficult when the systems need to dynamically reconfigure the distributed functionalities or tasks. Not only do we need to deal with the application-specific functionalities that are intricate, but we also have to handle the complex logic of coordinating the distribution and relocation of tasks. In this paper, we model an intrusion detection system that distributes its analysis units to a number of hosts and assigns fine-grained analysis tasks to these hosts in order to cope with the rapid increase of audit data from today's IT systems. The system is further capable to react to overload situations and to shift tasks to other hosts. To develop this complex system, we apply the model-based engineering method SPACE. In particular, we show that the collaborative specification style of the method can significantly reduce the development effort. Also, the formal semantics of SPACE ensures the correctness of important design properties.	complex system;correctness (computer science);distributed computing;intrusion detection system;relocation (computing);semantics (computer science)	Linda Ariani Gunawan;Michael Vogel;Frank Alexander Kraemer;Sebastian Schmerl;Vidar Slåtten;Peter Herrmann;Hartmut König	2011	ACM SIGSOFT Software Engineering Notes	10.1145/1921532.1921564	embedded system;complex systems;real-time computing;computer science;formal specification;distributed computing;programming language	SE	-40.60052101962078	38.14169460883717	54317
66647fbdbc21ed33ce4f55d46ac8537769742dca	publish-subscribe smartphone sensing platform for the acute phase of a disaster: a framework for emergency management support	pervasive computing publish subscribe emergency management mobile sensing human centered computing decision support sensors hazard tracking human tracking;disaster monitoring system publish subscribe smartphone sensing platform disaster acute phase emergency management support sensors mobile phone sensing human activity recognition context aware applications smartrescue project hazard developments people tracking communication capabilities crisis managers hazard detection hazard prediction risk minimizing evacuation plans smartphone based communication framework disaster specific machine learning techniques sensor readings crisis responders content based publish subscribe mechanism sensor data sharing;smart phones disasters emergency management hazards learning artificial intelligence middleware sensors;hazards;smart phones;monitoring;hazards publish subscribe intelligent sensors monitoring smart phones;publish subscribe;intelligent sensors	The advanced sensors embedded in modern smartphones opens up for novel research opportunities, as for instance manifested in the field of mobile phone sensing. Most notable is perhaps research activities within human activity recognition and context-aware applications. Along a similar vein, the SmartRescue project targets monitoring of both hazard developments as well as tracking of people in a disaster, taking advantage of smartphone sensing, processing and communication capabilities. The goal is to help crisis managers and the public in early hazard detection, hazard prediction, and in the forming of risk minimizing evacuation plans when disaster strikes. In this paper we propose a novel smartphone based communication framework for disaster specific machine learning techniques that intelligently process sensor readings into useful information for the crisis responders. Core to the framework is a robust content-based publish-subscribe mechanism that allows flexible sharing of sensor data and computation results. The proposed communication platform has been tested at the proof of concept level, with several detailed features providing promising results. We also provide the initial results from the development of this platform and discuss how to enhance the platform to become a disaster monitoring system for practical use.	activity recognition;computation;embedded system;gene prediction;machine learning;mobile phone;publish–subscribe pattern;sensor;smartphone	Jaziar Radianti;Jose J. Gonzalez;Ole-Christoffer Granmo	2014	2014 IEEE International Conference on Pervasive Computing and Communication Workshops (PERCOM WORKSHOPS)	10.1109/PerComW.2014.6815219	embedded system;simulation;hazard;computer science;internet privacy;publish–subscribe pattern;computer security;computer network;intelligent sensor	Robotics	-41.64921293233365	50.12160695510162	54392
51b217574bcbeb07e22ca28b3c2c5338e7bba8ea	reranking-based crash report deduplication		Software projects collect and deduplicate vastly numerous crash reports from users to fix bugs efficiently. However, most existing automated methods have performance issues during large-scale clustering. We propose a rerankingbased crash report clustering method. Our method is a combination of two earlier methods. By computing similarity used in ReBucket for the crash reports that are highly similar to the query crash report, the method can process reports with throughput equal to that of PartyCrasher. We also introduce an automatically generated dataset for crash report clustering tasks. The evaluation revealed that our method performs at high processing speed while maintaining high accuracy.	cluster analysis;data deduplication;firefox;futures studies;software bug;throughput;web search engine	Akira Moroo;Akiko Aizawa;Takayuki Hamamoto	2017		10.18293/SEKE2017-135	computer science;world wide web;data deduplication;crash	SE	-62.26933339861399	38.25983865215158	54460
2287820a0babdf1789b6b1949a68863586ca940c	security fatigue? shift your paradigm	software;bsimm;software measurement;information technology;computer security;software security software measurement computer security information technology internet;internet;software security;building security in;building security in maturity model;software security security building security in building security in maturity model bsimm;communities;security;security of data;buildings;real world measurements security fatigue software security it security field building security in maturity model project bsimm project	Software security is the fastest growing paradigm in the IT security field, and the Building Security in Maturity Model (BSIMM) project offers real-world measurements for assessment.	application security;capability maturity model;fastest;programming paradigm	Gary McGraw	2014	Computer	10.1109/MC.2014.70	software security assurance;the internet;computer science;information security;software engineering;software measurement;information technology;computer security	Security	-57.336900193129615	47.647031651914354	54492
bbef82795fd13caaf7a0ca92ccf47d14d238c106	a required security and privacy framework for smart objects	object recognition;privacy object recognition authentication authorization ecosystems biological system modeling;authentication;biological system modeling;trust internet of things security privacy;trusted computing data privacy internet of things;ecosystems;authorization;architectural reference model privacy framework smart object internet of things iot trust issue arm compliant security framework;privacy	The large scale deployment of the Internet of Things (IoT) increases the urgency to adequately address trust, security and privacy issues. We need to see the IoT as a collection of smart and interoperable objects that are part of our personal environment. These objects may be shared among or borrowed from users. In general, they will have only temporal associations with their users and their personal identities. These temporary associations need to be considered while at the same time taking into account security and privacy aspects. In this work, we discuss a selection of current activities being carried out by different standardization bodies for the development of suitable technologies to be deployed in IoT environments. Based on such technologies, we propose an integrated design to manage security and privacy concerns through the lifecycle of smart objects. The presented approach is framed within our ARM-compliant security framework, which is intended to promote the design and development of secure and privacy-aware IoT-enabled services.	smart objects	Antonio F. Gómez-Skarmeta;José Luis Hernández Ramos;Jorge Bernal Bernabé	2015		10.1109/Kaleidoscope.2015.7383648	personally identifiable information;privacy software;information privacy;privacy by design;business;internet privacy;world wide web;computer security	Security	-45.33997847035442	58.56186831061253	54495
298a02fc024b92d8fe1e093b75d3b3e4eba01490	evolution of ccollective object behavior in presence of simultaneous client-specific views	lenguaje programacion;context awareness;localite;programming language;java programming;separation of concern;customization;personnalisation;locality;concern separation;langage java;multiple objectives;separation preoccupation;personalizacion;langage programmation;lenguaje java;sensibilite contexte;dynamic adaptation;java language	When different clients, each with their own individual customization requirements, use the same system simultaneously, the system must dynamically adapt its behavior on a per client basis. Each non-trivial adaptation of the system’s behavior will very likely crosscut the implementation of multiple objects. In this paper we present an extension to the Java programming language that supports the dynamic evolution of collective object behavior in the presence of simultaneous client-specific views. In accordance with the separation of concerns and locality principles, client-specific customization of collective object behavior is organized as layers of mixin-like wrappers. Each layer of wrappers incrementally adds behavior and state to a group of core objects without modifying their respective implementations. Hence, collective object behavior can evolve in an additive and non-invasive way. The extension that we propose provides language constructs for defining, encapsulating and selecting behavioral refinements, and runtime support for transparently integrating them on demand.	context-sensitive grammar;evolution;experiment;java;locality of reference;mixin;programming language;real life;refinement (computing);requirement;run time (program lifecycle phase);separation of concerns;software developer;software system;stepwise regression;utility functions on indivisible goods	Bo Nørregaard Jørgensen;Eddy Truyen	2003		10.1007/978-3-540-45242-3_4	real-time computing;simulation;separation of concerns;computer science;artificial intelligence;operating system;database;distributed computing;programming language;computer security;algorithm	SE	-36.90774592536055	39.269674476031355	54560
03ab8b58ea19e251cead4c421bf61fb952ee2a50	the effects of graphical and textual visualisations in multi-representational debugging environments	programming environments;program debugging graphical visualisation textual visualisation multirepresentational debugging environment software debugging environment sde program execution debugging performance;program visualisation program debugging programming environments;program debugging;program visualisation;visualization debugging java computer displays switching frequency humans programming environments information resources psychology focusing	The effects of graphical and textual visualisations in a multi-representational debugging environment were investigated in computing students who used a software debugging environment (SDE) that allowed them to view the execution of programs in steps and that provided them with concurrently displayed, adjacent, multiple and linked representa-	debugging;graphical user interface;java platform debugger architecture	Pablo Romero;Benedict du Boulay;Rudi Lutz;Richard Cox	2003		10.1109/HCC.2003.1260234	computer architecture;real-time computing;computer science;algorithmic program debugging;programming language;debugging	HCI	-54.43307525176553	36.06021019719106	54653
69ae28adf652e181f564bd30e69f0533c88efdac	scope-aided test prioritization, selection and minimization for software reuse		Software reuse can improve productivity, but does not exempt developers from the need to test the reused code into the new context. For this purpose, we propose here specific approaches to white-box test prioritization, selection and minimization that take into account the reuse context when reordering or selecting test cases, by leveraging possible constraints delimiting the new input domain scope. Our scope-aided testing approach aims at detecting those faults that under such constraints would be more likely triggered in the new reuse context, and is proposed as a boost to existing approaches. Our empirical evaluation shows that in test suite prioritization we can improve the average rate of faults detected when considering faults that are in scope, while remaining competitive considering all faults; in test case selection and minimization we can considerably reduce the test suite size, with small to no extra impact on fault detection effectiveness considering both in-scope and all faults. Indeed, in minimization, we improve the in-scope fault detection effectiveness in all cases. © 2016 Elsevier Inc. All rights reserved.	code reuse;delimiter;fault detection and isolation;sensor;test case;test suite;white-box testing	Breno Miranda;Antonia Bertolino	2017	Journal of Systems and Software	10.1016/j.jss.2016.06.058	real-time computing;test management approach;systems engineering;computer science;prioritization;software;minification;test case;reuse;test suite;fault detection and isolation;reliability engineering	SE	-60.83474372331821	34.617341860155435	54656
ca0125f0fb978ec3272d1d7021ea83a1a1a1438a	flexible and continuous execution of real-time critical robotic tasks	real time;object oriented programming;industrial robots;task scheduling;robotic tasks;dataflow language;flexibility;robot programming	Today, industrial robots are usually programmed using specialised programming languages, different for every robot manufacturer. These languages provide good usability, because they are tailored to the functionality traditionally offered by robots. However, these languages are reaching their limits with the growing integration of sensors or multiple robot systems. Therefore, we propose an architecture based on the separation of application control and the execution of real-time robotic tasks. This article describes a flexible and extensible interface for the specification and continuous execution of robotic tasks.		Michael Vistein;Andreas Angerer;Alwin Hoffmann;Andreas Schierl;Wolfgang Reif	2014	IJMA	10.1504/IJMA.2014.059773	real-time computing;simulation;computer science;programming language	Robotics	-34.42980644841181	37.58165643726022	54688
20d3ddb354e3752081c75986bd79dda0f2ad81c0	preliminary design of jml: a behavioral interface specification language for java	software metrics;postcondition;model based approach;development skill;software engineering;assertion;model based specification;eiffel;behavioral interface specification;jml;frame;larch;software quality;precondition;java	JML is a behavioral interface specification language tailored to Java(TM). Besides pre- and postconditions, it also allows assertions to be intermixed with Java code; these aid verification and debugging. JML is designed to be used by working software engineers; to do this it follows Eiffel in using Java expressions in assertions. JML combines this idea from Eiffel with the model-based approach to specifications, typified by VDM and Larch, which results in greater expressiveness. Other expressiveness advantages over Eiffel include quantifiers, specification-only variables, and frame conditions.This paper discusses the goals of JML, the overall approach, and describes the basic features of the language through examples. It is intended for readers who have some familiarity with both Java and behavioral specification using pre- and postconditions.	debugging;eiffel;java modeling language;larch family;postcondition;quantifier (logic);software engineer;specification language;vienna development method	Gary T. Leavens;Albert L. Baker;Clyde Ruby	2006	ACM SIGSOFT Software Engineering Notes	10.1145/1127878.1127884	precondition;frame;postcondition;real-time computing;assertion;jsr 94;computer science;eiffel;software engineering;java modeling language;programming language;java;software quality;software metric	SE	-50.25705708598163	33.721172526978464	54728
09242515a89f03034954354c677f7289361afaac	importance of real-time distributed computing software building-blocks in realization of ubiquitous computing societies	software building blocks;real time distributed computing;fault tolerant;application software;building block;real time;object oriented programming ubiquitous computing distributed processing software engineering software fault tolerance security of data real time systems;distributed processing;distributed computing;software fault tolerance;object oriented programming;software engineering;accuracy;network connectivity;fault tolerant systems;fault tolerance;explosives;distributed computing ubiquitous computing application software explosives communications technology software engineering java timing accuracy fault tolerant systems;communications technology;ubiquitous computing;c ubiquitous computing software engineering fault tolerance security mobile agent real time distributed computing software building blocks c java;communication technology;mobile agent;c;security;security of data;java;real time systems;timing	Explosive advances in communication technologies that have occurred since 1993 have given birth to rapidly growing demands for new-generation distributed computing (DC) applications, all geared toward realization of better and better ubiquitous computing societies. Economic and reliable construction of new-generation DC applications has thus become an issue of national interests in all advanced nations. What is needed is a new-generation DC software engineering technology which is at least multiple times more effective in constructing new-generation DC applications than the currently practiced technologies are. In particular, this author believes that a new-generation building-block (BB), which is much more advanced than the current-generation DC object provided in languages C++, Java, and C#, is needed. Such a BB should enable systematic and economic construction of DC applications that are capable of taking critical actions with 100-microsecond-level or even 1-microsecond-level timing accuracy, fault tolerance, and security enforcement while being easily expandable and taking advantage of all sorts of network connectivity. Some directions considered worth pursuing for finding such BBs are discussed.	distributed computing;real-time locating system;ubiquitous computing	K. H. Kim	2005		10.1109/ISADS.2005.1452046	information and communications technology;fault tolerance;real-time computing;computer science;operating system;database;distributed computing;programming language;computer security;ubiquitous computing	HPC	-48.33178683802185	46.64145309080147	54797
