id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
b955132c7cfc25fe1816fbac6edf7a79db946946	phototacs: an image-based cell phone interface	cell phone;user interface;illiteracy;assistive technology;visual impairment;phone book;cognitive disabilities	As new features have been added to cellular phones, their user interfaces have become increasingly complex, making it difficult for people with cognitive or visual impairments to use them. This paper describes the development and preliminary testing of a simplified, intuitive image-based cell phone interface.	mobile phone;user interface	Michael J. Astrauskas	2008		10.1145/1414471.1414557	functional illiteracy;human–computer interaction;computer science;operating system;multimedia;user interface	HCI	-48.870769917856876	-41.28625800624095	14333
267e83dd10b5eed292c20cf6a3bb1b28006d0121	the computer graphics wars heat up	computer graphics rendering computer graphics military computing world wide web marketing and sales games computer displays computer industry kirk field collapse effect engines;computer graphics hardware;paper;ati radeon 9700 pro;gamecubes;computer graphics;computer graphic equipment;ati;rendering computer graphics computer graphic equipment;computer industry;computer graphic;engines;nvidia nv30;kirk field collapse effect;games;console games;3d graphics cards;computer displays;nvidia;world wide web;computer science;xbox;rendering computer graphics;siggraph;review;3d graphics;military computing;marketing and sales;pc game sales;ati radeon 9700 pro computer graphics hardware console games pc game sales siggraph nvidia nv30 3d graphics cards xbox gamecubes	The author describes the present status of computer graphics hardware. While console games flourished, PC game sales have plummeted. The plans of Nvidia and ATI, the 3D graphics world's two dominant players are described.	computer graphics	Michael R. Macedonia	2002	IEEE Computer	10.1109/MC.2002.1039523	video game graphics;games;simulation;computer science;operating system;multimedia;computer graphics;world wide web;software rendering;3d computer graphics;computer graphics (images)	Visualization	-55.28320960628051	-27.89864068601559	14336
501cc72289d06468d098d17787ed694a6df9969c	teaching a calligraphy robot via a touch screen	acceleration;pressing;personal writing replication calligraphy robot teaching chinese calligraphy chinese character writing art intangible cultural heritage universal education calligraphy character effect calligraphy character shape capacitive touch screen touch point positions writing velocity acceleration strokes language program brush writing;teaching calligraphy chinese characters robot;fingers;writing;writing brushes pressing fingers robot kinematics acceleration;touch sensitive screens art computer aided instruction educational robots history human robot interaction teaching;robot kinematics;brushes	Chinese calligraphy as a Chinese character writing art is an important part of Chinese art. As a representative of the intangible cultural heritage of humanity, Chinese calligraphy is under protection and its universal education is a very important inheritance means. This paper proposes a new method to teach a robot to implement the vivid replication of personal writing without losing the shape and effect of a calligraphy character. In the method, a capacitive touch screen used as an input device to obtain the features such as touch point positions, strokes, width, writing velocity and acceleration. After a series of processes, the writing is eventually transformed to a language program for a calligraphy robot to perform the repeated writing. The writing on the touch screen, writing with a brush by hand and one with a brush by the robot are compared. The results show that the robot taught with the proposed method can preserve the writing features of a writer and achieve the effect of brush writing.	artificial intelligence;input device;robot;self-replicating machine;systems design;touchscreen;velocity (software development)	Jun Li;Wei Sun;Mengchu Zhou;Xianzhong Dai	2014	2014 IEEE International Conference on Automation Science and Engineering (CASE)	10.1109/CoASE.2014.6899330	visual arts;engineering;multimedia;communication	Robotics	-41.1065703572709	-43.38535098927817	14368
1e434b91b5e78f24fbe08f0689e3709679cd4353	contextcontacts: re-designing smartphone's contact book to support mobile awareness and collaboration	group and organization interfaces;mobility;affective interaction;situation cues;mobile phone;boundary condition;awareness;mobile communication;computer mediated communication;smartphone;interaction design;context;privacy;self disclosure	"""Acontextuality of the mobile phone often leads to a caller's uncertainty over a callee's current state, which in turn often hampers mobile collaboration. We are interested in re-designing a Smartphone's contact book to provide cues of the current situations of others. ContextContacts presents several meaningful, automatically communicated situation cues of trusted others. Its interaction design follows social psychological findings on how people make social attributions based on impoverished cues, on how self-disclosure of cues is progressively and interactionally managed, and on how mobility affects interaction through cues. We argue how our design choices support mobile communication decisions and group coordinations by promoting awareness. As a result, the design is very minimal and integrated, in an """"unremarkable"""" manner, to previously learned usage patterns with the phone. First laboratory and field evaluations indicate important boundary conditions for and promising avenues toward more useful and enjoyable mobile awareness applications."""	interaction design;mobile phone;smartphone	Antti Oulasvirta;Mika Raento;Sauli Tiitta	2005		10.1145/1085777.1085805	awareness;mobile telephony;human–computer interaction;boundary value problem;computer science;operating system;interaction design;self-disclosure;multimedia;mobile computing;privacy;computer-mediated communication	HCI	-58.540452474851676	-40.82764237795655	14391
b9c9ca40c49d9a9c7fa167d7b28cdd080cc6850e	an empirical study of visitors' experience at kuching orchid garden with mobile guide application		This empirical study was conducted to measure visitors’ experiences with a mobile guide application at Kuching Orchid Garden (KOG). A between-group experimental design with 114 participants was conducted to test three groups; a group using the mobile guide application as an information aid, a control group (with no information aid), and a group using pamphlets to explore the KOG. The Museum Experience Scale (MES) was used to evaluate visitors’ experience for all participants, whilst the Multimedia Guide Scale (MMGS) was used to evaluate the visitors’ experience with the mobile guide group. The most notable result from the Museum Experience Scale (MES) showed an impact on the visitors in terms of knowledge and learning when using the mobile guide application. However, the study found that enhancing visitors experience goes beyond simply providing interactive technologies in public settings to aid with information delivery. A limitation was providing relevant information in a timely and seamless manner due to inaccuracies of mapping between physical and digital environments. Future works should consider beacons and other Bluetooth low energy (BLE) technology to address the issues with location based devices. It is also important to highlight that the use of one’s own device had a significant impact on learnability and control of the device, thus suggesting that the BYOD concept should be widely used in informal educational settings implementing mobile guide applications. The use of MES and MMGS informs future researches with an understanding of the different dimensions of visitors’ experiences with mobile guide technology in public spaces to inform mobile application development that may further boost visitors’ engagement, emotional connection, and meaningful experience.		Mohd Kamal Othman;Khairul Izham Idris;Shaziti Aman;Prashanth Talwar	2018	Adv. Human-Computer Interaction	10.1155/2018/5740520	learnability;empirical research;computer science;human–computer interaction;bluetooth low energy	HCI	-57.08441291644598	-42.764166636946605	14428
a54f42b1ce426c254f6ff0e2bad3d64f2270b3da	a novel framework for visuo-haptic percutaneous therapy simulation based on patient-specific clinical trials		Percutaneous therapy is a common clinical operation in minimally invasive surgery. Yet, learning curve of this skillful manual operation is steep, which imposes negative impacts on its further advances. In this paper, we proposed a novel workflow to simulate percutaneous therapy through visuo-haptic rendering based on the clinical trials. Intraoperative puncture data, obtained by our 6DOF force recording system in the operating room, is fitted as the original force model for the haptic rendering. Patient-specific medical images were also segmented and reconstructed for the highly immersive virtual training scenario. Last but not least, medical professors and novices have also been invited to practice on our training scenario by employed the Global Rating Scale (GRS) questionnaire and parameter metrics recording to validate framework's performance. Posttest values in experts and novices' groups after training showed great progress with respect to pretest values in both GRS scores and objective evaluation.	rating scale;simulation	Yonghang Tai;Lei Wei;Hailing Zhou;Saeid Nahavandi;Junsheng Shi;Qiong Li;Feiyan Li	2017	2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2017.8123149	rendering (computer graphics);machine learning;artificial intelligence;global rating;human–computer interaction;clinical trial;computer science;solid modeling;workflow;haptic technology;percutaneous;virtual training	Visualization	-41.46738234579803	-47.45944074389551	14440
5a489b3a1dbb0b82c5f12259c16c63c33051d1cf	supervisory control of multiple social robots for navigation	real-world environment;robot behavior;motion control;navigational tasks;human-robot interaction;human-robot team;slow locomotion speed;mobile robots;robot safety model;multiple mobile social robots;multiple mobile social robot;supervisory control;social robots;multiple social robots;entertaining people;semi-autonomous robot system;shopping mall;navigation;system implementation;human study;acceptable range;multiple social robot;educational robots;data collection;customer satisfaction;human robot interaction	This paper presents a human study and system implementation for the supervisory control of multiple social robots for navigational tasks. We studied the acceptable range of speed for robots interacting with people through navigation, and we discovered that entertaining people by speaking during navigation can increase people's tolerance toward robots' slow locomotion speed. Based on these results and using a robot safety model developed to ensure safety of robots during navigation, we implemented an algorithm which can proactively adjust robot behaviors during navigation to improve the performance of a human-robot team consisting of a single operator and multiple mobile social robots. Finally, we implemented a semi-autonomous robot system and conducted experiments in a shopping mall to verify the effectiveness of our proposed methods in a real-world environment.	algorithm;autonomous robot;experiment;interaction;semiconductor industry;social robot	Kuanhao Zheng;Dylan F. Glas;Takayuki Kanda;Hiroshi Ishiguro;Norihiro Hagita	2013	2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI)		human–robot interaction;motion control;mobile robot;computer vision;navigation;simulation;computer science;artificial intelligence;social robot;robot control;supervisory control;mobile robot navigation;personal robot;aisoy1	Robotics	-46.82403846614108	-50.9908705912332	14466
d3dff3ab84054f26ce54d90bd1a0bf4dcc8ba850	the effects of interpersonal attitude of a group of agents on user's presence and proxemics behavior	embodied conversational agents;group behavior;interpersonal attitude;human territoriality;multimodal nonverbal behavior;social presence	In the everyday world people form small conversing groups where social interaction takes place, and much of the social behavior takes place through managing interpersonal space (i.e., proxemics) and group formation, signaling their attentio to others (i.e., through gaze behavior), and expressing certain attitudes, for example, friendliness, by smiling, getting close through increased engagement and intimacy, and welcoming newcomers. Many real-time interactive systems feature virtual anthropomorphic characters in order to simulate conversing groups and add plausibility and believability to the simulated environments. However, only a few have dealt with autonomous behavior generation, and in those cases, the agents’ exhibited behavior should be evaluated by users in terms of appropriateness, believability, and conveyed meaning (e.g., attitudes). In this article we present an integrated intelligent interactive system for generating believable nonverbal behavior exhibited by virtual agents in small simulated group conversations. The produced behavior supports group formation management and the expression of interpersonal attitudes (friendly vs. unfriendly) both among the agents in the group (i.e., in-group attitude) and towards an approaching user in an avatar-based interaction (out-group attitude). A user study investigating the effects of these attitudes on users’ social presence evaluation and proxemics behavior (with their avatar) in a three-dimensional virtual city environment is presented. We divided the study into two trials according to the task assigned to users, that is, joining a conversing group and reaching a target destination behind the group. Results showed that the out-group attitude had a major impact on social presence evaluations in both trials, whereby friendly groups were perceived as more socially rich. The user’s proxemics behavior depended on both out-group and in-group attitudes expressed by the agents. Implications of these results for the design and implementation of similar intelligent interactive systems for the autonomous generation of agents’ multimodal behavior are briefly discussed.	autonomous robot;avatar (computing);intelligent agent;interactivity;multimodal interaction;norm (social);plausibility structure;real-time locating system;simulation;social presence theory;usability testing;virtual world	Angelo Cafaro;Brian Ravenet;Magalie Ochs;Hannes Högni Vilhjálmsson;Catherine Pelachaud	2016	TiiS	10.1145/2914796	simulation;group dynamics	HCI	-51.58722785368134	-49.00786220035813	14481
c8ca2da55c8f1264407b9f639c5299be6eb2cb53	ls3d: lego search combining speech and stereoscopic 3d	3d object retrieval;voice search;speech recognition;multimodal interaction;immersive visualization;user interfaces	The number of available 3D digital objects has been increasing considerably. As such, searching in large collections has been subject of vast research. However, the main focus has been on algorithms and techniques for classification, indexing and retrieval. While some works have been done on query interfaces and results visualization, they do not explore natural interactions. The authors propose a speech interface for 3D object retrieval in immersive virtual environments. As a proof of concept, they developed the LS3D prototype, using the context of LEGO blocks to understand how people naturally describe such objects. Through a preliminary study, it was found that participants mainly resorted to verbal descriptions. Considering these descriptions and using a low cost visualization device, the authors developed their solution. They compared it with a commercial application through a user evaluation. Results suggest that LS3D can outperform its contestant, and ensures better performance and results perception than traditional approaches for 3D object retrieval.	stereoscopy	Pedro B. Pascoal;Daniel Mendes;Diogo Henriques;Isabel Trancoso;Alfredo Ferreira	2015	IJCICG	10.4018/IJCICG.2015070102	computer vision;human–computer interaction;computer science;multimodal interaction;data mining;multimedia;user interface;world wide web	NLP	-49.20606065162305	-43.24595882413673	14531
9ed79de5b452a07bc6f1c7d15d6bc759c83dbee7	creativity and intelligence in brains and machines: from individuals to societies		During the IK, I felt “surrounded by knowledge”. The IK offers many workshops, evening talks and lectures. The programme of the IK is as interdisciplinary as are the participants. It is a unique chance to hear from many different fields, discover differences and similarities between them and get an integrated impression of the IK focus theme. I visited for example the lectures “Deep Learning to Deep Reinforcement Learning” (Razvan Pascanu, Google Deep Mind), “Animal social dynamics an evolutionary perspective” (Jennifer Fewell, Arizona State University) and “AI at the Movies” (Tony Veale, UCD Dublin). Additionally, the IK gives students the opportunity to present their actual research project or final thesis at the poster presentation. I used this chance and got helpful and diverse feedback.	aspartate transaminase;brain;deep learning;educational workshop;ik gene;inverse kinematics;lectures;reinforcement learning;social dynamics;societies;the movies;user-centered design	Katharina Weitz	2017	Cognitive Processing	10.1007/s10339-017-0845-2	cognitive psychology;psychology;creativity	ML	-54.53474625954	-25.725968298853875	14579
3f1b640533245a99296328876c2f9059bc5ea8f7	towards virtual videography (poster session)	image understanding;automatic presentation;videography;educational technology;image based rendering	Videographers have developed an art of conveying events in video. Through choices made in cinematography, editing, and post-processing, effective video presentations can be created from events recorded with little or no intrusion. In this paper, we explore systems that bring videography to situations where cost or time issues preclude application of the art. Our goal is to develop virtual videography, that is, systems that can help automate the process of creating an effective video presentation from given footage. In this paper, we discuss how virtual videography systems can be constructed by combining image-based rendering to synthetically generate shots with image understanding to help choose what should be shown to the viewer. To this, visual effects can be added to enhance the presentation, lessening the degradation caused by the medium.	computer vision;elegant degradation;video post-processing;visual effects	Michael Gleicher;James Masanz	2000		10.1145/354384.354537	computer vision;educational technology;image-based modeling and rendering;computer science;multimedia;world wide web;computer graphics (images)	HCI	-39.699798480688905	-35.00164940832075	14631
6ef4e372b96806018adb4f7135b9834a92c73b09	anarchy or order on the streets: review based characterization of location based mobile games		Location based mobile games have traditionally relied on implicit codes of conduct, legal ordinances, common social norms, or community emergent rules. However, these games are becoming increasingly popular and enforcing these implicit or explicit restrictions has become difficult. In this paper, we present a critical and systematic review of both commercial and non-commercial location based mobile games. We list selected characteristics of the games and highlight their connection to the affordances and restrictions on urban game arenas. We also demonstrate the feasibility of our characterization by applying it to two recent location based mobile games, Pokemon GO [53] and Street Art Gangs [4].	academy;anarchy;capability maturity model;code;emergence;geocaching;location-based service;mobile game;norm (social);open innovation;orchestration (computing);systematic review	Paula Alavesa;Minna Pakanen;Hannu Kukka;Matti Pouke;Timo Ojala	2017		10.1145/3116595.3116614	multimedia;affordance;simulation;video game design;computer science;emergent gameplay;turns, rounds and time-keeping systems in games;game design;combinatorial game theory;game mechanics;norm (social)	HCI	-57.403455526579116	-38.507385501036495	14686
3c6004efc6c18ab81482dc6d4d2e2741ec40673f	what you feel is what i do: a study of dynamic haptic interaction in distributed collaborative virtual environment	virtual reality;cve;human performence;haptic guide	"""In this paper we present the concept of """"What You Feel Is What I Do (WYFIWID)"""". The concept is fundamentally based on a haptic guide that allows an expert to control the hand of a remot trainee. When haptic guide is active then all movements of the expert's hand (via input device) in the 3D space are haptically reproduced by the trainee's hand via a force feedback device. We use haptic guide to control the trainee's hand for writing alphabets and drawing geometrical forms. Twenty subjects participated in the experiments to evaluate."""	collaborative virtual environment;haptic technology	Sehat Ullah;Xianging Liu;Samir Otmane;Paul Richard;Malik Mallem	2011		10.1007/978-3-642-21605-3_16	common vulnerabilities and exposures;simulation;human–computer interaction;computer science;virtual reality;multimedia	Visualization	-45.11645197507513	-48.254151907209746	14752
e7c53e59aa2071e54fcc05f6035f67389847b069	mobility, digital libraries and a rural indian village	mobility;digital library;digital libraries;information ecologies;mobile phone;digital content;developing country;developing world;information need;user generated content;interaction design;digital divide;mobile user	Millions of people in developed countries routinely create and share digital content; but what about the billions of others in on the wrong side of what has been called the 'global digital divide'? This paper considers three mobile platforms to illustrate their potential in enabling rural Indian villagers to make and share digital stories. We describe our experiences in creating prototypes using mobile phones; high-end media-players; and, paper. Interaction designs are discussed along with findings from various trials within the village and elsewhere. Our approach has been to develop prototypes that can work together in an integrated fashion so that content can flow freely and in interesting ways through the village. While our work has particular relevance to those users in emerging world contexts, we see it also informing needs and practices in the developed world for user-generated content.	digital library;digital recording;interaction technique;library (computing);mobile device;mobile phone;relevance;software prototyping;user-generated content	Matt Jones;Emma Thom;David Bainbridge;David M. Frohlich	2009		10.1145/1555400.1555451	digital transformation;digital divide;digital library;developing country;computer science;digital media;multimedia;world wide web	HCI	-55.56108644856884	-39.37447904578487	14777
8b52740ec8df81ca6fe34605c8515a242193f0d5	minimal yet integral - designing a gestural interface		Minimalism and simplicity have become key success factors in the post-PC era. Touchscreens have superseded physical buttons as the dominant user interface of mobile devices. Some of the industry's most successful prod- ucts tightly integrate hardware, software and services into one convenient solution. All this transformed the setting in which we are designing user expe- riences today. This paper describes the two-year development of a gestural user interface for a mobile app. Our design process can be broken down into five ba- sic principles: Find a tangible metaphor, understand your hardware, care for your content, reduce it to the essence, and if you feel you can do better, iterate. Finally some yet unsolved issues are described that may impede the design of truly natural interfaces on a fundamental level.	gesture recognition	Martin Osen	2013		10.1007/978-3-642-39238-2_42	simulation;human–computer interaction;computer science;communication	HCI	-48.94703656147513	-41.50716310537575	14795
25750a1edfa1f085cdc600f6e601c7f9273c541c	case-based reasoning on images and signals	application-oriented part;edited book;signal variation;environmental condition;introducing cbr strategy;knowledge-based technique;special topic;new strategy;signal-interpreting system;process requirement;signal-processing process;different learning capability;development process;case-based reasoning;case base reasoning	Feel lonely? What about reading books? Book is one of the greatest friends to accompany while in your lonely time. When you have no friends and activities somewhere and sometimes, reading book can be a great choice. This is not only for spending the time, it will increase the knowledge. Of course the b=benefits to take will relate to what kind of book that you are reading. And now, we will concern you to try reading case based reasoning on images and signals as one of the reading material to finish quickly.	book;case-based reasoning		2008			engineering;knowledge management;artificial intelligence;operations research	AI	-61.11378918922087	-25.859515912437836	14800
92f53614aa352f28f137aa1497bebdc21d54fe8f	support for content creation using conversation quanta	content management;cuantificacion senal;developpement logiciel;interfase usuario;user interface;conversacion;gestion contenido;intelligence artificielle;signal quantization;senal video;signal video;continuous flow;desarrollo logicial;agent intelligent;quantification signal;software development;intelligent agent;conversation;conversational agent;gestion contenu;video signal;artificial intelligence;interface utilisateur;agente inteligente;inteligencia artificial;information system;systeme information;sistema informacion	In this study, we present a computational approach to support content creation by recording and reusing the conversational contents as reusable nuggets. We introduce the concept of the conversation quantization a technique for approximating a continuous flow of conversation by a series of conversation quanta that correspond to the points in a discourse. We describe the creation of contents using conversation quanta. To realize the concept of conversation quanta, we attempt to manually extract conversation quanta from the videos of some meetings and create the conversational contents. As a result, we have confirmed that conversation quanta can be reused as conversational contents such as conversational agents and presentation contents. Further, we have obtained valuable insights into the nature of conversation quanta.	dialog system;quanta computer;quantum	Ken Saito;Hidekazu Kubota;Yasuyuki Sumi;Toyoaki Nishida	2005		10.1007/11780496_4	content management;computer science;artificial intelligence;software development;multimedia;user interface;intelligent agent;information system	HCI	-36.52782359756314	-26.042082549349985	14827
5afbcf1195993d4e29237d26fd975b0e25d52213	facing interfaces: paul otlet's visualizations of data integration	interfaces;information science history;information representations;knowledge organization systems;mental visualization	Most historical explanations of interfaces are technological and start with the computer age. We propose a different approach by focusing on the history of library and information sciences, particularly on the case of Paul Otlet (1868–1944). Otlet’s attempts to integrate and distribute knowledge imply the need for interfaces, and his conceptualizations are reminiscent of modern versions of interfaces that are intended to facilitate manual and mechanical data integration and enrichment. Our discussion is based on a selection from the hundreds of images of what we may think of as “interfaces” that Otlet made or commissioned during his life. We examine his designs for interfaces that involve bibliographic cards, that allow data enrichment, his attempts to visualize interfaces between the sciences and between universal and personal classifications, and even his attempts to create interfaces to the world. In particular, we focus on the implications of Otlet’s dissection of the organization of the book for the creation of interfaces to a new order of public knowledge. Our view is that the creative ways in which he faces tensions of scalability, representation, and perception of relationships between knowledge objects might be of interest today.	gene ontology term enrichment;library and information science;scalability	Charles van den Heuvel;W. Boyd Rayward	2011	JASIST	10.1002/asi.21607	social science;human–computer interaction;computer science;interface;data mining;multimedia;world wide web;information retrieval	HCI	-57.39348523530043	-27.401421414377115	14837
bae9ee8c1cffc9f075b866201046c69d69f9e855	performance optimizations of virtual keyboards for stroke-based text entry on a touch-based tabletop	ucl;keyboard layout;virtual keyboard;interactive tabletops;text entry;discovery;theses;conference proceedings;digital web resources;ucl discovery;open access;shape writing;touch input;ucl library;book chapters;open access repository;performance optimization;ucl research;fitts s law	Efficiently entering text on interactive surfaces, such as touch-based tabletops, is an important concern. One novel solution is shape writing - the user strokes through all the letters in the word on a virtual keyboard without lifting his or her finger. While this technique can be used with any keyboard layout, the layout does impact the expected performance. In this paper, I investigate the influence of keyboard layout on expert text-entry performance for stroke-based text entry. Based on empirical data, I create a model of stroking through a series of points based on Fitts's law. I then use that model to evaluate various keyboard layouts for both tapping and stroking input. While the stroke-based technique seems promising by itself (i.e., there is a predicted gain of 17.3% for a Qwerty layout), significant additional gains can be made by using a more-suitable keyboard layout (e.g., the OPTI II layout is predicted to be 29.5% faster than Qwerty).	fitts's law;lambda lifting;plover;virtual keyboard	Jochen Rick	2010		10.1145/1866029.1866043	human–computer interaction;computer science;operating system;multimedia;fitts's law;world wide web	HCI	-47.3036327905806	-45.150319733838124	14863
4ab50e9055d08365db3ae56187d3aba56a82a36d	development of an exoskeleton haptic interface for virtual task training	elbow;exoskeleton haptic interface;training;virtual reality;human robot interaction;joints;virtual environments;composite control;virtual reality graphical user interfaces haptic interfaces human robot interaction telemetry training;force;orbits;exoskeletons;graphical user interfaces;functional training;qt graphics library;shoulder;graphic user interface;telemetry;virtual task training;exoskeletons haptic interfaces virtual environment communication system control telemetry graphics libraries graphical user interfaces elbow viscosity;virtual environment;haptic interfaces;telemetry exoskeleton haptic interface virtual task training functional training virtual environments composite control qt graphics library graphical user interfaces;inter process communication;haptic interface	An exoskeleton haptic interface is developed for functional training in virtual environments. A composite control scheme enables a variety of tasks to be implemented, and a “Qt” graphics library is used to generate the virtual environment for the haptic interface at the hand and graphical user interfaces for input and telemetry. Inter-process communications convert telemetry from the exoskeleton into motion commands for objects in the virtual environment. A second haptic interface at the upper arm is used to control the elbow orbit self-motion of the arm during tasks. Preliminary results are reviewed for a wall-painting task in which the virtual wall stiffness and viscosity are generated using an admittance controller.	characteristic impedance;graphical user interface;graphics library;haptic technology;hercules graphics card;lopes (exoskeleton);self-replicating machine;simulation;virtual reality	Craig R. Carignan;Jonathan Tang;Stephen N. Roderick	2009	2009 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2009.5354834	embedded system;simulation;human–computer interaction;computer science;engineering;artificial intelligence;graphical user interface;virtual reality	Robotics	-36.34483407037884	-38.21384027131142	14868
734fb11ba2d077da01305fd25b8c8129979ae28a	the suitability of fddi in multimedia communications in light of the development in atm	qa75 electronic computers computer science;qa75 electronic computers computer science communication video recording sound recording and reproducing pattern recognition systems pattern perception image processing		atm turbo	David Leslie Robinson	1998			computer vision;computer science;multimedia;computer graphics (images)	Mobile	-46.39475692041745	-32.90300791801641	14929
0b1b88fb8bfa3b9f32aab667b9b1caab8b979d91	strip'tic: exploring augmented paper strips for air traffic controllers	air traffic control;interactive paper;digital pen;tangible interface;visualization;paper computing;augmented paper;tangible interfaces	The current environment used by French air traffic controllers mixes digital visualization such as radar screens and tangible artifacts such as paper strips. Tangible artifacts do not allow controllers to update the system with the instructions they give to pilots. Previous attempts at replacing them in France failed to prove efficient. This paper is an engineering paper that describes Strip'TIC, a novel system for ATC that mixes augmented paper and digital pen, vision-based tracking and augmented rear and front projection. The system is now working and has enabled us to run workshops with actual controllers to study the role of writing and tangibility in ATC. We describe the system and solutions to technical challenges due to mixing competing technologies.	advanced transportation controller;artifact (software development);digital pen;radar;strips	Christophe Hurter;Rémi Lesbordes;Catherine Letondal;Jean-Luc Vinot;Stéphane Conversy	2012		10.1145/2254556.2254598	simulation;visualization;human–computer interaction;computer science;air traffic control;multimedia;world wide web;computer graphics (images)	AI	-47.4203670104838	-33.03711500524271	14962
07d58e3923475b33c6f89e3eaaba9c246d5eceb3	robocup as a spectator sport: simulating emotional response in the four-legged league	human computer interaction;emotional intelligence;expressed emotion;human robot interaction;robocup;for 0806 information systems;sony aibo;robots;robot soccer	This paper presents a model for simulating emotion and personality in the Four-Legged League of the RoboCup Competition. This paper argues that by introducing simulated emotional responses and state dynamics to the robots competing in the competition, they will provide a more life-like display for spectators of the sport. Further, by simulating Emotional Intelligence a team may gain a competitive edge over the competition. For the RoboCup competition to be successful at achieving its goal of creating a soccer team that will be competitive with the number one human soccer team by the year 2050, all facets of soccer competition need to be considered, including the simulation of human-like behaviour for the competitors. Emotion theory and expression are explored, and a model is presented based upon emotional states. The capabilities of the Aibo ERS-7 robots are then presented, with the capabilities of the robots expressing emotional states considered. The paper concludes with a discussion of the implementation of this model on the Aibo robots, and future areas of research.	aibo;robot;simulation	Matthew Willis	2008		10.1145/1514402.1514410	simulation;engineering;artificial intelligence;multimedia	AI	-52.51985357446864	-49.90940549776059	15048
aa4dcc89c44a05d2e94665f3c1cdf2749699be88	what's real about virtual reality?	virtual reality;liquid crystal displays;vehicle driving;virtual reality production hardware rendering computer graphics vehicle dynamics augmented reality ergonomics costs vehicle driving liquid crystal displays;production;augmented reality;rendering computer graphics;ergonomics;vehicle dynamics;hardware	"""s usual with infant technologies, realizing the early dreams for virtual reality (VR) and harnessing it to real work has taken longer than the initial wild hype predicted. Now, finally, it's happening. In his great invited lecture in 1965, """" The Ultimate Display , """" Ivan Sutherland laid out a vision 1 (see the side-bar), which I paraphrase: Don't think of that thing as a screen, think of it as a window, a window through which one looks into a virtual world. The challenge to computer graphics is to make that virtual world look real, sound real, move and respond to interaction in real time, and even feel real. This research program has driven the field ever since. What is VR? For better or worse, the label virtual reality stuck to this particular branch of computer graphics. I define a virtual reality experience as any in which the user is effectively immersed in a responsive virtual world. This implies user dynamic control of viewpoint. a lecture that asked, """" Is There Any Real Virtue in Virtual Reality? """" 2 My assessment then was that VR almost worked—that our discipline stood on Mount Pisgah looking into the Promised Land, but that we were not yet there. There were lots of demos and pilot systems, but except for vehicle simulators and entertainment applications, VR was not yet in production use doing real work. Net assessment—VR now barely works. This year I was invited to do an up-to-date assessment of VR, with funding to visit major centers in North America and Europe. Every one of the component technologies has made big strides. Moreover, I found that there now exist some VR applications routinely operated for the results they produce. As best I can determine, there were more than 10 and fewer than 100 such installations as of March 1999; this count again excludes vehicle simula-tors and entertainment applications. I think our technology has crossed over the pass—VR that used to almost work now barely works. VR is now really real. Why the exclusions? In the technology comparison between 1994 and 1999, I exclude vehicle simula-tors and entertainment VR applications for different reasons. Vehicle simulators were developed much earlier and independently of the VR vision. Although they today provide the best VR experiences available, that excellence did not arise from the development of VR technologies nor does it represent the state of VR in …"""	computer graphics;dreams;simula;simulation;virtual reality;virtual world	Frederick P. Brooks	1999		10.1109/VR.1999.756916	augmented reality;computer-mediated reality;artificial reality;vehicle dynamics;simulation;computer science;artificial intelligence;liquid-crystal display;virtual reality;mixed reality;multimedia;immersion;computer graphics (images)	Visualization	-55.844426222624065	-27.002254076191335	15100
8d7fb219621f94a7a9e2e2432124c0b2753d6220	what went wrong? - case histories of process plant disasters (2. ed.)				Trevor A. Kletz	1988				NLP	-57.45017872867542	-24.710504540988275	15105
cabcc02dafb581b404934597a61a40cc51038552	dark patterns in proxemic interactions: a critical perspective	anti patterns;proxemic interactions;qa75 electronic computers computer science;dark patterns;conference item;proceedings paper;technical report;information interfaces and presentation	Proxemics theory explains peoples' use of interpersonal distances to mediate their social interactions with others. Within Ubicomp, proxemic interaction researchers argue that people have a similar social understanding of their spatial relations with nearby digital devices, which can be exploited to better facilitate seamless and natural interactions. To do so, both people and devices are tracked to determine their spatial relationships. While interest in proxemic interactions has increased over the last few years, it also has a dark side: knowledge of proxemics may (and likely will) be easily exploited to the detriment of the user. In this paper, we offer a critical perspective on proxemic interactions in the form of dark patterns: ways proxemic interactions can be misused. We discuss a series of these patterns and describe how they apply to these types of interactions. In addition, we identify several root problems that underlie these patterns and discuss potential solutions that could lower their harmfulness.	dark side;interaction;seamless3d;ubiquitous computing	Saul Greenberg;Sebastian Boring;Jo Vermeulen;Jakub Dostal	2014		10.1145/2598510.2598541	simulation;human–computer interaction;computer science;engineering;artificial intelligence;technical report;anti-pattern	HCI	-58.35264327705896	-40.67133049471293	15106
3765306a513b224dcea26e37ee0204926b66469d	designing for usability: key principles and what designers think	design principle;design guideline;system design;iterative design;design thinking;system simulation	This article is both theoretical and empirical. Theoretically, it describes three principles of system design which we believe must be followed to produce a useful and easy to use computer system. These principles are: early and continual focus on users; empirical measurement of usage; and iterative design whereby the system (simulated, prototype, and real) is modified, tested, modified again, tested again, and the cycle is repeated again and again. This approach is contrasted to other principled design approaches, for example, get it right the first time, reliance on design guidelines. Empirically, the article presents data which show that our design principles are not always intuitive to designers; identifies the arguments which designers often offer for not using these principles—and answers them; and provides an example in which our principles have been used successfully.		John D. Gould;Clayton Lewis	1985	Commun. ACM	10.1145/3166.3170	iterative design;simulation;probabilistic design;strategic design;design thinking;computer science;design education;systems design	HCI	-60.577226101780006	-35.54148990827746	15134
43648a686db063eb249732931029f589ad5dc679	elastic graphical interfaces to precise data manipulation	graphical interface;anchored instruction;optimal solutions;intelligent learning environments;macrocontext microworlds;interaction technique;heuristic techniques;trip planning	We propose an interaction technique for manipulating precise data or selecting one element from a large number of items. Although conventional graphical interaction tools like sliders cannot be used for selecting more items than the pixel size of the slider, we can specify more precise data by using the elastic slider based on the rubber-band metaphor, where a control object can be moved by pulling the object with a rubber-band between the object and the mouse cursor. The same technique can be applied to many graphical interface tools like scroll bars and drawing editors.	cursor (databases);graphical user interface;interaction technique;pixel;pointer (user interface);scroll lock	Toshiyuki Masui;Kouichi Kashiwagi;IV R. Borden GeorgeR.Borden	1995		10.1145/223355.223471	simulation;human–computer interaction;computer science;theoretical computer science;graphical user interface;interaction technique	HCI	-43.17524817175359	-39.97497269356281	15143
7533aad7d3f2d7bdd242842e63d3a38c4e65012a	information graphics: a comprehensive illustrated reference, by robert l. harris			graphics;harris affine region detector;infographic	Robert D. Wilson	1998	JASIS	10.1002/(SICI)1097-4571(19980401)49:4%3C383::AID-ASI11%3E3.0.CO;2-V		Graphics	-47.24793456302363	-29.031711875562177	15152
f65c59bcd208abd5286c68f9d9cac8044a814397	storytelling for computer-animated shorts	point of view;computer animation;power modeling	This course examines the purposes and value of stories and storytelling as they relate to the classical story structure. The role of genres and cliches is examined as they relate to storytelling. The top five storytelling techniques are presented in detail, with many examples. The 10 principles of animation are revisited from a storytelling point of view. Classical story design and structure are presented as a simple and powerful model that travels across cultures and generations. And the five parts of a classical story are analyzed in detail. The course concludes with a summary of characterization and revelation of deep character. Short exercises complement the lectures and dialog.	computer animation;point of view (computer hardware company);dialog	Isaac Kerlow	2010		10.1145/1900520.1900535	simulation;computer science;artificial intelligence;mathematics;computer animation;multimedia;computer graphics (images)	ML	-57.52941657701975	-30.117472345926817	15154
f45f5374ab7d06c29e25702add9c27b628a35c6e	what you see is what you do: applying ecological interface design to visual analytics		In the SPEEDD project, we are developing approaches to the design and evaluation of Visual Analytics which are informed by Human Factors theories and methods. As part of this process, we are using the concept of Allocation of Function to inform the design of User Interfaces for Visual Analytics. The paper presents a case study of the development of a Road Traffic Management User Interface. ACM Classification	acm computing classification system;ecological interface design;human factors and ergonomics;theory;user interface;visual analytics	Natan Morar;Chris Baber;Peter Bak;Adam Duncan	2015			ecological interface design;visual analytics;human–computer interaction;user interface;computer science;analytics	HCI	-55.15552905362448	-33.566420690979214	15223
84c9b83a330cc65ec03aef34a6a1328bb172a803	suitable graphical user interface selection based on human errors using analytic hierarchy process	audio visual systems;analytic hierarchy process;user interface;prototypes;real time;ahp;remote controller;remote controller ahp gui human error;user centred design audio visual systems control engineering computing decision making graphical user interfaces operating systems computers telecontrol;graphical user interfaces;design method;evaluation criteria;telecontrol;gui;graphic user interface;real time os graphical user interface selection human errors analytic hierarchy process audio visual remote controllers wireless remote controller embedded microprocessor;audio visual;user centred design;control engineering computing;human error;operating systems computers	In this paper, we propose a new model for the design method of graphical user interfaces for audio visual remote controllers based on analytic hierarchy processes. The goal of this model is to reduce the human error in order to modify the graphical user interface of a wireless remote controller to obtain the most suitable interface for every user. This paper proposes a new model with the seven evaluation criteria; inherent ability, lack of skill, lack of knowledge, slip, lapse, mistake and violation. As alternatives, we decided on four design strategies for the user interface; vision assistance, cognition assistance, operation assistance, and memorizing. The proposed method is evaluated by a prototype assuming a real-time OS on an embedded microprocessor. Furthermore, we confirmed this proposal as effective.	analytical hierarchy;cognition;embedded system;graphical user interface;human error;microprocessor;prototype;real-time clock;real-time operating system;remote control	Sumie Yamada;Takako Nonaka;Tomohiro Hase	2010	2010 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2010.5641689	user interface design;embedded system;user;simulation;human–computer interaction;computer science;graphical user interface;user interface;graphical user interface testing	Robotics	-48.32496039532784	-48.03485140433794	15259
f82c3b220c73b859e1c3bfe6511e8364fd2c0699	within an endless sky		"""Sailing amidst the clouds, in a world where floating castles share the sky with creatures that soar on wings of gold, a young boy must decide his future: to follow the passion of his heart or the traditions of his people. The production of Within an Endless Sky merged Maya, Photoshop, and After Effects to expand the scope of traditional cinematography and CGI. With these tools, visual sleight-of-hand, and a crew of one---an individual filmmaker, even a self-taught student filmmaker---is now able to dramatically and affordably extend the scale and quality of visual storytelling.  Within an Endless Sky won the award for """"Best Animated Short"""" at the 2002 Newport Beach Film Festival."""	adobe photoshop;autodesk maya;castles;common gateway interface	Lance Winkel	2002		10.1145/2931127.2931307	simulation;computer science;artificial intelligence;computer graphics (images)	HCI	-54.416429951924954	-27.217633101131238	15287
652a2079b826d923a5a47e703aa5f3ba7a839120	capturing player enjoyment in computer games	game design;artificial intelligent;machine learning;computer game	The current state-of-the-art in intelligent game design using Artificial Intelligence (AI) techniques is mainly focused on generating human-like and intelligent characters. Even though complex opponent behaviors emerge through various machine learning techniques, there is generally no further analysis of whether these behaviors contribute to the satisfaction of the player . The implicit hypothesis motivating this research is that intelligent opponent behaviors enable the player to gain more satisfaction from the game. This hypothesis may well be true; however, since no notion of entertainment or enjoyment is explicitly defined, there is therefore no evidence that a specific opponent behavior generates enjoyable games. This chapter introduces a discussion of quantitative entertainment capture in real-time and presents two dissimilar approaches for modeling player satisfaction. Successfully capturing the level of entertainment during play provides insights for designing the appropriate AI methodology for entertainment augmentation in real-time. For this purpose, adaptive on-line learning methodologies are proposed and their strengths and limitations are discussed.	artificial intelligence;online and offline;online machine learning;opponent process;pc game;real-time locating system	Georgios N. Yannakakis;John Hallam	2007		10.1007/978-3-540-72705-7_8	video game design;game design;game development tool;simulation;level design;human–computer interaction;computer science;artificial intelligence;game mechanics;game art design;mathematical game;game developer;multimedia;game design document;video game development;game programming	AI	-55.72940399114522	-47.175590934010465	15369
e2db7ba1e744242b5bac413bd666094d1573c64c	facilitating hri by mixed reality techniques		Mobile robots start to appear in our everyday life, e.g., in shopping malls, airports, nursing homes or warehouses. Often, these robots are operated by non-technical staff with no prior experience/education in robotics. Additionally, as with all new technology, there is certain reservedness when it comes to accepting robots in our personal space. In this work, we propose making use of state-of-the-art Mixed Reality (MR) technology to facilitate acceptance and interaction with mobile robots. By integrating a Microsoft HoloLens into the robot»s operating space, the MR device can be used to a) visualize the robot»s behavior-state and sensor data, b) visually notify the user about planned/future behavior and possible problems/obstacles of the robot, and c) to actively use the device as an additional external sensor source. Moreover, by using the HoloLens, users can operate and interact with the robot without being close to it, as the robot is able to sense with the users» eyes.	emoticon;human–robot interaction;microsoft hololens;mixed reality;mobile robot;robotics	Patrick Renner;Florian Lier;Felix Friese;Thies Pfeiffer;Sven Wachsmuth	2018		10.1145/3173386.3177032	human–computer interaction;mixed reality;simulation;computer science;robot;personal space;sensor fusion;augmented reality;mobile robot;robotics;everyday life;artificial intelligence	Robotics	-43.30391685124027	-44.40909766640434	15387
b8be42a29801718c74ae2dcf39f2cf0812455bdd	spontaneous avatar behavior for human territoriality	social groups;virtual environment;social interaction;social behavior;group dynamic;virtual worlds;social norm	This paper presents a new approach for generating believable social behavior in avatars. The focus is on human territorial behaviors during social interactions, such as during conversations and gatherings. Driven by theories on human territoriality, we define a reactive framework which allows avatar group dynamics during social interaction. We model the territorial dynamics of social interactions as a set of social norms which constrain the avatar’s reactive motion by running a set of behaviors which blend together. The resulting social group behavior appears relatively robust, but perhaps more importantly, it starts to bring a new sense of relevance and continuity to virtual bodies that often get left behind when social situations are simulated. We carried out an evaluation of the technology and the result confirms the validity of our approach.	avatar (computing);interaction;middleware;norm (social);relevance;scott continuity;social presence theory;social simulation;spontaneous order;virtual reality	Claudio Pedica;Hannes Högni Vilhjálmsson	2010	Applied Artificial Intelligence	10.1080/08839514.2010.492165	social group;social relation;social dynamics;simulation;social behavior;virtual machine;artificial intelligence;group dynamics;norm	AI	-52.86391965392379	-48.7021177743298	15467
e20cce82d5106edd61c0e78d1f08925b16e712e2	aces: a cross-discipline platform and method for communication and language research	disabilities;aphasia;empathy;speech;emulation software;assistive technology;language;messaging	"""While conducting research focused on individuals with impairments is vitally important, such experiments often have high costs (time and money), and researchers may be limited in the instructions they can give, or participant feedback they can gather (due to the impairment). We present how an impairment emulation system (ACES) can be used by researchers in the behavioral sciences. By repurposing this new technology within the context of a """"traditional"""" psychology experiment, we were able to analyze impaired linguistic and communication in a manner that was not possible without a system such as ACES. Our experiment on 96 participants provided strong support for a theory in the aphasia psychology community, and uncovered new understandings of how people communicate when one interlocutor's speech is distorted with aphasia. These findings illustrate a new direction of HCI research that directly helps researchers in Psychology, Communication, and Speech and Hearing Science."""	emulator;experiment;human–computer interaction	Joshua M. Hailpern;Marina Danilevsky;Andrew Harris;Sunah Suh;Reed LaBotz;Karrie Karahalios	2013		10.1145/2441776.2441835	psychology;cognitive psychology;message;speech;multimedia;sociology;language;communication;social psychology	HCI	-55.33121058552292	-47.25267104146003	15478
2c74dd90cfc7ed0a8e4be19353af6bb93e4e3132	context aware voice user interfaces for workflow support	hands-free interaction;audio design pattern;audio channel;ubiquitous computing community;possible solution;workflow support;wearable device;humans cognitive capability;ubiquitous computing;context aware voice user;additional modality;graphical capability	Audio is a significant factor in the design of the human computer interface in ubiquitous computing. The characteristics of the medium allow for a hands-free interaction without the need to switch the focus to a display (eyes-free). Moreover, determining the graphical capabilities of the wearable device to adapt the output to the device is not needed. But audio based interfaces are also challenging, since humans are visually oriented. The ubiquitous computing community recognized the advantages of the audio channel, but the restrictions inherent to the medium are mostly ignored. Authors of such systems who know about these challenges often look for a solution by using additional modalities, preferably visually oriented. This thesis analyses these challenges with respect to the humans cognitive capabilities and shows a possible solution using audio design patterns.		Dirk Schnelle	2007			human–computer interaction;computer science;multimedia;communication	HCI	-47.87492405203245	-40.27864109041614	15500
a95f012451ed2dafb9e923071f431e3fbbeb1f03	kinect unleashed: getting control over high resolution depth maps		The wide availability and economic price of Kinect popularized the concept of RGB-D cameras, which are increasingly used in a wide range of applications ranging from Human Computer Interfaces to Structural Analysis, Health Care and even Art. With over 20 million units sold, it is by far the most popular depth camera, although, being closed source, its internals are not entirely known. In this paper we focus on the PS1080 chip that executes the depth extraction algorithm inside Kinect and related devices. We propose an algorithm that can estimate the output of Kinect from the raw infrared camera data. Using this algorithm, the internal reference calibration image from Kinect can be obtained. This image allows us to model Kinect as a standard stereo camera, where alternative stereo algorithms can be used increasing the versatility of Kinect.	algorithm;depth map;dots per inch;human computer;kinect;principle of good enough;stereo camera;structural analysis;the sims	Manuel Martinez;Rainer Stiefelhagen	2013			computer vision;simulation;computer graphics (images)	Vision	-39.350654113107126	-40.463097044044524	15507
147d95574acb0047d1b22b18f056c30a599d171a	push-edge and slide-edge: scrolling by pushing against the viewport edge	rate control;autoscroll;edge scroll;position control;inertia	"""Edge-scrolling allows users to scroll a viewport while simultaneously dragging near or beyond a window's edge. Common implementations rely on rate control, mapping the distance between the pointer and the edge of the viewport to the scrolling velocity. While ubiquitous in operating systems, edge-scrolling has received little attention, even though previous works suggest that (1) rate control may be suboptimal for isotonic pointing devices like mice and trackpads and (2) space beyond the window's edge might be scarce, limiting scrolling control. To address these problems, we developed Push-edge scrolling (and Slide-edge scrolling, its inertial variant), two novel position-based techniques that allow scrolling by """"pushing"""" against the viewport edge. A controlled experiment shows that our techniques reduce overshoots and offer performance improvements by up to 13% over traditional edge-scrolling."""	drag and drop;isotonic regression;operating system;pointer (computer programming);scrolling;velocity (software development);viewport	Sylvain Malacria;Jonathan Aceituno;Philip Quinn;Géry Casiez;Andy Cockburn;Nicolas Roussel	2015		10.1145/2702123.2702132	embedded system;inertia;simulation;computer science;computer graphics (images)	HCI	-44.98500013003444	-46.10323696972832	15553
54ffbb062c68909f40051ccd7980940916e20069	user system interaction standards	standards;user system interaction standard;user system interaction;user interfaces	Within the past few years, several standards organisations have started work aimed at producing standards for user system interfaces. This activity has been motivated by a desire for more usable interfaces and many believe that such standards are long overdue. Others however, believe that quite the opposite is true that it is far too early think about developing standards in an area we know so little about. This article examines both sides of the issue, and reviews the work of several of the major standards organisations, describing their mission, scope and status. Finally an attempt is made at some projections for where all these activities will lead. No international standards have yet been adopted, but a great deal of effort by many dedicated people continues.		Ken Holdaway;Nigel Bevan	1989	Computer Communications	10.1016/0140-3664(89)90064-9	simulation	HCI	-61.24503453006649	-29.522344664411413	15563
b7cf957d1594f38209095409e52244b2a4e5b354	socializing virtual worlds with facebook: a prototypical implementation of an expansion pack to communicate between facebook and opensimulator based virtual worlds	social network;facebook;profitability;opensimulator;social media;secondlife;virtual worlds	In this paper we present a prototypical implementation of a novel expansion pack called OMFacebook for OpenSimulator based virtual worlds. The idea is to provide the user with a light-weight platform-independent Web application to communicate with friends from the virtual world and therefore profit from the success of Facebook as a social network platform. This add-on is to the best of our knowledge the first approach to allow the user of an OpenSimulator based virtual world to connect an in-world avatar with an existing Facebook profile.	add-ons for firefox;expansion pack;opensimulator;social network;socialization;virtual world;web application	Christoph Trattner;Michael Steurer;Frank Kappe	2010		10.1145/1930488.1930522	engineering;multimedia;internet privacy;world wide web	Web+IR	-55.340919675470694	-39.857021505208664	15600
dcb71a8f3f7e3caa905ab9568973043f3a385aee	detecting transitions in manual tasks from wearables: an unsupervised labeling approach		Authoring protocols for manual tasks such as following recipes, manufacturing processes or laboratory experiments requires significant effort. This paper presents a system that estimates individual procedure transitions from the user’s physical movement and gestures recorded with inertial motion sensors. Combined with egocentric or external video recordings, this facilitates efficient review and annotation of video databases. We investigate different clustering algorithms on wearable inertial sensor data recorded on par with video data, to automatically create transition marks between task steps. The goal is to match these marks to the transitions given in a description of the workflow, thus creating navigation cues to browse video repositories of manual work. To evaluate the performance of unsupervised algorithms, the automatically-generated marks are compared to human expert-created labels on two publicly-available datasets. Additionally, we tested the approach on a novel dataset in a manufacturing lab environment, describing an existing sequential manufacturing process. The results from selected clustering methods are also compared to some supervised methods.	algorithm;algorithmic efficiency;browsing;cluster analysis;computation;database;documentation;experiment;mathematical optimization;sensor;software prototyping;supervised learning;usability testing;wearable computer	Sebastian Böttcher;Philipp Scholl;Kristof Van Laerhoven	2018	Informatics	10.3390/informatics5020016	computer vision;wearable computer;cluster analysis;gesture;workflow;annotation;artificial intelligence;computer science	AI	-34.977296446354316	-43.7136610889978	15613
7742a6ff2153948f1c178144aebd074995547796	solid modeling with hardware (panel session)	approximation;solid modeling;stochastic modelling	The extremely rapid growth of computer graphics is due, in large measure, to the application of leading-edge technology devices. Initially, these devices were not inspired by graphics applications. However, the market has become so large that display oriented semi-conductor devices are becoming specialty product lines. In turn these devices are leading to different display hardware architectures which will profoundly influence emerging capabilities in computer graphics. The future will be shaped as much by these technical innovations as by the economic trends they engender. The panel will examine the forces behind these technological changes and estimate their influence on directions for the computer graphics industry.	computer graphics;semiconductor industry;solid modeling	Pierre J. Malraison;Gershon Kedem;Greg Lee;Donald Meagher	1985		10.1145/325334.325262	mathematical optimization;real-time computing;computer science;stochastic modelling;theoretical computer science;approximation;solid modeling	Graphics	-48.18136544707654	-29.15416332292152	15621
959cf95d99fc51f8af693209501906f3310015aa	comparison between the astronaut and standard operators as telemetry monitor for effects of audio feedback system in the operation of the engineering test satellite vii	engineering;feedback;satellites	It is the most important that we perform safe and reliable teleoperation of space robots in the experiment/operation using real satellite. Therefore we have developed and used Audio Feedback System (AFS) that use some auditory information such as motorized sound and prerecorded voice converted from a part of telemetry information to reduce loads to operators of space robots of the Engineering Test Satellite VII(ETS-VII). We applied AFS to the experiment ofAntenna Assembling Mechanism (AAM) which is experiment module of the Communications Research Laboratory (CRL) on the ETS-Vil for testing the mechanism of assembling structures in space. Our purpose of this study is to assess effectiveness of AFS by using the Eye Mark Recorder (EMR) which records eye movements for the astronaut and two operators as telemetry monitor who observe telemetry data. In results of macro shape of 0-500 (deg/sec) velocity distribution graph, average fixation time, and average velocity of eye movement, we have got important points as followings: ( 1) the spectrum patterns of velocity of eye movement of specialist operator of controlling space robots like an astronaut are significantly differed from other two operators, (2) the spectrum patterns of velocity of eye movement of specialist operator and nonexpert operator are not affected ofAFS, (3) the spectrum patterns of velocity of eye movement of well-trained expert operator was similar to specialist operator with AFS and similar to nonexpert operators without AFS, (4) average fixation time of specialist operator was not effected by AFS, average fixation time of well-trained expert operator, nonexpert operator became long without AFS support.	audio feedback;vii	Yasufumi Nagai;Shinichi Kimura;Shigeru Tsuchiya	2000		10.1117/12.417311	simulation;telecommunications;feedback;physics;satellite	HCI	-45.10448140274413	-51.88409800026845	15676
146b94b5014e6859a48cb9a6b3c8767fcfd3577b	a led-based ir/rgb end-to-end latency measurement device	standards;publikationer;clocks;light emitting diodes;konferensbidrag;reflective binary codes;artiklar;rapporter;cameras;hardware	Achieving a minimal latency within augmented reality (AR) systems is one of the most important factors to achieve a convincing visual impression. It is even more crucial for non-video augmentations such as dynamic projection mappings because in that case the superimposed imagery has to exactly match the dynamic real surface, which obviously cannot be directly influenced or delayed in its movement. In those cases, the inevitable latency is usually compensated for using prediction and extrapolation operations, which require accurate information about the occurring overall latency to exactly predict to the right time frame for the augmentation. Different strategies have been applied to accurately compute this latency. Since some of these AR systems operate within different spectral bands for input and output, it is not possible to apply latency measurement methods encoding time stamps directly into the presented output images as these might not be sensed by used input device.We present a generic latency measurement device which can be used to accurately measure the overall end-to-end latency of camera-based AR systems with an accuracy below one millisecond. It comprises a LED-based time stamp generator displaying the time as a gray code on spatially and spectrally multiple locations. It is controlled by a micro-controller and sensed by an external camera device observing the output display as well as the LED device at the same time.	augmented reality;basic stamp;end-to-end principle;extrapolation;input device;input/output;interrupt latency;microcontroller;oled;output device;overhead (computing);pixel;projection-slice theorem;requirement;stellar classification;video projector	Markus Billeter;Gerhard Rothlin;Jan Wezel;Daisuke Iwai;Anselm Grundhöfer	2016	2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)	10.1109/ISMAR-Adjunct.2016.0072	computer vision;latency;real-time computing;simulation;computer hardware;computer science;operating system;light-emitting diode	Visualization	-39.47187708236031	-41.70887610963154	15686
1caebab7a26b493d5f14db21b566e9f0fe8869fe	virtual space and place: theory and test	place;cognition;focused immersion;familiarity;second life;presence;perception;virtual space;social presence;virtual worlds	Participants Avatar Animated characters that are graphical representation of people (Davis, et al. 2009) Animated characters Avatar Virtual world(s) (VW(s)) An electronic environment that visually mimics complex physical spaces, where people can interact with each other and with virtual objects, and where people are represented by animated characters (Bainbridge 2007, p. 472) Digital world, social virtual, world, virtual environment, apparent three dimensional environment Second Life (SL) Virtual Object Object designed using apparent three dimensionality displayed in VWs IT tools, Interactive work tools, virtual tools, VW tools Tools namely, brain-storming tool, idea organizer, voting floor MIS Quarterly Vol. 35 No. 4—Appendices/December 2011 A1	electronic organizer;graphical user interface;sl (complexity);second life;virtual reality;virtual world	Carol Stoak Saunders;Anne-Françoise Rutkowski;Michiel van Genuchten;Douglas R. Vogel;Julio Molina Orrego	2011	MIS Quarterly		simulation;cognition;computer science;instructional simulation;location;communication;perception;social psychology	HCI	-48.97836479221491	-34.37190288387454	15694
189069e51067e56c4adc83fb51a2ad429013d908	smartphone banking: the factors influencing the intention to use			smartphone	Jinbaek Kim;Sungmin Kang;Hoon S. Cha	2013	TIIS	10.3837/tiis.2013.05.016	human–computer interaction;computer science;distributed computing	HCI	-53.91756918324034	-33.61674877909087	15810
eff252a2dc71a122156f77577f5c69c6c00678d9	voice recognition software on embedded devices		This paper deals with an area of voice recognition and its usage for controlling embedded devices and external components. Multiple voice recognition solutions are available right now but many of them are not the best solution for use in performance constrained devices, such as Raspberry Pi. The current state of the art in the field of voice recognition software is covered and appropriate candidates are selected. Tools are tested in terms of quality of evaluation of different commands in areas with various levels of noise. Time requirements for processing are also discussed.	embedded system;speech recognition	Pavel Vojtas;Jan Stepan;David Sec;Richard Cimler;Ondrej Krejcar	2018		10.1007/978-3-319-75417-8_60	embedded system;voice recognition software;computer science	EDA	-38.373288671310895	-45.22712723328219	15822
e97eb4d4f81cb1cbaa250580af76b7f411e6684e	toward a digital ecosystem: international symposium on ubiquitous virtual reality 2010	ecosystems augmented reality three dimensional displays biological system modeling solid modeling virtual reality;pervasive computing;biological system modeling;virtual reality;three dimensional;ubiquitous computing conferences virtual reality augmented reality;ecosystems;three dimensional displays;solid modeling;ubiquitous computing;augmented reality;article;conferences	"""The International Symposium on Ubiquitous Virtual Reality (ISUVR) series continues to provide an interdisciplinary forum for leading researchers and graduate students, especially in the areas of ubiquitous computing, virtual reality, and augmented reality. The 2010 symposium's theme, """"Toward a Digital Ecosystem,"""" emphasized the roles of end users in actively creating and maintaining content in a managed systematic way, referred to as the digital ecosystem."""	digital ecosystem;virtual reality	Hyoseok Yoon;Anton van den Hengel;Gerhard Reitmayr;Joshua Lifton;Sung-Hee Lee;Woontack Woo	2011	IEEE Pervasive Computing	10.1109/MPRV.2011.35	three-dimensional space;ecosystem;simulation;human–computer interaction;computer science;metaverse;mixed reality;multimedia;solid modeling;ubiquitous computing	Arch	-52.019422126920595	-31.04595944721693	15854
af1893d61151e5e5ea5bcb538cd48060625eedb1	graphics for social scientists	computer graphic;data analysis;social science;software development	The social science computing environment and the implications of this for the developers of computer graphics is described. Graphic software developed for social scientists at the University of Michigan is illustrated. MULTIGROUP is used to generate displays in the process of data analysis. GRAPH is used to report results to others. A graphical editor is employed to customize displays.	computer graphics;graphical user interface;graphics software	Edward J. Schneider;Sylvia Barge;Gregory A. Marks	1976		10.1145/563274.563298	human–computer interaction;computer science;data science;software development;graphics software;data analysis;computer graphics (images)	HCI	-47.834905989926966	-28.989677314339446	15887
63bbe16f8704a6bc92eceff27f3b7f3ad1a1b966	towards understanding of play with augmented toys	play;play pyramid;sparkubes;tangible interaction	This work is directed towards understanding how the transformation of a regular object/traditional toy into an augmented toy may affect the dynamics of play behavior. We present an observational user study with 8 children from kindergarten to understand the play value of SparKubes. SparKubes are stand-alone tangible objects that accept light from one direction and pass it on in another direction. We found that children who were aware of the SparKubes' interactivity features displayed more variety of patterns and showed greater interaction with SparKubes as compared to the control group who were not aware of the features. The play behaviour revealed that SparKubes have constructive play value on the play pyramid and that adding light features changed the patterns of constructions by children. This knowledge opens up an exciting area of research in technology-mediated play and designing augmented toys for children.	interactivity;toys;usability testing	Priyashri Kamlesh Sridhar;Suranga Nanayakkara	2017		10.1145/3041164.3041191	simulation;engineering;multimedia;communication	HCI	-58.2347122232713	-35.950852892919706	15891
ba5a6cfb8b7df2a9985f347143ef3144f9a7df73	dynamic generation, management and resolution of interactive plots	ユビキタス情報環境;art and entertainment;見守り支援システム;user participation;computer model;マルチエージェントシステム;共生コンピューティング;利用者指向	Abstract   Rapid advances in entertainment technology necessitate the development of computational models for interactive plots capable of creating engaging stories that are meaningfully interactive. This work describes a computational framework for supporting dynamic generation, management and resolution of interactive plots. In this framework, the user takes the place of the story protagonist. The rest of the cast consists of discrete computer characters, each playing specific roles in the story. The plot is dynamically shaped by the interaction between the user and the rest of the cast. This framework supports an Aristotelian plot conception, in which a conflict between antagonistic forces develops out of an initial situation. The plot moves from this initial situation towards its antagonistic climax, through a sequence of conflicts, and then towards an unambiguous solution at the end. This paper describes dynamic techniques that analyze the evolving plot to support user participation, adopt dramatically interesting story developments and resolve the plot in engaging ways based on the motives of the characters involved. This framework has been implemented as part of DEFACTO, a research project for designing interactive story systems.		Nikitas M. Sgouros	1999	Artif. Intell.	10.1016/S0004-3702(98)00106-4	computer simulation;simulation;human–computer interaction;computer science;artificial intelligence;multimedia	AI	-47.64242164358025	-35.05674503510618	15957
d0491bee223d8bfabd7f3cde54aa114d0a22b397	a comparison of system-controlled and user-controlled personalization approaches		Personalizing interactive systems including games increases their effectiveness. This paper explores and compares two main approaches to personalization: system-controlled and user-controlled adaptation. The results of large-scale exploratory studies of 1768 users show that both techniques to personalizing systems share seven common strengths of increasing usersu0027 perception of a systemu0027s relevance, usefulness, interactivity, ease of use, credibility and trust, and also increases usersu0027 self-efficacy. The results also reveal some unique strengths and weaknesses peculiar to each of the approaches that designers should take into consideration when deciding on a suitable adaptation technique to use in personalizing their systems. Users prefer system- over user-controlled adaptation.	interactivity;personalization;relevance;usability	Rita Orji;Kiemute Oyibo;Gustavo Fortes Tondello	2017		10.1145/3099023.3099116	user control;personalization;interactivity;credibility;usability;human–computer interaction;strengths and weaknesses;knowledge management;persuasive technology;computer science	HCI	-59.716813783290256	-46.153759974560096	15975
a5d8387eadbd2ed9e90988832e2b6b5ef94521f4	evaluating the effectiveness of physical shape-change for in-pocket mobile device notifications	shape change;notifications;mobile devices	Audio and vibrotactile output are the standard mechanisms mobile devices use to attract their owner's attention. Yet in busy and noisy environments, or when the user is physically active, these channels sometimes fail. Recent work has explored the use of physical shape-change as an additional method for conveying notifications when the device is in-hand or viewable. However, we do not yet understand the effectiveness of physical shape-change as a method for communicating in-pocket notifications. This paper presents three robustly implemented, mobile-device sized shape-changing devices, and two user studies to evaluate their effectiveness at conveying notifications. The studies reveal that (1) different types and configurations of shape-change convey different levels of urgency and; (2) fast pulsing shape-changing notifications are missed less often and recognised more quickly than the standard slower vibration pulse rates of a mobile device.	mobile device;notification system	Panteleimon Dimitriadis;Jason Alexander	2014		10.1145/2556288.2557164	embedded system;simulation;computer science;operating system;mobile device;multimedia	HCI	-46.02283020412678	-43.85722337220715	15976
43de7cf628ed9319342eb79b70af47ffeca7db74	a guided performance interface for augmenting social experiences with an interactive animatronic character	social experiment;emerging technology	Entertainment animatronics has traditionally been a discipline devoid of interactivity. Previously, we brought interactivity to this field by creating a suite of content authoring tools that allowed entertainment artists to easily develop fully autonomous believable experiences with an animatronic character. The recent development of a Guided Performance Interface (GPI) has allowed us to explore the advantages of nonautonomous control. Our new hybrid approach utilizes an autonomous AI system to control low-level behaviors and idle movements, which are augmented by high-level processes (such as complex conversation) issued by a human operator through the GPI. After observing thousands of interactions between human guests and our animatronic character at SIGGRAPH 2005’s Emerging Technologies Exhibition, we strongly feel that both autonomy and guided performance have important roles in interactive, entertainment robotics. Together, the autonomous system and the new Guided Performance Interface allow guests to experience extremely rich, believable, social experiences with robots using technology	animatronics;artificial intelligence;autonomous robot;autonomous system (internet);experience;foundation 9 entertainment;general-purpose input/output;high- and low-level;interaction;interactivity;robotics;siggraph	Seema Patel;William Bosley;David Culyba;Sabrina A. Haskell;Andrew Hosmer;T. J. Jackson;Shane J. M. Liesegang;Peter Stepniewicz;James Valenti;Salim Zayat;Brenda Harger	2006			robot;exhibition;entertainment;interactivity;computer science;conversation;animatronics;emerging technologies;human–computer interaction;artificial intelligence;multimedia;robotics	HCI	-37.91920512931184	-38.42642707636763	15981
5c0591d8abbffa79bbdb5707c7abdbbf50a5077c	automated webpage evaluation	interfaces;web content analysis;interface design;web page analysis;user experience;web measurement	Webpage evaluation and metrics have historically focused on page-level characteristics or on key words. We introduce an automated technique for graphically measuring specific elements on a webpage. Our technique provides a means to increase the fidelity of webpage analysis and introduces a novel metric focused on the number of pixels that certain elements occupy in a browser window. We implemented the technique as a Firefox extension and successfully tested it on Alexa?s top 25 U.S. websites. The technique is fully automatable and consistently measures a customizable set of elements as they appear to users in the Firefox web browser. Importantly, the application allows for communication with and the incorporation of other browser-based tools or extensions. We discuss design considerations and creative solutions to technical implementation challenges. The application provides for a wide range of research opportunities that may require a new level of fidelity in webpage analysis and comparison.	adblock;browser extension;context tree weighting;firefox;holism;image resolution;lambda lifting;pixel;test automation;web application;web page;web search engine	Ryan Tate;Gregory J. Conti;Edward Sobiesk	2013		10.1145/2512209.2512220	human–computer interaction;computer science;multimedia;world wide web	Security	-35.71545874147405	-48.36699402038911	15986
08ccf462a3d0a4968c5b73fe188230bbcc829100	an embodied music cognition approach to multilevel interactive sonification	embodiment;sonification;electroacoustic;interaction;performing arts;framework;ipemapplication	In this paper, a new conceptual framework and related implementation for interactive sonification is introduced. The conceptual framework consists of a combination of three components, namely, gestalt-based electroacoustic composition techniques (sound), user and body-centered spatial exploration (body), and corporeal mediation technology (tools), which are brought together within an existing paradigm of embodied music cognition. The implementation of the conceptual framework is based on an iterative process that involves the development of several use cases. Through this methodology, it is possible to investigate new approaches for structuring and to interactively explore multivariable data through sound.	cognition;conceptual schema;gestalt psychology;interactivity;iterative method;programming paradigm;sonification	Nuno Diniz;Pieter Coussement;Alexander Deweppe;Michiel Demey;Marc Leman	2011	Journal on Multimodal User Interfaces	10.1007/s12193-011-0084-2	interaction;sonification;human–computer interaction;computer science;software framework;performing arts;multimedia;communication	HCI	-52.670218380770585	-36.706115608342316	15989
941bc86a6c1bb86ddefddc459800bcae183b29bc	review: david cope: virtual music	david cope;virtual music	david cope;virtual music		Michael Theodore	2002	Computer Music Journal	10.1162/comj.2002.26.4.92	humanities;art;aesthetics;performance art	ML	-53.346004177687306	-28.658451309106525	16004
b087a56d449fdbc633b97b7421db9afcccbc0f7a	evaluation of a manipulator control interface that employs touch gestures and voice shortcuts	touch gesture;voice;interface;evaluation;manipulator	In this study, we evaluated a multimodal manipulator control interface that employs touch gestures and voice shortcuts. This interface allows a human operator to translate, rotate, open and close the gripper of a seven-degree-of-freedom robotic manipulator by using touch gestures on a touch panel. The manipulator speeds up as the number of contact points on the panel increases. The evaluated system employed voice shortcuts to a home position and five well-used gripper orientations. Twenty four novice users completed two different tasks without frequent errors and most of them highly evaluated the interface in learnability and usability points of view. In addition, they completed the first pick-and-place task with significantly fewer commands than an existing three-mode control interface.	learnability;multimodal interaction;robot end effector;smt placement equipment;touchscreen;usability	Tetsushi Oka;Kosuke Ichikawa	2017		10.1145/3029798.3038323	embedded system;simulation;computer science;evaluation;manipulator;interface;voice	Robotics	-44.118815938492666	-44.66220094712304	16059
9dc668482e2e7b73d1b9c4ce7a91d213ef91bfde	an augmented reality weather system	multimodal weather simulation;user study;wearable computer;augmented reality;mobile augmented reality	This paper presents ARWeather, a simulation application, which can simulate three types of precipitation: rain, snow, and hail. We examined a range of weather phenomenon and how they may be simulated in a mobile augmented reality system. ARWeather was developed and deployed on the Tinmith wearable computer system to enable autonomous and free movement for the user. The user can move freely inside the simulated weather without limitation. The result of this work, the ARWeather application, has been evaluated with a user study to determine the user's acceptance and draw conclusions to the applicability of augmented reality simulated weather.	augmented reality;autonomous robot;simulation;usability testing;wearable computer	Marko Heinrich;Bruce H. Thomas;Stefan Müller;Christian Sandor	2008		10.1145/1501750.1501790	augmented reality;computer-mediated reality;simulation;human–computer interaction;engineering;computer graphics (images)	HCI	-37.630436519624304	-35.789800325940426	16081
82cec294ef1747eb1169119aafc93b5ddf99f22e	mobile ad hoc collaboration	manet;spontaneous collaboration;ad hoc network;pan;qa75 electronic computers computer science;ad hoc networks;cscw;mobile collaboration	INTRODUCTION The combination of personal mobile devices and mobile adhoc networks creates opportunities for new forms of mobile collaboration involving interaction between people who are co-located and organized in an unforeseeable ad hoc way. Possible application scenarios include informal social interactions in public places, opportunistic meetings in office settings, ad-hoc collaboration of emergency response teams and educational multi-user applications for use in classrooms.	hoc (programming language);interaction;mobile device;multi-user	Gerd Kortuem;Hans-Werner Gellersen;Mark Billinghurst	2002		10.1145/506443.506665	vehicular ad hoc network;wireless ad hoc network;adaptive quality of service multi-hop routing;computer science;delay-tolerant networking;world wide web;computer security;computer network	HCI	-59.53942094178562	-40.450836329233525	16145
db57aecb6e1295cf2598c46a65de3d4e8f22d643	a study of cursor trajectories of motion-impaired users	assistive interfaces;cursor trajectories;interface design;endnotes;motion impaired;spatial distribution;pubications	"""This paper describes a study of the cursor trajectories of motion-impaired users in """"point and click"""" interactions. A characteristic of cursor movement is proposed that aims to capture the spatial distribution of cursor movement about a target. This characteristic indicates that users often exhibit increased cursor movement in the vicinity of the target, have more difficulty performing the """"clicking"""" part of the interaction as compared to the navigation part, and tend to navigate directly toward the target during the middle portion of the cursor trajectory. The implications of these characteristic behaviours on interface design are discussed."""	cursor (databases);interaction;point and click	Faustina Hwang	2002		10.1145/506443.506626	footmouse;computer vision;pointer;human–computer interaction;computer hardware;computer science;interface design	HCI	-46.088917574473534	-46.942197723402415	16188
66354fd5586ce17859526207069383c4dd441da1	ddmixer2.5d: drag and drop to mix 2.5d video objects	video editing;2 5d	We propose a 2.5D video editing system called DDMixer2.5D. 2.5D video contains not only color channels but also a depth channel, which can be recorded easily using recently available depth sensors, such as Microsoft Kinect. Our system employs this depth channel to allow a user to quickly and easily edit video objects by using simple drag-and-drop gestures. For example, a user can copy a video object of a dancing figure from video to video simply by dragging and dropping using finger on the touch screen of a mobile phone handset. In addition, the user can drag to adjust the 3D position in the new video so that contact between foot and floor is preserved and the size of the body is automatically adjusted according to the depth. DDMixer2.5D has other useful functions required for practical use, including object removal, editing 3D camera path, creating of anaglyph 3D video, as well as a timeline interface.	2.5d;anaglyph 3d;channel (digital image);clone tool;drag and drop;kinect;mobile phone;sensor;timeline;touchscreen	Tatsuya Kurihara;Makoto Okabe;Rikio Onai	2013		10.1145/2508468.2514714	video compression picture types;microsoft video 1;computer vision;uncompressed video;computer science;video capture;video tracking;multimedia;video processing;smacker video;motion compensation;video post-processing;computer graphics (images)	HCI	-42.726314218318095	-40.17631846340626	16245
1a28c21bfae7c74cc8efb10aa2a674d3570878bb	a vision-based path planner/follower for an assistive robotics project	mobile robot;visual landmarks;assistive technology	Assistive technology is an emerging area where robots can be used to help individuals with motor disabilities achieve independence in daily living activities. Mobile robots should be able to autonomously and safely move in the environment (e.g. the user apartment), by accurately solving the self-localization problem and planning efficient paths to the target destination specified by the user. This paper presents a vision-based navigation scheme designed for Sony AIBO, in ASPICE, an assistive robotics project. The navigation scheme is map-based: visual landmarks (white lines and coded squares) are placed in the environment, and the robot utilizes visual data to follow the paths composed by these landmarks, and travel to the required destinations. Performance of this vision-based scheme is shown by experiments and comparison with two previously existing ASPICE navigation modes. Finally, the system is clinically validated, in order to obtain a definitive assessment through patient feedback.	aibo;assistive technology;experiment;mobile robot;robotics	Andrea Cherubini;Giuseppe Oriolo;Francesco Macrì;Fabio Aloise;Febo Cincotti;Donatella Mattia	2007			mobile robot;computer vision;simulation;computer science;artificial intelligence;mobile robot navigation	Robotics	-43.305688542004596	-45.99901409344292	16265
7f43df706eb722d933385152dc86f1603f9c1ad9	thematic design and efficiency in virtual environments using metaphors	agent behaviors thematic design virtual environments metaphors relational perception virtual reality perception semantic based construction processes;construction process;virtual reality;computational linguistics virtual reality software engineering software agents;software engineering;navigation virtual environment joining processes virtual reality delay cognition computer architecture software libraries software agents legged locomotion;software agents;user experience;computational linguistics;virtual environment	The use of metaphors in designing virtual environments unifies design elements and improves efficiency and interaction. Metaphors informing VE design relate visual and sonic cues and structural elements using a connecting theme. Thematic design based on metaphor capitalises on iconic familiarity and existing user experience. The design metaphor affects relational and virtual reality perception of directedness, enclosure, engagement, navigation, and presence. The design metaphor relates semantic-based construction processes and agent behaviours.	user experience;virtual reality	Kirsty A. Beilharz;Rabee M. Reffat	2004	10th International Multimedia Modelling Conference, 2004. Proceedings.	10.1109/MULMM.2004.1265018	computer vision;simulation;human–computer interaction;computer science;virtual machine;artificial intelligence;instructional simulation;computational linguistics;software agent;virtual reality;multimedia	Visualization	-54.814978228512814	-44.81838798163242	16269
29fb6ba8c4937fbba74aad9d4bbbcc2c23b27e49	enriching buyers' experiences: the smartclient approach	ecommerce;electronic commerce;human computer interaction;client server architecture;user study;on line travel planning systems;satisfiability;online systems;interactive computer systems;visual overview;constraint solver;client server computer systems	In electronic commerce, a satisfying buyer experience is a key competitive element. We show new techniques for better adapting interaction with an electronic catalog system to actual buying behavior. Our model replaces the sequential separation of needs identification and product brokering with a conversation in which both processes occur simultaneously. This conversation supports the buyer in formulating his or her needs, and in deciding which criteria to apply in selecting a product to buy. We have experimented with this approach in the area of travel planning and developed a system called SmartClient Travel which supports this process. It includes tools for need identification, visualization of alternatives, and choosing the most suitable one. We describe the system and its implementation, and report on user studies showing its advantages for electronic catalogs.	e-commerce;smartclient;usability testing	Pearl Pu;Boi Faltings	2000		10.1145/332040.332446	e-commerce;simulation;human–computer interaction;computer science;operating system;world wide web;constraint satisfaction problem;client–server model;satisfiability	HCI	-52.730428685493884	-41.12179310465419	16308
1358d926d6e40a935a77270091d647fc52062590	sensing the fabric: to simulate sensation through sensory evaluation and in response to standard acceptable properties of specific materials when viewed as a digital image	force feedback;sensory evaluation;digital image;evaluation studies	This paper describes initial investigation of ideas for developing and refining current haptic parameters and interfaces for use in the textiles and related industries. A simple force-feedback mouse has been programmed to represent some of the more obvious tactile issues in fabrics. An evaluation study has been made of five different fabrics, and numerical values have been assessed for tactile parameters according to a new set of semi-quantitative descriptors. The results are discussed, and are displayed as a demonstration.	digital image;haptic technology;numerical analysis;semiconductor industry	Patricia Dillon;Wendy Moody;Rebecca Bartlett;Patricia Scully;Roger Morgan;Christopher James	2000		10.1007/3-540-44589-7_23	simulation;engineering;multimedia;engineering drawing	HCI	-44.72464933014896	-47.97953912884837	16321
178f896634e3d6f0dff4dfa26cefb108219539a0	balancing fusion, image depth and distortion in stereoscopic head-tracked displays	image distortion;head tracking;virtual reality;stereoscopic display	Stereoscopic display is a fundamental part of virtual reality HMD systems and HTD (head-tracked display) systems such as the virtual workbench and the CAVE. A common practice in stereoscopic systems is deliberate incorrect modeling of user eye separation. Underestimating eye separation is frequently necessary for the human visual system to fuse stereo image pairs into single 3D images, while overestimating eye separation enhances image depth. Unfortunately, false eye separation modeling also distorts the perceived 3D image in undesirable ways. This paper makes three fundamental contributions to understanding and controlling this stereo distortion. (1) We analyze the distortion using a new analytic description. This analysis shows that even with perfect head tracking, a user will perceive virtual objects to warp and shift as she moves her head. (2) We present a new technique for counteracting the shearing component of the distortion. (3) We present improved methods for managing image fusion problems for distant objects and for enhancing the depth of flat scenes. CR Categories and Subject Descriptions: I.3.7 [Computer Graphics] Three-Dimensional Graphics and Realism Virtual Reality; I.3.6 [Computer Graphics] Methodology and Techniques – Ergonomics; I.3.6 [Computer Graphics] Methodology and Techniques – Interaction Techniques; I.3.3 [Computer Graphics] Picture/Image Generation – Viewing Algorithms Additional	algorithm;color depth;computer graphics;distortion;head-mounted display;human factors and ergonomics;image fusion;interaction technique;motion capture;stereoscopy;virtual reality;workbench	Zachary Wartell;Larry F. Hodges;William Ribarsky	1999		10.1145/311535.311587	computer vision;computer science;artificial intelligence;virtual reality;computer graphics (images)	Graphics	-43.4061128956696	-48.150865074647626	16331
cee241fda524a488104779ec6b22bab4f3122493	colorplate: interactive and continuous collision detection for avatars in virtual environments	virtual environments;continuous collision detection;virtual environment		avatar (computing);collision detection;interactivity	Stéphane Redon;Young J. Kim;Ming C. Lin;Dinesh Manocha;Jim Templeman	2004		10.1109/VR.2004.10033	computer science;virtual machine;operating system	Visualization	-42.87848796466481	-35.2900206586811	16353
917a0fe651e0553942ac9caf1e01e006bb7b0c50	speech recognition technology in the ubiquitous/wearable computing environment	human computer interaction;system adaptability speech recognition technology ubiquitous computing environment wearable computing environment human computer interaction microphone networked computer environmental information noise task dependent information personalized recognizers;speech based user interfaces;portable computers;speech recognition wearable computers pervasive computing microphones computer networks information retrieval working environment noise computerized monitoring application software noise robustness;speech recognition;wearable computer;portable computers speech recognition speech based user interfaces;network computing	Due principally to the technology of making computers smaller, more powerful and cheaper, the ubiquitous and wearable computing era is expected to come into being in the beginning of 21st century. In such an environment, speech recognition will be widely used as one of the principal methods of human-computer interaction, and in most of cases it will be performed using a microphone and a networked computer worn and tailored to each person. Accordingly, speech recognition systems will comprise very different structures than do present systems. Promptly retrieving environmental information, such as noise, which is regularly monitored by computers, and task-dependent information transmitted according to each application to the personalized recognizers will significantly improve system adaptability to the new tasks and robustness. Nonetheless, a number of salient issues remain to be resolved to generate the ubiquitous/wearable computing environment.	speech recognition;wearable computer	Sadaoki Furui	2000		10.1109/ICASSP.2000.860214	speech recognition;wearable computer;human–computer interaction;computer science;multimedia;speech analytics	HPC	-50.66728439742872	-43.881094361147866	16355
6e4450b991c984a1bd3a5b56e71f7d137779c11f	interactive design of virtual worlds: combining procedural modeling with intuitive user control. (création interactive de mondes virtuels: combiner génération procédurale et contrôle utilisateur intuitif)		The complexity required for virtual worlds is always increasing. Conventional modeling techniques are struggling to meet the constraints and efficiency required for the production of such scenes. Procedural generation techniques use algorithms for the automated creation of virtual worlds, but are often non-intuitive and therefore reserved to experienced programmers. Indeed, these methods offer fewer controls to users and are rarely interactive. Moreover, the user often needs to find values for several parameters. The user only gets indirect control through a series of trials and errors, which makes modeling tasks long and tedious. The objective of this thesis is to combine the power of procedural modeling techniques with intuitive user control towards interactive methods for designing virtual worlds. First, we present a technique for procedural modeling of villages over arbitrary terrains, where elements are subjected to strong environmental constraints. Second, we propose an interactive technique for the procedural modeling of waterfall sceneries, combining intuitive user control with the automated generation of consistent content, in regard of hydrology and terrain constraints. Then, we describe an interactive sketch-based technique for editing terrains, where terrain features are extracted and deformed to fit the user sketches. Finally, we present a painting metaphor for virtual world creation and editing, where methods for example-based synthesis of vectorial elements are used to automate deformation and editing of the scene while maintaining its consistency.		Arnaud Emilien	2014				Graphics	-37.621967097786076	-33.07516170570404	16359
00c5746bf333ebae6b3ef2aad7f9ba4af6e3a408	the pulse of the earth and sonification	interdisiplinarity;sound art;sound installation;sonification;earth art;art science	Le Souffle de la Terre/The Pulse of the Earth is an on-going sound work (1996–20–) by the artist L. Abenavoli. She outlines the sonification methods employed in the making of the work in an analysis of its technical and conceptual features. The work is described in terms of her artistic objectives.	linear algebra;sonification	Lorella Abenavoli	2011	AI & SOCIETY	10.1007/s00146-011-0358-y	sonification;computer science	AI	-52.11962655541302	-28.011096776427603	16362
c5f5591dd0ce4ed9342fe1fec6d7e43f1ea1009c	cyrafour: an experiential activity facilitating empathic distant communication among copresent individuals	serious games;human avatars;telepresence;cyranoids;embodied cognition;copresence	Distant communication relies mostly on a non-embodied representation of participants (e.g. textual in chats, photographic in videoconference, auditory in telephony, etc) that lessens the sensory richness of conversational interactions. Cyrafour is a novel activity that explores the implications of using human avatars (cyranoids) for empathic interpersonal remote communication. An unscripted conversation between two individuals (the sources) is transmitted through radio waves and reproduced by two copresent subjects (the cyranoids) following certain conversational guidelines. In particular, the Sources were invited to discuss about a topic, play a conversation game and comment on an opinionated video. All Cyrafour sessions were video-taped and participants interviewed afterwards in order to support analysis and discussion. Cyrafour could be considered as a playful embodied identity game in which cyranoids are simultaneously together in and aside from a conversation generated elsewhere. This puzzling circumstance seems to allow for an empathic embodiment of the meaning transmitted and appears to create a frame for further discussion on the topics raised.	avatar (computing);interaction;radio wave	Enrique Encinas;Robb Mitchell	2015		10.1145/2735711.2735815	psychology;multimedia;communication;social psychology	HCI	-58.08661489537022	-39.01597120567689	16378
58310aed07ec96dcfa07e1ebc2d7f3e43f416d6b	constraint-based tiled windows	automatic control;concepcion asistida;control systems;computer aided design;paper technology laboratories windows technology management lapping automatic control force control control systems real time systems;paper technology;window manager;technology management;windows;fenetre;conception assistee;ventana;gestion fenetre;real time systems;lapping;force control	Typical computer workstations employ window managers for creating, destroying, and arranging windows on the screen. Window managers generally follow either a desktop metaphor, allowing windows to overlap each other like sheets of paper on a desk, or they use a tiling model, arranging each window with a specific size and location that avoids overlap. Desktop models allow for the most layout freedom, but can be frustrating to use when dealing with a large number of windows that must all be visible at once. Tiling models guarantee that each window will be completely visible on the screen, but thus far have provided relatively poor mechanisms for controlling layout decisions. This article describes work in tiled window management featuring a constraint-based layout mechanism. With it the user can specify the appearance of individual windows and constrain relationships between windows, thus exercising necessary control over the tiling process. We discuss our constraint model and then detail an implementation approach that would make use of those constraints.	desktop computer;desktop metaphor;microsoft windows;tiling window manager;workstation	Ellis S. Cohen;Edward T. Smith;Lee A. Iverson	1986	IEEE Computer Graphics and Applications	10.1109/MCG.1986.276790	computer vision;simulation;title bar;computer science;artificial intelligence;z-order;technology management;operating system;automatic control;data mining;commit charge;algorithm;lapping;computer graphics (images);mechanical engineering	Visualization	-41.03200127538374	-31.848046319832594	16422
86cf49cec224b9d13db2ad85f091a75cc0aaf822	sciva: a design process for applications on interactive surfaces	design process;user interface;gesture based interfaces;process models;process model;multitouch;interactive tabletops and surfaces;visual interfaces	When creating user interfaces for interactive surfaces, developers and researchers are confronted with the question what design processes can be applied to build an efficiently usable system, as existing techniques are often not sufficiently considering the special constraints and requirements of surface computers. In this poster, we present SCiVA, a 5-step iterative process for designing gesture-based, visual interfaces for interactive surfaces. We identify crucial steps in the development and suggest user-centric methods that can be applied to the respective steps.	computer;iterative method;requirement;user interface	Tobias Hesselmann;Susanne Boll	2010		10.1145/1936652.1936708	simulation;human–computer interaction;computer science;multimedia	HCI	-52.30673155740764	-37.50135755449868	16423
f3ac7f3b05cada1e56ff1e8ce9a1afd254aa4957	communication through physical interaction: a study of human collaborative manipulation of a planar object	robot sensing systems;robot sensing systems senior citizens collaboration decision trees data collection hardware;senior citizens;human robot interaction physical interaction human collaborative manipulation planar object classification algorithm dyads elderly care giver;data collection;collaboration;decision trees;pattern classification geriatrics groupware human robot interaction;hardware	In this paper we describe our progress towards understanding human communication through physical interaction. We describe a classification algorithm that can recognize four classes of actions that frequently occur during collaborative manipulation of planar objects. These actions were selected based on a user study involving dyads of elderly and care-giver in a realistic setting. Further user studies were conducted to collect the data necessary to develop the classification algorithm. As part of the data collection we also developed a sensory glove. The classification algorithm gives insight into human collaborative manipulation. More precisely, it identifies features in the data that are significant for classification. This information is particularly interesting as it only relies on physical aspects of the interaction and not on any particular sensor. As a result, the described work does not depend on any particular hardware and can be directly used by other researchers in human-robot interaction to develop further experiments and studies.	algorithm;decision tree;experiment;fundamental interaction;humans;human–robot interaction;robot;sensor;statistical classification;usability testing	Maria Javaid;Milos Zefran;Barbara Di Eugenio	2014	The 23rd IEEE International Symposium on Robot and Human Interactive Communication	10.1109/ROMAN.2014.6926357	simulation;computer science;artificial intelligence;decision tree;multimedia;data collection;collaboration	Robotics	-42.653611254559856	-44.118492379663266	16453
0594bcbe62a06d1a4d66bf42a49035b567fe6b3a	simulating chemical reactions using a swarm of miniature robots		We wish to simulate basic rules of chemistry using a swarm of miniature robots, which mimic atoms and forming molecules. Atomic scale interactions are difficult to observe and computer simulations or ball-and-stick models capture either behavioral or embodied aspects, but not both. Miniature robots that are able to determine their orientation and position with respect to each other and provide audible, visual, and tactile feedback to a user could make such simulations both interactive and tangible. We describe a working prototype of our swarm-robotic chemistry simulation which demonstrates concepts including electronegativity, reaction spontaneity, the octet rule, and hybridization. Here, the key challenge is that once we go beyond the most simple set of atoms, the outcome of reactions cannot be calculated from first principles. We solve this problem by letting robots exchange local measurements, the nearby atoms, their geometry, and molecules that have formed and then using a compact look-up table implementation, which suggests avenues of further studies for both physical chemistry and swarm robotics. We also present preliminary data recorded from a high-school demonstration evaluating using a tangible simulation of chemistry reactions as a teaching tool.	robot;swarm	Audrey Randall;John Klingner;Nikolaus Correll	2016		10.1007/978-3-319-43488-9_27	simulation;artificial intelligence	Robotics	-50.00145066300366	-49.818293397553276	16462
41f773c0847cd2d65b9fe8dfb2c73b7bee2e3227	painting with bob: assisted creativity for novices	painting;creativity;novices	Current digital painting tools are primarily targeted at professionals and are often overwhelmingly complex for use by novices. At the same time, simpler tools may not invoke the user creatively, or are limited to plain styles that lack visual sophistication. There are many people who are not art professionals, yet would like to partake in digital creative expression. Challenges and rewards for novices differ greatly from those for professionals. In this paper, we leverage existing works in Creativity and Creativity Support Tools (CST) to formulate design goals specifically for digital art creation tools for novices. We implemented these goals within a digital painting system, called Painting with Bob. We evaluate the efficacy of the design and our prototype with a user study, and we find that users are highly satisfied with the user experience, as well as the paintings created with our system.	prototype;usability testing;user experience	Luca Benedetti;Holger Winnemöller;Massimiliano Corsini;Roberto Scopigno	2014		10.1145/2642918.2647415	simulation;human–computer interaction;painting;multimedia;creativity	HCI	-49.972197765155705	-31.81998379175062	16495
23b3ecc5668fba5805bd143fbcf23a0a2af8d813	dual subtitles as parallel corpora	parallel data;dual subtitles;machine translation	Topic Most probable words 1 time home back long coming party times question half answer buy 2 ve made feel bad father head heard boy city listen hit mother 3 don work things care doesn problem won mind change worry sex 4 call car happen phone gonna side wanna back security safe cell 5 good uh idea family pretty friend business reason tonight inside office 6 house mr president school today high men white speak hassan hot 7 find jack wrong left bit bauer play hand fuck asked ctu law agent 8 big money thought told girl leave job dad late mom lost haven 9 place talking stop called case thinking truth pay telling child 10 life real show live guy rest cool force couldn perfect state earth date	gon;haven (graph theory);money;numerical aperture;parallel text;text corpus	Shikun Zhang;Wang Ling;Chris Dyer	2014			natural language processing;computer science;linguistics;machine translation;programming language	HCI	-59.07465671212344	-25.02576096234125	16527
d09093ea59d010d0b90cf95bedf89495db5b9f40	the quest for 'musically interesting' structures in computer music				David Keane	1981			aesthetics;computer music;art	ML	-53.376601438138586	-28.91665121063384	16554
937fe7a80c071fe36c0f3e2f61f2e8bbbf0b34f3	ubicomp's colonial impulse	postcolonialism;research practice;design rhetoric;discourse;partiality	Ubiquitous computing has a grand vision. Even the name of the area identifies its universalizing scope. In this, it follows in a long tradition of projects that attempt to create new models and paradigms that unite disparate, distributed elements into a large conceptual whole. We link concerns in ubiquitous computing into a colonial intellectual tradition and identify the problems that arise in consequence, explore the locatedness of innovation, and discuss strategies for decolonizing ubicomp's research methodology.	distributed element model;ubiquitous computing	Paul Dourish;Scott D. Mainwaring	2012		10.1145/2370216.2370238	human–computer interaction;computer science;postcolonialism	HCI	-62.14632880630741	-30.912246413887022	16573
85f4f16eed007e4332fffc9eb37c2a16fee82339	exploring human computation and social computing to inform the design process		Although several standards, recommendations and guidelines have been used to assist designers in their tasks, much of the design choices still rely heavily on the designer’s experience. In this work we argue that complex choices about interface elements (e.g. images, icons, sounds) could have the help of the users themselves to inform the designer ́s choices. The paper situates the contribution in the intersection of the human computation and social computing fields, showing a preliminary survey of related work. Moreover, we illustrate the idea with an instantiation of an environment for designers, within the frontiers of human computation and social computing.	human-based computation;image;social computing;universal instantiation;user interface	Roberto Romani;Maria Cecília Calani Baranauskas	2013		10.5220/0004434300670074	computer science;knowledge management;theoretical computer science;management science	HCI	-60.89469727073976	-36.04318596049563	16584
75b5b22a768300199819f6d5f22660f03bc7ab27	"""blizzard entertainment's starcraft ii cinematic teaser: """"building a better marine"""""""		"""Blizzard Entertainment's """" Building a Better Marine """" teaser debuted in Seoul, South Korea, to a stadium of 17,000 ecstatic Blizzard gamers as part of the initial announcement that StarCraft II was in development. The company decided that there would be no better way to make this announcement than with a full-CG short developed by the Blizzard Entertainment film department. """" Building a Better Marine """" takes the viewer through the process of creating one of the game's most basic units, the terran marine, and shows the epic scale of even the most mundane aspects of the StarCraft universe. Coming nearly 10 years after the release of the original StarCraft, this film is a visual representation of Blizzard Entertainment's affinity for the gritty sci-fi characters and settings that distinguish the critically acclaimed series, and the company's eagerness to share the next chapter with players around the world. Two nursing-home residents challenge each other to a wheelchair race."""	processor affinity;starcraft	Janet Garcia	2008		10.1145/1400468.1400480	simulation;computer graphics (images)	HCI	-55.181815085462745	-26.547803327211938	16707
21fa266d1d141faa193be306affda480ee230975	plastic imaginaries: becoming response-able stakeholders?	stakeholders;democratic design experiment;material participation	This piece in the interactive exhibition shows a prototype of a domestic plastic composting kit. More specifically it's a repurposed glass jar with a lid that has been cut open and replaced by a metal net. Inside it common mealworms are biodegrading styrofoam. A democratic design experiment, where similar prototypes were distributed to explore how it is to live with it, will be present in this exhibition through photos. A 30-minute workshop builds on this democratic design experiment and explores the becoming of stakeholders when the actors and issues are multiple and uncertain.	prototype	Kristina Lindström;Åsa Ståhl	2016		10.1145/2948076.2948117	social science;simulation;stakeholder;economics;human–computer interaction;engineering;civil engineering;management;mechanical engineering	HCI	-54.04078604050679	-32.98394552514244	16739
2aa789f06f75563b8a02a5f56c8214e6201c5334	time-of-flight cameras and microsoft kinect		Spend your few moment to read a book even only few pages. Reading book is not obligation and force for everybody. When you don't want to read, you can get punishment from the publisher. Read a book becomes a choice of your different characteristics. Many people with reading habit will always be enjoyable to read, or on the contrary. For some reasons, this time of flight cameras and microsoft kinect tends to be the representative book in this website.	kinect;microsoft windows	Carlo Dal Mutto;Pietro Zanuttigh;Guido M. Cortelazzo	2012		10.1007/978-1-4614-3807-6	stereo cameras;computer vision;simulation;computer science;computer graphics (images)	HCI	-59.25223951806336	-26.324164071703674	16770
30b37863f313a80e66b50623de092c569ac5e2c4	using the metroweb tool to improve usability quality of web sites	web pages;semantic network;user centered design;web design;software reusability;software reusability java web design user centred design;user centred design;usability guidelines web page design web pages knowledge management user centered design java ergonomics q factor process design;web pages metroweb tool web sites web designers user centered design java based application semantic network;usability guidelines;java;knowledge base	This work addresses the question of supporting Web designers in considering usability in their work in order to foster user-centered design of Web sites. With the MetroWeb tool that is described in this paper, designers can access usability guidelines contained in usability knowledge bases, and use them to design a particular Web site based on these usability guidelines. MetroWeb consists of a Java-based application helping usability experts to gather usability guidelines coming from different sources and to organize them in a structured way. It then provides designers with guidance in using these guidelines according to a semantic network of concepts structured around the notion of guideline, such as ergonomic criteria, development phase, bibliographic reference, type of Web site, type of Web page, etc. A first experiment was conducted with professional Web designers in order to evaluate their appreciation of the MetroWeb tool. They had to create two Web pages with or without the support of MetroWeb. The main results showed that designers using MetroWeb took into account more usability guidelines and made Web pages with less usability errors than designers without MetroWeb.	human factors and ergonomics;java;reference type;semantic network;usability;user-centered design;web design;web page;world wide web	Céline Mariage;Jean Vanderdonckt;Aline Chevalier	2005	Third Latin American Web Congress (LA-WEB'2005)	10.1109/LAWEB.2005.46	pluralistic walkthrough;web service;web usability;knowledge base;user-centered design;web development;web modeling;usability;web design;human–computer interaction;web standards;computer science;usability engineering;web navigation;social semantic web;web page;semantic web stack;database;semantic network;java;web 2.0;world wide web;web design program;usability lab;usability inspection	HCI	-42.111086066073554	-25.220296080862592	16775
510c4f087a816c76d41b98c2acd2446c59b157ee	padova soundscape: a crowdsourcing approach to describe the sound of a city (position paper)		This paper reports the results of an initial experiment on the acoustic description, called soundscape, of the city of Padova. A group of users has been involved in recording the sounds of the city and in tracking their position in space and in time using a web based interface. Collaboration and coordination among participants has been promoted using a wiki, where participants could assign themselves the locations to be recorded and define the standard to be followed. The result is the creation of an acoustic map of the city of Padova, which can be navigated in space and in time through a web interface. A mobile version of the interface is under development.	acoustic cryptanalysis;crowdsourcing;user interface;wiki	Nicola Orio	2016			soundscape;position paper;acoustics;crowdsourcing;engineering	HCI	-53.58077234821422	-41.06904429978727	16811
03a85dcc190093de8524c84d865d22d25850e862	usability investigation of anthropomorphic user interface feedback	thesis		usability;user interface	Pietro Murano	2009			user interface design;simulation;usability;human–computer interaction;computer science;multimedia;heuristic evaluation;usability lab	HCI	-51.837976812727895	-35.585996328798934	16859
64b27b88a00f733e3d0f44f8cd73af3b11285452	using physical memorabilia as opportunities to move into collocated digital photo-sharing	navegacion;controle acces;viewing system;reseau social;computadora personal;narrative;ordinateur personnel;electronic mail;sistema punteria;social interaction;red www;interaction sociale;personal computer;fotografia digital;narration;securite informatique;reseau web;photographie numerique;automatizacion domestica;correo electronico;ecran visualisation;computer security;digital photography;social network;navigation;pantalla visualizacion;analyse syntaxique;internet;collocated digital photo sharing;interaccion social;photo sharing;photo collection;analisis sintaxico;systeme visee;syntactic analysis;seguridad informatica;narracion;physical memorabilia;household;menage;world wide web;access control;technical report;display screen;domotique;familia;red social;courriel;home automation;tagging	The uptake of digital photos vs. print photos has altered the practice of photo sharing. Print photos are easy to share within the home, but much harder to share outside of it. The opposite is true of digital photos. People easily share digital photos outside the home, e.g., to family and friends by email gift-giving, and to social networks and the broader public by web publishing. Yet within the home, collocated digital photo sharing is harder, primarily because digital photos are typically stored on personal accounts in desktop computers located in home offices. This leads to several consequences. 1) The invisibility of digital photos implies few opportunities for serendipitous photo sharing. 2) Access control and navigation issues inhibit family members from retrieving photo collections. 3) Photo viewing is compromised as digital photos are displayed on small screens in an uncomfortable viewing setting. To mitigate some of these difficulties, we explore how physical memorabilia collected by family members can create opportunities that encourage social and collocated digital photo sharing. First, we studied (via contextual interviews with 20 households) how families currently practice photo sharing and how they keep memorabilia. We identified classes of memorabilia that can serve as memory triggers to family events, trips, and times when people took photos. Second, we designed SOUVENIRS, a photo-viewing system that exploits memorabilia as a social instrument. Using SOUVENIRS, a family member can meaningfully associate physical memorabilia with particular photo sets. Later, any family member can begin their storytelling with others through the physical memento, and then enrich the story by displaying its associated photos simply by moving the memento close to the home’s large-format television screen. Third, we re-examined our design premises by evoking household reactions to an early version of SOUVENIRS. Based on these interviews, we redesigned SOUVENIRS to better reflect the preferences and real practices of photo and memorabilia use in the home. ________________________________________________________________________	access control;contextual inquiry;desktop computer;digital photography;email;memento project;memento pattern;social network;television set	Michael Nunes;Saul Greenberg;Carman Neustaedter	2009	Int. J. Hum.-Comput. Stud.	10.1016/j.ijhcs.2009.09.007	social relation;human–computer interaction;computer science;artificial intelligence;multimedia;narrative;world wide web	HCI	-53.98359604722848	-41.053456613185034	16864
61db687095fc49af02c3506dec69830b61fe65e0	information and communication technologies for ageing well and e-health		The omnipresence and familiarity of textiles in combination with the integration of invisible sensors, actuators, and information and communication technology under the term “interactive digital textiles” offer the potential of bridging the gap between age, the aging-population, and latest information and communication technology. Digital textiles are reaching maturity and first technology augmented cloths are becoming commercially available. However, little is known about the acceptance and projected use of digital textiles for/in home environments and whether acceptance is shaped by age, gender, expertise in interacting with technology, or other aspects of user diversity. In a survey with n = 136 participants, we identified and quantified factors that influence the adoption and rejection of a smart cushion as example for digital textiles. We found that attitude towards technology and attitude towards automation are decisive for the projected acceptance, while age plays a minor role. In addition, we provide a customer segmentation based on the projected use and provide detailed descriptions of adopters and rejecters as well as their model-based evaluations of the smart interactive cushion. The article concludes with open research questions and strategies for practitioners to leverage smart textile interfaces as basis for many innovative products in the future.	bridging (networking);capability maturity model;e-textiles;interaction;open research;rejection sampling;sensor	Carsten Röcker;John O'Donoghue;Martina Ziefle;Leszek A. Maciaszek;William J Molloy	2017		10.1007/978-3-319-93644-4	public records;ageing;data science;information and communications technology;medicine	HCI	-61.8420353440629	-41.80395055340719	16944
051a03801c719d3d9303a4a1fb88abb0e2c957e3	easing the wait in the emergency room: building a theory of public information systems	social navigation;emergency department;social computing;ethnography;legal issues;social and legal issues;computer supported cooperative work;mobile computer;handheld devices and mobile computing;emergency room;public space;computer supported cooperative work cscw;handheld device;user interface design;interactive space;private information;interaction design;public information;social computing and social navigation;health care	In this paper we discuss a real world problem encountered during recent fieldwork: that of providing information in public settings when the information has both public and private components. We draw on our ethnographic studies in the waiting area of a busy hospital Emergency department. Despite evidence that lack of information can lead to stress, problem behaviours and poor levels of satisfaction with treatment, little information was made available to patients. We review the types of information needed and propose how the theoretical concepts of public, social and private information spheres relate to public spaces such as the Emergency department waiting area. We argue how the further theoretical concept of interaction spaces may be used in conjunction with these information spheres to inform interaction design for public settings.	field research;information system;interaction design;personally identifiable information;theoretical definition;theory	Eamonn O'Neill;Dawn Woodgate;Vassilis Kostakos	2004		10.1145/1013115.1013120	user interface design;simulation;private information retrieval;human–computer interaction;computer science;engineering;knowledge management;interaction design;computer-supported cooperative work;group information management;mobile device;ethnography;management;social computing;health care	HCI	-58.89330308477044	-41.44200577857171	16969
0a9aa91b04e0cb7a56bd0f6f10f0d65723d67624	a force sensitive multi-touch array supporting multiple 2-d musical control structures	touchpad;high resolution;pressure and force sensing;versapad;control structure;high resolution gestural signals	We describe the design, implementation, and evaluation with musical applications of force sensitive multi-touch arrays of touchpads. Each of the touchpads supports a three dimensional representation of musical material: two spatial dimensions plus a force measurement we typically use to control dynamics. We have developed two pad systems, one with 24 pads and a second with 2 arrays of 16 pads each. We emphasize the treatment of gestures as sub-sampled audio signals. This tight coupling of gesture with audio provides for a high degree of control intimacy. Our experiments with the pad arrays demonstrate that we can efficiently deal with large numbers of audio encoded gesture channels - 72 for the 24 pad array and 96 for the two 16 pad arrays.	experiment;gesture recognition;multi-touch;touchpad	David Wessel;Rimas Avizienis;Adrian Freed;Matthew Wright	2007		10.1145/1279740.1279745	touchpad;computer vision;speech recognition;image resolution;computer science;operating system;programming language;control flow	HCI	-42.90417776875014	-41.10260530198905	16974
c798e119140274c1d642488b9681ceb8dae1906b	comparison of finite-repertoire and data-driven facial expressions for sign language avatars		To support our research on ASL animation synthesis, we have adopted and enhanced a new virtual human animation platform that provides us with greater fine-grained control of facial movements than our previous platform. To determine whether this new platform is sufficiently expressive to generate understandable ASL animations, we analyzed responses collected from deaf participants who evaluated four types of animations: generated by our old or new animation platform, and with or without facial expressions performed by the character. For animations without facial expressions, our old and new platforms had equivalent comprehension scores; for those with facial expressions, our new platform had higher scores. In addition, this paper demonstrates a methodology by which sign language animation researchers can document transitions in their animation platforms or avatar appearance. Performing such an evaluation enables future readers to compare published results over time, both before and after such a transition in animation technology.	avatar (computing);virtual actor	Hernisa Kacorri;Matt Huenerfauth	2015		10.1007/978-3-319-20681-3_37	repertoire;natural language processing;sign language;animation;data-driven;american sign language;facial expression;artificial intelligence;comprehension;computer science	Graphics	-52.678032233434365	-45.750937862493174	17028
3718b0b58a52ea850a4017f0f3d3905f775d876b	interactive teaching material for the world wide web: a syntax tutor written in java	world wide web		java;world wide web	Ana von Klopp Lemon	1996			semantic web;world wide web;web standards;web page;multimedia;web navigation;web-based simulation;web 2.0;tutor;computer science;java	HCI	-43.65728122867876	-24.232159069092578	17031
1368f37fb60242a024e57ea8c566ff20b70da338	wearable remote control of a mobile device: comparing one- and two-handed interaction	mobile controls;hci;e textiles;wearable controls;wearable computing;conductive fabric	While wearable technologies are suitable for remotely controlling mobile devices, few studies have examined user preferences for one- or two-handed touch interaction with these wearables, especially when worn on the wrist and hand area. As these locations are recognized as socially acceptable and preferred by users, we ran a study of touch interaction to remotely control mobile devices. Our results suggest users prefer swipe gestures over touch gestures when interacting with wearables on the wrist or hand, and that users find both one- and two-handed interactions suitable for wearable remote controls.	interaction;mobile device;remote control;user (computing);wearable computer	Jessica Speir;Rufino R. Ansara;Colin Killby;Emily Walpole;Audrey Girouard	2014		10.1145/2628363.2634221	embedded system;simulation;wearable computer;human–computer interaction;computer science	HCI	-46.136789902770346	-43.64645118021318	17089
b6e74843827951ddedd111bc74f0983a462adc34	exploring augmented reality approaches to real-time captioning: a preliminary autoethnographic study		We explore an augmented reality (AR) approach to real-time captioning for people who are deaf and hard of hearing. In contrast to traditional captioning, which uses an external, fixed display (e.g., laptop or large screen), our approach allows users to manipulate the shape, number and placement of captions in 3D space. We discuss design factors, describe two early prototypes, and report on an autoethnographic evaluation of the prototypes. Preliminary findings suggest that, compared to traditional laptop-based captions, HMD captioning may increase glanceability, improve visual contact with speakers, and support access to other visual information (e.g., slides).	augmented reality;head-mounted display;real-time transcription	Dhruv Jain;Bonnie Chinh;Leah Findlater;Raja S. Kushalnagar;Jon Froehlich	2018		10.1145/3197391.3205404	engineering;laptop;multimedia;human–computer interaction;augmented reality;closed captioning	HCI	-46.81429209578794	-43.558415848952635	17104
3a62a9d6590fe2b64a9a93cf83877a04b144e9ce	a practical framework for comprehensive mobile context visualization	context aware;mobile device;sensors;mobile computer;reasoning with contexts;mobile context;data visualisation;mobile computing data visualisation;design and implementation;visualizing contexts;reasoning with contexts mobile context sensors visualizing contexts;context cognition sensors mobile communication servers databases mobile handsets;context reasoning methods comprehensive mobile context visualization mobile devices context aware mobile computing;mobile computing;mobile application;context effect	With the advent of various context sensors on mobile devices, context-aware mobile computing is emerging. Consequently, a number of advanced mobile applications utilizing contexts are expected to grow. However, there remain a number of research issues unresolved, reasoning for group of users, reasoning with contexts for past behavior and future trends and indications, visualizing various types of mobile contexts effectively for better comprehension. We present a design of an advanced context visualizer which provides unique and noble features which are required in developing advanced context-aware mobile applications. We present the design and implementation, and present context reasoning methods as well.	mobile app;mobile computing;mobile device;sensor	Du Wan Cheun;Moon Kwon Kim;Hyun Jung La;Hyun Joo Bae;Chang Sup Keum;Soo Dong Kim	2011	2011 IEEE 8th International Conference on e-Business Engineering	10.1109/ICEBE.2011.24	context effect;computer science;sensor;theoretical computer science;operating system;mobile device;multimedia;mobile computing;world wide web	Visualization	-53.80406473565594	-39.14519337777228	17140
7d64ca87c0f4a745f0275b9d4e853791833655d4	interface devices and software tools for expressive music creation	audio user interfaces;software tool;information technology;information technology interface devices software tools expressive music creation;interface devices;expressive music creation;software tools acceleration humans conductors instruments data gloves information technology haptic interfaces motion detection data mining;software tools;software tools audio user interfaces interactive devices music;music;interactive devices	In the past some decades, the advanced information technology has proposed a variety of new ways of music creation and performance to make people free from the physical limitations of instruments by providing different abilities on music creation. This paper describes some interface devices and software tools that have been developed in our group	programming tool	Shuji Hashimoto	2006	ROMAN 2006 - The 15th IEEE International Symposium on Robot and Human Interactive Communication	10.1109/ROMAN.2006.314483	human–computer interaction;computer science;music;multimedia;information technology;world wide web	Robotics	-45.465443305882495	-36.98458699794845	17150
84b8a7c1b804f1a13679769f78acfeadb316f54e	some external and internal factors affecting users of interactive information systems			information systems	John E. Evans	1996			human–computer interaction;nuclear magnetic resonance;computer science;information system	HCI	-53.338645896637395	-34.20562594110909	17186
fc43eb8dae32b27892f0c265ff93d956a4a2d528	scrutinizing pseudo haptic feedback of surface roughness in virtual environments	blender pseudo haptic feedback control display ratio psychophysical scaling surface roughness multi modal feedback virtual environment human machine interface;human computer interaction;virtual objects;psychophysical experiment pseudohaptic feedback surface roughness virtual environment visual feedback virtual scenes virtual objects haptic actuators;haptic interfaces surface roughness rough surfaces materials virtual environment visualization tracking;psychophysical scaling;surface roughness;virtual reality;actuators;layout;control display ratio;psychology;materials;multi modal feedback;virtual reality haptic interfaces human computer interaction surface roughness;rough surfaces;virtual scenes;human subjects;visualization;feedback;human machine interface;visual cues;displays;blender;haptic feedback;visual feedback;haptic actuators;humans;virtual environment;haptic interfaces;pseudohaptic feedback;pseudo haptic feedback;tracking;psychophysical experiment	Operators in virtual environments have to mostly rely on visual feedback when exploring virtual scenes and objects. If no additional haptic actuators are available then information about surface roughness will not be communicable and thus the degree of immersion will suffer. To compensate for this shortcome the usage of pseudo haptic feedback (Control/ Display ratio = visual cues encoding haptic object properties) is discussed and two psychophysical experiments are conducted. The first experiment deals with the perceptual scaling of different Control/ Display ratios while the second experiment examines the way, how human subjects assign suitable Control/ Display ratios to different real surfaces. It turns out that human operators can reliably perceive differences in Control/ Display ratios and a psychophysic function is established. The allocation of specific Control/ Display values to real materials does not give any generalizable results; only a global trend is observable.	experiment;haptic technology;image scaling;immersion (virtual reality);observable;virtual reality	Gunter Hannig;Barbara Deml	2008	2008 IEEE Conference on Virtual Environments, Human-Computer Interfaces and Measurement Systems	10.1109/VECIMS.2008.4592742	human–machine interface;layout;computer vision;simulation;visualization;surface roughness;sensory cue;computer science;virtual machine;artificial intelligence;operating system;feedback;virtual reality;tracking;multimedia;haptic technology;actuator	Visualization	-44.09665833197856	-48.76312936396824	17254
0d01877c07ee591c57ef791d1376a0c7c84c39dd	towards a set of design principles for hapto-virtual rehabilitation environments: preliminary results in fine motor hand therapy	design;user/machine systems;human factors;virtual rehabilitation;user-centered design;design principles;haptics;user centered design	In this paper, we propose a set of principles to facilitate the design of haptic feedback virtual environments for fine hand motor skills. Firstly, we conducted a contextual study in a rehabilitation center to identify preliminary design elements using grounded theory. Based on these results, we defined a set of principles to aid in the design of haptic feedback virtual environments to achieve therapy effectiveness and patient’s safety. In order to evaluate the proposed design principles, we developed a haptic feedback virtual environment based on them and conducted a formative evaluation with 5 patients and 3 therapist. Although preliminary results provide promising evidence, indicating a high perception of usefulness, ease of use and intention of use of the proposed environment and principles; further evaluations are needed to investigate therapy effectiveness and patient’s safety.		Cristina Ramírez-Fernández;Eloísa García-Canseco;Alberto L. Morán	2014			user-centered design;simulation;human–computer interaction;computer science;engineering;design elements and principles;biological engineering;haptic technology	Visualization	-61.19215870360133	-50.72117167657861	17292
716c3a2f40245e031274607eeb03fdb82e3f981d	emphasizing on the timing and type - enhancing the backchannel performance of virtual agent			backchannel	Xia Mao;Na Luo;Yu-Li Xue	2012			real-time computing	HCI	-53.30927004794229	-47.08668352443213	17361
71826ef99f0050aae556d84fc8502a9f6683cd3e	towards a generic framework for automatic measurements of web usability using affective computing techniques	web usability;framework;affective computing	We propose a generic framework for the automatic usability evaluation of web sites by combining traditional automatic usability methods with affective computing techniques. To evaluate a framework a pilot study was carried out where users (n=4) reported their affective states using dimensional and categorical models. Binary task completion, time, mouse clicks, and error rates as an indicator of web usability were automatically captured for each page. Results suggested that frustration experienced when error rates and time for the task were higher. Delight on the other hand was at the other side of the spectrum. In the case that usability measurements had almost same values (e.g. confusing or engaging pages), affective states may be a way to show the	affective computing;data logger;modality (human–computer interaction);paging;poor posture;server-side;web usability	Payam Aghaei Pour;Rafael Alejandro Calvo	2011		10.1007/978-3-642-24600-5_48	pluralistic walkthrough;web usability;component-based usability testing;cognitive walkthrough;usability;human–computer interaction;computer science;artificial intelligence;software framework;affective computing;multimedia;heuristic evaluation;world wide web;usability lab;usability inspection	Web+IR	-58.52249759618675	-46.784729031181705	17368
c4461e31f0e8db916d085186a52497223fe95fb4	what feels parallel strongly depends on hand orientation	egocentric;reference frame;allocentric;reference frames;hand orientation;left handed;right handed;spatial relation;parallel;spatial perception;haptic perception;haptic spatial perception	Parallel in the outside world is not necessarily perceived as parallel. Previous studies have shown that what is felt as parallel can deviate significantly from what is physically parallel. In a new set-up, the influence of hand/arm orientation is investigated in detail by systematically varying the angle between the two hands, while the participants have to make a test bar parallel to a reference bar. Large positive deviations were found of about 32 % of the angle between the hands. The deviations were always in the direction of the rotation of the right hand with respect to the left hand. These findings are consistent with the hypothesis that the haptic perception of spatial relations is biased in the direction of the egocentric reference frame connected to the hand.		Astrid M. L. Kappers;Bart J. Liefers	2012		10.1007/978-3-642-31401-8_22	computer vision;simulation;mathematics;communication	NLP	-44.78244009593488	-48.85782385029873	17395
0039e0b86a7e9051434e3e5f23a5231720ebc9cf	a family of single-user autostereoscopic displays with head-tracking capabilities	autostereoscopic display;medical imagery;formation image tridimensionnelle;affichage;ophthalmology;high resolution;helmet mounted display;mouvement oculaire poursuite;position;multimedia;application software liquid crystal displays computer displays prototypes glass optical design lenses visualization biomedical imaging high resolution imaging;realite virtuelle;image resolution;visualizacion;3d imaging;etude experimentale;raster;head tracking;virtual reality;tracking three dimensional displays visual perception liquid crystal displays flat panel displays large screen displays multimedia communication image resolution computer games virtual reality computer displays;liquid crystal displays;stereoscopy;stereoscopic vision;prototipo;medical and biological imaging;eyes;cristal liquide;display;three dimensional displays;flat panel displays;multimedia communication;trame;computer displays;imagerie medicale;positions;movimiento ocular seguimiento;stereoscopie;visual perception;estereoscopia;imageneria medical;rear projection technology single user autostereoscopic displays head tracking stereoscopic vision lenticular raster plates raster plates tracking projection lenses viewers position screen 3d multimedia desktop visualization medical imaging biological imaging architecture computer games 3d virtual reality high resolution monitors flat liquid crystal panel monitors pc desktop applications large screen high resolution displays;liquid crystals;liquid crystal;computer games;large screen displays;formacion imagen tridimensional;vision;prototype;oeil;tracking;pursuit eye movement;computer game;trama;ophtalmologie	We present prototypes of autostereoscopic displays which allow single users to experience stereoscopic vision without the need for special eye glasses or helmet-mounted displays. The design of the displays is based on lenticular raster plates and includes a number of novel concepts for tracking of raster plates or projection lenses to account for changes of the viewers position in front of the screen. Applications envisioned include 3-D multimedia desktop visualization for medical and biological imaging, design, and architecture, as well as computer games and 3-D virtual reality in general. Concepts and results for both high-resolution flat liquid-crystal panel monitors for PC desktop applications as well as large screen high resolution displays using rear-projection technology are discussed.	autostereoscopy;multi-user	Reinhard Borner;Bernd Duckstein;Oliver Machui;Hans Roder;Thomas Sinnig;Thomas Sikora	2000	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.825723	stereoscopy;computer vision;image resolution;liquid crystal;computer science;virtual reality;multimedia;computer graphics (images)	EDA	-41.56199543978781	-38.506889528137705	17411
3233060d40e728a22d04f6751a6dc5e4e3dbb6bf	cultural diversity for virtual characters (extended abstract)	behavior generation;culture;virtual agents	In human conversation, meaning is transported through several channels such as verbal and nonverbal behavior. Certain of these behavioral aspects are culturally dependent. Mutual understanding or acceptance is thus, amongst others, depended on the cultural background of the interlocutors. When designing virtual character behavior, culture should be considered as it may improve the character’s acceptance by users of certain cultural backgrounds. This paper proposes a hybrid approach for the generation of culture-specific behaviors in a multiagent system. A computational model has been established by refining theoretical knowledge of culture-specific behavior with statistical data extracted from a video corpus of German and Japanese first-time meetings. Evaluation studies of such culturally enhanced virtual characters were conducted in both targeted cultures. Results indicate that human observers tend to prefer character behavior that was designed to resemble their own cultural background.	agent-based model;computation;computational model;multi-agent system	Birgit Endrass	2013			artificial intelligence;multimedia;culture	AI	-53.29703509795378	-48.27613849291527	17446
51a3ae613d3e8836ede45db0ed799828b3c9fb3b	programming multimedia applications in gstreamer	digital signal processing;multimedia;dataflow pipeline;c language;c;gstreamer;video player;open source	This short course is an introduction to GStreamer, one of the main free/open-source frameworks for multimedia processing. We start presenting GStreamer, its architecture and the dataflow programming model, and then adopt a hands-on approach. Starting with an example, a simple video player, we introduce each concept of GStreamer's basic C API and implement it over the initial example incrementally, so that at the end of the course we get a complete video player with support for the usual playback operations (start, stop, pause, seek, fast-forward, and rewind). We also discuss sample filters'processing elements that manipulate audio and video samples. We present the various filters natively available in GStreamer and show how one can extend the framework by creating a plugin with a custom filter that manipulates video samples. The only prerequisite for the short course is a basic knowledge of the C programming language. At the end of the short course, we expect that participants acquire a general view of GStreamer, and be able to create simple multimedia applications and explore its more advanced features.	application programming interface;dataflow programming;fast forward;gstreamer;hands-on computing;media controls;open-source software;programming model;the c programming language	Guilherme Augusto Ferreira Lima;Rodrigo C. M. Santos;Roberto Gerson De Albuquerque Azevedo	2016		10.1145/2976796.2988193	real-time computing;simulation;human–computer interaction;telecommunications;computer science;artificial intelligence;operating system;digital signal processing;machine learning;database;multimedia;programming language;world wide web;computer network	PL	-45.3472370860078	-31.804374401458677	17481
1ef379883fca56789728cb26261965986fea0e03	someday, you will understand	technical information;digital technology computers and society computing profession;technological innovation;technical information computing invention computer discovery digital communication children education;computer aided instruction;computer discovery;computer applications;children education;computing profession;writing arithmetic;digital communication;engineering profession;digital technology;computers and society;computer applications computer aided instruction;computing invention;digital communications	T he incomprehension was clearly evident on the children’s faces. They couldn’t grasp what was happening to their parents. I was not in a position to intervene, and even if I had been, I could have offered nothing more than the reassurance that maturity and experience would bring enlightenment. Across the street from where I stood was a pair of stores, the one on the left devoted to the shoes of a famous Italian designer, and the one on the right to the products of a sports car manufacturer from the same country. The latest fashions of Milan could be seen in one window, and the red gleam of speed was displayed in the other. As families approached these establishments, mothers would be pulled to one and fathers to the other. Forced to choose which parent to follow, the children inevitably found themselves in a business that sold products of no interest to them. Some kids would cry. Some would pout. Others would find comfort in that universal children’s game that involves running, screaming, and hiding in the aisles of a retail establishment. If I had had the power, I would have explained to each child that this was the parting of the genders. Someday, they might like to know how their parents responded to the forces of designer shoes and high-powered exercise horses, a laptop, and several hundred cable ties. The cell phones were connected to the top rail of the pen at intervals that corresponded to 15 degrees of the circle. Upon command, each phone would take a picture of whatever stood at the center of the enclosure and transmit the image to the laptop. A simple program on the laptop stored those images in a database and displayed them in rapid order, which gave the illusion of three dimensions. Most kids walked by the photographic exhibit with barely a glance. From their perspective, a recent 3D movie was far more compelling and a home videogame was more intriguing. Perhaps they would be able to appreciate the value of the show only after they had struggled with the problems of creativity, when they had tackled an assignment to create something out of a bag of old cell phones, the old pony’s pen, and Aunt Florence’s obsolete laptop.	3d computer graphics;a new kind of science;capability maturity model;database;enlightenment;laptop;mobile phone;shoes;the circle (file system)	David Alan Grier	2010	IEEE Computer	10.1109/MC.2010.78	computer science;artificial intelligence;software engineering;multimedia;computer applications;digital electronics	Mobile	-58.46858926910041	-26.827032484647912	17534
0dd191381c6f968cdc4da62cea83dbc13f0c32aa	two-dimensional active type surface acoustic wave tactile display on a computer screen	motion control;vibrations;computer displays surface acoustic waves acoustic waves two dimensional displays fingers vibrations pulse modulation friction motion control humans;two dimensional displays;motion capture;surface acoustic wave;tactile display;computer displays;fingers;humans;acoustic waves;distributed generators;friction;surface acoustic waves;pulse modulation	We have already proposed a novel method to provide human tactile sensation using surface acoustic wave (SAW). A pulse modulated driving voltage excites temporal distribution of standing SAW. The distribution generates friction shift on the surface of a SAW substrate. When the surface with the burst SAW is explored through a slider by a finger, the friction shift generates a mechanical vibration similar to a stick-slip vibration on the finger. The vibration can be perceived as tactile sensation at mechanoreceptors in the finger skin. Controlling the burst frequency according to measured rubbing motion, reality of the displayed sensation can be enhanced. In this research, we install the tactile display with a transparent stator transducer on a computer screen. Two-dimensional rubbing motion is available for the display. An experimental apparatus with finger motion capture system is fabricated on trial and controlled according to the captured motion. In a demonstration, operators can rub on a solid mark under the display to recognize its shape through tactile sensation.	acoustic cryptanalysis;computer monitor;excited state;motion capture;pulse-width modulation;tactile imaging;transducer	Masaya Takasaki;Hiroyuki Kotani;Takeshi Mizuno;Takaaki Nara	2006	2006 14th Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems	10.1109/VR.2006.136	motion control;acoustic wave;computer vision;motion capture;surface acoustic wave;vibration;friction;pulse-width modulation	Graphics	-41.3313518628255	-41.94163654516819	17576
68c4dc701da68262884658f7f44baa5b79d66de7	openvl: a developer-level abstraction of computer vision	simulation;gravity;pre vis	classroom use is granted without fee provided that copies are not made or distributed for commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the Owner/Author. SIGGRAPH 2014, August 10 – 14, 2014, Vancouver, British Columbia, Canada. 2014 Copyright held by the Owner/Author. ACM 978-1-4503-2960-6/14/08 OpenVL: A Developer-Level Abstraction of Computer Vision	columbia (supercomputer);computer vision;siggraph	Gregor Miller;Sidney S. Fels	2014		10.1145/2614106.2614206	computer vision;simulation;gravity;computer science;computer graphics (images)	Mobile	-51.43053737166307	-25.239513990427987	17591
98056878cba4f7cf31dc7fa45613baa8e5dcdb90	impad: an inexpensive multi-touchpressure acquisition device	bilinear;hand held device;input device;flexible;capacitive sensor;form factor;fsr;musical instruments;scaling up;pressure sensor;multi touch;sensor;lotus;pressure	Recently, there has been great interest in multi-touch interfaces. These have taken the form of optical systems such as Microsoft Surface and Perceptive Pixel's FTIR display as well as hand-held devices using capacitive sensors such as the Apple iPhone. However, optical systems are inherently bulky while capacitive systems are only practical in small form factors and are limited in their application because they only respond to human touch.  We have created a technology that enables the creation of Inexpensive Multi-Touch Pressure Acquisition Devices (IMPAD) which are paper-thin, flexible and can easily scale down to fit on a portable device or scale up to cover an entire table. These devices can sense varying levels of pressure at a resolution high enough to sense and distinguish multiple fingertips, the tip of a pen or pencil and other objects.  Other potential applications include writing pads, floor mats and entry indicators, bio-pressure sensors, musical instruments, baby monitoring, drafting tables, reconfigurable control panels, inventory tracking, portable electronic devices, hospital beds, construction materials, wheelchairs, sports equipment, sports clothing and tire pressure sensing.	3d floor plan;british informatics olympiad;computer form factor;control reconfiguration;mobile device;multi-touch;pixel;portable computer;sensor	Ilya D. Rosenberg;Alexander Grau;Charles Hendee;Nadim Awad;Ken Perlin	2009		10.1145/1520340.1520460	embedded system;simulation;bilinear interpolation;form factor;computer hardware;computer science;sensor;operating system;pressure sensor;capacitive sensing;input device	HCI	-42.55229359881889	-41.82788100001607	17623
282e21e84e08f3cfac8bdff874922cf32a75cb55	weaving versus blending: a quantitative assessment of the information carrying capacities of two alternative methods for conveying multivariate data with color.	molecular visualization;usability	In this poster we present the results of two experiments in which we seek insight into the fundamental information-carrying capacities of two alternative methods for encoding multivariate information using color: color blending and color weaving [1].We begin with a baseline experiment, in which we assess participants' abilities to accurately read numerical data encoded via one of six different single-hued color scales defined via joint variations in saturation and luminance from 6 base colors (fig 1).To obtain the base colors, we selected six points evenly spaced about a circle of large constant saturation in the monitor gamut on a plane of constant luminance in the Lab color space (figure 2).From our first experiment, we were able to determine the average baseline level of accuracy that participants were able to achieve on the color matching task when asked to match a single color.In our main experiment, we assessed participants' abilities to read combinations of 2, 3, 4 and 6 different data values simultaneously represented across a common region of the domain, encoded using either color blending, in which a single mixed color is formed via linear combination of the individual component colors, defined in Lab space (figure 3), or color weaving, in which the individual component colors appear side-by-side in a high frequency texture that fills the region (figure 4). Participants' viewing distance was restricted so that the blocks of constant color within this texture would subtend either 3 or 6 minutes of visual angle.Our results indicate that performance was significantly better when the original color information was available via the high frequency texture than when the colors were blended (fig 5), and that this difference increased with the number of components.	alpha compositing;baseline (configuration management);color space;experiment;level of measurement;numerical analysis;xfig	Haleh Hagh-Shenas;Victoria Interrante;Christopher G. Healey;Sunghee Kim	2006	IEEE Transactions on Visualization and Computer Graphics	10.1145/1179622.1179848	computer vision;color depth;usability;computer science;computer graphics (images)	Visualization	-42.344426002948225	-51.709855124509545	17675
3cd53ee21a84feca397f29e6249ba9a80580c234	a multimedia vr system	3d imaging;video tape recorders;computer graphic equipment;virtual reality;3 m multimedia virtual reality system virtual reality museum interactive theatre hdtv screen 3d images loudspeakers virtual sound video recording machines high performance computer atm network technical limitations commercially available products computer architecture computer graphics processors sound generators second generation multimedia systems 17 m 10 m;multimedia systems;atm networks;computer graphic;computer architecture;loudspeakers;three dimensional displays;computer graphic equipment multimedia systems virtual reality high definition television loudspeakers interactive systems three dimensional displays video tape recorders asynchronous transfer mode b isdn computer architecture;video recording;high performance computer;b isdn;interactive systems;asynchronous transfer mode;high definition television;multimedia systems virtual reality multimedia databases computer networks data processing solid modeling high speed networks workstations computer graphics data structures	A multzmedza center, contaznzng sax mazn research groups, was establzshed at the Unzverszty of Azzu zn 1995. For one such group, a multzmedza VR (Vzrtual Realzty) system was constructed as a V R m u seum o r znterac2ive theater zri a 17 m2 room wzth HDTV, a IO in x 3 m large screen f o r 3-dzmenszonal zmages, 17 speakers f o r zizrtunl sound, and varzous kands of vzdeo-recordzng machznes Thzs system zs controlled by a high-perfomlance computer whzch has an ATM networX. Thzs paper first presents the technzcallamztatzons of thzs system owzng to zls constructzon from a cornbznatzon of commerczally avazlable products, and then zntroduces a new archztecture f o r computang machines, computer graphzcs processors, and sound generators which has been developed at the Unzverszty of Azzu an reply to t h e demand f o r second generatzon niultzmedza systems.	atm turbo;central processing unit;simple api for xml	Tsuneo Ikedo	1996		10.1109/MMCS.1996.534947	loudspeaker;stereoscopy;simulation;computer science;operating system;asynchronous transfer mode;virtual reality;multimedia;computer graphics (images)	Robotics	-46.92633987061759	-27.15253321302737	17690
3d3af74d13f913d729f43fb9594ede6477b266cd	video prototyping in human-robot interaction: results from a qualitative study	qualitative method;technology byts ev till engineering;video prototyping;qualitative methods;robotics;human robot interaction;explication interviews;teknik;qualitative study;conference item;user experience;robotteknik och automation	Motivation -- Explore and refine qualitative methods of video prototyping in Human-Robot Interaction in order to evaluate user experience of prototype systems.  Research approach -- An exploratory, scenario based study, in which participants were interviewed following some specific guidelines regarding the interviewing technique.  Findings/Design -- The results offer insights into how the context of a presented interaction through video impacts on participants' opinions and attitudes towards a particular interaction, and foster a reflection concerning the wider implications of a system.  Take away message -- The use of evocation in open-ended interviews regarding user experience of video prototypes is a valuable tool for research.	human–robot interaction;nonlinear gameplay;prototype;status message (instant messaging);user experience	Dag Sverre Syrdal;Nuno Otero;Kerstin Dautenhahn	2008		10.1145/1473018.1473055	simulation;human–computer interaction;computer science;qualitative research;multimedia;robotics	HCI	-61.388993457517024	-44.47063560917253	17706
d78a7188d7b5b99d6f960e45cf532dd863ac81ee	rsvp browser: web browsing on small screen devices	rapid serial visual presentation;wap;space time;information presentation;navigation;small screen devices;mobile communication;small screen device;web browsing;mobile internet	In this paper, we illustrate the use of space-time trade-offs for information presentation on small screens. We propose the use of Rapid Serial Visual Presentation (RSVP) to provide a rich set of navigational information for Web browsing. The principle of RSVP browsing is applied to the development of a Web browser for small screen devices, the RSVP browser. The results of an experiment in which Web browsing with the RSVP browser is compared with that of a typical WAP browser suggests that RSVP browsing may indeed offer alternative to other forms of Web browsing on small screen devices.	television	Oscar de Bruijn;Robert Spence;Min Yih Chong	2002	Personal and Ubiquitous Computing	10.1007/s007790200024	navigation;mobile telephony;wireless application protocol;computer science;operating system;space time;multimedia;internet privacy;client-side scripting;world wide web	Web+IR	-48.127865751640144	-41.97074872401274	17715
ded061e48d0d01e62f3e8cc3f7710dc9edc1b488	e-commerce direct marketing using augmented reality	electronic commerce;electronic mail;marketing data processing;augmented reality marketing and sales cameras streaming media educational institutions hardware paper technology tracking turning time factors;turning;e commerce;paper technology;web customers;pc camera;marker motion tracking;business graphics;3d product models;interactive sales model;product model;sales agents;three dimensional;time factors;streaming media;business graphics marketing data processing augmented reality electronic commerce electronic mail;pc camera e commerce direct marketing augmented reality web customers interactive sales model sales agents promotional e mails camera calibration marker motion tracking 3d product models;camera calibration;augmented reality;e commerce direct marketing;direct marketing;promotional e mails;cameras;tracking;hardware;marketing and sales	"""Turning Web customers from """"window shoppers"""" into buyers demands an interactive sales model that informs them, gives them individualized attention, and helps to close the sale at the customer's request. Ideally, sales agents should have in-person meetings with all prospective customers. However, this may not be desirable or feasible, The next best thing is for sales agents to send promotional e-mails to their prospective customers. In this paper, we describe the development of a direct marketing system that uses augmented reality (AR) technology. A set of specially designed markers is used to calibrate the camera and track the motion of the markers for the augmentation of three dimensional product models. There is no special hardware required for this system except a PC camera (e.g., WebCam or ViCAM),."""		Xiang Zhang;Nassir Navab;Shih-Ping Liou	2000		10.1109/ICME.2000.869552	e-commerce;three-dimensional space;camera resectioning;computer science;direct marketing;tracking;multimedia;world wide web	HCI	-41.21783151627234	-37.92731762422738	17717
5f6e308ddb0de120486b5664b8abb34e94522f89	reducing latency with a continuous prediction: effects on users' performance in direct-touch target acquisitions	direct touch;continuous prediction;user performances;dragging task;latency;target acquisition	Latency in direct-touch systems creates a spatial gap between the finger and the digital object when dragging. This breaks the illusion of presence, and has a negative effect on users' performances in common tasks such as target acquisitions. Latency can be reduced with faster hardware, but reaching imperceptible levels of latency with a hardware-only approach is a difficult challenge and an energy inefficient solution. We studied the use of a continuous prediction of the touch location as an alternative to the hardware only approach to reduce the latency gap. We implemented a low latency touch surface and experimented with a constant speed linear prediction with various system latencies in the range [25ms-75ms]. We ran a user experiment to objectively assess the benefits of the prediction on users' performances in target acquisition tasks. Our study reveals that the prediction length is strongly constrained by the nature of target acquisition tasks, but that the approach can be successfully applied to counteract a large part of the negative effect of latency on users' performances.	drag and drop;interrupt latency;performance;virtual artifact	Elie Cattan;Amélie Rochet-Capellan;Pascal Perrier;François Bérard	2015		10.1145/2817721.2817736	embedded system;latency;real-time computing;simulation;engineering	HCI	-46.00004332377489	-49.67447491164565	17719
57b2a51f9b552e41dc29a5cfabf9e838beb7104b	development of a korean language-based augmentative and alternative communication application		Communication is an essential element of human interaction with a community. People with communication disorders that would otherwise impede this interaction can interface with their communities via augmentative and alternative communication. The Korean government supports these disabled people with augmentative and alternative communication devices. However, most devices require high cost and have only simple functions. Furthermore, they require space for storage. As an effort to relieve the difficulties of using such devices, this study develops an augmentative and alternative communication application that can be mounted on widely spreading smart-phones and tablet PCs.		Chang-Geol Kim;Soo-Won Kwak;Ryu Juang Tak;Byung-Seop Song	2011		10.1007/978-3-642-27201-1_48	human–computer interaction;augmentative and alternative communication;government;computer science	HCI	-55.341294790905025	-41.6294109967154	17781
ca63e5d3a0ce9a55b2cd1f2f08fe92d8fd78bca1	bottester: testing conversational systems with simulated users		Recently, conversation agents have attracted the attention of many companies such as IBM, Facebook, Google, and Amazon which have focused on developing tools or API (Application Programming Interfaces) for developers to create their own chat-bots. In this paper, we focus on new approaches to evaluate such systems presenting some recommendations resulted from evaluating a real chatbot use case. Testing conversational agents or chatbots is not a trivial task due to the multitude aspects/tasks (e.g., natural language understanding, dialog management and, response generation) which must be considered separately and as a mixture. Also, the creation of a general testing tool is a challenge since evaluation is very sensitive to the application context. Finally, exhaustive testing can be a tedious task for the project team what creates a need for a tool to perform it automatically. This paper opens a discussion about how conversational systems testing tools are essential to ensure well-functioning of such systems as well as to help interface designers guiding them to develop consistent conversational interfaces.	application programming interface;context (computing);dialog manager;dialog system;natural language understanding;test automation	Marisa Vasconcelos;Heloisa Candello;Claudio S. Pinhanez;Thiago dos Santos	2017		10.1145/3160504.3160584	conversation;application programming interface;human–computer interaction;chatbot;natural language understanding;ibm;dialog box;computer science;system testing;project team	HCI	-44.14641972860772	-29.616451086273376	17788
05ef3f5135c87146b2213c9a53ef358d177f8e77	every day for an active self-promotion: the dialogue between the shower gel packaging on the shelf of a point of sale and the consumer		When a consumer walks into a point of purchase, their eyes start to search for the product they want. This study aims to investigate into the applications of the signs on shower gel package. For the mainstream applications, we have the following findings: In terms of packaging shape, press type is the mainstream of bottle head, while the body is largely rectangular solid. In terms of color, white is the mainstream. For the text on the package, we have at least six types of major text information, such as brand name, skin improvement effect, ingredient, aroma, attribute and volume. For text presentation, original characters are the mainstream. For packaging patterns, plant is the mainstream application, and realistic depiction is another mainstream in their presentation. In terms of packaging material, shower gel bottles are mainly made of non-transparent plastics. In this study, we have conducted a more extensive research, analyzing the applications of signs on the shower gel package in the market, and will make them available to designers or marketers as a reference.	point of sale	Mu-Chien Chou;Weng-Kit Chong	2016		10.1007/978-3-319-40093-8_73	engineering;marketing;operations management;advertising	NLP	-58.24329638561231	-27.278338432945564	17807
5916a29e65ab8cbf9cc197618f8503b3dd590645	detecting objects and obstacles for visually impaired individuals using visual saliency	visual saliency;bottom up;color;real time;video processing;area of interest;stereo;visual impairment;mobility aid;visual attention	In this demo, we present the detection module of the See ColOr (Seeing Colors with an Orchestra) mobility aid for visually impaired persons. This module points out areas that present either particular interest or potential threat. In order to detect object and obstacles, we propose a bottom-up approach based on visual saliency: objects that would attract the visual attention of a non-disabled individual are pointed out by the system as areas of interest for the user. The device uses a stereoscopic camera, a laptop, and standard headphones. Given the type of scene and/or scenario, specific feature maps are computed in order to indicate areas of interest in real-time. This demonstration shows that the module indicates objects and obstacles as accurately as a system using all available feature maps.	bottom-up parsing;color;feature model;headphones;laptop;map;real-time clock;sensor;stereoscopy;threat (computer);top-down and bottom-up design	Benoît Deville;Guido Bologna;Thierry Pun	2010		10.1145/1878803.1878857	computer vision;computer science;top-down and bottom-up design;multimedia;video processing;stereophonic sound;computer graphics (images)	Vision	-40.04309193246173	-42.25086697405237	17850
8e1e3a966548baf2b8c4201136b2276da09e191d	using work-flow software to support office collaboration				Frank Cheng;Steven Guan	1993			software;human–computer interaction;flow (psychology);software as a service;computer science	HCI	-53.03032546885441	-34.147550166703454	17892
be98b3028f17a604a70c466642de54aea2e919cb	immersive painting		This paper presents a human machine interface, which helps elderly people learn how to become aware of their physical state and how to influence it. One of the biggest requirements for such a system is to provide an intuitive interface which does not overexert an elderly person, while also being easily accepted. Here, the connection of art and computer science offers the ideal outlet for such an interface. In our work, we show an user interface that is pleasant, expressive and does not look like the traditional computer interactions. We use classical biosignals, such as skin temperature or skin conductance, to get the necessary information of the physical state of the user. This information is presented to the user as individual artwork, which is created from the measured biosignals and the position of a cursor. Of course, the traditional scientific graph output is also made available. Another aspect of the system is that its design, allows its smooth integration into a normal home environment, or art studio. All necessary components are off-the-shelf, commercially available products to reduce costs and to allow a quick setup time.	computer science;conductance (graph);cursor (databases);flip-flop (electronics);interaction;requirement;user interface	Stefan Soutschek;Florian Hönig;Andreas K. Maier;Stefan Steidl;Michael Stürmer;Hellmut Erzigkeit;Joachim Hornegger;Johannes Kornhuber	2009		10.1007/978-3-642-11577-6_5		HCI	-47.24267588003087	-40.01471004438556	17898
36ee4807e4de674c172b5d47eb7ea7482ee7d7bb	concurrent engineering in maxillo-facial surgery: the role of virtual reality and rapid prototyping	virtual reality;rapid prototyping;concurrent engineering		rapid prototyping;virtual reality	Camillo Bandera;Stefano Filippi;M. Felice;M. Politi;M. Robiony;I. Salvo	2003			biomedical engineering;concurrent engineering;virtual reality;rapid prototyping;engineering	Visualization	-49.095685749961184	-31.77988944410023	17913
1669fc317e0a09546a572e444f3cde769e775628	art, interaction and engagement	art;interactive systems art;long term engagement interactive art art systems;art humans games computers cameras image color analysis color;interactive art;interactive systems;conference proceeding;chapter;art interaction	This paper reviews the development of frameworks for thinking and talking about interactive art in the context of my personal practice over the last forty years. It traces a number of paths taken, from an early simple direct notion of interaction through to communication between people through art systems and, more recently, interactive art for long-term engagement. The frameworks consist of an evolving set of concepts, over several dimensions, which are developing together with the practice of interactive art..	cybernetics;experience;interactive art;interactivity;norm (social);tracing (software)	Ernest A. Edmonds	2011	2011 15th International Conference on Information Visualisation	10.1109/IV.2011.73	visual arts;studio art;art methodology;art;aesthetics;digital art;multimedia	Visualization	-58.19874057409348	-33.82135544423419	17915
e04a64338b318b9bb92feccf480553805bc6f887	skillvis: a visualization tool for boxing skill assessment	g400 computer science;information visualization;dimensionality reduction;motion graph	Motion analysis and visualization are crucial in sports science for sports training and performance evaluation. While primitive computational methods have been proposed for simple analysis such as postures and movements, few can evaluate the high-level quality of sports players such as their skill levels and strategies. We propose a visualization tool to help visualizing boxers' motions and assess their skill levels. Our system automatically builds a graph-based representation from motion capture data and reduces the dimension of the graph onto a 3D space so that it can be easily visualized and understood. In particular, our system allows easy understanding of the boxer's boxing behaviours, preferred actions, potential strength and weakness. We demonstrate the effectiveness of our system on different boxers' motions. Our system not only serves as a tool for visualization, it also provides intuitive motion analysis that can be further used beyond sports science.	adversary (cryptography);algorithm;boxing;computation;computer animation;dimensionality reduction;expectation propagation;experiment;high- and low-level;motion capture;performance evaluation;point of view (computer hardware company);scientific visualization;statistical classification	Hubert P. H. Shum;He Wang;Edmond S. L. Ho;Taku Komura	2016		10.1145/2994258.2994266	computer vision;simulation;information visualization;computer science;dimensionality reduction;computer graphics (images)	Visualization	-38.399823767077876	-38.908895969808306	17963
037f750a131cddcd9c3f32b632f3906999cbaf55	plater: read, tidy, and display data from microtiter plates		plater defines a simple, plate-shaped file format for data storage, so it’s easy to remember the experimental design, and provides functions to seamlessly convert between that format and a tidy (Wickham 2014) data frame that’s optimal for analysis. When the instrument produces data that’s already tidy, plater helps combine that data with plateshaped experimental metadata. Once the data is tidy, it’s sometimes useful to look back at it in plate shape, so plater makes that easy, too.		S. M. Hughes	2016	J. Open Source Software	10.21105/joss.00106	computer science	OS	-44.97180396116358	-29.171392838729965	18009
6b548bc407e20cd6d5052654b2335bb09049af6f	"""empowering low-vision rehabilitation professionals with """"do-it-yourself"""" methods"""				Stéphanie Giraud;Christophe Jouffrais	2016		10.1007/978-3-319-41267-2_9		Robotics	-52.89753373779914	-35.293369331383595	18048
57a80e12e0a4f5315d4c06a9acd44ddbd98b02bd	a look at spectator technology: location-based services and mobile habits of collegiate sports fans	location based services;spectators;college sports;second screen;mobile social interaction	This paper looks at the potential for mobile technology that targets sports spectators. Preliminary survey results from 83 college sports fans suggest the current usage of mobile technology, like location-based services, when attending games. Findings offer insight as to the expectations and concerns of fans and inform the design for a social, local mobile application.	location-based service;mobile app	Jessica Torrez Riley	2012		10.1145/2371664.2371674	simulation;computer science;location-based service;multimedia	HCI	-56.344005338257055	-41.64294623421543	18106
b6a2dac36854cd2eed09039e2484d71cc15b86d8	designing for the eye: design parameters for dwell in gaze interaction	eye gaze tracking;midas touch;dwell time	Eye gaze tracking provides a natural and fast method of interacting with computers. Many click alternatives have been proposed so far, each with their own merits and drawbacks. We focus on the most natural selection method, i.e. the dwell, with which a user can select an on-screen object by just gazing at it for a pre-defined dwell time.  We have looked at three design parameters of the dwell click alternative, namely dwell time, button size and placement of content. Two experiments, with similar user interfaces, were designed and conducted with 21 and 15 participants, respectively. Different combinations of dwell times and button sizes were tested in each experiment for each participant. One experiment had content placed on the buttons to be gazed at, while the other had content placed above the buttons.  One important finding is that moving the content outside the clickable areas avoids accidental clicking, i.e. the Midas Touch problem. In such a design, a combination of big buttons and short dwell times are most suited for maximizing accuracy and ease of use, due to a phenomenon identified as the 'gaze-hold' problem.	clickable;computer;error-tolerant design;experiment;eye tracking;interaction;usability;user interface	Abdul Moiz Penkar;Christof Lutteroth;Gerald Weber	2012		10.1145/2414536.2414609	simulation;dwell time;computer graphics (images)	HCI	-46.306324361390914	-44.58010895157297	18128
5b43e5f8028d6d6d6f2d4f1fca0c594b527e1a49	user satisfaction prediction with mouse movement information in heterogeneous search environment		Satisfaction prediction is one of the prime concerns in search performance evaluation. It is a non-trivial task for three major reasons: (1) The definition of satisfaction is subjective and different users may have different opinions in the process of satisfaction judgment. (2) Most existing studies on satisfaction prediction mainly rely on users’ click-through or query reformulation behaviors but there are many sessions without such interactions. (3) Most existing works primarily rely on the hypothesis that all results on search result pages (SERPs) are homogeneous, but a variety of heterogeneous search results have been aggregated into SERPs to improve the diversity and quality of search results recently. To shed light on these research questions, we construct an experimental search engine that could collect users’ satisfaction feedback as well as mouse click-through/movement data. Inspired by recent studies in predicting search result relevance based on mouse movement patterns (namely, motifs), we propose to estimate search satisfaction with motifs extracted from mouse movement data on SERPs. Besides the existing frequency-based motif selection method, two novel selection strategies (distance-based and distribution-based) are also adopted to extract high-quality motifs for satisfaction prediction. Experimental results show that the proposed strategies outperform existing methods and have promising generalization capability for unseen users and queries in both a homogeneous and heterogeneous search environment.	event (computing);interaction;list of google products;performance evaluation;relevance;search engine results page;sequence motif;web search engine	Ye Chen;Yiqun Liu;Min Zhang;Shaoping Ma	2017	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2017.2739151	artificial intelligence;data mining;computer science;machine learning;homogeneous;federated search;feature extraction;metasearch engine;search engine	Web+IR	-35.65367519422991	-51.403834284511746	18147
e19e77bd0fc8e5245772050cf2b2679d1f48a957	evaluation of three input techniques for selection and annotation of physical objects through an augmented reality view	input devices and strategies;head cursor;input device;handheld mouse;cursor manipulation;wearable computers augmented reality human factors interactive devices mobile computing user interfaces;mobile augmented reality system;user interface technology;wearable computers;virtual reality;computer graphic;usability issue;augmented reality wearable computers head mice collaborative work virtual reality usability user interfaces computer graphics cameras;h 5 2 user interfaces;physical object annotation;human factors;user interface technology mobile augmented reality system usability issue physical object selection physical object annotation cursor manipulation handheld mouse head cursor image plane vision tracked device wearable computer input device human factor;human factor;wearable computer;three dimensional graphics and realism;augmented reality;mobile computing;physical object selection;user interfaces;mobile augmented reality;interactive devices;image plane vision tracked device	This paper presents results from a study into the usability issues of two tasks (selection and annotation of a physical object) for users operating mobile augmented reality systems. The study compared the following three different modes of cursor manipulation: a handheld mouse, a head cursor, and an image-plane vision-tracked device. The selection task was evaluated based on number of mouse button clicks, completion time, and a subjective survey. The annotation task was evaluated based on accuracy of the annotation, completion time, and a subjective survey.	augmented reality;cursor (databases);handheld game console;mouse button;usability	Bruce H. Thomas	2006	2006 IEEE/ACM International Symposium on Mixed and Augmented Reality	10.1109/ISMAR.2006.297791	computer vision;wearable computer;human–computer interaction;computer science;human factors and ergonomics;operating system;multimedia	HCI	-44.192147867813034	-46.07563816199083	18161
e675750b0aa3fd817a256c0b0037f9544e0fa877	trade-off between resolution and interactivity in spatial task performance	animacion por computador;visual resolution;task performance;vision ordenador;human interaction;systeme intelligent;realite virtuelle;image resolution;resolution spatiale;resolucion espacial;interaction;unmanned aerial vehicle;sistema inteligente;teleinformatica;virtual reality;realistic images virtual reality;conception;classical psychophysics resolution interactivity spatial task performance virtual reality displays static image quality parameters virtual environments passive camera movement intensity resolution temporal resolution;mode conversationnel;interactive mode;jeu educatif;computer graphic;experience report;computer vision;teleinformatique;temporal resolution;imagen virtual;interactive system;modo conversacional;image quality;image virtuelle;intelligent system;interactivity;educative game;diseno;realistic images;design;spatial resolution virtual reality image resolution computer displays computer graphics pixel psychology image quality stress virtual environment;vision ordinateur;interaccion;juego educativo;virtual environment;computer animation;target detection;remote data processing;virtual image;spatial resolution;animation par ordinateur	"""irtual reality displays usually lag far V behind classical computer graphics displays in static image quality parameters, such as resolution. Both the popular press and scientific papers often stress that resolution will have to increase greatly before users can experience virtual environments as """" the real Experiments comparing search-and-act spatial task performance showed that image resolution is very important in static viewing, but not in immersive VR. Nor did animating the image always improve performance. thing. """" Nevertheless, it is already possible to do some useful work in VR environments. The point we experimentally demonstrate here is that resolution is much less important for interactive tasks that employ immersive VR, where users can explore the environment by moving their heads and bodies, than it is in classical computer graphics applications , where users can only explore by gazing at a single picture. Swartz, Wallace, and Tkacz' have shown, in the context of unmanned aerial vehicles, that frame rate (read: passive camera movement) is more important than resolution for target detection, recognition, designation , and tracking. They call these results """" surprising. """" In the experiments reported here, we investigated the relative importance ofvarious image parameters like spatial resolution (number of pixels per video frame), intensity resolution (number of gray levels per pixel), and temporal resolution (number of frame updates per second) .2 Most experimental data concerning these resolutions come from classical psychophysics. However, experimental conditions in classical psychophysics feature stationary observers looking at short-term, point-like flashes on stationarydisplays, and are thus far more representative of human interaction with pictures and photographs than with highly interactive systems like those employed in virtual reality. Our senses did not develop while we were sitting still. Pepper, Cole, and Spain3 conducted experiments illustrating the importance of movement for depth perception. They showed that results for depth estimates under monocular movement parallax conditions compare well with those under stereoscopic movement par-allax conditions. When the subject can move, one eye suffices for depth perception. Interest in the study of our senses as perceptual systems has grown in recent years. For example, Gibson4 initiated the approach of studying the perceptual capabilities of human observers while they explore or perform tasks involving perceptual and motor skills. These studies, recently dubbed """" active psychophy~ics, """" ~ show much more potential for measuring human capabilities and the technical requirements to support them. Furthermore, active psychophysics has more implementation potential, as …"""	aerial photography;computer graphics;depth perception;experiment;grayscale;image quality;image resolution;importance sampling;interactivity;parallax;pixel;requirement;scientific literature;stationary process;stereoscopy;unmanned aerial vehicle;virtual reality;wallace tree	Gerda Smets;Kees C. J. Overbeeke	1995	IEEE Computer Graphics and Applications	10.1109/38.403827	computer vision;simulation;image resolution;computer science;virtual reality;computer graphics (images)	Visualization	-42.96570985527478	-50.2148796188259	18166
0465505d0d307e55f5247f7175df3d2cf1454c74	mapping art's escape from the traps of technology		The 2005 SIGGRAPH jury was more than a chance to survey the digital art scene with a roomful of passionate but collegial comrades. It was also an opportunity to reflect on the role, for better or worse, that technology is playing in the production and exhibition of digital artwork. More than any of my fellow jurors, I think I was particularly conscious of the stereotype that many artists, critics, and curators attach to exhibitions of art with a technological focus. According to this perception, the SIGGRAPH Art Gallery is less art exhibition than display showroom, where technicians show off the latest Maya or Illustrator special effect rather than pushing the boundaries of art.	ascii art;autodesk maya;siggraph;stereotype (uml)	Jon Ippolito	2005		10.1145/1086057.1086060	artificial intelligence;computer graphics (images)	HCI	-53.94742404621395	-27.890224650314035	18168
84f5d6ef3d33e696bc80bfcf34258c8ad72d50be	the design of digital handwriting forces vector ink and its application in online signature verification	digital forces vector ink;human computer interaction;handwriting recognition;signature verification;f_tablet;digital signatures;online signature verification;improved dtw algorithm;dynamic time warping algorithm digital handwriting forces vector ink online signature verification force tablet human computer interaction;f_tablet digital forces vector ink online signature verification improved dtw algorithm;time warp simulation;kinetics;dynamic time warping;ink handwriting recognition application software communication effectiveness computer interfaces human computer interaction kinematics kinetic theory heuristic algorithms iterative algorithms;time warp simulation digital signatures handwriting recognition human computer interaction	The use of pen as an effective communication interface to computer becomes an active research area in recent years. However, the lack of complete solutions for interchange between platforms has hampered its application as no efficient device can record the entire procedure of human handwriting behavior. In this paper, we design a force Tablet (F-Tablet) for human-computer-interaction (HCI), which can acquire the kinematics and kinetics information of human handwriting, including strokes of pen-up and pen-down, pen nib trajectory and three-axis forces of pen tip directly and simultaneously. Any stylus- and pen-like device can be used to write on it. The core part of the system, named as F-Tablet, is introduced and its ink was defined according to InkML format. An improved DTW (dynamic time warping) algorithm is also put forward to verify the online signatures based on the digital handwriting forces vector ink. The iterative experiment is introduced to decide weights for writing forces in different direction and the classification threshold	algorithm;dynamic time warping;fallout;human–computer interaction;iterative method;kinesiology;kinetics internet protocol;optic axis of a crystal;sony tablet p;tablet computer;type signature	ZhongCheng Wu;Fei Shen;Yong Yu	2006	2006 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2006.281951	computer vision;digital signature;speech recognition;computer science;dynamic time warping;handwriting recognition;kinetics;computer graphics (images)	Robotics	-37.71575979921278	-43.872360299748195	18178
cafa48b22a4922644f2cda2e5e9f160eb4bc5dd4	introduction to the special issue on fabrication technologies and do-it-yourself accessibility		We are pleased to present this new issue of the TACCESS journal, which contains six articles. The first two articles are part of a special issue on personal fabrication technologies and do-it-yourself accessibility. The guest editors for this special issue were Daniel Ashbrook (University of Copenhagen), Shaun K. Kane (University of Colorado), and Tom Yeh (University of Colorado), and the supervising editor-in-chief for this issue was Kathleen F. McCoy (University of Delaware). In the call for submissions to this issue, the guest editors described how personal fabrication technologies such as 3D printers, laser cutters, and computer numerical controlled (CNC) mills present opportunities for end-users to design and create new hardware and interactive devices, including new types of assistive technologies to support people with a range of accessibility needs. Many communities have formed both online and offline to support the creation of accessible technologies for people with disabilities, and people with disabilities have themselves adopted Do-ItYourself (DIY) practices to create assistive technologies to support their own unique needs. The first article, “Wear it Loud: How and Why Hearing Aid and Cochlear Implant Users Customize Their Devices,” (https://doi.org/10.1145/3214382) investigates the customization of hearing aids and cochlear implants by users through an analysis of an online community forum and interviews with stakeholders. The authors discuss the motivations and socio-cultural factors, including self-expression, which influence users’ decisions to customize their assistive technology. The second article, “Autonomous Selection and Printing of 3D Models for People Who Are Blind,” (https://doi.org/10.1145/3241066) presents and evaluates a system to enable people with visual impairments to autonomously search for and print 3D models. Through multiple user studies, the authors investigate the efficacy and accessibility of their system. We thank the guest editors for their work on this issue, the authors for their excellent submissions, and all of the reviewers for the journal who contributed their time and expertise to this process.	3d modeling;3d printing;accessibility;assistive technology;cochlear implant;numerical analysis;online and offline;online community;tom;usability testing	Matt Huenerfauth;Kathleen F. McCoy	2018	TACCESS	10.1145/3242162	human–computer interaction;computer science;fabrication	HCI	-52.10062060962537	-31.97569895456456	18181
3623d8fe01e3c214b28ccc6b72cdc07eb45ee291	a dialogue with linked data: voice-based access to market data in the sahel		The Linked Data movement has facilitated efficient data sharing in many domains. However, people in rural developing areas are mostly left out. Lack of relevant content and suitable interfaces prohibit potential users in rural communities to produce and consume Linked Data. In this paper, we present a case study exposing locally produced market data as Linked Data, which shows that Linked Data can be meaningful in a rural, development context. We present a way of enriching the market data with voice labels, allowing for the development of applications that (re-)use the data in voice-based applications. Finally, we present a prototype demonstrator that provides access to this linked market data through a voice interface, accessible to first generation mobile phones.	linked data;mobile phone;prototype	Viktor de Boer;Nana Baah Gyan;Anna Bon;Wendelien Tuyp;Chris J. van Aart;Hans Akkermans	2015	Semantic Web	10.3233/SW-130132	speech recognition;geography;advertising;communication	HCI	-54.29313346142624	-40.41043630328975	18189
b31991ec1bb001819b87362c015cc37ac739a560	development of a virtual heritage model to enable a comparison of active navigation with passive observation	unreal runtime engine;games engine;urban environment;town and country planning;design automation;built environment;virtual heritage model;timber grass roofed building;heritage;virtual architecture virtual heritage model active navigation passive observation urban redesign public participation desktop virtual model unreal runtime engine timber grass roofed building narrow passageway;passive observation;runtime environment;game engine;virtual reality;virtual architecture;computer architecture;navigation;active navigation;engines;urban redesign;passivity observer;desktop virtual model;public participation;navigation virtual architecture games engine heritage;cities and towns;space technology;architectural cad;narrow passageway;virtual reality architectural cad town and country planning;navigation context modeling space technology design methodology engines buildings design automation runtime environment cities and towns computer architecture;context modeling;buildings;design methodology	In the context of urban redesign and public participation, this research project is investigating whether active exploration of a desktop virtual model of an urban environment leads to better understanding and perception than passive observation of a walkthrough of that same model. The results from the experimental study will potentially have implications for how architects and planners present their design ideas to clients and the public in the future. This paper also provides an overview of a methodology used to design, programme and enable a fully interactive virtual model of an existing built environment using the unreal runtime engine. The method emerged from a study of heritage values within the built environment, which included an assessment of the heritage values associated with the tradition of timber grass-roofed buildings and narrow passageways that make up the old town centre of Torshavn known as Tinganes	3d modeling;desktop computer;experiment;software walkthrough;unreal;virtual heritage	Richard Laing;Stephen Scott;Anna Conniff;Tony Craig;Carlos Galan Dia	2006	Tenth International Conference on Information Visualisation (IV'06)	10.1109/IV.2006.43	navigation;simulation;design methods;computer science;virtual reality;multimedia;space technology;context model;built environment	Visualization	-36.00737632772164	-31.067721573931713	18268
dcfd8758bb8386ea73c19493a4a11e83f9e43a48	sunny day display: mid-air image formed by solar light		We propose a mid-air imaging technique that is visible under sunlight and that passively reacts to light conditions in a bright space. Optical imaging is used to form a mid-air image through the reflection and refraction of a light source. It seamlessly connects a virtual world and the real world by superimposing visual images onto the real world. Previous research introduced light emitting displays as a light source. However, attenuation of the brightness under a strong light environment presents a problem. We designed a mid-air imaging optical system that captures ambient light using a transparent LCD (liquid crystal display) and a diffuser. We built a prototype to confirm our design principles in sunlight and evaluated several diffusers.  Our contribution is three-fold. First, we confirmed the principle of the mid-air imaging optical system in sunlight. Second, we chose an appropriate diffuser in an evaluation. Third, we proposed a practical design which can remove disturbance light for outdoor use.	backlight;liquid-crystal display;polarizer;prototype;virtual world;wearable technology	Naoya Koizumi	2017		10.1145/3132272.3134137	sunlight;optical imaging;attenuation;refraction;design elements and principles;brightness;optoelectronics;liquid-crystal display;geography;optics	Graphics	-40.0967255200968	-41.448209511588686	18460
bd69f63f11c3364e95817beb023630aebdfc0f63	robust step detection in mobile phones through a learning process carried out in the mobile		In this paper we describe an strategy to obtain a robust pedometer in mobile phones through a learning process that is carried out in the mobile itself. Using the vertical component of the acceleration, dynamic time warping and data collected on the mobile, we achieve a model able to detect steps and which exhibits an important robustness to the way the mobile is being carried out. We believe this robustness is due to the fact that the model, learnt on the mobile, requires less heuristic parameters and is linked to specific characteristics of the user and the hardware. We have tested our strategy in real experiments carried out at our research centre.	step detection	R. Iglesias;Carlos V. Regueiro;Senén Barro;Guillermo Rodriguez;A. Nieto	2017		10.1007/978-3-319-59773-7_35	robustness (computer science);computer science;dynamic time warping;real-time computing;step detection;pedometer;mobile computing;heuristic	HCI	-37.393439158444764	-46.52729210202416	18512
3e3b87298e25f3dc8dc96564ab8a117a55ce7996	guided by music: pedestrian and cyclist navigation with route and beacon guidance	walking;cycling;navigation;music;spatial audio	Music listening and navigation are both common tasks for mobile device users. In this study, we integrated music listening with a navigation service, allowing users to follow the perceived direction of the music to reach their destination. This navigation interface provided users with two different guidance methods: route guidance and beacon guidance. The user experience of the navigation service was evaluated with pedestrians in a city center and with cyclists in a suburban area. The results show that spatialized music can be used to guide pedestrians and cyclists toward a destination without any prior training, offering a pleasant navigation experience. Both route and beacon guidance were deemed good alternatives, but the preference between them varied from person to person and depended on the situation. Beacon guidance was generally considered to be suitable for familiar surroundings, while route guidance was seen as a better alternative for areas that are unfamiliar or more difficult to navigate.	mobile device;user experience	Robert Albrecht;Riitta Väänänen;Tapio Lokki	2016	Personal and Ubiquitous Computing	10.1007/s00779-016-0906-z	turn-by-turn navigation;navigation;simulation;music;multimedia;cycling;mobile robot navigation	HCI	-46.772308046691435	-43.17133054345853	18513
e92601585fd150fea1394ab13613ac1a83c593dc	usabilics: avaliação remota de usabilidade e métricas baseadas na análise de tarefas	analise de comportamento do usuario;modelo de interface;avaliacao remota de usabilidade;rastreamento da interacao do usuario	The availability of tools and facilities for the creation and publishing of information on the Web has allowed people without a Web development background to build and deploy applications on the Web. In this scenario, usability evaluation tasks are rarely adopted in the application's development process. Therefore, there is an increasing interest on studies related to the automatic or semi-automatic remote evaluation of usability. This paper presents USABILICS, a system targeted for the semi-automatic remote evaluation of usability based on an interface model. The proposed model allows the definition of tasks using a simple and intuitive approach, which can be applied to large and dynamic Web applications. USABILICS analyses the execution of tasks by calculating the similarity among sequence of events produced by users and those previously captured by the developer. The results produced by USABILICS, when compared to laboratory-based tests, indicate that our approach is effective towards identifying usability problems in Web applications.	numerical aperture;usability	Leandro Guarino de Vasconcelos;Laércio Augusto Baldochi	2011			simulation;human–computer interaction;world wide web	Crypto	-62.115345483853744	-48.788038481022085	18523
49820a5500b7e6fbb8d77d642c6360857da09cef	the influence of conversational agents on socially desirable responding		Conversational agents (CAs) are becoming an increasingly common component in many information systems. The ubiquity of CAs in cell phones, entertainment systems, and messaging applications has led to a growing need to understand how design choices made when developing CAs influence user interactions. In this study, we explore the use case of CAs that gather potentially sensitive information from people—for example, in a medical interview. Using a laboratory experiment, we examine the influence of CA responsiveness and embodiment on the answers people give in response to sensitive and non-sensitive questions. The results show that for sensitive questions, the responsiveness of the CA increased the social desirability of the responses given by participants.		Ryan M. Schuetzler;Justin Scott Giboney;Mark Grimes;Jay F. Nunamaker	2018			knowledge management;computer science	HCI	-57.922360211005646	-42.689239523663375	18581
fc0fe7a833ebfef4547c2c0d6dcd61062efe7bac	data entry for mobile devices using soft keyboards: understanding the effects of keyboard size and user tasks	mobile device;personal digital assistant;touch screen;handheld computer;mobile phone;error rate;soft keyboard;experience design	As mobile, handheld computing devices become more common and are used for an ever-increasing variety of tasks, new mechanisms for data entry must be investigated. Personal digital assistants often provide a small stylus-activated soft keyboard, as do some mobile phones that include touch screens. However, there is little data regarding the importance of keyboard size or the users’ tasks, the effectiveness of these keyboards, or user reactions to these keyboards. In this article, an experiment designed to investigate these issues in the context of a palm-style QWERTY keyboard is described. In this study, 30 novices completed 6 realistic tasks using either a small, medium, or large soft keyboard. The results not only confirm that keyboard size does not affect data entry rates but that making the keyboard smaller does not increase error rates or negatively impact preference ratings. However, tasks that required users to switch between the alphabetic keyboard and the numeric keyboard do result in significantly slower data entry rates. A model that accurately predicts the time required to enter predefined text is presented, and directions for future research are discussed.	computer keyboard;desktop computer;fitts's law;futures studies;handheld game console;interaction;mobile computing;mobile phone;personal digital assistant;plover;shift jis;stylus (computing);suicidegirls;tablet computer;touchscreen;word lists by frequency;words per minute;zhi-li zhang	Andrew Sears;Ying Zha	2003	Int. J. Hum. Comput. Interaction	10.1207/S15327590IJHC1602_03	footmouse;embedded system;computer hardware;computer science;operating system;mobile device;multimedia	HCI	-47.335593591180185	-45.390657885949814	18650
37b9c98b25a40ac3c2b6f17f2b79b780a3bdd1ff	facilitating idea generation using personas	ethnographic;design tool;scenario based design;user profile;proceedings paper;materials design;persona;idea generation	Persona and scenario are important design tools for new concept development. Usually, scenario is used to generate ideas, and persona is for evaluation. This article proposes a new approach that embeds persona data in scenario-based design for idea generation. It includes a persona dataset, a facilitation process, and a working field. The persona dataset is a user profile collected in ethnographic research and categorized by subjects, motives, activities, goals and behaviors. The facilitation process helps designer create new ideas via re-matching the elements in the persona dataset. The working fields allow the freedom of implementing with different sizes of the designer team and persona dataset. This new approach provides a direct and effective way that materializes designers’ internal experiences and persona data to create new ideas and scenarios.	categorization;linkage (software);persona (user experience);user profile	Der-Jang Yu;Wen-Chi Lin	2009		10.1007/978-3-642-02806-9_44	persona;simulation;human–computer interaction;engineering;knowledge management	HCI	-61.79756313508303	-37.82870509436421	18660
43a45be204c81d6948d404a97a95e232fd605b01	cooperative navigation for mixed human–robot teams using haptic feedback	haptic interfaces robot sensing systems navigation legged locomotion visualization;psychophysics autonomous vehicles formation control haptic feedback human body tracking human robot interaction human robot team	In this paper, we present a novel cooperative navigation control for human–robot teams. Assuming that a human wants to reach a final location in a large environment with the help of a mobile robot, the robot must steer the human from the initial to the target position. The challenges posed by cooperative human–robot navigation are typically addressed by using haptic feedback via physical interaction. In contrast with that, in this paper, we describe a different approach, in which the human–robot interaction is achieved via wearable vibrotactile armbands. In the proposed work, the subject is free to decide her/his own pace. A warning vibrational signal is generated by the haptic armbands when a large deviation with respect to the desired pose is detected by the robot. The proposed method has been evaluated in a large indoor environment, where 15 blindfolded human subjects were asked to follow the haptic cues provided by the robot. The participants had to reach a target area, while avoiding static and dynamic obstacles. Experimental results revealed that the blindfolded subjects were able to avoid the obstacles and safely reach the target in all of the performed trials. A comparison is provided between the results obtained with blindfolded users and experiments performed with sighted people.	algorithm;control theory;experiment;guidance system;haptic technology;human–computer interaction;human–robot interaction;mobile manipulator;mobile robot;multi-user;programming paradigm;robotic mapping;wearable computer	Stefano Scheggi;Marco Aggravi;Domenico Prattichizzo	2017	IEEE Transactions on Human-Machine Systems	10.1109/THMS.2016.2608936	computer vision;simulation;robot;social robot;wearable computer;computer science;artificial intelligence;haptic technology;visualization;human–robot interaction;mobile robot navigation;mobile robot	Robotics	-43.49672919247217	-46.0049516441335	18661
5b390b32b977836bf4bb8691915b43e93b415642	the construction of charles babbage's difference engine no. 2	stereotypie;informatica;computers;siecle 21;century 21;building techniques;nineteenth century computing;reconstructions;engineering design;computing engines;machine a differences;history;ordinateur;nineteenth century;century 20;data processing;computer;history charles babbage difference engine contemporary drawings stereotype molds conventional printing presses;reconstitution;prehistory of computing mechanical computers computing engines charles babbage difference engines difference engine no 2 reconstructions nineteenth century computing;difference engines wheels cams finite difference methods polynomials turning gears shafts printing machinery timing;babbage c;difference engines;siecle 20;ordinateur mecanique;computers history;prehistory of computing;england;impression;computador;siglo 21;informatique;inglaterra;angleterre;mechanical computers;construcciňn;charles babbage;construction;siglo 20;difference engine no 2	Charles Babbage designed Difference Engine No. 2 between 1846 and 1848. Contemporary drawings illustrate a machine - never built during his lifetime - that calculates and tabulates polynomials, printing results in hard copy and producing stereotype molds for plates intended for use in conventional printing presses. This article describes construction of the first complete physical realization of a full Babbage engine design and outlines Babbage's ambitions for this advanced engine.	analytical engine;babbage;difference engine;polynomial;printing;stereotype (uml)	Doron D. Swade	2005	IEEE Annals of the History of Computing	10.1109/MAHC.2005.45	history;construction;data processing;mechanical computer;computer science;engineering;electrical engineering;artificial intelligence;management;algorithm;mechanical engineering	Arch	-50.3822448975393	-24.85056618978477	18670
afefc22f65f6609e17972c6b5178df5458e462c4	enabling discovery through visual exploration: an introduction to data visualization & its applications	computer society;visualization	Visual metaphors have assisted human understanding since early days of mankind; the modern scientific and social-scientific evolution especially benefits greatly from the visual medium. With increasing size and complexity of contemporary data, visualization research has focused on developing techniques for novel mathematical and visual representations to assist data exploration for intended as well as fortuitous discovery. This article introduces the reader to the field of visualization, and discusses some of its important applications.	data visualization;scientific visualization	Harsh Bhatia	2016	SIGCAS Computers and Society	10.1145/3024949.3024952	visual analytics;information visualization;visualization;human–computer interaction;computer science;data science	Visualization	-57.65322312210607	-31.927880698360166	18676
76b6968923a4a7e7cf12475420c6890b82943913	cyclingmusic & cyclingmelody: a system for enriching scenery experience in cycling by real-time synaesthetic sonification of passing landscape			real-time transcription;sonification	Masaki Matsubara;Satoshi Kuribayashi;Haruka Nukariya;Yasuaki Kakehi	2012			cognitive psychology;psychology;cycling;sonification	Embedded	-53.11199060146252	-32.28199346738702	18695
58103389e3be8c96d8dd04763acc9da05e79c34a	the invisible work of accessibility: how blind employees manage accessibility in mixed-ability workplaces	collaborative accessibility;blindness;workplace;assistive technology;vision impairment	Over the past century, people who are blind and their allies have developed successful public policies and technologies in support of creating more accessible workplaces. However, simply creating accessible technologies does not guarantee that these will be available or adopted. Because much work occurs within shared workspaces, decisions about assistive technology use may be mediated by social interactions with, and expectations of, sighted coworkers. We present findings from a qualitative field study of five workplaces from the perspective of blind employees. Although all participants were effective employees, they expressed that working in a predominantly sighted office environment produces impediments to a blind person's independence and to their integration as an equal coworker. We describe strategies employed by our participants to create and maintain an accessible workplace and present suggestions for future technology that better supports blind workers as equal peers in the workplace.	accessibility;assistive technology;field research;futures studies;interaction;web worker;workspace	Stacy M. Branham;Shaun K. Kane	2015		10.1145/2700648.2809864	simulation;knowledge management	HCI	-59.40488640267641	-41.63339960654687	18724
c3e1f38917c6294b648a7bf111227f89d895024e	tongue drive: a tongue operated magnetic sensor based wireless assistive technology for people with severe disabilities	tongue operated magnetic sensor;hall effect magnetic sensors;wireless link;small battery powered wireless mouthpiece;tongue drive;magnetic sensors;powered wheelchair;graphical user interface;drives;wireless assistive technology;permanent magnets computerised instrumentation hall effect devices handicapped aids magnetic field measurement magnetic sensors;handicapped aids;graphical user interfaces;labview environment;magnetic field measurement;hall effect devices;magnetic transducers;tongue movements;tongue;gui;femlab;severely disabled people;small battery powered wireless mouthpiece tongue drive tongue operated magnetic sensor wireless assistive technology environment control severely disabled people hall effect magnetic sensors magnetic field measurement permanent magnet sensor signals wireless link powered wheelchair tongue movements femlab graphical user interface gui labview environment;permanent magnet;sensor signals;computerised instrumentation;proportional control;switches;permanent magnets;environment control;wireless sensor networks;sensor arrays;tongue drives magnetic sensors wireless sensor networks sensor arrays magnetic field measurement permanent magnets proportional control switches graphical user interfaces	"""The """"tongue drive"""" system is a tongue-operated assistive technology developed for people with severe disability to control their environment. The tongue is considered an excellent appendage in severely disabled people for operating an assistive device. Tongue Drive consists of an array of Hall-effect magnetic sensors mounted on a dental retainer on the outer side of the teeth to measure the magnetic field generated by a small permanent magnet secured on the tongue. The sensor signals are transmitted across a wireless link and processed to control the movements of a cursor on a computer screen or to operate a powered wheelchair, a phone, or other equipments. The principal advantage of this technology is the possibility of capturing a large variety of tongue movements by processing a combination of sensor outputs. This would provide the user with a smooth proportional control as opposed to a switch based on/off control that is the basis of most existing technologies. We modeled the effects of position and orientation of the permanent magnet on the sensors in FEMLAB and experimentally measured them. We built a prototype system using off-the-shelf components and tested it successfully by developing a graphical user interface (GUI) in LabVIEW environment. A small battery powered wireless mouthpiece with no external component is under development"""	assistive technology;computer monitor;cursor (databases);experiment;graphical user interface;hall effect;labview;low-power broadcasting;power electronics;prototype;sensor;unobtrusive javascript	Gautham Krishnamurthy;Maysam Ghovanloo	2006	2006 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2006.1693892	embedded system;electronic engineering;computer science;engineering;electrical engineering;operating system;graphical user interface;quantum mechanics	Mobile	-41.313316173748234	-44.04816415489401	18732
df66d8f0be2ed014e2f1fa86d19093a62dd30afc	the zkm klangdom	ambisonics;instrument development;acousmatic music;wave field synthesis;vector based additive panning vbap;spatial audio;sound spatialization	The Klangdom is an audio spatialization instrument developed at the Institut für Musik und Akustik at the ZKM. It is made up of 39 Meyer Sound loudspeakers hung on four sliding tracks, allowing for easy re-configuration of the speaker setup. The audience sits inside the Klangdom, which can be controlled either directly via a mixer, by externally developed software, or by a sequencer for sound movement, Zirkonium, developed at the ZKM. Zirkonium can accept and spatialize audio generated by other applications (even on remote machines) and can simulate the Klangdom over alternate speaker setups to aid composition and dissemination (e.g., in stereo or 5.1).	loudspeaker;microsequencer;simulation	Chandrasekhar Ramakrishnan;Joachim Goßmann;Ludger Brümmer	2006			ambisonics;speech recognition;acoustics	HCI	-52.35791261446342	-25.524581257814983	18741
5807bdaf50aee7f4a41c1418c9c2f766ec0a797e	evaluation of a haptic-based interaction system for virtual manual assembly	haptic based interaction;force feedback;system evaluation;vr system evaluation;virtual manual assembly;interactive system;mixed reality;mechanical systems;low cost technology;haptic interface	This paper describes a mixed reality application for the assessment of manual assembly of mechanical systems. The application aims at using low cost technologies and at the same time at offering an effective environment for the assessment of a typical task consisting of assembling two components of a mechanical system. The application is based on the use of a 6-DOF interaction device that is used for positioning an object in space, and a haptic interface that is closer to reality and is used for simulating the insertion of a second component into the first one while feeling a force feedback. The application has been validated by an expert user in order to identify the main usability and performance problems and improve its design.	haptic technology	Monica Bordegoni;Umberto Cugini;Paolo Belluco;Marcello Aliverti	2009		10.1007/978-3-642-02771-0_34	embedded system;simulation;human–computer interaction;engineering	Visualization	-41.49855278202754	-46.39182891346217	18766
c9c3d8d93c2a4b94b015754d7a29e314b7f6a0dd	business: trends in future web designs: what's next for the hci professional?	web design	One thing is certain—the amount of information available on the Web continues to grow at a dizzying pace. For a Web site to add significant value to the user, it must provide an overview of accessible information to all users of varying Web expertise. This article attempts to outline many of the new interaction trends for information management that can now be observed on the World Wide Web. This overview was prepared with an eye toward attempting to ferret out research techniques and methods that might be most advantageous to a HCI (human–computer interaction) professional working on future Web designs. For the sake of brevity, we focus on a subset of Web designs that we feel is indicative of what will catch on in the future. Our approach has been to collect, study, and analyze critically a large set of these new Web browsers, along a variety of interaction dimensions. The dimensions were generated from our research on both novel HCI techniques and technological breakthroughs (hardware and software). To leverage these new interaction and technology trends, it is critical that any new Web design be of high usability in order to garner the widest possible acceptance. To that end, once the important interaction dimensions of a new Web design are identified, it should be a less daunting task to put forth a research agenda evaluating its ease of use. All of the new Web interaction techniques discussed in this article have one theme in common: They all try to present a very large set of information to the user in a format that allows the user to (automatically or preatten-tively) recognize patterns in the data easily. In other words, the new Web designs are an attempt to leverage the human's perceptual system in order to ease the burden of manually organizing or cognitively processing the displayed information. One implication of this commonality across the designs is that the HCI professional working on future browsers must be well versed in human perception and cognition. In addition, the professional needs to have a keen understanding of future technological breakthroughs or to have the luck of being on a multidisciplinary team that includes that expertise. The following sections identify the trends and techniques that could be increasingly prevalent over the next few years. Effectively shunting the burden of sensemak-ing during browsing Web documents from the cognitive to the perceptual system is a great …	cognition;human–computer interaction;information management;interaction technique;organizing (structure);usability;web design;world wide web	Mary Czerwinski;Kevin Larson	1998	Interactions	10.1145/287821.287824	web design;human–computer interaction;web standards;computer science;engineering;knowledge management;multimedia;web intelligence	HCI	-56.85168358011912	-32.63071176459332	18801
a0ff3096e7abbbeaa368f1f4a9b1662d5e0bfd87	challenges in developing a collaborative robotic assistant for automotive assembly lines	human robot interaction;industrial robots;article	Industrial robots are on the verge of emerging from their cages, and entering the final assembly to work along side humans. Towards this we are developing a collaborative robot capable of assisting humans in the final automotive assembly. Several algorithmic as well as design challenges exist when the robots enter the unpredictable, human-centric and time-critical environment of final assembly. In this work, we briefly discuss a few of these challenges along with developed solutions and proposed methodologies, and their implications for improving human-robot collaboration.	algorithm;cobot;human–robot interaction;robot;the verge;window of opportunity	Vaibhav V. Unhelkar;Julie A. Shah	2015		10.1145/2701973.2702705	human–robot interaction;simulation;computer science;artificial intelligence	Robotics	-36.57801541176581	-40.32064884647157	18802
95086944e371967908f3bbfcbd2459b836803a83	e-comm, un éditeur pour spécifier l'interaction multimodale et multiutilisateur	groupware;design tool;direct manipulation;multi user;specification notation;graphic user interface;multimodal system;multimodal interaction	Addressing this issue of lack of design tools for multi-user multimodal systems, we present the COMM (Collaborative and MultiModal) notation and its on-line editor for specifying multi-user multimodal interactive systems. Extending the CTT notation, the salient features of the COMM notation include the concepts of interactive role and modal task. The e-COMM graphical user interface emphasizes direct manipulation of the COMM concepts in order to avoid forms, menus or toolbars as much as possible. Such UI should encourage focus on the specification under development.	direct manipulation interface;graphical user interface;line editor;modal logic;multi-user;multimodal interaction;online and offline	Frédéric Jourde;Yann Laurillau;Laurence Nigay	2010		10.1145/1941007.1941048	human–computer interaction;computer science;multimedia;world wide web	HCI	-41.91243478604759	-29.138590943460347	18820
d7d18dd699bc46b0b133448e455e3cf0ace9540c	modernized latvian ergonomic keyboard		"""Increasingly more people use computers and create content using keyboards (even with leading edge touch-screen technology). As in the most part of the world, in Latvia also conventional """"Qwerty"""" keyboard is used. Though for Latvian it is much worse than for English, especially due to enormous load to little fingers. It causes repetitive strain injuries and affects productivity of workers with extensive keyboard usage, especially for data input operators, call centers, inquiry office workers, etc. Improving computer keyboard layout decrease stress to hands and fingers thus minimizing exhaustion and injuries. With analysis of English and Latvian public domain novels and modern texts, letter appearance an sequence distribution for Latvian language was found. Qualities of alternative layouts for English (Dvorak, Colemak, Hallinstad) were investigated and open source carpalx simulation tool was adjusted according to the findings. Then carpalx was used to check more than 25 million keyboard layouts, measuring finger/hand effort, stroke typing convenience etc., to find the best one. It was proved that existing """"Šusildatec"""" (classic Latvian Ergonomic standard) keyboard is only slightly better than """"Qwerty"""" for Latvian, though it is much worse for English. After computer simulation, several best layouts were tried practically for more than 6 months and most convenient one was promoted as a new """"Latvian Modern"""" keyboard. Its typing effort is less than for """"Šusildatec"""", load is distributed according to finger strength, and typing strokes are alternating better between hands and fingers. Comparing to """"Qwerty"""" keyboard new layout is better not only for Latvian but for English also. Keyboard drivers are developed for Microsoft Windows and Linux operating systems and are freely available in the web under permissive license."""	computer keyboard;computer simulation;dvorak simplified keyboard;ergonomic keyboard;human factors and ergonomics;linux;microsoft windows;open-source software;operating system;plover;repetitive strain;touchscreen	Valdis Vitolins	2011	CoRR		license;human–computer interaction;typing;public domain;microsoft windows;human factors and ergonomics;keyboard layout;latvian;computer science	HCI	-47.78310367556934	-45.022358476399496	18821
19c54e0da3e18d3c7ed230077a8696d5ad6c7222	embodied learning using a tangible user interface: the effects of haptic perception and selective pointing on a spatial learning task	embodied cognition;cognitive load theory;interactivity;virtual and physical learning environment;human computer interface	Tangible User Interfaces offer new ways of interaction with virtual objects, yet little research has been conducted on their learner-friendly design in the context of spatial learning. Although frameworks such as Embodied Cognition stress the importance of sensory perception and movement, studies have found that high interactivity can be overwhelming and may lead to a lower learning performance. In a 2 2 factorial design participants (n 1⁄4 96) learned heart anatomy using a 3D model that was either controlled using a mouse or a tangible object, i.e. a motion tracked plastic model of the virtual heart. Secondly, we varied the interaction mode featuring either a selective pointing mode in which only the label that the user currently activated was displayed or permanent display of all labels. Retention performance, cognitive load scores, and motivation measures indicate that the tangible object leads to significantly higher learning outcomes. The effect of the label display mode is different for the two input devices: The performance with selective pointing in the mouse condition is better than the performance with permanent display in the mouse condition; in the TUI condition this is exactly the other way around. Based on these results, we propose extensions for Embodied Cognition and Cognitive Load	computer display standard;embodied cognition;input device;interactive media;interactivity;tangible user interface	Alexander Skulmowski;Simon Pradel;Tom Kühnert;Guido Brunnett;Günter Daniel Rey	2016	Computers & Education	10.1016/j.compedu.2015.10.011	psychology;simulation;computer science;embodied cognition;multimedia;cognitive load;interactivity;communication;cognitive robotics	HCI	-46.41703366241158	-48.870244144159635	18852
fc6c40806ed73e59b7d3e5c698b989316b0e7e13	mixed reality. from rendering to gaming with pets	mixed reality		mixed reality	Michael J Haller;Huagen Wan	2006	IJVR		human–computer interaction;rendering (computer graphics);multimedia;computer-mediated reality;mixed reality;computer science	Visualization	-48.711910978719544	-33.22853846628894	18858
6f8a0ed761a6b260dee2dbb0cae24ba5ca595b5b	flipper: a new method of digital document navigation	digital documents;rapid serial visual presentation;sdaz;user study;flipping;scrolling;visual search;rsvp;speed dependent automatic zooming	Page flipping is an important part of paper-based document navigation. However this affordance of paper document has not been fully transferred to digital documents. In this paper we present Flipper, a new digital document navigation technique inspired by paper document flipping. Flipper combines speed-dependent automatic zooming (SDAZ) [6] and rapid serial visual presentation (RSVP) [3], to let users navigate through documents at a wide range of speeds. It is particularly well adapted to rapid visual search. User studies show Flipper is faster than both conventional scrolling and SDAZ and is well received by users.	automatic dependent surveillance – broadcast;multiple buffering;robot combat;scrolling;usability testing	Liyang Sun;François Guimbretière	2005		10.1145/1056808.1057077	scrolling;computer vision;visual search;computer science;multimedia;world wide web;computer graphics (images)	HCI	-37.392910782046926	-48.55768494373271	18882
fb5f6f6e79dcd10b4908113710fcd058dd39dceb	navigation system for the visually impaired individuals with the kinect and vibrotactile belt	vibrations;sensors;belts;arrays;roads;global positioning system	This paper presents an efficient navigation system which presents the navigation information to the visually impaired via Kinect and vibrotactile belt. The system consists of three parts. The first part is Kinect, which is used for obstacles detection. The second part is a laptop, the laptop processes the depth data of the context acquired by the Kinect, and then translates safe guidance hint into vibration information. The system also combines the function of GPS utilizing the speech recognition module and Google Maps. The third part is the vibrotactile belt which is fixed on the waist. Once wearing the belt, the visually impaired can accurately “feel” the direction prompt with the 8×2 vibration motors array. The laptop, Kinect and other elements are mounted on a walking aid. The visually impaired can walk independently with the support of the walker. Two experiments are conducted to test the performance of the system. The results show that the system is able to identify the obstacles in front and give the visually impaired corresponding prompt. It could make the visually impaired walking in a larger range independently and more safely.	amiga walker;experiment;global positioning system;kinect;laptop;speech recognition	Lu Wang;Na Li;Dejing Ni;Juan Wu	2014	2014 IEEE International Conference on Robotics and Biomimetics (ROBIO 2014)	10.1109/ROBIO.2014.7090609	embedded system;computer vision;simulation;global positioning system;engineering;sensor;vibration	Robotics	-43.198321677824914	-43.640051624096	18922
ee803b1c936dbaaf0541254454ee2ebed2e611b9	information retrieval in the office environment	information retrieval system;information retrieval;user requirements	The question of information retrieval in the office environment must be considered in two contexts: personal and organizational. In the personal context the issue is increasing individual effectiveness and productivity; in the organizational context it is one of increasing an organization's overall effectiveness and productivity, partially by improvements at the level of the individual but primarily by improvements to the systems and procedures through which individuals work together. The end user requirements of the two contexts are quite different, a fact which must be taken into account when designing information retrieval systems for the office environment.	information retrieval;requirement;user requirements document	M. T. Pezarro	1983		10.1145/800173.809735	relevance;cognitive models of information retrieval;computer science;knowledge management;multimedia;information retrieval;human–computer information retrieval	Web+IR	-60.27778032074409	-42.4869845880918	18926
b9fd80e16ab5a9a05f05362072df9d4ec08df8a8	the agnes system for ambient social interaction	social isolation;quality of life;sensing;user states;human computer interaction;manniska datorinteraktion interaktionsdesign;home based;user sensitive;engineering and technology;information systems social aspects;teknik och teknologier;ageing;ambient;sensor fusion;systemvetenskap informationssystem och informatik med samhallsvetenskaplig inriktning	This paper describes the AGNES system, a technology for connecting people at home with their social network be means of off-the-shelf sensing devices and an Internet-based social network. The system provides the user with relevant information of the network by use of traditional information and communication technology as well as dedicated ambient and tangible devices. We describe briefly the requirements on such technology that were elaborated in the frame of the AGNES project, and give an overview of the system developed.	display resolution;exponential hierarchy;information;internet;letter-quality printer;limewire;logical volume management;rs-232;requirement;social network	Christian Peter;Andreas Kreiner;Martin Schröter;Gerald Bieber;John A. Waterworth	2012		10.1145/2413097.2413151	ageing;simulation;quality of life;human–computer interaction;computer science;sensor fusion	HCI	-53.34775573702881	-35.91561799573365	18938
216c6eeab9ace9b878a22292fc2c1261e2f9c9ff	capturing the response of players to a location-based game	location mobile devices games;mobile device;bepress selected works;mobile phone;multimodality;location based interaction;emotion;games;location based games;mental model	Location-based games offer opportunities for us to learn more about people’s interactions and feelings towards the environment they are in as well as to understand more about the mental models and locations associated with known environments, e.g. a university campus with its associations of learning. In our study, we wanted to manipulate the activities in a game to take advantage of certain locations in the hope of producing certain emotional reactions. However, it is not enough to simply produce these reactions; one must also have a way of capturing any emotions produced whether these are the ones expected or not. The objective of this paper, therefore, was to trial a new methodology for location-based games that aims at capturing the players’ emotional reactions to the activities in a game whilst in certain locations. In order to test the methodology, we designed a location-based game that can be played on any Bluetooth-enabled mobile phone that has an accelerometer. The game has been designed to interweave with a persons’ normal activity. As a result, there is little distinction between gaming time and non-gaming time.	bluetooth;emotion markup language;interaction;iteration;location-based game;mental model;mobile app;mobile phone;scroll wheel;while	Lynne Baillie;Lee Morton;David C. Moffat;Stephen Uzor	2010	Personal and Ubiquitous Computing	10.1007/s00779-010-0309-5	non-cooperative game;games;simulation;emotion;computer science;operating system;game mechanics;mobile device;multimedia;screening game;sequential game	HCI	-55.50533045542073	-42.551692133586776	19004
0052823c4843c30e61dea044c95d331e00a948f6	the making of performativity in designing [with] smart material composites		As the material becomes active in disclosing the fullness of its capabilities, the boundaries between human and nonhuman performances are destabilized in productive practices that take their departure from materials. This paper illuminates the embodied crafting of action possibilities in material-driven design (MDD) practices with electroluminescent materials. The paper describes and discusses aspects of the making process of electroluminescent materials in which matter, structure, form, and computation are manipulated to deliberately disrupt the affordance of the material, with the goal to explore unanticipated action possibilities and materialize the performative qualities of the sample. In light of this account, the paper concludes by urging the HCI community to performatively rupture the material, so to be able to act upon it as if it was always unfinished or underdeveloped. This, it is shown, can help open up the design space of smart material composites and reveal their latent affordances.	biconnected component;computation;denial-of-service attack;emergence;human–computer interaction;model-driven engineering;performance;smart pascal;social affordance;steam rupture	Bahareh Barati;Elisa Giaccardi;Elvin Karana	2018		10.1145/3173574.3173579	human–computer interaction;multimedia;affordance;performativity;computation;performative utterance;making-of;smart material;computer science;embodied cognition;composite material	HCI	-59.98350940391704	-34.74628966806274	19006
70fa5f2e3fa384dc934b9e31a6126383f150ddf9	haptic training in a virtual environment to train cognitive functions of medical students: work in progress		This paper introduces the development of exercises to be embedded in a lightweight laparoscopic haptic simulator to help surgeons starting their training to Minimal Invasive Surgery (MIS) gestures. These exercises were created by observing professionals in operation rooms and by isolating key gestures, which have been combined to create desired trajectories with a slow learning curve. These exercises combine memory, new gestures, new environments and new visual feedback so that the trainees’ cognitive load remains low. This favors an effective training. Hence, the simulator displays a simple 3D virtual environment in order to focus on the gestures and trajectories, performed on an haptic device by means of real MIS tool handles. Its ludic dimension, which make it a Serious Game, should help users to make progress in their first gesture training in order to continue on more evolved medical simulators. This paper introduces the software architecture analysis and the methods used for creating the exercises.		Nemanja Babic;Charles Barnouin;Benjamin De Witte;Arnaud Lelevé;Richard Moreau;Minh Tu Pham;Xavier Martin	2018		10.1007/978-3-030-04375-9_10	computer science;work in process;systems engineering;simulation;haptic technology;cognition;virtual machine;software architecture;virtual reality;gesture;cognitive load	HCI	-51.756683685891545	-44.67271214628444	19023
ecb6686b6e74ab818c609e190149eab9cb67d96f	user interface for interaction with heterogeneous vehicles for cyber-physical systems	cyber physical systems;marine vehicles;three dimensional displays;user interfaces;vehicle dynamics;robot kinematics	"""This paper provides a brief overview of recent research activities by Mobile & Marine Robotics Research Centre (MMRRC) research team focused toward development of user interfaces for interaction with heterogeneous vehicles in smart Cyber-Physical Systems (CPS). The system, which is currently in development, include both classical user interface elements but also a set of novel """"natural"""" multi-modal user interfaces based on exploration and integration of the state-of-the-art emerging technologies, including Virtal Reality (VR) headsets, spatial tracking devices, 6 DoF input devices, hand gesture recognition devices and touch screens."""	cyber-physical system;gesture recognition;input device;modal logic;robotics;touchscreen;user interface;virtual reality	Edin Omerdic;Daniel J. F. Toal;Zoran Vukic	2016	2016 14th International Conference on Control, Automation, Robotics and Vision (ICARCV)	10.1109/ICARCV.2016.7838747	user interface design;vehicle dynamics;simulation;human–computer interaction;computer science;engineering;artificial intelligence;multimedia;natural user interface;cyber-physical system;user interface;robot kinematics	Robotics	-37.550898433209035	-41.24452941172776	19156
93fcd808dcb66824c82102d5edc9b1809f61009b	quill: birds of a feather tool	level of detail;multiperspective rendering;exaggeration;animated character	Legend of the Guardians: The Owls of Ga'Hoole (LotG) features over sixty distinctive, art-directed, hyper-realistic feathered characters. We developed a procedural feathering pipeline, Quill, to efficiently realise more than fifteen unique bird species required by the story. This toolset allows a procedural representation of feathers to be developed by surfacing artists, augmented with automatic deintersection, animation, character effects and dynamics and rendered with extensive level-of-detail support.	feathering;level of detail	Daniel Heckenberg;Damien Gray;Bryan Smith;Jonathan Wills;Chris Bone	2011		10.1145/2037826.2037871	level of detail;computer graphics (images)	Graphics	-37.768173516349954	-32.75769338081373	19185
58f7fce31b9efdd837ed6828f6ca93b0858400c2	understanding social media engagement: have expectations exceed results?	media use;media sharing;social media	One of the original applications of the analysis of social media use was to find a 'back door' through which we (as a community) could obtain insights in the motivations and interactions of users and producers of content. By studying when, where and how often ancillary social content was created, and by studying when, where and how often that social content was received (and resent), valuable new insights might be obtained that could help us understand the importance of social media and social interactions. Now that several years have passed in which Twitter streams, video and photograph metadata, and topic popularity have been monitored, it is time to pause and to ask the question: have the results of this analysis provided valuable new insights into the users of media content (and their behavior). This keynote presentation will survey several important developments in the recent history of monitoring social interactions while using media, and it will try to draw some initial conclusions on what we have learned from monitoring users of interactive media.	interaction;interactive media;social media	Dick C. A. Bulterman	2013		10.1145/2509916.2509919	media relations;social media;computer science;media system dependency theory;multimedia;internet privacy;world wide web	Web+IR	-55.39556604203932	-41.70397484207887	19266
f3db45b87d330c6e8ee0cc890a4838206b8d5fc5	projection-based city atlas: an interactive, touchless, virtual tour of the urban fabric of ascoli piceno	geography augmented reality cartography;political periods projection based city atlas interactive touchless virtual tour urban fabric ascoli piceno spatial arrangement spatial augmented geographycal representation touchless interaction system content design process italy historical periods;prototypes;cartography spatial augmented reality 3d model physical model urban fabric projection;three dimensional displays;solid modeling;fabrics;cities and towns;augmented reality;cities and towns fabrics three dimensional displays solid modeling prototypes buildings augmented reality;buildings	This proposal is aimed at illustrating a low cost exhibit of a projection-based city atlas. The exhibit could be replicated and reconfigured in accordance with the size of the mock-up and the spatial arrangement of the place where it will be located. The paper will be divided into different sections in order to explain and illustrate: the state of the art and previous experiences about spatial augmented geographycal representation; the technical issues regarding the projection set-up and the technological framework related to the touchless interaction system; the content design process. Within this framework, the experimentation has been carried out on the urban area of Ascoli Piceno (Italy). Based on the description of events occurring over time, the interactive virtual tour aims to highlight how the urban fabric has changed throughout the historical and political periods that have affected the city.	augmented reality;browsing;desktop computer;emergence;hdmi;interaction;internet;map;mock object;planimetrics;prototype;self-replicating machine;video projector;virtual tour	Daniele Rossi;Enrica Petrucci;Alessandro Olivieri	2014	2014 International Conference on Virtual Systems & Multimedia (VSMM)	10.1109/VSMM.2014.7136672	computer vision;augmented reality;simulation;prototype;solid modeling;computer graphics (images)	HCI	-50.68484263398055	-29.082604212866332	19270
f7de38d8f00916d90bd2dc0cf7727a0db3bda4ac	left-over windows cause window clutter... but what causes left-over windows?	window logger;window clutter;left over windows	Sleep mode lets users go for days or weeks without rebooting, supporting work on multiple tasks that they can return to later. However, users also struggle with window clutter, facing an increasing number of `left-over windows' that get in the way. Our goal is to understand how users create and cope with left-over windows. We conducted a two-week field study with ten notebook users. We found that they work in very short sessions, switching often between computer-based and external tasks. 34% of left-over windows remain untouched for a day or more, increasing in quantitity until they all disappear after a reboot. Some users reboot as a deliberate `clean-up' strategy, whereas others lose left-over windows after an unexpected system crash. Users intentionally keep left-over windows as to-do lists, as reminders of upcoming tasks, and for facilitating future access; the rest are simply forgotten. Tools for visualizing and managing left-over windows should help users reduce window clutter, while maintaing the benefits of interruptible work sessions.	booting;clutter;field research;microsoft windows;sleep mode	Julie Wagner;Wendy E. Mackay;Stéphane Huot	2012		10.1145/2652574.2653442	embedded system;real-time computing;engineering;operating system	HCI	-52.26744649585498	-43.60155981551289	19290
ef7077b4ebe6b44b465a2d8505842aa2c1f8e109	spotlights: attention-optimized highlights for skim reading	scrolling techniques;attentional blink;attention brokering;skim reading;visual attention;comprehension	The paper contributes a novel technique that can improve user performance in skim reading. Users typically use a continuous-rate-based scrolling technique to skim works such as longer Web pages, e-books, and PDF files. However, visual attention is compromised at higher scrolling rates because of motion blur and extraneous objects with overly brief exposure times. In response, we present Spotlights. It complements the regular continuous technique at high speeds (2--20 pages/s). We present a novel design rule informed by theories of the human visual system for dynamically selecting objects and placing them on transparent overlays on top of the viewer. This improves the quality of visual processing at high scrolling rates by 1) limiting the number of objects, 2) ensuring minimal processing time per object, and 3) keeping objects static to avoid motion blur and facilitate gaze deployment. Spotlights was compared to continuous scrolling in two studies using long documents (200+ pages). Comprehension levels for long documents were comparable with those in continuous-rate-based scrolling, but Spotlights showed significantly better scrolling speed, gaze deployment, recall, lookup performance, and user-rated comprehension.	book;e-book;gaussian blur;human visual system model;list comprehension;lookup table;portable document format;scrolling;smart common input method;software deployment;theory;web page	Byungjoo Lee;Olli Savisaari;Antti Oulasvirta	2016		10.1145/2858036.2858299	scrolling;comprehension;computer science;multimedia;attentional blink;world wide web;computer graphics (images)	HCI	-37.605187734310384	-48.4985889791282	19294
9866694d56e51703c4ecec9e2980689a48163d51	videotater: an approach for pen-based digital video segmentation and tagging	video tagging;video segmentation;tablet pc;human factors;digital video	The continuous growth of media databases necessitates development of novel visualization and interaction techniques to support management of these collections. We present Videotater, an experimental tool for a Tablet PC that supports the efficient and intuitive navigation, selection, segmentation, and tagging of video. Our veridical representation immediately signals to the user where appropriate segment boundaries should be placed and allows for rapid review and refinement of manually or automatically generated segments. Finally, we explore a distribution of modalities in the interface by using multiple timeline representations, pressure sensing, and a tag painting/erasing metaphor with the pen.	database;digital video;interaction technique;refinement (computing);tablet computer;timeline	Nicholas Diakopoulos;Irfan A. Essa	2006		10.1145/1166253.1166287	computer vision;computer science;human factors and ergonomics;multimedia;computer graphics (images)	HCI	-46.019630936976135	-41.0176581212585	19324
820c0b13f213d1f5004e0a898013589f97109561	developing a university wikipedia	database;ios;web service;iphone;qr code;php;lookup;quick response;information	This project stems from an idea to design and implement an iOS application that can scan Quick Response (QR) codes to look up information about locations around a university's campus. The application allows users to post various types of media including images, text and video associated with a specific location from the application. This project has three main components. First is the backend server with an SQL database and PHP web services to interact with the database. Second is a web application that users can access to post information and to look up posts and locations that are available. Also, the web application allows the logging in of administrators who can manage the system. The third and last component is the iOS application that connects to the server primarily to retrieve but also to post information.	image;login;php;qr code;sql;server (computing);web application;web service;wikipedia;ios	Douglas Edmonson	2012		10.1145/2184512.2184613	computer science;operating system;database;world wide web	Web+IR	-44.04915839640915	-24.504691868518456	19325
b9ad1190b8462fa88be2103d865ac0756d4a17d8	indirect 2d touch panning: how does it affect spatial memory and navigation performance?	indirect touch;embodied cognition;spatial memory;user study;panning;navigation;touch;multi touch	We present experimental work which explores the effect of touch indirectness on spatial memory and navigation performance in a 2D panning task. In this regard and based on the theory of embodied cognition, prior work has observed performance increases for direct touch input over indirect mouse input. As indirect touch systems gain in importance, we designed an experiment to systematically investigate the effect of spatial indirectness while maintaining the proprioceptive and kinesthetic cues provided by touch input. In an abstract search task, participants of our study navigated a 2D space and were asked to reproduce spatial item configurations in a recall task. Our results indicate that spatial memory performance is not decreased by a spatial separation of touch input gestures and visual display. Further, our results suggest that decreasing the size of the input surface in the indirect condition increases the navigation efficiency.	computer mouse;embodied cognition	Henri Palleis;Heinrich Hußmann	2016		10.1145/2858036.2858334	spatial memory;computer vision;navigation;computer science;embodied cognition;multimedia;panning	HCI	-45.97851085906667	-47.82465460242253	19342
95f0e810dce0522d29cbfdf879cef689f3a20bb7	mountain guitar: a musical instrument for everyone	musical expression;intuitive interaction;interactive music;musical instruments;physical computing;guitar instrument;midi to sensor mapping	"""This instrument is a part of the """"Gangu Project"""" at IAMAS, which aim to develop digital toys for improving children's social behavior in the future. It was further developed as part of the IAMAS-Interface Cultures exchange program.  """"Mountain Guitar"""" is a new musical instrument that enables musical expression through a custom-made sensor technology, which captures and transforms the height at which the instrument is held to the musical outcome during the playing session. One of the goals of """"Mountain Guitar"""" is to let untrained users easily and intuitively play guitar through their body movements. In addition to capturing the users' body movements, """"Mountain Guitar"""" also simulates standard guitar playing techniques such as vibrato, choking, and mute. """"Mountain Guitar's"""" goal is to provide playing pleasure for guitar training sessions. This poster describes the """"Mountain Guitar's"""" fundamental principles and its mode of operation."""	bass amplifier;block cipher mode of operation;guitar pro;mute;toys	Junichi Kanebako;James Gibson;Laurent Mignonneau	2007		10.1145/1279740.1279833	simulation;acoustics;computer science;multimedia;physical computing	HCI	-47.27768938481954	-36.09552746944059	19354
bd04bae52c716b7ecc1ca74583755e402967ebe7	book review: being there together: social interaction in virtual environments	social interaction;virtual environment	Virtual reality is a curious cultural phenomenon with a long-standing position in popular culture in the form of science fiction depictions and actual services. Yet despite continuous technological improvements of different types of multiuser virtual environments (MUVEs), massively multiplayer online role-playing games are the only really successful mainstream services. Virtual social environments such as Active Worlds and Second Life remain environments for the geeky, and immersive virtual environments still come across as nascent technologies. In Being There Together Ralph Schroeder looks back at where we have been, provides a comprehensive overview of research findings of current telepresence practices, and predicts the future of MUVEs. The result is a convincing argument that MUVEs will play an increasingly important role for telepresence collaboration and sociability. Schroeder acknowledges the importance of technological affordances, yet his most important thesis throughout the book is that the emphasis should be on understanding human beings and their interactions in virtual environments rather than on what is technologically possible. Accordingly, Being There Together has a stronger focus on how people comprehend and cope with the affordances of different MUVEs than on the affordances as such. Schroeder defines virtual reality as computer-generated environments that compel the user to have a feeling of being present in that environment, and he defines MUVEs as systems ‘in which users experience other participants as being present in the same environment and interacting with them – or as “being there together”’ (p. 4). This latter definition emphasizes the subjective experience as the crucial point, omitting technological characteristics. As is common in MUVEs-research, different types of technologies are included: from head-mounted displays (HMD) systems and immersive projection technology (IPT) to desktop online worlds. Schroeder foresees two end-points for the development of technologies for being there together: immersive VEs (computer-generated embodiments) and immersive video-environments (3D video capture of people and scenes/videoconferencing). The book cuts across scholarly disciplines and presents research findings and theories from human–computer interaction, social psychology, symbolic interactionism, media	360-degree video;active worlds;bandwagon effect;computer-generated holography;desktop computer;geek;head-mounted display;human–computer interaction;information processes and technology;massively multiplayer online role-playing game;multi-user;needham–schroeder protocol;second life;theory;virtual reality;virtual world	Marika Lüders	2011	New Media & Society	10.1177/1461444811417084	human–computer interaction	Visualization	-59.075988825777145	-33.67266138015805	19362
1f4785d051f9b2aa58024dbe60962abef4c3cefc	organization, communication, and control in the galaxy-ii conversational system	control system;scripting language	Galaxy-ii is the designated initial common architecture for the DARPA Communicator project in the U.S. Its key feature is the ability to control system integration via a run-time executable scripting language. This paper describes our experience in developing complex systems based on the galaxy-ii framework. Our current system consists of four domains and two languages (English and Mandarin). Users can interface with the system in both displayful and displayless modes. Users can switch freely among the various domains in a single conversation, and multiple users can access the system in simultaneous conversations. In addition to the hub script that controls live i n teraction with users, we h a ve also conngured many other hub scripts that permit various batchmode runs, including the capability to reprocess log les through improved versions of the system to measure progress. The hub scripting capability has greatly accelerated our pace of system development, and has allowed us to conngure considerably more complex systems than we w ould have previously envisioned.	complex systems;control system;executable;multi-user;scripting language;super robot monkey team hyperforce go!;system integration;the hub (forum);usb hub	Stephanie Seneff;Raymond Lau;Joseph Polifroni	1999			natural language processing;computer science;programming language;communication	OS	-39.18770331540014	-27.32672264373407	19390
ce95d9af54288e40e20e97d6614d0cbdf33cf72a	abstract state machines 2004. advances in theory and practice		State Machines 2003 Advances In Theory And Practice Proceedings Of The 10th International Workshop Asm 2003 Taormina Italy March 3 7 2003 Many people are trying to be smarter every day. How's about you? There are many ways to evoke this case you can find knowledge and lesson everywhere you want. However, it will involve you to get what call as the preferred thing. When you need this kind of sources, the following book can be a great choice. abstract state machines 2003 advances in theory and practice proceedings of the 10th international workshop asm 2003 taormina italy march 3 7 2003 is the PDF of the book. If you really want to be smarter, reading can be one of the lots ways to evoke and realize. Many people who like reading will have more knowledge and experiences. Reading can be a way to gain information from economics, politics, science, fiction, literature, religion, and many others. As one of the part of book categories, abstract state machines 2003 advances in theory and practice proceedings of the 10th international workshop asm 2003 taormina italy march 3 7 2003 always becomes the most wanted book. Many people are absolutely searching for this book. It means that many love to read this kind of book. Even this book is made in soft file forms; you can enjoy reading by getting the file in your laptop, computer device, and also gadget. Nowadays, reading doesn't become a traditional activity to do by certain people. Many people from many places are always starting to read in the morning and every spare time. It proves that people now have big curiosity and have big spirit to read. Moreover, when abstract state machines 2003 advances in theory and practice proceedings of the 10th international workshop asm 2003 taormina italy march 3 7 2003 is published, it becomes a most wanted book to purchase. When visiting this page, you have decided that you will get this book in easily way, haven't you? Yeah, that's true. You can easily get the book right here. By visiting this site, you can find the link to connect to the library and publisher of abstract state machines 2003 advances in theory and practice proceedings of the 10th international workshop asm 2003 taormina italy march 3 7 2003. So, you can get is as easy as possible. It means also that you will not run out of this book. However, this site also brings you many more collections and categories of books from many sources. So, just be in this site every time you will seek for the books. Popular Books Similar With Abstract State Machines 2003 Advances In Theory And Practice Proceedings Of The 10th International Workshop Asm 2003 Taormina Italy March 3 7 2003 Are Listed Below: abs ra ct sta te m ac hi ne s 2 00 3 a dv an ce s i n t he or y a nd pr ac tic e p ro ce ed in gs of th e 1 0t h i nt er na tio na l w or ks ho p a sm 20 03 ta or m in a i ta ly m ar ch 3 7 2 00 3 PDF File : Abstract State Machines 2003 Advances In Theory And Practice Proceedings Of The 10th International Workshop Asm 2003 Taormina Italy March 3 7 2003 Page : 1	abstract state machines;artificial intelligence;as-easy-as;book;laptop;numerical aperture;portable document format	Gerhard Goos;Juris Hartmanis;Jan van Leeuwen;Wolf Zimmermann;Bernhard Thalheim	2004		10.1007/b98118	abstract state machines;theoretical computer science;computer science	Visualization	-61.67683405360564	-25.356608818772383	19532
d2e2b7f326d3e688661ef216a8b47931217d79c9	applicability of hci techniques to systems interface design	computer science human engineering electric engineering computer science;thesis;computer science		human–computer interaction	Victoria Bellotti	1990			computing;human–computer interaction;computer science;informatics engineering;software engineering;computer engineering	HCI	-54.26654892205955	-34.78669045342124	19552
de95cc768175c8a4d3ceed3e1c3da9ab2b6f5cce	mobile notion: multimodal device augmentation for musical applications	mobile;multimedia;motion;interactive;sensor;electronics;multimodal;music	Mobile devices have become an integral part of 21 st century lifestyle. From social networking and business to day-to-day scheduling an d multimedia applications, smartphones and other portable handsets are now the go-t o devices for interaction in the digital world. Currently, mobile devices typically utilise direct user interfaces such as touch screens, where interactions are performed dir ectly by controlling graphical elements or controls on the interface. This project looks to bring device interaction out of the virtual world and into the physical world. With a ‘free-gesture’ approach, portable applications can break away from the virtual world, enabling the mobile platform to be harnessed as a physical augmented interface for mus ical performance, education, medical research and beyond.	floor and ceiling functions;graphical user interface;mobile device;multimodal interaction;scheduling (computing);smartphone;touchscreen;virtual world;ical	Matthew Benatan;Ian M. Symonds;Kia Ng	2011		10.1007/978-1-4471-5406-8_13	mobile search;simulation;mobile web;human–computer interaction;engineering;multimedia;mobile computing	HCI	-48.72744013422808	-38.25747100886458	19571
f9c5d9dbf98e70dc15d70b60b6d719b8fe5bb6e9	the xft font library: architecture and users guide	users guide;freetype font rasterizer;x application;sophisticated font matching;x server;font management;x client;font usage;new font;x render extension;render extension;xft font library	TheX RenderExtensionprovidesanew glyphrendering architecturebasedonclient-sideglyphandfont management. While this resolvesmany toughtechnicalissues relatingto the extensiondesign,it placesthe burdenof rasterizing,configuringandcustomizingfont usageon everyX client. The Xft library was written to provide X applications a convenient interface to the FreeType font rasterizer andtheRenderextension.As FreeTypeprovidesfor no configurationor customization,Xft also performsthis task. Xft provides new font namingconventions,sophisticatedfont matchingandselectionmechanismsand sufficient abstrationsto permit commonapplicationsto benefitfrom Renderextensionbasedtext outputwhile still working on X serverswithout supportfor this extension.	freetype;rasterisation;xft	Keith Packard	2001			computer science;database;multimedia;world wide web	OS	-42.033737123750505	-31.01101406799358	19617
3bc10fd4074787a6e9214988455d3b4d8f8c6708	nextslideplease: agile hyperpresentations	theoretical framework;user study;slide ware;time management;hypermedia;navigation;presentations;hyperpresentations	In this video presentation, we introduce NextSlidePlease, a novel slide authoring and presentation application. The video begins with a dramatization illustrating the shortcomings of existing slide-ware tools identified through our prior research. We then describe our theoretical framework for addressing these identified problems and present a dramatization of the process by which our NextSlidePlease application can be used to overcome such issues in a business context. In addition, we illustrate the novel functional aspects of our application algorithm that enable effective time management and flexible presentations. Finally, we present promising results from two user studies.	agile software development;algorithm;usability testing;warez	Ryan P. Spicer;Yu-Ru Lin;Aisling Kelliher	2009		10.1145/1631272.1631508	navigation;simulation;time management;computer science;multimedia;world wide web	HCI	-54.052266022851924	-37.96951219441973	19631
1c2e4c356ceccca081d9bf5d065c9c7bc1972fe8	new directions for the design of virtual reality interfaces to e-commerce sites	3d interface;e commerce;navigation aids;virtual reality;3d interfaces;design guideline	Virtual Reality (VR) interfaces to e-commerce sites have recently begun to appear on the Internet, promising to make the e shopping experience more natural, attractive, and fun for customers. Unfortunately, switching to a desktop VR design for an e-commerce site is not trivial and does not guarantee at all that the interface will be effective. In this paper, we first briefly discuss the potential advantages of these interfaces, stressing the need for a better approach to their design. Then, we present the directions we are following to build more usable and effective VR stores, i.e.: (i) reformulating design guidelines from real-world stores in the VR context, (ii) exploiting VR to create user empowerments that meet both customer and merchant needs, and (iii) personalizing the VR store to better reflect customer's taste, preferences, and interests. For each of the three directions, we illustrate and discuss a detailed case study.	desktop computer;e-commerce;virtual reality	Luca Chittaro;Roberto Ranon	2002		10.1145/1556262.1556311	simulation;human–computer interaction;computer science;operating system;virtual reality;multimedia;world wide web	HCI	-51.7200514923561	-39.46690864856298	19635
88e749165d269678a5831e0ea0692fa620e888aa	multi-modal geometry tutoring system using speech and touchscreen figure tracing		People sometimes use gestures with speech to exchange information. As such an example, people explain to someone a solution of a geometric problem of mathematics. In this paper, we introduce an application system, on which a user can explain a solution of a geometric problem displayed on a screen by speech and pointing. Then, the system understands these inputs, adds the explanations to the figure, and an explanation is translated to formulas. In addition, we aim to construct the system that can be provided as a learning application for the general public, which does not require special knowledge, technology, or device for introduction into ICT education.		Kanta Kiyohara;Ryota Nishimura;Norihide Kitaoka	2018	2018 IEEE 7th Global Conference on Consumer Electronics (GCCE)	10.1109/GCCE.2018.8574714	natural language processing;touchscreen;multimodal interaction;information and communications technology;artificial intelligence;gesture;computer science;tracing	Robotics	-57.33296755001698	-34.23282180708434	19643
c7621ef2ef9ce88b4e1f7c749dcf6837e64822b1	i bet you look good on the wall: making the invisible computer visible	ambient intelligence;mental model	The design ideal of the invisible computer, prevalent in the vision of ambient intelligence (AmI), has led to a number of interaction challenges. The complex nature of AmI environments together with limited feedback and insufficient means to override the system can result in users who feel frustrated and out of control. In this paper, we explore the potential of visualising the system state to improve user understanding. We use projectors to overlay the environment with a graphical representation that connects sensors and devices with the actions they trigger and the effects those actions produce. We also provided users with a simple voice-controlled command to cancel the last action. A small first-use study suggested that our technique might indeed improve understanding and support users in forming a reliable mental model.	ambient intelligence;experiment;graphical user interface;iteration;mental model;movie projector;real-time transcription;sensor;ubiquitous computing	Jo Vermeulen;Jonathan Slenders;Kris Luyten;Karin Coninx	2009		10.1007/978-3-642-05408-2_24	simulation;ambient intelligence;human–computer interaction;computer science;artificial intelligence	HCI	-46.49844353516425	-41.50623091009718	19663
7ba1dccb211a79b572b6660be1cad71c06535820	jumpstarting relationships with online games: evidence from a laboratory investigation	online game;multiplayer games;game design;team building;teambuilding;multiplayer game;casual gaming;relationships;interpersonal attraction;online games	"""The popularity of online games, particularly casual games, has increased tremendously in recent years. Often these game experiences involve partner-based or multi-player interactions. Previous work has shown that computer-mediated interactions and online activities with a stranger have the potential to impact attitudes and liking for that person. Can experiences in online games have a similar impact? This paper presents results from two experiments suggesting that cooperative online game experiences (even without any direct communication interactions) can significantly impact liking for another person and perceptions of that person's characteristics. Implications for the design of online """"team-building"""" style game experiences are briefly discussed."""	experience;experiment;interaction	Laura A. Dabbish	2008		10.1145/1460563.1460620	psychology;video game design;game design;interpersonal relationship;simulation;computer science;emergent gameplay;game mechanics;game developer;multimedia;social psychology;online participation	HCI	-59.96294852091568	-44.381499803305054	19673
d559dae488cea1a05e72b0d051faa44f04689a74	database in depth: relational theory for practitioners	relational theory;database applicationsmake;relational databases;modern database technology;database developer;database administrator;author c.j. date;experienced database developer;daily database;relational model;database consultant	database in depth relational theory for practitioners What to say and what to do when mostly your friends love reading? Are you the one that don't have such hobby? So, it's important for you to start having that hobby. You know, reading is not the force. We're sure that reading will lead you to join in better concept of life. Reading will be a positive activity to do every time. And do you know our friends become fans of database in depth relational theory for practitioners as the best book to read? Yeah, it's neither an obligation nor order. It is the referred book that will not make you feel disappointed.	database;relational theory	C. J. Date	2005			computer science;knowledge management;data mining;database	DB	-62.035700966184905	-24.704273584549384	19674
ce16855b1ae31cdd1a0adf6446d7a64e2e1d51c8	touching the void: gestures for auditory interfaces	mobile;auditory interface;mobile device;hit;technology;lab;mobile computer;form factor;tangible interface;design space;hitlab;eyes free;gestures;interface;human;nz;auditory display;handheld device;participatory design;embodied interaction	Nowadays, mobile devices provide new possibilities for gesture interaction due to the large range of embedded sensors they have and their physical form factor. In addition, auditory interfaces can now be more easily supported through advanced mobile computing capabilities. Although different types of gesture techniques have been proposed for handheld devices, there is still little knowledge about the acceptability and use of some of these techniques, especially in the context of an auditory interface. In this paper, we propose a novel approach to the problem by studying the design space of gestures proposed by end-users for a mobile auditory interface. We discuss the results of this explorative study, in terms of the scope of the gestures proposed, the tangible aspects, and the users' preferences. This study delivers some initial gestures recommendations for eyes-free auditory interfaces.	embedded system;mobile computing;mobile device;sensor;the void (virtual reality)	Katrin Wolf;Christina Dicke;Raphaël Grasset	2010		10.1145/1935701.1935772	human–computer interaction;engineering;multimedia;communication	HCI	-46.82169103734465	-43.28370183266585	19741
c71609fa511a8fdd2afb9fbececc7dbd786df3d0	visual integration: a dynamic approach to the interactive combination of disparate data	three dimensional displays visualization data visualization virtual reality rendering computer graphics data visualization intellectual property	This paper explores the practicality and promise of combining the 3D visual output of multiple applications as an alternative to the growing challenges of accessing, merging, and rendering data from different sources. The need for integrating data from multiple sources for the purpose of collaborative visualization has increased in recent years. However, traditional approaches are proving inadequate for several reasons: data size, incompatibility across proprietary file formats, limitations owing to data intellectual property rights, data security, and incompatibility across computing platforms. To address these challenges, this paper explores a simple approach, called Visual Integration (VI), which purposefully limits the exchange of information to only visual artifacts for generating interactive views. VI integrates 3D images from disparate sources in real-time, by comparing the depth information at every combined pixel. VI also preserves independent interactive views of the integrated data for all parties. We report on the design and implementation of a simple prototype created in OpenGL, discuss necessary improvements to create a practical solution, and explore various business scenarios that can benefit from using VI.	data security;opengl;pixel;prototype;real-time clock;software incompatibility;visual artifact	Paul Borrel;Bireswar Laha	2015	IBM Journal of Research and Development	10.1147/JRD.2015.2402532	computer vision;visual analytics;scientific visualization;information visualization;visualization;interactive visualization;human–computer interaction;interactive visual analysis;computer science;parallel rendering;data visualization;computer graphics (images)	HCI	-38.64554496682798	-34.7283131853747	19743
7c5999f8c156aea1f237b97353ae75929dabd378	adaptive multimodal exploration of music collections		Discovering music that we like rarely happens as a result of a directed search. Except for the case where we have exact meta data at hand it is hard to articulate what song is attractive to us. Therefore it is essential to develop and evaluate systems that support guided exploratory browsing of the music space. While a number of algorithms for organizing music collections according to a given similarity measure have been applied successfully, the generated structure is usually only presented visually and listening requires cumbersome skipping through the individual pieces. To close this media gap we describe an immersive multimodal exploration environment which extends the presentation of a song collection in a video-game-like virtual 3-D landscape by carefully adjusted spatialized plackback of songs. The user can freely navigate through the virtual world guided by the acoustic clues surrounding him. Observing his interaction with the environment the system furthermore learns the user’s way of structuring his collection by adapting a weighted combination of a wide range of integrated content-based, meta-data-based and collaborative similarity measures. Our evaluation proves the importance of auditory feedback for music exploration and shows that our system is capable of adjusting to different notions of similarity.	acoustic cryptanalysis;algorithm;auditory processing disorder;multimodal interaction;organizing (structure);similarity measure;virtual world	Dominik Lübbers;Matthias Jarke	2009			simulation;computer science;machine learning;multimedia;world wide web	Web+IR	-33.77664311939366	-45.881577531777296	19764
0f2236fcc037facd46b8badc0aaf0aee812af295	smartphones: user engagement motivations effect on their value, satisfaction, and future engagement intention		The growth of mobile technology mediated environments is accelerated by its accessibility and easy usage of mobile technology tools, such as smartphones and tablets. User friendly and intuitive features drive user value and satisfaction. These features motivate and drive further engagement. Smartphones allow users to control when, where, and how they engage in chosen activities that serve their needs, saving time, completing a task (utilitarian), entertain them (hedonic), or connect with others (social). This study investigates, proposes, and tests an engagement motivation model to explain smartphone users’ motivations, and their value, satisfaction, and overall continued engagement intention. Findings indicate users’ engagement motivations do influence their value, satisfaction and overall engagement intention.	accessibility;smartphone;tablet computer	Young Ho Kim;Dan Kim;Kathy Wachter	2012			public relations;knowledge management;business;social psychology	HCI	-58.280144018342504	-45.054745910655136	19796
4150d8d24fc376b3641572eb2a3ea5661de5a9a1	privacycamera: cooperative privacy-aware photographing with mobile phones	mobile phones privacycamera cooperative privacy aware photographing;face mobile handsets privacy cameras image color analysis buildings prototypes;photography cameras image processing mobile handsets	Nowadays, mobile phones are usually embedded with powerful cameras. Due to the convenience of carrying mobile phones, an increasing number of people use mobile phones to take photos anytime and anywhere. However, when a user takes a photo of a scenery, a building or a target person, sometimes an unexpected stranger is also included in the photo. Such photos reveal where the stranger has been and thus can breach his privacy. This problem has received little attention in the literature. In this paper, we propose PrivacyCamera, a cooperative system to protect the stranger's privacy in the above scenario. Through cooperation between the photographer and the stranger, the system can automatically blur the stranger's face in the photo upon the stranger's request when the photo is being taken. This paper describes the design, analysis, prototype implementation, and experimental evaluation of the system. Experiments show that PrivacyCamera can effectively protect stranger's privacy in an efficient way.	anytime algorithm;consensus dynamics;embedded system;gaussian blur;mobile phone;privacy;prototype	Ang Li;Qinghua Li;Wei Gao	2016	2016 13th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)	10.1109/SAHCN.2016.7733008	internet privacy;computer security	Mobile	-34.44456245018405	-44.754273679721486	19800
0e302cd4a18b218ada1c2b41235e8f39ecd9c9f6	transparency of a bilateral tele-operation scheme of a mobile manipulator robot		This work presents the design of a bilateral tele-operation system for a mobile manipulator robot, allowing a human operator to perform complex tasks in remote environments. In the tele-operation system it is proposed that the human operator is immersed in an augmented reality environment to have greater transparency of the remote site. The transparency of a tele-operation system indicates a measure of how the human feels the remote system. In the local site an environment of augmented reality developed in Unity3D is implemented, which through input devices recreates the sensations that the human would feel if he were in the remote site, for which is considered the senses of sight, touch and hearing. These senses help the human operator to “transmit” their ability and experience to the robot to perform a task. Finally, experimental results are reported to verify the performance of the proposed system.	mobile manipulator;robot;television	Víctor Hugo Andaluz;Washington X. Quevedo;Fernando A. Chicaiza;José Varela;Cristian Gallardo;Jorge S. Sánchez;Oscar Arteaga	2016		10.1007/978-3-319-40621-3_18	mobile robot;mobile manipulator;control theory	Robotics	-42.7620888452807	-44.762025924117765	19830
bf24ababd73e623916dd0b6cae7ff7aaab998017	curveship's automatic narrative style	narrative;interactive fiction;electronic literature;out of order;abstract syntax;narrating;interactive storytelling;system simulation;article;text adventure games	Curveship, a Python framework for developing interactive fiction (IF) with narrative style, is described. The system simulates a world with locations, characters, and objects, providing the typical facilities of an IF development system. To these it adds the ability to generate text and to change the telling of events and description of items using high-level narrative parameters, so that, for instance, different actors can be focalized and events can be told out of order. By assigning a character to be narrator or moving the narrator in time, the system can determine grammatical specifics and render the text in a new narrative style. Curveship offers those interested in narrative systems a way to experiment with changes in the narrative discourse; for interactive fiction authors and those who wish to use of the system as a component of their own, it is a way to create powerful new types of narrative experiences. The templates used for language generation in Curveship, the string-with-slots representation, shows that there is a compromise between highly flexible but extremely difficult-to-author abstract syntax representations and simple strings, which are easy to write but extremely inflexible. The development of the system has suggested ways to refine narrative theory, offering new understandings of how narrative distance can be understood as being composed of lower-level changes in narrative and how the order of events is better represented as an ordered tree than a simple sequence.	abstract syntax;high- and low-level;natural language generation;python;string (computer science);tree (data structure)	Nick Montfort	2011		10.1145/2159365.2159394	narrative inquiry;abstract syntax;simulation;narrative criticism;narrative network;computer science;out-of-order execution;artificial intelligence;multimedia;narrative;programming language;narrative psychology	AI	-38.85876354125914	-31.540226687147733	19870
81b0f637b293a7906763e35afe2121b0ade987dd	haptic virtual reality training environment for micro-robotic cell injection	virtual training;virtual reality;haptics;cell injection	Micro-robotic cell injection is typically performed manually by a trainedbio-operator, and success rates are often low. To enhance bio-operator performance during real-time cell injection, our earlier work introduced a haptically-enabled micro-robotic cell injection system. The system employed haptic virtual fixtures to provide haptic guidance according to articular performance metrics. This paper extends the work by replicating the system within a virtual reality (VR) environment for bio-operator training. Using the virtual environment, the bio-operator is able to control the virtual injection process in the same way they would with the physical haptic micro-robotic cell injection system, while benefiting from the enhanced visualisation capabilities offered by the 3D VR environment. The system is achieved using cost-effective components offering training at much lower cost than using the physical system.	cell (microprocessor);haptic technology;robot;virtual reality	Syafizwan Faroque;Ben Horan;Husaini Adam;Mulyoto Pangestu;Samuel Thomas	2014		10.1007/978-4-431-55690-9_46	embedded system;simulation;engineering;multimedia	Robotics	-41.56109216448924	-46.845634716786265	19893
0b9b876098357cd425660e8dc24e571680f6dc62	identifying usability issues in instant messaging apps on ios and android platforms		With the rise of mobile devices and the high number of instant messaging applications available in the stores, it is necessary to evaluate the usability of such applications to provide a more satisfying user experience. To this end, this paper presents a methodical usability evaluation of instant messaging applications both in iOS and Android platforms. A predefined evaluation was used, which was created to detect the main usability issues of mobile applications, regardless of the device used and the evaluated applications. Consequently, two methods were used: the keystroke level model and the mobile heuristic evaluation. Also, the results suggest that the main problems of this type of applications are difficulties in performing tasks (some of them were not agile nor easy to complete), lack of element cohesion (some icons and buttons did not follow the style of the operating system, bad translations, and too much information on the screen), problems with the user interface (pop-ups overlapping the status bar, clipped elements, sometimes the interface did not rotate, and, in other cases, the interface changed considerably when the device was rotated), and lack of information about privacy and security features. Finally, based on the results, we propose a set of recommendations that will be helpful for developers of this kind of applications.	android;instant messaging;usability;ios	Sergio Caro-Alvaro;Eva García;Antonio García-Cabot;Luis de Marcos;José Javier Martínez-Herráiz	2018	Mobile Information Systems	10.1155/2018/2056290	heuristic evaluation;computer network;status bar;keystroke-level model;human–computer interaction;android (operating system);mobile device;usability;user experience design;computer science;user interface	Mobile	-47.74576467870994	-44.28131731224926	20025
94425e4492b8945874a2e32cee333bd3d9bc08f2	face as mouse through visual face tracking	modelizacion;sistema operativo;interfase usuario;vision ordenador;curva bezier;computadora personal;pistage;ordinateur personnel;estimation mouvement;sensorial perception;image processing;facies;personal computer;user interface;camera mouse;visual face tracking;real time;estimacion movimiento;rastreo;procesamiento imagen;motion estimation;computer configuration;traitement image;configuracion ordenador;motion tracking;sistema reactivo;face tracking;computer vision;man machine system;modelisation;deficiencia fisica;3d model;senal video;signal video;operating system;courbe bezier;temps reel;reactive system;utilisabilite;systeme reactif;video signal;tiempo real;sistema hombre maquina;systeme exploitation;interface utilisateur;vision ordinateur;handicap physique;perceptual user interface;physical handicap;usabilidad;percepcion sensorial;usability;modeling;deformable model;hand free control;perception sensorielle;human machine interaction;tracking;bezier curve;systeme homme machine;configuration ordinateur	This paper introduces a novel camera mouse driven by visual face tracking based on a 3D model. As the camera becomes standard configuration for personal computers (PCs) and computation speed increases, achieving human-machine interaction through visual face tracking becomes a feasible solution to hands-free control. Human facial movements can be broken down into rigid motions, such as rotation and translation, and non-rigid motions such as opening, closing, and stretching of the mouth. First, we describe our face tracking system which can robustly and accurately retrieve these motion parameters from videos in real time [H. Tao, T. Huang, Explanation-based facial motion tracking using a piecewise Bezier volume deformation model, in: Proceedings of IEEE Computer Vision and Pattern Recogintion, vol. 1, 1999, pp. 611-617]. The retrieved (rigid) motion parameters can be employed to navigate the mouse cursor; the detection of mouth (non-rigid) motions triggers mouse events in the operating system. Three mouse control modes are investigated and their usability is compared. Experiments in the Windows XP environment verify the convenience of our camera mouse in hands-free control. This technology can be an alternative input option for people with hand and speech disability, as well as for futuristic vision-based games and interfaces.	facial motion capture	Jilin Tu;Hai Tao;Thomas S. Huang	2007	Computer Vision and Image Understanding	10.1016/j.cviu.2006.11.007	computer vision;facial motion capture;simulation;systems modeling;usability;facies;image processing;reactive system;computer science;bézier curve;motion estimation;tracking;user interface;computer graphics (images)	Vision	-38.47959277106513	-42.03742084831386	20031
09e4e7cbeb4dc254be2edb30e4b1cd65fcecd877	a hybrid image-based and model-based telepresence system using two-pass video projection onto a 3d scene model	real time;computational geometry;virtual reality;optical projectors;static 3d geometry image based telepresence model based telepresence system two pass live video projection 3d scene model free viewpoint control real time color update virtual environment;image texture;virtual environment;optical projectors virtual reality solid modelling computational geometry image texture rendering computer graphics natural scenes;rendering computer graphics;natural scenes;layout cameras rendering computer graphics virtual environment geometry buffer storage prototypes optical sensors real time systems mice;solid modelling	A telepresence system is presented that has the advantage of both model-based and image-based approaches, namely, free viewpoint control and real-time color update with live video. A remote place is presented as a virtual environment by using live video projection captured by a head-worn camera onto the static 3D geometry. The observer can then observe the remote place in cooperation with the remote camera man, and give him a set of 3D instructions by a mouse.	real-time locating system;video projector;virtual reality	Takefumi Ogawa;Kiyoshi Kiyokawa;Haruo Takemura	2005		10.1109/ISMAR.2005.3	image texture;computer vision;rendering;computational geometry;computer science;virtual machine;virtual reality;multimedia;computer graphics (images)	Graphics	-40.4594856041793	-37.952400223166876	20034
1ef8b81c36b286dd562523183e87a30d77e1ae94	generating multilingual documents from a knowledge base: the techdoc project	knowledge base;system architecture	TECHDOC is an implemented system demonstrating the feasibility of generating multilingual technical documents on the basis of a language-independent knowledge base. Its application domain is user and maintenance instructions, which are produced from underlying plan structures representing the activities, the participating objects with their properties, relations, and so on. This paper gives a brief outline of the system architecture and discusses some recent developments in the project: the addition of actual event simulation in the KB, steps towards a document authoring tool, and a multimodal user interface.	application domain;knowledge base;language-independent specification;multimodal interaction;simulation;systems architecture;user interface	Dietmar F. Rösner;Manfred Stede	1994			natural language processing;knowledge base;human–computer interaction;computer science;database;multimedia;programming language	AI	-40.382153272455255	-27.011638841933987	20061
f2d7e3e53e2238e81316e9a292326ecbc2c7d063	bridging art and science with creativity support tools	sonification;visualization;metaphor;perceptualization;reification	Leaders of the science, technology, design and art worlds will examine how creativity support tools are changing the nature of creativity in their fields. They are invited to reflect on how creative processes are similar across disciplines…or not.	bridging (networking)	Ben Shneiderman;Rita R. Colwell;Sara Diamond;Paul Greenhalgh;William A. Wulf	2007		10.1145/1254960.1255044	visualization;reification;sonification;human–computer interaction;computer science;multimedia	HCI	-61.11970010176954	-34.23481194267125	20129
0684d49ae539bdbe07cb8bf2efe4eda7ee3a90d9	challenges in doing participatory design with people with dementia	codesign;alzheimer;dementia;pd;people with dementia;participatory design	This paper critically looks at the role of people with dementia (and their network) when involved in a participatory design (PD) process and the role of designers when involving a person with dementia (and their network). Two participatory projects (ATOM and Dementia Lab) were analyzed and challenges in doing PD together with people with dementia are defined.		Niels Hendriks;Liesbeth Huybrechts;Andrea Wilkinson;Karin Slegers	2014		10.1145/2662155.2662196	co-design;engineering	HCI	-62.36431089459733	-41.1933888510625	20146
fd52a509ad3acdb6803cfa4b4eec49706e2374a7	mediajourney: capturing and sharing digital media from real-world and virtual journeys	multimedia blogging;mobile web;digital media;ubiquitous link anchors;location awareness;geo spatial hypermedia	In this poster, we discuss a novel MediaJourney concept and infrastructure with integrated applications for capturing, annotating, and automatically tagging captured media objects during physical journeys as well as virtual journeys on the web or in media collections. The main objective of MediaJourney is to radically reduce overhead in collecting and organizing captured digital media for planned or ad-hoc sharing with family and friends, e.g. in a home setting. This is supported with a mix of automatic tagging and manual selection or keyword tagging of journeys at the time of capture. The idea is to provide automatic and simple mechanisms for structuring while capturing.	digital media;hoc (programming language);organizing (structure);overhead (computing);tag (metadata)	Kaspar Rosengreen Nielsen;Rasmus Gude;Marianne Graves Petersen;Kaj Grønbæk	2009		10.1145/1557914.1557979	mobile web;computer science;digital media;multimedia;internet privacy;world wide web	HCI	-41.04425429694947	-24.269637095653557	20157
47b8b2cb5efce54c0d9e55786b81f3592aeabba7	live demonstration: a tongue-operated multimodal human computer interface and robotic rehabilitation system	user interfaces human computer interaction medical robotics patient rehabilitation;tongue robots headphones wrist exoskeletons computer interfaces computers;patient recovery tongue operated multimodal human computer interface multimodal tongue drive system tongue operated robotic rehabilitation system patient rehabilitation stroke patients	This demonstration features two systems showcasing different applications of the Tongue Drive System (TDS). The Multimodal Tongue Drive System (mTDS) serves as a comprehensive human computer interface (HCI) for individuals with severe physical disabilities, while the tongue-operated robotic rehabilitation system (TDS-HM) facilitates rehabilitation of stroke patients during their recovery.	human computer;human–computer interaction;multimodal interaction;rehabilitation robotics;robot	Md. Nazmus Sahadat;Z. Zhang;Arish Alreja;Pooja Srikrishnan;Sarah Ostadabbas;Nordine Sebkhi;Maysam Ghovanloo	2015	2015 IEEE Biomedical Circuits and Systems Conference (BioCAS)	10.1109/BioCAS.2015.7348323	simulation;physical medicine and rehabilitation;engineering;communication	Robotics	-41.39100367082723	-45.48069113917216	20214
e5fce9f758a68e100448f32d811446a8ee2d7183	a sense of self: the role of presence in virtual environments	personality;virtual environments;virtual self;massively multiplayer online games;presence;observed behaviors;avatar	0747-5632/$ see front matter 2013 Elsevier Ltd. All rights reserved. http://dx.doi.org/10.1016/j.chb.2013.02.002 ⇑ Corresponding author. Tel.: +1 501 569 3552; fax: +1 501 569 3547. E-mail addresses: mpmccreery@ualr.edu (M.P. McCreery), pg.schrader@unlv.edu (P.G. Schrader), kkrach@troy.edu (S.K. Krach), randy.boone@unlv.edu (R. Boone). Michael P. McCreery a,⇑, P.G. Schrader , S. Kathleen Krach , Randy Boone b	fax;like button;virtual reality	Michael P. McCreery;P. G. Schrader;S. Kathleen Krach;Randy Boone	2013	Computers in Human Behavior	10.1016/j.chb.2013.02.002	psychology;multimedia;personality;communication;social psychology	AI	-53.6665766536	-51.41785650207796	20241
629e7e1a5ee756990f5558515bb0e991398951b4	inside relational databases - with examples in access		That's it, a book to wait for in this month. Even you have wanted for long time for releasing this book inside relational databases with examples in access; you may not be able to get in some stress. Should you go around and seek fro the book until you really get it? Are you sure? Are you that free? This condition will force you to always end up to get a book. But now, we are coming to give you excellent solution.	relational database	Mark Whitehorn;Bill Marklyn	1998				DB	-62.58057611529622	-24.003197700823947	20333
22a44607c1475bd3e4746d5dc3de0ef41412c213	projector phone: a study of using mobile phones with integrated projector for interaction with maps	high resolution;map interaction;user study;projector phone;text input;mobile phone;qa75 electronic computers computer science;experimental comparison;interaction design;mobile application	First working prototypes of mobile phones with integrated pico projectors have already been demonstrated and it is expected that such projector phones will be sold within the next three years. Applications that require interaction with large amounts of information will benefit from the large projection and its high resolution. This paper analyses the advantages and disadvantages of an integrated projector when interacting with maps, and discusses findings useful for the development of mobile applications for projector phones. We report in particular the implementation of an application that uses either the screen of the mobile phone, the projection or a combination of both. These three options were compared in a user study in which the participants had to perform three different tasks with each option. The results provide clear evidence for the positive aspects of using a built-in projector, but also show some negative aspects related to text input.	image resolution;interaction;map;mobile app;mobile phone;movie projector;projector phone;usability testing;video projector	Alina Hang;Enrico Rukzio;Andrew Greaves	2008		10.1145/1409240.1409263	image resolution;human–computer interaction;computer science;interaction design;multimedia;computer graphics (images)	HCI	-45.11726132692677	-40.84073828306227	20342
b1ddbf76f1db95b3a9a23a00701a602894a94aaf	case-based and multimodal reasoning: the contribution of piero torasso			case-based reasoning;multimodal interaction	Luigi Portinale	2018	Intelligenza Artificiale	10.3233/IA-180117	computer science;world wide web;human–computer interaction	NLP	-51.11238323304248	-34.131341564082064	20387
81f9df1b296a4042f6dab24e3f1a8f68cedfa2ba	audio indexing for youtube	decision support systems internet sea surface youtube indexing;youtube content audio indexing internet ocean surface unindexed audio contents file content;social networking online internet;deep web audio indexing search engine transcription;sea surface;youtube;internet;indexing;decision support systems	The internet is often compared to a sea of information, and searching on the internet is like throwing a net on the surface of the ocean, the search will only include results from the surface of the ocean leaving the depth untouched, naming it the deep web. The deep web includes, among other things, the un-indexed audio contents present today, where the search is based on descriptors accompanying the file rather than the file' contents. This paper introduces an efficient way for searching audio content. The focus is on using YouTube content.	dark web;deep web;internet	Mohamad Nour Al Laham;Imad Ayass;Majd Ghareeb;Zouhair El-Bazzal;Mohamad Raad	2015	2015 Fifth International Conference on Digital Information and Communication Technology and its Applications (DICTAP)	10.1109/DICTAP.2015.7113181	search engine indexing;the internet;decision support system;computer science;multimedia;internet privacy;world wide web	DB	-44.86879868072254	-24.079463129187282	20437
9a7d06963fb499b714d19f22e7341896c38c0cff	yet another statistical analysis of bob ross paintings		In this paper, we analyze a sample of clippings from paintings by the late artist Bob Ross. Previous work focused on the qualitative themes of his paintings (Hickey, 2014); here, we expand on that line of research by considering the colorspace and luminosity values as our data. Our results demonstrate the subtle aesthetics of the average Ross painting, the common variation shared by his paintings, and the structure of the relationships between each painting in our sample. We reveal, for the first time, renderings of the average paintings and introduce “eigenross” components to identify and evaluate shared variance. Additionally, all data and code are embedded in this document to encourage future research, and, in the spirit of Bob Ross, to teach others how to do so.	color space;embedded system;i. michael ross;yet another	Christopher Steven Marcum	2015	CoRR		econometrics;mathematics;yet another;rendering (computer graphics);preprint;aesthetics;painting	HCI	-50.73770048961832	-27.16780384970237	20506
e8d74cf4e7019be2ded768a7854bf833a955be73	brain springs: fast physics for large crowds in wall•e	pixar;fast physics;human crowds;software platform;computer graphics;spring physics;entertainment computer animation;crowd simulation;brain models;group behavior;spring physics system;behavioral animation;acceleration;computer graphic;springs;simulation methods;believable physics;spring physics system fast physics wallldre believable physics human crowds robot crowds pixar;physics based animation;signal processing;fake physics;robots;animation;robot crowds;wallldre;computer graphics physics based animation crowd simulation motion signal processing spring physics fake physics behavioral animation;computer animation;springs physics animation computational modeling robots frequency brain modeling banking humans orbital robotics;motion signal processing;entertainment	A major challenge of making WALLldrE was creating believable physics for human and robot crowds. To do this, Pixar technical directors combined a custom spring-physics system in the Massive software platform with traditional simulation methods. The performance was fast enough to scale for large crowds and maintain interactivity for previews. Computer animation is a developed, thriving field that has benefitted greatly from the techniques of traditional animation and its insights into what elements of motion create appealing characters. Pixar Animation Studios introduced the adaptation of such elements as squash, stretch, follow-through, and anticipation to the field, which guide some of the most successful computer animation to date. However, these elements are often neglected in crowd simulations, where rendering challenges, group behavior, and appearance variation are the dominant focus.		Paul Kanyuk	2009	IEEE Computer Graphics and Applications	10.1109/MCG.2009.59	acceleration;robot;anime;computer vision;entertainment;simulation;computer facial animation;computer science;artificial intelligence;signal processing;crowd simulation;computer animation;multimedia;computer graphics;hooke's law;group dynamics;computer graphics (images)	Visualization	-39.50329981498799	-36.55712209935451	20551
78d62c5e98445fa7d7973e6c4aded8062b03b49a	interactive paths through tree music		This paper discusses the design of Tree Music, an interactive computer music installation created with RTcmix and GAIA (Graphical Audio Interface Application), a new open-source software package by Dave Topper, Technical Director of the Virginia Center for Computer Music. Commissioned by the University of Virginia Art Museum to accompany the New Directions exhibit of tree-trunk sculpture by Emilie Brzezinski (Figure 1),Tree Musicinvolves four networked levels of interactivity, each projecting a different relationship between the composer and the spectator. A wireless camera tracks changes in motion and occlusion; GAIA uses these data to navigate four networked levels. While people visiting the installation repeatedly may experience some sonic overlap, there is a vanishingly small probability that they will have identical musical experiences during subsequent visits.		Judith Shatin	2004			communication	HCI	-52.28407870383228	-25.5209678628435	20617
0e0734663f548a1e15fe5855edefb6fb14128622	investigation of human subjective feelings for different surface textures of slipping objects based on the analysis of contact conditions	contact deformation;affective evaluation experiment;tactile sensation	Humans can manipulate objects with grasp forces maintained slightly above the minimum required force to prevent slipping. When humans grasp and lift an object, they use fast acting receptors in their skin. These receptors respond to local slips that occur before the gross slip. The estimation of the coefficient of friction and the detection of the incipient slip play a key role in human grasp stability. However, it is not easy to strictly measure the coefficient of friction between the skin and the object’s surface during object manipulations because the shape and dryness of an individual’s skin vary widely. To quantitatively and continuously evaluate the slip condition during a sliding motion, we propose ‘eccentricity’ in the contact area as a measure, which is determined based on the changes in the contact area before and after the sliding motion. In this paper, we employ eccentricity-based slip condition measurement and evaluate the slip condition between a human’s skin and various surface textures. Then, we compare the results with the affective evaluation experiments to assess the subjective feeling of the surface texture and discuss the influence of the slip condition on the subjective feeling.		Tsuyoshi Arakawa;Akira Nakahara;Kiyotaka Yarimizu;Masato Takahashi;Michiko Ohkura;Toshio Tsuji;Yuichi Kurita	2016		10.1007/978-3-319-42324-1_13	psychology;computer vision;communication;social psychology	HCI	-44.99193011848788	-50.85877588288394	20770
1171161181a649a51e770d0b34bd1e9274755dd2	coding format independent multimedia content adaptation			content adaptation		2008		10.1007/978-0-387-78414-4_632	multimedia	DB	-45.41196509626085	-24.963720214396094	20793
d6ebeb5c5c23ff055682cbb3ad9f14dc0e532c71	secrets and lies in computer-mediated interaction: theory, methods and design	secrecy;social interaction;theory methods;social behavior;design;security;privacy;deception;social cohesion	The keeping of secrets and practicing of deception are commonplace in everyday social interaction. They also serve an important role in encouraging social cohesion. However, for HCI practitioners, the challenge is to design systems that enable exactly this kind of flexibility and ambiguity in social behavior while also maintaining trust and authenticity. This workshop will bring together researchers of both deception and secrecy in computer-mediated interaction, alongside designers of systems, to face up to these challenges and develop a road map for the future. The workshop will act as a venue for the synthesis of theory with design, and propose ways to face the challenges of enabling authentic social interaction in computerized environments.	group cohesiveness;human–computer interaction;venue (sound system)	Adam N. Joinson;Jeffrey T. Hancock;Pamela Briggs	2008		10.1145/1358628.1358975	social relation;design;social behavior;computer science;information security;privacy;world wide web	HCI	-61.01174488773415	-35.7250107977388	20810
972043490df67adc0dfbb21bfcbb5ac6e75282ae	session overview: adaptation strategies and adaptation management		Researchers in the field of Augmented Cognition (AugCog) have often focused on detecting and classifying cognitive problem states (e.g. via physiological sensors). Yet, knowing what cognitive state to address through adaptation is merely a first step to building an adaptive system. The next steps – to determine what to adapt and how to do it – are just as interesting and challenging. There is great untapped potential in the adaptation component, yet it has been underrepresented and underappreciated in the AugCog community discourse. The goal of this contribution and the associated conference session is to get our community thinking about how to put their cognitive state diagnoses to use in an innovative manner, how to develop innovative adaptation strategies, and how to address adaptation management issues. This session overview lists a number of challenges faced by AugCog researchers with respect to the development of adaptation strategies and adaptation management frameworks. The papers featured in this session encompass a diverse set of approaches and ideas to address these challenges.		Sven Fuchs	2018		10.1007/978-3-319-91470-1_1	management science;human factors integration;adaptive system;cognition;augmented cognition;computer science	NLP	-62.82263664755877	-32.229533912945584	20822
18014e6ea0197eced4e261144891138583fa324c	cms-topss: efficient dissemination of rss documents	discussion forum;content management system	Recent years have seen a rise in the number of unconventional publishing tools on the Internet. Tools such as wikis, blogs, discussion forums, and web-based content management systems have experienced tremendous rise in popularity and use; primarily because they provide something traditional tools do not: easy of use for non computer-oriented users and they are based on the idea of “collaboration.” It is estimated, by pewinternet.org, that 32 million people in the US read blogs (which represents 27% of the estimated 120 million US Internet users) while 8 million people have said that they have created blogs. Web-based collaboration is the common idea for this new breed of content-management tools. The center piece of such a tool is a web page that is being used as an area where multiple users participate in content creation. More significantly, the collaboration enabling tool used is the web page itself (accessed through the all-pervasive web browser). With these new web applications, there rouse a need for users to stay informed about changes to the content. In general, users want to be updated about daily news headlines of interest to them, or be notified when there is a reply in a discussion they participate in, or their favorite web personality has updated his/her blog (online diary etc.). RDF Site Summary (RSS), a RDF-based language for expressing content changes, is an application on the Web that has grown considerably in popularity. The RSS specification is so flexible, that any kind of changes can be described starting from blog updates, source code changes, forum discussions to database content changes and others, but it is up to the end user application to ”understand” what the content is. Being based on RDF helps RSS to this end, since RDF	blog;cp/cms;content management system;internet;multi-user;online diary;rss;resource description framework;web application;web content;web page;wiki;world wide web	Milenko Petrovic;Haifeng Liu;Hans-Arno Jacobsen	2005			computer science;multimedia;internet privacy;world wide web	Web+IR	-46.655563806940584	-24.65237754954936	20855
fdf7389a3565d967a7aed4740d2b4045da9e89f3	involving the crowd in future museum experience design	museum systems;experience;crowds;museums;design	A general trend of museums and cultural heritage institutions besides digitizing their collections is to involve the public more and at various levels. Technology plays an increasingly important role in this involvement. Developments we have observed in museum experience design, include trends towards 1) dialogical engagement of the public; 2) addressing crowds as audiences; 3) the use of Internet of Things (IoT) and Do-It-Yourself (DIY) technology in museums; and 4) designing for museum systems and institutional ecologies instead of for individual museums only. In this one-day workshop we especially focus on exploring the implications of museums reaching out to crowds beyond their local communities, and of museums increasingly becoming part of connected museum systems and large institutional ecosystems. By means of a tangible game we will brainstorm about future opportunities and challenges, cluster and evaluate them, and suggest future work.	brainstorm;ecology;ecosystem;experience design;internet of things	Arnold P. O. S. Vermeeren;Licia Calvi;Amalia G. Sabiescu;Raffaella Trocchianesi;Dagny Stuedahl;Elisa Giaccardi	2016		10.1145/2851581.2856482	museology;museum informatics;design;simulation;human–computer interaction;multimedia;management	HCI	-57.919178128359256	-38.119782780919046	20871
6aa905464d959a3734655e935f6ffc58e9ddb390	point-of-view: custom information delivery via hand-held devices	hand held device;information resources;document handling;transcoding information management large screen displays handheld computers calendars organizing focusing web pages streaming media;profile sensitive document abstraction custom information delivery hand held devices online news access transcoding methods remote proxies generic summarisation techniques text based news documents summary for a hand held genre linguistic analysis context sensitive document abstraction;linguistic analysis;information resources mobile computing document handling linguistics notebook computers;notebook computers;point of view;mobile computing;linguistics	Socially, hand-held devices are becoming ubiquitous; technologically, they are already in a position to mediate access to on-line news. We argue that current frameworks for news delivery to hand-helds, typically involving transcoding methods by remote proxies layered over generic summarisation techniques are not well suited to the task. This work addresses two questions in this context: the specialised ‘transcoding’ strategy for a well-defined sub-type of content, namely that of primarily text-based news documents, and the emergence of a ‘summary-for-a-hand-held’ genre, which exploits advanced linguistic analysis to meet the particular requirements of news skimming on hand-helds. Directly related is the issue of how novel methods for deriving contextand profilesensitive document abstractions interact with novel metaphors for mediating these abstractions on the basis of who, where, and when is using them.	context-sensitive grammar;desktop computer;digital distribution;document;emergence;framing (world wide web);information access;mobile device;online and offline;point of view (computer hardware company);pointing device;portals;proxy pattern;proxy server;recommender system;requirement;technical standard;text-based (computing)	Branimir Boguraev;Rachel K. E. Bellamy;Calvin Swart	2001		10.1109/HICSS.2001.926471	computer science;artificial intelligence;operating system;database;multimedia;programming language;mobile computing;world wide web	Web+IR	-50.051338179423695	-39.331939961832305	20906
afdd7a6ed6a63b407b6a27177de9a999ad8d1bb4	charting the audience perceptions of projected 3d media installations	user studies;user experience;3d display;3d media installation	In this paper we present our work on exploring the possibilities of using 3D projection, instead of 2D displays, for a media installation set-up, which we aim to use for exhibition and teaching purposes. We compare two visual media installations presenting a rotating Earth, one presented on a 2D display and the other projected on a physical 3D object, and present the feedback collected by using the product evaluation cards method.	3d projection	Minna Karukka;Pekka Nisula;Jonna Häkkilä;Jussi Kangasoja	2012		10.1145/2406367.2406426	user experience design;simulation;stereo display;human–computer interaction;computer science;multimedia	HCI	-49.21150742323694	-35.0680295210138	20914
9db26d4ee6fc89e72ae1d5fecf6e797021d8e2a6	irc quest: using the commons dilemma to support a single-screen game for hundreds of players	game design;massively multi player game;commons dilemma	In this paper we describe the challenges of creating a game that can be played by large groups on a single display. Our solutions include the use of smart phones as game controllers using the standard IRC protocol, voting-based turn interaction, and automatically cus-tomized avatars allowing hundreds of players to appear on the display simultaneously. To provide meaningful gameplay for large numbers of people, the game is designed around a series of commons dilemmas.	game controller;internet relay chat;smartphone	Dan John Moran;Carey Metcalfe;T. C. Nicholas Graham	2014		10.1145/2658537.2662991	non-cooperative game;video game design;game design;cheap talk;simulation;level design;simultaneous game;computer science;win-win game;game mechanics;game art design;metagaming;repeated game;game developer;multimedia;screening game;game design document;sequential game;computer security	HCI	-54.17018221823586	-39.78309187993412	20941
479df7618fecd302b4cdc348d884d5626193a5cc	like being there	technology forecasting;technological innovation;technological innovation technology forecasting	As a young engineer, I felt that 50 years was an impossibly long time over which to foretell technical advances. As an elderly engineer, 50 years does not seem so long, after all. This feeling is emboldened, in part, by repeated observations of the lags among concept, realization, and societal deployment. In evolutionary development, the intervals can sometimes amount to years. In revolutionary instances, the activity is explosive and faster. Horgan’s book, The End of Science, puts forward interviews with prominent physicists, some of whom subscribe to the notion that most of the major discoveries have been made, and all that remains is to fill in the details. Quantum mechanics, de rigueur, is frequently cited. This view seems to turn on how one classifies Bmajor[ and Bdetails.[ My preference is a proactive stance that seeks new knowledge and application, wherever it is to be found. Disruptive discoveries, while critically important, are infrequent, often fortuitous, and difficult to predict, while expansion of detail evolves continuously with discernible goals. For me the Bbig idea[ is communications over distance. This ancient aspiration and need has seen many incarnations, from Caesar’s voice towers, linking Gaul to Rome by a sequence of leather-lung shouters (compute the channel equivocation!), through various forms of heliograph and their derivatives. Perhaps the greatest impetus came from the discovery of the relationship between electricity and magnetism, leading to telecommunications and spawning its many implementations: telegraph, teletype, telephone, radio, televisionVeach with corresponding transport: wire line, coaxial cable, wave guide, optical fiber, and electromagnetic radiation. A longtime objective has been to make communication at a distance as good as being there; that is, having (as much as possible) the advantages enjoyed in face-to-face information exchange. There are instances where physical presence is mandatory, but others where naturalness without presence is of value. Businesses are rapidly discovering the economics of existing audiovisual conferencing as an alternative to travel. But, there remain Bdetails[ that could substantially expand capabilities. A fledgling and primitive demonstration of concept, called human–machine network (HuMaNet), was reported in the AT&T Technical Journal as early as 1990, and followed by continuing academic work. The U.S. National Science Foundation, during the 1990s, supported research on handsfree multimodal communication, based upon simultaneous signaling in image, speech, and gesture. Communication over distance is machine mediated, and principle sensory modalities are sight, sound, and touch. All will likely receive continued research over the next years. Human– machine interaction for all these modalities, used particularly in combination and simultaneously, offers Digital Object Identifier: 10.1109/JPROC.2012.2190158	amiga reflections;caesar;identifier;information exchange;multimodal interaction;optical fiber;quantum mechanics;software deployment	James L. Flanagan	2012	Proceedings of the IEEE	10.1109/JPROC.2012.2190158	technological change;technology forecasting	HCI	-58.0804651296292	-26.550996043278595	20979
ddfe6c2fd8447265e1a2fe7236863e6218c6bb8e	the coming age of computer graphics and the evolution of language	gesture;language;augmented reality;eccescopy	Sometime in the coming years -- whether through ubiquitous projection, AR glasses, smart contact lenses, retinal implants or some technology as yet unknown -- we will live in an eccescopic world, where everything we see around us will be augmented by computer graphics, including our own appearance. In a sense, we are just now starting to enter the Age of Computer Graphics.  As children are born into this brave new world, what will their experience be? Face to face communication, both in-person and over great distances, will become visually enhanced, and any tangible object can become an interface to digital information [1]. Hand gestures will be able to produce visual artifacts.  After these things come to pass, how will future generations of children evolve natural language itself [2]? How might they think and speak differently about the world around them? What will life in such a world be like for those who are native born to it?  We will present some possibilities, and some suggestions for empirical ways to explore those possibilities now -- without needing to wait for those smart contact lenses	artifact (software development);computer graphics;digital data;natural language;retinal implant;visual artifact	Ken Perlin	2014		10.1145/2659766.2661116	simulation;computer science;multimedia;communication	Graphics	-56.72714081728615	-28.752367596528558	20981
430552e363c2e6617902e887cdcc59f6ba794872	characterizing the performance and behaviors of runners using twitter	social network services;information retrieval;running;physical activity;psychology;medical computing;data analysis social network services twitter physical activity running information retrieval;cdc runner behavior characterization runner performance characterization twitter physical activity physical wellbeing improvement mental wellbeing improvement runner psychological wellbeing information runner performance information running times running frequencies data collection social networking services sns fitness tracking devices personal physical activity information tracking personal physical activity information sharing tweets monitoring runner group tweets messages nike fitness trackers north america;data analysis;twitter global positioning system portable media players sensors cities and towns monitoring;social networking online;sport;twitter;sport information retrieval medical computing psychology social networking online	Running is a popular physical activity that improves physical and mental well being. Unfortunately, up-to-date information about runners' performance and psychological well being is limited. Many questions remain unanswered, such as how far and how fast runners typically run, their preferred running times and frequencies, how long new runners persist before dropping out, and what factors cause runners to quit. Without hard data, establishing patterns of runner behavior and mitigating challenges they face are difficult. Collecting data manually from large numbers of runners for research studies is costly and time consuming. Emerging Social Networking Services (SNS) and fitness tracking devices make tracking and sharing personal physical activity information easier than before. By monitoring the tweets of a runner group on Twitter (SNS) over a 3-month period, we collected 929,825 messages (tweets), in which runners used Nike+ fitness trackers while running. We found that (1) fitness trackers were most popular in North America (2) one third of runners dropped out after one run (3) Over 95% of runners ran for at least 10 minutes per session (4) less than 2% of runners consistently ran for at least 150 minutes a week, which is the level of physical activity recommended by the CDC (5) 5K was the most popular distance.	activity tracker;fitness function;nike+ipod	Qian He;Emmanuel Agu;Diane M. Strong;Bengisu Tulu;Peter Pedersen	2013	2013 IEEE International Conference on Healthcare Informatics	10.1109/ICHI.2013.56	simulation;engineering;multimedia;advertising	Visualization	-36.769908569570745	-50.053414209323634	21010
929743ef1e89ef5b9511e81fce529e37565103bf	is anyone looking? mitigating shoulder surfing on public displays through awareness and protection	design human factors;proxemic interaction;public displays;technical report;privacy;territoriality	Displays are growing in size, and are increasingly deployed in semi-public and public areas. When people use these public displays to pursue personal work, they expose their activities and sensitive data to passers-by. In most cases, such shoulder-surfing by others is likely voyeuristic vs. a deliberate attempt to steal information. Even so, safeguards are needed. Our goal is to mitigate shoulder-surfing problems in such settings. Our method leverages notions of territoriality and proxemics, where we sense and take action based on the spatial relationships between the passerby, the user of the display, and the display itself. First, we provide participants with awareness of shoulder-surfing moments, which in turn helps both parties regulate their behaviours and mediate further social interactions. Second, we provide methods that protect information when shoulder-surfing is detected. Here, users can move or hide information through easy to perform explicit actions. Alternately, the system itself can mask information from the passerby's view when it detects shoulder-surfing moments.	interaction;semiconductor industry	Frederik Brudy;David Ledo;Saul Greenberg;Andreas Butz	2014		10.1145/2611009.2611028	simulation;engineering;social psychology;computer security	HCI	-57.58818769139699	-42.55971058320627	21048
2168ca72b1cf5124d1863178d4cbe79827e54c13	simple user-generated motion cueing can enhance self-motion perception (vection) in virtual reality	input device;wheelchair;self motion perception;virtual reality;visual quality;visual motion;force feedback;motion perception;human factors;motion cueing;self motion simulation;virtual environment;vection;psychophysics	Despite amazing advances in the visual quality of virtual environ-ments, affordable-yet-effective self-motion simulation still poses a major challenge. Using a standard psychophysical paradigm, the effectiveness of different self-motion simulations was quantified in terms of the onset latency, intensity, and convincingness of the per-ceived illusory self motion (vection). Participants were asked to actively follow different pre-defined trajectories through a naturalistic virtual scene presented on a panoramic projection screen using three different input devices: a computer mouse, a joystick, or a modified manual wheelchair. For the wheelchair, participants exerted their own minimal motion cueing using a simple force-feedback and a velocity control paradigm: small translational or rotational motions of the wheelchair (limited to 8cm and 10°, re-spectively) initiated a corresponding visual motion with the visual velocity being proportional to the wheelchair deflection (similar to a joystick). All dependent measures showed a clear enhancement of the perceived self-motion when the wheelchair was used instead of the mouse or joystick. Compared to more traditional approaches of enhancing self-motion perception (e.g., motion platforms, free walking areas, or treadmills) the current approach of using a simple user-generated motion cueing has only minimal requirements in terms of overall costs, required space, safety features, and technical effort and expertise. Thus, the current approach might be promising for a wide range of low-cost applications.	computer mouse;haptic technology;input device;joystick;motion compensation;motion simulator;onset (audio);programming paradigm;projection screen;requirement;simulation;user-generated content;velocity (software development);virtual reality	Bernhard E. Riecke	2006		10.1145/1180495.1180517	computer vision;simulation;motion perception;computer science;virtual machine;artificial intelligence;operating system;virtual reality;haptic technology;psychophysics;input device	Visualization	-44.81886302959541	-48.7078158150398	21059
1129e7e416f3b558bd1df440dc8759f922bb86af	interactive facial robot system on a smart device enhanced touch screen input recognition and robot's reactive facial expression	proposed facial system;input pattern;facial robot;facial simulator;robot reactive facial expression;human-robot interaction;microphones;enhanced touch screen input recognition;emotion;interactive facial robot system;touch screen;display component;interactive;touch sensitive screens;input sensors;smart device;emotional state;input procedure;input sensor;visual output;reactive facial expression;enhanced touch screen input;built-in microphone;face recognition;human robot interaction;face	This paper suggests an interactive facial robot system on a smart device which has a touch screen and a built-in microphone. The recognition process for touch inputs is enhanced by analyzing input patterns and a built-in microphone. Recognized results from the input procedure are converted to emotional states of the system, and then the emotional states are reactively expressed at a facial simulator displayed on the device's touch screen. Therefore, the proposed facial system can be implemented in one smart device at which input sensors and a visual output are on a same display component.	canonical account;facial recognition system;microphone;robot;sensor;simulation;smart device;touchscreen	Won Hyong Lee;Jeong Woo Park;Woo Hyun Kim;Myung Jin Chung	2013	2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI)			Robotics	-42.89185265861344	-43.294055112837114	21064
44ec02290cf936767028428f8d3f23e5c6c0c2ba	a computational model of perceptual saliency for 3d objects in virtual environments		When giving directions to the location of an object, people typically use other attractive objects as reference, that is, reference objects. With the aim to select proper reference objects, useful for locating a target object within a virtual environment (VE), a computational model to identify perceptual saliency is presented. Based on the object’s features with the major stimulus for the human visual system, three basic features of a 3D object (i.e., color, size, and shape) are individually evaluated and then combined to get a degree of saliency for each 3D object in a virtual scenario. An experiment was conducted to evaluate the extent to which the proposed measure of saliency matches with the people’s subjective perception of saliency; the results showed a good performance of this computational model.	3d modeling;algorithm;color;computation;computational model;emoticon;experiment;fits;natural language;point of view (computer hardware company);spatial reference system;spatial variability;time complexity;unit disk graph;virtual reality	Graciela Lara López;Angélica de Antonio Jiménez;Adriana Peña Pérez Negrón	2017	Virtual Reality	10.1007/s10055-017-0326-z	computer science;computer vision;simulation;perception;virtual machine;human visual system model;salience (neuroscience);artificial intelligence	Vision	-42.01280938980248	-51.58774489022853	21097
6a651207414a0a7b9a9c156394ca7696efd2f9ec	zero-rating in emerging mobile markets: free basics and wikipedia zero in ghana	free basics;network neutrality;empirical studies in ubiquitous and mobile computing;ghana;ictd;mobile data;human factors;ubiquitous and mobile devices;mobile phones;human centered computing;non users;article;zero rating;wikipedia zero	Despite widespread controversy surrounding zero-rating---that is, the practice of subsidizing mobile data---the field suffers from a lack of inquiry into user understanding of and experience with zero-rated services. This paper explores how Ghanaian mobile users interact with zero-rated mobile applications Free Basics and Wikipedia Zero. Based on semi-structured interviews with users and non-users of the applications, I discuss how mobile phone users perceive Free Basics and Wikipedia Zero, what motivates them to use or not use the applications, and how the availability of the applications influences their data-buying strategies. Findings suggest that respondents, including those who did not actively use the applications, understood and experienced Free Basics and Wikipedia Zero in ways divergent from the providers' aim of expanding access to online content and services.	mobile app;mobile phone;semiconductor industry;web content;wikipedia;zero-rating	Genevieve Gebhart	2016		10.1145/2909609.2909663	mobile broadband;mobile search;mobile web;computer science;engineering;human factors and ergonomics;human-centered computing;multimedia;internet privacy;world wide web	HCI	-55.97366888162543	-40.992893370906266	21138
cd38141e3f3e72d83ef04d3abf726a0335a1cc80	deploying thick mobile clients using thin client architecture: a case in mobile tourist guides	school of no longer in use;electronics and computer science;mobile device;design and implementation	This paper introduces an approach of enhancing tourism web sites, giving the ability for such sites to be used as a tool to allow tourists to tag content of choice for downloading and viewing on their own mobile device whilst on visit. Tourists, upon installation to their mobile device, can view content material and a map of Mytilene (Lesvos, Greece) without the need to “connect” to any mobile operator network, thereby saving high roaming charges. Our case-study has been the design and implementation of a MultiPlatform Mobile Tourist Guide system for the Municipality of Mytilene (Greece), which uses a thin PC client via Internet to present tourist content giving the user the opportunity to select content of choice and the ability to dynamically build a thick client mobile tourist application or use the PDA Installation available at the Municipal tourist office.	client (computing);download;fat client;mobile app;mobile device;mobile phone;personal digital assistant;thin client;web application;while	Michael Kenteris;Daphne Economou;Damianos Gavalas;Dionysios Zamplaras	2008		10.1007/978-3-540-87783-7_81	mobile search;simulation;mobile web;mobile database;computer science;marketing;mobile technology;mobile device;multimedia;advertising;mobile business development;mobile computing;world wide web	HCI	-51.5136560854947	-40.88873195245083	21164
b7824a52b17b45ce04667d598f8d7ea7ed8954d4	social network analysis and interactive device design analysis	interaction programming;graph theory;social network analysis;network center	What methods can we use to help understand why users adopt certain use strategies, and how can we evaluate designs to anticipate and perhaps positively modify how users are likely to behave? This paper proposes taking advantage of social network analysis (SNA) to identify features of interaction. There are plausible reasons why SNA should be relevant to interaction programming and design, but we also show that SNA has promise, identifies and explains interesting use phenomena, and can be used effectively on conventionally-programmed interactive devices. Social network analysis is a very rich field, practically and theoretically, and many further forms of application and analysis beyond the promising examples explored in this paper are possible.	social network analysis	Harold W. Thimbleby;Patrick Oladimeji	2009		10.1145/1570433.1570453	simulation;computer science;dynamic network analysis;artificial intelligence;data mining	HCI	-60.37641764667185	-43.95929258911447	21237
37ec79d34adfe92550f50596f1b4cc57c2f4374d	water simulation in jupiter ascending	rgb d;blendshapes;face animation;depth sensors	We knew from the beginning that we would be working with Directors renowned for their visual creativity and it would be the responsibility of VFX to help bring the Wachowski's wonderful imaginings to dazzling yet realistic fruition. One of the major sequences in the film is the Chicago Chase, a futuristic dog fight including multiple interactions with the Chicago river. The complex and unusual nature of the chase choreography meant that our highly experienced water team encountered fresh problems. In this talk we will look at the unusual challenges the Chicago Chase Sequence set our team and the technical and artistic solutions found to resolve those challenges.	dogfight;interaction;simulation;visual effects	Fabio Cerrito	2015		10.1145/2775280.2792588	simulation;computer science;artificial intelligence;computer graphics (images)	Graphics	-53.71072131365826	-27.620101321521915	21270
f55c4c861292b757e627199cffaff3f29116b8b0	the role of choice in longitudinal recall of meaningful tactile signals	confidence level;user interface management systems graphical user interfaces haptic interfaces human factors;user interface;haptic icon;factor h;haptic interfaces displays interference user interfaces computer science chromium human factors computer interfaces embedded computing portable computers;human factor longitudinal recall meaningful tactile signal haptic icon embedded interface user interface design;interface design;longitudinal recall;factors;graphical user interfaces;embedded interface;human factors;human factor;human;design;meaningful tactile signal;user interface design;user interface management systems;haptic interfaces;experimentation;languages;h 5 2 user interfaces haptic i o;languages h 5 2 user interfaces haptic i o design experimentation human factors	Haptic icons (brief tactile stimuli with associated meanings) have the potential to convey abstract information through touch; however, there has been little systematic investigation of how sets of perceptually distinct tactile signals can be best utilized to convey meanings, nor of how enduring these associations can be. We hypothesized that when users can choose the signals which will represent specific concepts, their learning and recall will be eased and enhanced. Taking future embedded interfaces as context, we used two sets of 10 distinct tactile signals to compare recall of concept-meaning associations in two conditions: (1) arbitrarily assigned and (2) participant-chosen associations. Participants learned associations in under 20 minutes at 80% accuracy; at 2 weeks, recall of the associations previously learned was 86% with no significant effect of assignment condition. Subjective confidence levels sharply lagged actual performance, with zero expectation of ability to recall at 2 weeks. To the best of our knowledge, this is the first study of either longitudinal recall or the role of user choice on synthesized stimulus-meaning leamability. Its results underscore the eminent practicality of using haptic icons in everyday interface design, suggesting high learnability and a surprising user ability to find their own mnemonics for carefully composed stimuli, regardless of how associations are assigned.	embedded system;haptic technology;learnability	Mario J. Enriquez;Karon E. MacLean	2008	2008 Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems	10.1109/HAPTICS.2008.4479913	human–computer interaction;recall test;computer science;multimedia;communication	HCI	-48.867421859480686	-46.86136049486433	21283
fccdf3b3cf18a1b988e4b0f79507eaef19ff1044	ubimedia based on memory tags	user evaluation;ambient intelligence;proof of concept;mobile phone;ubiquitous media;web survey;evaluation;user interaction;scenarios;memory tags;user acceptance	Ubimedia is a concept where media files can be embedded in everyday objects and the environment. The user can access those files with his/her personal mobile phone simply by touching the physical objects. This facilitates easy access e.g. to video or audio files related to the physical object in question. In parallel with the technical development of the mobile phone platform and the memory tags that will facilitate ubimedia in practice, we have studied possible usages for ubimedia and user acceptance of those usages. User acceptance has been studied in a web survey and in user evaluations of a proof-of-concept. In addition, an ethical assessment has been carried out. The users appreciated especially the simplicity, speed, low cost and reliability of ubimedia. Concerns were related to control over the download with regard to viruses and other unwanted content.	accessibility;download;embedded system;mobile phone	Eija Kaasinen;Timo Tuomisto;Pasi Välkkynen;Iiro Jantunen;Javier Sierra	2008		10.1145/1457199.1457218	computer science;multimedia;internet privacy;world wide web	HCI	-50.532059133721546	-40.66985228353982	21285
9a6fd153ee656093503fda73f3b10b25367c7004	exploiting the robot kinematic redundancy for emotion conveyance to humans as a lower priority task	task priority;emotion conveyance;human robot interaction;info eu repo semantics article;pepper robot;interaccio persona robot;arees tematiques de la upc informatica;social robotics;info eu repo semantics submittedversion;article;robot kinematics	Current approaches do not allow robots to execute a task and simultaneously convey emotions to users using their body motions. This paper explores the capabilities of the Jacobian null space of a humanoid robot to convey emotions. A task priority formulation has been implemented in a Pepper robot which allows the specification of a primary task (waving gesture, transportation of an object, etc.) and exploits the kinematic redundancy of the robot to convey emotions to humans as a lower priority task. The emotions, defined by Mehrabian as points in the pleasure–arousal–dominance space, generate intermediate motion features (jerkiness, activity and gaze) that carry the emotional information. A map from this features to the joints of the robot is presented. A user study has been conducted in which emotional motions have been shown to 30 participants. The results show that happiness and sadness are very well conveyed to the user, calm is moderately well conveyed, and fear is not well conveyed. An analysis on the dependencies between the motion features and the emotions perceived by the participants shows that activity correlates positively with arousal, jerkiness is not perceived by the user, and gaze conveys dominance when activity is low. The results indicate a strong influence of the most energetic motions of the emotional task and point out new directions for further research. Overall, the results show that the null space approach can be regarded as a promising mean to convey emotions as a lower priority task.	humans;robot	Josep-Arnau Claret;Gentiane Venture;Luis Basañez	2017	I. J. Social Robotics	10.1007/s12369-016-0387-2	human–robot interaction;simulation;computer science;artificial intelligence;social robot;robot kinematics	Robotics	-51.17480189164258	-50.41664670876783	21374
a7a4ca8283509bb106707e20694650f579f246f7	robots with projectors: an alternative to anthropomorphic hri	touch sensitive screens control engineering computing graphical user interfaces human robot interaction;touch screen;indirect direct interaction;human robot interaction;graphical user interfaces;large display;robots;humans;large displays;anthropomorphism;facial expression;large display human robot interaction anthropomorphism indirect direct interaction projector;usability;touch screen based direct hri anthropomorphic hri projectors human robot interaction direct interaction anthropomorphic interface moving robot kiosk;floors;projector;robots humans human robot interaction usability graphical user interfaces floors educational institutions	"""Current forms of Human Robot Interaction (HRI) pursue mostly anthropomorphism and direct interaction. That is, the interaction paradigm is based on imitating how """"people"""" interact with one another (e.g. using spoken language, gestures, facial expression, etc.). However, """"direct"""" interaction/contact with the """"robot,"""" often causes significant inconvenience and usability problems. In this paper, we present an alternative to the anthropomorphic interface using a projected display and indirect, yet already familiar GUI based interaction. That is, the projector (on the moving robot) projects information on the nearby surface and provides a relatively large area through which indirect GUI based interaction can occur. As an instance of such an HRI paradigm, we present a moving robot kiosk that projects displays around itself and serve and interact with multiple people at once. We report our on-going development efforts and a pilot experimental study that compares it to the typical touch screen based direct HRI."""	experiment;graphical user interface;human–robot interaction;movie projector;programming paradigm;robot;touchscreen;usability;video projector	Jongkyeong Park;Gerard Jounghyun Kim	2009	2009 4th ACM/IEEE International Conference on Human-Robot Interaction (HRI)	10.1145/1514095.1514146	human–robot interaction;robot;computer vision;simulation;usability;human–computer interaction;computer science;artificial intelligence;graphical user interface;facial expression	HCI	-44.81841830053855	-43.33337150676664	21376
b8c36874a547ca64c83891c7763db28d9bcb339f	neurosurgical simulation system with smooth tactile sensation			simulation	Mayumi Shimizu;Yasuaki Nakamura	2005			distributed computing;computer vision;computer science;sensation;artificial intelligence	Robotics	-36.6994998022325	-39.13991447323438	21428
75169a95c19faf8b7a38a90a2b40528c8454bba5	model-driven gaze simulation for the blind person in face-to-face communication	eye contact;conversation quality;eye tracking	In face-to-face communication, eye gaze is integral to a conversation to supplement verbal language. The sighted often uses eye gaze to convey nonverbal information in social interactions, which a blind conversation partner cannot access and react to them. In this paper, we present E-Gaze glasses (E-Gaze), an assistive device based on an eye tracking system. It simulates gaze for the blind person to react and engage the sighted in face-to-face conversations. It is designed based on a model that combines eye-contact mechanism and turn-taking strategy. We further propose an experimental design to test the E-Gaze and hypothesize that the model-driven gaze simulation can enhance the conversation quality between the sighted and the blind person in face-to-face communication.	assistive technology;design of experiments;eye tracking;interaction;model-driven architecture;model-driven integration;prototype;simulation;testbed;tracking system	Shi Qiu;Siti Aisyah Anas;Hirotaka Osawa;Matthias Rauterberg;Jun Hu	2016		10.1145/2974804.2980482	psychology;computer vision;eye tracking;communication;social psychology	HCI	-54.20866010827598	-46.4883656200297	21440
25a4293849fc224046ff5e348dde83a266f78019	nonvisual presentation of graphical user interfaces: contrasting two approaches	auditory interface;auditory interfaces;tactile interfaces;interface design;blind users;human interface;graphical user interfaces;nonvisual hci;tactile interface;graphic user interface	At the present time almost all information presented goes through the visual channel. This means information can be missed because of visual overload or because the user was not looking in the right place at the right time. An interface that integrates information output to both senses can capitalise on the interdependence between them and present information in the most efficient way possible. A structured method is described for the analysis of interactions to identify situations where hidden information may exist and where non-speech sound might be used to overcome the associated problems. Interactions are considered in terms of events, status and modes to find any hidden information. This is then categorised in terms of the feedback needed to present it. An auditory-enhanced scrollbar, based on the method described, was then experimentally tested. Timing and error rates were used along with subjective measures of workload. Results from the experiment show a significant reduction in time to complete one task, a decrease in the mental effort required and an overall preference for the auditory-enhanced scrollbar. The work reported here is part of a research project looking at the best ways to integrate auditory and graphical information at the interface. The sounds used in this paper are based around structured audio messages called Earcons first put forward by Blattner, Sumikawa & Greenberg [3] and experimentally tested by Brewster, Wright & Edwards [4]. Earcons are abstract, synthetic tones that can be used in structured combinations to create sound messages to represent parts of an interface.	bernard greenberg;brewster's angle;earcon;experiment;feedback;graphical user interface;interaction;interdependence;synthetic intelligence	Elizabeth D. Mynatt;Gerhard Weber	1994		10.1145/191666.191732	computer vision;10-foot user interface;human–computer interaction;computer science;operating system;graphical user interface;multimedia;post-wimp;natural user interface;user interface	HCI	-48.816857004154336	-46.46745449777556	21452
5918390563721ccad323d7bf5a774ca88fa3e36f	tilting operations for small screen interfaces	palmtop computers;interaction techniques;small screen interfaces;interaction technique	This TechNote introduces new interaction techniques for small screen devices such as palmtop computers or handheld electric devices, including pagers and cellular phones. Our proposed method uses the tilt of the device itself as input. Using both tilt and buttons, it is possible to build several interaction techniques ranging from menus and scroll bars, to more complicated examples such as a map browsing system and a 3D object viewer. During operation, only one hand is required to both hold and control the device. This feature is especially useful for field workers.	computer;handheld pc;handheld game console;interaction technique;mobile phone;palmtop pc;scroll lock;television	Jun Rekimoto	1996		10.1145/237091.237115	embedded system;human–computer interaction;computer science;interaction technique;computer graphics (images)	HCI	-44.52912807997229	-41.84631728797712	21462
023afa26125c3a5288eca7a86a76f83e2d474f6e	combination of semantic localization and conversational skills for assistive robots		The recognition of objects and their features is a fundamental task for social robots that could be improved with the combination of different sources of information, such as the ones provided by visual or speech understanding systems. In this paper, we present a first approach to fusion semantic localization and conversational skills for social robots which may act as assistants. Our solution is based on a mobile robot that is able to detect and recognize objects from an environment and store them in its base of knowledge to later act as an assistant for any user who is searching for any object. In the conversation the robot tries to help the user to find a specific object depending of the location and the features of the object which is looking for. The proposal has been empirically evaluated within a research lab where the robot recognizes objects in the environment and the users require, by means of speech commands, finding suitable objects that are placed in the environment.	robot	Daniel González-Medina;Cristina Romero-González;Ismael García-Varea	2018		10.1007/978-3-319-99885-5_5	conversation;social robot;human–computer interaction;robot;mobile robot;human–robot interaction;computer science	Robotics	-34.29719407403679	-41.27850039193549	21514
1c0893de6de53c440ef0bdc702ae3ecac1277dd1	an intelligent intrusion detection system based on upnp technology for smart living	image recognition;protocols;smart home security system intelligent intrusion detection system upnp technology smart living pda home intrusion intrusion image processing universal plug and play instant messaging three step search subject identification;intelligent systems intrusion detection competitive intelligence information security simple object access protocol xml smart homes design engineering image processing image storage;instant messaging;image processing;smart home;authorisation;plug and play;search algorithm;intelligent intrusion detection system;three step search;home intrusion;intrusion detection;image recognition authorisation electronic messaging feature extraction home automation;smart home security system;pda;servers;smart living;block search algorithm intrusion detection system image processing upnp;intrusion image processing;feature extraction;secure system;software component;xml;electronic messaging;artificial intelligence;block search algorithm;universal plug and play;security;user equipment;subject identification;intrusion detection system;upnp technology;home automation;image sequences;upnp	In this paper, an intelligent intrusion detection system, based on UPnP technology for smart home is proposed. In the developed system, home users equipped with PDAs may locally and remotely receive the alert of home intrusion with captured subject image. There are two key components developed in this system. One is the intrusion image processing (IIP) to recognize the intrusion and extract the intrusion subject. The other is the universal-plug-and-play (UPnP-based) instant messaging (UIM) The proposed IIP employs a modified three-step search (TSS) to improve the accuracy of the subject identification. Through the integrating of the two software component, intelligent intrusion detection is capable of offering a smart home security system. The implementation results show the accuracy of subject identification is about 90%~100%, depending on the granulity of the subject searching. The presented system demonstrates a smart and automatic solution in comparison with legacy approaches.	component-based software engineering;home automation;image processing;instant messaging;intrusion detection system;personal digital assistant;universal plug and play;uim	Mong-Fong Horng;Bo-Chao Chang;Bei-Hao Su	2008	2008 Eighth International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2008.361	intrusion detection system;universal plug and play;embedded system;home automation;host-based intrusion detection system;image processing;computer science;world wide web;computer security	Mobile	-34.371237475470515	-44.57004569336504	21746
d9d7bf0def3e515cf21f1c91cbfe12c63d7ff087	sensors model student self concept in the classroom	minimally invasive;intelligent tutoring system;sensor model;facial expression;user model	In this paper we explore findings from three experiments that use minimally invasive sensors with a web based geometry tutor to create a user model. Minimally invasive sensor technology is mature enough to equip classrooms of up to 25 students with four sensors at the same time while using a computer based intelligent tutoring system. The sensors, which are on each student’s chair, mouse, monitor, and wrist, provide data about posture, movement, grip tension, arousal, and facially expressed mental states. This data may provide adaptive feedback to an intelligent tutoring system based on an individual student’s affective states. The experiments show that when sensor data supplements a user model based on tutor logs, the model reflects a larger percentage of the students’ self-concept than a user model based on the tutor logs alone. The models are further expanded to classify four ranges of emotional self-concept including frustration, interest, confidence, and excitement with over 78% accuracy. The emotional predictions are a first step for intelligent tutor systems to create sensor based personalized feedback for each student in a classroom environment. Bringing sensors to our children’s schools addresses real problems of students’ relationship to mathematics as they are learning the subject.	experiment;mental state;minimally invasive education;personalization;poor posture;sensor;user modeling	David G. Cooper;Ivon Arroyo;Beverly Park Woolf;Kasia Muldner;Winslow S. Burleson;Robert Christopherson	2009		10.1007/978-3-642-02247-0_6	simulation;engineering;multimedia;communication	HCI	-56.201357176950104	-46.30097912863535	21761
6d2bc6dba060dbd411cdd4ba50f35c6296ed9b5f	neural preprocessing and control of reactive walking machines - towards versatile artificial perception-action systems		Give us 5 minutes and we will show you the best book to read today. This is it, the neural preprocessing and control of reactive walking machines towards versatile artificial perception action systems cognitive technologies that will be your best choice for better reading book. Your five times will not spend wasted by reading this website. You can take the book as a source to make better concept. Referring the books that can be situated with your needs is sometime difficult. But here, this is so easy. You can find the best thing of book that you can read.	book;preprocessor;situated	Poramate Manoonpong	2007		10.1007/978-3-540-68803-7	perception;preprocessor;engineering;control engineering	NLP	-50.12715252811168	-47.55702820851161	21774
921eb00b4fedc6754729cb77ac431944073e78ce	interactive graphics for transportation systems planning and design	computers;information systems;computer graphics;network analysis planning;transport system;computer graphic;high priority;transportation;transportation planning;private sector;interactive graphics;graphics	This paper summarizes the content and findings of a three-day seminar designed to explore and assess the potential utility of interactive computer graphics for application in the transportation systems planning and design fields. The seminar was structured around the concept of finding good matches between existing technology in the interactive graphics field (capabilities) with high priority and suitable problems in the transportation planning and design fields (needs). The basic objective was to define and specify, as clearly as possible, some guidelines for the U.S. Dept. of Transportation that could assist the formulation of a DOT development and demonstration program strategy for utilizing existing technology in the computer graphics area. The seminar was structured into two parts. The first was tutorial in nature and was designed to provide the participants with an up-to-date description of capabilities and limitations. The second was a review and critique of eight resource papers, prepared especially for and prior to the seminar, which together provided twenty-two recommended development/demonstration projects. Three other recommendations were received from other sources. All twenty-five projects received a priority rating of 1 to 4. Twelve projects were rated as Priority 1 or 2. A wide variety of other recommendations are also included. Fifty persons participated in the seminar, representing a wide variety of activities in both the public and private sectors of the transportation field.	computer graphics;human–computer interaction	Jerry B. Schneider	1974		10.1145/563182.563231	transport;simulation;human–computer interaction;computer science;graphics;multimedia;computer graphics;information system;private sector;transportation planning;computer graphics (images)	HCI	-48.5043435658669	-31.044694803127257	21825
989806a2ed27540aa14f631c2b5dca0ff67cf2c0	motion-sound mapping through interaction: an approach to user-centered design of auditory feedback using machine learning		Technologies for sensing movement are expanding toward everyday use in virtual reality, gaming, and artistic practices. In this context, there is a need for methodologies to help designers and users create meaningful movement experiences. This article discusses a user-centered approach for the design of interactive auditory feedback using interactive machine learning. We discuss Mapping through Interaction, a method for crafting sonic interactions from corporeal demonstrations of embodied associations between motion and sound. It uses an interactive machine learning approach to build the mapping from user demonstrations, emphasizing an iterative design process that integrates acted and interactive experiences of the relationships between movement and sound. We examine Gaussian Mixture Regression and Hidden Markov Regression for continuous movement recognition and real-time sound parameter generation. We illustrate and evaluate this approach through an application in which novice users can create interactive sound feedback based on coproduced gestures and vocalizations. Results indicate that Gaussian Mixture Regression and Hidden Markov Regression can efficiently learn complex motion-sound mappings from few examples.	experience;hidden markov model;interaction;iteration;iterative design;machine learning;markov chain;real-time locating system;user-centered design;virtual reality	Jules Françoise;Frédéric Bevilacqua	2018	TiiS	10.1145/3211826	auditory feedback;machine learning;computer science;user-centered design;artificial intelligence	HCI	-55.111431706579324	-46.86252409949541	21863
1ba462ef24cdad6b815ba1a349868e3bed2a86d8	passive viscous haptic textures	viscosity;haptic interfaces eddy currents viscosity accelerometers rendering computer graphics frequency measurement force measurement virtual reality frequency response pulse width modulation;eddy currents;haptic textures;user interface;haptic device;h 5 1 information interfaces and presentation multimedia information systems artificial;h 5 1 information interfaces and presentation multimedia information systemsâ artificial augmented and virtual realities;h 5 2 information interfaces and presentation user interfaces haptic i o haptic rendering haptic devices haptic textures h 5 1 information interfaces and presentation multimedia information systems artificial augmented and virtual realities;virtual reality;passive viscous haptic textures;viscous force;frequency measurement;h 5 2 information interfaces and presentation user interfaces haptic i o;multimedia information system;h 5 2 information interfaces and presentation user interfacesâ haptic i o;eddy current;augmented;frequency response;brakes linear dynamics;brakes linear dynamics passive viscous haptic textures eddy current brakes synthesis haptic textures viscous damping coefficient brakes haptic texture rendering accelerometer viscous force;haptic rendering;and virtual realities;brakes;rendering computer graphics haptic interfaces;synthesis haptic textures;force measurement;accelerometer;viscous damping coefficient;haptic interfaces;information interfaces and presentation;rendering computer graphics;haptic devices;eddy current brakes;accelerometers;high frequency;pulse width modulation;haptic texture rendering	We describe the use of eddy current brakes for the synthesis haptic textures. Textural effects are achieved through rapid variations of the viscous damping coefficient that these brakes create when activated. We demonstrate that eddy current brakes can be actuated reliably at frequencies typical of haptic texture rendering. Performance is evaluated by measuring the movement of the manipulandum with an accelerometer while modulating the viscous force at high frequency. Key advantages of this technique include guaranteed passivity of the haptic synthesis, accurate results from the linear dynamics of these brakes, and elimination of the need to estimate or observe velocity.	coefficient;haptic technology;velocity (software development)	Gianni Campion;Andrew H. C. Gosline;Vincent Hayward	2008	2008 Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems	10.1109/HAPTICS.2008.4479979	computer vision;simulation;computer science;multimedia	EDA	-41.29465888077163	-42.13737168602422	21913
11841f3366c2d0f81e14285cc8a2fe087dfe9de6	a wavelet network speech recognition system to control an augmented reality object	computers;wavelet networks augmented reality human interaction machine speech recognition system;wavelet transforms augmented reality human computer interaction learning artificial intelligence speech recognition;human interaction machine;speech recognition system;training wavelet network speech recognition system augmented reality object control virtual object control method augmented reality scene human machine interaction;augmented reality;wavelet networks	This paper presents a virtual object control method of augmented reality scene. We have based on control approach on speech recognition. The idea came from human-machine interaction. The speech recognition system is based on wavelet network. In this paper, we have briefly described the used toolkit to do with the augmented reality. Then, we present the speech recognition approach the training and recognition approach. Finally, we present the results.	augmented reality;human–computer interaction;speech recognition;wavelet	Dhekra Bousnina;Ridha Ejbali;Mourad Zaied;Chokri Ben Amar	2013	2013 9th International Conference on Information Assurance and Security (IAS)	10.1109/ISIAS.2013.6947744	computer vision;augmented reality;speech recognition;computer science;computer graphics (images)	Visualization	-37.1421350985698	-43.13653514749843	21937
ec2256c58983905254e1782255e4d43c1d139f34	scentery: a calming multisensory environment by mixing virtual reality, sound, and scent		Scentery proposes a novel approach to create calming multisensory environments by displaying visualizations, reproducing audios and activating olfactory sensations. By the use of recent literature, we introduce an initial Emotive Design Taxonomy that intersects emotions, and colors, sounds and scents. Scentery's users switch between different multisensory scenarios that promote calm sensation. The first VR scenario immerses the user into the scenery of lavender field, which bursts into a carnival of purple, a lavender scent and ambient instrumental sound. The other scenario is the scenery of raining forest, a ylang-ylang scent and nature sound. Scentery was developed with Unity 3D for creating the 3D scenarios, Unity Remote for the camera control and viewer's perspectives, and a microcontroller for triggering the scents in the vaporizer.	color;microcontroller;unity;virtual reality	Elle Luo;Katia Vega	2018		10.1145/3236112.3236135	multimedia;sensation;emotive;virtual reality;computer science	HCI	-48.99379756832806	-35.65206362147927	21945
5ec024f869fb5b3470ff646d53113f90f76b30ea	mapping cityscapes into cyberspace for visualization	georeference;cyberspace;panoramic view;route panorama;visualization;streaming media;virtual tour	This work establishes a cyberspace of a real urban area for visiting on the Internet. By registering entire scenes along every street and at many locations, viewers can visually travel around and find their destinations in cyberspace. The issues we discuss here are mapping of a large-scale area to image domains in a small amount of data, and effective display of the captured scenes for various applications. Route Panoramas captured along streets and panoramic views captured at widely opening sites are associated to a city map to provide navigation functions. This paper focuses on the properties of our extended images—route panorama, addressing the archiving process applied to an urban area, an environment developed to transmit image data as streaming media, and display for scene traversing on the WWW in real time. The created cyberspaces of urban areas have broad applications such as city tour, real estate searching, e-commerce, heritage preservation, urban planning and construction, and vehicle navigation. Copyright # 2005 John Wiley & Sons, Ltd.	anim;archive;cyberspace;e-commerce;john d. wiley;mobile device;seamless3d;smart city;streaming media;virtual tour;virtual world;www	Jiang Yu Zheng;Min Shi	2005	Journal of Visualization and Computer Animation	10.1002/cav.66	georeference;computer vision;visualization;computer science;multimedia;computer graphics (images)	Visualization	-33.83454363011392	-34.691848315505986	21954
6a7e1d9dcdd0b5efc8219198fbcf3635faaa27fa	green software defined radios - enabling seamless connectivity while saving on hardware and energy		It sounds good when knowing the green software defined radios enabling seamless connectivity while saving on hardware and energy integrated circuits and systems in this website. This is one of the books that many people looking for. In the past, many people ask about this book as their favourite book to read and collect. And now, we present hat you need quickly. It seems to be so happy to offer you this famous book. It will not become a unity of the way for you to get amazing benefits at all. But, it will serve something that will let you get the best time and moment to spend for reading the book.	book;integrated circuit;seamless3d	Liesbet Van der Perre;Jan Craninckx;Antoine Dejonghe	2009		10.1007/978-1-4020-8212-2		HCI	-61.571568670848215	-26.90121802162394	21958
da5c1cf545531cee311d61a354bbf629cfb348e1	decision mechanisms for interactive character animations in virtual environment		Recently, interactive character animations in computer games are mainly rely on motion-captured or carefully crafted motion clips. However, it is impractical and difficult to provide motion data samples for all possible behaviors with creating realistic responses to unexpected changes in the world. In order to control characters animations more precisely and realistically in real-time, an decision mechanism that synthesizes animations with interaction between characters in virtual environment is necessary. An efficient near-optimal interactive motion controller that can intelligently adopt the input motion data to a dynamically changing virtual environment is present. The controllers using reinforcement learning to reduce the data requirement by finding the most effective set of motion data to create the desired behaviors, and can be used as basic components in global path planning, and it further reduces the computational processing burden in the real-time interactive applications.	virtual reality	Jun Du;Qiang Liang	2015		10.1007/978-3-662-47791-5_36	computer vision;human–computer interaction;multimedia	Graphics	-38.960082841618515	-36.88068376732683	21979
fe11723a3830fb430456e215f47351d5a958d279	creating synergies between traditional crafts and fablab making: exploring digital mold-making for glassblowing		"""Traditional crafts and the Maker movement have in the last decade to some extent been evolving in parallel, with little intermingling. We held an experimental five day workshop with six experienced craftspeople - """"traditional"""" glassblowers - while providing modern digital Fablab production apparatus; specifically 3D printers and CNC (Computer Numerical Control) routers - to explore possibilities and synergies in working with a traditional craft in conjunction with new generalized digital production possibilities. This paper summarizes seven generalisable takeaways that highlight relevant reflections on the potential for cross-fertilization and learning, enriching the repertoire of both the """"traditional"""" craft and the Fablab, based on a shared interest in exploring aesthetic material exploration and production."""	3d printing;accreditation;amiga reflections;fab lab;fertilization;filamentous fungus;friction;glass;jason;mads domain proteins;rapid update cycle;software prototyping;stage level 1;synergy;meeting	Nicolas Padfield;Mads Hobye;Michael Haldrup;Jason Anthony Knight;Maja Fagerberg Ranten	2018		10.1145/3213818.3213821	multimedia;repertoire;maker movement;3d printing;glassblowing;intermingling;engineering;craft	HCI	-58.74926536826869	-36.07592156218161	21988
9a54593613ca30b4b64f85e400d343659cfeb5e5	tv2web: generating and browsing web with multiple lod from video streams and their metadata	video streaming;metadata;level of detail;generation of web content;video stream;web browser from video streams and their metadat	We propose a method of automatically constructing Web content from video streams with metadata that we call TV2Web. The Web content includes thumbnails of video units and caption data generated from metadata. Users can watch TV ona normal Web browser. They can also manipulate Web content with zooming metaphors to seamlessly alter the level of detail (LOD) of the content being viewed. They can search for favorite scenes faster than with analog video equipment, and experience a new cross-media environment. We also developed a prototype of the TV2Web system and discuss its implementation.	level of detail;original net animation;page zooming;prototype;streaming media;thumbnail;video;web content;world wide web	Kazutoshi Sumiya;Mahendren Munisamy;Katsumi Tanaka	2004	International Conference on Informatics Research for Development of Knowledge Society Infrastructure, 2004. ICKS 2004.	10.1145/1013367.1013494	static web page;web development;framing;web design;computer science;web api;level of detail;web navigation;web page;database;multimedia;internet privacy;client-side scripting;web 2.0;metadata;world wide web	Visualization	-43.78260849952725	-27.58917132632912	22006
656e01ea4088dd2f45f7b1277a69d0175b139714	"""erratum to """"forgetting in the recall-based elicitation of personal and social networks"""": [social networks 22 (2000) 29-43]"""	social network		social network	Devon D. Brewer	2000	Social Networks	10.1016/S0378-8733(00)00032-0	psychology;cognitive psychology;social science;developmental psychology;sociology;social psychology;social network	ML	-51.762523895953606	-33.05976593830065	22029
4b916ceb39dcd70ea410e2f67404f9949eb44cdb	programming by examples - and its applications in data wrangling		Programming by Examples (PBE) has the potential to revolutionize enduser programming by enabling end users, most of whom are non-programmers, to create scripts for automating repetitive tasks. PBE involves synthesizing intended programs in an underlying domain-specific language (DSL) from example based specifications (Ispec). We formalize the notion of Ispec and discuss some principles behind designing useful DSLs for synthesis. A key technical challenge in PBE is to search for programs that are consistent with the Ispec provided by the user. We present a divide-and-conquer based search paradigm that leverages deductive rules and version space algebras for manipulating sets of programs. Another technical challenge in PBE is to resolve the ambiguity that is inherent in the Ispec. We show how machine learning based ranking techniques can be used to predict an intended program within a set of programs that are consistent with the Ispec. We also present some user interaction models including program navigation and active-learning based conversational clarification that communicate actionable information to the user to help resolve ambiguity in the Ispec. The above-mentioned concepts are illustrated using practical PBE systems for data wrangling (including FlashFill, FlashExtract, FlashRelate), several of which have already been deployed in the real world.	active learning (machine learning);cloud computing;data science;debugging;digital revolution;digital subscriber line;domain-specific language;emoticon;end-to-end principle;formal methods;heuristic (computer science);human–computer interaction;interactive programming;killer application;logo;machine learning;modal logic;natural language;norm (social);personal computer;programmer;programming language;programming paradigm;search algorithm;smartphone;sorting;spreadsheet;user (computing);version space learning	Sumit Gulwani	2016		10.3233/978-1-61499-627-9-137	computer science	PL	-37.40418910088949	-29.041376803940615	22032
9a2815dd13ef2e9f3859e10d87550961ca9dcb22	perspectives on agency interacting with and through personal robots		Personal robots present opportunities for understanding the ways that people perceive agency—both in-the-moment and reflectively. Autonomous and interactive personal robots allow us to explore how people come to perceive agency of non-human agents. Remote presence and tele-operation systems are expanding our understandings of how people interact through robots, incorporating these systems into their own sense of agency. As such, robotics can inform our understanding of both robotic agency and human agency.	autonomous robot;television	Leila Takayama	2012		10.1007/978-3-642-25691-2_8	psychology;simulation;communication;social psychology	Robotics	-52.60539332302859	-50.203976843172015	22187
dc2d65f3a697248846b5b9eabba71a705da40361	e-gibalec: mobile application to monitor and encourage physical activity in schoolchildren		We present the e-Gibalec system, designed to encourage schoolchildren towards a more active lifestyle. The system consists of a mobile application that, through sensors built into the smartphone, detects children’s physical activity and rewards them in a game-like manner. It also consists of a web application that allows the parents and physical education teachers to look at the children’s physical activity history, so they can further motivate them if needed. We discuss the motivational mechanisms employed in the system, provide an evaluation of the accuracy of the activity-recognition component, and present a pilot study that measured the effect of our system on a sample schoolchildren population.	activity recognition;ar (unix);autoregressive model;avatar (computing);computational complexity theory;data acquisition;intelligent systems for molecular biology;intelligent environment;interaction;mobile app;purchasing;push technology;sensor;smartphone;wearable computer;wearable technology;web application;world wide web	Vito Janko;Bozidara Cvetkovic;Anton Gradisek;Mitja Lustrek;Boro Strumbelj;Tanja Kajtna	2017	JAISE	10.3233/AIS-170453	human–computer interaction;computer science	HCI	-53.34164431250613	-42.992658220860704	22205
6d5681d1d03885025ab5aff3adbec25c8e95ed7b	a study of micro-augmentations: personality, gender, emotions and effects on attention and brain waves		The concept of micro-augmentations is a new augmented reality tool and methodology. It can provide novel ways to interact with cultural content (including learning content). The purpose of the present work was to study the effects of micro-augmentations on human brain waves and to find possible relations between micro-augmentations and personality characteristics, such as cognitive styles. The gender of the participants and the emotions they experienced during their exposure to micro-augmented content was also recorded and analyzed. An EEG headset was used to measure participantu0027s brainwave activity and waves measured were theta, high beta, low gamma and high gamma, as well as participantsu0027 attention levels. After the experiments, participants answered questionnaire recording the emotions they experienced and their personality characteristics. The results of the study showed that micro-augmentations increased the brain activity and affected peoplesu0027 certain emotions.		Konstantinos Alachouzakis;Nikolaos-Dimitrios Veneris;Spyridon Kavvadias;Angeliki Antoniou;Giorgos Lepouras	2018		10.1145/3291533.3291582	data mining;brain activity and meditation;cognitive psychology;electroencephalography;headset;computer science;human brain;cognitive style;augmented reality;personality	HCI	-54.62928767418744	-49.269077288187766	22276
ae2dc55f6cacd9ad29acdb4201a301a44da1def9	sdm: malleable information graphics	visualizations;moving object;feedback mechanism;selective dynamic manipulation;graphical user interfaces data visualisation interactive systems;interactive techniques;sdm;direct manipulation sdm malleable information graphics selective dynamic manipulation visualizations object set selection interactive techniques user action information analysis tasks;direct manipulation;data visualisation;graphics visualization manipulator dynamics feedback information analysis;visualization;graphical user interfaces;malleable information graphics;information analysis tasks;user action;interactive systems;information analysis;interaction technique;object set selection	Selective dynamic manipulation (SDM) is a paradigm for interacting with objects in visualizations. Its methods offer a high degree of selectivity, in choosing object sets, in the selection of interactive techniques and the properties they affect, and in the degree to which a user action affects the visualization. Our goal is to provide a flexible set of techniques and feedback mechanisms that enable users to move objects and transform their appearance to perform a variety of information analysis tasks.	feedback;graphics;infographic;interaction;programming paradigm;selectivity (electronic)	Mei C. Chuah;Steven F. Roth;Joe Mattis;John Kolojejchick	1995	Proceedings of Visualization 1995 Conference	10.1109/INFVIS.1995.528684	visualization;human–computer interaction;computer science;theoretical computer science;feedback;graphical user interface;multimedia;data analysis;interaction technique;data visualization;statistics	Visualization	-35.42791790489139	-34.12706484651604	22289
ca5cfb02daabf876eb6fdf8b2194a95df1030fd8	storycrate: tabletop storyboarding for live film production	high pressure;professor patrick olivier;team production;live prototyping;dr tom bartindale;tangible interface;dr nick taylor;data representation;broadcast;interactive display;eprints newcastle university;professor peter wright;human resource;collaborative production;open access	Creating film content for broadcast is a high pressure and complex activity involving multiple experts and highly specialized equipment. Production teams are under continuous pressure to produce ever more creative and groundbreaking content while reducing the budgets and human resources required. While technologies are being developed for digitizing and streamlining sections of the production workflow, a gap remains between creative decisions made on location, and those made during digital editing and post-production. We describe a prototype tangible, tabletop interface to be deployed on a film shoot, which uses a storyboard as a shared data representation to drive team creativity. We define creativity in terms of team production, discuss our implementation and describe a deployment in which the prototype was used by a professional production team during a film shoot. Finally we describe a number of interesting interactions that were observed and consider the implications of our design decisions on the creative process of film making and the benefits of tangible, tabletop collaborative interactive displays in live film production.	data (computing);digital media;interaction;prototype;software deployment;storyboard	Tom Bartindale;Alia Sheikh;Nick Taylor;Peter C. Wright;Patrick Olivier	2012		10.1145/2207676.2207700	simulation;human–computer interaction;human resources;computer science;external data representation;high pressure;multimedia;management;world wide web	HCI	-48.34742853030393	-33.703936121756655	22331
e6619fc4c1a2f05a55c2cd001aec435006c65d02	interactive layout method for object diagrams of omt	computer aided software engineering diagrams object oriented methods interactive systems;object oriented methods;springs computer aided software engineering lapping programming tree data structures steel;diagram drawing case tool;diagrams;tree data structures;magnetic spring model;inter node distance;object oriented methodology;overlapping nodes;interactive layout method;computer aided software engineering;springs;object modeling technique;case tool;object diagrams;software development tool;steel;diagram drawing case tool interactive layout method object diagrams omt magnetic spring model object modeling technique overlapping nodes inter node distance;interactive systems;programming;omt;lapping	Proposes a new layout method using the magnetic spring model for object diagrams used in the Object Modeling Technique (OMT). In our new method, there are two main characteristics. Firstly, our new method considers the interactive layout. User can use the layout function many times during the drawing of the diagram. The result of the layout is suitable for continuing to draw the diagram smoothly. Secondly, our method can avoid the overlapping of nodes with a new definition of the distance between nodes. We have implemented the new layout method into an object diagram-drawing CASE tool. We compare the results of the new layout method with results of the old layout method. We have made sure that our new layout method is more suitable for the interactive layout than the old layout method was, and that it avoids node overlaps.		Takayoshi Noguchi;Jiro Tanaka	1999		10.1109/APSEC.1999.809591	layout versus schematic;layout;programming;computer science;systems engineering;theoretical computer science;software engineering;document layout analysis;comprehensive layout;computer-aided software engineering;engineering drawing;lapping	EDA	-36.638932846153956	-30.132363396845363	22354
bae2434506002ffebb26cb280ed095128b52558e	an overview of human interactive robots for psychological enrichment	robot design;human interaction;articulo sintesis;cross cultural evaluation;article synthese;relacion hombre maquina;human robot interaction psychology service robots medical robotics mobile robots manufacturing industries isolation technology educational robots robotic assembly cleaning;man machine relation;mobile robots;robotics;human robot interaction;medical robotics;type of service;handicapped aids;evaluation subjective;industrial robots;subjective evaluation human interactive robots psychological enrichment industrial robots service robots render assistance human robot interactions cross cultural differences;robotica;telerobotics;design;handicapped aids man machine systems telerobotics mobile robots medical robotics;robotique;relation homme machine;value creation;cross cultural differences;review;subjective evaluation;man machine systems;evaluacion subjetiva	The market for industrial robots grew rapidly during the 1970s and 1980s, with a peak demand in 1991. However, due to the subsequent recession in the world economy, the market for industrial robots has been slow or stagnant over the last decade. The value of industrial robots has decreased in that term, even though they have undergone considerable technical advances. On the other hand, human interactive robots for psychological enrichment are a type of service robots that provide a service by interacting with humans while stimulating their minds. As opposed to industrial robots, accuracy or speed is not always of prime importance. Their function or purpose is not simply entertainment, but also to render assistance, to guide, to provide therapy, to educate, to enable communication, and so on. The market for human interactive robots designed for psychological enrichment is expected to grow rapidly and to become more widespread. This overview explains human-robot interactions in terms of the relationship between humans and robots, in terms of the duration of these interactions and in terms of design issues affecting human interactive robots for psychological enrichment. Then examples of cross-cultural differences in the subjective evaluation of human interactive robots are described.	experiment;gene ontology term enrichment;humans;human–robot interaction;industrial robot;interactive storytelling;type of service	Takanori Shibata	2004	Proceedings of the IEEE	10.1109/JPROC.2004.835383	telerobotics;design;simulation;computer science;engineering;artificial intelligence;type of service;robotics	Robotics	-52.437267885465964	-51.93215891652967	22485
9831519aa48a3eb70c99d832418c0a2aa1dde49e	siggraph 2007 opening and title animations		The Electronic Theater Opening Animation is a mixed-media animation that blends actual laser-projected objects in combination with the projected image. For the Title Animations, 46 sounds and 52 animations were combined to create a unique title sequence for each piece in the show. The overall design was inspired by the SIGGRAPH 2007 logo, which depicts a stylized, iconic android as a graphic homage to the conference theme: Face Tomorrow. The Title Animations are featured on the SIGGRAPH Video Review (SVR) as well as www.florianwitzel.com. Contributors	logo;siggraph	Florian Witzel	2007		10.1145/1281740.1281741	human–computer interaction;computer science;multimedia;computer graphics (images)	HCI	-51.86338895516201	-26.475293000602573	22514
fd6ce2f4ba09f707ff963ea734a459d2f19d0161	a touch-operation-based dynamic geometry system: design and implementation		GeometryTouch is a dynamic geometry software system with touch operation. Developed by JavaScript and SVG, GeometryTouch is a Web-based application which can run on browsers of mobile devices. When using GeometryTouch, users can draw geometric figures and create or modify geometry-based interactive manipulatives. A virtual cursor is designed to implement precise operations in GeometryTouch. Geometric operations consist of 4 continuous actions and it is a challenge to implement the action of “the first point confirming”. Three methods have been investigated to tackle the problem in the paper.	list of interactive geometry software	Wei Su;Paul S. Wang;Chuan Cai;Lian Li	2014		10.1007/978-3-662-44199-2_37	scalable vector graphics;web application;theoretical computer science;computer science;software system;cursor (user interface);javascript;systems design;geometry;mobile device	EDA	-42.171053385790934	-31.307253687874386	22553
0812be206f66af3de12bc25b43e574ada54a0b0d	nerdherder: designing for physical actions in an augmented reality puzzle game	human computer interaction;mobile device;design and development;user study;tabletop;motion;game design;natural environment;games;physical interface;puzzle;digital games;augmented reality;mobile augmented reality;experience design	Physical interfaces have been widely adopted in digital games. In our project, we research one such type of physical interface---handheld augmented reality interfaces, and aim to create an engaging and novel play experience. Designing such games requires us to combine knowledge from both Human-computer interaction and game design domains. In this paper, we present a few research questions that we raised or encountered during the process of designing and developing the handheld AR game NerdHerder. We also present our current solutions and answers to these questions. We are going to conduct user studies to understand how handheld AR games are played in the wild. We will release the game and let mobile device users play it in their natural environments. Through the process of designing and iterating NerdHerder, we call for more research that explores how to adopt HCI principles into game design.	augmented reality;handheld game console;human–computer interaction;mobile device;mobile game;physical symbol system	Sam Mendenhall;Vu Ha;Yan Xu;Paul Tillery;Joshua Cohen;John Sharp;Blair MacIntyre	2012		10.1145/2282338.2282388	game design;games;augmented reality;simulation;level design;human–computer interaction;experience design;computer science;game mechanics;motion;game art design;mobile device;game developer;mixed reality;multimedia;natural environment;game design document	HCI	-50.99044707795777	-37.64063879569839	22578
52c353ab37ae5f52f6a19b07c1a7b4a03df9fae0	the connector service-predicting availability in mobile contexts	sensibilidad contexto;interfaz multimodal;red sin hilo;metodo adaptativo;interfase usuario;empirical study;context aware;mobile radiocommunication;informatique mobile;methode empirique;multimodal interface;reseau sans fil;availability;user interface;disponibilidad;metodo empirico;wireless network;empirical method;methode adaptative;intelligence artificielle;radiocommunication service mobile;context aware service;adaptive method;mobile communication;artificial intelligence;interface utilisateur;inteligencia artificial;sensibilite contexte;radiocomunicacion servicio movil;mobile computing;disponibilite;interface multimodale	In this thriving world of mobile communications, the difficulty of communication is no longer contacting someone (the receiver), but rather contacting them in a socially appropriate manner. Ideally, senders should have some understanding of a receiver’s availability in order to make contact at the right time, in the right contexts, and with the optimal communication medium. This paper describes our ongoing research on the Connector, an adaptive and contextaware service designed to facilitate efficient and appropriate communication. We describe a set of empirical studies whose results converge upon the important subject of people’s availability in mobile contexts.s	converge;mobile phone;prototype;robby garner;situated;speaker recognition;speech recognition	Maria Danninger;Erica Robles;Leila Takayama;QianYing Wang;Tobias Kluge;Rainer Stiefelhagen;Clifford Nass	2006		10.1007/11965152_12	telecommunications;computer science;artificial intelligence;empirical research;mobile computing	Mobile	-55.27329032491082	-44.102710838929816	22580
e84d9e27cbaf3d4217f362ca92148efa3b06d9db	supporting 3d and vr applications in a metacomputing environment	3d animation;time scale;real time;virtual surgery;codine;metacomputing;rendering	3D and VR applications require large amounts of computing time. In most applications it has to be available in a specific time span. In Cave or Immersive desk environments the power has to be available real-time. Much preparation can, however, be done on a longer time scale. Complex 3D applications in for instance the media industry require large computation power for rendering purposes in a timescale ranging from hours to a few days. Metacomputing environments, including a large number of machines, can be a useful tool for supporting these VR and 3D applications. Within the EROPPA project, a software environment for use by post production companies has been developed. Currently extensions to virtual surgery applications are investigated.	metacomputing	Ad Emmen;Martijn Mulder;Ion Barosan	1998	Future Generation Comp. Syst.	10.1016/S0167-739X(98)00021-1	real-time computing;simulation;rendering;computer science;operating system;computer animation;computer graphics (images)	Arch	-46.40439482425069	-30.802269682344264	22591
8bd00266c4cf8a6fbd8dbff1ef34c8b8e873f0e2	what your brain says about your password: using brain-computer interfaces to predict password memorability		"""Recent advances in brain-computer interfaces (BCI) have enabled them as affordable consumer-grade devices for nonmedical purposes such as academic research, marketing, and entertainment. We report on the possibility of using BCIs to classify passwords into two classes—one class may be deemed as memorable and the other one as non-memorable—based on electroencephalogram (EEG) potentials collected by the BCI upon presenting the passwords to human participants. The memorable set consists of the most commonly used passwords, also known as """"worst passwords lists"""", while the non-memorable set consists of randomly generated strings of characters, symbols, and numbers. When classifying passwords as memorable vs. nonmemorable, a classification accuracy of 76.5% was achieved. We found a positive correlation between password EEG features and password recall. We also report on users' choice of passwords, where 74% of participants were found to inadvertently choose the password with higher elicited voltage, when presented with two passwords to choose from."""	brain–computer interface;electroencephalography;password strength;procedural generation;random password generator;string (computer science)	Ruba AlOmari;Miguel Vargas Martin;Shane MacDonald;Christopher Bellman;Ramiro Liscano;Eva Maria Willing	2017	2017 15th Annual Conference on Privacy, Security and Trust (PST)	10.1109/PST.2017.00024	brain–computer interface;computer science;computer security;password;task analysis;entertainment;human–computer interaction;recall	HCI	-48.987377074409686	-45.46025615984189	22604
2fbc7fa79aa418a12092b233dc8ea6420ad695da	from interaction designer to product manager	interaction designer;product manager	the product—the team that is going to work on building the product. The output for a product manager is their product roadmap and, more important, a collabora-tive enthusiasm for that roadmap. Fundamentally, a product manager must recruit others to champion their product vision because they typically have no direct management influence over individual contributors. They are the glue that holds the product together. Though the two roles are distinct , they have a large area of overlap. Both disciplines care about people and look to human behavior to understand what to build. Both disciplines care about technology and are tasked with making it easier for people to achieve their goals. And both disciplines are storytellers by necessity: Designers and product managers have neither carrot the right thing to be built; this story often shows how a design improves an existing situation. Product management is about building and shipping the right product to the right people at the right time. It's about finding market opportunity, understanding product/person fit and product/ market fit, and shaping a product that can be built and shipped quickly and efficiently. A product manager sits at the hub of a set of disciplinary spokes and works to align various constituents around a set of features, functions, and goals. The secret ingredient for a product manager is broad-based empathy: feeling what it's like to be in the shoes of a community of people who will engage with the design. This broad form of empa-thy is gained by absorbing market signals from related communities of people. This helps a product manager identify the right thing to build and the right time to build it in order to deliver the most value to the largest group of potential product users. The input for a product manager includes existing technical capabilities, a rich understanding of the business and market goals, and an awareness of broad patterns of competitive product use. The constraints placed on a product manager are the resources available to actually design, build, and Interaction design is about shaping behavior—about creating a rep-resentational dialogue between a person and technology. An interaction designer thinks mostly about people and works to craft an interface on top of technology to help a person achieve his or her goals. The secret ingredient for an interaction designer is empathy with a specific person: feeling what it's like to be in the shoes of another …	align (company);interaction design;noise shaping;reputation;shoes;the hub (forum);usb hub	Jon Kolko	2013	Interactions	10.1145/2517443	human–computer interaction;engineering;interaction design	HCI	-59.42632331613644	-34.931063853068565	22605
177b13d973dcc2f359f2083f5eb6f5c3a59527ca	moving on from weiser's vision of calm computing : engaging ubicomp experiences	sensibilidad contexto;distributed system;engaged living;pistage;systeme reparti;context aware;surveillance;ambient intelligence;pervasive computing;user experiences;rastreo;ubi comp history;intelligence artificielle;calm computing;computer vision;informatica difusa;proactive computing;vigilancia;sistema repartido;monitoring;informatique diffuse;weiser;user experience;artificial intelligence;inteligencia artificial;monitorage;sensibilite contexte;monitoreo;pervasive technologies;tracking	A motivation behind much UbiComp research has been to make our lives convenient, comfortable and informed, following in the footsteps of Weiser’s calm computing vision. Three themes that have dominated are context awareness, ambient intelligence and monitoring/tracking. While these avenues of research have been fruitful their accomplishments do not match up to anything like Weiser’s world. This paper discusses why this is so and argues that is time for a change of direction in the field. An alternative agenda is outlined that focuses on engaging rather than calming people. Humans are very resourceful at exploiting their environments and extending their capabilities using existing strategies and tools. I describe how pervasive technologies can be added to the mix, outlining three areas of practice where there is much potential for professionals and laypeople alike to combine, adapt and use them in creative and constructive ways.	ambient intelligence;artificial general intelligence;bricolage;computer;context awareness;experience;humans;mind;norm (social);pervasive informatics;proactive parallel suite;theory;tom;ubiquitous computing	Yvonne Rogers	2006		10.1007/11853565_24	user experience design;simulation;ambient intelligence;human–computer interaction;computer science;artificial intelligence;operating system;tracking;computer security;ubiquitous computing	HCI	-50.62751242070837	-32.687830150448825	22674
19a7a14546ed2e5e6f77c363b29027621a6dad5c	briefing news reporting with mobile assignments: perceptions, needs and challenges	assignment;mobile;news;user study;task;journalist;location;information content;professional;work;handheld device;smartphone;crowdsourcing;privacy	Mobile handheld devices are an increasing part of everyday fieldwork of news professionals. Mobile assignments delivered to mobile journalists' smartphones are one potential future development step. We present findings on using mobile assignments from two exploratory user studies in which smartphones were used as news reporting tools. Mobile assignments were perceived as handy for fast reporting situations and simple stories but challenging in case of more complex tasks. Structured information content of assignments, process phase based information and supporting situation and activity awareness would support the work of both editorial staff and mobile journalists. The locationing of reporters for sending location-based assignments was found acceptable for coordinating the work although some privacy concerns were expressed. The findings provide new information on using mobile assignments in work where carrying out tasks involves creativity and the tasks may be complex, not strictly limited or they may not have clear completion criteria.	field research;handy board;mobile device;self-information;smartphone	Heli Väätäjä;Paul Egglestone	2012		10.1145/2145204.2145280	human–computer interaction;news;computer science;operating system;assignment;multimedia;internet privacy;work;location;communication;privacy;social psychology;world wide web;crowdsourcing	HCI	-57.571102459209946	-42.898469651122305	22857
b9893119652a559e38fbb73fc84bd0c329b11636	integração de contextos e habilidades pessoais, sociais e profissionais no desenvolvimento de soluções tecnológicas para o profissional da saúde	contextos profissional pessoal e social;design baseado nas habilidades do profissional;model of design process;design based on abilities professional;natural interaction;interacao natural;social and personal contexts;integration of professional;modelo de processo de design;profissional da saude;healthcare professional;tese			Janaina Cintra Abib	2016				Vision	-52.00999797641724	-33.44655375190683	22900
77d07f58950183991742654c7549fc096c4ff859	evaluating non-speech sound visualizations for the deaf	etude utilisateur;etude utilisation;sound visualization;sonido;surdite;peripheral display;visualizacion;accesibilidad;ambient noise;user study;information technology;estudio utilizacion;estudio usuario;user feedback;utilisateur defavorise;technologie information;sensory handicap;usuario desaventajado;handicap sensoriel;visualization;bruit ambiant;sound;peripheral displays;ruido ambiente;visualisation;design guideline;accessibility;son;desventaja sensoria;disadvantaged user;deaf;hearing loss;tecnologia informacion;accessibilite;use study;sordera;non speech sound	Sounds such as co-workers chatting nearby or a dripping faucet help us maintain awareness of and respond to our surroundings. Without a tool that communicates ambient sounds in a nonauditory manner, maintaining this awareness is difficult for people who are deaf. We present an iterative investigation of peripheral, visual displays of ambient sounds. Our major contributions are: (1) a rich understanding of what ambient sounds are useful to people who are deaf, (2) a set of visual and functional requirements for a peripheral sound display, based on feedback from people who are deaf, (3) lab-based evaluations investigating the characteristics of four prototypes, and (4) a set of design guidelines for successful ambient audio displays, based on a comparison of four implemented prototypes and user feedback. Our work provides valuable information about the sound awareness needs of the deaf and can help to inform further design of such applications.	bitcoin faucet;functional requirement;iteration;peripheral	Tara Matthews;Janette Fong;F. Wai-ling Ho-Ching;Jennifer Mankoff	2006	Behaviour & IT	10.1080/01449290600636488	speech recognition;visualization;computer science;engineering;communication;information technology	HCI	-55.36204907488117	-44.069612989678554	22911
07b8fa1a63b6a2c631da40936915b64b315bb2b5	simultaneity in perception of knocking	forward backward;telesurgery;robots force data models trajectory delay vectors haptic interfaces;sensory event;perception active guidance haptic motor learning motor system;motor system;haptics;force;data model;augmented reality system;active movement;robots augmented reality haptic interfaces neurophysiology;trajectory;vectors;structure and function;robots;telepresence application knocking impact perception forward backward motion sensory event tactile event proprioceptive information central nervous system robotic device active movement passive movement motor system engineering knowledge augmented reality system haptics telerobotics telesurgery;engineering knowledge;telerobotics;robotic device;forward backward motion;telepresence application;neurophysiology;perception;augmented reality;motor learning;haptic interfaces;tactile event;haptic;proprioceptive information;central nervous system;knocking impact perception;active guidance;passive movement;data models;haptic interface	-“Knock, knock-who's there?” Here, we do not address this question but rather the underlying mechanism behind the perception of knocking impacts. When one knocks on a surface, her hand makes a forward-backward motion and, at the point of reversal, the knuckles collide with the rigid surface. How does one perceive the unity, or simultaneity, of the sensory events associated with this impact? Does this binding derive from a temporal estimate of simultaneity, or does the brain use some other mechanism? In this paper, we ask whether the tap and the reversal of the hand are perceived as happening together, since both took place at the same time or at a particular state of motion. The aim of this research is to find out whether a tactile event and the flow of proprioceptive information regarding the state of the arm are matched within the central nervous system according to time or state. We tested this experimentally with subjects who actively moved one arm, as well as subjects who were servoed by a robotic device. Our results suggest that time is the mechanism used for judging the unity of the modalities for both active and passive movements. Taken together, these results provide a useful cue for neuroscientists as to the structure and function of the perceptual and motor systems and essential engineering knowledge for the development of effective and realistic augmented reality systems with haptics for the telerobotics, telesurgery, and telepresence applications of the future.	augmented reality;experiment;haptic technology;port knocking;remote surgery;robot;telerobotics;unity	Assaf Pressman;Amir Karniel;Ferdinando A. Mussa-Ivaldi	2012	IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans	10.1109/TSMCA.2012.2183354	computer vision;augmented reality;simulation;computer science;artificial intelligence;haptic technology;neurophysiology	Robotics	-45.244673368798374	-49.89233933224928	22959
1ba24eaa096936147ea785239c7bdde4c63f5465	place storming: performing new technologies in context	brainstorming;pervasive gaming;innovation techniques	We present Place Storming, an original method of brainstorming technological concepts, particularly in the area of pervasive computing. Place Storming is context-driven and play-based, combining real world environments with the immersive and performative aspects of gaming. In this paper, we discuss the background and techniques we used to create and deploy our method. Examples are drawn from a February 2004 Place Storming event to highlight key strengths of the method. Suggestions are made for what produces successful Place Storming sessions.	immersion (virtual reality);software testing controversies;ubiquitous computing	Ken Anderson;Jane McGonigal	2004		10.1145/1028014.1028026	simulation;human–computer interaction;brainstorming;computer science;multimedia	HCI	-57.09907290293727	-35.18921740023454	23007
d8525c001065ccccf371bb7b4a74975e59c65974	virtual interactive environment for low-cost treatment of mechanical strabismus and amblyopia		This study presents a technique that uses an interactive virtual environment for the rehabilitation treatment of patients with mechanical strabismus and/or amblyopia who have lost eye movement. The relevant part of this treatment is the act of forcing the two eyes to cooperate with each other by increasing the level of adaptation of the brain and allowing the weak eye to see again. Accordingly, the game enables both eyes to work together, providing the patient with better visual comfort and life quality. In addition, the virtual environment is attractive and has the ability to overcome specific challenges with real-time feedback, coinciding with ideal approaches for use in ocular rehabilitation. The entire game was developed with free software and the 3D environment, which is made from low-cost virtual reality glasses, as well as Google Cardboard which uses a smartphone for the display of the game. The method presented was tested in 41 male and female patients, aged 8 to 39 years, and resulted in the success of 40 patients. The method proved to be feasible and accessible as a tool for the treatment of amblyopia and strabismus. The project was registered in the Brazil platform and approved by the ethics committee of the State University of Piaui—UESPI, with the CAAE identification code: 37802114.8.0000.5209.		Arata Andrade Saraiva;Matheus Pereira Barros;Alexandre Tolstenko Nogueira;Nuno M. Fonseca Ferreira;António Valente	2018	Information	10.3390/info9070175	machine learning;artificial intelligence;google cardboard;computer science;mechanical strabismus;software;strabismus;human–computer interaction;rehabilitation;virtual reality;eye movement;virtual machine	HCI	-41.86937698197019	-46.474900375669215	23088
9e6d10719e47d75abb7ca748f8fc9b05b9274146	hallway based automatic indoor floorplan construction using room fingerprints	context sensing;indoor localization;indoor floorplan	People spend approximately 70% of their time indoors. Understanding the indoor environments is therefore important for a wide range of emerging mobile personal and social applications. Knowledge of indoor floorplans is often required by these applications. However, indoor floorplans are either unavailable or obtaining them requires slow, tedious, and error-prone manual labor.  This paper describes an automatic indoor floorplan construction system. Leveraging Wi-Fi fingerprints and user motion information, this system automatically constructs floorplan via three key steps: (1) room adjacency graph construction to determine which rooms are adjacent; (2) hallway layout learning to estimate room sizes and order rooms along each hallway, and (3) force directed dilation to adjust room sizes and optimize the overall floorplan accuracy. Deployment study in three buildings with 189 rooms demonstrates high floorplan accuracy. The system has been implemented as a mobile middleware, which allows emerging mobile applications to generate, leverage, and share indoor floorplans.	cognitive dimensions of notations;converge;crowdsourcing;data point;dilation (morphology);fingerprint;image noise;indoor positioning system;middleware;mobile app;software deployment	Yifei Jiang;Xiang Yun;Xin Pan;Kun Li;Qin Lv;Robert P. Dick;Li Shang;Michael Hannigan	2013		10.1145/2493432.2493470	simulation	HCI	-38.22993452100028	-44.70698726303078	23089
ad81dafb878471a2912fe59237ea1d8166328e8c	not just a face in the crowd: addressing the intrusive potential of the online application of face recognition technologies			facial recognition system;web application	Niloufer Selvadurai	2015	I. J. Law and Information Technology	10.1093/ijlit/eav006	computer vision;multimedia	HCI	-53.05972880638571	-33.51819163457201	23095
46729afac10b6fdc826dbe1f2bbf94da0c44dbd6	collaborative manipulation of 3d virtual objects in augmented reality scenarios using mobile devices	h 5 3 information interfaces and presentation group and organization interfaces computer supported cooperative work;h 5 2 information interfaces and presentation user interfaces input devices and strategies	Interaction in augmented reality environments may be very complex, depending on the degrees of freedom (DOFs) required for the task. In this work we present a 3D user interface for collaborative manipulation of virtual objects in augmented reality (AR) environments. It maps position - acquired with a camera and fiducial markers - and touchscreen input of a handheld device into gestures to select, move, rotate and scale virtual objects. As these transformations require the control of multiple DOFs, collaboration is proposed as a solution to coordinate the modification of each and all the available DOFs. Users are free to decide their own manipulation roles. All virtual elements are displayed directly on the mobile device as an overlay of the camera capture, providing an individual point of view of the AR environment to each user.	3d user interaction;augmented reality;fiducial marker;map;mobile device;point of view (computer hardware company);touchscreen;user interface	Jerônimo G. Grandi;Iago U. Berndt;Henrique Debarba;Luciana P. Nedel;Anderson Maciel	2017	2017 IEEE Symposium on 3D User Interfaces (3DUI)	10.1109/3DUI.2017.7893373	augmented reality;simulation;human–computer interaction;computer science;multimedia	Visualization	-43.45424270334466	-38.833402407132205	23104
25263df04d20e33577a41cd4b934c8cbface0e60	online handwriting recognition using depth sensors	reliability manganese visualization accuracy	In this work, we propose an online handwriting solution, where the data is captured with the help of depth sensors. Users may write in the air and our method recognizes it in real time using the proposed feature representation. Our method uses an efficient fingertip tracking approach and reduces the necessity of pen-up/pen-down switching. We validate our method on two depth sensors, Kinect and Leap Motion Controller. On a dataset collected from 20 users, we achieve a recognition accuracy of 97.59% for character recognition. We also demonstrate how this system can be extended for lexicon recognition with reliable performance. We have also prepared a dataset containing 1,560 characters and 400 words with the intention of providing common benchmark for handwritten character recognition using depth sensors and related research.	algorithm;benchmark (computing);handwriting recognition;human–computer interaction;kinect;lexicon;modality (human–computer interaction);motion controller;optical character recognition;real-time computing;sensor;usability	Rajat Aggarwal;Sirnam Swetha;Anoop M. Namboodiri;Jayanthi Sivaswamy;C. V. Jawahar	2015	2015 13th International Conference on Document Analysis and Recognition (ICDAR)	10.1109/ICDAR.2015.7333924	computer vision;speech recognition;computer science;machine learning;data mining	Robotics	-37.3140738235668	-44.71990786698686	23156
07a339fe559a10444a96f7c6ec24954cba16649f	exploring design for multi-device, multi-environment and multimodal connected experiences		Increasing user encounters with connected devices, IoT, Smart Home and Connected Cars have been motivating designers and HCI researchers need to craft interaction solutions for such scenarios of the future. Designing solutions for scenarios, which we named M3 that involves multiple contexts (Multi-Envi‐ ronment) with users using connected devices (Multi-Device), using natural inter‐ actions (Multimodal) is complex. In this research, we employed visual stimuli and activity based methodology to explore such scenarios for connected home infotainment and connected car contexts. We explored the interrelationships among these M3 aspects and identified user preferences to evolve design direction for designing effective interaction, user workflows and tasks for such encounters. This helped us evolve the M3 Design framework which describes cause-effect relationships among various themes of a sample scenario of connected cars. We also present the applicability of the framework as a reference tool for brain‐ storming, comparative evaluation of design alternatives and solution detailing.	connected car;human–computer interaction;multimodal interaction;smart tv;user (computing)	Himanshu Bansal;Sai Shruthi Chivukula;Sanjay Ghosh	2016		10.1007/978-3-319-39862-4_2	brainstorming;multimodal interaction;home automation;human–computer interaction;computer science;workflow;internet of things	HCI	-53.13737946940106	-38.19753819562523	23172
cc267b9e57774b4c009cbb5b5e76d2374b138ec8	an approach to assess the quality of collaboration in technology-mediated design situations	collaboration;computer supported collaborative learning;design;methodology;cognitive ergonomics	Our objective is to measure and compare the quality of collaboration in technology-mediated design activities. Our position is to consider collaboration as multidimensional. We present a method to assess quality of collaboration which is composed of seven dimensions concerning communication processes such as grounding, coordination processes, taskrelated processes, symmetry of individual contributions as well as motivational processes. This method is used in a study aiming to compare the quality of collaboration in architectural design. In this experimental study, design situations vary according to technology-mediation co-presence with an augmented reality (AR) environment versus distance with AR and visio-conferencing -, and according to number of participants pairs versus groups of four architects -. Our results show that distinctive dimensions of collaboration are affected by the technology mediation and/or the number of co-designers. We discuss these results with respect to technology affordances such as visibility and group factors.	augmented reality;experiment;microsoft visio	Jean-Marie Burkhardt;Françoise Détienne;Anne-Marie Hébert;Laurence Perron;Stéphane Safin;Pierre Leclercq	2009			design;simulation;human–computer interaction;engineering;knowledge management;methodology;cognitive ergonomics;collaboration	HCI	-62.166141232032686	-40.0220016612591	23173
b4a9b5ca1fcedbf6f081d318a2fe68eab9a8af3f	computer-human interaction. cognitive effects of spatial interaction, learning, and ability		Traditional digital sculpting software offers a wealth of specificpurpose tools discouraging artist play in the environment. These tools interact with the projection of the geometry to a flat screen, through the use of a mouse or graphics tablet devices poorly suited to this domain given their lack of a direct method for indicating the depth of interactions. This project investigated the use of full body gestures to facilitate such artistic expression. Skeletal data drives a natural user interface providing users with the ability to sculpt a virtual clay-like substance into different forms. The application was tested in this mode, as well as with a supporting secondary interface for perceived and measured speed and accuracy, users’ reported fatigue and ease of use. This secondary interface provided various touch gestures on a smartphone held in the user’s right hand, while supplementing the positional data provided by the Kinect with orientation data. Results indicated that users were able learn the interface quickly, but depth-perception, grip detection and speech performance were lacking. The secondary interface resulted in fewer undo events, though users reported it as offering little benefit and awkward to use.	computer mouse;depth perception;digital sculpting;direct method in the calculus of variations;flat panel display;graphics tablet;human–computer interaction;kinect;natural user interface;smartphone;tablet computer;undo;usability	Josef Kittler;Moni Naor	2013		10.1007/978-3-319-16940-8	human–computer interaction;computer science;cognition	HCI	-45.30754632971658	-43.12041074570177	23182
119e82ca8ad62e019e75003f31dd03f9f61f33c4	reflections on the work of c. a. r. hoare	eprints newcastle university;professor cliff jones;open access	What do you do to start reading reflections on the work of c a r hoare? Searching the book that you love to read first or find an interesting book that will make you want to read? Everybody has difference with their reason of reading a book. Actuary, reading habit must be from earlier. Many people may be love to read, but not a book. It's not fault. Someone will be bored to open the thick book with small words to read. In more, this is the real condition. So do happen probably with this reflections on the work of c a r hoare.	amiga reflections;hoare logic;reflection (computer graphics)	Angela Hoare	2010		10.1007/978-1-84882-912-1	computer science;design by contract;artificial intelligence;functional logic programming;programming paradigm;hoare logic;inductive programming;fifth-generation programming language;programming language theory;axiomatic semantics;algorithm	AI	-60.589214422238165	-24.327089506602665	23183
a445b33e453c5cc3579d8b72553a0329db686f8a	study on 3d solid reconstruction from 2d views based on intelligent uunderstanding of mechanical engineering drawings	feature recognition;multiple views;mechanical engineering;boolean operation;use case;3d reconstruction	This study presents a new algorithm of 3D reconstruction to obtain automatically 3D solid from mechanical engineering drawings.  Main steps of the algorithm are as follows: first, reconstruct multiple views from dxf-type files; second, recognize primitives  feature and combined relationship between primitives by engineering semantics understanding and loops searching on views;  third, partition and reconstruct body primitives; last, all reconstructed primitives are assembled into a goal solid by Boolean  operation. The algorithm adopts integrated reconstruction techniques to enlarge the figuration domain of reconstructed objects.  And it uses case-based reasoning to set up a self-learning mechanism of feature recognition. It is implemented at the desktop  of AutoCAD 2004 by using Object ARX 2002 and VC++6.0. The results show that utility scope and reconstruction speed are improved  when using the new algorithm.  	engineering drawing	Jianping Liu;Bangyan Ye;Xiaohong Wu;Miaoan Ouyang	2006		10.1007/0-387-34403-9_98	computer vision;engineering;theoretical computer science;engineering drawing	Robotics	-37.622636186174994	-31.76799499261974	23211
120f0bafe7f8d51d956dfca32d6569fd4d2fa208	the effects of social exclusion on play experience and hostile cognitions in digital games	video games;motivation;toxicity;player experience;social exclusion;aggression;ostracism	The social nature of multiplayer games provides compelling play experiences that are dynamic, unpredictable, and satisfying; however, playing digital games with others can result in feeling socially excluded. There are several known harmful effects of ostracism, including on cognition and the interpretation of social information. To investigate the effects of social exclusion in the context of a multiplayer game, we developed and validated a social exclusion paradigm that we embedded in an online game. Called Operator Challenge, our paradigm influenced feelings of social exclusion and access to hostile cognitions (measured through a word-completion task). In addition, the degree of experienced belonging predicted player enjoyment, effort, and the number of hostile words completed; however, the experience measures did not mediate the relationship between belonging and access to hostile cognitions. Our work facilitates understanding the causes and effects of exclusion, which is important for the study of player experience in multiplayer games.	cognition;embedded system;experience;experiment;programming paradigm;virtual world	Max Birk;Benjamin Buttlar;Jason T. Bowey;Susanne Poeller;Shelby C. Thomson;Nicola Baumann;Regan Lee Mandryk	2016		10.1145/2858036.2858061	simulation;motivation;social exclusion;toxicity	HCI	-54.65450724286602	-51.85111540175529	23327
471414d1736178deb67dd7925f2aff4b82ab2b90	storyply: designing for user experiences using storycraft		The role of design shifts from designing objects towards designing for experiences. The design profession has to follow this trend but the current skill-set of designers focuses mainly on objects; their form, function, manufacturing and interaction. However, contemporary methods and tools that support the designers’ creative efforts provide little help in addressing the subjective, context-dependent and temporal nature of experiences. Designers hence need to learn by trial and error how to place experiences at the center of their creative intentions. We are convinced that there is room for new tools and methods that can assist them in this process. In this chapter, we argue that storycraft can offer part of the guidance that designers require to put experiences before products right from the very start of the design process. First, we establish the background behind the shift from products to experiences and explain the challenges it poses for the designer’s creative process. Next we explore the contemporary conceptual design process to understand its shortcomings, point out the opportunity that storycraft offers and propose our approach to take on this challenge. Last but not least, we propose a specific method called Storyply that we have designed and developed iteratively by testing it in conceptual design workshops with students and professionals.		Berke Atasoy;Jean-Bernard Martens	2016		10.1007/978-3-319-29155-0_9	user journey	HCI	-61.23547822138803	-36.91692403708441	23339
c714d68912fedca8c0508774080181cb552d91ad	context-based conversational hand gesture classification in narrative interaction	conversation analysis;small group;contextual information;hand gesture recognition	Communicative hand gestures play important roles in face-to-face conversations. These gestures are arbitrarily used depending on an individual; even when two speakers narrate the same story, they do not always use the same hand gesture (movement, position, and motion trajectory) to describe the same scene. In this paper, we propose a framework for the classification of communicative gestures in small group interactions. We focus on how many times the hands are held in a gesture and how long a speaker continues a hand stroke, instead of observing hand positions and hand motion trajectories. In addition, to model communicative gesture patterns, we use nonverbal features of participants addressed from participant gestures. In this research, we extract features of gesture phases defined by Kendon (2004) and co-occurring nonverbal patterns with gestures, i.e., utterance, head gesture, and head direction of each participant, by using pattern recognition techniques. In the experiments, we collect eight group narrative interaction datasets to evaluate the classification performance. The experimental results show that gesture phase features and nonverbal features of other participants improves the performance to discriminate communicative gestures that are used in narrative speeches and other gestures from 4% to 16%.	experiment;gesture recognition;interaction;pattern recognition	Shogo Okada;Mayumi Bono;Katsuya Takanashi;Yasuyuki Sumi;Katsumi Nitta	2013		10.1145/2522848.2522898	computer vision;speech recognition;gesture recognition	HCI	-50.98646539387231	-49.03138621502916	23345
10594ba8d5b498ce89bc5345685ec81ccd29470a	adaptive user-centered design for safety and comfort of physical human nursing - care robot interaction	safe transferring;nursing care robot;physical human robot interaction	Nowadays serving robots are more and more popular in human society. However, most of them are designed for the special people or for the special scenario. There is little robot designed to apply appropriate interface for different people that can accommodate age-related and body-related in physical interaction. We propose that user-centered design should be used in physical Human-robot interaction. In this research, we take a nursing-care robot as an example. Based on the results of the experiment, we proved that the distance between two arms of nursing-care robot, which affected the comfort and safety of patient, should be applied by different patients with different body length. We try to build the adaptive human robot interface based on the physical properties of people, such as body length. This study is an attempt to explore the adaptive human robot interaction and contributes to giving insights and implications for the future design of general serving robot.	coat of arms;human–computer interaction;human–robot interaction;kinect;lambda lifting;robot;sensor;user-centered design;vocabulary	Minghui Sun;Hiromichi Nakashima;Shinya Hirano;Kazuya Matsuo;Ming Ding;Chang-an Jiang;Toshiharu Mukai;Guihe Qin	2013		10.1007/978-3-642-39173-6_43	robot learning;simulation;engineering;artificial intelligence;social robot;communication;mobile robot navigation;personal robot	Robotics	-41.05083745850756	-45.932023591727315	23367
a1f9c2fa5609f5016dab554863fe87cba5ac028e	augmented reality cubes for cognitive gaming: preliminary usability and game experience testing		Early detection is important in dementia care; however, cognitive impairment is still under-recognised and under-diagnosed. Cognitive screening and training are two important preventative treatments, which can lead to early detection of cognitive decline. In this work, the “Cognitive Augmented Reality Cubes” (CogARC) system is presented, i.e. a serious game for cognitive training and screening, utilising an interaction technique based on Augmented Reality and the manipulation of tangible, physical objects (cubes). The game is a collection of cognitive mini-games of preventative nature and is, primarily, targeting elderly players (≥60 years old). A preliminary testing was conducted focusing on the game experience that CogARC offers (utilising the In-Game Experience Questionnaire), the usability of the system (using the System Usability Scale), and the specific user observations and remarks, as documented by open, semi-structured interviews. Overall, CogARC demonstrated satisfying positive responses, however, the negative reactions indicated that there are specific problems with aspects of the interaction technique and a number of mini-games. The open interview shed more light on the specific issues of each minigame and further interpretation of user interactions. The current study managed to provide interesting insights into the game design elements, integration of Augmented Reality, tangible interaction of the system, and on how elderly players perceive and use those interaction components.	augmented reality;cognition;cubes;interaction technique;semiconductor industry;system usability scale;tangible user interface	Costas Boletsis;Simon McCallum	2016	Int. J. Serious Games		simulation;human–computer interaction;engineering;multimedia	HCI	-58.20485583545766	-46.647568584907965	23432
349a83f38f5bebacb61d65fedd2a70fd6daea56c	nanoar: mobile ar application with microscopic interaction	gpu computing;motion estimation;mpeg 4;augmented reality	We propose a novel mobile AR (Augmented Reality) application that uses a microscopic interaction prototype called NanoAR. A user can view the AR content using a USB or an iPhone microscope via NanoAR. The objects that the user can view using this application are related to the magnification rate; in other words, the application resembles a microscope in the real world. This indicates that a user can view two different objects on the same spot of the paper.  In our previous project MicroAR, we used the metaphor of familiar actions in order to view small objects in the real world, such as the use of a magnifying glass. This makes the AR tool more natural, leading users to concentrate on the AR world more deeply and easily. NanoAR is the developed prototype of this concept.	augmented reality;prototype;usb	Shintaro Kitazawa;Koh Sueda;Henry Been-Lirn Duh	2012		10.1145/2342896.2342984	computer vision;augmented reality;computer science;operating system;motion estimation;multimedia;mpeg-4;general-purpose computing on graphics processing units;computer graphics (images)	HCI	-43.06375029971861	-38.862351727953516	23504
a307a713c5cd19307347f28b91bfcd2b6cb17b38	promotion of learning motivation through individualization of learner-game interaction		"""Educational games have been used in educational settings for ages. The reasons behind this interest are different, e.g., games increase enjoyment, involvement, and motivation, as well as they influence emotions. In an area where competence level and challenge level are well balanced the so-called """"flow-experience"""" appears. But how can we maintain """"flow-experience"""" by using serious games? Computational intelligence could be a pioneer answer, if the technology will be implemented in a didactic and meaningful manner. A serious game with emotion-based adaptation has been developed and described in the paper. An experimental study with 244 participants has been carried out involving the usage of the developed game. Results of the experiment in terms of learning outcomes, flow-experience, and motivation are described in the paper."""	computation;computational intelligence;emotion recognition;experiment;generalized büchi automaton;knowledge level;point of view (computer hardware company);self-reflection;video game design	Dr. Tek Bahadur Gurung;Sintija Petrovica	2018	2018 IEEE Conference on Computational Intelligence and Games (CIG)	10.1109/CIG.2018.8490371	machine learning;task analysis;knowledge management;artificial intelligence;emotion recognition;computer science;computational intelligence	HCI	-56.90404984356036	-49.3944750408091	23543
ceaf206ddb4b3d835bbf30ffe0bf01dcbe3032f6	rapid development of interactive applications based on online social networks		Online social networks, like Twitter or Google+, are widely used for all kind of purposes, and the proliferation of smartphones enables their use anywhere, anytime. The instant messaging capabilities of these services are used in an ad-hoc way for social activities, like organizing meetings or gathering preferences among a group of friends, or as a means to contact community managers of companies or services.		Ángel Mora Segura;Juan de Lara;Jesús Sánchez Cuadrado	2014		10.1007/978-3-319-11746-1_37	computer vision	HCI	-55.2817037404611	-40.16288533004148	23564
64f86be6790609827817e8f4a548ebdc7f7f478a	sequential model of user browsing on websites - three activities defined: scanning, interaction and reading		This paper presents a model of user browsing behaviour on websites. Main user activities on websites are suggested, discussed and supported by previous research. Proposed activities are then associated with three main aspects of the website usability, aesthetics and information quality. Their role in each phase of user browsing on the website is discussed. Basic browsing model is then constructed on the basis of previous research ́s conclusions, accompanied by new considerations. Model variations are taken into consideration and discussed in relevance to the mode of use.	browsing;informatics;information quality;relevance;simulation;ubiquitous computing;usability testing;web usability	Aneta Bartuskova;Ondrej Krejcar	2014		10.5220/0004963901430148	computer vision;simulation	Web+IR	-58.04849161396679	-44.873966582444254	23577
03242700e7944383dfa02fc75cd2817691231055	modalities for building relationships with handheld computer agents	user agent;handheld computer;social interface;pda;relational agent;embodied conversational agent;mobile computing;affective computing	In this paper we describe the design of a relational agent interface for handheld computers and the results of a study exploring the effectiveness of different user-agent interaction modalities. Four different agent output modalities-text only, static image plus text, animated, and animated plus nonverbal speech-are compared and their impact on the ability of the agent to establish a social bond with the user, the perceived credibility of information delivered, and user acceptance is evaluated. Subjects ranked the two animated versions of the system higher on all measures.	computer;mobile device;user agent	Timothy W. Bickmore;Daniel Mauer	2006		10.1145/1125451.1125567	user agent;human–computer interaction;embodied agent;computer science;operating system;affective computing;multimedia;mobile computing	HCI	-56.467315720856554	-47.35527339274609	23579
4862c370e3fb30f0eaa1943619af3820ef16473d	digital rights management for the mobile internet	location based service;collaborative application;revenue sharing;collaborative computing;mobile internet;multimedia services;digital right management	Digital rights management (DRM) is assuming much importance in the mobile Internet where collaborative computing and content networking are taking center stage. Collaborative applications cover a wide range of applications, from multiparty gaming to shared viewing and/or processing of any object. Content typically refers to any sharable object in any of its manifestation. With sharing naturally arises the issues of copyrights, licensing, and revenue sharing. Then there are issues of balancing the societal behaviours, expectations, the rights of the content owners, and business imperatives. Mobile entertainment and multimedia services are the next big things on the horizon. Recent deployment of mobile Internet has opened many new opportunities to DRM. Mobile handheld provides a personal content-rich trusted device to users with connectivity anytime anywhere. It leverages the impulsive behaviour of people to share fun or join workgroup at different locations. This can be only possible with wireless mobile access. Some usage scenarios in the future would also contrast the legacy behaviour. For example, a wireless dating service may send subscribers the personal information of their potential dates in the current neighborhood by location-based services. Then, either party can initiate a SMS or MMS to start a conversation. Due to the sensitivity of the personal information, subscribers will not want the information to be falsified or forwarded without their consent. There are many similar usage scenarios that can be configured. Thus, DRM will be an enabler for this kind of trusted services in order to gain popularity. On the other hand, DRM may also be an inhibitor of widespread content sharing, especially for new games or services. Therefore, the design of DRM model may require a combination of promotion and protection at the same time, with their weights varying across different products and services. Moreover, the interests of equipment vendors and content providers vary significantly. Content providers earn from selling contents and surely want their rights to be protected. On the opposite end of the scale, equipment vendors earn from selling player equipment and gadgets and would naturally prefer to disable any DRM features to accelerate and enlarge user demand. With these competing forces, DRM would continue to evolve slowly over the next several years. In this article, we discuss the pivotal role of the DRM in the mobile Internet, the current state of DRM technology, and the issues and challenges that lie ahead for the industry.	digital rights management	Yin-Ling Liong;Sudhir S. Dixit	2004	Wireless Personal Communications	10.1023/B:WIRE.0000037573.25350.51	mobile search;the internet;mobile web;internet access;computer science;location-based service;multimedia;internet privacy;mobile computing;world wide web	Mobile	-55.1596646177712	-40.30439861172842	23581
36112a3ac56fea146f5f3da1784a9c5857666262	beneath the paint: a visual journey through conceptual metaphor violation		Metaphors are an undeniable part of many forms of art and they hint at the underlying conceptualisation that takes place in the silent conversation between an art piece and its perceiver. Abstract art, in particular, requires the viewer not only to analyse the colour palette and the shapes of the strokes but to subconsciously react to the underlying structures that often define the metaphors. After introducing a few cognitive theories involved in perception and knowledge transfer the paper introduces the reader to the painting “Beneath the Paint”. It is an abstract acrylic painting that plays on the two conceptual metaphor structures ‘UP is GOOD’ and ‘DARK is BAD’ and by presenting them in contradiction force the viewer to subconsciously choose its primary conceptualisation.	dark;palette (computing);theory	Maria M. Hedblom	2017			aesthetics;conceptual metaphor;art	AI	-56.80384170450501	-31.149945925126122	23593
1f765805305c563782c8d576dc7cd63e8d2887f7	an attempt to induce a strong rubber hand illusion under active-hand movement with tactile feedback and visuotactile stimulus		We combined a few methods to effectively create the illusion of embodiment and sense of body motion in the framework of the rubber hand illusion. In our experiments, active hand movements and self-stimulation were employed instead of classical passive tactile stimuli applied to still hands. The combination of these conditions effectively created the illusion. Furthermore, we collectively tested the effects of visual stimuli that were accompanied with tactile sensations. Specifically, we observed that when an object associated with tactile sensations was moving around the fake hand, the illusion tended to be more intensively induced than when an object that was unlikely to be associated with tactile sensations was placed still near the fake hand.	experiment;inductive reasoning;tactile imaging	Ken Itoh;Shogo Okamoto;Masayuki Hara;Yoji Yamada	2016		10.1007/978-3-319-42324-1_34	psychology;cognitive psychology;communication;social psychology	Robotics	-45.578914744494504	-50.758273151965476	23644
b2cc099c4e4d0e9dbdc21b07ff4a4f2d20b01d3e	scripted and unscripted aspects of creative work with knowledge		Advances in scripting theory and advances in support for student-driven knowledge construction call for a reconsideration of long-standing issues of guidance, control, and agency. This symposium undertakes a fresh analysis based on the relations between two widely adopted approaches that may be poles apart but arguably viewed as variations within a common applied epistemological framework. The two approaches are scripted collaboration and Knowledge Building. Rather than focusing on similarities and differences, the symposium will address deeper problems such as reconciling external supports of all kinds with the selforganizing character of knowledge construction and integrating such supports into classrooms viewed as knowledge-creating communities. The centerpiece of the symposium is a panel discussion that includes experts who provide different theoretical viewpoints. In its synthesis the symposium will capture and make sense of what is strongest in the two approaches and provide a broad conceptual basis for next-generation initiatives.		Carl Bereiter;Ulrike Cress;Frank Fischer;Kai Hakkarainen;Marlene Scardamalia;Freydis Vogel	2017		10.22318/cscl2017.119	panel discussion;computer science;creative work;knowledge management;viewpoints;knowledge building;scripting language	Logic	-62.27024473469098	-34.31911934132166	23645
03a0f0999ef26072874955f46a9009d2ae3fc074	dawn of the planet of the apes	animation;design	"""With the benefit of new performance capture workflows, Weta Digital animators and digital artists bring emotional depth and stunning realism to the apes in """"Dawn of the Planet of the Apes"""". It is a powerful story about the fight for survival as two species battle for dominance."""	motion capture;the fight: lights out	Amy Minty	2014		10.1145/2633956.2634029	astrobiology;planet;geography	HCI	-52.87562059697026	-29.031865746926318	23660
ef93fa80009476f446575f834bda78d887c8833e	using java 3d graphics as an element of a computer graphics course.	computer graphic;3d graphics			L. Carl Leinbach;Rodney S. Tosten	2003			vector graphics;graphics pipeline;scientific visualization;2d computer graphics;computer graphics metafile;molecular graphics;computer science;graphics;computer graphics lighting;real-time computer graphics;graphics software;computer graphics;graphics address remapping table;graphics hardware;turtle graphics;general-purpose computing on graphics processing units;3d computer graphics;computer graphics (images)	Graphics	-46.945399688155966	-29.525526895192392	23712
953c4132963e28f3713aa6c8f51620d0d27ef92a	illustrating how mechanical assemblies work	frame sequence;motion depiction;causal chain;mechanical assemblies work;shape analysis;cad model;mechanical interaction;motion arrow;automated approach;visualization;wide variety;individual part;present result;complex mechanical assembly;causal chaining;mechanical assembly	How things work visualizations use a variety of visual techniques to depict the operation of complex mechanical assemblies. We present an automated approach for generating such visualizations. Starting with a 3D CAD model of an assembly, we first infer the motions of individual parts and the interactions between parts based on their geometry and a few user specified constraints. We then use this information to generate visualizations that incorporate motion arrows, frame sequences and animation to convey the causal chain of motions and mechanical interactions between parts. We present results for a wide variety of assemblies.		Niloy Jyoti Mitra;Yong-Liang Yang;Dong-Ming Yan;Wilmot Li;Maneesh Agrawala	2010	ACM Trans. Graph.	10.1145/1833351.1778795	simulation;computer science;theoretical computer science;software engineering;hardware architecture;technology	Graphics	-38.435047663207186	-32.652738526549285	23827
7b44bf01242b88b776fd91fad522f8dc09d3c2d0	telepointer: hands-free completely self-contained wearable visual augmented reality without headwear and without any infrastructural reliance	reality user interfaces wearable visual augmented reality telepointer hands free visual collaborative telepresence;laser aremac;augmented reality computer aided manufacturing cadcam base stations wearable computers user interfaces dairy products band pass filters computer displays computer graphics;user interface;applications free computing;metaphor free computing;reality user interface rui;user interfaces augmented reality;personal imaging;visual ar without head mounted displays;aremac;augmented reality ar;augmented reality;user interfaces	Telepointer is a wearable hands-free, headwear-free device that allows the wearer to experience a visual collaborative telepresence, with text, graphics, and a shared cursor, displayed directly on real world objects. It is completely portable and can be used almost anywhere since it does not rely on infrastructure. It is operated through a Reality User Interfaces (RUI) that allows direct interaction with the real world, establishing a kind of computing that is completely free of metaphors.	augmented reality;cursor (databases);graphics;telepointer;user interface;wearable computer	Steve Mann	2000	Digest of Papers. Fourth International Symposium on Wearable Computers	10.1109/ISWC.2000.888489	augmented reality;computer-mediated reality;human–computer interaction;computer science;operating system;mixed reality;multimedia;user interface;computer graphics (images)	HCI	-43.35803877148029	-39.29252654570493	23871
60ff591d8e7c8990d39b6f6289ca538b773e7595	visual robot programming for generalizable mobile manipulation tasks	mobile manipulation;domain specific languages	General-purpose robots present the opportunity to be programmed for a specific purpose {em after} deployment. This requires tools for end-users to quickly and intuitively program robots to perform useful tasks in new environments. In this paper, we present a flow-based visual programming language (VPL) for mobile manipulation tasks, demonstrate the generalizability of tasks programmed in this VPL, and present a preliminary user study of a development tool for this VPL.	general-purpose markup language;mobile manipulator;robot;software deployment;usability testing;vpl research;visual programming language	Sonya Alexandrova;Zachary Tatlock;Maya Cakmak	2015		10.1145/2701973.2702052	computer vision;simulation;computer science;domain-specific language	Robotics	-37.76900471530394	-40.617461916064414	23876
77ce7a25281c557eb8967201efc1f520f9f8d4c0	exploring agent support at the user interface in e-commerce applications	software;commerce electronique;interfase usuario;electronic commerce;comercio electronico;negociation;logiciel;user interface;e commerce;software agent;besoin utilisateur;necesidad usuario;information overload;user need;negociacion;agent intelligent;bargaining;agent technology;intelligent agent;logicial;interface utilisateur;agente inteligente;electronic trade;user model	In this paper, we present some of the results from our ongoing research work in the area of ‘agent support’ for electronic commerce, particularly at the user interface level. Our goal is to provide intelligent agents to assist both the consumers and the vendors in an electronic shopping environment. Users with a wide variety of different needs are expected to use the electronic shopping application and their expectations about the interface could vary a lot. Traditional studies of user interface technology have shown the existence of a ‘gap’ between what the user interface actually lets the users do and the users’ expectations. Agent technology, in the form of personalized user interface agents, can help to narrow this gap. Such agents can be used to give a personalized service to the user by knowing the user’s preferences. By doing so, they can assist in the various stages of the users’ shopping process, provide tailored product recommendations by filtering information on behalf of their users and reduce the information overload. From a vendor’s perspective, a software sales agent could be used for price negotiation with the consumer. Such agents would give the flexibility offered by negotiation without the burden of having to provide human presence to an online store to handle such negotiations.	e-commerce;information overload;intelligent agent;java;online shopping;personalization;software agent;usability;user interface	P. Desharnais;Jinhu Lu;Thiruvengadam Radhakrishnan	2002	International Journal on Digital Libraries	10.1007/s007990100040	user interface design;e-commerce;user;user modeling;interface metaphor;human–computer interaction;computer science;software agent;information overload;user interface;world wide web;negotiation	HCI	-52.625094530141816	-41.539456360249936	23895
d7d12b4f959ba6e02e006baa1d2c07cbba18ccf0	taking off to the third dimension, schematization of virtual environments	3d;virtual environments;wayfinding;spatial representation;3d representation;virtual environment;schematization	Virtual environments are increasingly popular in different areas in both research and industry. However, interaction with these environments is challenging, posing a variety of difficulties to human users. In this paper, we explore how well known principles of abstraction and information reduction for 2D spatial representations, which we term schematization, can be transferred to the 3D representations of virtual environments in order to increase their utility.	categorization;coherence (physics);distortion;map;schematic;virtual reality	Denise Peters;Kai-Florian Richter	2008	IJSDIR		computer vision;human–computer interaction;computer science;multimedia	Visualization	-36.184416383608585	-35.414891777690734	23909
bac8599cc0a4ce71d1d7625cef746ae383f20b18	a framework for the simulation and haptic display of dynamic systems subject to holonomic constraints	human performance argumentation telerobotics;haptic display;human performance;haptics and haptic interfaces;simulation;visual interaction;virtual reality;dynamic system;dynamic environment;interfaces and virtual reality;geometric constraints;physical human robot interaction;simulation environment;constrained dynamics;haptic interface	In this paper we present a framework that enables an operator to haptically and visually interact with a simulated dynamic environment subject to virtual holonomic constraints. The framework combines a geometric constraint solver with a constrained dynamics simulation engine that controls an admittance-type haptic display. This system takes on relevant issues in the context of assisted teleoperated tasks, from providing an intuitive interface for creating and combining virtual constraints, to haptically displaying rigid motion constraints in simulated environments subject to desired inertial dynamics. Two experiments carried out using the Cobotic Hand Controller haptic display are presented.	dynamical system;haptic technology;simulation	Adolfo Rodriguez;Luis Basañez;J. Edward Colgate;Eric L. Faulring	2010	I. J. Robotics Res.	10.1177/0278364909104841	human performance technology;computer vision;simulation;human–computer interaction;computer science;engineering;artificial intelligence;dynamical system;virtual reality;haptic technology	Robotics	-37.64659376341753	-37.939359170725034	23912
3b79e0ba08515846a1351c13f78cf47b4fb788ff	influence of 10 seconds' interval in pragmatic interpretation	pragmatics timing labeling robots joints auditory system face;pragmatic interpretation;pragmatics;human robot interactions;eye gaze cues;joint attention experiment;auditory system;immature interactive system pragmatic interpretation joint attention experiment eye gaze cues word meaning interpretation human robot interactions;human robot interaction;joints;interactive systems computational linguistics human robot interaction;immature interactive system;interactive system;robots;word meaning interpretation;face;computational linguistics;joint attention;interactive systems;eye gaze;labeling;timing	This study examined interpreting word meanings and movement of line-of-regard of participants in a joint attention experiment. In addition to immediately giving an object label to a child, we tested an effect of 10 seconds' interval on children and adults using a joint attention experiment. Results were that adults used both pragmatic and eye gaze cues and interpreted word meanings appropriately. However, 4-year-old children tended to use only a pragmatic cue in the similar task ignoring eye gaze when a label was given after 10 seconds elapsed. 2-year-old children used only eye gaze. The study suggested that with an immature interactive system a child tends to rely only one or a few non-linguistic cues in interaction. Implication of this study is that robots must be able to use various non-linguistic cues with an integrated manner in human-robot interactions.	ccir system a;human–robot interaction;interactivity;norm (social);robot	Tetsuya Yasuda;Harumi Kobayashi	2010	19th International Symposium in Robot and Human Interactive Communication	10.1109/ROMAN.2010.5598630	human–robot interaction;robot;face;joint attention;computer vision;labeling theory;eye tracking;computer science;artificial intelligence;computational linguistics;pragmatics	HCI	-50.37702629862665	-50.802195346874186	23930
2f4e2f21f71de42a1e0f547b6b8536067bdb43cc	assisting system of visually impaired in touch panel operation using stereo camera	microphones;finger motion direction visually impaired assisting system touch panel operation stereo camera visual disability verbal input navigation method;image processing;visually impaired stereo vision wearable sensors user interfaces touch panel;user interface;wearable sensors;image sensors;navigation cameras headphones usability conferences microphones image processing;navigation;handicapped aids;stereo image processing;stereo vision;visual impairment;visually impaired;stereo image processing handicapped aids image sensors;headphones;touch panel;usability;user interfaces;cameras;conferences	In this paper, we propose an assisting system of touch panel operation for people with visual disability. In the system, a user specifies the target button on the panel by verbal input. The system detects the button and user's fingertip by analyzing images obtained through stereo camera. Navigation is made by indicating the direction of the fingertip on the panel through headphones with sound. To construct an efficient navigation method, comparisons were made experimentally concerning to indication of the finger motion direction, choice of navigation sound, and indication of the distance. The effectiveness of the proposed method was verified through experiments.	experiment;headphones;stereo camera;touchscreen	Atsushi Yamashita;So Kuno;Toru Kaneko	2011	2011 18th IEEE International Conference on Image Processing	10.1109/ICIP.2011.6116728	computer vision;simulation;image processing;computer science;multimedia;user interface	Robotics	-42.86297536043359	-44.6256959994226	23959
5d6529bc3c5cad1a5937f930d5ac539aa5521c34	standing-up motion support device by using elbow support face to adapt to human physical and motion characteristics	human interface;medical and welfare assistance;human engineering			Takuro Hatsukari;Norihisa Miyase;Jun'ichi Higuchi;Yasuhisa Hirata;Kazuhiro Kosuge	2008	JRM	10.20965/jrm.2008.p0828	computer vision;simulation;human–computer interaction;engineering	Vision	-41.170064851126014	-45.68825236217833	23980
1a46a5d6b7442352e026157c3812d7c55d21ad51	grafa: faceted search & browsing for the wikidata knowledge graph		We present a demo of the GraFa faceted search and browsing interface over the Wikidata knowledge graph. We describe the key aspects of the interface, including the types of interactions that the system allows, the ranking schemes employed, and other features to aid usability. We also discuss future plans for improving the system. Online Demo: http://grafa.dcc.uchile.cl/	browsing;faceted classification;game demo;interaction;knowledge graph;usability;wikidata	José Moreno-Vega;Aidan Hogan	2018				HCI	-43.0371100356547	-24.052423734863325	24023
ac9bdeb7fd49327ea23c31ae636085ef982d16b4	the vienna development method: the meta-language	vienna development method	Why should wait for some days to get or receive the the vienna development method the meta language book that you order? Why should you take it if you can get the faster one? You can find the same book that you order right here. This is it the book that you can receive directly after purchasing. This the vienna development method the meta language is well known book in the world, of course many people will try to own it. Why don't you become the first? Still confused with the way?	purchasing;vienna development method		1978		10.1007/3-540-08766-4	vienna development method;computer science;programming language;algorithm	NLP	-61.05997781861904	-24.47532649526024	24031
8d20ec9bb1b6672025c5502051d2ffcdb32f5cf5	an editor for lute tablature	busqueda informacion;estensibilidad;information retrieval;musica;asservissement visuel;acoustique musicale;teclado;musical acoustics;musique;retroaccion;retroaction;acustica musical;recherche information;feedback regulation;keyboard;extensibilite;scalability;visual servoing;music;audio acoustics;clavier;servomando visual;acoustique audio	We describe a system for the entry and editing of music in lute tablature. The editor provides instant visual and MIDI feedback, mouse and keyboard controls, a macro recording facility, and full runtime extensibility. We conclude by discussing planned future functionality and considering other potential applications for the technology.	extensibility;midi	Christophe Rhodes;David Lewis	2005		10.1007/11751069_23	scalability;speech recognition;acoustics;computer science;musical acoustics;music;information retrieval	HCI	-45.387578929987775	-32.12142475606201	24052
110dd67ffd24d75574a52fe0860b0b91368e2407	interaction in virtual world views-linking 3d gis with vr	3d gis;virtual worlds	To support 3D GIS interaction within VR-environments we propose a multi-view approach based on three types of visualization: plan view, model view and world view. The visualization in these views ranges from a conventional map, through a partly symbolic and simplified 3D representation to a full immersive and photo-realistic 3D display. The views can be used simultaneously or intermittently, and each provides a repertoire of interaction possibilities that are apt but not necessarily limited to that view. We are currently developing a 3D GIS&VR system (Karma VI) based on existing GIS and VR technology that uses the three views to support the design, development and presentation of large infrastructure plans in The Netherlands. Operational use of this system showed that the multi-view approach leads to a more sophisticated understanding of those plans.	geographic information system;virtual world	Edward Verbree;Gert van Maren;Rick Germs;Frederik W. Jansen;Menno-Jan Kraak	1999	International Journal of Geographical Information Science	10.1080/136588199241265	enterprise gis;simulation;human–computer interaction;geography;computer science;metaverse;computer graphics (images)	HPC	-36.168188643991456	-32.919566132195094	24065
4541df929d8c6fc383d76609e927c84aadd058ee	virtual hairy brush for painterly rendering	user interface;computer graphics;real time simulation;computer graphic;writing primitive;general sweeping operation;machine intelligence;solid modeling;design software;virtual hairy brush;computational efficiency;article;power modeling	We propose a novel ‘‘e-brush’’ for calligraphy and painting, which meets all the criteria for a good e-brush. We use only four attributes to capture the essential features of the brush, and a suitably powerful modeling metaphor for its behavior. The e-brush s geometry, dynamic motions, and pigment changes are all dealt with in a single model. A single model simplifies the synchronization between the various system modules, thus giving rise to a more stable system, and lower costs. By a careful tradeoff between the complexity of the model and computation efficiency, more elaborate simulation of the e-brush s deformation and its recovery for interactive painterly rendering is made possible. We also propose a novel paper–ink model to complement the brush s model, and a machine intelligence module to empower the user to easily create beautiful calligraphy and painting. Despite the complexity of the modeling behind the scene, the high-level user interface has a simplistic and friendly design. The final results created by our e-brush can rival the real artwork. 2004 Elsevier Inc. All rights reserved.	artificial intelligence;brush (video game);computation;high- and low-level;pigment;simulation;synchronization (computer science);user interface	Songhua Xu;Min Tang;Francis C. M. Lau;Yunhe Pan	2004	Graphical Models	10.1016/j.gmod.2004.05.006	computer vision;simulation;computer science;solid modeling;computer graphics;user interface;algorithm;computer graphics (images)	AI	-39.86068660363244	-34.53064858756999	24066
609e3efdbcafdbccb804473db8a3766ab847d8af	integrated text entry from power wheelchairs	etude utilisateur;touchpad;etude utilisation;interfase usuario;wivik;unistrokes;real estate;user interface;saisie donnee;accesibilidad;computer access;joystick;user study;information technology;estudio utilizacion;text entry;estudio usuario;text input;teclado;utilisateur defavorise;technologie information;silla de ruedas;usuario desaventajado;window manager;deficiencia motora;pebbles;ecran visualisation;pantalla visualizacion;motor handicap;on screen keyboard;accessibility;gestures;toma dato;focus of attention;disadvantaged user;wheel chair;keyboard;interface utilisateur;display screen;handicap moteur;tecnologia informacion;data acquisition;edgewrite;accessibilite;use study;clavier;power wheelchair;fauteuil roulant	Power wheelchair joysticks have be used to control a mouse cursor on desktop computers, but they offer no integrated text entry solution, confining users to point-and-click or point-and-dwell with on-screen keyboards. On-screen keyboards reduce useful screen real-estate, exacerbating the need for frequent window management, and impose a secondary focus of attention. By contrast, we present two integrated gestural text entry methods designed for use from power wheelchairs: one for use with joysticks and the other for use with touchpads. Both techniques are adaptations of EdgeWrite, originally a stylus-based unistroke method designed for people with tremor. In a preliminary text entry study of 7 power wheelchair users, we found that EdgeWrite with a touchpad was faster than the on-screen keyboard WiViK with a joystick, and EdgeWrite with a joystick was only slightly slower. These results warranted a multi-session comparison of text entry with EdgeWrite and WiViK using joysticks and touchpads, in which we fo...	chi;control system;human factors and ergonomics;joystick;refinement (computing);touchpad	Jacob O. Wobbrock;Htet Htet Aung;Brad A. Myers;Edmund F. LoPresti	2005	Behaviour & IT	10.1080/01449290512331321729	touchpad;embedded system;simulation;human–computer interaction;computer science;engineering;artificial intelligence;accessibility;operating system;joystick;data acquisition;communication;user interface;gesture;information technology;world wide web;real estate	HCI	-47.343442723244344	-44.997667731873925	24071
764356d4f0a09c0d6c19ba99c63fb2a7b3ae2987	the impact of advanced vehicle technologies on older driver safety: a scoping review of subjective outcomes		Advanced vehicle technologies (AVTs) have the potential to modify an older driver's behind-the-wheel performance by compensating for age and/or health-related changes that can negatively impact their ability to operate a motor vehicle. However, the safety implications of these rapidly evolving technologies are not well understood. A scoping review was conducted to understand the current state of research on AVTs with a particular focus on subjective outcome measures specific to older drivers. Sixteen articles met the inclusion criteria for this scoping review. The methods used to address subjective outcomes across studies were summarized. Seven main subjective outcomes were identified: trust, functionality, satisfaction, usability, workload, acceptability, and usefulness. The results highlight inconsistencies in the research with defining these concepts. Consequently, there is an identified need for a more rigorous classification system and consistent application and interpretation of subjective measures with regard to AVTs.	scope (computer science);usability	Andrea D Furlan;Brenda Vrkljan;Hana Hussein Abbas;Jessica Babineau;Jennifer L. Campos;Shabnam Haghzare;Tara Kajaks;Margaret Tiong;Maria Vo;Martin Lavallière	2018		10.1145/3239092.3265965	human–computer interaction;engineering;driver safety;workload;applied psychology;subjective variables;usability	HCI	-61.434882465524986	-50.84292345135439	24116
0950bd06a54d3c3ad2be40782a31f439c1863e77	supporting museum co-visits using mobile devices	sensibilidad contexto;distributed system;red sin hilo;interfase usuario;museo;teleenseignement;systeme reparti;context aware;informatique mobile;musee;mobile device;social interaction;interaction sociale;reseau sans fil;user interface;juego cooperativo;relacion hombre maquina;wireless network;man machine relation;collaborative learning;cooperative game;sistema repartido;educational game;interaccion social;jeu cooperatif;contexto;contexte;interface utilisateur;museum;teleensenanza;relation homme machine;jeu ordinateur;sensibilite contexte;remote teaching;computer games;mobile computing;context	The goal of this work is to provide tools that promote social interactions between visitors through cooperative and educational games. In this paper, we describe how to support collaborative learning in museum visits and show an example application based on mobile palmtop systems. To this end, we have developed a system that is able to support collaborative and independent activities, and offer context-aware content.	carrara;embedded system;ipaq;interaction;mobile device;palmtop pc;personal digital assistant	Yann Laurillau;Fabio Paternò	2004		10.1007/978-3-540-28637-0_55	collaborative learning;simulation;computer science;artificial intelligence;operating system;wireless network;mobile device;user interface;mobile computing	HCI	-35.99669723572152	-25.772011412304817	24180
d3c64ccc4b1b1f8af76f0119268e3304fb8dc245	mobile chase - towards a framework for location-based gaming		Pervasive Gaming and Location-based Games in particular have gained more and more attention recently. Researchers from a variety of fields, media artists, mobile service providers as well as the entertainment industry all seem to have their specific interests in this area. Today a couple of different games exist from basic applications that are already available to the consumer market to bleeding edge research projects. In this paper we introduce a framework for Location-based Gaming that on the one hand helps with the development of market ready games. On the other hand it serves as a toolkit for researchers aiming to rapidly develop Location-based Games, not having to deal with implementation details far away from their research interests in order to focus on their specific research aspects.	horst rittel;hudson;human–computer interaction;internet gateway device protocol;location-based game;rapid prototyping;thad starner;ubiquitous computing	Mirko Fetter;Markus Etz;Heiko Blechschmied	2007			substructure;acoustics;deflection (engineering);strain gauge;computer graphics (images);computer science;road surface;simulation;species evenness	HCI	-50.13253254646832	-36.34824310538575	24186
7b80e810bc65b0411b3f324370da1e027ee9ffe1	minimally restrictive decision support systems		Decision-making behavior is heterogeneous. We therefore suggest building a decision support system for online purchase decisions that can support various different decision strategies and also allows users to mix them. We design this minimally restrictive system by decomposing different strategies into their component steps and implementing aids for supporting these steps. We empirically compare this system to a typical one, which restricts the users by supporting only one normative, utilitymaximizing strategy. Users perceive the minimally restrictive system as requiring lesser effort and being more enjoyable to use than the typical decision support system. Furthermore, they exhibit a higher intention to re-use the minimally restrictive system. However, there is no difference with respect to the perceived usefulness. Our results imply that webstores should implement minimally restrictive systems not only because of high user satisfaction, but also because analyzing clicks on the aids provides information as to which strategies are being used.	decision support system;mike lesser;online shopping	Jella Pfeiffer;Izak Benbasat;Franz Rothlauf	2014			decision aids;decision analysis;knowledge management;r-cast;decision support system;pluralistic walkthrough;decision engineering;business decision mapping;computer science;usability goals	HCI	-38.06482926793103	-51.52190976450706	24199
c5657be4a9b253aa9e05df9033cd1c8908acfeae	brain activity during observation of other's action in live and delayed video-mediated social interaction			electroencephalography	Sotaro Shimada	2012			brain activity and meditation;cognitive psychology;psychology;social relation	HCI	-55.22331325224509	-50.79331687896295	24200
c74b0e9b7e04cf9b10edfc146d8c48f4423de9c5	virtual worlds: experiencing virtual factories of the future	virtual reality;project work;simulation model;virtual worlds	This paper explains the latest project work being undertaken at the Ford Motor Company in the generation of simulation models from spreadsheet interfaces and in particular the latest advances in the automatic creation of virtual reality worlds based on these model layouts. The ease of creation is the key to the use of the third dimension but being able to visualise a facility more accurately overcomes obstacles to understanding and discussion. The paper explains the technical process involved in creating these worlds using the WITNESS VR simulation package from the Lanner Group.	simulation;spreadsheet;three-dimensional integrated circuit;virtual reality;virtual world	Tony Waller;John Ladbrook	2002			simulation;human–computer interaction;computer science;engineering;simulation modeling;virtual reality;mixed reality	Visualization	-48.12926304152962	-30.957813514865503	24209
5bc8fce0b97a831f9c3f4cb7cc8e46345fc3a9af	human-computer interaction. user interface design, development and multimodality		This essay proposes an analytical methodological process called Audiovisual Design, originated from the intersection between HCI theories and Audience Studies. Resources available from this two academic fields are required in the conception of complex audiovisual products, developed from more precise information about the target audiences, and distributed and brought to fruition through interaction interfaces or software. Methodologically, the flow of infor‐ mation inside the traditional communication models has been changed. The intro‐ duction of the actions of interaction, sharing and spread of content modified the audiovisual fruition, previously considered passive. This new dynamic is graph‐ ically represented as a workflow of Audiovisual Design. Therefore, it becomes possible to predict and design new interactive products, adapted to the needs of integration and sharing present in the contemporary audiovisual consumption. Moreover, it allows the analysis and identification of inherent issues and equi‐ vocated approaches of already concluded products.	human–computer interaction;logic synthesis;theory;user interface design	Masaaki Kurosu	2017		10.1007/978-3-319-58071-5	user experience design;interactive systems engineering;human–computer interaction;user interface	HCI	-60.58704059730929	-36.69278653536954	24210
4f1163f8cd9e46e6d3915c8b5cf1d868714369eb	hyperlinked comic strips for sharing personal contexts	context aware;human computer interaction;social networking services;time series;automatic generation;storytelling;friend of a friend;comics;foaf;web development;human computer interface	Comic strips can be used as a style of visualization on a human–computer interface because they can represent a wide variety of affairs with contexts or time series. This paper describes two systems for sharing personal context as comic strips: ComicDiary and Comic-FOAF-Viewer. Both the systems depict personal experiences or profiles including personal relationships in their comic strips and hyperlinks among related comics based on other characters in the story. ComicDiary allegorizes individual episodes that happen during touring exhibitions by creating a comic from a user’s touring records accumulated from personal guidance systems and environmental facts, e.g., social events. For example, a ComicDiary might show a user’s personal diary during a Japanese academic conference. The comic describes where the conference was held, the most interesting presentations, what happened, and so on. Exhibitions are places visited by people of all generations. Comic representation of a personal diary with amiable expressions fits such places. The comic strip is automatically generated, composed of 12 frames, and shown as a diary. Users can view their diaries at information kiosks located at exhibition sites. Friend of a Friend (FOAF), which is an XML/RDF application for expressing personal information and relationships, has attracted attention from Web developers because its files can describe human-centered networks such as Social Network Service (SNS). Current FOAF visualization tools utilize graphs or tables; however, it is difficult to represent a variety of relations. Comic-FOAF-Viewer aims to represent the multifarious relations and personal information that FOAF has to offer for surfing interfaces in FOAF networks.	3d modeling;book;computer graphics;experiment;fits;foaf (ontology);frame language;friend of a friend;guidance system;human–computer interaction;hyperlink;non-photorealistic rendering;personally identifiable information;strips;social network;source data;table (database);time series;unbiased rendering;web developer;xml	Ryuuki Sakamoto;Yasuyuki Sumi;Kiyoshi Kogure	2007	International Journal of Information Technology and Decision Making	10.1142/S0219622007002563	web development;computer science;artificial intelligence;marketing;comics;friend of a friend;time series;data mining;multimedia;advertising;world wide web;statistics	HCI	-44.883536564505356	-27.864730727596253	24218
d7bd8f40df9a1b80bfbaace2cf6835ed8a0754c1	support for keyboard performing practice with mirrored video of model performances		In this paper, we propose a support system for musical keyboard performance that is inspired by dance or ballet instruction in front of a mirror. In our system, the mirrored movie of a model performance is presented in a display on the user’s keyboard. The user copies every movement of the model movie to learn the right performance with the correct timing, without reading any scores. We conducted a series of experiments to assess the usability of the presented system.		Taisuke Nemoto;Nobuyuki Umezu	2018	2018 IEEE 7th Global Conference on Consumer Electronics (GCCE)	10.1109/GCCE.2018.8574620	multimedia;ballet;synchronization;midi;dance;usability;computer science	Visualization	-47.565696895748985	-47.17595875655364	24221
59a28d4c6c0beb3f9f64dda20ad6bd77fe97369a	i.ge: exploring new game interaction metaphors with interactive projection	hci;game design;spatial augmented reality;interactive projection;mixed reality;tangible interaction	Video games are traditionally presented and visualized on a screen (flat, portable, touch sensitive or projected) and are mostly restricted by predefined scenarios and character movements. Although the ways in which we interact with games have improved over recent years, the interaction is still limited to their functions and game presets and still happens inside the screen. In this paper we explore new interaction paradigms using i.Ge4, a video game level editor that allows users to interact with their own environment to create game content with every-day physical objects in real time. At this stage, i.Ge explores new ways to interact with the real world by augmenting it with interactive projections, reducing the gap between the real and the digital world in a spatial augmented reality. We also propose other novel naturalistic approaches to create and interact with digital content that involve embodiment and more use of depth.	augmented reality;digital recording;embodied cognition;interactivity;level editor;touchscreen	Patrick Oswald;Jordi Tost;Reto Wettach	2015		10.1145/2677199.2687910	game design;simulation;human–computer interaction;computer science;game mechanics;game art design;multimedia	HCI	-45.55263513721222	-40.04696044321795	24223
07f0a1edea1be7cad462c2c25b1ad69837a55fc3	the future of video: user experience in a large-scale, high-definition video display environment	television;resolution;narrative;high resolution;video quality;video art;large scale;postproduction;display;user experience;ambient;video;high definition;ambient display	"""The new flatscreen displays provide a high-quality platform for the presentation of the moving image. The Introduction of High-Definition Television (HDTV) will push video quality to higher levels. We will see unprecedented cinema-quality moving images available in our homes. These and other emergent video technologies will support the use of more complex visual narrative constructions and enable the utilization of high-resolution flatscreen panel displays as ambient """"video paintings"""". This paper provides an outline of some of the emergent production techniques enabled by large-scale high-definition video display environments. It also discusses the need for a new generation of postproduction tools in order to realize the aesthetic possibilities of this new medium. Finally, the paper reviews our creative visual explorations in the development of an ambient video genre through a set of three video works."""	cinema 4d;display device;emergence;flat panel display;hdmi;image resolution;push video;user experience	Belgacem Ben Youssef;Jim Bizzocchi;John Bowes	2005		10.1145/1178477.1178508	computer vision;video production;computer science;video capture;video tracking;multimedia;video processing;computer graphics (images)	HCI	-51.81288224313554	-30.5751977424266	24237
0a9f913979e422852e9596d8176a71cff67233a6	an evaluation of authoring interfaces for image-based navigation	pedestrian navigation;user created content;mobile navigation;authoring system;evaluation	We present the development and evaluation of an authoring system for image-based pedestrian navigation which lets authors take pictures and annotate instructions on the go in three interface variants. Results indicate that a freehand manner of photograph annotation is fastest, while authors strive toward visually pleasing annotation compositions.	adobe freehand;fastest;usb on-the-go	Benjamin Walther-Franks;Dirk Wenig;Rainer Malaka;Barbara Grüter	2009		10.1145/1613858.1613930	turn-by-turn navigation;computer vision;computer science;evaluation;multimedia;management;mobile robot navigation;computer graphics (images)	HCI	-45.64356747827312	-40.23698800162712	24242
599ff74ab3b3f67237cbe5ded5e87ecefb0a0bb7	self-adaptive prototype for seat adaption	personal computing;reaction rules pervasive adaptation middleware driving experience physical comfort;software;reflective tier;sensors;tangible tier;prototypes;data collection;pervasive adaptive application;selfadaptive prototype;actuators;adaptive behavior;center of pressure speed;physical comfort;reaction rule selfadaptive prototype seat adaption physical state reflective middleware pervasive adaptive application tangible tier reflective tier data analysis decision making center of pressure speed driver physical state;seat adaption;data analysis;reaction rules;pervasive adaptation;roads;reflective middleware;adaptive applications;ubiquitous computing data analysis decision making middleware personal computing prototypes seats traffic engineering computing;driver circuits;ubiquitous computing;traffic engineering computing;middleware;seats;reaction rule;driver circuits sensors prototypes middleware actuators roads software;driver physical state;center of pressure;driving experience;physical state	Self-adaptive prototype for seat adaptation aims at enhancing the physical comfort of a driver by taking into account not only the state of the environment (state of the road, car settings), but also the driver’s emotional, cognitive and physical state. To implement this prototype we used a REFLECTive middleware, which provides a programming framework for the development of pervasive-adaptive applications. The REFLECTive middleware supports self-adaptive behavior and is generally composed of three tiers: Tangible tier contains services that read sensors data and send commands to actuators, REFLECTive tier is responsible for analyzing the data collected from sensors and for defining the actions that will be performed by actuators, Application tier facilitates high-level decision making. The seat adaptation prototype uses the information about Center of Pressure (COP) speed and number of bumps to determine the driver’s physical state, and then it combines this information with the driver’s cognitive and emotional state to figure out if the driver feels uncomfortable, and to change the state of seat cushions in an attempt to make driver feel more comfortable. The components of the seat adaptation prototype in the REFLECTive and Application tier are implemented using reaction rules.	adaptive behavior;high- and low-level;middleware;multitier architecture;pervasive informatics;prototype;reflection (computer programming);sensor	Gian Mario Bertolotti;Andrea Cristiani;Remo Lombardi;Marko Ribaric;Nikola M. Tomasevic;Mladen Stanojevic	2010	2010 Fourth IEEE International Conference on Self-Adaptive and Self-Organizing Systems Workshop	10.1109/SASOW.2010.29	embedded system;real-time computing;simulation;computer science;sensor;center of pressure;operating system;adaptive behavior;middleware;prototype;data analysis;state of matter;ubiquitous computing;statistics;actuator;data collection	Robotics	-55.76598173485642	-44.79765476617953	24243
9dc22507edc42a8dd809db77562a4e4daf9c2123	seeing, hearing, and touching: putting it all together		The morning talks gave a perspective on how vision science can be used to inform the design of visually complex interfaces such as those used in information visualization systems. The second half of the course looks at intersensory interactions and how they can inform the move to multimodal environments. These environments combine visual, auditory, and haptic displays with a richer set of inputs from users, including speech, gesture,and Biopotentials.	cell signaling;haptic technology;information visualization;multimodal interaction	Brian D. Fisher;Sidney S. Fels;Karon E. MacLean;Tamara Munzner;Ronald A. Rensink	2004		10.1145/1103900.1103908	computer vision;computer graphics (images);artificial intelligence;computer science	HCI	-54.99003888763936	-36.017956040254234	24251
fdcddf475aefbccd8f3d639fcc4d94a74f2b3710	development of advanced cellular manufacturing system with human-robot collaboration	stress;manipulators;cellular manufacturing system;human robot collaboration;system designer;voice guidance;service robots;human robot interaction;mental strain assessment;assembly;optical instruction;physiological parameter;text instruction;system design;emotion;robotic assembly cellular manufacturing cognition human robot interaction industrial manipulators;cognition;optical instruction advanced cellular manufacturing system human robot collaboration human operator assembly support mental strain assessment system designer physiological parameter cognition emotion robot motion text instruction voice guidance;human operator;assembly support;robot motion;robotic assembly;humans;advanced cellular manufacturing system;strain;strain humans assembly manipulators stress service robots;industrial manipulators;cellular manufacturing	Even if a cellular manufacturing system is robot assisted with assembly support, the main operator is human. A system designer cannot therefore ignore the mental strain experienced by the operator as a result of improper assembly support. In this paper, authors analyzed operators' mental strain induced by physical and informational support in an advanced cellular manufacturing system with human-robot collaboration using physiological parameters related to cognition and emotion. Our results showed that higher speeds of an approaching robot's motion and closer distances between human operator and robot increase mental strain related to emotion, while augmenting text instruction with voice guidance and optical instruction increase mental strain related to cognition and emotion.	cognition;industrial robot;systems design	Ryu Kato;Marina Fujita;Tamio Arai	2010	19th International Symposium in Robot and Human Interactive Communication	10.1109/ROMAN.2010.5598700	human–robot interaction;simulation;cognition;emotion;computer science;artificial intelligence;assembly;strain;stress	Robotics	-40.528880416612864	-47.07017551687923	24260
64a1b5661a7e38a83f6f0dadc640b9a6385e5d0a	visual-haptic feedback interaction in automotive touchscreens	tl motor vehicles aeronautics astronautics;automotive;touchscreen;human machine interface;haptic feedback;tk electrical engineering electronics nuclear engineering	0141-9382/$ see front matter 2011 Elsevier B.V. A doi:10.1016/j.displa.2011.09.002 ⇑ Corresponding author. Tel.: +44 2476 575936; fax E-mail addresses: M.Pitts@warwick.ac.uk (M.J. Pi m.ac.uk (G. Burnett), lskrypch@jaguarlandrover.com warwick.ac.uk (T. Wellings), A.Attridge@warwick.a liams.1@warwick.ac.uk (M.A. Williams). Touchscreen interfaces offer benefits in terms of flexibility and ease of interaction and as such their use has increased rapidly in a range of devices, from mobile phones to in-car technology. However, traditional touchscreens impose an inevitable visual workload demand that has implications for safety, especially in automotive use. Recent developments in touchscreen technology have enabled feedback to be provided via the haptic channel. A study was conducted to investigate the effects of visual and haptic touchscreen feedback on visual workload, task performance and subjective response using a medium-fidelity driving simulator. Thirty-six experienced drivers performed touchscreen ‘search and select’ tasks while engaged in a motorway driving task. The study utilised a 3 2 within-subjects design, with three levels of visual feedback: ‘immediate’, ‘delayed’, ‘none’; and two levels of haptic feedback: ‘visual only’, ‘visual + haptic’. Results showed that visual workload was increased when visual feedback was delayed or absent; however, introducing haptic feedback counteracted this effect, with no increases observed in glance time and count. Task completion time was also reduced when haptic feedback was enabled, while driving performance showed no effect due to feedback type. Subjective responses indicated that haptic feedback improved the user experience and reduced perceived task difficulty. 2011 Elsevier B.V. All rights reserved.	andy wellings;driving simulator;fax;haptic technology;mobile phone;simulation;touchscreen;user experience	Matthew J. Pitts;Gary E. Burnett;Lee Skrypchuk;Tom Wellings;Alex Attridge;Mark A. Williams	2012	Displays	10.1016/j.displa.2011.09.002	human–machine interface;embedded system;simulation;computer hardware;computer science;engineering;haptic technology	HCI	-47.35289831991081	-46.7926876245351	24324
ca2d1a1e418c30665655a4d816fca8d244a54490	plex cards: a source of inspiration when designing for playfulness	design methods;card;design method;workshop;inspiration;idea generation;playfulness;human activity	Playfulness can be observed in all areas of human activity. It is an attitude of making activities more enjoyable. Designing for playfulness involves creating objects that elicit a playful approach and provide enjoyable experiences. In this paper we introduce the design and evaluation of the PLEX Cards and its two related idea generation techniques. The cards were created to communicate the 22 categories of a Playful Experiences framework to designers and other stakeholders who wish to design for playfulness. We have evaluated the practical use of the cards by applying them in three design cases. The results show that the PLEX Cards are a valuable source of inspiration when designing for playfulness and the techniques help create a large amount of ideas in a short time.	edmonds' algorithm;emergence;experience design;francis;flickr;palo;playtest;programme delivery control;user-centered design;video card;wolf	Andrés Lucero;Juha Arrasvuori	2010		10.1145/1823818.1823821	simulation;human–computer interaction;engineering;multimedia	HCI	-62.77701025867087	-36.6735757299366	24389
819bb031d32b943d554da7de1d52d03e5ab4add1	the news context project	intelligent information systems;contextual search;computational journalism	We describe intelligent information technologies designed to automatically provide both journalists and ordinary newsreaders with a broad range of the contextual information they need in order to better understand news stories, presented in an immediate and compelling fashion. These systems automatically identify, select, and present appropriate contextual information based on the story a user is currently viewing. Our experiences in building a number of specific systems of this kind have led to the creation of a general architecture and platform for developing such applications. These systems interact with news consumers directly through mechanisms such as browser extensions.	browser extension;experience	Lawrence Birnbaum;Miriam Boon;Scott Bradley;Jennifer Wilson	2015		10.1145/2732158.2732178	human–computer interaction;computer science;multimedia;world wide web;contextual advertising;information retrieval	Web+IR	-54.52044192564835	-40.50083095206064	24428
5753bafb6ddaad7b3b77639bb9f741db2d78b014	creating awe: artistic and scientific practices in research-based design for exploring a profound immersive installation		The paper describes AWE (2018), an immersive mixed and virtual reality installation designed to elicit feelings of awe and wonder. Experiences of awe are found to prompt feelings of interconnectedness and an improvement to perceived well-being. To address the challenging prospect of designing for a specific emotional experience in a wellness application, we combined artistic and scientific practices through a research-based design process in order to identify awe-inspiring traits, generate a typology of awe, identify emotion validation techniques, and undertake iterative prototyping of the installation directly with participants. The resulting installation integrates a pre-VR mixed-reality experience to prime immersants for openness to the experience, followed by an immersive VR environment, and it uses a novel, custom interface for intuitive hands-free navigation. Our methods involve phenomenological interviews and physiological sensors to evaluate the evoked emotional experiences, which then inform design decisions to improve the system. Additionally, we integrate bio-responsive elements into the environment to further personalize the experience. Results suggest that AWE can elicit the target emotional experience of awe, prompt a transformative experience, and improve well-being in some participants.	biological anthropology;british informatics olympiad;demography;documentation;emotions;experience;imperative programming;interconnectedness;interface device component;intuition;iteration;iterative design;mixed reality;navigation;openness;personalization;rna recognition motif;rough set;scientific literature;synergy;trait;usability;vr - veterans rand health survey;virtual reality;sensor (device)	Denise T. Quesnel;Ekaterina R. Stepanova;Ivan Abdo Aguilar;Patrick Pennefather;Bernhard E. Riecke	2018	2018 IEEE Games, Entertainment, Media Conference (GEM)	10.1109/GEM.2018.8516463		HCI	-56.2794637835634	-44.931790622276594	24433
2cf4e980157cea0a90cfc7357474f50f0105fca7	a linear approach towards modeling human behavior	sistemas de monitorizacao;sistemas de diagnostico;human behavior;controlo digital;linear model;difference set;dynamic characteristic;dinâmica de sistemas;mechatronic systems	"""The human operator is, no doubt, the most complex and variable element of a Mechatronics system. On simpler manual control tasks, a linear model may be used to capture the human dynamics, however experiences on human operator response during pursuit manual tracking tasks, show that the dynamics of the human operator appear to depend on the specific task that the subject is asked to perform. This means that a unique truly human model cannot be completely achieved. Rather, a different set of models, each for a certain class of task, seems to be needed. This ongoing PhD work introduces several approaches on the human operator dynamic characteristic modeling and identification procedures, which may be useful for developing improved """"humetronic"""" systems, i.e. human-machine systems which may be able to adapt themselves to the skill level of humans, aiming, with reduced effort, to achieve for best performance and safety."""	experience;human dynamics;human-based computation;joystick;labview;linear model;mag technology co.;mechatronics;point of sale;r language;real-time clock	Rui Antunes;Fernando Vieira Coito;Hermínio Duarte-Ramos	2011		10.1007/978-3-642-19170-1_33	control engineering;simulation;engineering;artificial intelligence;linear model;control theory;human behavior;difference set;statistics	Robotics	-38.84353099797412	-48.09554308943632	24467
75482ad63cef26d30d5f15e3f0c4edd81369796d	3d dynamic visualization of swallowing from multi-slice computed tomography	2d shadow;additive shadow;2d self shadowing;dynamic shadow;shadow map	classroom use is granted without fee provided that copies are not made or distributed for commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the Owner/Author. SIGGRAPH 2014, August 10 – 14, 2014, Vancouver, British Columbia, Canada. 2014 Copyright held by the Owner/Author. ACM 978-1-4503-2958-3/14/08 3D Dynamic Visualization of Swallowing from Multi-Slice Computed Tomography	ct scan;columbia (supercomputer);siggraph;tomography	Andrew K. Ho;Mark A. Nicosia;Angela Dietsch;William R. Pearson;Jana Rieger;Nancy Solomon;Maureen Stone;Yoko Inamoto;Eiichi Saitoh;Sheldon Green;Sidney S. Fels	2014		10.1145/2614217.2633399	computer vision;computer science;shadow mapping;computer graphics (images)	Visualization	-51.406853617607155	-25.212179346282145	24473
36829261948c530edfd7bb7d943d30be9f203294	bigger is not always better: display size, performance, and task load during peephole map navigation	user study;map navigation;navigation performance;display size;peephole navigation;inproceedings;experimentation	Dynamic peephole navigation is an increasingly popular technique for navigating large information spaces such as maps. Users can view the map through handheld, spatially aware displays that serve as peepholes and navigate the map by moving these displays in physical space. We conducted a controlled experiment of peephole map navigation with 16 participants to better understand the effect of a peephole's size on users' map navigation behavior, navigation performance, and task load. Simulating different peephole sizes from 4' (smartphone) up to 120' (control condition), we confirmed that larger peepholes significantly improve learning speed, navigation speed, and reduce task load; however, this added benefit diminishes with growing sizes. Our data shows that a relatively small, tablet-sized peephole can serve as a 'sweet spot' between peephole size and both user navigation performance and user task load.	computer performance;display size;handheld game console;map;peephole optimization;smartphone;tablet computer	Roman Rädle;Hans-Christian Jetter;Jens Müller;Harald Reiterer	2014		10.1145/2556288.2557071	computer vision;simulation;display size;computer science;operating system;mobile robot navigation;computer graphics (images)	HCI	-45.17057809857195	-46.08565056503667	24495
2ad3d83753689b0c16de798e06054e2455a88a2d	system operation improvement of p-cube2 for visually impaired people		We have been developed P-CUBE2 which is a tangible programing tool intended to be suitable for beginners including visual impairments. This paper describes the system improvement and evaluation of P-CUBE2 using RFID system. P-CUBE2 enables users to make a program by an easy operation of set blocks on the mat. Since, we knew that visually impaired children felt difficulty on a process of transferring program to a controlled object, we adopt the Bluetooth module for improving the program transfer procedure easier for their.	bluetooth	Mariko Tsuda;Tatsuo Motoyoshi;Kei Sawai;Hiroyuki Masuta;Takumi Tamamoto;Ken'ichi Koyanagi;Toru Oshima	2018	2018 World Automation Congress (WAC)	10.23919/WAC.2018.8430431	control engineering;human–computer interaction;engineering;bluetooth;transfer procedure	HCI	-41.80456908013559	-45.35037302860418	24540
79afe6dc1724e21dc903fb26db0479e884a3efca	low-fidelity prototyping for collaborative user interface specifications		The paper describes a procedure for requirements engineering workshops where attendees of different expertise are guided to identify and to describe user interface requirements. Based on a workshop structure with user-centered design constraints, participants are assisted in scoping and ideation processes using low-fidelity techniques and the World Cafe conversation method to determine user and system requirements.	user interface	Jan Wojdziak;Bastian Bansemir;Bettina Kirchner;Berit Lochner;Rainer Groh	2016		10.1007/978-3-319-40548-3_28	user interface design;user interface;graphical user interface testing	HCI	-62.05745150567053	-38.90492760972462	24554
15ccb4a1d140f80bf138eeec46783098b8b36709	gesture and emotion: can basic gestural form features discriminate emotions?	image motion analysis;video signal processing;speech;emotion recognition;filmed theater stagings emotion affective artificial agents gestural form features hand shape palm orientation motion direction gesture handedness;data mining;hand shape;indexes;multi agent systems;shape;emotion;affective artificial agents;shape speech synthesis production natural languages character generation videos displays context history organisms;video signal processing emotion recognition image motion analysis multi agent systems;gestural form features;correlation;filmed theater stagings;encoding;palm orientation;motion direction;gesture handedness	The question how exactly gesture and emotion are interrelated is still sparsely covered in research, yet highly relevant for building affective artificial agents. In our study, we investigate how basic gestural form features (handedness, hand shape, palm orientation and motion direction) are related to components of emotion. We argue that material produced by actors in filmed theater stagings are particularly well suited for such analyses. Our results indicate that there may be a universal association of gesture handedness with the emotional dimensions of pleasure and arousal. We discuss this and more specific findings, and conclude with possible implications and applications of our study.	circular polarization;intelligent agent;multimodal interaction;rl (complexity);speech synthesis;synthetic intelligence;text corpus	Michael Kipp;Jean-Claude Martin	2009	2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops	10.1109/ACII.2009.5349544	psychology;computer vision;speech recognition;emotion;shape;computer science;speech;multi-agent system;geometry;communication;correlation;encoding	HCI	-51.939789956701034	-46.83956434613402	24592
c43d5096194d47e23dff42acc084c981bc05d9ea	openlight: a concept of urban lighting to make urbanites aware of each other	community;internet of things;public space;design;lighting	hough there are many examples of attempts to create interactive lighting installations in urban public space, its meaning for urbanites has not been fully explored and defined. What could interactive lighting contribute to urban public space? Using the concept of Third Place, this research focuses on the social potential of urban public space using the concepts of, especially the role of space in connecting people and fostering social capital. Our hypothesis is that interactive urban lighting can assist this role of urban public space. Openlight is a concept of networked interactive lighting that provides urbanites with open access to penetrate psychological barriers between individuals and groups in urban public space. Hence the interaction would provide more possibilities for urbanites becoming more aware of and getting to know each other. For this first attempt, we have created a scaled prototype for a Café/Restaurant setting.	hough transform;prototype;social capital	Noriyuki Fujimura;Masa Inakage;Hideki Sunahara;Satoru Tokuhisa;Atsuro Ueki;Masato Yamanouchi	2013		10.1145/2494091.2494097	design;community;simulation;human–computer interaction;computer science;knowledge management;lighting;multimedia;internet of things	HCI	-57.733049914300814	-37.750280175750916	24611
0436d0c3d963d38c09888da587d08eb7b67aaff5	erratum to: theoretical and methodological implications of designing and implementing multiuser location-based games	methodological implication;multiuser location-based game;spatial context;interpersonal communication;data analysis;user experience;social interaction	Multiuser location-aware applications present a new form of mediated communication that takes place within a digital as well as a physical spatial context. The inherently hybrid character of locative media use necessitates that the designers of such applications take into account the way communication and social interaction is influenced by contextual elements. In this paper, an investigation into the communicational and social practices of users who participate in a location-based game is presented, with an emphasis on group formation and dynamics, interpersonal communication, and experienced sense of immersion. This investigation employs a methodological approach that is reliant on both qualitative and quantitative data analysis. A series of this user experience studyu0027s results are presented and discussed.	location-based game;multi-user	Katerina Diamantaki;Charalampos Rizopoulos;Dimitris Charitos;Nikos Tsianos;Angeliki Gazi	2010	Personal and Ubiquitous Computing	10.1007/s00779-010-0334-4	simulation;human–computer interaction;computer science;spatial contextual awareness;data analysis;interpersonal communication	HCI	-58.99420273500836	-39.17070367704985	24614
b5d868923b5e898ea0f0be1c922642d5c227d53b	a voice picking system using localized navigation speech and head tracking	merchandise;magnetic heads;heating;magnetic heads merchandise navigation heating slabs three dimensional displays tracking;navigation;slabs;three dimensional displays;task efficiency voice picking system navigation speech head tracking guide speech speech position user head movement diotic systems sound localization user satisfaction;sound localization voice picking head tracking usability testing;tracking;target tracking audio signal processing object tracking speech processing	We propose a voice picking system that uses localized guide speech oriented towards the target object, and Head Tracking (HT), which maintains the localized speech position according to the user's head movement. We verified the usability improvements achieved by the proposed system. The task completion time was reduced by about 1.8 seconds (11.2%) compared to conventional diotic conditions. The post test questionnaire showed that the HT condition is preferred over conventional diotic systems by 1.92 on a 7-point scale. Thus, with the introduction of sound localization in voice picking systems, both the user satisfaction and task efficiency can be improved.	motion capture;usability	Kazuhiro Kondo;Yasuhiro Abe	2014	2014 IEEE 3rd Global Conference on Consumer Electronics (GCCE)	10.1109/GCCE.2014.7031126	computer vision;speech recognition;engineering;communication	Robotics	-43.835190156203666	-44.82856555788328	24699
093e056dbece714a2f97479d85738655c8dd8321	improving the accessibility of dynamic web content for older users	accessibility;interaction pattern;web 2 0;world wide web;dynamic web content;dynamic content;older users	"""The Web is changing. The much vaunted Web 2.0 sees once static pages evolving into hybrid applications. Content that was once simple to surf is now becoming increasingly complicated due to the many areas of dynamic content """"dotted"""" throughout the page. In previous studies, we have shown that unlike younger users, older users have more varied interaction patterns when using dynamic content. In addition, some older users are not aware of what to expect when interacting with dynamic content and show signs of hesitancy and uncertainty when completing tasks. In this paper, we present a tool designed to assist older uses as they use Web 2.0 content and reduce the hesitancy and frustration that was previously identified."""	accessibility;dynamic web page;interaction;speeded up robust features;web 2.0;web content;world wide web	Darren Lunn;Simon Harper	2011		10.1145/1969289.1969312	web accessibility initiative;computer science;multimedia;internet privacy;world wide web	HCI	-50.83593230982922	-42.31146582433163	24739
ea83107abba36ea92d09610f8ddf93e70eb94f25	hanging out at the computer lab: how an innovative australian program is helping young 'aspies'	high functioning autism;object centred sociality;computer mediated communication;third place;social distance;asperger s syndrome	Technology-based interventions for young people diagnosed with autism have focused largely on individual use. Yet research into use of technology 'in the wild' emphasises the value of computer-mediated social interaction. In this paper we use HCI to examine the success of a program premised on the social use of technology in safe offline spaces. Participants typically go through stages of object-centred and computer-mediated communication before engaging in face-to-face interaction. We use the concepts of third place, social distance and ticket-to-talk to explain how this hybrid space helps 'Aspies' engage comfortably in social interaction.	computer lab;computer-mediated communication;human–computer interaction;online and offline	Greg Wadley;Stefan Schutt	2013		10.1145/2541016.2541078	computer-mediated communication;social distance	HCI	-62.67711106222941	-41.75392841984481	24742
1d116ab857fd615861b6b907fba1b8d305d3cc6e	how agents' turn-taking strategies influence impressions and response behaviors	wizard of oz;social skill	Different turn-taking strategies of an agent influence the impression that people have of it and the behaviors that they display in response. To study these influences, we carried out several studies. In the first study, subjects listened as bystanders to computer-generated, unintelligible conversations between two speakers. In the second study, subjects talked to an artificial interviewer which was controlled by a human in a Wizard of Oz setting. Questionnaires with semantic differential scales concerning personality, emotion, social skill, and interviewing skills were used in both studies to assess the impressions that the subjects have of the agents that carried out different turn-taking strategies. In addition, in order to assess the effects of these strategies on the subjects' behavior, we measured several aspects in the subjects' speech, such as speaking rate and turn length. We found that different turn-taking strategies indeed influence the user's perception. Starting too early (interrupting the user) is mostly associated with negative and strong personality attributes and is perceived as less agreeable and more assertive. Leaving pauses between turns is perceived as more agreeable, less assertive, and creates the feeling of having more rapport. Finally, we found that turn-taking strategies also influence the subjects' speaking behavior.	computer-generated holography;interrupt	Mark ter Maat;Khiet P. Truong;Dirk Heylen	2011	PRESENCE: Teleoperators and Virtual Environments	10.1162/PRES_a_00064	social skills;computer science	HCI	-52.28873576416444	-50.08002232305624	24762
aa8162ab1ab87228f4ce39b159d55238d02dbbd7	poster: 3-d display using motion parallax for outdoor user interface	car navigation systems;artificial motion parallax;automobiles;user interface;car navigation;moving observers;three dimensional displays user interfaces retina augmented reality testing motion control lenses optical beam splitting navigation humans;liquid crystal displays;multimedia information system;3 d display;augmented reality technology;graphical user interfaces;h 5 1 information interfaces and presentation multimedia information systems;outdoor user interface;long distance;three dimensional displays;lenses;h 5 1 information interfaces and presentation multimedia information systems 3 d display motion parallax car navigation;three dimensional displays augmented reality automobiles computerised navigation graphical user interfaces lenses liquid crystal displays optical beam splitters;beam splitter 3d display artificial motion parallax outdoor user interface moving observers car navigation systems augmented reality technology compact lcd convex lens;optical beam splitters;convex lens;navigation system;compact lcd;augmented reality;motion parallax;information interfaces and presentation;3d display;beam splitter;computerised navigation	This paper describes a 3-D display that can express differences between depths at extended distances of over a hundred meters for moving observers using motion parallax. The results of a subjective test demonstrated that the perceived depth of the image at long distances could be controlled using artificial motion parallax generated by changing the rate of expansion of the image size, irrespective of its real depth. As a result, we demonstrated its possibility for applications to car-navigation systems combined with augmented-reality technology.	augmented reality;image resolution;parallax;stereo display;user interface	Kazutake Uehira;Masahiro Suzuki;Yoshiaki Kobayashi	2008	2008 IEEE Symposium on 3D User Interfaces	10.1109/3DUI.2008.4476619	parallax mapping;parallax barrier;computer vision;parallax occlusion mapping;engineering;multimedia;computer graphics (images)	Graphics	-41.317541822589156	-40.09351936531277	24770
f9cb1033f2f9ad840467d394e9c3a5af496b36f2	breath is to be perceived - breathing signal sharing involved in remote emotional communication		In the city life with increasingly advanced technology, sociality tends to be more subtle. Although people, who have remote interpersonal intimacy, can watch or hear from each other, they cannot feel the real emotions from the other party. Especially, breath means existence in intimacy, and it is conductive to soothing emotion. This paper proposed a pair of interactive breathing sofa systems to communicate with each partner’s breath tempo in real time. Also, the shared semi-virtual space and telepresence are established to enhance the emotional exchange and resonance in long-distance relationship. Each sofa can measure the user’s breathing tempo and send it to the partner’s sofa. In addition to strengthening emotional communication of long-distance relationship, the installation aims to design and realize the enhancement of remote emotional interaction, bio-signals communication, telepresence, and telepathy of ExtraSensory Perception.		Xiaotian Sun;Kiyoshi Tomimatsu	2017		10.1007/978-3-319-58697-7_35	medical emergency;communication;social psychology	HCI	-57.55224182332997	-50.051377170582334	24796
a0f73bd7d482a0f82f6b70fed239f7cadcac7bb9	gamepad vibration methods to help blind people perceive colors		Visually impaired people, completely or partially blind cannot perceive the surrounding environment as normal healthy people can. Not knowing how colors and shapes look like is a serious penalty and they have to live with it an entire life. It is known that the human brain adapts and compensates with greater capabilities for hearing and touch when vision is low or missing, that is why blind people develop special skills and perform so good when it comes to music and kinetotherapy. Based on this information we are proposing a method that explains colors using vibrations. In this paper we would like to present a new way of transforming an image to vibration by using the capabilities of an Xbox gamepad. During the study on real blind volunteers, this kind of approach proved to be very effective and promising, allowing them to slightly understand how an image, painting or friend looks like. Author	color;gamepad	Vlad Trifanica;Alexandru Butean;Alin Moldoveanu;Diana Butean	2015			multimedia;communication;painting;computer science	HCI	-56.70707945419674	-28.966100301430753	24813
8e0c036708d973420ea59446f6209b8eedc01595	equuleus: presentation from legacy documents	annotated description equuleus legacy document computational model human computer interaction knowledge representation interactive document presentation adaptive document presentation;document handling;human computer interaction;computer model;artificial intelligence;knowledge representation human computer interaction document handling;knowledge representation	We are investigating computational models for humancomputer interaction based on a uniform, declarative, knowledge representation. We describe a new implementation of our work called Equuleus, a system for interactive, adaptive, document presentation. Equuleus achieves this by using annotated descriptions of a document to be presented and a planner to plan the document’s presentation. All presentation occurs as a result of planning and re-planning presentation goals.	automated planning and scheduling;computational model;declarative programming;feedback;knowledge base;knowledge representation and reasoning;planning;prototype;user modeling;zero suppression	Susan McRoy;Syed S. Ali;Nipat Nalamlieng	2003		10.1109/TAI.2003.1250245	well-formed document;natural language processing;knowledge representation and reasoning;computer science;artificial intelligence;data mining;multimedia;design document listing	AI	-39.47273026623242	-28.717444215685827	24835
443e17ebd09d8779bdeb73f9e6eedcb3b6ed1e18	speech: a privileged modality		Ever since the publication of Bolt’s ground-breaking “Put-That There” paper [BOLT 80], providing multiple modaliti es as a means of easing the interaction between humans and computers has been a desirable attribute of user interface design. In Bolt’s early approach, the style of modality combination required the user to conform to a rigid order when entering spoken and gestural commands. In the early 1990s, the idea of synergistic multimodal combination began to emerge [COHEN 89], although actual implemented systems (generally using keyboard and mouse) remained far from being synergistic. Nextgeneration approaches involved time-stamped events to reason about the fusion of multimodal input arriving in a given time window, but these systems were hindered by time-consuming matching algorithms. To overcome this limitation, we proposed [JULIA 93] a truly synergistic application and a distributed architecture for flexible interaction that reduces the need for explicit time stamping. Our slot-based approach is command directed, making it suitable for applications using speech as a primary modality. In this article, we use our interaction model to demonstrate that during multimodal fusion, speech should be a privileged modality, driving the interpretation of a query, and that in certain cases, speech has even more power to override and modify the combination of other modaliti es than previously believed.	algorithm;computer;distributed computing;game controller;modality (human–computer interaction);multimodal interaction;oracle fusion architecture;synergy;user interface design	Luc E. Julia;Adam Cheyer	1997			user interface design;speech recognition;modalities;machine learning;interaction model;computer science;artificial intelligence	HCI	-47.750079998495515	-37.37102328807883	24846
e0fb7a931cb19d6e7bc1acc022341a756346710a	metaphor interpretation and context-based affect detection	g400 computer science;intelligent user interfaces;meetings and proceedings;open ended text based dialogue;book chapter;metaphorical and contextual affect detection	Metaphorical and contextual affect detection from open-ended text-based dialogue is challenging but essential for the building of effective intelligent user interfaces. In this paper, we report updated developments of an affect detection model from text, including affect detection from one particular type of metaphorical affective expression and affect detection based on context. The overall affect detection model has been embedded in an intelligent conversational AI agent interacting with human users under loose scenarios. Evaluation for the updated affect detection component is also provided. Our work contributes to the conference themes on sentiment analysis and opinion mining and the development of dialogue and conversational agents.	dialog system;embedded system;intelligent user interface;interaction;nonlinear gameplay;sentiment analysis;text-based (computing)	Xiang Lin	2010			natural language processing;computer science;multimedia	AI	-52.738500761892865	-46.68333247433858	24866
cd6d3456e4a3a6d8f39dbc60705abe0babefec96	designing a new mobile search service: a user-centered approach	search engine;design process;mobile user interfaces;usability testing;visual design;user centered design;mobile search;prototyping;telecommunications industry;mobile internet;interaction design;mobile user interface ui design	This case study explores a user-Centered design (UCD) approach to the design of a new, mobile internet search engine aimed at consumers in a commercial context.	user-centered design;web search engine	Matt Davies	2008		10.1145/1409240.1409310	user interface design;user experience design;user-centered design;mobile search;mobile web;design process;human–computer interaction;computer science;mobile technology;interaction design;prototype;multimedia;design education;mobile computing;world wide web;search engine	HCI	-52.030255336101376	-39.732316707667714	24876
8353f3f35ea5dcfa42ea28898950112d5e73951d	understanding media situatedness and publication practices in place-based digital displays	display networks;media situatedness;publication practices;place based displays	Digital public displays are evolving towards becoming more ubiquitous, but also towards alternative media publication paradigms that challenge prevailing assumptions. In our work, we are particularly interested in the concept of place-based displays, which are managed independently by a local display owner to serve the communications goals of a specific place. We analysed usage and content data from 35 place-based displays to understand how their specific properties can affect the respective media publication practices and the situatedness of the content. The results show that publication practices tend to be less formal and much more situated than what is now common in most display networks. These display owners seem to have perceived the need to balance between the values of immediate and spontaneous communication and the values of branding and media planning. This might confirm the expectation that future display systems may evolve towards becoming a medium that is more open to new forms of self-expression, appropriation and Human connectedness.	autonomous robot;global serializability;internet branding;situated;social media;spontaneous order	Pedro Coutinho;Rui José;Bruno Silva	2016		10.1145/2914920.2915025	public relations;computer science;knowledge management;multimedia	HCI	-58.30794156670555	-38.742334012359606	24879
061d90af77d8fecfe92ca0045e8c6c9610ad19be	nasa johnson space center, human-computer interaction	human computer interaction;johnson space center	THE LABORATORY The Human-Computer Interaction Laboratory (I-ICIL) was established in 1984 at the NASA Johnson Space Center (JSC) in Houston, TX, to: • Perform applied research examining user interaction with complex computer systems; • Apply the results of this research to the design of spacecraft computer-based workstations and HCIs; and • Serve as a resource in the discipline of humancomputer interaction to support ongoing programs. The HCIL is one of five R&D laboratories within the Crew Interface Analysis Section of the Man-Systems Division in the Space & Life Sciences Directorate.	human–computer interaction;workstation	Marianne Rudisill;Douglas J. Gillan	1989		10.1145/67449.67462	simulation;human–computer interaction;computer science	HCI	-49.3372432524293	-25.60064263231032	24991
abd9a9f9b62221d77b123392d09e401fef604131	end-to-end solution for accessible chemical diagrams	accessible diagrams;image transformation;chemistry	Chemical diagrams are an important means of conveying information in chemistry and biosciences to students, starting as early as secondary school. But even in electronic teaching material, diagrams are commonly given as bitmap graphics leaving them inaccessible for visually impaired learners. We present an end-to-end solution to making these diagrams Web accessible, by employing image analysis solutions to recognise and semantically analyse diagrams, and by regenerating them in a format that makes them amenable to assistive technology. We provide software tools that allow readers to interactively engage with diagrams by exploring them step-wise and on different layers, enabling aural rendering of diagrams and their individual components together with highlighting and magnification to assist readers with low vision or learning difficulties. Our technology builds on open standards, supporting a number of computing platforms, browsers, and screen readers, and is extensible to diagrams in other STEM subjects.	assistive technology;bitmap;diagram;end-to-end principle;image analysis;interactivity;raster graphics	Volker Sorge;Mark Lee;Sandy Wilkinson	2015		10.1145/2745555.2746667	computer science;theoretical computer science;multimedia	HCI	-44.975542102106346	-30.460260776277735	24999
1e87bdaf05b04820e67d2eb8015adbd1f781ebe6	describing user interactions in adaptive interactive systems	input device;task model;probabilistic automata;interactive system;user behavior;user interaction	The description of the user-system interaction plays a crucial role in adaptive interactive systems, since the adaptations depend on this description. User actions in interactive systems can be described as a sequence of events, which are created by input through input devices as well as by the system as reactions to these inputs. An interactive system can observe these events and thus extract information about the user's behavior. This paper presents a two-step approach for describing user behavior from sequences of basic events. First, user actions are recognized in the sequence of interaction events by means of previously trained probabilistic automata. Second, a task model describes the higher-level user activity as a hierarchical composition of these actions. Different kinds of adaptive support can be derived from this description of user behavior, such as recommending next interaction steps to the user.	interaction;interactivity	Matthias Bezold	2009		10.1007/978-3-642-02247-0_16	user;simulation;user modeling;interactive systems engineering;human–computer interaction;computer science;multimedia;user interface	HCI	-55.212044765428644	-44.95647554118523	25022
7a7a3f2af6c335ba68b53a3d546382eb2e13fa96	the chi management community	community;hci;management	This SIG will provide those interested in the interplay between management and HCI the opportunity to explore this subject and the ongoing development of the Management Community at the CHI conferences.	chi;human–computer interaction	Austin Henderson;James A. Euchner	2006		10.1145/1125451.1125544	community;human–computer interaction;computer science;knowledge management;multimedia	HCI	-61.98876998784818	-32.683988994485546	25035
6f359b907775c3fc54cc8a450b9b5fdc576724f5	museums and virtual reality: using the cave to simulate the past	virtual reality		simulation;virtual reality	Delia Tzortzaki	2001	Digital Creativity	10.1076/digc.12.4.247.3216	visual arts;cave automatic virtual environment;simulation;computer science;artificial intelligence;virtual reality;mixed reality;multimedia;immersion	Visualization	-48.64851874665372	-32.91968826611807	25117
e63dee35bf85f08c1df649a24f418eb3812d2307	device-orientation is more engaging than drag (at least in mobile computing)	human computer interaction;manniska datorinteraktion interaktionsdesign;panorama;engagement;movement based interaction;navigation;immersion;user experience;device orientation based panning;affective computing	Does device-orientation-based panning on mobile devices facilitate engagement? 20 users were asked to pan panoramas by turning around and changing the direction of the device, and by swiping with the finger on the touchscreen. The participants were also asked to rate how engaging they found it on the User Engagement Scale. It turned out that device-orientation-based panning was more engaging than drag-based panning. Moving your body to navigate information can pull you into an affective loop.	mobile computing;mobile device;touchscreen	Mattias Arvola;Anna Holm	2014		10.1145/2639189.2670245	computer vision;navigation;user experience design;simulation;human–computer interaction;computer science;affective computing;multimedia;immersion	HCI	-46.294631285228	-43.54451717948393	25128
d7b558fb6ed590597637bfd381e1d60bb671efba	continuous gaze cursor feedback in various tasks: influence on eye movement behavior, task performance and subjective distraction		Using gaze as input modality has often been promoted as a method for advanced computer interaction. One important detail in gaze controlled interfaces is the design of optimal feedback. Highlighting the current point of gaze by a gaze contingent cursor represents a simple form of feedback. In an experimental study, we investigated the influence of gaze cursor feedback on eye movement behavior, task performance and subjective distraction. Participants of the study completed three different tasks (gaze typing, reading and image exploration) with five different feedback conditions. No-feedback was implemented as baseline condition and compared with gaze cursor feedback of various spatial precision and temporal delays. A blue, semitransparent small dot served as gaze cursor. The observed findings are discussed in the context of user friendly feedback for gaze based computer interaction.		Sven-Thomas Graupner;Sebastian Pannasch	2014		10.1007/978-3-319-07857-1_57	computer vision;simulation	HCI	-46.98809484368418	-47.31858708510663	25139
24e0ff707b29fb99cfd5c5c4eb8fa028e8636919	holibraille: multipoint vibrotactile feedback on mobile devices	output;blind;input;multitouch;vibrotactile;braille	We propose HoliBraille, a system that enables Braille input and output on current mobile devices. We use vibrotactile motors combined with dampening materials in order to actuate directly on users' fingers. The prototype can be attached to current capacitive touchscreen devices enabling multipoint and localized feedback. HoliBraille can be leveraged in several applications including educational tools for learning Braille, as a communication device for deaf-blind people, and as a tactile feedback system for multitouch Braille input. We conducted a user study with 12 blind participants on Braille character discrimination. Results show that HoliBraille is effective in providing localized feedback; however, character discrimination performance is strongly related with number of simultaneous stimuli. We finish by discussing the obtained results and propose future research avenues to improve multipoint vibrotactile perception.	input/output;mobile device;multi-touch;multipoint ground;prototype;touchscreen;usability testing	Hugo Nicolau;Kyle Montague;Tiago João Vieira Guerreiro;Vicki L. Hanson	2015		10.1145/2745555.2746643	computer hardware;engineering;multimedia;communication	HCI	-45.21953631487504	-43.97707531154001	25141
0867a86c9e9216ebbb58b89b209c268b8f50cf52	measuring user confidence in smartphone security and privacy	application installation;user study;mobile phone usage;laptop usage;user perception;smartphones;mobile phone;security and privacy;mobile application	In order to direct and build an effective, secure mobile ecosystem, we must first understand user attitudes toward security and privacy for smartphones and how they may differ from attitudes toward more traditional computing systems. What are users' comfort levels in performing different tasks? How do users select applications? What are their overall perceptions of the platform? This understanding will help inform the design of more secure smartphones that will enable users to safely and confidently benefit from the potential and convenience offered by mobile platforms.  To gain insight into user perceptions of smartphone security and installation habits, we conduct a user study involving 60 smartphone users. First, we interview users about their willingness to perform certain tasks on their smartphones to test the hypothesis that people currently avoid using their phones due to privacy and security concerns. Second, we analyze why and how they select applications, which provides information about how users decide to trust applications. Based on our findings, we present recommendations and opportunities for services that will help users safely and confidently use mobile applications and platforms.	ecosystem;mobile app;mobile device;mobile security;privacy;smartphone;usability testing	Erika Chin;Adrienne Porter Felt;Vyas Sekar;David A. Wagner	2012		10.1145/2335356.2335358	mobile web;internet privacy;world wide web;computer security	HCI	-58.089221373168385	-43.46864937217138	25144
8df603c68dc06bca2366d23144dcb4349148cb87	geocoach: a cross-device hypermedia system to assist visually impaired people to gain environmental accessibility		Benefiting from today’s global positioning systems (GPS) and geographic information systems (GIS), visually impaired individuals are able to travel more independently than before. However, due to a lack of accessibility information about geographic features, those navigation systems fail to satisfy their special requirements. To help visually impaired people access and share environmental accessibility information in cities ubiquitously, this paper presents a collaborative and cross-device hypermedia system, namely GeoCoach. In addition to desktop computers, visually impaired people are able to access environmental accessibility on a pin-arrayed display and on mobile phones. The two user studies conducted with visually impaired users indicate that the proposed system was effective.	accessibility;desktop computer;geographic information system;global positioning system;hypermedia;mobile phone;requirement;usability testing	Limin Zeng;Gerhard Weber	2017	Informatik-Spektrum	10.1007/s00287-017-1071-0	human–computer interaction;computer science;multimedia;hypermedia;global positioning system;geographic information system	HCI	-50.09709502104695	-41.49139255730533	25194
28b1de50f2e1f228e94691e19dc9996c43bbf0cb	lightdb: a dbms for virtual reality video		We present the data model, architecture, and evaluation of LightDB, a database management system designed to efficiently manage virtual, augmented, and mixed reality (VAMR) video content. VAMR video differs from its two-dimensional counterpart in that it is spherical with periodic angular dimensions, is nonuniformly and continuously sampled, and applications that consume such videos often have demanding latency and throughput requirements. To address these challenges, LightDB treats VAMR video data as a logically-continuous six-dimensional light field. Furthermore, LightDB supports a rich set of operations over light fields, and automatically transforms declarative queries into executable physical plans. We have implemented a prototype of LightDB and, through experiments with VAMR applications in the literature, we find that LightDB offers up to 4× throughput improvements compared with prior work. PVLDB Reference Format: Brandon Haynes, Amrita Mazumdar, Armin Alaghi, Magdalena Balazinska, Luis Ceze, Alvin Cheung. LightDB: A DBMS for Virtual Reality Video. PVLDB, 11 (10): 1192-1205, 2018. DOI: https://doi.org/10.14778/3231751.3231768	angularjs;data model;database;digital video;executable;experiment;jart armin;light field;mixed reality;prototype;requirement;throughput;virtual reality	Brandon Haynes;Amrita Mazumdar;Armin Alaghi;Magdalena Balazinska;Luis Ceze;Alvin Cheung	2018	PVLDB	10.14778/3231751.3231768	database;virtual reality;computer science	DB	-34.79329742550894	-33.83498456385034	25199
f5e6ac23739fffca4fb09f4fab7e3ff4f8030a3f	evaluation of a human-robot interface: development of a situational awareness methodology	user interface;evaluation method;robotics and automation user interfaces dictionaries intelligent robots human computer interaction mobile robots physics computing vehicle dynamics remotely operated vehicles air traffic control;robots;situation awareness;situational awareness human robot interface supervisory user interfaces robotic vehicles;man machine systems;man machine systems robots;human robot interface	This paper outlines a methodology to evaluate supervisory user interfaces for robotic vehicles based on an assessment of situational awareness. The results of an initial experiment are discussed. The evaluation method will be validated in a future experiment that will also result in a benchmarked user interface.	robot;user interface	Jean Scholtz;Brian Antonishek;Jeff Young	2004	37th Annual Hawaii International Conference on System Sciences, 2004. Proceedings of the	10.1109/HICSS.2004.1265327	user interface design;human–robot interaction;robot;situation awareness;simulation;human–computer interaction;computer science;artificial intelligence;robot control;natural user interface;user interface	Robotics	-36.937665827560494	-41.53993212278656	25207
454fcbb01d1641d6d064105bf4b008481bc6fbc7	integrating usability models into pervasive application development	ddc 500;fakultat fur mathematik informatik und statistik;ddc 510	This thesis describes novel processes in two important areas of human-computer interaction (HCI) and demonstrates ways to combine these in appropriate ways.#N#First, prototyping plays an essential role in the development of complex applications. This is especially true if a user-centred design process is followed. We describe and compare a set of existing toolkits and frameworks that support the development of prototypes in the area of pervasive computing. Based on these observations, we introduce the EIToolkit that allows the quick generation of mobile and pervasive applications, and approaches many issues found in previous works. Its application and use is demonstrated in several projects that base on the architecture and an implementation of the toolkit.#N#Second, we present novel results and extensions in user modelling, specifically for predicting time to completion of tasks. We extended established concepts such as the Keystroke-Level Model to novel types of interaction with mobile devices, e.g. using optical markers and gestures. The design, creation, as well as a validation of this model are presented in some detail in order to show its use and usefulness for making usability predictions.#N#The third part is concerned with the combination of both concepts, i.e. how to integrate user models into the design process of pervasive applications. We first examine current ways of developing and show generic approaches to this problem. This leads to a concrete implementation of such a solution. An innovative integrated development environment is provided that allows for quickly developing mobile applications, supports the automatic generation of user models, and helps in applying these models early in the design process. This can considerably ease the process of model creation and can replace some types of costly user studies.	pervasive informatics;usability	Paul Holleis	2008			simulation;human–computer interaction;computer science;artificial intelligence	HCI	-52.19580839462076	-37.69523627457957	25333
95a20a5d8258d2b2c25fa6910c0a4b9665995c23	anemonestarheart: an emotive wearable	wearable technology;data;hci;visualisation;algorithms;design;eeg;physiological;wearable computing;emotive wearables	This paper describes an emotive wearable called AnemoneStarHeart, an illuminated, heart-shaped pendant created for visualising and displaying EEG data between couples, close friends and family, or for the wearer's relaxation or productivity monitoring purposes. The device maps data sent from an EEG headset and displays it by illuminating the AnemoneStarHeart pendant accordingly. This paper discusses the motivation for creating the device and how its purpose and design was influenced by focus groups and field trial feedback. The paper goes on to chart the device's development through execution of the concept and ends with conclusions and next steps.	electroencephalography;focus group;headset (audio);linear programming relaxation;map;wearable computer	Rain Ashford	2016		10.1145/2968219.2971349	design;human–computer interaction;computer science;multimedia;wearable technology;data	HCI	-55.21600294745285	-42.92967871641019	25343
5828d89a2a761f544518dd45b11b4a518380619e	reading text from computer screens	legibility;empirical study;affichage;visualizacion;ecran visualisation;pantalla visualizacion;display;legibilidad;lisibilite;display screen	This paper reviews empirical studies concerning the readability of text from computer screens. The review focuses on the form and physical attributes of complex, realistic displays of text material. Most studies comparing paper and computer screen readability show that screens are less readable than paper. There are many factors that could affect the readability of computer screens. The factors explored in this review are the features of characters, the formatting of the screen, the contrast and color of the characters and background, and dynamic aspects of the screen. Numerous areas for future research are pinpointed.	color;computer monitor;human-readable medium	Carol Bergfeld;Linda J. Weldon	1987	ACM Comput. Surv.	10.1145/45075.46162	human–computer interaction;computer science;multimedia;empirical research;computer graphics (images)	HCI	-60.36170668250195	-47.97162838797736	25443
9697246d8b07ef748996cbf7ba72944049236bcf	corona: haptic sensation using body-carried electrostatic charge for body area network feedback companion	electrostatic charge and discharge;haptic feedback device;on body sensor actuator;physical stimuli	Improvements in Body Area Network (BAN) technical feasibility have encouraged research, which propose applications exploring data transferring touch interaction. In this work, we aim to provide tactile feedback for these applications to improve user experience. We propose Corona, a wearable tactile feedback device that uses electrostatic force to provide physical stimuli when a user touches objects that are electrically grounded, or of opposite polarity. Unlike previous approach, we neither physically actuate objects nor use any hand-worn tactile feedback devices. We introduce a mechanism to artificially build up electrostatic charge within the human body, and further leverage effects of electrostatic discharge to create novel physical stimuli. In this paper, we report underlying theory of operation as well as details of our prototype implementation. We illustrate our results using two applications: a) SandStorm: visualization of actuated force field using sand, and b) CheckMate: future vision of a touch-based transaction checkout system equipped with Corona to provide haptic sensation for more reassuring feedbacks.	discharger;emoticon;force field (chemistry);haptic technology;point of sale;prototype;user experience;wearable computer	Adiyan Mujibiya	2014		10.1145/2669485.2669535	control engineering;simulation;engineering;electrical engineering	HCI	-46.61253521022914	-40.95380476889799	25498
b423a75b4d0d52eba2a775fb9e254d0a11eb251b	digital longmen project: a free walking vr system with image-based restoration		Located in China’s ancient capital Luoyang, Longmen Grottoes are one of the finest examples of Buddhist stone carving art. Nowadays, many caves do not have public access due to heritage preservation. In order to let people appreciate these relics, we setup a VR system with smartphones and helmets based on scanned models and textures. Motion capture system is also utilized to make the viewpoint not fixed so that users can walk freely as if in the cave. Moreover, since some sculptures have been heavily damaged, we propose a digital restoration framework to enhance exhibition contents. The framework includes general and detailed restoration from a single old image by shape from shading and landmark driven mesh deformation respectively. In practice, we develop this system for the representative Middle Binyang Cave, with interactions such as gesture recognition exploited to provide satisfactory user experience, which can ease the conflict between tourism and preservation.		Zeyu Wang;Xiaohan Jin;Dian Shao;Renju Li;Hongbin Zha;Katsushi Ikeuchi	2016		10.1007/978-3-319-54427-4_15	computer graphics (images)	Robotics	-52.017564887672826	-29.151597515616718	25504
8f94559a5e6c03805852fa0f24be919f8eac8bd1	functional presentation of travel opportunities in flexible use airspace: an eid of an airborne conflict support tool	cognitive science;ecological interface design;aircraft separation assurance task;air traffic control;air traffic control trajectory spatial resolution aerospace control aircraft navigation aerospace engineering aircraft propulsion path planning visualization biological system modeling;flexible use airspace;abstraction hierarchy;eid;state vector envelope functional presentation flexible use airspace eid airborne conflict support tool aircraft separation assurance task ecological interface design;user interfaces air traffic control;state vector envelope;vector field;airborne conflict support tool;user interfaces;functional presentation	"""In a flexible airspace environment the pilot disposes of an increased amount of travel opportunities. At the same time the airspace traffic situation becomes more complex and the aircraft separation assurance task is shifted towards the cockpit. The design paradigm of ecological interface design is applied to support the pilot with the airborne planning of efficient trajectory paths that maintain spatial separation from other traffic. The desired pilot behavior is achieved by visualizing travel-relevant airspace affordances in terms of realistic aircraft locomotion. As a result, a novel interface the """"state vector envelope"""" presents safe and efficient travel opportunities in a state vector field. The concept has been evaluated through on-line simulations of a number of basic conflict situations"""	airborne ranger;ecological interface design;online and offline;programming paradigm;quantum state;separation kernel;simulation	Stijn B. J. Van Dam;An L. M. Abeloos;Max Mulder;René van Paassen	2004	2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)	10.1109/ICSMC.2004.1398401	free flight;vector field;simulation;ecological interface design;computer science;air traffic control;user interface	Robotics	-37.213380443062135	-40.86106529730096	25524
3e1f50bed526628edf536d269a757494f43b0f92	tangible bits and malleable atoms in the design of a computer music instrument	malleable atoms;instrument design;input output;computer music;tangible bits;physical interface;musical instrument design;affective computing	We present SensOrg, a computer music instrument designed as a modular assembly of input/output devices and musical software, mapped and arranged according to functional characteristics of the musician-instrument system. Using tangible bits and malleable atoms, we externally represented the musical software functionality in a physical interface which is freezable yet totally flexible.	input/output;jaquet-droz automata;output device	Roel Vertegaal;Tamas Ungvary	2001		10.1145/634067.634251	input/output;human–computer interaction;computer hardware;computer science;artificial intelligence;affective computing;multimedia;computer music	HCI	-45.84432468737841	-36.45733168390393	25564
d5ae19e471bdd8efee2bf7257c1f9c931e131d9b	automatic form filling on mobile devices	context aware;critical point;mobile device;text input;mobile text entry;qa75 electronic computers computer science;visualization of uncertainty;mobile commerce;automatic form filling;mobile devices;web based service	Filling out forms of web based services on mobile devices is a very time consuming and frustrating task for users because of the limited text input capabilities. This is a critical point to get a wide acceptance of such services, especially mobile commerce that often requires filling user data. We developed an architecture based on a local proxy on a mobile device and a lightweight algorithm for a comprehensive analysis of forms, which leads to the highest probable user data to be filled in, driven by an initial rule set [1]. We further discuss our implementation and the evaluation results of the algorithm as well as the usability of the prototype.	algorithm;critical point (network science);mobile commerce;mobile device;prototype;usability	Enrico Rukzio;Chie Noda;Alexander De Luca;John Hamard;Fatih Coskun	2008	Pervasive and Mobile Computing	10.1016/j.pmcj.2007.09.001	embedded system;mobile search;simulation;mobile web;human–computer interaction;mobile database;computer science;operating system;mobile technology;mobile device;multimedia;mobile computing;world wide web;computer security;computer network	HCI	-50.02542230079868	-41.98345528977842	25707
668d3f3daac65eced9ca0f245019e03e307f570e	the fandom of the opera: how the audience for a centuries-old art form has incubated electrical and electronics technologies [scanning our past]	audio systems;history;history music electronic music audio systems lighting tv motion pictures;motion pictures;lighting;tv;music;electronic music	This intriguing article provides a technological history of the several innovations relating to opera and new electrical and electronic technologies through its many years of operation.	electrical engineering;opera (web browser)	Mark Schubin	2016	Proceedings of the IEEE	10.1109/JPROC.2016.2521129	electronic music;music industry;music;lighting;multimedia;computer graphics (images)	Visualization	-52.57644370335643	-28.676485993980297	25761
147420e23d1587762ed5441ec1eacab1887047b7	distributions of function words across narrative time in 50, 000 novels				David McClure;Scott Enderle	2018			literature;narrative;art	NLP	-53.36940373989482	-28.84296892314602	25767
c16835cd86e35b098bce07f934310fbcd742d2bf	tilt & touch: mobile phone for 3d interaction	3d interaction;gyroscope;smart phone;mobile phone;virtual reality environment;large displays	Mobile phones are becoming de facto pervasive devices for people's daily use. This demonstration illustrates a new interaction, Tilt & Touch, to enable a smart phone to be a 3D controller. It exploits capacitive touchscreen and built-in MEMS motion sensors. When people want to navigate in a virtual reality environment on a large display, they can tilt the phone for viewpoint transforming, touch the phone screen for avatar moving, and pinch screen for viewing camera zooming. The virtual objects in the virtual reality environment can be rotated accordingly by tilting the phone.	3d interaction;microelectromechanical systems;mobile phone;pervasive informatics;sensor;smartphone;touchscreen;virtual reality	Yuan Du;Haoyi Ren;Gang Pan;Shijian Li	2011		10.1145/2030112.2030183	embedded system;computer vision;gyroscope;multimedia	HCI	-44.625629429860304	-41.51202450504543	25769
0b1231a8b9a2b9dc7cba43dcb1ec683ca00dcdfd	gesture recognition in flow based on pca analysis using multiagent system	virtual theatre;gesture command;motion capture;gesture recognition	Our context is Virtual Theater. Our aim is to put on a short play featuring a real actor and a virtual actor, who will communicate through movements and choreography with mutual synchronization. We currently work on a system that can recognize key-gestures made by a real actor.  In this paper, we cover a method for real-time recognition. Our idea is that the data for movements from any motion-capture system can be reduced to a single artificial signature. We use properties from Principal Component Analysis (PCA) to generate it. This artificial gesture representation is used in real-time by our multiagent systems to simultaneously perform segmentation (gesture's beginning and end) and recognition. We conducted various experiments which demonstrate our system and define its limitations.	agent-based model;experiment;gesture recognition;motion capture;multi-agent system;principal component analysis;real-time locating system;virtual actor	Ronan Billon;Alexis Nédélec;Jacques Tisseau	2008		10.1145/1501750.1501783	computer vision;speech recognition;computer science;gesture recognition;communication	AI	-36.73270156663807	-39.42250104552877	25784
766ba86dc79727df6c001154afc2d7f5ec44b9bc	life is sharable: blogging life experience with rfid embedded mobile phones	mobile device;social interaction;wireless network;mobile phone;weblog;wireless communication;rfid;computer mediated communication;peer to peer communication;peer to peer;wireless networking;directory service	Recent development and proliferation of mobile devices, wireless communication, and sensor technologies have prompted a new vision of social interactions in the world in which we inhabit. For example, the number of mobile phones that are capable of capturing users' spontaneous life experiences not only in pictures but also in audio and video clips is on the increase. In addition, captured experiences can be sent and shared with others over wireless networks such as WiFi, GSM, GPRS, EDGE, or recently commenced third-generation (3G) protocols. Other technology trends such as weblog and peer-to-peer communication provide a ubiquitous platform and a model of motivating users to share life experiences with other people. The hypothesis is that everyone can be an owner of a weblog which records their experience about place, people, and things that interest them. Initial form of peer-to-peer communication, centralized directory service, can help authors of weblogs attain widespread of popularity and increase the level of participation in this type of interaction. The convergence of these technologies provides new design opportunities for computer-mediated communications. The aim of this paper is to share our experiences in designing and implementing a novel physical prototype which incorporates peer-to-peer communication, weblog, RFID, wireless networking, and mobile phone technologies to enhance social quality of shared life experience.	blog;centralized computing;directory service;embedded system;experience;image;interaction;mobile device;mobile phone;peer-to-peer;prototype;radio-frequency identification;spontaneous order;video clip	Yun-Maw Cheng;Wai Yu;Tzu-Chuan Chou	2005		10.1145/1085777.1085837	social relation;computer science;operating system;wireless network;multimedia;internet privacy;mobile computing;world wide web;computer-mediated communication	HCI	-56.5840438029148	-39.90648812644761	25807
ee561acc81feb9f44ad140c6f963e7fcd39e2884	soundspot: a next-generation audio-guide system for museums	location sensing systems;speaker array system;rfid;next generation	In this paper, we propose a system called SoundSpot, which is A Next-generation Audio-guide System for museums. Although the original goal of the system is to solve problems in audio-based annotations, the system can attract a lot of interest as an magical entertainment system. So though the system we aim that children can enhance their motivation to know information which are related to exhibitions. We evaluated the system in a Japanese science museum and confirmed its effectiveness.		Fusako Kusunoki;Ichiro Satoh;Hiroshi Mizoguchi;Shigenori Inagaki	2007		10.1145/1255047.1255120	geography;archaeology;cartography	SE	-50.995047937957665	-36.521783808056256	25862
9084b4a87e2774c43510a180649dc096161dd976	toward intelligent environments: supporting reflection with smart objects in the home		Interaction design is increasingly about embedding interactive technologies in our built environment; architecture is increasingly about the use of interactive technologies to reimagine and dynamically repurpose our built environment. This forum focuses on this intersection of interaction and architecture. --- Mikael Wiberg, Editor	intelligent environment;interaction design;smart objects	Maliheh Ghajargar	2017	Interactions	10.1145/3095712	multimedia;architecture;human–computer interaction;home automation;engineering;interaction design;built environment;smart objects	HCI	-52.722972751800995	-34.92480901743777	25872
95e69c66f6cdacb18236615a1e9228d56abce245	incorporating layout managers into an evolutionary programming algorithm to design graphical user interfaces	design principle;design process;human computer interaction;user interface;evolutionary programming;interface design;graphic user interface;genetic algorithm;human computer interaction hci;genetic algorithms;evolutionary process	Designing graphical user interfaces (GUIs) is an arduous task that requires both functional and aesthetic considerations, often relying on documented style guides and design principles. Style guides and principles are mostly prescriptive in terms of what should be included and what should be avoided in interface design, but do not specify the how of the interface design process. Previous research has employed Genetic Algorithms to assist in the design process, but focused more on evolving colour schemes and ordering of user interface (UI) components than on the general layout of the interface. Components were essentially placed in a static grid which resulted in unappealing interfaces. This research seeks to evolve the placement of components on the screen through the use of layout managers. A user guides the evolution process by iteratively selecting promising interfaces from a collection of candidate interfaces. Various constraints are placed on the grouping of components to prevent inappropriate groupings in the UI layout and to reduce the number of selections that the designer has to make. Each candidate UI is encoded in a tree which made crossover operations inappropriate. This resulted in an Evolutionary Programming algorithm being used rather than a Genetic Algorithm. Various mutation operators are discussed. Through this evolutionary process, aesthetically pleasing and functional interfaces can then be created in a reasonable number of iterations.	evolutionary programming;genetic algorithm;graphical user interface;iteration;layout manager;programming style;tree (data structure)	Mathys C. du Plessis;Lynette Barnard	2008		10.1145/1456659.1456665	user interface design;simulation;human–computer interaction;computer science;theoretical computer science;user interface	HCI	-39.947295470078025	-29.8050344274201	25905
d71d08adc8275e57f9e6dbbffec9e383d4d489c4	sensepods: a zigbee-based tangible smart home interface		Low-cost sensors and ubiquitous wireless networking is enabling novel ways in which homeowners can interact with their smart homes. Many complementary approaches like using voice commands, direct interaction by using touch or weight, or by using body gestures are emerging. This paper shows the design and implementation of a novel low-power, low-cost, hand-held wireless device called a SensePod. SensePods can be used by a consumer to interact with a smart home using simple gestures like rubbing, taping or rolling the device on any home surface like a dining table. The device is only 4.5 cm long, forms an ad-hoc wireless network using the ZigBee protocol, and can be easily interfaced to existing home management systems using a universal serial bus port. The gestures in each device can be programmed to control various objects of a smart home like smart curtains, for example. Hidden Markov models were used to train the device to recognize various gestures. The device was tested with a variety of gestures and has a recognition rate of over 99.7%, and a response time of less than two milliseconds.	enterprise javabeans;hidden markov model;hoc (programming language);home automation;interaction;low-power broadcasting;markov chain;mesh networking;mobile device;percussion mallet;response time (technology);sensor;serial communication;speech recognition;usb	Waqqas M. Khan;Imran A. Zualkernan	2018	IEEE Transactions on Consumer Electronics	10.1109/TCE.2018.2844729	artificial intelligence;wireless network;computer science;wireless sensor network;voice command device;embedded system;computer vision;home automation;intelligent sensor;gesture;serial communication;wireless ad hoc network	HCI	-41.71676284109395	-42.92248231164764	25913
d0ffb5e4dad7f8b292be8d7efa0578fbe451c5cf	speech and human language technology at the naval research laboratory	speech interfaces;human computer interaction;information technology;artificial intelligent;human language technology;applied research;naval research laboratory	Following a brief introduction to the Naval Research Laboratory (NRL), specific on-going research in speech and human language technology will be presented. Areas of research fall across two laboratories in the Information Technology Division, the Human Computer Interaction Laboratory (HCI) and the Navy Center for Appl ied Research in Artificial Intelligence (NCARAI). Speech research focuses on narrowband algorithm development and uses human based intelligibility to evaluate success. Demonstrations of results will be played. Navy has been a prominent participant in evaluation of narrowband speech. Current efforts on human-human communica t ion evaluation will provide a basis for new approaches to multi-mode interactions which include speech interfaces. Finally, a video of on-going efforts at NCARAI on the EUCALYPTUS system which include graphics and spoken language interaction will be shown.	algorithm;artificial intelligence;graphics;human computer;human–computer interaction;intelligibility (philosophy);language technology	Helen M. Gigley	1994			natural language processing;speech recognition;human–computer interaction;computer science;information technology	HCI	-49.28020827235158	-25.64095594470663	25916
26d426691ee2a3922f6c437e4f74e308c019af4e	reducing stress by bonding with a social robot: towards autonomous long-term child-robot interaction		Pediatric oncology patients could benefit from bonding with a social robot and talking about their day in the hospital. With our research we aim to contribute to the development of a robot that is able to facilitate a child-robot bond autonomously and long-term. We propose to use robot-disclosure and a shared interaction history to create a child-robot bond where the child feels comfortable and familiar enough to talk about their day with the robot.	autonomous robot;social robot	Mike Ligthart;Koen V. Hindriks;Mark A. Neerincx	2018		10.1145/3173386.3176904	human–computer interaction;pediatric oncology;social robot;robot;computer science;robotics;artificial intelligence;human–robot interaction	Robotics	-55.637994528014325	-51.78067730976291	25981
6b66485a8e39a8e2f2a172dd5b969f574e7d40c7	one hundred data-driven haptic texture models and open-source methods for rendering on 3d objects	force haptic interfaces friction data models rendering computer graphics acceleration arrays;texture vibrations synthesis data driven haptic texture models open source methods rendering methods 3d objects hatt toolkit penn haptic texture toolkit friction models impedance type haptic interface sensable phantom omni haptic virtual textures coulomb friction modeling handheld tool acceleration signal piecewise autoregressive process ar process delaunay triangulation force function speed function texture force vector normal force tangential speed;autoregressive processes;rendering computer graphics autoregressive processes friction haptic interfaces mesh generation;haptic interfaces;rendering computer graphics;friction;mesh generation	This paper introduces the Penn Haptic Texture Toolkit (HaTT), a publicly available repository of haptic texture models for use by the research community. HaTT includes 100 haptic texture and friction models, the recorded data from which the models were made, images of the textures, and the code and methods necessary to render these textures using an impedance-type haptic interface such as a SensAble Phantom Omni. This paper reviews our previously developed methods for modeling haptic virtual textures, describes our technique for modeling Coulomb friction between a tooltip and a surface, discusses the adaptation of our rendering methods for display using an impedance-type haptic device, and provides an overview of the information included in the toolkit. Each texture and friction model was based on a ten-second recording of the force, speed, and high-frequency acceleration experienced by a handheld tool moved by an experimenter against the surface in a natural manner. We modeled each texture's recorded acceleration signal as a piecewise autoregressive (AR) process and stored the individual AR models in a Delaunay triangulation as a function of the force and speed used when recording the data. To increase the adaptability and utility of HaTT, we developed a method for resampling the texture models so they can be rendered at a sampling rate other than the 10 kHz used when recording data. Measurements of the user's instantaneous normal force and tangential speed are used to synthesize texture vibrations in real time. These vibrations are transformed into a texture force vector that is added to the friction and normal force vectors for display to the user.	algorithm;autoregressive model;characteristic impedance;delaunay triangulation;executable;handheld game console;haptic technology;imaging phantom;open-source software;phantom reference;rendering (computer graphics);resampling (statistics);sampling (signal processing);tooltip;virtual reality;voice coil	Heather Culbertson;Juan Jose Lopez Delgado;Katherine J. Kuchenbecker	2014	2014 IEEE Haptics Symposium (HAPTICS)	10.1109/HAPTICS.2014.6775475	computer vision;simulation;computer science;computer graphics (images)	Visualization	-39.00959394842973	-38.723312656979125	25999
ce85d66a837eef9b53c94f9e4ebf85a20b99e4a0	making scents: aromatic output for hci	aromatic output	or higher-level categories, other than the smells themselves. What does mint taste like? Well...mint. Higher level categories, like “floral,” merely indicate “this set of smells are found on flowers.” Even that category has holes; many people find the scent of daisies unpleasant and would not describe it as floral if they were smelling it without seeing the flower itself in front of them. It’s like trying to develop a system of color classifi: / 50 i n t e r a c t i o n s / j a n u a r y + f e b r u a r y 2 0 0 4 <	human–computer interaction;mint	Joseph Kaye	2004	Interactions	10.1145/962342.964333	multimedia;human–computer interaction;engineering	Mobile	-52.26021263230438	-45.55390872544141	26045
1b836c967ae839c79531f28df25339a7f68c0215	the chatbot strikes back	child speech recognition;social hri;conversational context	Categories and Subject DescriptorsCategories and Subject DescriptorsH.1.2 [Models and Principles]: User/Machine SystemsGeneral TermsExperimentation, Human Factors, Theory	human factors and ergonomics	James Kennedy;Joachim de Greeff;Robin Read;Paul Baxter;Tony Belpaeme	2014	2014 9th ACM/IEEE International Conference on Human-Robot Interaction (HRI)	10.1145/2559636.2559650	speech recognition	Robotics	-54.141399697777324	-46.30671734960639	26050
a0fe9c6b4dc7a33ff525d232ad1807bf6b6e80f9	a rapid prototyping tool for constructing web-based mmi applications		We have developed Interaction Builder (IB), a rapid prototyping tool for constructing web-based Multi-Modal Interaction (MMI) applications. The goal of IB is making it easy to develop MMI applications with speech recognition, life-like agent, speech synthesis, web browsing, etc. For this purpose, IB supports the following interface and functions: (1) GUI for implementing MMI systems without details of MMI and MMI description language, (2) functionalities for handling synchronized multimodal inputs/outputs, (3) a test run mode for run-time testing. The results of evaluation tests showed that the application development cycle using IB was significantly shortened in comparison with the time using a text editor both for MMI description language experts and for	graphical user interface;multimodal interaction;rapid prototyping;speech recognition;speech synthesis;text editor;web application	Kouichi Katsurada;Kunitoshi Sato;Hiroaki Adachi;Hirobumi Yamada;Tsuneo Nitta	2005			speech recognition;web application;computer engineering;computer science;rapid prototyping	HCI	-42.77987553649737	-29.400213448963637	26175
07baab8f722339962bc4e4283e72d648f8ba21bb	service design determinants for user value design: online store case study	service design determinants;consumer services;online store;value;prototyping;service concept design;user centred design	Understanding user value creation is an important part of service design; however, creating value for users is rarely an easy task. Going beyond likes and dislikes into the motivations behind the use requires specific approaches and methods. The present paper, based on an online food store case study, shows how value creation can be predicted and what kind of determinants affect service design. We used a questionnaire, rapid prototypes, and a co-design workshop to retrieve user data and to develop the concepts. The goal of the study was to form design drivers for a functional prototype that would be evaluated in a real world context. The main determinants affecting the service design in our case were Design base, Concept/Product, Domain/Context, Users, Company, and Resources. Our findings suggest that the user value elements can be grasped using cost-efficient methods.	cost efficiency;online food ordering;online shopping;prototype;software prototyping	Sampo Teräs;Mari Mäkelä	2012		10.1145/2414536.2414625	user interface design;user experience design;user-centered design;simulation;human–computer interaction;experience design;differentiated service;knowledge management;service design;prototype;design education	HCI	-60.91294522662866	-46.11021924998311	26214
4f4407b756a43eee763ea0c88369f3c76cf5a012	microcomputer graphics (panel)	particle systems;stochastic modeling;raster graphics;computer graphics;light scattering;clouds;ray tracing;simulation of natural phenomena;radiative transport	Ranging from home computers for entertainment and education to personal work-stations for design and presentation, microprocessor hardware spans the entire spectrum of graphic applications. Through live demonstrations of the latest and most popular hardware and software offerings, we hope to give you a feeling for the rapidly growing field of personal, “human scale” computing. The session will include a visual tour of the MacPaint package and the Quickdraw firmware on Apple's Macintosh personal computer, as well as brief demos of software for educational videogrames, low resolution typography and interactive, real-time animation.  Depending on audience questions, we might also take a look at issues and trends involved in mass marketing of software, personalization of technology and ownership of knowledge...and what the ramifications might be for the future of computer graphics and the human (-) computer dialogue.	computer graphics;educational entertainment;firmware;image resolution;macpaint;microcomputer;microprocessor;personal computer;personalization;quickdraw;real-time transcription	Howard Pearlmutter	1984		10.1145/800031.808595	ray tracing;computer vision;graphics pipeline;computing;simulation;raster graphics;computer science;operating system;particle system;computer graphics lighting;real-time computer graphics;multimedia;light scattering;graphics software;programming language;computer graphics;stochastic;3d computer graphics;computer graphics (images)	Graphics	-49.6426743125163	-29.377479399182967	26244
942fd1362b105ab5810c9e2d537c7aee2c65594d	user: a new framework for redoing	computer graphics;computational modeling	US&R (which stands for Undo, Skip, & Redo) is a new interactive approach to user recovery that offers significant advantages over current Undo/Redo packages. In the US&R package, a SKIP or REDO command may be ambiguous, in which case US&R enumerates the logical interpretations of the command and prompts the user both textually and graphically for the desired choice. US&R also allows new commands to be executed during the redo process. With US&R, novices can perform recoveries that might be difficult or impossible to do with other systems; experienced users can take even greater advantage of its functionality. US&R's data structure organizes the recovery information in a natural tree-like fashion that is easy to implement in a variety of interactive settings, including text editors, graphics layout systems, algorithm simulators, and program development systems.	algorithm;command & conquer:yuri's revenge;command-line interface;data structure;graphics;marc (archive);redo log;simulation;text editor;undo	Jeffrey Scott Vitter	1984		10.1145/800020.808262	computer science	HCI	-39.13985020874032	-30.620216327107	26249
7fb2a77ee3aac84161da139d041fbdf0b39fac54	is this person real? avatar stylization and its influence on human perception in a counseling training environment		This paper describes a pilot study planned by the Defense Equal Opportunity Employment Management Institute (DEOMI). By leveraging previous work in maturing a low-cost real-time puppeted character in a virtual environment, the team is seeking to explore the role stylization plays in how participants perceive emotions and connect with an avatar emotionally within a training atmosphere. The paper also describes future work in exploring how biases might be exposed when interacting with puppeted virtual characters.		Jennie Ablanedo;Elaine Fairchild;Tami Griffith;Christopher Rodeheffer	2018		10.1007/978-3-319-91581-4_20	perception;virtual machine;applied psychology;avatar;cultural bias;psychology;virtual training	AI	-54.080463271641065	-50.77046430793634	26291
3132ce162a6aede2bd656b1f818fea6ce3d5c96c	constructures: supporting human ingenuity in software	early stage creativity;multimedia;guidelines;software design	Existing media authoring software programs are generally regarded as offering little support for the earliest stages of the creative process, which have characteristics and needs that are distinct from the later stages. As a step toward addressing this, we present our constructural design philosophy: a set of principles that can be applied to software in any creative medium and under which we believe effective, early-stage creativity tools can be created. We then illustrate the application of these principles with Wheelsong, a complete, constructurally designed, early-stage music composition exploratorium and discuss a selection of musical sketches resulting from its use.	baudot code;experiment;subject matter expert turing test;web design	Jeff Smith;David Mould;Mark Daley	2009	Digital Creativity	10.1080/14626260902868079	simulation;human–computer interaction;computer science;artificial intelligence;software design;multimedia;management;world wide web	HCI	-61.861626704829604	-35.95505082360553	26328
4c83b4bb80ccdc74bf3a05bca1c76cc990586ff4	poster: mvce - a design pattern to guide the development of next generation user interfaces	next generation user interfaces;model view controller;sensors;user interface;virtual reality;visualization;prototyping;model view controller mvc;three dimensional displays;solid modeling;design pattern;next generation;mathematical model;model view controller design pattern next generation user interfaces sensors;atmospheric modeling;model view controller mvc mixed reality prototyping;mixed reality;user interfaces;structural design;interaction technique;user interfaces virtual reality process design virtual prototyping augmented reality usability hardware multimedia systems information systems graphical user interfaces	The development of next generation user interfaces that employ novel sensors and additional output modalities has high potential to improve the usability of applications used in non-desktop environments. The design of such interfaces requires an exploratory design approach to handle the interaction of newly developed interaction techniques with complex hardware. As a first step towards a structured design process we extended the MVC design pattern by an additional dimension “Environment” to capture elements and constraint from the real world.	desktop computer;interaction technique;model–view–controller;next-generation network;post-wimp;sensor;software design pattern;structured analysis;usability;user interface	Jörg Stöcklein;Christian Geiger;Volker Paelke;Patrick Pogscheba	2009	2009 IEEE Symposium on 3D User Interfaces	10.1109/3DUI.2009.4811232	user interface design;user experience design;simulation;human–computer interaction;computer science;multimedia	HCI	-43.17601950534056	-35.689030276135455	26430
08d55343c38ba0a4eed8b9913160e606a5eef1c5	sketchy rendering for information visualization	sketch;non photorealistic rendering;mathematics computing;uncertainty;za4050 electronic information resources;computational geometry;shape analysis;diagrams;trees mathematics;visualization annotation sketchy rendering sketchy style information visualization data graphics processing graphics environment line rendering polygon rendering ellipse rendering graphical features bar chart line chart treemap node link diagram sketchiness programming modification design effort statistical graphics spatial imprecision aesthetic quality narrative quality user perception stimulus response test shape rendering visualization design;npr;data visualization shape analysis rendering computer graphics;data visualisation;visualization;statistical analysis;trees mathematics charts computational geometry data visualisation diagrams mathematics computing rendering computer graphics statistical analysis;data visualization;perception;rendering computer graphics;hand drawn;visualization npr non photorealistic rendering sketch hand drawn uncertainty;charts;non photorealistic;rendering	We present and evaluate a framework for constructing sketchy style information visualizations that mimic data graphics drawn by hand. We provide an alternative renderer for the Processing graphics environment that redefines core drawing primitives including line, polygon and ellipse rendering. These primitives allow higher-level graphical features such as bar charts, line charts, treemaps and node-link diagrams to be drawn in a sketchy style with a specified degree of sketchiness. The framework is designed to be easily integrated into existing visualization implementations with minimal programming modification or design effort. We show examples of use for statistical graphics, conveying spatial imprecision and for enhancing aesthetic and narrative qualities of visualization. We evaluate user perception of sketchiness of areal features through a series of stimulus-response tests in order to assess users' ability to place sketchiness on a ratio scale, and to estimate area. Results suggest relative area judgment is compromised by sketchy rendering and that its influence is dependent on the shape being rendered. They show that degree of sketchiness may be judged on an ordinal scale but that its judgement varies strongly between individuals. We evaluate higher-level impacts of sketchiness through user testing of scenarios that encourage user engagement with data visualization and willingness to critique visualization design. Results suggest that where a visualization is clearly sketchy, engagement may be increased and that attitudes to participating in visualization annotation are more positive. The results of our work have implications for effective information visualization design that go beyond the traditional role of sketching as a tool for prototyping or its use for an indication of general uncertainty.	annotation;attitude;chart;cognitive dimensions of notations;data visualization;definition;depth perception;diagram;experiment;framing (social sciences);graphical user interface;high- and low-level;high-level programming language;imagery;information visualization;interactive design;judgment;level of measurement;node - plant part;rendering (computer graphics);statistical graphics;treemapping;uncertain data;usability testing;zelda ii: the adventure of link;endodeoxyribonuclease rsai	Jo Wood;Petra Isenberg;Tobias Isenberg;Jason Dykes;Nadia Boukhelifa;Aidan Slingsby	2012	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2012.262	computer vision;visualization;uncertainty;rendering;computer science;diagram;chart;shape analysis;mathematics;multimedia;perception;data visualization;statistics;computer graphics (images)	Visualization	-36.47495677741603	-36.0443534578188	26440
62c7ed314eeee1605280d81d77d9f169adbcdc50	elastic scroll for multi-focus interactions	content distortion;map navigation	This paper proposes a novel and efficient multi-focus scroll interface that consists of a two-step operation using a con-tents distortion technique. The displayed content can be handled just like an elastic material that can be shrunk and stretched by a user's fingers. In the first operation, the us-er's dragging temporarily shows the results of the viewport transition of the scroll by elastically distorting the content. This operation allows the user to see both the newly obtained and the original focus on the viewport. Then, three types of simple gestures can be used to perform the second operation such as scrolling, restoring and zooming out to get the demanded focus (or foci).	distortion;drag and drop;interaction;naruto shippuden: clash of ninja revolution 3;scrolling;viewport	Kazuki Takashima;Kazuyuki Fujita;Yuichi Itoh;Yoshifumi Kitamura	2012		10.1145/2380296.2380307	simulation;computer science;multimedia	HCI	-43.01004934655164	-40.170207506756846	26473
5328441e7444fd572e0e31665a0a16f6d054c45c	comparing unobtrusive gaze guiding stimuli in head-mounted displays		In this paper we investigate the efficiency of five different image-space post-processing gaze guidance techniques adapted for immersive environments, probing our peripheral vision's sensitivity to different stimuli embedded in complex, real-world panorama still images. We conducted an extensive perceptual study for the commercially available HTC Vive Head-Mounted Display that is equipped with a high-quality eye tracking system to monitor the subjects' gaze direction and saccades in real-time. Despite evaluation reveals no outstanding winner, the local magnification effect by Dorr et al. performed the best with up to 30 % target-directed saccades within the first second.		Steve Grogorick;Georgia Albuquerque;Marcus A. Magnor	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451784	computer vision;magnification;gaze;artificial intelligence;visualization;immersion (virtual reality);eye tracking;perception;computer science;virtual reality;peripheral vision	Visualization	-44.64007945121894	-47.39510193755485	26491
596af0ac239cef35a1eea4943d9431ec3aa8a1db	data acquisition glove for hand movement impairment rehabilitation		The present paper describes a data acquisition wearable device for hand rehabilitation. The main goal of this glove is to be used by patients with hand movement impairment. It has position sensors to measure the bending of synovial joints and sensors to measure the fingertip contact pressure. There is a coin motor and a LED placed on each finger to produce a vibratory and visual stimulus. The glove also tracks the hand rotation and translation using a MPU (Motion Processing Unit) that contains an accelerometer and a gyroscope. A graphical application for an HMI module was developed in order to create rehabilitation game like exercises where sensor data can be logged for further analysis by a therapist. The wearable device electronic hardware comprises a Glove module and an HMI module that communicate through SPI protocol (Serial Peripheral Interface). The wearable device supports USB connection to send data to a computer or to be used as a peripheral device in virtual or augmented reality applications.	data acquisition	Rafael Tavares;Paulo Abreu;Manuel Rodrigues Quintas	2016	iJOE		embedded system;simulation;computer hardware;engineering;wired glove	HCI	-41.8837975301794	-43.2949786766134	26556
8ba7d536acf090b27de07d1ec9432e05f713c8f3	paperweb: paper-triggered web interactions	document image processing;mobile cameras;web tasks	While mobile phones have penetrated deep into tier 2 and 3 cities in India and similar emerging economies, adoption of mobile web content and web services is likely to require the creation of large numbers of relevant applications and services with usable interfaces and interaction paradigms. This paper describes PaperWeb, our effort to enable mobile phone users to use the web for day to day transactions such as paying bills, buying tickets, or fixing appointments, using familiar objects such as paper artifacts. We discuss (i) the creation of useful PaperWeb interactions -- without programming -- by moderately tech-savvy users, and (ii) the use of these interactions by tech-naive users, and briefly describe the underlying technology. We conclude the paper with a discussion of current status and next steps.	interaction;mobile phone;tier 2 network;web content;web service	Sriganesh Madhvanath;Geetha Manjunath;Suryaprakash Kompalli;Serene Banerjee;Ramachandrula Sitaram;Srinivasu Godavari	2013		10.1145/2442882.2442931	web service;web development;web modeling;mobile search;mobile web;engineering;web navigation;multimedia;advertising;world wide web	HCI	-51.702340141833936	-40.86197028618437	26567
6d5b9d1cbd8894393a6aa5c419e02c7164d52e30	paisa stick: tangible currency interface for cashless transactions	digital literacy;emergent users;tangible currency;india	Economies around the world are moving towards cashless transactions. The elderly and digitally illiterate seem to have been marginalized by this move. In this paper, we discuss the findings of our research on the problems faced by these users in performing digital transactions. Our findings have helped us in designing a tangible hardware interface that enables these users to perform cashless transactions easily. Our interface is inspired from the existing model of currency transaction from a physical wallet. We have evaluated our design with our target audience through a first-time usability test. In this paper, we present only the user's side of design considerations and evaluation results. These, we believe, may contribute to designing cashless transaction interfaces in an emergent setting.	contextual inquiry;emergence;hardware interface design;prototype;simulation;usability testing	Rohit Gupta;Udayan Vidyanta;Silpa Murali	2017		10.1145/3027063.3049268	multimedia	HCI	-57.64136682861207	-37.13036361107388	26569
2a11864ba548c00bc90f440bd8d683274a843c08	a location-sensitive visual interface on the palm: interacting with common objects in an augmented space	ubiquitous;interface on body;interaction;system;location awareness;augmented reality	We have created a visual interface using the human palm that is location sensitive and always available. To accomplish this, we constructed an augmented space in an actual workspace by installing several depth cameras. To manage and connect the multiple depth cameras, we constructed a distributed system based on scalable client––server architecture. By merging depth images from different cameras, the distributed system can track the locations of users within their area of coverage. The system also has a convenient feature that allows users to collect the locations of objects while visualizing the objects via images from the depth cameras. Consequently, the locations of both users and objects are available to the system, thus providing a location-based context for determining which user is close to which object. As a result, the visual interface on the palm becomes location sensitive, which could lead to various applications in daily life. In this paper, we describe the implementation of the aforementioned system and demonstrate its potential applicability.	digital camera;distributed computing;interaction;location-based service;personalization;scalability;workspace	Seokhwan Kim;Shin Takahashi;Jiro Tanaka	2014	Personal and Ubiquitous Computing	10.1007/s00779-014-0769-0	computer vision;augmented reality;interaction;simulation;human–computer interaction;computer science;system;multimedia;ubiquitous computing	HCI	-46.91241488200547	-40.50227606790352	26574
1f2b78270dab6710fd19d47fd3c1f76e69c0e310	a framework proposal of ux evaluation of the contents coherency on multi-screens	coherence usability market research concrete smart phones guidelines standards;multiscreen content ux evaluation content coherency digitalized content;user interfaces human factors;coherency;coherency multi screens ux evaluation;ux evaluation;multi screens	As technical development has made it possible to access network at anywhere and anytime, various contents are being digitalized these days. Digitalized contents may be used on various devices without undergoing additional conversion. In the past, there were killer contents that were specialized for each device. But unlike the past, it is now possible to access to network anywhere and sync between devices such that users can enjoy the same content on a range of devices by pushing just a few buttons. Within this network environment, the multi-screen content UX has become a very important issue. As same content is displayed on various screens, a consistent UX must be used to minimize user confusion and maintain reliable experience. The purpose of this study is examine elements which enable users' expectation of experience about the contents to be maintained equally, regardless of any devices, for example, screen size or operation method of the interface, and research them specifically. PC, Pad, Smart Phone, and TV are representative examples of multi screens. In addition, smart watches, watches functionally connected with smart phones, are spreading very rapidly these days. Size, operating environment and method of each screen are different, but we use the same contents (for example, YouTube) at each screen. Although many researchers or contents producers think they are creating contents considering the consistency of users' experience, it is hard to find written documentation of the guidelines so far. Starting from the homogeneity of the surface design, users become to demand the consistency of the structure as they operate the screens. When all these things are in balance, users can experience the consistency of the contents used. In this study, guidelines which will enable users' consistency to be kept steadily will be implemented, and then this will be applied to the existent contents for the evaluation. Outcome of the study can be utilized as the framework which will evaluate experience consistency of UX on multi screens.	a/ux	Wangmi Seok	2015		10.1109/IIAI-AAI.2015.231	simulation;engineering;multimedia;world wide web	NLP	-51.65131876035204	-40.76486135361164	26576
c80083e17c6d11f09c6509984ad5ce27da085cd8	ar dental surgical simulator using haptic feedback		We describe about our dental surgical simulator which enable users to simulate dental surgical operation. Our simulator which enables the user to learn dental surgical methods through actual hand and body postures. The proposed system uses a display showing a virtual tooth model and real teeth and gums that are positioned close to the hands of the user, which allows the user to directly manipulate objects with haptic feedback. As a preliminary evaluation, in display system, we measured the deviation between real object image and virtual object image at user’s view positions. And we confirmed the capability and the limitation of our system.	haptic technology	Katsuhiko Onishi;Kiminori Mizushino;Hiroki Ikemoto;Hiroshi Noborio	2013		10.1007/978-3-642-39476-8_42	virtual image;simulation;haptic technology;augmented reality;computer science	Robotics	-42.64968928444392	-46.31713392718857	26590
d4cddf99f465a9fbc1aa4e6900fb2b3f1b3af7c2	handier use of scilab to draw fine latex figures: usage of ketpic version for scilab	software;ketpic version;symbol manipulation;mathematica;symbolic computation;fine latex figures;electronic mail;mathematics computing;computer graphics;shareware;packaging;graphics in latex documents computer algebra system ketpic scilab;computer algebra system;communication industry;computer architecture;communication effectiveness;computer science education;graphical file;medical services;internet;algebra;ketpic;medical service;scilab;maple;educational technology;graphics in latex documents;symbol manipulation computer graphics internet mathematics computing process algebra;content addressable storage;process algebra;command and control systems;programming;scientific research;shareware scilab fine latex figures ketpic version scientific research medical service computer algebra system symbolic computation graphical file web tech based communication mathematica maple;content addressable storage educational technology computer science education communication effectiveness communication industry computer architecture medical services algebra computer graphics packaging;graphics;web tech based communication	"""In today's technological era, technologies for drawing fine figures have become necessary for effective communication in areas such as scientific research, industry, architecture, medical service, and education. In the areas of scientific research and education, computer algebra system (CAS) has become the preferred tool because of its superb drawing utilities and symbolic computation capabilities. However, embedding the graphics generated with CAS into preferred documents such as LaTeX is limited, cumbersome and inefficient. For instance, a large sized graphical file becomes an issue for fast web-tech based communication. To remedy this issue, we have developed a macro package of CAS, named KETpic, as a convenient tool for supplying high quality graphics (produced with the aid of CAS) embedded in high-quality scientific documents (edited by LaTeX). Although its versions for Mathematica and Maple have been fully developed, we recently migrated it to a popular shareware Scilab so that more people can use it. In a sense, the extent of the facilities which a CAS provides and the expense to purchase it present a """"tradeoff'"""" relation. Consequently, some extra contrivance was necessary in the migration described above. This paper illustrates that the facilities of KETpic version for Scilab become almost equal to those of its Mathematica and Maple counterparts."""	approximation;computer algebra system;display resolution;embedded system;graphical user interface;graphics;latex;maple;scilab;symbolic computation;wolfram mathematica	Hiroaki Koshikawa;Masataka Kaneko;Satoshi Yamashita;Kiyoshi Kitahara;Setsuo Takato	2010	2010 International Conference on Computational Science and Its Applications	10.1109/ICCSA.2010.33	programming;packaging and labeling;educational technology;process calculus;symbolic computation;the internet;scientific method;computer science;graphics;artificial intelligence;theoretical computer science;operating system;database;programming language;computer graphics;algorithm;computer graphics (images)	HPC	-50.41813357165713	-24.188134998286927	26627
1ecf3e005c92b56852528f5114d2e9d2a48cba88	exploring the creation of useful interfaces for music therapists		Music therapy is utilized worldwide to connect communities, strengthen mental and physiological wellbeing, and provide new means of communication for individuals with phonological, social, language, and other communication disorders. The incorporation of technology into music therapy has many potential benefits. Existing research has been done in creating user-friendly devices for music therapy clients, but these technologies have not been utilized due to complications in use by the music therapists themselves. This paper reports the iterative prototype design of a compact and intuitive device designed in close collaboration with music therapists across the globe to promote the usefulness and usability of prototypes. The device features interchangeable interfaces for work with diverse populations. It is portable and hand-held. A device which incorporates these features does not yet exist. The outlined design specifications for this device were found using human centered design techniques and may be of significant use in designing other technologies in this field. Specifications were created throughout two design iterations and evaluations of the device. In an evaluation of the second iteration of this device it was found that 5/8 therapists wanted to incorporate it into their practices.		Leya Breanna Baltaxe-Admony;Tom Hope;Kentaro Watanabe;Mircea Teodorescu;Sri Hastuti Kurniawan;Takuichi Nishimura	2018		10.1145/3243274.3243307	globe;human–computer interaction;music therapy;usability;user-centered design;computer science	HCI	-56.32590343912432	-44.79223986395604	26632
01d54a131d15a450161357ed3431c8728a2d65ef	the virtual director: a correlation-based online viewing of human motion	i 3 7 computer graphics three dimensional graphics and realism camera control;human motion	Automatic camera control for scenes depicting human motion is an imperative topic in motion capture base animation, computer games, and other animation based fields. This challenging control problem is complex and combines both geometric constraints, visibility requirements, and aesthetic elements. Therefore, existing optimizationbased approaches for human action overview are often too demanding for online computation. In this paper, we introduce an effective automatic camera control which is extremely efficient and allows online performance. Rather than optimizing a complex quality measurement, at each time it selects one active camera from a multitude of cameras that render the dynamic scene. The selection is based on the correlation between each view stream and the human motion in the scene. Two factors allow for rapid selection among tens of candidate views in real-time, even for complex multi-character scenes: the efficient rendering of the multitude of view streams, and optimized calculations of the correlations using modified CCA. In addition to the method’s simplicity and speed, it exhibits good agreement with both cinematic idioms and previous human motion camera control work. Our evaluations show that the method is able to cope with the challenges put forth by severe occlusions, multiple characters and complex scenes.	autonomous robot;computation;control system;imperative programming;kinesiology;motion capture;pc game;programming idiom;real-time clock;real-time locating system;requirement;video blog;video clip;virtual camera system;virtual reality	Jackie Assa;Lior Wolf;Daniel Cohen-Or	2010	Comput. Graph. Forum	10.1111/j.1467-8659.2009.01629.x	computer vision;camera auto-calibration;match moving;simulation;computer science;artificial intelligence;computer graphics (images)	Graphics	-38.398606440571704	-36.71377610598734	26672
f70aad12f587032e8555e35a4f3265ee002755fe	computer graphics and games, agent based modeling in	agent based model;computer graphic		agent-based model;computer graphics	Brian Mac Namee	2009		10.1007/978-0-387-30440-3_90	human–computer interaction;computer science;multimedia;computer graphics (images)	AI	-48.77507888269299	-32.74434878792281	26691
b674ab27d4f1175219853614522f340f2cb29264	five design challenges for human computation	games with a purpose;design space;design guideline;design framework;human computation;citizen science;crowdsourcing	Human computation systems, which draw upon human competencies in order to solve hard computational problems, represent a growing interest within HCI. Despite the numerous technical demonstrations of human computation systems, however, there are few design guidelines or frameworks for researchers or practitioners to draw upon when constructing such a system. Based upon findings from our own human computation system, and drawing upon those published within HCI, and from other scientific and engineering literatures, as well as systems deployed commercially, we offer a framework of five challenging issues of relevance to designers of systems with human computation elements: designing the motivation of participants in the human computation system and sustaining their engagement; orienting participants, framing and orienting participants; using situatedness as a driver for content generation; considering the organisation of human and machine roles in human computation systems; and reconsidering the way in which computational analogies are applied to the design space of human computation.	computational problem;framing (world wide web);human-based computation;human–computer interaction;relevance;systems design	Stuart Reeves;Scott Sherwood	2010		10.1145/1868914.1868959	simulation;citizen science;human–computer interaction;computer science;management science;management;crowdsourcing	HCI	-61.41791761189708	-35.06830009235837	26693
7db6c165c292dac6ee3f32274165e3430bac900f	"""""""some assembly required"""": starting and growing a game lab [abstracts]"""		This panel will present case studies of four different game laboratories, exploring the uses of the lab as a research venue and as part of a game or digital media curriculum. The examples will focus on game labs in Humanities departments, where the use of laboratories as a resource is less common. OVERVIEW A game lab is an invaluable resource for games research, which can take many forms depending on its purpose. There are many functions it can fulfill: a venue for collaboration between disciplines, a resource to study games and players, as well as an environment to develop experimental games and foster innovation in game design. Setting up a game lab also demands a considerable effort. Depending on the intended purpose of the lab, there will be different technical needs, human resources and funds required. Integrating the game lab into the context of a department and within a curriculum is an additional challenge in justifying the creation of a game lab, particularly in the Humanities, where the use of laboratories as a resource is less frequent. This panel will share the stories of four labs, covering what has worked best for them and what has not worked. Each panelist will provide an overview of what they needed to put together their labs, how they have grown an identity for them, and how they have integrated them in their different institutional contexts. The goal is to discuss the value of a game lab, and to encourage the participation of attendants who have a game lab as part of their resources, or may be thinking of setting one up. The Experimental Game Lab (EGL) extends the mission of Georgia Tech's Digital Media Graduate Program by blending theory and practice to both investigate and extend the expressive capabilities of games, to foster critical analysis, and to better understand and influence the role of video games in culture. Both a research community and a facility, the EGL creates a locus for diverse disciplines to meet, collaborate and exchange ideas, including: critical theory and media studies; psychology and cognitive science, anthropology and sociology; human computer interaction; computer science and engineering; art, media production, design and architecture. Topics explored include persuasive and activist games, game and virtual space, augmented reality, tangible media, pervasive games, cognition and creativity, AI in narrative and games, the cultures of virtual worlds, platform studies, procedural aesthetics, and interactive narrative. The …	alpha compositing;augmented reality;cognition;cognitive science;computer science;digital media;egl (opengl);human computer;human–computer interaction;locus;pervasive informatics;tangible user interface;venue (sound system);video game design;virtual reality;virtual world	Mary Flanagan;Celia Pearce	2009				HCI	-61.07216071840767	-33.64399180461038	26778
182910e9b04ee16ec542f38dbb0db4cbdfc3ec45	how a freeform spatial interface supports simple problem solving tasks	development strategy;user study;interaction style;spreadsheets;freeform interaction;spatial interfaces;problem solving	We developed DataBoard, a freeform spatial interface, to support users in simple problem solving tasks. To develop a deeper understanding of the role of space and the tradeoffs between freeform and structured interaction styles in problem solving tasks, we conducted a controlled user study comparing the DataBoard with a spreadsheet and analyzed video data in detail. Beyond improvements in task performance and memory recall, our observations reveal that freeform interfaces can support users in a variety of ways: representing problems flexibly, developing strategies, executing strategies incrementally, tracking problem state easily, reducing mental computation, and verifying solutions perceptually. The spreadsheet also had advantages, and we discuss the tradeoffs.	computation;problem solving;spatial file manager;spatial navigation;spreadsheet;usability testing;verification and validation	Eser Kandogan;Juho Kim;Thomas P. Moran;Pablo Pedemonte	2011		10.1145/1978942.1979079	simulation;human–computer interaction;computer science	HCI	-47.22939773971675	-48.14596094281308	26808
69c8273b952322a1f41ed4d4333095ab79cc37b2	design of a mobile app for interspeech conferences: towards an open tool for the spoken language community		In this paper, we describe the development of a mobile phone app for supporting participants and organizers of Interspeech conferences. Based on a survey amongst future organizers and attendees, we identified the most relevant functionalities and implemented an initial set of them on two popular platforms, iOS and Android. The app is meant as an open tool to be handled under the auspices of ISCA, and can be extended with speech and language functionalities in the future. This way, we hope to turn it into a community platform which can be used for experimenting with new speech technologies on site. A first version of this app will be presented at Interspeech 2013.	android;experiment;international symposium on computer architecture;mobile app;mobile phone;ios	Robert Schleicher;Tilo Westermann;Jinjin Li;Moritz Lawitschka;Benjamin Mateev;Ralf Reichmuth;Sebastian Möller	2013			spoken language;multimedia;natural language processing;artificial intelligence;computer science	NLP	-51.114300806882085	-36.07441964791655	26809
e338f5002e4aa1bcd1be6c9e72664eadd7f6b89a	preferred mental models for direct manipulation and command-based interfaces	direct manipulation;mental model	"""Abstract   One of the main methods used to compare direct manipulation and command-based interfaces is to examine user preferences. User preferences are extended in our research to examine which mental model, direct manipulation or command-based, subjects prefer to use in transfer situations. One group of subjects was provided with both a command-line and direct manipulation interface during a training phase. After training, several transfer experiments were conducted to determine the preferred mental model. The developed mental models were investigated by """"running"""" the models and extending the models to new situations. They were evaluated by determining the operators specified by the subjects. By comparing the class of operators to subjects trained on only one of the interfaces, direct manipulation or command-based, the preferred model could be determined. The preferred mental model for """"running"""" the model was the direct manipulation. For extensions that had a concrete or graphical basis in the interface, the direct manipulation was preferred. For extensions that were abstract, one model was not preferred over the other. Some ability to use multiple mental models, based upon the task, was also observed."""	direct manipulation interface;mental model	Ray E. Eberts;Kalappa P. Bittianda	1993	International Journal of Man-Machine Studies	10.1006/imms.1993.1036	simulation;human–computer interaction;computer science	Visualization	-47.41965262752855	-48.07934294728207	26857
44a7b6860a3d07ef4d2f534f24eabd3a1bcc9dd8	online metrics for web search relevance	evaluation	Information Retrieval has a long tradition of being metrics driven. Ranking algorithms are assessed with respect to some utility measure that reflects the likelihood of satisfying an information need. Traditionally these metrics are based on offline judgments. This is very flexible since judgments can be made for any desired output. However, judgments are no better than judgment guidelines and are at some distance from the actual user experience. Modern Web Search engines enjoy an additional resource; existing web search traffic and its attendant wealth of user engagement data. Primarily this signal consists of logged queries and user actions, including clicks and reformulations. I will discuss how this data can be used to derive Web Search quality metrics that have very different properties than traditional offline metrics.	algorithm;information needs;information retrieval;online and offline;relevance;user experience;web search engine	Jan Pedersen	2013		10.1145/2513150.2513165	computer science;data mining;world wide web;information retrieval	Web+IR	-34.413615132121706	-51.18275467774727	26867
a5d480b781fc5729621bbf2b1be5c40e22c43eea	configurable software framework for 2d/3d video see-through displays in medical applications		Augmented Reality (AR) has already proven its worth in various applications in the medical domain. However, most of the solutions proposed were bound to specific hardware or software configurations, and/or their application was limited to specific cases, thus lacking in flexibility. In this paper, we present a software framework suitable for AR video see-through systems conceived for medical applications: our solution allows merging of real world images grabbed by one or more external cameras with computer-generated sceneries coregistered to the acquired images. The software framework is highly configurable and extensible thanks to the employment of two text configuration files that make it suitable for many typologies of potential applications. The proposed solution can be easily adapted to functioning with different tracking and AR visualization modalities. The versatility of the software for video see-through AR applications was already tested on various medical applications, in conjunction with head-mounted displays or with external spatial displays.	software framework	Fabrizio Cutolo;Mentore Siesto;Stefano Mascioli;Cinzia Freschi;Mauro Ferrari;Vincenzo Ferrari	2016		10.1007/978-3-319-40651-0_3	embedded system;computer hardware;computer graphics (images)	HCI	-41.18296155325215	-33.656354435188184	26936
c547dd81e357139458e6562b986c86ce92c713fd	delivering haptic sensations in mobile marketing	mobile and wearable technology;multisensory marketing;haptic i o	"""The sense of touch has a significant role in establishing effective communication with customers and improving brand awareness and product experience. With mobile and wearable devices at the core of communication today, mobile marketing is crucial for advertisers to appeal to a larger audience and new haptic technologies should be developed to deliver more realistic and meaningful haptic messages to these devices. We introduce RingU and Kissenger, two haptic devices for mobile devices that enhance the vibration haptic feedback using force-variable vibrotactile and linear force actuation, allowing users to hug and kiss each other over the Internet in real-time. Businesses can use these devices to communicate with their customers using haptic stimulations to enhance customer experience and advertise their services. Artist agencies or game producers could also increase the popularity of their idols and virtual characters by allowing fans from worldwide to come in """"close contact"""" with them."""	haptic technology;mobile device;real-time clock;wearable technology	Gilang Andi Pradana;Emma Yann Zhang;Adrian David Cheok;Yukihiro Morisawa	2015		10.1145/2832932.2856223	simulation;engineering;multimedia;advertising	HCI	-53.87930273130149	-39.946127567109535	27000
0ad1652703ff91b9cdc84af00026bb0f7061c186	method to build good usability: task analysis and user interface design using operation flowcharts	task analysis;user interface design	The method of user interface design introduced here is a design technique actively utilizing operation flowcharts. This technique attempts to determine usability logically and visually by describing all the elements forming usability on operation flowcharts. By taking examples from 15 development project cases performed to date, the concept of flowcharts, the methods of describing them and the effects of introduction of the technique will be reported.	flowchart;task analysis;usability;user interface design	Haruhiko Urokohara	1999			user interface design;user;10-foot user interface;user experience design;usability;shell;human–computer interaction;computer science;task analysis;database;natural user interface;programming language;user interface;heuristic evaluation;multiple document interface;usability inspection	HCI	-41.701620452479176	-30.165350880632616	27003
877ef6003939d3cbf67620ed0b0f8d5ffb551d57	reconstructing cultural heritage objects from storytelling		Once upon a time in a small city of Sjenica, Serbia, there was a fortress. The Ottoman Sultan Murat IV has built it to protect and control this important region. Today at that location there is an elementary school. There is no trace of the fortress neither at the place nor in the memory of people. But, an old man still remembers it from his childhood. In this paper we describe how we recreated the cultural heritage object of Sultan Murat's fortress in Sjenica from storytelling. We present the created interactive multimedia application and the impressions of Sjenica citizens after a part of their history was returned to their collective memory.	fortress;immersion (virtual reality);positive feedback;virtual reality	Selma Rizvic;Izabela Skalonjic	2015	2015 Digital Heritage	10.1109/DigitalHeritage.2015.7419501	humanities;multimedia;anthropology	HCI	-53.4436909252493	-28.72894659565847	27030
cb9cab670f7a549a038442f288843465cb57a731	relationship of blink, affect, and usability of graph reading tasks	affect;response time;attention;accuracy;attentional blink;visual search;graph;time series data;line graph;blink	Dynamic graphs that represent a great deal of time-series data have become increasingly common these days. Although previous research revealed that blink facilitates visual search tasks by attracting human attention, blink features have not been clearly evident in a dynamic graph reading context. This study examines the effects of blink on the user's affective experience and usability of using a blinking line graph. Additionally, this study describes the empirical experiment setup for investigating the characteristics of task types as a moderator to the relationship between blink and the user's experience. This research aims (1) to theoretically contribute graph comprehension domain by investigating the effects of blink on a graph comprehension process, (2) to design a quantitative experiment and to propose possible hypotheses, and (3) to understand the influence of task types on reading of a blinking graph.	blink;google moderator;line graph;time series;usability	Gia Kim;Siu Man Lui	2009		10.1145/1655925.1655935	attention;visual search;computer science;blink;time series;accuracy and precision;attentional blink;graph;response time;line graph;affect	HCI	-55.268245304264944	-48.896475606942936	27048
dd4c759cb5e537761af5768750ddb2bb3a913884	sentient sketchbook: computer-aided game level authoring		This paper introduces the Sentient Sketchbook, a tool which supports a designer in the creation of game levels. Using map sketches to alleviate designer effort, the tool automates playability checks and evaluations and visualizes significant gameplay properties. This paper also introduces constrained novelty search via a two-population paradigm for generating, in real-time, alternatives to the author’s design and evaluates its potential against current approaches. The paper concludes with a small-scale user study in which industry experts interact with the Sentient Sketchbook to design game levels. Results demonstrate the tool’s potential and provide directions for its improvement.	programming paradigm;real-time transcription;sentience;usability testing	Antonios Liapis;Georgios N. Yannakakis;Julian Togelius	2013			simulation;multimedia;computer-aided;novelty;human–computer interaction;computer science	HCI	-51.730376071203885	-37.76900999323877	27054
91f4dedb70f74044db80a9d6cf453a7b6b35df42	ultrafast facial tracker using generic cameras with applications in intelligent lifestyle		The core of Human-Computer Interaction (HCI) is to analyze and understand the user’s intension, which can be mostly manifested from the facial movement and expression of the user. Hence, the stage facial detection and tracking is extremely important in an user-friendly interface between human and computer. In ULSee, we developed an ultrafast markerless facial tracking system which is robust to variation in environmental lighting, pose and occlusion. It can be run at a speed of 10 ms/frame on an iPhone 6S system. With such accuracy and speed, it can be used to support many intelligent HCI applications. In this work, we envision an intelligent lifestyle in the future that can be built upon the basis of the ULSee’s ultrafast markerless facial tracker, ranging from virtual reality, augmented reality, real-time facial recognition and driver drowsiness detection. We believe, that through the joint force between ULSee’s world-class tracker and our clients, more user-awareness HCI application will be invented and a new lifestyle will arise.	facial recognition system	Yung-Hui Li;Yuan-Ting Hu;Jethro Shen;Mihai Bogdan Preda;Andrei Drexler;Carmen Sosoiu;Dragos Florin Stanculescu;Paul Liu;Joe Ye	2016		10.1007/978-3-319-39907-2_23	humanities;visual arts;social psychology	Vision	-40.639861523869506	-43.175756009679745	27094
43256b3e5f23f9a85914f10193727617a5435126	design principles for resource management systems for intelligent spaces	design principle;resource manager;journal article;smart spaces;ubiquitous computing;resource management system;smart environment	The idea of ubiquitous computing and smart environments is no longer a dream and has long become a serious area of research and soon this technology will start entering our every day lives. There are two major obstacles that prevent this technology from spreading. First, different smart spaces are equipped with very different kinds of devices (e.g. a projector vs. a computer monitor, vs. a TV set). Second, multiple applications running in a space at the same time inevitably contend for those devices and other scarce resources. The underlying software in a smart space needs to provide tools for self-adaptivity in that it shields the rest of the software from the physical constraints of the space, and that it dynamically adjusts the allocation of scarce resources as the number and priorities of active tasks change. We argue that a resource manager can provide the necessary functionality. This paper presents a set of guiding principles for building high-level resource management tools for smart spaces. We present conclusions we arrived at after two years of exploring the topic in the Intelligent Room Project at the MIT AI Lab. The paper is based on a number of implemented and tested tools.	computer monitor;high- and low-level;mit computer science and artificial intelligence laboratory;management system;smart tv;smart environment;spaces;television set;ubiquitous computing;video projector	Krzysztof Z. Gajos;Luke Weisman;Howard E. Shrobe	2001		10.1007/3-540-36554-0_15	simulation;engineering;knowledge management;operations management;smart environment	HCI	-60.28822309492384	-27.083634754216664	27101
79732cd386226f44e790c653fbecc22cc4dbc3c2	ravencalendar: a multimodal dialog system for managing a personal calendar	dialog application;handheld device;dialog systems research;convenient use;personal calendar;web-based calendar application;dialog interface;multimodal dialog system	Dialog applications for managing calendars have been developed for every generation of dialog systems research (Heidorn, 1978; Yankelovich, 1994; Constantinides and others, 1998; Horvitz and Paek, 2000; Vo and Wood, 1996; Huang and others, 2001). Today, Web-based calendar applications are widely used. A spoken dialog interface to a Web-based calendar application permits convenient use of the system on a handheld device or over the telephone. In this demo, we present RavenCalendar, a multimodal dialog system built around the Google Calendar and Google Maps Web applications. RavenCalendar allows the user to create, modify and remove calendar events, query for events, and hear descriptions of events. In our demonstration we will focus on two aspects of RavenCalendar: its flexible approach to language understanding and dialog management, and its multimodal interface.	calendaring software;dialog manager;dialog system;google calendar;mobile device;multimodal interaction;natural language understanding;systems theory;web application	Svetlana Stenchikova;Basia Mucha;Sarah Hoffman;Amanda Stent	2007			simulation;speech recognition;dialog system;world wide web	NLP	-43.79342544323532	-29.53608275807668	27119
246d1b8d26e8111ea9be02fa9a687854d391e9e5	interactive sculpting with implicit surfaces	implicit surface;piping;routing;force feedback;transport elements;scalar field;multi criteria or multi objective optimization;tubing;design;interaction model;constraintbased	Providing the user with an intuitive sculpting system similar to real clay is one of the most challenging long term goals in interactive modeling. The user should ideally be able to deform, add and remove material, with no restriction on the geometry and topological genius of the solid being edited. Implicit surfaces, defined as iso-surfaces of scalar fields, are a very attractive model in such situations. This talk reviews two alternative implicit representations, the constructive approach versus sampled fields, and discusses their convenience for modeling virtual clay. An implicit sculpting system which incorporates force feedback and relies on multiresolution to accelerate editing is presented.	haptic technology;implicit surface;interactivity;multiresolution analysis	Marie-Paule Cani	2002		10.1145/566282.566284	design;scalar field;routing;simulation;computer science;geometry;computer graphics (images)	Graphics	-37.246090483666144	-33.23371081587352	27124
0af1307e788b19884d78519ea054af266612c2cf	sonik spring	digital music instrument;gestural control of sound;sound and music computing	The Sonik Spring is an interface for real-time control of sound that directly links gestural motion and kinesthetic feedback to the resulting musical experience.  The interface consists of a 15-inch spring with unique flexibility, which allows multiple degrees of variation in its shape and length. These are at the core of its expressive capabilities and wide range of functionality as a sound processor.	real-time clock;real-time computing	Tomás Henriques	2012		10.1145/2212776.2212375	speech recognition	Robotics	-46.11271297065961	-35.86239741298041	27148
cbf170fdffe7e44d6bee319967624f522b7362de	efficient rejoining for ubiquitous collaborative graphics editing systems	ubiquitous collaborative graphics editing system;groupware;collaborative work character generation pervasive computing computer graphics prediction algorithms prototypes computer networks bandwidth international collaboration educational institutions;computer graphics;user interest prediction algorithm;ubiquitous environment collaborative graphics editing system rejoin;ubiquitous computing computer graphics graphical user interfaces groupware;graphical user interfaces;collaborative graphics editing system;rejoin;rejoining scheme;ubiquitous computing;collaborative design session;ubiquitous environment ubiquitous collaborative graphics editing system rejoining scheme user interest prediction algorithm collaborative design session;collaborative design;shared workspace;ubiquitous environment	Collaborative graphics editing systems in ubiquitous environment support users to join and leave the collaboration dynamically at any time. The shared workspace state of these users will be inconsistent for the modification of the other session members. It is therefore necessary to initialize the application instance of the latecomer with the current state. In this paper, we propose a rejoining scheme that is based on the user interest prediction algorithm we proposed. The scheme is efficient for latecomers to rejoin the collaborative design session. The scheme is realized in our prototype system of collaborative graphics editing system that can work well in ubiquitous environments.	algorithm;graphics;prototype;retransmission (data networks);selective repeat arq;state (computer science);workspace	Bo Jiang;Jiajun Bu;Can Wang;Chun Chen	2007	2007 11th International Conference on Computer Supported Cooperative Work in Design	10.1109/CSCWD.2007.4281412	human–computer interaction;computer science;graphical user interface;multimedia;computer graphics;world wide web;ubiquitous computing	DB	-42.660708192966915	-35.6976096887666	27230
2aeeff28b5a62be75701e98254a17ebed92bc30f	data visualisation, user experience and context: a case study from fantasy sport	hci;computing;data visualisation;social games;design;evaluation	Fantasy Football is a rapidly growing online social game. As users become more sophisticated and technology advances, the amount of data that is available to inform users' decision making is growing rapidly. Representing this data in an informative and engaging way can be a challenge but data visualisation offers many ways to achieve this. This paper focuses on the design of interactive solutions that are measured against existing products by way of a comparative evaluation. In order to study the impact on user performance, efficiency and accuracy are measured for clearly defined tasks carried out on each design. The user experience is measured to understand the satisfaction and perceived ease of use of each visualisation system. This study will be useful to validate or challenge existing principles of data visualisation design and perception as well as offering suggestions for improving fantasy football products. The study will also serve as a case study to support further research into data visualisation evaluation methods. The paper concludes by discussing the findings and possible areas for further research and design.	data visualization;user experience	Rob Euman;José L. Abdelnour-Nocera	2013		10.1007/978-3-642-39265-8_16	design;computing;simulation;human–computer interaction;computer science;evaluation;operating system;multimedia;management;data visualization	HCI	-61.10820869045869	-45.38668760481251	27262
fd716ebef455758116a7358dad7e9f307b2106d5	[d87] remote tactile interaction	smart phones haptic interfaces;smart phones;tacticones remote tactile interaction piezoelectric cells tactile information smartphone control tactile stimulators blind people graphic information software tactos software intertact mobile access tactile internet bus lines emotional communication;haptic interfaces;educational institutions software graphics mobile communication internet games	The interface and the software presented use piezoelectric cells to distribute tactile information. The movements of the finger on the screen of a smartphone control 16 tactile stimulators that are located, either on the back of this smartphone, or on the back of the smartphone of the partner. This new device allows at the same time (1) to give to blind people an access to graphic information on the screens (software TACTOS), and (2) to allow tactile interactions with remote partners (software INTERTACT).: http://www.intertact.net/. This device opens the possibility of a mobile access to a tactile Internet, for which we developed some applications: games like “Memory”, teaching spaces, practical information like maps of bus lines; emotional communication (tacticones).	interaction;map;piezoelectricity;smartphone;tactile imaging	Charles Lenay;Matthieu Tixier;Gabrielle Le Bihan;Dominique Aubert;Jérôme Mara	2014	2014 IEEE Haptics Symposium (HAPTICS)	10.1109/HAPTICS.2014.6775566	embedded system;human–computer interaction;computer science;multimedia	HCI	-43.787079257316144	-42.10801194714903	27296
c9365fe9caac758534fee2ce72cd50aea8b40dd1	wi-fi human behavior analysis and ble tag localization: a case study at an underground shopping mall	indoor localization;wi fi localization;ble tag;human behavior analysis;o2o marketing	"""Techniques for obtaining customers' behavior (dwell time, count, and flow) in a shopping mall or large exhibit are highly sought-after by organizers or shop owners. Additionally, ways of effectively directing customers from cyber space such as the Web or smartphone apps to physical retail stores are also in high demand. """"Online to Offline (O2O) Marketing"""" has recently been attracting attention to make these a reality. However, the know-how accumulated through """"O2O Marketing"""" is not shared widely as it can be sensitive for business and customer's privacy. In this paper, we provide the knowledge obtained through the demonstration experiments held by the """"O2O Digital Marketing Study Group"""" organized by NPO Lisra at a large underground shopping mall in Nagoya. 250 BLE tags and 12 Wi-Fi scanners were installed in this mall. We also organized a """"coupon campaign"""" to increase the number of participants. We were able to effectively collect visiting customer count measurements via Wi-Fi scanners irrespective of the anonymization of BSSID, as our results showed similar trends collected by optical human detectors inherently installed at the venue. This paper provides preliminary insight into understanding the behaviors of retail shoppers and we believe this is a firm starting point for this area."""	cyberspace;digital marketing;experiment;mobile app;online and offline;sensor;smartphone;underground;venue (sound system);world wide web	Nobuo Kawaguchi;Kei Hiroi;Atsushi Shionozaki;Masamichi Asukai;Toshimune Nasu;Yu Hashimoto;Takeharu Nakamura;Tetsuya Gotou;Shinsuke Ando	2016		10.1145/2994374.2994403	embedded system;telecommunications;operating system;computer security;computer network	HCI	-55.72472537189173	-31.71767509283177	27298
8a2a2747f3d2cd3c44408389669dae7128b91786	configuring social agents	data collection;user agent;user interface;three dimensions	Social agents have recently been more frequently used in the user interface. However, so far not many studies have been conducted on what impact such interfaces have on users behavior. This paper discusses this and reports on empirical findings, which focus on impact of social agents on user behavior. We talk of social agents as interfaces that act autonomously but are related to the actions of the user. However, to really figure out what social impact these interfaces have on humans, we discuss what characteristics of social agents that should be possible to configure, in order to establish, maintain and develop a fruitful relation with the user. In order to do so, we needed to explore the impact for real users. The exploration of the impact of social agents such as BonzyBuddy the Parrot and Bob, the Paper-clip guy, was done empirically through observations and interviews with users. Based on empirical data collected in the study, a user-agent interaction model was constructed. The model illustrates three dimensions for configuration of social interfaces. Given the interaction model the two agents investigate are discussed followed by a discussion on what implications these observations has for design of social agents. Having identified the need for self-examining and selfadapting social agents and related problems we then conclude the paper and points at some future work.	humans;social interface;user agent;user interface	Charlotte Wiberg;Mikael Wiberg	2001			human–computer interaction;data collection;computer science;interaction model;information and computer science;user interface;user agent	HCI	-57.32400263697387	-43.571768490828156	27315
9f257a5b534e087215bbd5da710544abc73b0f4d	research alerts		Stretchy widgets, such as rubber-band lines, have been a part of GUIs for over a decade. But, with only a few notable exceptions, such as Krueger (1983), all of these stretchy widgets are “nailed down” at one end. Consequently, the way that we interact with rubber-band lines, for example, resembles how we interact with a catapult in the physical world, rather than how we typically manipulate an elastic band. Bearing this in mind, consider the task of sweeping a rectangle around an oval so that each side comes within one pixel of the lateral and vertical extremities of the oval. Figure 1 is an example of my attempt to do this with the Paint program supplied with my PC. The technique used, which is the norm with GUIs, involved selecting point A, dragging the lower right corner of the rectangle, and releasing the mouse button at point B. The problem in performing the task is in selecting where to start (point A). One has to sight laterally and vertically in order to line the point up with the top and left extremities of the oval.	drag and drop;lateral thinking;mind;mouse button;personal computer;pixel;raster graphics editor	Jennifer Bruer	1999	Interactions	10.1145/301153.301159		Vision	-41.502302803750986	-31.82809490392741	27357
8aa88e3c837bb07fe50e247d8c7295405c818ed1	the interaction design of household intelligent breathing training system		This paper achieved the acquisition of the breathing signal and identification of different respiratory modes by several attempts. On this basis, we designed an intelligent breathing-exercise system to meet the household needs. The intelligent system consists of three parts: the first part is the user interface of the breathing-exercise system, the second part is the background database and the last part is the internet community platform. The user interface of the breathing-exercise system is used for displaying user’s breathing curve and other visual feedback. All the breathing training data will be recorded in the second part: database. Those breathing exercisers can share their results on the internet community platform. The intelligent breathing-exercise system will provide a memorable user experience for users by blending these three parts.	interaction design	Zhanxun Dong;Lu Liu;Weiwei Li	2016		10.1007/978-3-319-40406-6_29	simulation;intensive care medicine;engineering;physical therapy	Robotics	-52.07170168695928	-43.82404233601806	27389
7bd8198310b5ed254103f9638825d0de0917efc4	a multidisciplinary framework for empirical analysis of the applicability of 3d stereoscopic in air traffic control	multidisciplinary framework;air traffic control;performance monitoring;empirical analysis;stereoscopic visualization;visualization interfaces;radar tracking;air traffic information;three dimensional displays multidisciplinary framework empirical analysis air traffic control air traffic information stereoscopic visualization visualization interfaces human factors;interface design;three dimensional;data visualisation;aerospace control;air traffic control aircraft aerospace control air safety radar tracking monitoring airborne radar data visualization human factors displays;human factors;monitoring;air safety;three dimensional displays;displays;data visualization;air safety air traffic control human factors data visualisation three dimensional displays;airborne radar;virtual environment;augmented reality;visual interfaces;aircraft	Human dependent safety critical domains such as Air Traffic Control require careful and focused interface design. The way air traffic information is presented heavily impacts the way controllers perform monitoring tasks and on their workload level. Our investigation aims to study the main implications deriving from the adoption of a three-dimensional space for the Air Traffic Control domain. In this paper, we discuss a multidisciplinary framework for the empirical analysis of the applicability of 3D stereoscopic visualization in Air Traffic Control. This framework takes into consideration three different major components: the design of visualization interfaces, the design of interactivities, and the associated human factors. This paper describes the framework adopted and the experimentation being conducted.		Nguyen Thong Dang;Hong-Ha Le;Monica Tavanti;Vu Duong	2003		10.1109/CIRA.2003.1222284	three-dimensional space;computer vision;radar tracker;simulation;computer science;virtual machine;interface design;air traffic control;data visualization	ML	-42.929321654095254	-47.858434999421824	27410
280eea4565035cf695a84b6c67b92993fd375d93	ebbinghaus illusion in the tactile modality	touch physiological;human computer interaction;tactile equivalent ebbinghaus illusion tactile modality ebbinghaus circles illusion induced stimulus perception action debate;visualization grasping context sensitivity psychology analysis of variance;visual pathways ebbinghaus illusion titchener illusion tactile modality tactile visual analogy perception action;visual perception;haptic interfaces;visual perception haptic interfaces human computer interaction touch physiological	In this paper, we report the first evidence for the existence of the Ebbinghaus illusion in the tactile modality. Participants were asked to explore bimanually two sets of Ebbinghaus circles while blindfolded. The results shows that the participants are more likely to be deceived when an illusion-induced stimulus is present than when a control stimulus (no illusion) is present. These results contribute to the perception-action debate. The existence of the tactile equivalent of the Ebbinghaus illusion weighs in favor of the two-stream hypothesis that assumes the existence of separated pathways for action and perception.	modality (human–computer interaction)	Mounia Ziat;Erin Smith;Cecilia Brown;Carrie DeWolfe;Vincent Hayward	2014	2014 IEEE Haptics Symposium (HAPTICS)	10.1109/HAPTICS.2014.6775520	psychology;auditory illusion;computer vision;communication;social psychology	Robotics	-45.71904058486392	-50.82593446338113	27511
8e05f06fbf13fd152faa4356df60a994b03cfc43	"""implementation of interactive poster """"suipo"""""""	smart card;internet access;ic card;public transport;advertisement;public transportation;mobile phone;internet;lessons learned;new media;two dimensional bar code	"""This paper explains an implementation of new media """"SuiPo,"""" or Suica Poster, which uses a combination of IC card ticket """"Suica"""" and Internet accessible mobile phone. Customers can get e-mail information by touching their IC card ticket on the reader located near the poster. Two pilot tests are conducted before the service has begun. The first test revealed that many people preferred the interactive poster but the registration process was complicated. The second test was conducted after improving the registration process. The lessons learned through two pilot tests are that in addition to the easier registration process, the increased popularity of two dimensional barcode reader in mobile phone has lowered the barrier of registration process.  The SuiPo has been introduced in July 31st, 2006 and started service at Shinjuku Station and Tokyo Station. We hope the implementation would change customers' perception of Suica not only as a ticket or e-money but also as an information tool."""	barcode reader;email;mobile phone;new media;smart card;subscriber identity module	Fuminori Tsunoda;Takayuki Matsumoto;Takeshi Nakagawa;Mariko Utsunomiya	2007		10.1145/1240866.1240912	smart card;human–computer interaction;computer science;operating system;public transport;internet privacy;world wide web;computer security	HCI	-51.313627455497176	-41.81732775423758	27560
a40c80e0bd8b4fe25f4565bb8c23e32ba2f6bbda	design vs. content: a survey of ten popular web sites that made emotional connections with the user	interface design;controversy;web content;internet;interactive media;humor;entertainment	"""This article presents a humorous overview and commentary on recent web sites that captivated the Internet community. It asks the question, """"Does a web site need to display design and interface excellence to be popular?"""" The article is divided into ten short parts. Each part presents a summary of the web site in question, along with a visual sample. Special emphasis is placed on pop culture's influence and involvement in audience perceptions."""	exemplification;terrestrial television;tivo	David Vogler	2005	Computers in Entertainment	10.1145/1063723.1063734	entertainment;web development;the internet;web design;computer science;marketing;interface design;web navigation;multimedia;advertising;interactive media;web 2.0;world wide web	Web+IR	-54.983198766808215	-32.20837154443899	27671
3a8136b2c33465e0de3191271ae47b11de4c1593	a review on emotional evaluations for smart phone		Human Computer Interaction (HCI) has become progressively more concerned with user experience and emotions. A variety of what experience and emotions is has been expressed in modern years which propose an amount of vital insights but it is unclear on how they inform design. The aim of this paper is to gain a deeper understanding on the techniques to collect and evaluate emotional responses conducted by other researchers. Four evaluation methods were reviewed, namely valence method, self-reported method, experiment method and semantic differential method.		Amaka Mercy Udengwu;Tek Yong Lim;Soon-Fatt Cheong	2013		10.1007/978-3-642-39473-7_17	computer science;semantic differential;multimedia;human–computer interaction;phone;user experience design	HCI	-57.97807838837906	-46.215476728753956	27709
25a4ef70b70e4706a832371ed4886916dfce2d28	design issues for usability of residential multifunction terminals	interfase usuario;command language;domestic appliances;design engineering;concepcion sistema;user interface;lenguaje;usability consumer electronics telecommunications telephony home automation microwave technology microwave communication cable tv communication cables communication system control;usuario;terminal;langage;automatizacion domestica;utilisateur;standardisation;interactive terminals;system design;electronics industry;user interfaces design engineering domestic appliances electronics industry interactive terminals standardisation telecommunication equipment;next generation;standardization system design residential multifunction terminals home electronics equipment user interface levels user command language product design financial cost usability;interface utilisateur;user;language;product design;domotique;user interfaces;conception systeme;telecommunication equipment;home automation	Some of the major issues confronting designers of home electronics equipment are discussed, and some things that should be done to ensure that the next generation of products will be useful and usable are suggested. Issues connected with user interface levels and user command language are addressed, along with the consequences for system and product design (i.e., the financial cost of usability and standardization issues). >	multi-function printer;usability	Michel Naël	1991	IEEE Journal on Selected Areas in Communications	10.1109/49.81943	usability goals;pluralistic walkthrough;embedded system;simulation;usability;human–computer interaction;telecommunications;computer science;usability engineering;product design;user interface;usability inspection	Mobile	-51.469252267078765	-42.82807303813732	27714
696028ebc73c9a0aa0928466bca4d42dac0c33ce	electronic roads in the information society	cyprus;hierarchical structure;chipre;red www;building block;information retrieval;cultural heritage;chypre;internet;asie;heritage culturel;recherche information;agent intelligent;indexation;intelligent agent;information society;world wide web;nearest neighbour;reseau www;agente inteligente;recuperacion informacion;semantic relations;asia	The objective of this study is to investigate an approach for dynamic construction of Electronic Roads. We envision Electronic Roads spanning a virtual multidimensional space, a distributed digital information repository, comprising primarily of video data of cultural heritage. A visitor (user) will be able to travel in this multidimensional space along different, semantically related historical, geographical, economic or cultural paths. Consider the following example. At a particular point in time our visitor (user) is located at a node of this multidimensional space and s/he has to decide how to proceed with his/her journey. A cluster of nearest neighbours is computed and presented to the user based on the current node but also the most recently visited nodes in order to postulate as to which semantic path the visitor is actually following. Even though the next node along this path has precedence the user will have the option to override this, thus migrating to a different path. The building block of the system is the information unit, which consists of the actual data (e.g. segment of video, image, sound or text) with an attached metadata index. All information units are elementary in granularity. That is, there is no hierarchical structure. The repository of information can be viewed as a pool of information units. There is of course a tradeoff as to what will be precisely the level of granularity, i.e. how elementary the information units will be. The more elementary the information units are the more flexibility the system has to adapt dynamically to the user needs, but of course the overall set of activities incur more overhead. On the other hand, the more coarse grain the information units are the easier it is to retrieve them and combine them but flexibility is limited. The structure of the metadata index is central to the system design. The metadata index would therefore consist of four principal entries one for each subspace.	digital data;elementary;file spanning;information repository;inverted index;overhead (computing);systems design;video	Costas Zervos;Stathis Panis;Dionysis Dionysiou;Michaelis Dionysiou;Constantinos S. Pattichis;Andreas Pitsillides;George Angelos Papadopoulos;Antonis C. Kakas;Christos N Schizas	1998		10.1007/3-540-49653-X_63	the internet;computer science;cultural heritage;artificial intelligence;world wide web;intelligent agent;information retrieval	DB	-39.40538111096538	-24.70182855478362	27718
55e091ccde06e63bd5b99884e5e3688d31ee798b	cicro: an interactive visual interface for crowd communication online	crowd communication;online community;visual interface	"""As a means for online communication has become sophisticated and diverse (e.g. twitter, SNSs, YouTube, and etc.), a large indefinite number of users actively communicate each other in online communities. Thorough such the communications, they sometimes form the """"crowd mind"""" which is temporally shared values and beliefs among mass users and can affect our society either positively or negatively. In this paper we present a concept of the online crowd and introduce CICRO, an interactive visual interface for the crowd communication. As a result of our experiment, we found that CICRO encouraged 1.7 times more active discussions among users than that of BBS since it provides users with an easy way to understand the existence of a variety of others' opinions and/or impressions in the discussions."""	interactivity	Masao Ohira;Hitoshi Masaki;Ken-ichi Matsumoto	2011		10.1007/978-3-642-21796-8_27	computer science;multimedia;internet privacy;world wide web	HCI	-57.69663612281226	-39.90068174865046	27741
4590c07e409dbdc8361af86794b01cadee1ed492	remote hri and mixed reality, an ontology		This work builds a panorama of resources in Remote HRI identified in a systematic literature review with focus on Mixed reality solutions. This study builds a panorama of HRI with mixed reality through the definition of the terms presented in this ontology which serves as a reference to facilitate to create new robotic solutions relying on this resource.	human–robot interaction;mixed reality	L. Cani D. CarolinaCaniD.;Felipe Borba Breyer;Judith Kelner	2016		10.1007/978-3-319-40406-6_23	augmented virtuality;teleoperation;ontology;human–computer interaction;mixed reality;computer science;panorama;augmented reality;virtual reality;human–robot interaction	HCI	-50.87948547656068	-35.534825552661495	27750
c778320410b3797bd376b165e90979f50925d5db	virtual smart home controlled by thoughts	virtual reality system;control systems;smart home;virtual realtiy vr;brain computer interface;virtual reality brain computer interfaces home automation;training;tv channel switching virtual smart home control electroencephalogram brain computer interface virtual reality system p300 component control command;virtual reality;tv channel switching;virtual realtiy vr eeg bci brain computer interface p300 smart home;data mining;p300 component;smart homes electroencephalography virtual reality control systems brain computer interfaces testing conferences collaborative work international collaboration tv;accuracy;control command;cost effectiveness;eeg;brain computer interfaces;electroencephalography;electroencephalogram;virtual smart home control;bci brain computer interface;smart homes;home automation	An electroencephalogram (EEG) based brain-computer interface (BCI) was connected to a virtual reality (VR) system in order to control a smart home application. Therefore special control masks were developed which allowed using the P300component of the EEG as input signal for the BCI system. Control commands for switching TV channels, for opening and closing doors and windows,for navigation and conversation were realized.Experiments with 12 subjects were made to investigate the speed and accuracy that can be achieved if several hundred of commands are used to control the smart home environment.The study clearly shows that such a BCI system can be used for smart home control. The Virtual Reality approach is a very cost effective way for testing the smart home environment together with the BCI system.	brain–computer interface;closing (morphology);electroencephalography;home automation;microsoft windows;virtual reality	Clemens Holzner;Christoph Guger;Günter Edlinger;Christoph Groenegress;Mel Slater	2009	2009 18th IEEE International Workshops on Enabling Technologies: Infrastructures for Collaborative Enterprises	10.1109/WETICE.2009.41	brain–computer interface;home automation;simulation;human–computer interaction;electroencephalography;computer science;virtual reality;multimedia	HCI	-40.155761381803806	-44.963488758070135	27818
4fd1d3cae539011f80a51dec58f8265fae17e7a0	"""""""megamind"""": fire, smoke and data"""	video capture;time of flight camera;large scale;computational photography	"""Exploding a super hero in DreamWorks Animation's """"MegaMind"""" was a large scale task. The detail and scale of the explosion was to be massive and the collaboration between effects artists unprecedented. This created several technical hurdles and eventually several innovations such as: a fluid clamping plugin, a volume splatting tool adjustable by noise, and a collaborative caching tool."""	clamping (graphics)	Krzysztof Rost;Greg Hart;Scott Peterson	2011		10.1145/2037826.2037916	computer vision;computational photography;simulation;computer science;operating system;video capture;computer graphics (images)	HCI	-50.59493461563814	-29.873668828256317	27828
0f9fa0a492c7cd538de735c86d515f7305135b51	development and evaluation of an interactive texture design method				Tsuneo Kagawa;Hiroaki Nishino;Kouichi Utsumiya	2012	Comput. Syst. Sci. Eng.		database;computer science	HCI	-47.04547448978749	-30.695586953758934	27834
776aae722b84bfd84ef5208f598905642673b5d4	multimodal acting in mixed reality interactive storytelling	institutional repositories;acting;fedora;multimodal communication;virtual reality;virtual reality engines image segmentation graphics context cameras image analysis speech enhancement computers artificial intelligence;vital;multimedia computing;multimodal input processing multimodal acting mixed reality interactive storytelling user interactions speech interaction attitude interaction gesture interaction virtual world multimodal communication;interactive;interactive narrative;knowledge based systems virtual reality multimedia computing;multimodal;interactive storytelling;vtls;mixed reality;user interaction;article;knowledge based systems;ils;virtual worlds	In this paper, an experimental mixed reality using a multimodal approach is introduced which lets users play characters in interactive narratives as though acting on a stage. Users interact with characters through speech, attitude, and gesture, enhancing their immersion in the virtual world. This system provides a small-scale but complete integration of multimodal communication in interactive storytelling. It uses a narrative's semantic context to focus multimodal input processing-that is, the system interprets users' acting (the multimodal input) in the mixed reality stage in terms of narrative functions representing users' contributions to the unfolding plot.	immersion (virtual reality);interactive storytelling;mixed reality;multimodal interaction;unfolding (dsp implementation);virtual world	Marc Cavazza;Fred Charles;Steven J. Mead;Olivier Martin;Xavier Marichal;Alok Nandi	2004	IEEE MultiMedia	10.1109/MMUL.2004.11	human–computer interaction;acting;computer science;artificial intelligence;multimodal interaction;virtual reality;mixed reality;multimedia;interactivity	HCI	-47.40833358085429	-35.82873579090794	27865
8b2117da4e3baa459ef392c48291c949fd3086f8	browsing and searching e-encyclopaedias	web design;electronic information resources	Educational websites and electronic encyclopaedias employ many of the same design elements, such as hyperlinks, frames and search mechanisms. This paper asks to what extent recommendations from the world of web design can be applied to e-encyclopaedias, through an evaluation of users’ browsing and searching behaviour in the free, web-based versions of Encyclopaedia Britannica, the Concise Columbia Encyclopaedia and Microsoft’s Encarta. It is discovered that e-encyclopaedias have a unique set of design requirements, as users’ expectations are inherited from the worlds of both web and print.	browsing;color;columbia (supercomputer);framing (world wide web);heuristic (computer science);hyperlink;interaction;requirement;scrolling;web application;web design;web search engine	Ruth Wilson;Julie Shortreed;Monica Landoni	2004			web modeling;web design;engineering;web page;multimedia;world wide web;web design program;information retrieval	HCI	-44.14329792324105	-25.068699425485523	27881
d97116a356f4834cf646ea9aa6c8618e1c19dcad	a method for the representation, evaluation and display of csg models in phigs and phigs+	modelizacion;outil logiciel;concepcion asistida;computer aided design;fabricacion asistida por computador;software tool;geometrie solide;normalisation;computer graphics;geometria solidos;sintesis imagen;modelisation;image synthesis;fabrication assistee;herramienta controlada por logicial;computer aided manufacturing;normalizacion;conception assistee;synthese image;modeling;grafico computadora;infographie;solid geometry;standardization	Abstract#R##N##R##N#The Programmer's Hierarchical Interactive Graphics System (PHIGS) is about to become a standard graphics system† which caters for the definition, display and modification of two and three-dimensional graphical data.#R##N##R##N##R##N##R##N#PHIGS, however, is mainly a wireframe system, and the PHIGS+, extensions to it have been put forward to allow the incorporation of shaded 3D graphics into PHIGS.,#R##N##R##N##R##N##R##N#One area that is important to a large constituency and which has so far not been considered in PHIGS, and PHIGS+, is that of solid modelling. This paper addresses one aspect of solid modelling by describing a simple method for the representation, evaluation and display of Constructive Solid Geometry (CSG) models in PHIGS, and PHIGS+..	constructive solid geometry;phigs	Manjula Patel	1989	Comput. Graph. Forum	10.1111/j.1467-8659.1989.tb00515.x	simulation;systems modeling;computer science;computer aided design;solid geometry;geometry;computer graphics;standardization;computer graphics (images)	NLP	-37.4943656455036	-30.31359347114539	27923
b4499fb98c6753e9c95161d35a7b662f3fc35519	a kinematic analysis of directional effects on trackball mouse control in novel normal users: an alternating treatments single subject design	kinematic analysis;repeated measures;left to right;experimental research;correlation coefficient	To know the directional efficiency of cursor moving is important for the purpose of guiding the rearrangement of icons and toolbars in the window environment. This rearrangement resolution can achieve better computer access especially in the clients with quadriplegia. However, the information about the directional efficiency of cursor movement is not clear even in the typical persons. Therefore, before surveying the quadriplegics, typical persons were researched in this study. Four typical persons simulated quadriplegics to operate trackball with their right dorsal hand and the kinematic parameters of cursor moving were measured. The single subject experimental research (SSER) with alternating treatments design was used to compare the effects of four cursor moving direction (right to left, down to up, left to right, and up to down) on the kinematic variables. The prior auto-correlation coefficients and Bartlett's ratio values were computed to make sure there was no any series dependence between measuring points before conducting parametric one-way repeated measures ANOVAs. From analyzing the parameter of deviation from the straight line, velocity, movement unit and execution time, the efficiency to move on the horizontal direction (left to right or right to left) was better than move on the vertical direction (up to down or down to up). To further know the cursor kinematic performances in patients with quadriplegics will be important.	trackball	Ling Fu Meng;Ming-Chung Chen;Chi Nung Chu;Chiu-Ping Lu;Ting-Fang Wu;Ching-Ying Yang;Jing-Yeah Lo	2007		10.1007/978-3-540-73333-1_31	simulation;engineering;artificial intelligence;engineering drawing	HCI	-45.66005558922514	-47.15762578238323	27926
df7de11d5f1121e963f0f156c45ae22ae7d90f47	exploring mediation effect of mental alertness for expressive lights: preliminary results of led light animations on intention to buy hedonic products and choose between healthy and unhealthy food		Expressive light has been explored in a handful of previous studies as a means for robots, especially appearance- constrained robots that are not able to employ human-like expressions, to convey internal states and interact with people. However, it is still unknown how different light expressions can affect a person's perception and behavior. In this poster, we explore this research question by studying the effects of different expressive light animations on people's intention to buy hedonic products and how they choose between healthy and unhealthy food. Our preliminary results show that participants assigned to a positive and low arousal light animation condition had a higher intention of purchasing hedonic products and were inclined to choose unhealthy over healthy food. Such findings are in line with previous literature in marketing research, suggesting that mental alertness mediates the effect of external stimuli on a person's behavioral intentions. Future work is thus required to evaluate such findings in a human-robot interaction context.	human–robot interaction;light pen;purchasing;robot	Sichao Song;Seiji Yamada	2017		10.1145/3125739.3132598	low arousal theory;social psychology;perception;animation;mediation (marxist theory and media studies);expression (mathematics);research question;marketing research;purchasing;psychology	HCI	-51.89182960687673	-50.60534428359012	27978
aabf888a0d89ca3a136bf8d860585b1149f18c97	active objects in interactive mobile tv	moving object;eurecom ecole d ingenieur telecommunication centre de recherche graduate school research center communication systems;active objects;mobile tv;middleware;mobile terminal	The porTiVity project is developing a converged rich media iTV system, which integrates broadcast and mobile broadband delivery to portables and mobiles and which will enable the end-user to act on moving objects within TV programmes. The developments of the project include the playout of portable rich media iTV and the middleware, data and presentation engine in the handheld receiver. In this demonstration, we will present, on the production side, the Live Annotation Tool which allows the video editor to define and include active objects in Live TV Programs, and on the user side, the interaction with active objects on a mobile terminal. 1 porTiVity Live Authoring The porTiVity project is developing a complete production chain to allow direct interaction with video objects in content broadcast to mobile devices. In other words: an object-based interactive mobile TV system [2]. Documentaries can be expanded with further details, children programs can be augmented with games, live events such as football matches may include online statistics, all extra information being at the disposal and will of the user. This demonstration will show how porTiVity enables the video editor to create interactive programs from live broadcast with the Live Annotation Tool. Before the program, the video editor prepares the list of interesting objects and defines the relevant additional content. During the broadcast, the video editor will define the objects using the authoring interface shown in Figure 1. The input video is displayed in the top right window. At each new shot, a frozen view is automatically displayed in the object selection window for a duration of 4 seconds. This leaves sufficient time for the editor to draw a bounding box around the object, and identify the object in the available list. The object is then automatically tracked in the remaining frames of the shot, and the result is displayed in the bottom right window. Other event buttons allow the video editor to dynamically include or suppress static objects in the outgoing stream, or to stop the tracking in case of error. The resulting information is encoded in Laser scenes, and multiplexed with the video program, then broadcasted to the end-user terminal (we will not be able to demonstrate the complete broadcast chain, due to the extensive equipment required). Figure 1: (1) Object selection window (2) Incoming video window (3) Outgoing video window (4) Progress bar (5) Moving objects (6) Event buttons 2 End-user interaction We will also demonstrate the interaction with a mobile terminal. The terminal receives a DVB-H stream (simulated for the demonstration), consisting of an audio, video and LASeR MPEG-4 Elementary Stream. The player for rendering the porTiVity service is based on the open source player Osmo4, which is integrated in the GPAC framework developed at ENST. The additional content, which is linked by interactive graphical elements in the LASeR DVB-H stream, can be accessed by an HTTP or Streaming Server via any broadband connection (here WiFi or UMTS). Examples of interaction can be seen on Figure Figure 2: Children program scenario Aknowledgements This work was developed within porTiVity’ (www.portivity.org), under the European Commission IST FP6 programme.	dvb-h;digital video broadcasting;elementary stream;gpac;handheld game console;hypertext transfer protocol;interactive media;mpeg-4 part 20;middleware;minimum bounding box;mobile device;mobile phone;mobile television;multiplexing;object-based language;online community;open-source software;playout;streaming media	J. Deigmöller;G. Fernàndez;A. Kriechbaum;Alejandro López;Bernard Mérialdo;Helmut Neuschmied;F. Pinyol Margalef;Rémi Trichet;P. Wolf;R. Salgado;F. Milagaia	2009		10.1007/978-3-540-92892-8_25	simulation;computer science;operating system;middleware;multimedia;world wide web	HCI	-45.09808330124712	-27.594521606248357	28103
82ae748d5fc2907f16f14f0a6a9db43bd1f1f359	"""a conversation with my """"friend"""" technology"""				Philip Baron	2013	Cybernetics and Human Knowing		conversation;psychotherapist;cognitive science;friend of a friend;psychology	Robotics	-51.45513694932619	-33.46509092267266	28144
4509884c495dbdd068bde3a022a4cd9a849e2c28	customizable automatic detection of bad usability smells in mobile accessed web applications		Remote usability evaluation enables the possibility of analysing users' behaviour in their daily settings. We present a method and an associated tool able to identify potential usability issues through the analysis of client-side logs of mobile Web interactions. Such log analysis is based on the identification of specific usability smells. We describe an example set of bad usability smells, and how they are detected. The tool also allows evaluators to add new usability smells not included in the original set. We also report on the tool use in analysing the usability of a real, widely used application accessed by forty people through their smartphones whenever and wherever they wanted.	client-side;interaction;log analysis;smartphone;usability;web application	Fabio Paternò;Antonio Giovanni Schiavone;Antonio Conte	2017		10.1145/3098279.3098558	web usability;usability lab;usability inspection;world wide web;usability engineering;human–computer interaction;pluralistic walkthrough;usability;cognitive walkthrough;usability goals;computer science	HCI	-62.45094895026211	-48.2507744262591	28145
7428d3633b1ea33ae8bd2a5da9e8b4faf53621d1	body tracking as a generative tool for experience design		Beyond ergonomic measurements, the study of human movements can help designers in exploring the rich, non-verbal communication of users’ perception of products. This paper explores the ability of human gestures to express subjective experiences and therefore, to inform the design process at its early stages. We will investigate the traditional techniques used in the Experience Design domain to observe human gestures, and propose a method to couple Experience-driven design approach with Motion Capture technique. This will allow integrating qualitative user observations with quantitative and measurable data. However, the richness of information that Motion Capture can retrieve is usually inaccessible for designers. This paper presents a method to visualize human motion data so that designers can make sense of them, and use them as the starting point for concept generation.	experience design	Monica Bordegoni;Serena Camere;Giandomenico Caruso;Umberto Cugini	2015		10.1007/978-3-319-21070-4_13	computer vision;simulation;knowledge management;generative design	HCI	-51.482628086351	-45.374219385777785	28149
2a4e44c16d9e06efdcda6309ce51a544c0a23aef	touchable: a camera-based multitouch system	3d geographic routing;gdstr;sensor networks;infrared	Touchscreens enable users to interact directly and intuitively with computers by simply touching the display area without requiring any intermediate devices. There are various touchscreen technologies that generally utilize resistive or capacitive panels. Typical touchscreens are constrained by the fixed size and high cost panels. Many research efforts have been made towards achieving multitouch functionality using vision-based systems. However, existing approaches have limitations such as relying on pre-defined gestures [5], requiring users to wear a glove with a custom pattern [4], or using infrared light pens [2].	capacitive sensing;computer;gesture recognition;light pen;multi-touch;touchscreen	Lin-Shung Huang;Feng-Tso Sun;Pei Zhang	2010		10.1145/1869983.1870045	embedded system;simulation;wireless sensor network;infrared;computer hardware;computer science	HCI	-43.105274629830035	-42.06932447682182	28193
ddf7ed730cbdcbcf1ff0d59910fe13b3a23dd1c8	hierarchical narrative collage for digital photo album	i 4 9 image processing and computer vision applications;i 3 3 computer graphics picture image generation display algorithms	Collage can provide a summary form on the collection of photos in an album. In this paper, we introduce a novel approach to constructing photo collage in the hierarchical narrative manner. As opposed to previous methods focusing on spatial coherence in the collage layout, our narrative collage arranges the photos according to the basic narrative elements from literary writings, i.e., character, setting and plot. Face, time and place attributes are exploited to embody those narrative elements in the collage. Then, photos are organized into the hierarchical structure for the multi-level details in the events recorded by the album. Such hierarchical narrative collage can present a visual overview in the chronological order on what happened in the album. Experimental results show that our approach offers a better summarization to browse on the photo album content than previous ones.	browsing;coherence (physics);digital photography;high- and low-level	Lei Zhang;Hua Huang	2012	Comput. Graph. Forum	10.1111/j.1467-8659.2012.03210.x	computer science;multimedia;computer graphics (images)	AI	-34.08622205812919	-34.354369560263144	28214
7982b557dcd5ccdc47f3e97d2a44d033b522296f	using handheld devices and situated displays for collaborative planning of a museum visit	museum technology;collaborative planning;support group;situated display;public space;handheld device;single display groupware;museum guide;group interaction	A museum visit is often a collaborative activity with many people visiting the museum with family or friends. However, mobile museum guides often separate the group rather than enhance group interaction. As part of our efforts to support group activity in the museum we introduce a novel museum visit planning system that enables small groups of visitors to collaboratively plan their visit. After individual planning from home, a group of visitors can re-plan their visit as needed to accommodate for changes due to group preferences or environmental constraints. Visitors use a handheld device, also used as a museum visitors' guide, to interact with a situated display located at the entrance of the museum. We describe the system and discuss some of the design challenges we had of designing a groupware system in a public space such as a museum.	collaborative software;mobile device;situated	Inna Belinky;Joel Lanir;Tsvi Kuflik	2012		10.1145/2307798.2307817	simulation;human–computer interaction;geography;multimedia	HCI	-56.196504214862706	-39.00702262903641	28216
5b7f4690796bf30c8c4d068e0d3c1f8341662cd0	may humanists learn from artists a new way to interact with digital technology?				Stefano Franchi	2012				HCI	-53.85885190786237	-29.524320718571406	28261
ce3048d2116ef53509f53c21ef264e96dc4decfe	an evaluation of shape changes for conveying emotions	organic user interfaces;shape displays;actuated interfaces;shape changing interfaces	In this paper, we explore how shape changing interfaces might be used to communicate emotions. We present two studies, one that investigates which shapes users might create with a 2D flexible surface, and one that studies the efficacy of the resulting shapes in conveying a set of basic emotions. Results suggest that shape parameters are correlated to the positive or negative character of an emotion, while parameters related to movement are correlated with arousal level. In several cases, symbolic shape expressions based on clear visual metaphors were used. Results from our second experiment suggest participants were able to recognize emotions given a shape with a good accuracy within 28% of the dimensions of the Circumplex Model. We conclude that shape and shape changes of a 2D flexible surface indeed appear able to convey emotions in a way that is worthy of future exploration.		Paul Strohmeier;Juan Pablo Carrascal;Bernard Cheng;Margaret Meban;Roel Vertegaal	2016		10.1145/2858036.2858537	computer vision	HCI	-49.04876850392946	-49.709793685025055	28273
14d5ffdbb2c17b05ffd143a3068fef791e29ca00	exploring novice approaches to smartphone-based thermographic energy auditing: a field study		The recent integration of thermal cameras with commodity smartphones presents an opportunity to engage the public in evaluating energy-efficiency issues in the built environment. However, it is unclear how novice users without professional experience or training approach thermographic energy auditing activities. In this paper, we recruited 10 participants for a four-week field study of end-user behavior exploring novice approaches to semi-structured thermographic energy auditing tasks. We analyze thermographic imagery captured by participants as well as weekly surveys and post-study debrief interviews. Our findings suggest that while novice users perceived thermal cameras as useful in identifying energy-efficiency issues in buildings, they struggled with interpretation and confidence. We characterize how novices perform thermographic-based energy auditing, synthesize key challenges, and discuss implications for design.	field research;semiconductor industry;smartphone	Matthew Louis Mauriello;Manaswi Saha;Erica Brown Brown;Jon Froehlich	2017		10.1145/3025453.3025471	simulation;knowledge management;multimedia	HCI	-61.27809001793493	-44.00770164527799	28285
43877aa4e32be0fbe0fbfe4028d7bc15d6bfb14f	modeling the past: digital technologies and excavations in polis, cyprus	cyprus;3 d digital modeling;3 d scanning;archaeology;exhibition;public;arsinoe;museum;marion;excavation;students;polis chrysochous	This research and educational project aimed to create virtual 3-D walkthroughs of four principal buildings from the Princeton University excavations at Polis Chrysochous, Cyprus. The structures date from the CyproArchaic period beginning in the 7th century BCE to the Late Antique period of the 7th century CE. The project was conceived together with a special exhibition, a long-term exhibition in Cyprus, and a presentation on the web. In a joint Computer Science and Art and Archaeology seminar in the spring of 2012, students created reconstructions and populated them with 3-D scanned objects. The challenge was to find appropriate visual metaphors for conveying uncertainty and change in these 3-D visualizations as well as to create a computer-animated movie focused on the buildings, their spatial relationships, and possible recon-structions consistent with the excavations.	computer animation;computer science;population	Joanna S. Smith;Szymon M. Rusinkiewicz	2012		10.1007/978-3-642-34234-9_42	humanities;visual arts;art;archaeology	HCI	-51.0774841577174	-28.011108332460974	28296
263a577b7e59ca080376043c999404f37b6afb40	virtualhuman: dialogic and affective interaction with virtual characters	mobile;multimodal interface;speech and conversational interfaces;virtual characters;affective interaction;mobile tangible virtual augmented multimodal interfaces;virtual human;conversational interface;multi user;tangible virtual augmented multimodal interfaces;interactive application;3d environment;multimodal interaction;ai techniques adaptive multimodal interfaces;markup language;multimodal input and output interfaces;knowledge base	Natural multimodal interaction with realistic virtual characters provides rich opportunities for entertainment and education. In this paper we present the current VIRTUALHUMAN demonstrator system. It provides a knowledge-based framework to create interactive applications in a multi-user, multi-agent setting. The behavior of the virtual humans and objects in the 3D environment is controlled by interacting affective conversational dialogue engines. An elaborate model of affective behavior adds natural emotional reactions and presence of the virtual humans. Actions are defined in a XML-based markup language that supports the incremental specification of synchronized multimodal output. The system was successfully demonstrated during CeBIT 2006.	affective computing;educational entertainment;markup language;multi-agent system;multi-user;multimodal interaction;xml	Norbert Reithinger;Patrick Gebhard;Markus Löckelt;Alassane Ndiaye;Norbert Pfleger;Martin Klesen	2006		10.1145/1180995.1181007	knowledge base;speech recognition;human–computer interaction;computer science;mobile technology;multimodal interaction;multimedia;markup language	HCI	-45.589937943062715	-36.20242426121751	28339
6bb4c591ed0ec48a627e051c436179ad9756c473	where third wave hci meets hri: report from a workshop on user-centred design of robots	3rd wave human computer interaction;computers;environmental issues;human computer interaction;experience centred design;manniska datorinteraktion interaktionsdesign;service robots;user centered design;hri;human robot interaction;human computer interaction conferences service robots user centered design cultural differences computers;experience centred design user centred design hri;user centred design;physical interaction;conferences;cultural differences	In this report we discuss some of the challenges when applying a user-centred design approach in the field of human-robot interaction (HRI). The discussion is based on a one-day workshop at the NordiCHI'08 conference, investigating how methods, techniques and perspectives from the field of Human Computer Interaction (HCI) could contribute to and learn from recent developments in the area of HRI. Emphasis was put on topics that are infrequent in mainstream HCI such as machine movement, autonomy, anthropomorphism, physical interaction, environmental issues and issues concerned more generally with cultural notions of robots.	autonomy;human computer;human–computer interaction;human–robot interaction;robot;user-centered design	Ylva Fernaeus;Sara Ljungblad;Mattias Jacobsson;Alex Taylor	2009	2009 4th ACM/IEEE International Conference on Human-Robot Interaction (HRI)	10.1145/1514095.1514182	human–robot interaction;user-centered design;simulation;human–computer interaction;computer science;multimedia;cultural diversity	HCI	-55.42309517278017	-35.42718257989912	28345
36d130066efdcfb7c9ac955bcded32505036a9b4	marking up a world: visual markup for creating and manipulating virtual models	visual marker;visual markup;real world scene;model user;model construction;preprinted marker;real space;pantheia system;virtual model;semantic constraint;scene element	We describe Pantheia, a system that constructs virtual models of real spaces from collections of images, through the use of visual markers that guide and constrain model construction. To create a model users simply ‘mark up’ the real world scene by placing preprinted markers that describe scene elements or impose semantic constraints. Users then collect still images or video of the scene. From this input, Pantheia automatically and quickly produces a model. The Pantheia system was used to produce models of two rooms that demonstrate the effectiveness of the approach.	item unique identification;markup language	Don Kimber;Chong Chen;Eleanor G. Rieffel;Jun Shingu;Jim Vaughan	2009			simulation;computer science;multimedia;world wide web	Vision	-38.527766686098964	-34.02983542071628	28375
0056d568bea62a4163e82be11f8904358505b3ed	controlling an avatar's pointing gestures in desktop collaborative virtual environments	deictic pointing;cves;gestural communication	Collaborative Virtual Environments (CVEs) allow people to interact with each other in virtual worlds through computer-generated avatars. Avatars are much less expressive than real bodies, and one main limitation is their lack of support for non-verbal communication such as pointing gestures. Part of the problem is that these gestures must be created through an input device, but the user is already busy controlling the avatar's location, rotation, and view direction. Pointing gestures are only useful for collaborative communication if they can be controlled simultaneously with all other avatar actions. To determine whether there are input configurations that make pointing gestures feasible, we carried out a study that compared five different widely-available input devices in three non-verbal communication tasks. We found that users were able to successfully incorporate pointing gestures into tasks that already involved moving, turning, and looking, but that there were significant and substantial differences between devices. Two configurations performed best: a mode-switched version of standard mouse-and-keyboard control, and a direct-pointing scheme using a Wii remote. There were also minor effects of gender and video-game experience. Our study suggests that users will be able to successfully create free pointing gestures in CVEs, greatly improving the communicative richness of these environments.	collaborative virtual environment;computer-generated holography;desktop computer;input device;pdf/a;virtual reality;virtual world;wii remote plus	Nelson Wong;Carl Gutwin	2012		10.1145/2389176.2389180	simulation;computer science;multimedia;communication	HCI	-48.74856576440602	-44.35794462685496	28376
5bcf8bbfe779efa6c2b23394a06a3c2d7eeb1158	café allongé	confuses reality;thousand way;shaky position;beautiful young woman;nearby table	Yannick has just been stood up. A beautiful young woman sits down at the nearby table. Instantly in love, Yannick imagines a thousand ways to get in touch with her and confuses reality and fantasy. That will place him into a shaky position.		Alexis Laffaille;Maxime Paccalet	2010		10.1145/1900264.1900330	computer graphics (images);fantasy;computer science;allonge	HCI	-58.32185422294592	-25.55689462165711	28383
af67615381176d59a94436e6484f08499530c8ee	emotion-sensitive robots - a new paradigm for human-robot interaction	human robot interaction robot sensing systems mobile robots anthropometry biosensors communication system control psychology mobile communication testing performance evaluation;intelligent robots;mobile robot;self adjusting systems;emotion recognition;mobile robots;test bed;human robot interaction;emotion recognition mobile robots man machine systems feedback intelligent robots self adjusting systems biosensors;mobile robot emotion sensitive robots human robot interaction emotion sensitive human robot cooperation framework peripheral physiological responses wearable biofeedback sensors riley original information flow model implicit communication explicit communication;information flow;feedback;control architecture;service robot;man machine systems;physiological response;human robot interface;biosensors	An emotion-sensitive human-robot cooperation framework where a robot is sensitive to the emotions of the human working with it and is also capable of changing its behavior based on this perception is presented in this paper. Peripheral physiological responses of a human are measured through wearable biofeedback sensors to detect and identify his/her underlying level of anxiety. A control architecture inspired by Riley's original information-flow model is designed. In this human-robot interaction framework, the robot is responsive to the psychological states of the human and detects both implicit and explicit communication from the human to determine its own behavior. Human-robot cooperation experiments using a mobile robot as a test bed are performed where the robot senses anxiety level of the human and responds appropriately. The results presented here validate the proposed framework and demonstrated a new way of achieving emotion-based interaction between a human and a robot.	adaptive neuro fuzzy inference system;affective computing;emotion recognition;experiment;human–robot interaction;inference engine;mobile robot;neuro-fuzzy;norm (social);peripheral;programming paradigm;sensor;synergy;testbed;wearable computer	Pramila Rani;Nilanjan Sarkar	2004	4th IEEE/RAS International Conference on Humanoid Robots, 2004.	10.1109/ICHR.2004.1442120	human–robot interaction;mobile robot;simulation;computer science;artificial intelligence;social robot	Robotics	-34.697198405572394	-39.74439206754255	28399
4d195c3238d421fbe9529afbb050f05820077de5	sig-blocks: tangible game technology for automated cognitive assessment	cognitive assessment;sensor embedded blocks;tangible user interface;wireless sensor network	This paper presents the SIG-Blocks system developed for automated cognitive assessment via tangible geometric games (TAG-Games). Computerized game administration and real-time cognitive and behavior assessments were realized by wireless self-synchronization in communication, decentralized hybrid-sensing, assembly and motion detection, and graphical visualization. The measurable performance data included time and accuracy at each manipulation step, overall speed of manipulative motions, and the total number of rotational motions. For preliminary evaluation, three types of TAG-Games were designed: TAG-GameA for assembly, TAG-GameS for shape matching, and TAG-GameM for memory. As a part of the game design, a computational measure of play complexity was defined for each TAGGame based on the geometric properties and the number of blocks in the item. An evaluation with 86 participants assessed both reliability of the TAG-Game items using split-half and test-retest reliability tests and validity of the proposed complexity measures by comparing the results with three subtests of the Wechsler Adults Intelligence Scale 4th Edition (WAIS-IV), i.e. Block Design (BD), Matrix Reasoning (MR), and Digit Span (DS). The high reliability coefficients showed that TAG-Games were reliable. Regarding validity, correlations were found between TAG-GameA and BD and between TAG-GameS and MR. Behavioral analysis also showed that the TAG-Game performance was positively correlated with the manipulation speed, but not correlated with the total number of rotations applied to the blocks. © 2016 Elsevier Ltd. All rights reserved.	assembly language;blu-ray;coefficient;real-time locating system;repeatability;signature block;span and div	Kiju Lee;Donghwa Jeong;Rachael C. Schindler;Elizabeth J. Short	2016	Computers in Human Behavior	10.1016/j.chb.2016.08.023	cognitive assessment system;psychology;simulation;wireless sensor network;computer science;artificial intelligence;communication;social psychology	HCI	-46.933791387487986	-49.499003179040976	28411
46bdb6fac0f77c6d03e60fd6aa0f6bf4b3e63298	waggling the form baton: analyzing body-movement-based design patterns in nintendo wii games, toward innovation of new possibilities for social and emotional experience		This chapter describes research conducted to analyze and better understand what is compelling about particular body-movement-based design pat- terns in Nintendo Wii games, towards innovating new possibilities for social and emotional experience with movement-based games and other interactive experi- ences. The authors analyzed games from diverse genres, to generate a bottom-up set of dimensions and characteristics of the mechanics, that can help build a foundation for heightening social and emotional engagement and enjoyment through design of novel mechanics, and/or through combining and extending successful existing mechanics. Key findings include the prevalence of kinesthetic mimicry, the value of whole body versus piecemeal movement, tensions between precision and loose movement in design, and the value of using Laban's dimensions of Effort as a lens through which to understand which sorts of movement patterns are more engaging.	baton;wii remote plus	Katherine Isbister;Christopher DiMauro	2011		10.1007/978-0-85729-433-3_6	simulation;engineering;multimedia;communication	HCI	-56.939122445734014	-45.976888645136874	28426
f9d27c8832590c4de8c6552dd461771adfc6297a	the story of libraries: from the invention of writing to the computer age	libraries;history;public libraries	Following your need to always fulfil the inspiration to obtain everybody is now simple. Connecting to the internet is one of the short cuts to do. There are so many sources that offer and connect us to other world condition. As one of the products to see in internet, this website becomes a very available place to look for countless the story of libraries from the invention of writing to the computer age sources. Yeah, sources about the books from countries in the world are provided.	book;internet;library (computing)	Chris Baggs	2002	Journal of Documentation	10.1108/jd.2002.58.4.488.4	computer science;multimedia	Theory	-62.79362727070803	-25.201130956825665	28446
8c6d6433b437f0070862a8c56e4c0c52ce7ff0ad	development of a webots simulator for the lauron iv robot	robotics;webots;lauron iv;mca2	This article describes the work accomplished to get a Webots simulator for the Lauron IV robot and how we have solved the difficulties encountered during the project. The evolution of the simulator (and of the different prototypes aimed at obtaining one that ressembles the real robot) incorporating characteristics on it, allow us to work “indistinctly” so much in the real robot as in the simulator. This way, experiences and results can be interchanged between them. In order to be able to develop our study with the Lauron IV, we should do a “physical” use of the robot with the advantages and disadvantages that it implies (refilling of batteries, starting tests, sensor re-calibration, piece wear, etc). Then, we had the idea of developing a project that consisted in developing two applications; the first application was the one that used the Lauron IV robot (the application that moves the robot physically) and the another one was the simulation of the robot able to control the physical robot movements without depending on it.		Julio Pacheco;Francesc Benito	2005			control engineering;simulation;engineering;artificial intelligence;social robot;robot control	Robotics	-37.220400160969476	-39.81879058402116	28450
03272b12173a1fca3541edb3fff17e0fd21a6e0a	do we need formal education in visualization?	data visualization data models videos computer graphics biomedical imaging humans data processing biochemistry history stress;computer graphic;data visualisation;computer science education;visual representation;high performance computer;computer science education data visualisation;visual processing;high performance computing formal education visualization erroneous interpretation decision making visual representations mapping process interaction issues computer graphics course	The following three reasons are sufficient to answer this question in the affirmative: careless mapping from data to pictures may lead to erroneous interpretation; a substantial amount of knowledge is necessary to generate images depicting complex information in a way that prevents erroneous interpretation; and decision making is increasingly based on visual representations. The organization and content of the core topics of visualization were finalized at a 1997 workshop at the Colorado School of Mines. These comprise the following eight themes: introduction to visualization; the data; the user and the task; mapping process; the representations; interaction issues; concepts of the visualization process; and systems and tools. The needs of educators and students vary too widely to bring one curriculum into focus. Instead, the eight themes recommended can be expanded into a curriculum, or compressed into several subtopics of a high-performance computing or computer graphics course. The themes can stand as separate modules, taught in a different order from that suggested. While my collaborators and I strongly recommend covering each of the core topics, we also encourage educators to expand individual themes to encompass the particular objectives of their students.		Gitta Domik	2000	IEEE Computer Graphics and Applications	10.1109/38.851744	computer vision;visual analytics;scientific visualization;information visualization;visualization;human–computer interaction;computer science;artificial intelligence;data mining;mathematics;multimedia;programming language;data visualization;algorithm;computer graphics (images)	Visualization	-58.39289506843234	-31.62541911421658	28471
6fa1f503952b6dd4fbc424c00a7b4e7271891f64	evaluation of collaborative construction in mixed reality	categories and subject descriptors according to acm ccs i 3 7 computer graphics three dimensional graphics and realism i 6 3 simulation and modeling applications;imperceptible projection blanking;segmentation;gestural interfaces;networked collaboration;mixed and augmented reality	Collaborative virtual and augmented reality are an active area of research and many systems supporting collaboration have been presented. Just like there are many different systems for VR and AR, there are many different types of collaboration. In some cases, virtual reality is used to enhance an existing collaborative process. In other cases, it enables new types of collaboration that previously were not possible (e.g. distributed VR). Other systems support tasks that can be performed either individually as well as collaboratively. While these tasks may allow to be performed collaboratively, little has been said on what the benefit is in doing so. We present a user study of a collaborative construction task in a shared physical workspace virtual reality environment under various degrees of interaction in collaboration. Our results show that, for this type of task, a pair of subjects concurrently interacting can be significantly more effective, even though individual user performance decreases. Our results further show that there is no significant benefit in giving only verbal and non-verbal assistance over a single user performing the task.	ar (unix);augmented reality;drag and drop;expect;gesture recognition;interaction;mixed reality;usability testing;virtual reality;workspace	Breght R. Boschker;Jurriaan D. Mulder	2005		10.2312/EGVE/IPT_EGVE2005/171-179	simulation;human–computer interaction;computer science;mixed reality;multimedia	Visualization	-43.92182717001809	-38.22996349471699	28507
90e9f15e7e1463716772562606e7a07290d41611	challenges of human behavior understanding for inducing behavioral change		We summarize the contributions presented at the 2nd International Workshop on Human Behavior Understanding (HBU’11) and the subsequent discussions. The scientific contributions focused on techniques and algorithmic aspects of human behavior analysis, including visual and audio modalities, as well as the design aspect of persuasive systems. One important conclusion of the workshop is that while persuasive systems and carefully designed persuasive messages form the crux of inducing behavior change, the design of systems informed by large-scale analysis of human behavior can also lead to engineered contexts for affecting behavior, at different resolutions ranging from an individual level (e.g. mobile platforms or smart homes) to urban level (e.g. carefully planned cities).		Albert Ali Salah;Bruno Lepri	2011		10.1007/978-3-642-31479-7_43	simulation;computer science;modalities;human–computer interaction;home automation;behavior change;ranging;ambient intelligence	HCI	-55.36622507756737	-38.023888506111156	28527
93ab61a7dbe4a3e9938deddfe335e7ff08d9772c	photocloud: interactive remote exploration of joint 2d and 3d datasets	cache storage;image resolution;computer graphics;real time systems client server systems servers three dimensional displays two dimensional displays data processing cloud computing interactive systems;solid modelling cache storage client server systems cloud computing data acquisition data visualisation image resolution interactive systems rendering computer graphics;interaction techniques;data processing;client server systems;two dimensional displays;interaction techniques real time systems client server systems servers three dimensional displays two dimensional displays data processing cloud computing interactive systems remote systems real time systems client server systems servers three dimensional displays two dimensional displays data processing cloud computing interactive systems computer graphics visualization systems and software image based rendering;data visualisation;servers;three dimensional displays;visualization systems and software;remote systems;3d models interactive remote exploration joint 2d 3d datasets real time client server system interactive visualization calibrated 2d photographs complex 3d description data acquisition process multiresolution dynamic hierarchical data representation cache system image browser multiresolution model renderer iconic image visualization 3d scene photocloud effectiveness;image based rendering;rendering computer graphics;interactive systems;data acquisition;solid modelling;cloud computing;real time systems	PhotoCloud is a real-time client-server system for interactive visualization and exploration of large datasets comprising thousands of calibrated 2D photographs of a scene and a complex 3D description of the scene. The system isn't tailored to any specific data acquisition process; it aims at generality and flexibility. PhotoCloud achieves scalability through a multiresolution dynamic hierarchical representation of the data, which is remotely stored and accessed by the client through an efficient cache system. The system includes a compact image browser and a multiresolution model renderer. PhotoCloud employs iconic visualization of the images in the 3D space and projects images onto the 3D scene on the fly. Users can navigate the 2D and 3D spaces with smooth, integrated, seamless transitions between them. A study with differently skilled users confirms PhotoCloud's effectiveness and communication power. The Web extras at http://www.youtube.com/playlist?list=PLHJB2bhmgB7cmYD0ST9CEDMRv1JlX4xPH are videos demonstrating PhotoCloud, a real-time client-server system for interactive exploration of large datasets comprising 2D photos and 3D models.	3d modeling;cpu cache;client–server model;data acquisition;image viewer;imagery;interactive visualization;on the fly;real-time computing;real-time locating system;scalability;seamless3d;server (computer);server (computing);used quit cigarette smoking videos;world wide web;photograph	Paolo Brivio;Luca Benedetti;Marco Tarini;Federico Ponchio;Paolo Cignoni;Roberto Scopigno	2013	IEEE Computer Graphics and Applications	10.1109/MCG.2012.92	computer vision;image-based modeling and rendering;image resolution;data processing;cloud computing;computer science;operating system;database;data acquisition;computer graphics;world wide web;data visualization;server;computer graphics (images)	Visualization	-34.724173434685476	-33.85287129043691	28616
82174d81900973a36cefb2fe4baa8dc8e59e0c28	editorial: shared values and shared interfaces: the role of culture in the globalisation of human-computer systems	cultural difference;new technology;system approach;social context;usability study;interface design;ease of use;cognitive process;data analysis;western europe;mental model	If the study of interface usability is a systematic examination of key factors that affect users’ perceptions of system usefulness and ease of use, the social and cultural context that users bring to computerised tasks must form the ultimate test of usability. For decades, the HCI literature has attempted to identify those aspects of interfaces that make the most cognitive sense to computer users the design, control and ergonomic features that help well-designed systems approach that holy grail of HCI, the intuitive interface. Most studies of usability have taken place within the U.S. and Western Europe. Certainly, significant progress has been made in identifying and isolating interface features that impact the way users negotiate their systems. However, all such studies are in fact a special case of more comprehensive phenomena that have only recently come to be appreciated: specifically, they are the elements of cultural context brought to all technology by users, regardless of the their applications, training or goals. There is an argument to be made that HCI attempts to understand the preferences, mental models and cognitive processing of computer users from Herb Simon to Donald Norman are in fact characterisations of personal culture, of the ways that each user’s social context predisposes him or her toward responding to new technology. That, in fact, is the premise of this special issue of Interacting with Computers. The problem, of course, with expanding the broad but nevertheless narrowly conceived notion of usability based upon individuals within a largely homogeneous culture is that complexity mounts exponentially. Even the best studies of usability within the limited contexts addressed thus far are difficult affairs, fraught with procedural and data analysis problems. The addition of significant cultural differences among users to the usability challenge complicates the problem enormously. Yet, in interface design as well as in manufacturing of all kinds, the past decade has seen rapidly expanding markets for products in cultures vastly different from those in which such products were developed initially. Although it may appeal in terms of carefully controlled scientific experimentation to isolate the multitude of cultural factors from usability studies, the inexorable pace of software intemationalisation will not permit it. Several major players on the world stage (not all of them based in Redmond, Washington) have been working feverishly for years trying to establish approaches and procedures for the efficient	cognition;experiment;hardware description language;human factors and ergonomics;human–computer interaction;mental model;usability testing;user (computing);user interface design	Donald L. Day	1998	Interacting with Computers	10.1016/S0953-5438(97)00025-8	social environment;cognition;usability;human–computer interaction;computer science;knowledge management;interface design;management science;data analysis	HCI	-60.63346079475811	-33.91203648514985	28651
2d0c538eabea9d9357307797406b1013f392435d	wireless sensor network technologies for the information explosion era	efficient utilization;wireless sensor network technologies;practical experiment;multiple wsns;information explosion era;wsns system;technical issue;practical aspect;information-explosion era;main part	wireless sensor network technologies for the information explosion era. Book lovers, when you need a new book to read, find the book here. Never worry not to find what you need. Is the wireless sensor network technologies for the information explosion era your needed book now? That's true; you are really a good reader. This is a perfect book that comes from great author to share with you. The book offers the best experience and lesson to take, not only take, but also learn.	information explosion		2010		10.1007/978-3-642-13965-9	telecommunications;engineering;electrical engineering	Mobile	-62.2739466071462	-24.736025911515892	28687
2d48779c06559f9212daf65b18e90993a6a12ec4	tag-based chat support system to remind users of contents of past conversations☆		Abstract   In this article, we propose a chat system that helps users remember and resume past conversations by using tags. In computer- mediated communication such as online chat, it is often difficult in communication to continue conversations regarding issues that have been discussed in the past because they may have forgotten the contents of the issue. Our system adds tags for each chat logs based on words that were used in the chat and displays the tags when users restart the interrupted chat. As a result of three experiments with the proposed system, it revealed that display of tags is useful to remind users of contents of past conversations.		Rina Tanaka;Junko Itou;Jun Munemori	2015		10.1016/j.procs.2015.08.252	human–computer interaction;computer science;multimedia	HCI	-52.321836692110246	-42.93774667123145	28701
4125eda4094d542875a99f38ccfbda4c2c7a46a8	sensible interface using multi-sensors in mobile device	sensible interface;multiple sensors;mobile device;distance measure;digital tv;interactive method;wireless communication;user experience;handheld device;mobile application;mobile interaction	This research aim is to create sensible interactions between multisensors in mobile devices. The proposal covers sensible interactions for all three kinds of methods: Mobile Application Interaction, Mobile to Mobile Interaction, and Mobile to Home Appliance Interaction. Five components are built in the handheld sensible interface device: distance measurement component, acceleration measure component, main processing component, haptic generation component, and wireless communication component. This research can support emotional user experience better than the button type input method or touch type input method can do. In addition, the proposal developed from the research can support Mobile to Mobile Interaction and Mobile to Home Appliance Interaction. The implement of the proposed sensible interaction method will demonstrate an appliance with a navigation application and digital TV control.	handheld game console;haptic technology;image viewer;input method;mobile device;mobile interaction;mobile phone;sensor;touch typing;user experience	Tae Houn Song;Soonmook Jeong;Min Kyung Kim;Key Ho Kwon;Jae Wook Jeon	2009		10.1145/1821748.1821872	embedded system;user experience design;mobile search;mobile web;human–computer interaction;mobile database;computer science;operating system;mobile technology;mobile device;multimedia;mobile station;mobile computing	HCI	-45.36308342532601	-42.026759866110226	28719
79b2737da3c93dcac9efc1f6bc3417aa5bc7e371	visual access for 3d data	universal design;disability;accessibility	ABSTRACTWe describe a novel solution to the problem of occlusion inviewing three-dimensional data. A distortion function is usedto clear a line of sight to previously obscured interior ele-ments.KeywordsDistortion viewing, 3D interactionINTRODUCTIONThere is accumulating evidence [1] supporting the idea thatthree-dimensional representations of data are advantageous.Unlike 2D techniques, 3D viewing encounters a fundament-al problem in the display of information in that it is possiblefor an object of interest to be partially or wholly occluded byother objects. Current solutions provide access to such inter-nal details through the use of cutting planes, layer removal,fly-through, and transparency. However, such techniques re-sult in the loss of contextual information.The recent desire to integrate knowledge and experiencefrom the field of cognitive science into the design of user in-terfaces and new interaction paradigms has led to an appre-ciation of the importance of presenting data within its con-text [3, 4]. Our goal is to allow interactive access to 3D in-formation spaces while maintaining context. This is achievedby providing a viewer aligned visual access distortion whichclears a line of sight to the object of interest, permitting ex-amination from all angles.VISUAL ACCESS DISTORTIONThe problem of occlusion arises when objects lie on or nearthe line of sight, between the viewpoint and the focus. Thesolution is to move data objects away from the line of sightwhere necessary, clearing a path to the focus. A distortion isapplied radially about the line of sight, displacing data itemsin gradually decreasing amounts as their distance from theline of sight increases (Figure 1). Here the scale and hue ofthe focal node have been adjusted to distinguish it from thefield.	distortion;hidden surface determination	David J. Cowperthwaite;M. Sheelagh T. Carpendale;F. David Fracchia	1996		10.1145/257089.257242	universal design;computer science;accessibility;data mining;world wide web	ML	-44.1358557936756	-45.95729376650001	28722
e3f8d79f9232dacc40f4aa1dfdae2c01d0d83cc2	evaluation of a digital grand piano for vibrotactile feedback experiments and impact of finger touch on piano key vibrations		Pianists usually pay little attention to piano key vibrations but it was shown that they influence the perceived quality of an instrument and it was suggested that they might play a role for the precise timing and dynamic control in piano playing. To objectively measure and understand the influence of vibrotactile feedback in the pianist-piano interaction, we plan experiments with a digital hybrid grand piano — the Yamaha AvantGrand N3X — that simulates piano key vibrations with a rendering system. In this paper, we evaluate piano key vibrations of this instrument with a laser Doppler vibrometer and compare the vibrations to measurements with an acoustic grand piano. The peak levels of both instruments (13 to 35 μm for the acoustic grand piano and 16 to 25 μm for the AvantGrand) are comparable but the rendering system has limitations outside the frequency range from 150 to 400 Hz. Furthermore, the perceptibility of the vibrations at the left and right hand playing positions is investigated. Finally, the impact of the finger on the vibrations during different stages of a key press is analyzed and it is demonstrated that the finger influences the displacement levels and the spectral weighting of the piano key vibrations.	acoustic cryptanalysis;displacement mapping;doppler effect;event (computing);experiment;finger touching cell phone;frequency band;key (instrument);microsoft outlook for mac;rendering (computer graphics);touchscreen;yamaha v9958;yamaha xg	Matthias Flückiger;Tobias Großhauser;Gerhard Tröster	2018	2018 IEEE Haptics Symposium (HAPTICS)	10.1109/HAPTICS.2018.8357163	rendering (computer graphics);piano;piano playing;laser doppler vibrometer;acoustics;vibration;computer science	Mobile	-44.66204982247009	-50.33190262660526	28750
c97d2a2672f05abc6d870f834a388bbf002e0207	in front of and behind the second screen: viewer and producer perspectives on a companion app	producers;interactive tv;second screen;user experience;second screens	The growing success of tablets and smartphones has shifted the focus of the interactive TV industry to the introduction of second screen applications. One example is second screen companion apps that offer extra information about a television program, often synchronized with what happens on screen. In this paper, we investigate a second screen companion app, from the perspective of the viewers and producers of such apps. Based on observations and interviews with viewers and producers, and actual usage data of a companion app from Google Analytics, we present several insights and recommendations for how to design companion apps related to ease of use, timing, social interaction, attention and added value.	google analytics;second screen;smartphone;usability;usage data	David Geerts;Rinze Leenheer;Dirk De Grooff;Joost Negenman;Susanne Heijstraten	2014		10.1145/2602299.2602312	engineering;multimedia;advertising;internet privacy	HCI	-55.28540497813859	-41.45581519812891	28757
abb35ffaa071e489efae2a106bf0a2f0fb3762d9	designing technology with older people	interactive technology;older adult;qa75 electronic computers computer science;assistive technology;older people;older adults;cost effectiveness;augmentative communication	Designing applications to support older people in their own homes is increasing in popularity and necessity. The increase in supporting older people in the community means that cash-strapped resources are required to be utilised in the most effective manner, which often lends itself to technology deployment, rather than human deployment as the former is perceived as more cost effective. Therefore, the concern arises as to how technology can be designed inclusively and acceptably to the people who are to receive it. This paper discusses the issue of design, and how these concerns have been addressed in a series of projects targeted towards directly supporting people in the community.	cash;software deployment	Guy Dewsbury;Mark Rouncefield;Ian Sommerville;Victor Onditi;Peter Bagnall	2007	Universal Access in the Information Society	10.1007/s10209-007-0079-7	simulation;cost-effectiveness analysis;human–computer interaction;multimedia	HCI	-59.078673780538914	-42.21281568822361	28793
872a64a8c40da79f2be8dc3e5b8cc3222b469be4	effects of visual information on teaching of passive drawing using a haptic interface	atrophy;eye;sense of force;visual perception educational robots eye haptic interfaces interactive devices teaching;force;educational robots;visualization;haptic interface eyes arm movement visual haptic perception visual information visual illusions upper limb motion haptic device numerical data line drawing experimenter length recognition passive drawing teaching;visualization haptic interfaces humans force atrophy educational institutions;visual perception;haptic interface vision sense of force teaching;humans;haptic interfaces;vision;teaching;interactive devices;haptic interface	Although eyes and upper limbs work together over the whole course of arm movement, vision usually dominates in visual-haptic perception. The dependence of the brain on visual information is demonstrated by the wide variety of visual illusions, such as “shade and shape,” that have been developed. In order to quantitatively measure upper-limb motion, a haptic device can be used to obtain numerical data. Although, as the name suggests, such a device is capable of transmitting force, human perception mechanisms tend not to measure force accurately owing to interference. In this paper, we analyze the role played by visual information in line drawing. In experiments where participants were asked to estimate the length of a line using a haptic device operated by an experimenter and then to draw a line on their own, there was a tendency to draw a longer line when their eyes were open. These results reinforce the importance of visual information to length recognition in drawing.	experiment;haptic technology;interference (communication);level of measurement;line drawing algorithm;numerical analysis;shader;transmitter	Nozomi Toyoda;Tetsuro Yabuta;Kazuki Matsunaga	2012	2012 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/ICSMC.2012.6378252	education;vision;computer vision;simulation;visualization;visual perception;computer science;artificial intelligence;multimedia;haptic technology;educational robotics;force	Robotics	-44.87548628815183	-49.88659630126579	28851
090199533a45fa93e552250dd64a893c761d3705	information archiving with bookmarks: personal web space construction and organization	empirical study;bookmark;web pages;information space;design;survey;www	"""Bookmarks are used as """"personal Web information spaces"""" to help people remember and retrieve interesting Web pages. A study of personal Web information spaces surveyed 322 Web users and analyzed the bookmark archives of 50 Web users. The results of this study are used to address why people make bookmarks, and how they create, use, and organize them. Recommendations for improving the organization, visualization, representation, and integration of bookmarks are provided. The recommendations include simple mechanisms for filing bookmarks at creation time, the use of time-based visualizations with automated filters, the use of contextual information in representing bookmarks, and the combination of hierarchy formation and Web page authoring to aid in organizing and viewing bookmarks."""	archive;bookmark (world wide web);organizing (structure);recommender system;tiling window manager;web page	David Abrams;Ronald Baecker;Mark H. Chignell	1998		10.1145/274644.274651	design;static web page;computer science;web page;multimedia;world wide web;information retrieval	HCI	-43.05694375480753	-24.885321358138736	28891
33676b672772bec1f8f90384072943e6981533c4	information presentation in spoken dialogue systems	user preferences;information presentation;spoken dialogue system;user satisfaction	To tackle the problem of presenting a large number of options in spoken dialogue systems, we identify compelling options based on a model of user preferences, and present tradeoffs between alternative options explicitly. Multiple attractive options are structured such that the user can gradually refine her request to find the optimal tradeoff. We show that our approach presents complex tradeoffs understandably, increases overall user satisfaction, and significantly improves the user’s overview of the available options. Moreover, our results suggest that presenting users with a brief summary of the irrelevant options increases users’ confidence in having heard about all relevant options.	algorithm;cluster analysis;dialog system;refinement (computing);relevance;user (computing);user modeling	Vera Demberg;Johanna D. Moore	2006			natural language processing;computer user satisfaction;multimedia;communication	NLP	-36.387310343764355	-51.44128800773302	28935
1909d0e187e2ed1c143b490d43058512d07745e0	the dictator and the web design	general interest and reference computing profession computers and society web design;standards;website design;computing profession;web design;programming profession;web sites;computers and society;public sphere;general interest and reference;web technology;web design graphics blood navigation weapons technology management laboratories web page design;graphics;web site;web site web design	As Web technology moved from the laboratory to the public sphere, website design moved from being the product of a single individual to become the responsibility of a group.	web design;world wide web	David Alan Grier	2009	Computer	10.1109/MC.2009.166	web service;web application security;web development;web modeling;data web;web analytics;web mapping;web design;human–computer interaction;web accessibility initiative;web standards;computer science;graphics;software engineering;web navigation;web page;multimedia;web intelligence;web 2.0;world wide web;web design program	HCI	-43.5692792352027	-24.701164666243212	28939
d91d21c19047f74241c256ec1fe80ea83358d8e6	smas - stereovision mobility aid system for people with a vision impairment		  New computer vision solutions dedicated for blind and partially sighted people have been recently introduced as a result of  significant progress in computer science. Also the growing computation power of mobile and portable devices together with  development of information systems allow to adopt and apply new and robust solutions that are able to work in nearly in a  real-time and share and use information spread over IP network. Many of currently developed solutions are dedicated to support  the user, giving the information about divert obstacles located in the environment. However many of them are using simple  detectors (commonly ultrasonic echo-location) for obstacles tracking without its classification and recognition. Therefore  the solution presented in this paper engages the stereo camera and image processing algorithms to facilitate its user with  object detection and recognition mechanisms. The inference engine combined together with ontology based problem modeling allows  to handle the risk, predict possible user’s moves and provide the user with appropriate set of tips that will eliminate or  reduce the discovered risk.    	stereopsis	Rafal Kozik	2010		10.1007/978-3-642-16295-4_36	computer vision;simulation;communication	Vision	-39.75215482874704	-43.701700100293266	29010
44c4fa979b9bd379613242e588a3b06093fda00a	encounters with hci pioneers: a personal photo journal		5 4 I N T E R A C T I O N S M A R C H – A P R I L 2 016 A L L IM A G E S B Y B E N S H N E ID E R M A N these remarkable individuals and a celebration of their contributions to HCI. The choice of 50+ pioneers was guided by my encounters at conferences and on my campus, so there is a bias toward those who work on topics close to my interests. There are many more important contributors to the field and, over time, I hope to add them. This selection from the HCI Pioneers website is meant to encourage you to visit the site so as to learn more about these leading researchers and innovators.	human–computer interaction	Ben Shneiderman	2016	Interactions	10.1145/2886009	multimedia;human–computer interaction;engineering	HCI	-60.97811827921174	-28.46487389918245	29018
9b848b613ae7ae01c438832abea16290b30f3e29	there's an automobile in hci's future: an update		automobile and home. He lives at www.jnd.org. I am writing this from Driving Assessment 2007, a conference on automobile safety, held this year on the beautiful shores of the Columbia River in the state of Washington. Members of the HCI community would feel comfortable at this conference: Issues of design, workload, and distraction dominate. Two years ago I wrote a column for <interactions> entitled “There’s an Automobile in HCI’s Future.” This conference reinforces and adds to my view: The problems of interface design are ever present in the automobile, but are accompanied by new issues, especially that of safety. New technologies are rapidly entering the automobile. New forms of automation interact with the driver (some don’t even bother to interact, but simply take over), controlling speed, braking, lane-keeping, distance from the vehicle ahead or behind or to the sides, and automatic parking. Automatic instructions, navigation, warnings, and suggestions can also be present. There is also an ever-increasing number of third-party add-ons, such as music players, videogame players, cell phones, handheld navigation systems, and computers. All this gives rise to the potential for distraction and the danger of objects flying through the air during a collision. Altogether, there are major implications for safety. The HCI profession is used to dealing with confusion and frustration. Here is a situation where physical injury is involved. As one researcher at the conference commented, “What I really like about this area is that our research saves lives.” All of our proud, graphically oriented screen devices, especially those with touch-sensitive screens and a paucity of physical controls, may be delightful to use while in a comfortable environment, but they become safety hazards when combined with driving a car. Studies show a dramatic rise in accident rates if a driver’s eyes leave the road for even two seconds. Try selecting a song or a cell contact or programming a street address into a navigation system in less than two seconds: impossible. Moreover, because the driver’s attention is shifting back and forth, not only must the eyes shift from road to device and back again, but all the context must be restored: memory structures, intentions, planned activities. Task switching lengthens the time needed for each task considerably, thereby magnifying the danger. So here is a scientific question for which I do not know the answer. Suppose we have n tasks, T1, T2, ... Tn. Now suppose we do them all simultaneously, switching among them. How long does it take to do n tasks when switching between them? I am sure the answer is task-dependent, but I would not at all be surprised to discover that the time to do n tasks in this manner is between two and ten times the sum of the times required to do the tasks separately, without switching. In other words, if in a pure, pristine laboratory test, someone can change a radio station or dial a phone number in T seconds, while on the road, this same task might take 2T to 10T seconds while timesharing. Think of the added danger. But, you may complain, people shouldn’t be programming navigation systems while driving, dialing telephone numbers, changing radio stations, or selecting which piece of music to listen to. Yes, that is logically true, but, as usual, we must be guided by people’s real behavior, not by logic. How can we design the devices people insist on using in the car so that they do not increase danger? How do we design the automatic warning systems for speed, distance from the car ahead, curve speeds, lane-straying, and all of the other safety features now being designed so that they are truly of assistance and not just a nagging, annoying set of sounds, buzzes, light flashes, and vibrations? There is an interesting design problem here: Although accidents are common, they occur with very low frequency for any individual driver. Thus, although more than a million people are killed every year in automobile accidents across the world, with tens of millions injured, according to the World Health Organization, the chance that any individual will actually have an accident is remarkably low. So warnings of an impending accident, though of great importance, will almost always be a false alarm. If we could design a system so that there were no false alarms, then drivers would seldom receive warnings, and when one occurred, they might now know how to respond. But we know that false alarms cause people to lose trust in the system. So we need	automatic parking;columbia (supercomputer);computer;dos;handheld game console;human–computer interaction;mobile phone;radio broadcasting;telephone number;the times;time-sharing;touchscreen;user interface design	Donald A. Norman	2007	Interactions	10.1145/1300655.1300683	elevation;human–computer interaction;pressure drop;flow (psychology);nozzle;axle;acoustics;engineering	HCI	-58.29025716086611	-26.04918688188512	29034
50d9c142b50c86a58e3fe6fffd8a7458e1a00a5e	mmspace: multimodal meeting space embodied by kinetic telepresence	telepresence;kinetic avatar;video conferencing	This video illustrates MMSpace, a social telepresence system using kinetic avatars, that can mirror the head movements of remote users. It aims to enhance remote nonverbal exchanges including gazes and head gestures. Its main features are i) realistic mechanical reproduction of head motions and ii) support of remote eye contact. The production, direction, screenplay, and video editing/color-grading were done by the author, with help from professional actors/actresses, camera operators, and engineers. A rush version first appeared in an oral presentation at IEEE VR'16; this was followed by the full version presented at NTT-CSL's Open House 2016 and on the author's web page.	avatar (xbox live);color;multimodal interaction;web page	Kazuhiro Otsuka	2017		10.1145/3027063.3049779	computer vision;simulation;computer science;multimedia;videoconferencing	HCI	-48.65155415481655	-35.22396138651505	29088
2b2e76ff49d5c0d8537a54b188ae3181741e6aa6	adaptive depth cue adjustments of interactive and stereoscopic 3d product models for design education		Recently, presenting Stereoscopic 3D (S3D) images for product design education has become an option. However, visual discomfort caused by interacting with S3D contents should be minimized. In this research, representative S3D virtual models of automobiles were constructed for experiments. These models were displayed on a 50-inch S3D TV and viewed through polarized glasses. The task was to control the rotation of an automobile and identify the design problems. Thirty students, majored in Industrial Design, were invited to participate in these experiments. The result showed that although S3D images had advantages in the task of dimension and distance estimations, the degree of visual discomfort increased significantly while the participants were interacting with the virtual product model intensively. Furthermore, adaptive adjustments of binocular depth cues, such as disparity, could reduce visual discomfort and accommodate individual differences.	interactivity;stereoscopic video game;stereoscopy	Li-Chieh Chen;Po-Ying Chu;Yun-Maw Cheng	2015		10.1007/978-3-319-21380-4_68	theoretical computer science	HCI	-44.36206256333623	-47.91058105460402	29096
b8f82cde4a7b8e9379fdaa97884eb1503af41446	possibility of using entropy method to evaluate the distracting effect of mobile phones on pedestrians	walking;dual task;mobile phone;standing;pedestrian;entropy	The number of mobile phone users keeps increasing every year and mobile phones have become a primary need for most people. Ordinarily, people are not aware of the risk from a common dual-task study, such as using a mobile phone while walking or simply standing. This study reviewed the methodology in evaluating the distracting effect of mobile phones on pedestrians. A comprehensive review of literature revealed that the most common method in quantifying pedestrian performance is to evaluate postural task performance. Since using a mobile phone while crossing the road is a type of dual-task study, it would give more clarity to investigate it using entropy methods that have been proven more sensitive than the traditional center of pressure (COP) in discriminating the changes in human balance. The descriptions of commonly used entropy methods were also given in order to give a broad overview of the possibility in applying the methods to further clarify the distracting effect of mobile phones.	mobile phone	Nurul Retno Nurwulan;Bernard C. Jiang	2016	Entropy	10.3390/e18110390	entropy;simulation;thermodynamics;physics;quantum mechanics	HCI	-47.280497193854494	-51.36403864788851	29099
57b3f0e28c2e5188efe6ff523a46fb9e825761d8	virtual reality for putting people with disabilities in control	communication system;graphical interface;physical impairment;virtual reality;telecommunication computing;three dimensional;medical computing;handicapped aids;graphical user interfaces;people with disabilities;attention deficit hyperactivity disorder;virtual environment;educational computing;virtual reality navigation application software space technology virtual environment space exploration layout communication system control switches displays;virtual space;entertainment;entertainment handicapped aids virtual reality graphical user interfaces medical computing educational computing telecommunication computing;disabled users people with disabilities virtual reality technology mobility problems virtual environment minimal physical movement motor ability virtual spaces physical impairments educational systems entertainment systems therapeutic systems virtual representation graphical interfaces metaphorical representations three dimensional realistic representation complex computer based communication systems non speaking people conversational material realistic scenes virtual reality scene children attention deficit hyperactivity disorder	"""Virtual reality technology offers a number of useful possibilities for people with disabilities. An obvious application is for those who face mobility problems to navigate around a virtual environment with minimal physical movement, allowing someone with limited motor ability the opportunity to explore virtual spaces with the same freedom of someone without physical impairments. This has implications for educational and entertainment systems, but also opens up possibilities for therapeutic systems, where the effects of a small amount of movement might be exaggerated in a virtual representation. We have been investigating some less obvious applications of virtual reality for people with disabilities. The development of graphical interfaces with metaphorical representations which the user manipulates suggests that a three-dimensional, realistic representation may be helpful as an interface in some situations. Work on improving complex computer-based communication systems for non-speaking people has involved investigating new ways to present conversational material to the user. A system has been developed which presents the material in terms of realistic scenes through which the user can navigate. In another project a virtual reality scene has been used to attempt to capture the interest and then channel the attention of children with attention deficit hyperactivity disorder. In both cases the approach seems to offer advantages. One feature of both projects is giving the disabled user more and better control over the interface, and also in their interactions with others. A PICTORIAL INTERFACE DESIGN FOR A COMMUNICATION SYSTEM An obvious application of virtual reality is for people with disabilities who face mobility problems. Potentially, they could navigate around a virtual environment with minimal physical movement, allowing them the opportunity to explore virtual spaces with the same freedom of someone without physical impairments. This has implications for educational and entertainment systems, but also opens up possibilities for therapeutic systems, where the effects of a small amount of movement might be exaggerated in a virtual representation. We have been investigating some less obvious applications of virtual reality for people with disabilities, specifically its application in computer-based communication systems for non-speaking people. People who are severely physically disabled and are nonspeaking have formidable challenges in communicating with others. Their physical capabilities can vary widely, many have very limited muscle control, and some can only interact with the world through the operation of a switch. In order to communicate, these people usually rely on some form of communication aid or system to interact with others. Depending on literacy ability (approximately 60% have some literacy problem) these communication aids may be textor symbol-based. Speed of communication through a communication system can be very low, however, particularly when the physical disability is severe. Word rates which can be achieved with existing technology usually lie in the range 2-10 words per minute [l]. Conversations can Virtual reality for putting people with disabilities in control ii thus be difficult to conduct and prone to breakdown. This is especially the case when the conversation partner is unfamiliar with this method of communication, and is a big obstacle to non-speaking people when they wish to meet people and interact with them. A basic form of communication device consists of a word board or display panel on which a number of language items are arranged in a grid layout, and the user communicates by pointing to the language item he or she wishes to communicate. The communication partner then reads the language item directly from the word board or display panel. With the advent of microcomputers and speech synthesisers, it became possible for these word boards to become dynamic and for the user to be able to """"speak"""" with a synthetic voice. The resulting systems were still far from ideal, however. It takes considerable time and effort for a user of a communication device to produce each speech act. This is because the speech acts usually have to be constructed word by word. The result can be dysfunctional communication caused by long pauses and low communication rate. It is possible to have systems hold a large amount of prestored phrases, sentences and larger text units (such as commonly told stories), but there remains a major problem of remembering what is stored and how to access it when needed. We have been experimenting with a new method of organising pre-stored utterances to attempt to overcome this memory load problem. It is proposed that users' memory load can be reduced by making use of their existing long term memory to help them locate and select appropriate utterances from the system. Schank and Abelson [2] proposed that people remember frequently encountered situations in structures in long term memory which they called """"Scripts"""". A script captures the essence of a stereotypical situation, and allows people to make sense of what is happening in a particular situation, and to predict what will happen next. As a way of completely modelling the outside world, scripts obviously have significant limitations. However the potential application of the scripts concept to communication systems for non-speakers has been recognised for some time. The proposal is that if pre-stored utterances are stored within script type structures inside a communication system, then this would aid the user's memory by finding utterances through appropriate scripts. Scripts are most likely to be useful in transactional type conversations, where the speaker is trying to accomplish a particular task, as these tend to be very structured, and predictable in content and flow. Many users of communication systems find this type of conversation particularly difficult as it very often involves them talking with strangers who are not familiar with their particular method of communicating. After consultation with a group of non-speaking users of communication systems [3] it was decided to develop initially a set of seven scripts. These scripts were chosen because they represented situations that users thought important, but found great difficulty communicating. The seven scripts were: """"Activities of daily living"""", """"Restaurant"""", """"Doctor"""", """"Shopping"""", """"Meeting someone new"""", """"Talking about emotions"""" and """"On the telephone"""". The scripts were developed by considering all the conversation tasks or goals a user would wish to achieve in any given situation. These conversation tasks were further broken down until each subtask could be performed with a single phrase. A suitable phrase was then composed for each of these conversation sub-tasks. These phrases were grouped into sequences and scenes, and then mapped onto suitable props. The complexity of each scene was carefully controlled to make the scenes easy to use, and easy to learn to use. Wherever possible the number of props within a scene was restricted to nine, as this is approximately the limit of human short-term memory [4]. A scene based user interface to the scripts was devised. In this interface, scripts are presented to the user as a sequence of cartoon style scenes. Each scene is populated with objects chosen to represent the conversation tasks that can be performed. Research into picture recognition and memory structures has indicated that groups of objects organised into realistic scenes corresponding to stereotypical situations assist recognition and memory, compared to groups of arbitrarily placed objects [5]. The scene approach gives users a pictorial indication of subject matter, and allows for users with varying levels of literacy skills. Figure 1 shows a scene within the restaurant script. Several types of object exist, some allow the user to switch between scenes and others allow the user to speak useful utterances. Special objects allow the user to speak detailed information regarding times, dates and numbers. Access to all the Scripts is provided through the town plan scene. To move from one script to another requires the users to navigate through the town plan. In order to make it easy to locate the town plan, a special control, the overview button, was provided. The development of a practical device has been completed. It has been shown, throughout its development, to an advisory group of nine non-speaking people. Their ideas were instrumental in the design of the system, the first set of scripts produced, and the interface design. This group thought the new type of interface would be an advance on existing systems, and was particularly suitable for providing access to scripts. Extended evaluation with disabled nonspeaking people is taking place at the moment. From the first set of preliminary evaluations it emerged that users liked the interface style, and it offered them an improved ability to Virtual reality for putting people with disabilities in control iii interact effectively in the script settings. The commercial companies involved in the project consortium will be producing a commercial communication system, called ScripTalker, based on this research. It will be available initially in Dutch and German versions, and soon in English, with other languages to follow. One advantage of working at the level of entire phrases is that customising the system for new languages is just a matter of replacing the set of phrases. A VIRTUAL ENVIRONMENT FOR CHILDREN WITH BEHAVIOUR PROBLEMS In another project a virtual reality environment has been used to hold the attention and involvement of children with problems in concentration and impulsive behaviour. In both cases the key issue has been one of control. The script interface is an attempt to give the user improved control at a glance over a system while they attend to the primary task, which is interacting with """		Norman Alm;John L. Arnott;Iain R. Murray;Iain Buchanan	1998		10.1109/ICSMC.1998.727865	simulation;human–computer interaction;computer science;graphical user interface;virtual reality;multimedia	HCI	-49.18167478371961	-43.99020400481967	29104
8c34994153419aeba7a4a4593dd1b9e710752321	gamification aware: users perception about game elements on non-game context	interaction;social network;gamification	Many applications such as Foursquase, Stackoverflow and Livemocha are using gamification to try keep users motivated to perform tasks that require the users collaboration and to increase data collection from users feedback. The term gamification refers to the use of game elements on systems and researchers from human-computer interaction (HCI) area have recently started studies to explore its effects on user experiences. In this paper we report the results from a survey performed with 368 participants about the use of gamification, their motivation and perception about it. The results show that users are not aware from some game elements and they have distinct motivation and knowledge about gamification.	gamification	Angelina de C. A. Ziesemer;Luana Müller;Milene Selbach Silveira	2013			interaction;simulation;human–computer interaction;computer science;multimedia;social network	HCI	-59.896931694733276	-44.36675767351111	29129
cadc050a871eb119197e6d2ac6e165782f4a612a	aegeanboard: an interactive whiteboard messenger for ipad	messenger;tablet computers servers relays electronic mail software libraries bandwidth;whiteboard;communication;voip;messenger communication whiteboard voip	In this paper, we present the design and implementation of Aegean Board - an interactive whiteboard messenger - which runs on the iPad, with VoIP feature. Two people can easily talk with each other via it during a session, just like using a general Internet phone. Meanwhile, the two participants share a common whiteboard, on which they can write down words, draw pictures or take photographs. When the session is over, the whole process including both video and audio will be recorded and stored locally. Either user can replay it whenever he/she wants.	image;interactive whiteboard;ipad	Fan Zhang;Hongliang Yu	2014	2014 2nd IEEE International Conference on Mobile Cloud Computing, Services, and Engineering	10.1109/MobileCloud.2014.42	embedded system;computer science;operating system;voice over ip;multimedia;world wide web	Visualization	-45.9221092265305	-32.46680445553883	29146
4a089cac13d9effe465385b101596a2f18d412ac	an interactive robot workstation for applications in rehabilitation	game play;control systems;rehabilitation;manipulators;severely physically disabled individuals;cookery;mixed manual robot assembly;vocational settings;intelligent robots;physical disability;service robots;human robot interaction;inspection;educational robots;computer vision;inspection interactive robot workstation rehabilitation educational vocational settings severely physically disabled individuals game play cookery science experiments mixed manual robot assembly;robots computer vision handicapped aids;handicapped aids;robot control;rehabilitation robotics;rehabilitation robotics workstations educational robots intelligent robots service robots robotic assembly human robot interaction control systems robot control manipulators;workstations;robots;disabled person;educational;robotic assembly;interactive robot workstation;science experiments	A novel approach to controlling robots has been developed which forms the basis of an interactive robotic workstation for use in educational and vocational settings by severely physically disabled individuals. In such settings, the intelligence of the disabled person is supplemented with the manipulation power of a robot in order to carry out activities such as game-play, cookery, science experiments, and mixed manual/robot assembly and inspection. These activities are in sharp contrast with mainstream robotics applications where the emphasis is on removing the human involvement in manual activities. >	robot;workstation	Ray G. Gosine;William S. Harwin;Robin D. Jackson	1990		10.1109/IROS.1990.262522	human–robot interaction;robot;education;computer vision;simulation;workstation;inspection;computer science;engineering;control system;artificial intelligence;social robot;robot control;multimedia;educational robotics	Robotics	-41.031183258945866	-46.79028670752137	29180
a48fd433aba1ed2e0b9ab7ef9800dbcd9222775b	pattern recognition with embedded systems technology: a survey	digital signal processing;paper;multimedia;fast reaction time;multimedia embedded systems pattern recognition biometrics computer vision dsp gpu fpga;information display subsystems;biometrics;fast reaction time pattern recognition algorithms embedded systems technology artificial intelligence information display subsystems physical sensors perception;biomedical imaging;gpu;fpga;embedded system;computer vision;artificial intelligent;embedded systems;pattern recognition embedded system artificial intelligence humans intelligent sensors sensor systems and applications intelligent actuators displays embedded computing algorithm design and analysis;embedded system design;embedded systems technology;pattern recognition;speech recognition;artificial intelligence;pattern recognition artificial intelligence computer vision embedded systems;computer science;perception;field programmable gate arrays;point of view;pattern recognition algorithms;program processors;reaction time;dsp;physical sensors	Pattern Recognition (PR) tasks are natural candidatesfor embedded systems, since they usually interact with humans and other complex processes in the real world. Often regarded as the part of Artificial Intelligence (AI) closer to perception, a typical PR application reacts to external events that the system perceives through physical sensors or input devices and produces a response using actuators or information display subsystems. Being usually far from trivial, very demanding from the computational point of view, and requiring a fast reaction time, PR algorithms constitute a real challenge to the embedded system designer. In this talk, some of the main application domains and optimization approaches proposed to deal with these relevant issues, along with many open problems and paths to improvement, are presented.	algorithm;artificial intelligence;computation;display device;embedded system;input device;mathematical optimization;pattern recognition;point of view (computer hardware company);sensor;systems design	Juan Carlos Pérez-Cortes;Jose Luis Guardiola;Alberto J. Pérez Jiménez	2009	2009 20th International Workshop on Database and Expert Systems Application	10.1109/DEXA.2009.87	medical imaging;embedded system;computer vision;real-time computing;computer science;artificial intelligence;digital signal processing;field-programmable gate array	AI	-35.998245138522975	-43.159906136801084	29330
848811d479e6da0672366ed8890e399746d59eb6	a system for automatic structure discovery and reasoning-based navigation of the web	web accessibility;agents;visual impairment;knowledge representation;direct current;intelligent software agent;table navigation	In this paper, we highlight the main research directions currently pursued by the investigators for the development of new tools to improve Web accessibility for users with visual disabilities. The overall principle is to create intelligent software agents used to assist visually impaired individuals in accessing complex on-line data organizations (e.g. tables, frame structures) in a meaningful way. Accessibility agents make use of knowledge representation structures (automatically or manually derived) to assist users in developing navigation plans; these are employed to locate given pieces of information or to answer user’s desired goals. q 2004 Elsevier B.V. All rights reserved.	case-based reasoning;intelligent agent;knowledge representation and reasoning;online and offline;software agent;web accessibility	Enrico Pontelli;Tran Cao Son;Keshav Reddy Kottapally;Co Thai Ngo;Ravikumar Reddy Kotthuru;Douglas J. Gillan	2004	Interacting with Computers	10.1016/j.intcom.2004.04.006	knowledge representation and reasoning;computer vision;human–computer interaction;web accessibility initiative;computer science;artificial intelligence;software agent;web accessibility;data mining;world wide web	AI	-39.88414238307466	-25.050275205857837	29344
3dceaa2e377892f47191b244af5351637ade9efa	accessibility of captcha methods	computer program;disabled people;image processing;speech synthesis;text to speech tts;turing test;text to speech;ocr;elderly people;visual impairment;captcha	CAPTCHA (Completely Automatic Public Turing Test to Tell Computer and Human Apart) systems are a group of methods designed to distinguish between real human users and computer programs that are interacting with the system. Their goal is to ask questions which human users can easily answer, but current computers cannot. So their evaluation can be done in two domains: how hard are they for computers and how easy are they for humans. In this paper, we focus on the second part, and review accessibility of different types of CAPTCHA for human users, especially visually impaired and elderly people.	accessibility;captcha;computer program;interaction;turing test	Sajad Shirali-Shahreza;Mohammad Hassan Shirali-Shahreza	2011		10.1145/2046684.2046704	speech recognition;computer science;multimedia;communication	HCI	-49.587546854200745	-43.29537329954675	29347
46d18ff9f29d4687d56387143c5ee15aa0499529	collaborative digital sports systems that encourage exercise		Although the importance of health and exercising as a way to maintain fitness and physical wellbeing is widely recognized, it is often difficult for people to persist with a regular workout schedule. In this paper, we propose a solution to this problem through “Collaborative Digital Sports.” This is a digital sports environment where participants are given a shared goal. Through the use of body motion sensors and video projection feedback, this environment works as a fitness playground that requires physical movements by participants. This environment is adaptable to the fitness levels of the participants, as its sensor-feedback loop is digital and unencumbered by real sports equipment. Based on this concept, we designed and implemented two collaborative digital sporting activities. The “Group Jump Rope Orchestra” is a simulated jump rope environment where people are required to synchronize jumping over a projected rope as it periodically swings by. The “How Many Legged Race!?” is a variation of the three-legged race that can accommodate any number of participants as they synchronize their steps. We tested these sports environments with numerous participants and discovered that the cooperative nature of these digital sports helps motivate the players and fosters a shared sense of caring among them.	feedback;sensor;video projector	Ayaka Sato;Anna Yokokubo;Itiro Siio;Jun Rekimoto	2014		10.1007/978-3-319-07227-2_32	jumping;computer science;multimedia;sports equipment;synchronization;rope;jump	HCI	-53.79277785975614	-45.312204600601795	29357
4e3bdaa2d11ac5afee7305850e8f9bedc47fc0bf	granularity in the design of interactive illustrations	applets;interactive illustration;user interface;interface design;software engineering;reuse;computer graphic;educational software;java applet;granularity;hypertext	We describe some issues in designing and building educational Java applets for an introductory computer graphics course. The design problem involves balancing educational goals of building intuition about fundamental concepts in a domain against heterogeneity both in subject material and in student backgrounds. We present our design approach for resolving these forces --- fine-grained units addressing small concepts --- and discuss its effects on other areas including hypertext structure, interface design, and software engineering.	computer graphics;hypertext;java applet;software engineering;user interface design	Daniel L. Gould;Rosemary Michelle Simpson;Andries van Dam	1999		10.1145/299649.299794	granularity;hypertext;human–computer interaction;computer science;theoretical computer science;interface design;operating system;software engineering;reuse;multimedia;educational software;programming language;user interface;java applet	HCI	-40.71709758379766	-30.978217107859322	29383
69182579888374bfd94580b7cf3244f29c823a3d	visualizing the internet: putting the user in the driver's seat (panel session).	volume rendering;3d ultrasound;morphology;multiresolution analysis		internet	Nahum D. Gershon;Bran Ferren;James D. Foley;Joseph Hardin;Frank Kappe;William Ruh	1995		10.1145/218380.218529	multiresolution analysis;simulation;morphology;computer science;multimedia;volume rendering;computer graphics (images)	Theory	-50.003148764242916	-28.496787795547807	29391
4170a87876a7aa3c918b39ad9b1d9875e052976d	a perceptual user interface using mean shift	interfase usuario;methode empirique;user interface;gesture;metodo empirico;empirical method;mean shift;intelligence artificielle;robustesse;artificial intelligence;robustness;interface utilisateur;perceptual user interface;inteligencia artificial;geste;gesto;robustez	This work describes a perceptual user interface based on the Mean Shift algorithm to control mouse events by gestures using a generic usb camera. To demonstrate the usefulness of our work, we present two preliminaries experiments that are controlled by the mouth. The first experiment corresponds to a specific application in charge of controlling a game called pitfall. The second one is a generic experiment that evaluates the precision and robustness of the interface. These preliminaries results show that potentiality and applicability of our study to disable people.		Edson Prestes e Silva;Anderson P. Ferrugem;Marco Aurélio Pires Idiart;Dante Augusto Couto Barone	2004		10.1007/978-3-540-30498-2_59	simulation;mean-shift;computer science;artificial intelligence;programming language;user interface;empirical research;gesture;robustness	HCI	-39.709620214316715	-48.648493366452634	29410
23e9aff6a7b8a38057c9bee6d5840707b4c1e881	research on 3d seismic data visualization system based on windows	libraries;computers;windows operation system;software;user s interface;unix workstation;3d visualization;3d explanation;earthquake engineering;three dimensional earthquake materials 3d seismic data visualization system 3d visualization in scientific computing unix workstation xjseis3d windows operation system user s interface 3d explanation 3d materials density heavy;3d seismic interpretation;3d visualization in scientific computing;data visualization linux space technology workstations petroleum computer displays operating systems computer aided manufacturing software systems scientific computing;three dimensional;data visualisation;visualization;user interfaces data visualisation earthquake engineering operating systems computers seismic waves structural engineering computing;seismic waves;operating system;structural engineering computing;object oriented;three dimensional displays;xjseis3d;workstations;3d seismic data visualization system;data visualization;3d seismic interpretation visualization pc;scientific computing;space technology;visual system;user interfaces;operating systems computers;pc;3d materials density heavy;three dimensional earthquake materials	By taking the features of seismic data into consideration, the theory of 3D visualization in scientific computing (VISC) was studied. It broke the defects of previous software, that is based on Unix workstation and develop a 3D seismic visualization system XJSeis3D which is based on PC and windows operation system. The system has used the philosophy of object-oriented, which makes it an expandable one with a friendly user's interface and a good ability of user's interaction. It realizes that real 3D explanation to 3D seismic data, and it gives full play to 3D materials density heavy, huge amount of information and space playback formation of image accurate advantage, this system explains three-dimensionally and mutually and the quality and working efficiency after raising the three-dimensional earthquake materials to the three-dimensional data body on the space.	computational science;data visualization;information design;microsoft lumia;microsoft windows;opengl;operating system;programming tool;unix;visualization software;workstation	Jie Li;Fangzhou Zhang;Yingying Niu;Xiaoyu Sheng	2008	2008 The 9th International Conference for Young Computer Scientists	10.1109/ICYCS.2008.179	visualization;human–computer interaction;computer science;operating system;database;data visualization	HPC	-33.80509133091781	-30.436379348131787	29506
681eca31c14298db48f47d9d045bafb818402b49	"""""""wear it loud"""": how and why hearing aid and cochlear implant users customize their devices"""		We investigate the role of aesthetic customization in managing sociocultural issues of assistive technology (AT) use. First, we examined an online forum dedicated to customized hearing aids and cochlear implants to understand the breadth of activity occurring in this space. Next, we conducted a series of interviews to understand motivational factors and sociocultural outcomes related to expressive AT. We found that community members discussed customization tools and techniques, shared their customizations, and provided each other with encouragement and support. Community members customized their devices as a means of self-expression that demonstrated the weareru0027s fashion sense, revealed favorite sports teams and characters, and marked holidays and personal milestones. We also found that aesthetic customization worked on multiple levels to create personal and meaningful relationships with oneu0027s AT and with other AT users, and also to manage societal expectations regarding hearing loss. Our findings may inform the design of assistive technologies that better support personalization, customization, and self-expression.	cochlear implant	Halley P. Profita;Abigale Stangl;Laura Matuszewska;Sigrunn Sky;Raja S. Kushalnagar;Shaun K. Kane	2018	TACCESS	10.1145/3214382	personalization;personal milestones;human–computer interaction;online forum;hearing aid;computer science;hearing loss;cochlear implant;sociocultural evolution;applied psychology	HCI	-59.70997110133585	-39.738369529001865	29509
3d9880b3794f14583680e1ca603273e71c4b511f	3d reconstruction of a disappeared museum	computers;art;color;geometry;museums image reconstruction;three dimensional displays art reflection cameras color geometry computers;three dimensional displays;photogrammetry history of art virtual museum 3d reconstruction;museum heritage sector 3d reconstruction disappeared museum alexandre lenoir museum french monuments art historians museum curators computer scientists digital tools;reflection;cameras	The paper aims to present methods used and difficulties encountered in an experiment of 3D reconstruction of a disappeared museum: The Alexandre Lenoir's museum of French Monuments. This experiment is a part of a larger research whose objective is to pool knowledge of art historians, museum curators and computer scientists in order to develop digital tools for enhanced communication, teaching and research in the museum heritage sector.	3d reconstruction;computer scientist	Camille Autran;François Guena	2014	2014 International Conference on Virtual Systems & Multimedia (VSMM)	10.1109/VSMM.2014.7136697	reflection;archaeology;geometry	Robotics	-51.00582767118247	-28.37336524883365	29533
7df9f59d6df3f5815f7d17e136c3f5406c90e008	the effect of font weight and rendering system on glance-based text legibility	legibility;automotive human machine interface;typeface style;driver safety;font characteristics;distraction;psychophysics	In-vehicle user interfaces increasingly rely on digital text to display information to the driver. Led by Apple's iOS, thin, lightweight typography has become increasingly popular in cutting-edge HMI designs. The legibility trade-offs of lightweight typography are sparsely studied, particularly in the glance-like reading scenarios necessitated by driving. Previous research has shown that even relatively subtle differences in the design of the on-screen typeface can influence to-device glance time in a measurable and meaningful way. Here we investigate the relative legibility of four different weights (line thicknesses) of type under two different rendering systems (suboptimal rendering and optimal rendering). Results indicate that under suboptimal rendering, the lightest weight typeface renders poorly and is associated with markedly degraded legibility. Under optimal rendering, lighter weight typefaces show enhanced legibility compared to heavier typefaces. The reasons for this pattern of results, and its implications for design considerations in modern HMIs, are discussed.	human–computer interaction;rendering (computer graphics);user interface;ios	Jonathan Dobres;Bryan Reimer;Nadine Chahine	2016		10.1145/3003715.3005454	simulation;engineering;multimedia;computer graphics (images)	HCI	-47.1491466059889	-44.152658262180935	29548
2c714a9e7dd131353a36b4bb33d9123145ebe1bf	new reflection transformation imaging methods for rock art and multiple-viewpoint display	cultural heritage;best practice;texture mapping;field of view;hewlett packard;rock art	We offer two new methods of documenting and communicating cultural heritage information using Reflection Transformation Imaging (RTI). One imaging method is able to acquire Polynomial Texture Maps (PTMs) of 3D rock art possessing a large range of sizes, shapes, and environmental contexts. Unlike existing PTM capture methods requiring known light source positions, we rely on the user to position a handheld light source, and recover the lighting direction from the specular highlights produced on a black sphere included in the field of view captured by the camera. The acquisition method is simple, fast, very low cost, and easy to learn. A complementary method of integrating digital RTI representations of subjects from multiple viewpoints is also presented. It permits RTI examination “in the round” in a unified, interactive, image-based representation. Collaborative tests between Cultural Heritage Imaging, HewlettPackard Labs, and the UNESCO Prehistoric Rock-Art Sites in the Côa Valley, a World Heritage Site in Portugal, suggest this approach will be very beneficial when applied to paleolithic petroglyphs of various sizes, both in the field and in the laboratory. These benefits over current standards of best practice can be generalized to a broad range of cultural heritage material. The 7th International Symposium on Virtual Reality, Archaeology and Cultural Heritage VAST (2006) M. Ioannides, D. Arnold, F. Niccolucci, K. Mania (Editors)	arnold;best practice;handheld game console;polynomial texture mapping;run-time infrastructure (simulation);software documentation;specular highlight;viewpoint;virtual reality	Mark Mudge;Thomas Malzbender;Carla Schroer;Marlin Lum	2006		10.2312/VAST/VAST06/195-202	texture mapping;computer vision;field of view;computer science;cultural heritage;artificial intelligence;best practice;computer graphics (images)	Graphics	-50.00206472895656	-27.53107863633021	29550
7cb03d9e15a406717400482ea8d1bc4882a48e75	le rôle fondamental du système d'interaction dans une installation de réalité virtuelle	interaction;virtual reality;interactive system;information processing;augmented reality;virtual worlds	The aim of this paper is to assess the basic role of the interaction system within a virtual reality application. Starting with an introduction to our sense of virtual reality, we put the stress on the interaction between the human operator and the virtual world. After an overview of different kinds of interaction which shows the diversity and the richness of this function, our approach consists of considering the interaction as a genuine information processing system which communicates and cooperates with the human and the virtual world.	information processing;information processor;virtual reality;virtual world	Alain Grumbach	2002		10.1145/777005.777007	simulation;human–computer interaction;engineering;metaverse;mixed reality;multimedia	Visualization	-50.57129638586771	-33.278590941596946	29555
898ab0b1cef41526ba530b17af8fff42f5ad711b	computers helping people with special needs		Sound table tennis (also known as vision impaired table tennis, and abbreviated as STT) is one of the most internationally popular sports for visually impaired people. Since the players of STT cannot rely on visual information, they must grasp the game situation using their auditory sense. However, it is difficult, especially for STT beginners, to perceive the state of the game, including the positions of the opponents, and the direction and distance of the ball. Therefore, in this paper, we aim to develop an application that enables STT players, especially beginners, to train their acoustic sense to instantaneously recognize the direction and distance of the ball at the service phase hit by opponents without going to the gym. We implemented the application named AcouSTTic (Acoustic + STT), and then evaluated its training effectiveness.	acoustic cryptanalysis	Klaus Miesenberger;Georgios Kouroupetroglou;Gerhard Goos;Juris Hartmanis;Jan van Leeuwen	2018		10.1007/978-3-319-94274-2		HCI	-44.40721820688185	-44.297874780750384	29582
7dc9ed7137bffc113521ca38b28c5034d8861e85	emerging data management systems: close-up and personal	distributed system;implicit feedback;data management;personalized search;informed trading;user experience;guided tour;interactive retrieval;data management system;user model	Conventional data management occurs primarily in centralized servers or in well-interconnected distributed systems. These are removed from their end users, who interact with the systems mostly through static devices to obtain generic services around main-stream applications: banking, retail, business management, etc. Several recent advances in technologies, however, give rise to a new breed of applications, which change altogether the user experience and sense of data management. Very soon several such systems will be in our pockets, many more in our homes, the kitchen appliances, our clothes, etc. How would these systems operate? Many system and user aspects must be approached in novel ways, while several new issues come up and need to be addressed for the first time. Highlights include personalization, privacy, information trading, annotation, new interaction devices and corresponding interfaces, visualization, etc. In this talk, we take a close look at and give a very personal guided tour to this emerging world of data management, offering some thoughts on how the new technical challenges might be approached.	centralized computing;distributed computing;personalization;privacy;server (computing);user experience	Yannis E. Ioannidis	2005		10.1145/1099554.1099556	simulation;user modeling;data management;computer science;artificial intelligence;data mining;database;multimedia;world wide web	DB	-55.434000751087154	-31.896438107636403	29623
a87eea108b989f2a30e163030c20ba10d16b2d19	toontalk - concurrent constraint programming for kids		The answer is all of the above. ToonTalk is a colleague because it shares with Logo so many goals and ways of thinking (so nicely described in Papert's book Mindstorms [Papert 80]). It is a competitor because teachers and learners have a limited amount of time to devote to such things. It can be argued that ToonTalk is a successor to Logo because it is built upon more advanced and modern ideas of computation and interfaces. ToonTalk is like Logo’s little sister – looking up to her big brother while striving to out do him. And ToonTalk is a child of Logo in that it grew out of experiences of what worked well and what didn't in using Logo. A Brief Introduction to ToonTalk ToonTalk ([Kahn 96], [Kahn 01]) started with the idea that perhaps animation and computer game technology might make programming easier to learn and do (and more fun). Instead of typing textual programs into a computer, or even using a mouse to construct pictorial programs, ToonTalk allows real, advanced programming to be done from inside a virtual animated interactive world. The ToonTalk world resembles a modern city. There are helicopters, trucks, houses, streets, bike pumps, toolboxes, hand-held vacuums, boxes, and robots. Wildlife is limited to birds and their nests. This is just one of many consistent themes that could underlie a programming system like ToonTalk. A space theme with shuttlecraft, teleporters, and so on, would work as well, as would a medieval magical theme or an Alice in Wonderland theme. The user of ToonTalk is a character in an animated world. She starts off flying a helicopter over the city. (See Figure 1.) After landing she controls an on-screen persona. The persona is followed by a dog-like toolbox full of useful things. (See Figure 2.)	admissible numbering;archive;box counting;computation;concurrent constraint logic programming;constraint programming;experience;image;inferring horizontal gene transfer;kahn process networks;logo;mobile device;online and offline;pc game;robot;toontalk	Kenneth M. Kahn	1995			constraint programming;programming language;concurrent constraint logic programming;computer science;constraint satisfaction	AI	-55.71661277059258	-25.838291541761663	29722
f3b4262eefd1f97f2a4417f170671c9fb618e168	system and interface framework for scape as a collaborative infrastructure	multiple perspectives;design principle;human computer interaction;virtual reality;multi user;multiple scales;computer supported collaborative work;design and implementation;shared space;augmented reality;hardware implementation;head mounted display	We have developed a multi-user collaborative infrastructure, SCAPE (an acronym for Stereoscopic Collaboration in Augmented and Projective Environments), which is based on recent advancement in head-mounted projective display (HMPD) technology. SCAPE combines the functionalities of an interactive workbench and a room-sized immersive display to concurrently create both exocentric and egocentric perspectives. SCAPE intuitively provides a shared space in which multiple users can simultaneously interact with a 3D synthetic environment from their individual viewpoints, and each user has concurrent access to the environment from multiple perspectives at multiple scales. SCAPE also creates a platform to merge the traditionally separate paradigms of virtual and augmented realities. In this paper, we discuss the design principles we have followed to conceptualize the SCAPE system and briefly summarize SCAPE's hardware implementation. Furthermore, we discuss in detail the high-level design and implementation of the SCAPE architecture, and present a set of unique widget interfaces currently available in our implementation that enable and facilitate interaction and cooperation. Finally, we demonstrate SCAPE's unique visualization and interface capabilities via a testbed application Aztec Explorer.	ar (unix);concurrency control;high- and low-level;interaction technique;level design;multi-user;multimodal interaction;runescape;stereoscopy;synthetic intelligence;testbed;usability testing;workbench;workspace	Hong Hua;Leonard D. Brown;Chunyu Gao	2004	Presence: Teleoperators & Virtual Environments	10.1162/1054746041382429	augmented reality;simulation;human–computer interaction;computer science;optical head-mounted display;virtual reality;computer graphics (images)	HCI	-43.52061453181711	-36.62813040232054	29725
e8cec3cbb8ce643c99ef29ca4c50a5bfa85d60bf	waving to a touch interface: descriptive field study of a multipurpose multimodal public display	urban informatics;multipurpose public displays;proxemics;hci;multimodal interaction;ubiquitous computing	Multipurpose public displays are a promising platform, but more understanding is required in how users perceive and engage them. In this paper, we present and discuss results and findings from a two-day descriptive field trial with a multipurpose public display prototype called FluiD. Our main objective was to uncover emerging issues of interaction to inform future evaluations. During the field trial within a public research exhibition, people were able to freely interact with the prototype. Twenty-six persons filled out short questionnaires and gave free-form feedback. In addition, researchers in the vicinity of the display gathered observation data. Our main findings include the difficulties encountered with mid-air gesture commands, the lack of agency in case of larger interaction area, and the possibility for stepping out from the implicit-explicit continuum in the face of potential social conflicts.	apache continuum;field research;multimodal interaction;prototype;stepping level;touch user interface	Marko Jurmu;Masaki Ogawa;Sebastian Boring;Jukka Riekki;Hideyuki Tokuda	2013		10.1145/2491568.2491571	simulation;human–computer interaction;engineering;multimedia	HCI	-55.96135771358599	-42.37039785068851	29732
2d40282d1e4d7ca57bc065f1dae8d33935cd9408	synchronous gestures in multi-display environments	distributed system;mobile device;relative orientation;wireless network;social relationship;smart phone;time synchronization;data association;interactive display;near field communication;network connectivity;digital content;infrared;face to face	University of Buenos Aires where he worked on image compression and wavelets. He later obtained his M.Sc. in Computer Science at the University of Toronto, focusing on numerical analysis and scientific visualization issues. He completed his doctoral studies Interaction. Currently he is a scientist at Microsoft's Live Labs. the expressiveness and richness of user interfaces by designing novel input technologies and techniques. He attacks his research from a systems perspective, by designing and prototyping advanced functionality, as well as via user studies of novel human-computer interactions and quantitative analysis of user performance in experimental tasks. He holds a Ph.D. in Computer Science from the University of Virginia, where he studied with Randy Pausch. This article is dedicated to Randy's heroic battle against pancreatic cancer. Andy Wilson is a member of the Adaptive Systems and Interaction group at Microsoft Research. His current areas of interest include applying sensing techniques to enable new styles of human-computer interaction, as well as machine learning, gesture-based interfaces, inertial sensing and display technologies. Before joining Microsoft, Andy obtained his B.A. current interests include search user interfaces, mobile devices, special-purpose devices, and pen-operated devices, with an emphasis on the realization of research concepts in pragmatic systems that people can use. He holds a B.S. from Rensselaer Polytechnic Institute (RPI). See hci-journal.com/editorial/final-guidelines.html for explanations of the various parts of an article in HCI format.ABSTRACT Synchronous gestures are patterns of sensed user or users' activity, spanning a distributed system that take on a new meaning when they occur together in time. Synchronous gestures draw inspiration from real-world social rituals such as toasting by tapping two drinking glasses together. In this paper, we explore several interactions based on synchronous gestures, including bumping devices together, drawing corresponding pen gestures on touch-sensitive displays, simultaneously pressing a button on multiple smart-phones, or placing one or more devices on the sensing surface of a tabletop computer. These interactions focus on wireless composition of physically co-located devices, where users perceive one another and coordinate their actions through social protocol. We demonstrate how synchronous gestures may be phrased together with surrounding interactions. Such connection-action phrases afford a rich syntax of cross-device commands, operands, and one-to-one or one-to-many associations with a flexible physical arrangement of devices. Synchronous gestures enable co-located users to combine multiple devices into a heterogeneous display environment, where the users may establish a transient network connection with other select co-located users to facilitate the …	computer science;display device;distributed computing;file spanning;human–computer interaction;image compression;machine learning;microsoft research;mobile device;numerical analysis;one-to-many (data model);one-to-one (data model);operand;scientific visualization;smartphone;surface computer;table computer;touchscreen;usability testing;user interface;wavelet	Gonzalo Ramos;Ken Hinckley;Andy Wilson;Raman Sarin	2009	Human-Computer Interaction	10.1080/07370020902739288	simulation;infrared;human–computer interaction;computer science;artificial intelligence;operating system;wireless network;mobile device;multimedia;near field communication;communication	HCI	-47.19736443766096	-38.0737461796632	29739
5a70ffc3c801f18edaba0b4296051a77825ba4b9	luminocity: a 3d printed, illuminated city generated from ladar data	3d printing ladar;three dimensional displays optical radar radar imaging;image color analysis cities and towns laboratories paints buildings plastics;network traffic luminocity 3d printed illuminated city ladar data 3d data display 3d printed model cambridge translucent plastic model display system satellite imagery	In this work, we describe LuminoCity, a novel three-dimensional data display. A 3D printed model of Cambridge, MA was generated from LADAR data. A translucent plastic model was then cast from a mold of the 3D printed model. We developed a display system to project data onto the translucent model, and we can project a wide range of analyses onto the city, including satellite imagery and network traffic.	3d printing;network traffic control	Matt R. Fetterman;Zachary J. Weber;Robert A. Freking;Andrew Volpe;David Scott	2014	2014 IEEE International Conference on Technologies for Practical Robot Applications (TePRA)	10.1109/TePRA.2014.6869161	computer vision;geography;cartography;remote sensing	Robotics	-35.232418343242074	-31.696049901055726	29782
d9ee56b724ab72f2d1913dc6e9007f0df6fc4030	protogmi musicbrush - an exercise in general multimedia instrument interface design	dictionaries;interface design;music;user interfaces;painting;real time systems;art;prototypes;image sensors			Timothy Chen;Kazushi Nishimoto;Kenji Mase	2000				HCI	-44.88321294516528	-34.265479303952226	29798
603ff7740fe1a9959e3f8d6b47d3037f5140fb05	inferring intent in eye-based interfaces: tracing eye movements with process models	user models;human computer interaction;multimodal interface;hidden markov model;tracing;cognitive models;hidden markov models;eye movements;eye based interfaces;eye movement;process model;cognitive model;user model	While current eye-based interfaces offer enormous potential forefficient human-computer interaction, they also manifest thedifficulty of inferring intent from user eye movements. This paperdescribes how fixation tracing facilitates the interpretation ofeye movements and improves the flexibility and usability ofeye-based interfaces. Fixation tracing uses hidden Markov models tomap user actions to the sequential predictions of a cognitiveprocess model. In a study of eye typing, results show that fixationtracing generates significantly more accurate interpretations thansimpler methods and allows for more flexibility in designing usableinterfaces. Implications for future research in eye-basedinterfaces and multimodal interfaces are discussed.	eb-eye;hidden markov model;human–computer interaction;markov chain;multimodal interaction;usability	Dario D. Salvucci	1999		10.1145/302979.303055	computer vision;simulation;computer science;hidden markov model;eye movement	HCI	-51.19870675624218	-45.17117937386441	29887
36cd36ce5c9773c3167c779196029ff0b3a050c7	the effect of graphical and textual visualisation on the comprehension of prolog execution by novices: an empirical analysis			graphical user interface;infographic;list comprehension;marian petre;pain;programmer;prolog;trusted platform module	Paul Mulholland	1994			programming language;visualization;natural language processing;artificial intelligence;comprehension;prolog;computer science	HCI	-39.935588440598494	-28.408902904462284	29972
7372cb21ccc62de514685c19e6dc08c2d3b21728	algorithmic and complexity results for drawing euclidian trees				Sue Whitesides;Rongyao Zhao	1992			human–computer interaction;computer science	Logic	-51.76294016679925	-32.1471425422571	29974
e049ff0d22e2a08e664cc621b18eea31a2f86f90	usability report. teaching mathematics with an augmented reality-based game	augmented reality		augmented reality;usability	Weena Jimenez Naceroz;Gustavo Valero Simancas	2011	e-Minds		computer-mediated reality;human–computer interaction;usability;mixed reality;augmented reality;multimedia	HCI	-50.857944317389936	-36.13141666320506	29981
cb8d51e1189b8149d5da8f3bef8710879e6b9fcc	character recognition in road signs using a smartphone		In recent years, with the spread of smartphone usage (Android, iPhone) the need for onsite automatic character recognition systems have emerged. For foreign tourists in Japan, it is paramount to navigate their way around by capturing images of road signs and automatically recognizing them. Such users can them freely roam around without the need for tour guides. In this work, we propose a system that can recognize and interpret road navigation signs on a smartphone using Deep Learning.	android;deep learning;experiment;gaussian blur;image noise;optical character recognition;smartphone	Moriwake Ryo;Stephen Karungaru;Kenji Terada	2017	2017 6th IIAI International Congress on Advanced Applied Informatics (IIAI-AAI)	10.1109/IIAI-AAI.2017.200	deep learning;image processing;multimedia;android (operating system);computer science;artificial intelligence	HCI	-39.75649068546719	-43.29524927661612	30010
aa8a8fecc83de2948e42816ae0d118c6fbcd9c92	design of predictive text input mechanism for swarachakra	prediction mechanism;keyboards;soft keyboards;indian languages;text input;predictive interfaces	In the earlier experiments carried in the lab, it was observed that users pay a cognitive toll to use current prediction mechanisms for Marathi on Smartphones and it slows down the text entry speed of users[3]. Two main hypothesized reasons which account for this cognitive toll were constant shift of attention because of the current predictive interface and the difficulty in building the conceptual model of the predictive system. In this project, a novel prediction mechanism for Swarachakra keyboard for Marathi was designed and developed to test for these hypothesized reasons and also to test if the new prediction mechanism improves text entry speed.A within subject evaluation was conducted with 5 users for 3 keyboards (Swarachakra with prediction chakra and corpus coverage of 47.3%, Swarachakra with prediction chakra and corpus coverage of 79.8% and Swarachakra without prediction). Swarachakra without prediction performed best among them followed by Swarachakra with prediction and with 47.3% corpus coverage.	chakra;experiment;feedback;predictive text;projection screen;smartphone;subject reduction;swarachakra	Prasad Ghone	2016		10.1145/3014362.3014376	simulation;speech recognition;computer science;artificial intelligence	HCI	-47.16631528309822	-45.514089526063614	30032
1b92a4d6cf83b2c9f091952446da51927cda4b6b	mining web pages using features of rendering html elements in the web browser		  The Web is the largest repository of useful information available for human users, but it is usual that Web Pages do not provide  an API to get access to its information automatically. In order to solve this problem, Information Extractors are developed.  We present a new methodology to induce Information Extractors from the Web. It is based on rendering HTML elements in the  Web browser. The methodology uses a KDD process to mining a dataset with features of the elements in the Web page. An experimentation  over 10 web sites has been made and the results show the effectiveness of the methodology.    	html element;web page;world wide web	F. J. Fernández;José Luis Álvarez;Pedro J. Abad;Patricia Jiménez	2011		10.1007/978-3-642-19931-8_20	html scripting;web mining;static web page;web development;site map;framing;web mapping;web-based simulation;html;web design;comet;web api;dynamic web page;web navigation;web page;database;client-side scripting;world wide web;website parse template;information retrieval;web server	Web+IR	-42.09449630321316	-24.85542323996157	30048
167bec1d8c536b42317bb5da5606b72899d7a6ff	shiftmask: dynamic oled power shifting based on visual acuity for interactive mobile applications		OLED power management on mobile devices is very challenging due to the dynamic nature of human-screen interaction. This paper presents the design, algorithms, and implementation of a lightweight mobile app called ShiftMask, which allows the user to dynamically shift OLED power to the portion of interest, while dimming the remainder of the screen based on visual acuity. To adapt to the user's focus of attention, we propose efficient algorithms that consider visual fixation in static scenes, as well as changes in focus and screen scrolling. The results of experiments conducted on a commercial smartphone with popular interactive apps demonstrate that ShiftMask can achieve substantial energy savings, while preserving acceptable readability.	algorithm;experiment;mobile app;mobile device;oled;power management;scrolling;smartphone;visual basic[.net]	Han-Yi Lin;Pi-Cheng Hsiu;Tei-Wei Kuo	2017	2017 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED)	10.1109/ISLPED.2017.8009181	computer science;visual acuity;real-time computing;visualization;scrolling;power management;fixation (visual);embedded system;mobile device;remainder;mobile telephony	EDA	-37.77179886099243	-48.369488076571464	30054
175fa0d150bb9c3f69b37d6663f335cc7e629c30	technology, culture and location in a recent new zealand sonic art project	conference contribution;music	The HIEMPA (Hybrid Instruments from Electroacoustic Manipulation And Models of Pütorino and Aquascape) project combined a team of people with technical, artistic, environmental and cultural expertise toward the artistic outcome of extending the New Zealand sonic art tradition. The project involved collecting audio samples from the aquascape of the Ruakuri Caves and Nature Reserve in Waitomo, South Waikato, New Zealand; and samples of a variety of Pütorino – a New Zealand Mäori wind instrument. Following a machine learning analysis of this audio material and an analysis of the performance material, hybrid digital instruments were built and mapped to suitable hardware triggers. The new instruments are playable in real-time, along with the electroacoustic manipulation of Pütorino performances. The project takes into account the environmental and cultural significance of the source material, with the results to be released as a set of compositions. This paper discusses the research process.	hd radio;machine learning;performance;real-time transcription	Ian Whalley	2008			humanities;engineering;media studies	Robotics	-51.75008605012866	-29.620076805967614	30063
fd95399a45293b4c09a60a4f077d127c15a0b883	systool — an online learning tool for signals and systems	convolution;virtual university;online learning;linear system;demodulation;java applet;interactive simulation;demodulation convolution	SYSTOOL is an online learning tool for real or virtual courses in signals and systems. It provides a collection of Java applets for interactive simulation and visualisation of linear systems. While based on a traditional textbook, SYSTOOL is accessible online though the Virtual University of Bavaria. This article describes structure and content of this learning tool and presents selected modules.	java applet;linear system;online machine learning;simulation	Rudolf Rabenstein	2002	2002 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2002.5745566	computer science;theoretical computer science;mathematics;multimedia;convolution;demodulation;linear system;java applet;computer graphics (images)	Visualization	-44.36527577699554	-28.618913692383675	30187
77588d281b7f7ac74b1b4d71704cf2860f86f56a	realizing affect in speech classification in real-time	field programmable gate array;human performance;real time	Robots can recognize pitch, perceived loudness, or simple categories such as positive or negative attitude in real-time from speech. Existing research has demonstrated specialized systems able to perform these operations. Additionally, off-line classification has shown that affect can be classified at rates that are better than random although worse than human performance. Taken together, this previous research indicates the possibility of real-time classification of a variety of affective states. We are exploring this problem with the eventual goal of designing an field-programmable gate array (FPGA) based system that will rapidly process relevant features in parallel. Harsh Words and Robotics It would be useful and amusing to make robots capable of responding to emotional information in human speech. Indeed this information could be used to train robots using techniques like reinforcement learning. The muttering of profanities to un-comprehending computers or automobiles seems to indicate an innate desire for these machines to respond. This paper examines the possibility of designing a system that would allow rapid response to the affective content of speech. As a starting point, existing systems for real-time processing of different aspects of speech are reviewed. Proceeding from this we will describe some work-in-progress to design a FPGA that processes information derived from speech in parallel. Robotics and Emotions In addition to the fiction of Kapek and Asimov, a variety of literature has discussed the possibility of robots having emotions (Sloman & Croucher 1981), responding to emotions (Picard 1997), and displaying emotions (Breazeal & Scassellati 1999). There are also a number of actual robots such as Kismet and Sparky (Scheeff et al. 2000), which seek to display emotions. With robotics there has been a focus on “output” of models of emotional states while “input” still remains impoverCopyright c © 2006, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. ished. This is likely because systems to recognize emotional states remain an area of active research.	aaron sloman;artificial intelligence;computer;field-programmability;field-programmable gate array;human reliability;kismet;musicbrainz picard;online and offline;pitch (music);real-time clock;real-time locating system;real-time transcription;reinforcement learning;robot;robotics	Carson Reynolds;Masatoshi Ishikawa	2006			simulation;engineering;artificial intelligence;communication	AI	-52.42263895068405	-49.83581936191694	30196
fe866c0ccba665e30c1fd177e198ffefc119ed9b	surgery scene representation in 3d simulation training sdk	surgery computer based training digital simulation educational institutions medical computing software engineering;surgery training solid modeling three dimensional displays software visualization force feedback;samara state medical university surgery scene representation 3d simulation training sdk software development kit medical community surgery training formal model simulation objects scenario representation objects interaction modeling surgery scene simulation laparoscopy surgery training	This paper introduces a software development kit (SDK) to provide IT developers and medical community a platform to build new simulation technologies for surgery training. There is described a formal model for simulation objects, scenes and scenarios representation and a software solution for objects interaction modeling and surgery scene simulating. The results are illustrated by the simulation suite for laparoscopy surgery training delivered and installed at Samara State Medical University.	algorithm;graphical user interface;mathematical model;ontology (information science);requirement;simulation software;software development kit	Anton Ivashchenko;Nikolay Gorbachenko;Alexandr Kolsanov;Andrey Kuzmin	2016	2016 18th Conference of Open Innovations Association and Seminar on Information Security and Protection of Information Technology (FRUCT-ISPIT)	10.1109/FRUCT-ISPIT.2016.7561511	computer vision;simulation;computer science;multimedia	Robotics	-36.06600592223528	-37.78237701519337	30203
18ee92998531b32dff18dfeaa3de7e6f84924c70	on communication assistance via bots - towards imdj		Communication robots (bots) have been popular in our lives. Actually robots have several shapes and possibilities. In several applications such as SNS, bots with simple pattern matching are installed and they do not provide natural conversation. However users sometimes can enjoy the conversation. There might be a certain shikake in the applications or users can enjoy the vertual conversation as it is. In this paper we investigate what types of bot are preferable in the Human-Computer interaction. We prepare several types of bot to determine the preferable features of bots. From the experiments, we could determine types of words for an enjoyable or comfortable conversation (interaction). Thus we can install such a conversation shikake in the bot applications. In addition, we will give a certain suggestion for the preparation of a certain mechanism for the conversation activation in the Innovators Marketplace on Data Jacket (IMDJ). c © 2015 The Authors. Published by Elsevier B.V. Peer-review under responsibility of KES International.	experiment;human–computer interaction;pattern matching;robot	Akinori Abe;Moeko Hayashi	2016		10.1016/j.procs.2016.08.213	simulation;artificial intelligence;multimedia	SE	-50.1949880074368	-48.360970575605876	30236
14d4b96d0468d93f80642ed23580d691983b933e	defcon		The aim of the CIG 2009 Defcon AI competition is to see which bot is the best Defcon player. Each bot plays 30 matches against each other bot, in a series of one-on-one matches with bots playing in each of the possible starting territory configurations. The game will run on a limited-information mode (i.e., bots cannot see units hidden by the fog of war) and the victory timer will start no later than 4 hours into the game (additionally to the usual game ending conditions). For each match, the resulting score (standard rules: 2*kills - own casualties) is recorded for each player. In the end, the player with the highest cumulative score wins. Screenshots are available at http://www.introversion.co.uk/defcon/about/screenshots.html. The Defcon API has been developed by Imperial College and Introversion Software to allow easy access to all the functions and data required to write a fully functional AI for Introversion's thermonuclear war simulation Defcon. Interfaces to C++, Java and LUA are available, and the original implementation of the Defcon AI is provided as a starting point. The API is available for free from http://www.doc.ic.ac.uk/~rb1006/projects:api, and it works with the demo version of Defcon, which can be downloaded from http://www.introversion.co.uk/defcon/downloads/.	accessibility;application programming interface;c++;inscriptiones graecae;java;lua;screenshot;simulation;timer	Robin Baumgarten	2009	2009 IEEE Symposium on Computational Intelligence and Games	10.1109/CIG.2009.5286509		Logic	-52.901724811001635	-24.77598570340174	30237
15f5c638c4cef6e8de5a2fb26e981f263c828a3e	breathvr: leveraging breathing as a directly controlled interface for virtual reality games		With virtual reality head-mounted displays rapidly becoming accessible to mass audiences, there is growing interest in new forms of natural input techniques to enhance immersion and engagement for players. Research has explored physiological input for enhancing immersion in single player games through indirectly controlled signals like heart rate or galvanic skin response. In this paper, we propose breathing as a directly controlled physiological signal that can facilitate unique and engaging play experiences through natural interaction in single and multiplayer virtual reality games. Our study (N = 16) shows that participants report a higher sense of presence and find the gameplay more fun and challenging when using our breathing actions. From study observations and analysis we present five design strategies that can aid virtual reality game designers interested in using directly controlled forms of physiological input.	anomalous experiences;experience;galvanic isolation;head-mounted display;immersion (virtual reality);multilevel security;virtual reality	Misha Sra;Xuhai Xu;Pattie Maes	2018		10.1145/3173574.3173914	human–computer interaction;multimedia;immersion (virtual reality);game design;computer science;virtual reality;skin conductance	HCI	-53.26072444729847	-43.9622576117069	30239
793cb6a87f2d82bc01b60e4af64099ded60e3012	a multimodal framework for music inputs (poster session)	human computer interaction;singing voice analysis;indexing and retrieval;pitch tracking;digital music;music interfaces;multimodality;music information retrieval;symbolic representation	The growth of digital music databases imposes new content-based methods of interfacing with stored data; although indexing and retrieval techniques are deeply investigated, an integrated view of querying mechanism has never been established before. Moreover, the multimodal nature of music should be exploited to match the users' expectations as well as their skills. In this paper, we propose a hierarchy of music-interfaces that is suitable for existent prototypes of music information retrieval systems; according to this framework, human/computer interaction should be improved by singing, playing or notating music. Dealing with multiple inputs poses many challenging problems for both their combination and the low-level translation needed to transform an acoustic signal into a symbolic representation. This paper addresses the latter problem in some details, aiming to develop music-interfaces available not only to trained-musician.	acoustic cryptanalysis;database;high- and low-level;information retrieval;list of online music databases;multimodal interaction	Goffredo Haus;Emanuele Pollastri	2000		10.1145/354384.354539	computer vision;speech recognition;digital audio;computer science;multimedia;pop music automation;world wide web	Web+IR	-46.11483281839665	-33.467720026441384	30273
294b38799f273f6039abadb51cdb03fabd6c43f5	interaction with magic lenses: real-world validation of a fitts' law model	magic lens pointing;field experiment;three dimensional;human performance modeling;urban area;augmented reality;mobile augmented reality;target acquisition;fitt s law	Rohs and Oulasvirta (2008) proposed a two-component Fitts' law model for target acquisition with magic lenses in mobile augmented reality (AR) with 1) a physical pointing phase, in which the target can be directly observed on the background surface, and 2) a virtual pointing phase, in which the target can only be observed through the device display. The model provides a good fit (R2=0.88) with laboratory data, but it is not known if it generalizes to real-world AR tasks. In the present outdoor study, subjects (N=12) did building-selection tasks in an urban area. The differences in task characteristics to the laboratory study are drastic: targets are three-dimensional and they vary in shape, size, z-distance, and visual context. Nevertheless, the model yielded an R2 of 0.80, and when using effective target width an R2 of 0.88 was achieved.	ar (unix);augmented reality;fitts's law	Michael Rohs;Antti Oulasvirta;Tiia Suomalainen	2011		10.1145/1978942.1979343	three-dimensional space;computer vision;augmented reality;simulation;field experiment;computer science;fitts's law;computer graphics (images)	HCI	-43.505965502803036	-49.1060150202566	30292
4bdd54389c4bcaf8cb01398ad24b259b87aea1fe	rhythmic attention in child-robot dance play	robot sensing systems;rhythm;pediatrics;keepon robot;human robot interaction;robot kinematics rhythm human robot interaction robot sensing systems robustness speech educational robots appropriate technology educational technology design for experiments;pressure sensor;child robot dance play;human social behavior;rhythmic attention;lead;social behavior;synchronization;rhythmic interaction;pressure sensors;music human robot interaction;rhythmic interaction rhythmic attention child robot dance play human social behavior keepon robot pressure sensors;music;robot kinematics	Human social behavior is rhythmic, and synchrony plays an important role in coordinating and regulating our interactions. We are developing technology that allows the robot Keepon to perceive and behave rhythmically, and to synchronize its dancing behaviors to music or to children's movement as perceived using pressure sensors. We present two experiments in which Keepon dances with children to music, and in which the robot's rhythmic attention and role of leader or follower are manipulated in order to examine the effects on engagement and rhythmic synchrony. We found that children can assume the roles of leader or follower in a rhythmic interaction, that followers indeed tend to synchronize with the robot's movements, and that the role of follower causes the children to more closely follow a musical rhythm.	experiment;interaction;keepon;robot;sensor	Marek P. Michalowski;Reid G. Simmons;Hideki Kozima	2009	RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication	10.1109/ROMAN.2009.5326143	human–robot interaction;simulation;computer science;artificial intelligence;pressure sensor;multimedia	Robotics	-51.269771509997106	-50.60960213511214	30410
e35538eb3c1eb8144164bc3216edb94ddf7d63f6	a type-2 fuzzy logic based system for asset geolocation within augmented reality environments		This paper presents a type-2 Fuzzy Logic System (FLS) to support technical employees in finding companyu0027s assets in outdoor settings. The system provides the user with directions for asset location by comparing his/her current position with assetsu0027 location in real-time, giving auditory and visual feedback via a Head Mounted Display (HMD). We carried out 35 path explorations in a predefined area to test the system. The results indicated that the proposed type-2 fuzzy logic produces better performance than the type-1 based fuzzy system, giving more precise indications to reach assetu0027s position.	augmented reality;free library of springfield township;fuzzy control system;fuzzy logic;geographic coordinate system;geographic information system;geolocation;head-mounted display;real-time transcription	Anasol Peña-Ríos;Hani Hagras;Michael Gardner;Gilbert Owusu	2017	2017 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2017.8015627	simulation;fuzzy logic;augmented reality;asset location;computer science;optical head-mounted display;geolocation;fuzzy control system	Robotics	-39.092930326261595	-42.90009625157544	30438
7ca25b8a1dadad55869f5816aaa9e59bdcffcf77	kathryn e. merrick: computational models of motivation for game-playing agents		Merrick’s book helps game designers, programmers and researchers to create NonPlayer Characters (NPCs, i.e. agents) that have human like motives. She suggests that adding motives will create a richer and more life-like user experience for game players. Her book is very well written overall, and it would be of great interest to game developers, game designers and programmers. It will introduce most of them to new concepts that range from game theory and multi-agent systems to the philosophy and psychology of power. It delivers what it sets out to do. After reading it you should be able to create more interesting characters for your game. Readers interested in evolutionary processes will recognise notions of co-evolution and evolutionary game theory; note that in line with the rest of the book, evolution is not introduced as an optimisation process in the strict sense, but rather as a tool for getting and/or understanding interesting (population) behaviour. The book is split in four parts. Part I, which includes chapters 1, 2 and 3, discusses human motivation: how it can be modelled and how one can embed it in (artificial) agents. Part II, i.e. chapters 4 and 5, discuss how one can take inspiration from it and create individual profiles for motivated agents. Part II effectively applies methods from the previous chapters in a more applied context. Part III includes chapters 6, 7 and 8 and aims at providing useful scenarios for motivated agents as opponents, collaborators or story support characters. Part IV includes chapter 9 which discusses scenarios where motivated agents change their behaviour over time. Chapter 10 wraps up the book by providing further insights and proposing future directions, which include further things to read and further research	computation;computer simulation;game theory;mathematical optimization;multi-agent system;programmer;user experience;video game developer	Spyridon Samothrakis	2018	Genetic Programming and Evolvable Machines	10.1007/s10710-018-9323-6	machine learning;computer science;computational model;artificial intelligence	HCI	-56.74152091296261	-30.344852058405362	30492
95c4775f6b46bbefa761df206a1ed9042703392e	exploring the interaction between visual flux and users on mobile devices		This research introduces the concept of visual flux to depict the process of the physical interaction between users and mobile devices in the scenario of controlled visual stimulation over various screen dimensions. Position sensors of mobile devices and motion sensors are applied in this research to incorporate the effects of corresponding distance and orientation between devices and users. A reconstruction system is established to provide a pragmatic approach to the process of user activities. The experimental results indicate that the participant performance under the condition of the same number and even size of stimulation decreases with increasing device dimensions, and the correlation between the angle of view and participant performance is revealed. Future research based on this concept could be conducted to extend the screen dimensions and thereby discover the entire spectrum of the informative glasses.	mobile device	Shih-Wen Hsiao;Yi-Cheng Tsao	2017		10.1007/978-3-319-58637-3_16	flux;human–computer interaction;angle of view;position sensor;user experience design;mobile device;computer science	HCI	-46.534645969954305	-44.29824270430556	30516
ef2dc793280be9e9c40fc4b1b22c06eb20a072f5	analysis and modelling of affective japanese sitting postures by japanese and british observers	psychology cultural aspects emotion recognition;emotion estimation model;emotion recognition;observers legged locomotion accelerometers educational institutions sensor phenomena and characterization neck;psychology;pressure sensor;accelerometer sitting posture cross cultural analysis emotion estimation model pressure sensor;cross cultural analysis;accelerometer;cultural aspects;trunk position affective japanese sitting posture analysis affective japanese sitting posture modelling japanese observers british observers body gestures body postures nonverbal communication affective standing postures affective dimensions arousal dimension valence dimension potency dimension avoidance dimension cross cultural differences facial expression perception cross cultural perception body expressions minimal universality sitting posture perception body descriptors japanese body postures dominance rating modulation leg position arm position;sitting posture	Not only facial expressions but also body gestures and postures play an important role in non-verbal communication. Whilst evidence shows that affective standing postures account for at least four affective dimensions - arousal, valence, potency and avoidance - it is not clear if the same is true for sitting postures. In addition, whilst there is a large body of work investigating cross-cultural differences in the perception of facial expressions, there is very little work on the cross-cultural perception of body expressions. We investigated the minimal universality and cross-cultural difference in the perception of sitting postures and the body descriptors that guide these perceptual processes. Japanese body postures were collected and measured through conventional sensors. Japanese and British observers were recruited for the cross-cultural study. The results show that, for Japanese observers, the three dimensions of arousal, valence and dominance were necessary to account for most of the variance in the perception of the set of affective postures whilst, for British observers, only valence and arousal were necessary. An analysis of the body descriptors highlighted a few differences in the way these were associated with affective dimensions. It also showed that, for Japanese observers, the rating of dominance was modulated by the position of the legs, arms and trunk.		Tatsuya Shibata;Akito Michishita;Nadia Bianchi-Berthouze	2013		10.1109/ACII.2013.22	psychology;developmental psychology;pressure sensor;communication;social psychology;accelerometer	NLP	-51.15293420536203	-51.25982766093122	30525
485131258b76a211d9f3b7a2b59ba719a4180e75	audio–vision substitution for blind individuals: addressing human information processing capacity limitations	object recognition;performance evaluation;training;wearable computer audio vision substitution device blind individuals human information processing capacity limitations sensory substitution devices tactile devices voice visual objects recognition sonifications 2d images visual modalities tactile modalities assistive technology;visualization shape performance evaluation training object recognition information processing signal processing;sensory substitution assistive technology blindness wearable computers;visualization;shape;signal processing;information processing;wearable computers assisted living audio signal processing computer vision handicapped aids haptic interfaces object recognition	In this contribution, we consider the factors that influence the information processing capacity of the person using sensory substitution devices, and the influence of how the translated information, here in audio, impacts performance. First, we review aspects of vision substitution by tactile and audio devices, and then we review key theory in human information processing limitations to devise and test use of an audio-vision substitution device, The vOICe, for recognizing visual objects with audio substitution for vision. Participants heard sonifications of two-dimensional (2-D) images and had to match them to alternatives presented either in visual or tactile modalities. To assess whether capacity limits constrain performance, objects were either presented with all information simultaneously (top and bottom as whole objects), or successively (top and bottom of the object one after the other). Performance was superior in the successive trials, indicative of a capacity limit in processing the auditory information. We discuss the implications for training protocols and design to provide a useful accessibility device for blind individuals.	accessibility;computer performance;information processing;sensory substitution;visual objects	David J. Brown;Michael J. Proulx	2016	IEEE Journal of Selected Topics in Signal Processing	10.1109/JSTSP.2016.2543678	computer vision;visualization;information processing;shape;computer science;cognitive neuroscience of visual object recognition;signal processing;multimedia	Visualization	-46.43355459768229	-46.79262725868453	30542
ece49f518a76393a3837f6ac44e9f6e472f901f0	presentation trainer: a toolkit for learning non-verbal public speaking skills	conference paper;sensor based learning;immediate feedback;demonstration;open source	The paper presents and outlines the demonstration of Presentation Trainer, a prototype that works as a public speaking instructor. It tracks and analyses the body posture, movements and voice of the user in order to give instructional feedback on non-verbal communication skills. Besides exploring the background of the feedback theory used by the prototype, this paper describes its conceptual implementation and proposes its use and deployment for the conference demonstration.	poor posture;prototype;software deployment	Jan Schneider;Dirk Börner;Peter van Rosmalen;Marcus Specht	2014		10.1007/978-3-319-11200-8_56	simulation;human–computer interaction;computer science;artificial intelligence;multimedia;world wide web	HCI	-54.82867945171254	-46.257603784011316	30544
0667c015cf82ed532584f87d9ac4d914c00c02d0	dynamic evacuation planning by visual analytics - an expert survey		In a formative user study, we evaluated the requirements of visual analytics for dynamic evacuation planning. To this end, we created a prototype that implements the visual analytics methodology and is able to compute, visualize, and interact with evacuation routes in emergency situations in buildings. Using this prototype, we conducted an expert survey including a psychologist, a consultant for building safety measures, and a building planner. The survey provides essential information needed to build a tool that is able to evacuate people inside a building safely. Additionally, we report on results of the survey regarding technical limitations in obtaining data such as the evacuees’ position and their number to calculate the shortest routes during evacuation.	expert system;prototype;requirement;sensor;usability testing;visual analytics	Peter Hoffmann;Markus Höferlin;Andreas Kirstädter;Daniel Weiskopf	2013				HCI	-34.15991889150373	-32.5156693320064	30547
ef921ca143d622ac9cd731801d407728127c882c	perception and emotion modeling of avatars in ive	avatars facial animation humans virtual environment face recognition cloning computational modeling deformable models visual perception skeleton;moving object;object recognition;facial muscular model intelligent virtual environment emotion modeling perception sensor visual perception rule based transition;intelligent virtual environment;emotion recognition avatars visual perception object recognition knowledge based systems face recognition;rule based;emotion recognition;3d model;face recognition;avatars;visual perception;facial expression;virtual actor;knowledge based systems;virtual worlds	In intelligent virtual environment (IVE), it is always a challenge research issue for the virtual actor (avatar) to have the ability of visual perception and response to the virtual world. And facial expression and walk behavior of avatars play an important role in IVE. In our method, the avatar is considered as an agent, it recognizes 3D objects in the environment and calculates the direction of the moving objects by using the avatar's perception sensor, avatar's facial expression is rule-based transition. Avatar walk behavior is simulated by skeleton model and is denoted by seven-motion sequence. Facial expression of individuals can be simulated after expression cloning and face 3D model reconstruction based on facial muscular model. Experimental results show the efficiency of our approach.	avatar (computing);logic programming;virtual actor;virtual reality;virtual world	Ronghua Liang;Zhongyu Chen;Xujia Qin;Xiao Zhou;Liang Tang	2004	2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)	10.1109/ICSMC.2004.1400768	rule-based system;facial recognition system;computer vision;virtual actor;visual perception;computer science;artificial intelligence;cognitive neuroscience of visual object recognition;knowledge-based systems;multimedia;facial expression	Robotics	-35.034407406960014	-38.93662718728075	30553
f336141bb12ad0359477944a93f076e3d2787161	challenges in speech-based human-computer interfaces	dialogue system;emotion recognition;dialogue management;intelligence;spoken language dialogue systems;reasoning;dialogue manager;human computer interface	In this article we present an overview of our current research activities falling into the scope of developing advanced spoken language dialogue systems. These systems need to react flexibly and adaptively depending on the current status of the user and the situation of use. In particular, they require emotion recognition and adaptive dialogue management techniques. Advanced dialogue systems also need proactive capabilities to act as intelligent assistants to their users.	blue (queue management algorithm);coherence (physics);dialog system;emotion recognition;endeavour (supercomputer);floor and ceiling functions;next-generation network;systems architecture;usability	Wolfgang Minker;Johannes Pittermann;Angela Pittermann;Petra-Maria Strauß;Dirk Bühler	2007	I. J. Speech Technology	10.1007/s10772-009-9023-y	natural language processing;intelligence;speech recognition;computer science	AI	-50.58591781763614	-34.77244150650211	30568
797513469c9c44cb6f46c451799a287117233e59	easing interaction through user-awareness	groupware;computer supported cooperative work;computer human interaction;intelligent agents;eurecom ecole d ingenieur telecommunication centre de recherche graduate school research center communication systems;intelligent agent;mediaspace;multi agent architecture	In the context of CSCW (Computer Supported Cooperative work), we propose to ease the interaction between users through the use of user-aware agents. The purpose of those agents is to be aware of the user’s tate (e.g. is the user typing on the keyboard, meeting with o herpeople, on the phone, etc.). We will first describe an application we developed on top of a Mediaspace, EasyMeeting, based on user-aware agents. Second, we will present the implementation (multi-agent architecture, language). Finally we will discuss various aspects ofthe agents. We believe, the user-aware agents are a step toward a better communication man-machine. Instead of the usual approach in which users consciously interact with the machine, we make the computer aware of the users and thus make users unconsciously interact with the computer.	agent architecture;computer-supported cooperative work;multi-agent system	Alain Karsenty	1997		10.1145/238218.238329	simulation;human–computer interaction;computer science;knowledge management;artificial intelligence;computer-supported cooperative work;intelligent agent;collaborative software	HCI	-52.328757658183875	-38.55043625322279	30598
f94a6795606ddd006be54ecd7299f68652049c58	towards minimalism and expressiveness in interactive drama	author centered design;animation engine;serveur institutionnel;interactive narrative;archive institutionnelle;open access;character animation;authoring;interactive drama;archive ouverte unige;cybertheses;minimalism;authoring tool;use case;institutional repository;virtual worlds	Having found it difficult for authors to be creative with current interactive drama systems, we propose an alternative author-centered approach allowing authors of nonlinear media to express themselves easily and smoothly.  At the level of character animation, a bottleneck in Interactive Drama, our idea is to build authoring tools and author-centered engines for interactive drama that make it possible for an author to create virtual worlds, characters and animation instantly. To achieve this goal, a minimalist design for each of those features is proposed. A Use Case is described to better illustrate the approach.	integrated development environment;interactive storytelling;minimalism (computing);nonlinear system;smoothing;virtual world	Nicolas Szilas;Jue Wang;Monica Axelrad	2008		10.1145/1413634.1413703	use case;character animation;human–computer interaction;minimalism;computer science;multimedia;world wide web;computer graphics (images)	HCI	-41.94996504148696	-33.13962261203781	30605
5fc7fbd1eea4be72cbdd73897c1752ac0ac41f8a	all the world's a stage: what makes a wearable socially acceptable			wearable technology	Norene Kelly	2017	Interactions	10.1145/3137093	human–computer interaction;multimedia;wearable computer;engineering	Vision	-52.96548416599321	-35.412949682707996	30618
f83cab4eed244cbf46bb24a27bffcd49d2d6843b	striking a balance between free and guided exploration - conceptualizing support for exploratory learning environments	computer science and information systems	Exploratory Learning Environments (ELE) are virtual environments that adhere to constructivist theories of learning emphasizing learner control. However, research suggests that lack of sufficient explicit support may undermine their effectiveness. Advanced technologies provide opportunities to supply learners with the right information at the right time. This workshop, 3rd in a series, focuses on striking a balance between free and guided exploration and provides a forum for conceptualizing and raising requirements for support in ELE.		Ido Roll;Manolis Mavrikis;Sergio Gutiérrez Santos	2010			simulation;engineering;knowledge management;management science	Robotics	-62.823660518946426	-35.33267282374293	30643
1fd1d9ca5591c5269cb9a0c52a4d36dadc207bd8	multimodal interaction in a haptic environment	human computer interaction;virtual reality;biomedical education intelligent tutoring systems multi agent systems virtual reality haptic interfaces human computer interaction graphical user interfaces;haptic interfaces needles speech cotton virtual reality keyboards graphics multiagent systems medical services instruments;multi agent systems;graphical user interfaces;intelligent tutoring systems;biomedical education;biomedical education multimodal tutoring environment haptic device virtual tutor virtual patient multiagent system multimodal interaction haptic interface human computer interaction graphical user interfaces;haptic interfaces	In this paper we investigate the introduction of haptics in a multimodal tutoring environment. In this environment a haptic device is used to control a virtual piece of sterile cotton and a virtual injection needle. Speech input and output is provided to interact with a virtual tutor, available as a talking head, and a virtual patient. We introduce the haptic tasks and how different agents in the multi-agent system are made responsible for them. Notes are provided about the way we introduce an affective model in the tutor agent.	haptic technology;input/output;modal logic;multi-agent system;multimodal interaction	Anton Nijholt;Sander Kole;Job Zwiers	2005	First Joint Eurohaptics Conference and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems. World Haptics Conference	10.1109/WHC.2005.98	simulation;human–computer interaction;computer science;artificial intelligence;graphical user interface;virtual reality;multimedia	Robotics	-45.581907287903014	-36.863224374141225	30653
bd8c32c0ada0fa645084e4a9b04475259f2151af	turning music theory on its ear do we hear what we see; do we see what we say?				Jeanne Bamberger	1996	I. J. Computers for Math. Learning	10.1007/BF00191471	humanities;computer science;mathematics education;music theory;transpersonal psychology	Theory	-54.05400076660645	-26.547454767114594	30680
84a3550c778d6af5cde33937bb2804b329d9d955	how people anthropomorphize robots	humans anthropomorphism robot sensing systems abstracts educational robots software;social context;robot interviewer anthropomorphism social context health interview human interviewer;social aspects of automation human robot interaction;human robot interaction;social aspects of automation;social robots;social robots human robot interaction;social robot	We explored anthropomorphism in people's reactions to a robot in social context vs. their more considered judgments of robots in the abstract. Participants saw a photo and read transcripts from a health interview by a robot or human interviewer. For half of the participants, the interviewer was polite and for the other half, the interviewer was impolite. Participants then summarized the interactions in their own words and responded true or false to adjectives describing the interviewer. They later completed a post-task survey about whether a robot interviewer would possess moods, attitudes, and feelings. The results showed substantial anthropomorphism in participants' interview summaries and true-false responses, but minimal anthropomorphism in the abstract robot survey. Those who interacted with the robot interviewer tended to anthropomorphize more in the post-task survey, suggesting that as people interact more with robots, their abstract conceptions of them will become more anthropomorphic.	interaction;robot	Susan R. Fussell;Sara B. Kiesler;Leslie D. Setlock;Victoria Yew	2008	2008 3rd ACM/IEEE International Conference on Human-Robot Interaction (HRI)	10.1145/1349822.1349842	human–robot interaction;simulation;media lab europe's social robots;computer science;artificial intelligence;social robot	Robotics	-52.43067498273515	-51.182634458161104	30705
b631f38f72e87f456a5b08b53bc1ee3c96200659	human needs and mobile technologies: small, fast, and fun	energy conservation;new technology;mobile device;wireless;e commerce;sensor networks;mobile technology;tracking	"""The central thesis of """"Leonardo's Laptop"""" (MIT Press, 2002) is that designers who are sensitive to human needs are more likely to make the breakthroughs that yield new technologies successes. Therefore, a theory of mobile devices would focus on compact devices that support human relationships, provide salient information, and enable creative expression. The foundations are not only the megahertz of connectivity, but also the usability and universality of interfaces. Demonstrations include digital photo applications, personal info, healthcare, and e-commerce."""	digital photography;e-commerce;laptop;mobile device;theory;universality probability;usability	Ben Shneiderman	2004		10.1145/990064.990065	e-commerce;mobile search;simulation;wireless sensor network;energy conservation;human–computer interaction;computer science;operating system;mobile technology;mobile device;tracking;multimedia;mobile computing;wireless	Mobile	-56.605255910743324	-39.744706674142115	30762
e7253f334590a2014717ff80f6460213b9acea77	an empirical study of use contexts in the mobile internet, focusing on the usability of information architecture	information architecture;empirical study;context information;ke yw ords;conceptual framework;exploratory study;mobile internet;context;usability problem	The mobile Internet--accessing the Internet via a mobile device--has become quite popular recently. The mobile Internet is mainly different from the stationary Internet because it may be used in various contexts, whereas the stationary Internet is mostly used in pre-determined environments. However, it is unclear when the mobile Internet is used most frequently, and in what context it is most useful. A greater understanding of the contexts for using the mobile Internet will relieve usability problems that mobile Internet users often encounter.#R##N##R##N#This paper proposes a conceptual framework of use contexts, which includes various facets of contexts related to the mobile Internet. It then presents the results of an exploratory study in which the use contexts for the mobile Internet and corresponding usability problems have been empirically monitored. The results indicate that use of the mobile Internet is heavily clustered around a few key contexts, rather than dispersed widely in diverse contexts. Moreover, different contexts are found to cause different kinds of usability problems. The paper ends with theoretical and practical implications of the study results.	information architecture;usability	Hoyoung Kim;Jinwoo Kim;Yeonsoo Lee	2005	Information Systems Frontiers	10.1007/s10796-005-1486-z	mobile search;computer science;conceptual framework;multimedia;internet privacy;empirical research;world wide web;information architecture;exploratory research	HCI	-57.15640273083463	-42.797437398568974	30771
0dcc8f88759f328dd60e0a840326b22b2b5f6828	alternative reality: a new platform for virtual reality art	software tool;intelligent virtual environment;meetings and proceedings;book chapter;game engine;virtual reality;virtual reality for art and entertainment;artificial intelligent;intelligent virtual environments;qualitative physics;virtual environment;modelling and simulation	Virtual Reality Art involves the design of artificial worlds that offer new experiences to spectators. An important aspect for the development of VR Art installations is the principled definition of behaviour for the environment as a whole, which would facilitate experiments with alternative laws of physics, time, and causality. We describe the first results of an ongoing project dedicated to the development of software tools for the use of Intelligent Virtual Environments in VR Art. Using the architecture of a state-of-the-art game engine, we have developed Artificial Intelligence techniques that support the definition of alternative laws of physics. After discussing the principles behind alternative reality we describe two complementary modes of description for alternative behaviour: qualitative physics and causal simulation. This is illustrated by examples integrated into the virtual environment.	ascii art;artificial intelligence;causality;experience;experiment;game engine;simulation;virtual reality	Marc Cavazza;Simon Hartley;Jean-Luc Lugrin;Mikael Le Bras	2003		10.1145/1008653.1008672	computer-mediated reality;artificial reality;simulation;human–computer interaction;computer science;virtual machine;artificial intelligence;instructional simulation;metaverse;virtual reality;mixed reality;multimedia	AI	-53.20769164681015	-30.63832720247472	30819
11b86db95d0593fa144a24f0d7471d4ac51c99c8	the drawstream station or the avcs of video cocktail napkins	groupware;user interfaces multimedia systems groupware design engineering cad;human computer interaction;design engineering;cad;asynchronous video conversations drawstream station avcs video cocktail napkins multimedia system collaborative design processes problem characteristics system requirements high quality multimedia infrastructure space above space below interface virtual representations control interface logical representations threaded conversation genre systems;multimedia systems;cscw;collaborative design;automatic voltage control space technology process design collaborative work merging control systems displays internet sociotechnical systems streaming media;user interfaces;applications;genre	This paper reports on the development of a multimedia system to support collaborative design processes. The DrawStream Station was developed through a cycle of observation of real work, identification of problem characteristics and system requirements, development of the technology, and observation of the technology in use. The system is a combination of a high quality multimedia infrastructure and a particular form of “Space Above / Space Below” interface merging real and virtual representations. The control interface mixes a variety of temporal and logical representations to display a threaded conversation. We report on the use of the system as a way to understand the dynamics of genre systems. Two nascent genres are identified: Asynchronous Video Conversations and Video Cocktail Napkins.	display resolution;requirement;system requirements	Steve R. Harrison;Scott L. Minneman;Joshua Marinacci	1999		10.1109/MMCS.1999.779259	simulation;human–computer interaction;computer science;operating system;computer-supported cooperative work;cad;multimedia;user interface;information technology;world wide web	HCI	-46.485271118808164	-27.30932013076301	30862
d118fb163c16ed6d08ec4a074d8a8b34b7a11030	web-based multipointer interaction on shared displays	collocated interaction;multipointer interaction;shared displays;web technology	Interaction with multiple mouse pointers is becoming widespread for collocated collaboration on shared displays, but most technologies used to implement it do not support telepresent users. While web technologies are a common standard for applications that enable telepresent collaboration, they do not support the multipointer interaction needed for collaboration in collocated settings. We propose an approach to provide multipointer interaction in web applications by adding multipointer support to the web browser and addressing pointer handling in a JavaScript framework.	javascript framework;pointer (computer programming);web application	Muriel Bowie;Oliver Schmid;Agnes Lisowska Masson;Béat Hirsbrunner	2011		10.1145/1958824.1958926	web modeling;human–computer interaction;computer science;multimedia;world wide web	HCI	-42.7923304588176	-27.209210226958824	30887
925339ed1f7ad36d9119c01be633e1030457a966	developing games with magic playground: a gesture-based game engine	human computer interaction;usability evaluation;real time;game engine;conferenceobject;gesture based human computer interaction	This paper presents Magic Playground, a game engine that enables the development of entertainment applications with realtime gesture-based Human-Computer Interaction (HCI). We describe the main architectural elements of our system and provide a guideline on how to program the engine in order to create games. Finally, we present usability evaluation results of a game, which emulates the known Tetris game1	emulator;game engine;human–computer interaction;tetris;usability	Carolina Cabral;Juana Dehanov;José Miguel Salles Dias;Rafael Bastos	2005		10.1145/1178477.1178548	video game design;game design;game development tool;simulation;level design;human–computer interaction;computer science;game mechanics;game art design;multimedia;game design document;video game development;game programming;game testing	HCI	-51.05532348988248	-37.52963802687853	30888
4937c06b54ba9eb1352c39bbf3cc19759899db86	fitts at 50: for link design, size does matter	link design	© ACM 1072-5220/05/0500 $5.00 I missed the 50th anniversary of Paul Fitts’ paper The information capacity of the human motor system in controlling the amplitude of movements. I wasn’t exactly expecting street parties, but June 2004 slipped by without even a celebratory cup of tea. The title of the paper may not mean much on its own, but I end up discussing Fitts’ Law with interaction designers at least once a week, so I would count it as one of the more robust works in our field. The conversations usually go something like this:	channel capacity;fitts's law;interaction design	William Hudson	2005	Interactions	10.1145/1060189.1060231	human–computer interaction;engineering	HCI	-55.81967473891264	-23.958336676660455	30896
042b77581c4be3a06f1e5ee72c6c983486d9b9ec	dynamically simulated characters in virtual environments	user participation;real time;virtual reality;motion estimation;bicycles computational modeling animation virtual environment robots testing navigation games libraries real time systems;motion estimation virtual reality digital simulation computer animation;dynamic simulation;interactive virtual environment;synthetic competitors dynamically simulated characters virtual environments animated characters motion compelling virtual environment interesting behavior complex behavior synthetic characters border collie environment olympic bicycle race environment;virtual environment;computer animation;digital simulation	nimated characters can play the role of teachers or guides, teammates or competitors , or just provide a source of interesting motion in virtual environments. Characters in a compelling virtual environment must have a variety of complex and interesting behaviors, and be responsive to the user's actions. The difficulty of constructing such synthetic characters currently hinders the development of these environments, particularly when realism is required. In this article, we present one approach to populating virtual environments—using dynamic simulation to generate the motion of characters. We explore this approach's effectiveness with two virtual environments: the Border collie environment, in which the user acts as a Border collie to herd robots into a corral, and the Olympic bicycle race environment, in which the user participates in a bicycle race with synthetic competitors (see Figure 1). Motion for characters in virtual environments can be generated with keyframing, motion capture, or dynamic simulation. All three approaches require a tradeoff between the level of control given to the animator and the automatic nature of the process. Animators require detailed control when creating subtle movements that are unique or highly stylized. Generating expressive facial animations usually requires this low level of control. Automatic methods are beneficial because they can interactively produce motion for characters based on the continuously changing state of the user and other characters in the virtual environment. Keyframing requires that the animator specify critical , or key, positions for the animated objects. The computer then fills in the missing frames by smoothly interpolating between those positions. The specification of keyframes for some objects can be partially automated with techniques like inverse kinematics. However, keyframing still requires that the animator possess a detailed understanding of how moving objects should behave over time as well as the talent to express that information through the character's configuration. A library of many keyframed animations can be generated offline and subsequently accessed in an interactive environment to provide the motion for a character that interacts with the user. In motion capture, one of the most commonly used animation techniques, magnetic or vision-based sensors placed on an actor record the positions of body parts or joint angles as the actor performs a desired action. This recorded motion is then played back through a graphi-cal character. Motion capture is growing in popularity because of the relative ease of recording many human actions. In particular, sports video games often use …	facial recognition system;flash animation;interactivity;interpolation;inverse kinematics;key frame;motion capture;online and offline;population;robot;sensor;simulation;smoothing;synthetic intelligence;virtual reality	David C. Brogan;Ronald A. Metoyer;Jessica K. Hodgins	1998	IEEE Computer Graphics and Applications	10.1109/38.708561	computer vision;dynamic simulation;simulation;computer science;virtual machine;instructional simulation;operating system;motion estimation;virtual reality;computer animation;multimedia;computer graphics (images)	Graphics	-39.10838111596038	-36.7497347366186	30917
29629f9990daf66a3e0c27cace674d7240f9b861	grounding verbs for tool-dependent, sensor-based robot tasks		Speech-based robot instruction is a promising field in private households and in small and medium-sized enterprises. It facilitates the use of robot systems for experts as well as non-experts, even while the user executes other tasks. The common approach is to map action verbs to robot tasks represented by well-defined interfaces. While this method works well for a wide variety of robot motions, it contains the necessity for defining additional robot tasks for each change in the physical properties of the manipulated objects. To overcome this drawback, we contribute a novel approach for defining complex and sensor-based robot tasks called Combined Verbalized Effects. It allows to systematically ground action verbs to sensor-based robot motions, while considering accessible tools and highlighting the motion primitive combinations complex motions are made of. Furthermore, real-world examples and a user study show the applicability of our approach.	arm architecture;elemental;german research centre for artificial intelligence;robot;sensor;usability testing	Kim Wolfel;Dominik Henrich	2018	2018 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)	10.1109/ROMAN.2018.8525827		Robotics	-37.753087238146385	-41.057587008257464	30934
af70e13dce233f03b232f52d4c4e72d281a0ceb2	viennar: user-centered-design of a bring your own device mobile application with augmented reality		In many museums it is still common that visitors have to read static texts from boards to gain information about the exhibits. In times where almost every visitor carries a smartphone in their pocket, these devices could be utilized for a more personalized and interactive visitor experience. In this paper we present a design study for a “Bring your own device” setting that combines augmented reality (AR) and navigation in museums. We applied an iterative user centered design approach that included conceptual design, prototyping, user tests, as well as a field test in a large museum in Vienna. One of the main results is that a new and digital form of navigation isn’t as essential as the museum thought it would be. Apart from that the application was well received during the field test.	augmented reality;bring your own device;mobile app	Andrea Schönhofer;Sabine Hubner;Perihan Rashed;Wolfgang Aigner;Peter Judmaier;Markus Seidl	2018		10.1007/978-3-319-95282-6_21	multimedia;visitor pattern;bring your own device;conceptual design;augmented reality;user-centered design;computer science	HCI	-50.17319029876654	-38.36847709599824	30977
9eaa67e35bdc80ae09e46c3b7af584df46dd0785	face to face - rick sammon's complete guide to photographing people	face to face	It sounds good when knowing the face to face rick sammons complete guide to photographing people in this website. This is one of the books that many people looking for. In the past, many people ask about this book as their favourite book to read and collect. And now, we present hat you need quickly. It seems to be so happy to offer you this famous book. It will not become a unity of the way for you to get amazing benefits at all. But, it will serve something that will let you get the best time and moment to spend for reading the book.	book;sammon mapping	Rick Sammon	2008			speech recognition;art;aesthetics;artificial intelligence	HCI	-60.3226093767356	-25.72136554484375	30990
dc2cb43a6d5a117b9fbc29da4767ce4ed05cf663	imagination amplification	computer graphics;user interfaces	It would be easy to gaze into the distant future and speculate what image synthesis technology may be. I hope to relate some of my own inspiration for getting involved with computer graphics, an inspiration that is still an unrealized dream, one that I am uncertain how to fully achieve. My interest in computer graphics can best be summed up as a desire to create tools for imagination amplification.		Michael F. Cohen	2000	IEEE Computer Graphics and Applications	10.1109/38.814562	computer vision;human–computer interaction;computer science;artificial intelligence;operating system;multimedia;computer graphics;user interface;computer graphics (images)	Visualization	-56.185613100680406	-29.118905507027517	30993
30afb5fc19fd3acef3db25fb67313e7fbbe77277	user experience quality in multi-touch tasks	mouse;input device;collaborative work;task;multi user;input;video game;scientific visualization;complex data;multi touch;user experience;large screen;target acquisition	In this paper, we present an updated set of experimental tasks and measures for large multi-touch (MT) input devices. In addition to a multi-user condition, we have employed an updated set of tasks, as well as subjective measures for user enjoyment. In the first experiment (a target acquisition task with two moving targets), the MT was more efficient than the mouse. Surprisingly, we found that the reduced accuracy of MT did not affect the perceived usability, or the enjoyment of the users. In the second experiment (a multiple shapes docking task), the MT was again more efficient and enjoying than the mouse. In the two-user condition, we found that performance and enjoyment was always higher than the single-user conditions, regardless of input device and task. Besides the quantitative results, we observed that users employed diverse interaction strategies in the MT condition, such as bi-manual input. The proposed tasks and the results support the use of MT in entertainment applications (multimedia and video-games), collaborative work, and scientific visualizations with complex data.	docking (molecular);input device;multi-touch;multi-user;usability;user experience	Ioannis Leftheriotis;Konstantinos Chorianopoulos	2011		10.1145/1996461.1996536	simulation;human–computer interaction;computer science;multimedia	HCI	-45.481488501574724	-46.668308476151594	31006
b39ca1c4ce0123976afeec2084e8c53c457cb34b	software cursor algorithm for interactive graphics	interactive graphics	bKMLb YOU LIKE 1. A DIRECTDRY OF THE bRTA DISKETTE 2. ROOM LEFT ON THE DISKETTE FOR DFlTFl 2. A PRINT OUT W R SPECIFC EXPERIHENT t. 13 GRAPH OF A SPECIFIC EXf’LRIWNT 5. A GFzWH OVER f~ USER SFECIFIEb INTERWL 6. IiRfRK FOR fd’4 ENTIRE DRTA DISKETTE 1 MITE MTERVIWIION FROM R bVL INDICRTOR t~?WkIPlEN7 8. AVEFSiL AHI, STt-iNDfikb EW3TION OF R 543 OF WIN-K 9. ‘I’il CfMtLCT A DlRECToRY ENTW 10. 1URVWFGERGlXWWDATFISETS 78 IFU’U’I TH EfXXT NRfiE OF 1tL FILE “NOISE Ut‘lUMTRETICN	ahi (amiga);algorithm;cursor (databases);floppy disk;graphics	F. James Holler;S. R. Crouch;Christine G. Enke	1980	Computers & Chemistry	10.1016/0097-8485(80)85009-1	biology;computer architecture;vector graphics;2d computer graphics;computer graphics metafile;chemistry;computer hardware;computer science;computer graphics lighting;real-time computer graphics;graphics software;computer graphics;graphics address remapping table;turtle graphics;general-purpose computing on graphics processing units;software rendering;3d computer graphics;computer graphics (images)	AI	-46.790025947318554	-29.47240926049095	31017
874e35db0d8ccc53ad8c1c080d09bc6a38736220	photorealistic visualization techniques for using spatial augmented reality in the design process			augmented reality	Christoffer Menk	2012				HCI	-47.8165175274218	-32.73410266267733	31041
3a99c39ffdb6d778459d3d15759296a8dfb5ebd4	stroke-based text entry and other gestural interfaces - movement modeling and biometric authentication			authentication;biometrics	Ulrich Burgbacher	2015				HCI	-43.06127005560852	-41.54409319175195	31069
6f865a27021cac65af22d70cbf8d7bb1f298b558	a little goat builds the world - an interactive children story for tablets			tablet computer	Kamil Kamysz;Marcin Wichrowski	2014		10.1007/978-3-319-12337-0_27		NLP	-53.96680425443289	-31.76274594172419	31117
ba99322987124b3a3ecfe5d0f62a14618e244377	what makes young children active game players; ethnographic case study		Young children’s digital game play tends to be discouraged and policed, and controlled by adults, who nevertheless give their young children mobile phones and hand held game devices to keep them occupied. The purpose of this study is to uncover the tactics used by three-year-old children as digital game players and the strategies used by their parents to put limits on this play. The method is an ethnographic case study of six families having a three-year-old child playing digital games on a daily basis. This study shows that threeyear-old children are active, avid digital gamers, and also adept at employing a range of tactics to gain access to opportunities to play.	mobile device;mobile phone	Youn Jung Huh	2014			simulation;engineering;multimedia;communication	HCI	-58.82499745247918	-44.61793527984215	31168
64a46241f1595199e9d14e634c4865a53a7a720b	vol-a-tile &#8212; a tool for interactive exploration of large volumetric data on scalable tiled displays	tiled display;geophysics;hardware liquid crystal displays geophysics physics anatomy probes transfer functions tiles rendering computer graphics data visualization;transfer functions;data collection;texture mapping;liquid crystal displays;high performance networks;probes;physics;level of detail;solid object decomposition;data visualization;tiles;rendering computer graphics;anatomy;physically based modeling;hardware;scripps institution of oceanography	We present the current state of Vol-a-Tile, an interactive tool for exploring large volumetric data on scalable tiled displays. Vol-a-Tile presents a variety of features employed by scientists at the Scripps Institution of Oceanography on data collected from the Anatomy of a Ridge-Axis Discontinuity seismic experiment. Hardware texture mapping and level-of-detail techniques provide interactivity. A high-performance network protocol is used to connect remote data sources over high-bandwidth photonic networks.	communications protocol;interactivity;level of detail;reflections of signals on conducting lines;scalability;texture mapping	Nicholas Schwarz;Shalini Venkataraman;Luc Renambot;Naveen K. Krishnaprasad;Venkatram Vishwanath;Jason Leigh;Andrew E. Johnson;Graham Kent;Atul Nayak	2004	IEEE Visualization 2004	10.1109/VISUAL.2004.126	texture mapping;computer vision;computer science;level of detail;liquid-crystal display;multimedia;transfer function;data visualization;statistics;data collection;computer graphics (images)	Visualization	-34.70062873361801	-31.185085815738585	31169
fa842ce8d00f9751c7658201d38179ddb10445a1	composing for the (dis)embodied ensemble: notational systems in (dis)appearances	composition;notation systems;virtual reality;physical modeling;violin;controllers;physical model;string	This paper explores compositional and notational approaches for working with controllers. The notational systems devised for the composition (dis)Appearances are discussed in depth in an attempt to formulate a new approach to composition using ensembles that navigates a performative space between reality and virtuality.	virtuality (gaming)	Matthew Burtner	2003			visual arts;string;composition;simulation;physical model;artificial intelligence;virtual reality;violin	HCI	-46.615981647155195	-32.76200103200446	31172
ad9486215d9e73960ef2cbe07e5b44436b8b349f	an integrated approach of knowledge acquisition by the hypertext system concorde	typed links;knowledge acquisition;concorde		hypertext;knowledge acquisition	Martin Hofmann;Uwe Schreiweis;Horst Langendörfer	1990			human–computer interaction;artificial intelligence	NLP	-44.97994516493856	-26.072471145714534	31174
3964adc76e7a01793613eb5821ee951a1eeda8a0	the pretence awareness contexts and oscillating nature of coaching frames		Drawing on data from three studies, this paper argues that the learning and teaching of player coaching is an important frame of temporary motivation for players during gameplay. Furthermore, play framed temporarily as a coaching experience exhibits what Fine (1983) called the oscillating nature of engrossment and operates under the same kind of pretence awareness context (Glaser & Strauss, 1964) that he described in relation to role-playing games. We argue the teaching of a new game, or parts of a game, is a fleeting yet recurring experience, with participants oscillating between regular mundane everyday play and coaching new players. The coach and other players are often expected to continue play as if they had not seen any strategically important information during their time coaching and learning. This is of course a pretence, the implications of which are explored.		Mitchell Harrop;Martin R. Gibbs;Marcus Carter	2013			coaching;social psychology;psychology	HCI	-59.38187560451961	-33.540392143587034	31197
051ddd9c97941e8b7f12fa408f550321357c2183	neurophysiological and behavioral studies of human-swarm interaction tasks		This paper studies human-swarm interaction performance by evaluating both the neurophysiological and behavioral characteristics of human subjects. By utilizing our unique test facility, we conduct a series of real-world-scenario-inspired tasks in which subjects are asked to guide a group of ground robots with various configurations to arrive at a sequence of randomly assigned targets. A range of neurophysiological and behavioral sensors are used to measure how cognitive states, e.g., cognitive load, behaviors, e.g., gazes, and performances, e.g., success rate, of human performs unfold in real time as the tasks evolve. Through an analysis of changes in gaze and cognitive load, we gain a wider understanding of the mechanisms of task failure; most notably the difficulty of estimating the complete state of the robotic group. The results of this study can help to inform the design of efficient interaction policies which can maximize task effectiveness between humans and robot swarms.	elegant degradation;experiment;performance;randomness;robot;sensor;swarm	Gregory Bales;Zhaodan Kong	2017	2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2017.8122684	swarm behaviour;machine learning;computer science;artificial intelligence;electroencephalography;robot kinematics;cognition;cognitive load	Robotics	-48.52996656985125	-51.74134900318674	31215
008219daef6fe72baf1666aa9340380b650ed234	from user participation to user seduction in the design of innovative user-centered systems	system design;industrial design;social interaction	Many approaches have been proposed and used to design computer-based systems. Nevertheless, generally the outcomes are not satisfactory from various perspectives (e.g., user satisfaction, technology innovation, etc.). Within this paper —after analyzing strengths and weaknesses of some of these approaches— we propose a new ‘seductive’ way for designing innovative user-centered systems. This approach is based on the integration of the CSCW and industrial design cultures. One of the first experiences of its use is also discussed.	computer user satisfaction;computer-supported cooperative work;experience;point of view (computer hardware company);user-centered design	Antonietta Agostini;Giorgio De Michelis;Marco Susani	2000				EDA	-61.71495659321018	-37.46246910884043	31221
4e16f01335811fcf303087d68370167ea0de6ff4	supporting collaborative practices across wall-sized displays with video-mediated communication. (communication médiatisée par la vidéo pour les pratiques collaboratives à distance entre murs d'écrans)		Collaboration can take many forms, we might sit side by side to work on an artifact, stand around a table to manipulate shared objects, or stand in front of a large display to visualize big datasets. Technology has long provided support for these practices through many devices: desktop computers let two people work side by side on digital objects, tabletops let groups of people gather around shared data, and wall-sized displays support visualizing and manipulating large digital data sets. Their traits determine how people use them in co-located collaboration. But when collaborators are located remotely, to what extent does technology support these activities? In this dissertation, I argue that the success of a telecommunications system does not depend on its capacity to imitate co-located conditions, but in its ability to support the collaborative practices that emerge from the specific characteristics of the technology. I explore this question using wall-sized displays as a collaborative technology. Wall-sized displays are large and can present massive data sets at a high resolution, two traits that establish the behaviors that take place in these spaces. Notably, people physically navigate data, moving close to and far away from the display instead of zooming in and out, and walking towards objects instead of panning. These opportunities shape how people collaborate: they can simultaneously and independently navigate data, indicating far objects to each other through gestures; as well as perform tightly-coupled work, physically navigating data together while talking about it. To explore technological support for remote collaboration across wall-sized displays, I started by observing collaborators perform their daily work at a distance using low-fidelity and technological prototypes. These observations showed how video of the remote collaborator is used during collaboration, mainly for deictic gestures and for discussions, and guides the rest of the work in the dissertation. I then found through experiments that people can accurately interpret remote deictic instructions and direct gaze when performed by a remote collaborator through video, even when this video is not placed directly in front of the observer. This suggests that we can move video during remote collaboration while people physically navigate the space, without disrupting communication. Based on these findings, I built CamRay, a telecommunication tool that realizes collaboration across large interactive spaces. CamRay uses an array of cameras to capture users’ faces as they physically navigate data on a wall-sized display, and presents this video in a remote display on top of existing content. This tool is designed to explore collaboration needs across wall-sized displays and how to support them. I use CamRay to observe how pairs perform two different collaborative tasks at a distance. Based on these observations, I propose two ways of displaying video: Follow-Local and Follow-Remote. In Follow-Local, the	big data;desktop computer;digital data;experiment;gesture recognition;linear algebra	Ignacio Avellino	2017				HCI	-53.26610546413255	-39.309464755377476	31255
58a657e2b56d203185fa759bf7829cd7ba92764d	overcoming the lack of screen space on mobile computers	pilot study;t technology general;mobile device;mobile computer;interface design;qa75 electronic computers computer science;sonically enhanced widgets;qa76 computer software;non speech sound	One difficulty for interface design on mobile computers is lack of screen space caused by their small size. This paper describes a small pilot study and two formal experiments that investigate the usability of sonically-enhanced buttons of different sizes. The underlying hypothesis being that presenting information about the buttons in sound would increase their usability and allow their size to be reduced. An experimental interface was created that ran on a 3Com Palm III mobile computer and used a simple calculator-style interface to enter data. The buttons of the calculator were changed in size between 4×4, 8×8 and 16×16 pixels and used a range of different types of sound from basic to complex. Results showed that sounds significantly improved usability for both standard and small button sizes – more data could be entered with sonically-enhanced buttons and subjective workload reduced. More sophisticated sounds that presented more information about the state of the buttons were shown to be more effective than the standard Palm III sounds. The results showed that if sound was added to buttons then they could be reduced in size from 16×16 to 8×8 pixels without much loss in quantitative performance. This reduction in size, however, caused a significant increase in subjective workload. Results also showed that when a mobile device was used in a more realistic situation (whilst walking outside) usability was significantly reduced (with increased workload and less data entered) than when used in a usability laboratory. These studies show that sound can be beneficial for usability and that care must be taken to do testing in realistic environments to get a good measure of mobile device usability.	experiment;glossary of computer graphics;mobile computing;mobile device;palm os;pixel;usability;while	Stephen A. Brewster	2002	Personal and Ubiquitous Computing	10.1007/s007790200019	simulation;human–computer interaction;computer science;interface design;operating system;mobile device;multimedia;mobile computing	HCI	-47.10360838469562	-45.29819777188434	31283
91bacc46e6e4e97b10e63b542defc45036cf432c	a user study on touch interaction for user-perspective rendering in hand-held video see-through augmented reality		This paper presents a user study on touch interaction with hand-held Video See-through Augmented Reality (V-AR). In particular, the commonly used Device Perspective Rendering (DPR) is compared with User Perspective Rendering (UPR) with respect to both performance and user experience and preferences. We present two user study tests designed to mimic the tasks that are used in various AR applications.	augmented reality;usability testing	Ali Samini;Karljohan E. Lundin Palmerius	2016		10.1007/978-3-319-40651-0_25	augmented reality;computer-mediated reality	HCI	-43.143262059869784	-39.11268762665419	31292
32373b70c4cc69007d35e91df08a256880e64326	multi-touch tabletop system using infrared image recognition for user position identification	ftir panel;infrared image recognition;multi-touch gesture;system usability;tabletop system;user position identification	A tabletop system can facilitate multi-user collaboration in a variety of settings, including small meetings, group work, and education and training exercises. The ability to identify the users touching the table and their positions can promote collaborative work among participants, so methods have been studied that involve attaching sensors to the table, chairs, or to the users themselves. An effective method of recognizing user actions without placing a burden on the user would be some type of visual process, so the development of a method that processes multi-touch gestures by visual means is desired. This paper describes the development of a multi-touch tabletop system using infrared image recognition for user position identification and presents the results of touch-gesture recognition experiments and a system-usability evaluation. Using an inexpensive FTIR touch panel and infrared light, this system picks up the touch areas and the shadow area of the user's hand by an infrared camera to establish an association between the hand and table touch points and estimate the position of the user touching the table. The multi-touch gestures prepared for this system include an operation to change the direction of an object to face the user and a copy operation in which two users generate duplicates of an object. The system-usability evaluation revealed that prior learning was easy and that system operations could be easily performed.	chairs;computer vision;cursor (databases);deletion mutation;effective method;estimated;experiment;gesture recognition;hl7publishingsubsection <operations>;hidden surface determination;infrared rays;lightheadedness;mind;multi-touch;multi-user;numerous;physical therapy exercises;spectroscopy, fourier transform infrared;system usability scale;touchscreen;endopeptidase clp;meeting;sensor (device)	Shota Suto;Toshiya Watanabe;Susumu Shibusawa;Masaru Kamada	2018		10.3390/s18051559	engineering;infrared;multi-touch;effective method;computer vision;group work;shadow;artificial intelligence;gesture	HCI	-43.870997337328276	-42.942078760812684	31337
6a69202e96c79eca837970c2d1cff56f50d1d1d5	appreciative gis and strength-based community change		Abstract#R##N##R##N#Problem-solving is embedded deeply in the digital DNA of GIS. Most projects involving GIS 1.0 and 2.0 raise community weaknesses, failures and other problems and use them to motivate change. The projects often create a future that does not differ greatly from the past or last long. Although looking into community problems is important and sometimes necessary, the steps of problem-solving do not always leave people feeling happy. To inspire communities to create change based on their strengths and achievements, and to provide better participant experience, this article develops the concept of a new form of GIS called Appreciative GIS (AGIS). AGIS are grounded in the affirmative premise that every community, however challenged, is gifted with certain assets, potentials and strengths which, when effectively exploited, can take that community to a better and sustainable future. Based on Earth's internal structure, the article develops a useful layered community model where crust, mantle and core strata represent the community's body, life-draining and life-giving elements in that order. The model helps explain the different views of the community from current GIS and AGIS perspectives. A Total GIS (TGIS) cube is also developed and used to indicate that current GIS commonly focus on elements of the mantle or what we do not have, what we cannot do and what makes us weak. AGIS, on the other hand, dig up the core and cultivate what we have, what we can possibly do and what makes us strong. In a significant departure from the linear approach of problem-solving, the article suggests implementing AGIS through a 7G cyclical method. The article contends that although AGIS may not be the whole enchilada, they can serve as a beacon of hope and a powerful source of inspiration especially for individuals and groups who are perceived or made to perceive themselves as dysfunctional, weak or poor.	geographic information system	Paddington Hodza	2014	Trans. GIS	10.1111/tgis.12046	simulation;geography;artificial intelligence;ecology;cartography	HCI	-60.06722913121195	-29.876000445394073	31338
7698ee2d015abbcb1e8dfcf196736c43b0dc2173	gui change method according to roles of widgets and change patterns	software;iterative method;interfaz grafica;tecnologia electronica telecomunicaciones;logiciel;graphical interface;man machine dialogue;widget;metodo iterativo;codificacion;methode iterative;source program modification;coding;gui;graphic user interface;logicial;dialogo hombre maquina;tecnologias;grupo a;usability;interface graphique;codage;dialogue homme machine	To develop usable software, it is necessary to develop Graphical User Interfaces (GUIs) in iterative steps, such as evaluating the usability of GUIs and improving GUIs. In improving GUIs, developers are often required to modify both the GUI and the logic code of the software. In our research, to facilitate GUI improvement, we propose a method of automatically searching for code to be modified and suggesting how to modify them. To search for appropriate code to be modified, we define the roles of widgets according to their purpose and the patterns for how to change GUIs. In our method, how to change GUIs is specified, and then the parts of source programs that are required to be modified are searched for. Also, we classify methods for each widget according to their functions. Using this classification, a method of modifying the code that is searched for is suggested.	graphical user interface	Junko Shirogane;Hajime Iwata;Kazuhiro Fukaya;Yoshiaki Fukazawa	2008	IEICE Transactions	10.1093/ietisy/e91-d.4.907	human–computer interaction;computer science;operating system;graphical user interface;programming language;world wide web	SE	-37.590688948951815	-27.48134533127298	31349
44c27a03023696b683e6c0461ee5450e90c69fc5	improvisation with the toob	on the fly	The TOOB is a unique wireless electronic instrument I created to extend wind instrument performance techniques into the electroacoustic realm. Being originally a trumpet player, I wanted to utilize some of the skills of trumpet performance in a new way, creating and manipulating completely different sounds and musical material. The TOOB has been tweaked for more than two years to give the performer a vast but intuitive range of sonic choices, allowing creative freedom in solo or group improvisation. Sound can be generated within the instrument from a library of 'noisy' sound samples or sampled on the fly from the environment or the instrument itself and manipulated.	library (computing);on the fly;structure of observed learning outcome	Arvid Tomayko-Peters	2009		10.1145/1597983.1597987	simulation;computer science;artificial intelligence;computer graphics (images)	AI	-47.167211197574105	-35.42171165956742	31455
e0ca16e4fd31e7a0886ad92e3d2c44bbb55a13b7	identification of illustrators	personal interpretation;computer vision technique;different illustrator;human right;young reader	This paper is motivated by a book in which artists and illustrators from all over the world offer their personal interpretations of the declaration of human rights in pictures [1]. It was enthusiastic for a young reader to see an illustration of an artist that he already knows from his books . The characters were different, the topic was irrelevant, but still it was easy to identify the illustrators based on the style of the illustration. Inspired by the human’s ability to identify illustrators, in this study we propose a method that can automatically learn to distinguish illustrations of different illustrators using computer vision techniques.	book;computer vision;declaration (computer programming);image;relevance	Fadime Sener;Nermin Samet;Pinar Duygulu Sahin	2012		10.1007/978-3-642-33863-2_61	computer science;multimedia	AI	-56.683635676992274	-31.351910693728364	31476
d90634bb1c0d0ad1693023a8e615b6fe26d2281b	songrium: a music browsing assistance service based on visualization of massive open collaboration within music content creation community	web service;massive open collaboration;visualization;social tagging;user generated content;music interface	This paper describes a music browsing assistance service, Songrium (http://songrium.jp), that helps a user enjoy songs while seeing visualization of open collaboration. Songrium focuses on open collaboration for music content creation on the most popular Japanese video-sharing service. Since this open collaboration generates more than half a million video clips with a rich variety of music content, we call it massive open collaboration. To develop a shared understanding of this collaboration we have analyzed, we developed Songrium that visualizes relations among both original songs and derivative works generated from the collaboration. Songrium also features a social annotation framework to verbalize and share various relations among songs, and a flexible ranking mechanism to find interesting songs. After we launched Songrium in August 2012, more than 7,000 users have used our service in which over 98,000 songs and 520,000 derivative works have automatically been registered. We hope Songrium will not only encourage creators to create more derivative works, but also attract consumers to participate in the collaboration as creators.	open collaboration;video clip	Masahiro Hamasaki;Masataka Goto	2013		10.1145/2491055.2491059	engineering;multimedia;internet privacy;world wide web	Web+IR	-45.49659065399562	-27.331088175454678	31530
4a1fd4da97320fd1a6d2b8b3e81fbbc2b63c8b04	pc hardware in a nutshell - a desktop quick reference			desktop computer	Robert Bruce Thompson;Barbara Fritchman Thompson	2000				Arch	-45.16036321407794	-28.356097158865452	31562
ddace7086414cdd894e0bba500f4c3c84f9fa5ad	explanations and privacy in intelligent social awareness applications	social awareness	Explanations play an important part in the interaction with any intelligent system. This is particular important in context-aware and social awareness systems that regularly assume responsibility for a user and act proactively. Explanations are often generated using all available information. However, privacy issues in context-aware systems might dictate a limited distribution of information. The work presented here demonstrates how personal awareness-systems can fulfil different goals a user can have towards explanations, yet maintain a sensible level of	artificial intelligence;context awareness;context-aware pervasive systems;privacy	Jörg Cassens;Anders Kofod-Petersen;Sobah Abbas Petersen;Monica Divitini	2008			social consciousness;computer science;knowledge management	HCI	-58.5783672454739	-42.580876786907645	31607
2bf59305fa01df243bf6bb00648d990cb68e9b13	algorithmic approach for learning a comprehensive view of online users		Abstract   Online users may use many different channels, devices and venues for any online user experience. To make all services such as web design, ads, web content, shopping, personalized for every user; we need to be able to recognize them regardless of device, channels and venues they are using. This, in turn, requires building up a comprehensive view of the user which includes all of their behavioral characteristics - that are spread all over these different venues. This would not be possible without having all behavioral related data of the user which requires the capacity of connecting the user all over the devices, and channels, so to have all of their behavior under a single view. This work is a major attempt in doing this using only behavioral data of users while protecting the user's privacy.		Kourosh Modarresi	2016		10.1016/j.procs.2016.05.378	user interface design;computer science;multimedia;internet privacy;world wide web	ML	-56.574038240254495	-41.77821302061106	31633
30c031811bef848357d9ad57ffc2d9dac94c35fa	some (provocative) consideration about preservation, systems and perfect wolrds (using a lot of quotes)		Let us imagine I want to communicate with someone else that is far away of me and that for such a purpose I want to use a simple, cheap and ubiquitous technology. Therefore, for that, I might consider using “email”. In this case I can claim that I’m making use of a valuable technology, with value for a natural human need, in order to surpass a human limitation, so it makes sense to build a specific technological system for that, an “email system”.	email	José Luis Borbinha	2010			management science;simulation;digital preservation;computer science	ML	-60.14180546476362	-27.060469632307576	31642
ecaea38a9168154837fb1e955e099d3d4d647f42	the influence of a robot's embodiment on trust: a longitudinal study	trust;embodiment;longitudinal study;physical robot;human robot interaction hri;article in monograph or in proceedings;virtual agent	Trust, taken from the human perspective, is an essential factor that determines the use of robots as companions or care robots, especially given the long-term character of the interaction. This study investigated the influence of a robot's embodiment on people's trust over a prolonged period of time. The participants engaged in a collaborative task either with a physical robot or a virtual agent in 10 sessions spread over a period of 6 weeks. While our results showed that the level of trust was not influenced by the type of embodiment, time here was an important factor showing a significant increase in user's trust. Our results raise new questions on the role of the embodiment in trust and contribute to the growing research in the area of trust in human-robot interaction.	human–robot interaction;robot	Anouk van Maris;Hagen Lehmann;Lorenzo Natale;Beata Grzyb	2017		10.1145/3029798.3038435	simulation;computer science;artificial intelligence;trustworthy computing	HCI	-53.09074864833835	-50.84001792992692	31659
68e7cc79f34b034a871a1f978a75d35fe68b2fdf	effects of speed and transitions on target-based travel techniques	cyber sickness;travel techniques;virtual reality;navigation	Travel on Virtual Environments is the simple action where a user moves from a starting point A to a target point B. Choosing an incorrect type of technique could compromise the Virtual Reality experience and cause side effects such as spatial disorientation, fatigue and cybersickness. The design of effective travelling techniques demands to be as natural as possible, thus real walking techniques presents better results, despite their physical limitations. Approaches to surpass these limitations employ techniques that provide an indirect travel metaphor such as point-steering and target-based. In fact, target-based techniques evince a reduction in fatigue and cybersickness against the point-steering techniques, even though providing less control. In this paper we investigate further effects of speed and transition on target-based techniques on factors such as comfort and cybersickness using a Head-Mounted Display setup.	head-mounted display;virtual reality sickness	Daniel Medeiros;Eduardo Cordeiro;Daniel Mendes;Maurício Sousa;Alberto Barbosa Raposo;Alfredo Ferreira;Joaquim A. Jorge	2016		10.1145/2993369.2996348	computer vision;navigation;simulation;computer science;artificial intelligence;virtual reality	HCI	-45.28765677098781	-48.78052084394035	31664
ffcf53b62b787dd68931fa21a09f57d5228c47ee	a stereoscopic vision system with delay compensation for 360° remote reality		The rapid development of virtual reality systems and their increasing acceptance result in a high demand for 3D content, in particular, content that can be viewed in 360°. The acquisition of monoscopic 360° videos is straightforward and is typically done with a single camera in combination with panoramic optics or an arrangement of two 180° fisheye cameras. Stereoscopic 360° videos support the perception of depth but are considerably more challenging to capture. They require sophisticated multi-camera arrangements, which leads to heavy, bulky and expensive systems. Furthermore, they need computationally demanding post-processing. For telepresence applications, where the user wears VR glasses to get a 3D view of the scene in front of a mobile telepresence platform, such a vision system must be real-time capable. Another challenge is the network-induced delay, which leads to motion sickness. In this paper, we present a stereoscopic vision system that captures stereo video content for VR displays and consists of only two cameras, a pan-tilt-unit and a visual delay compensation algorithm. With our approach, the user is allowed to rotate his head by 360°. The proposed system compensates the perceived delay when the user rotates the head around the z-axis by streaming a larger field-of-view (FoV) than needed by the VR display. We provide an analytical solution for the required camera FoV as a function of the FoV of the HMD and the communication delay. Our experimental evaluation for typical head motions shows that the proposed system achieves a mean compensation rate of up to 95% for the tested communication delays of 0-500ms.	algorithm;apache axis;backup rotation scheme;digital video;emergence;fisheye;head-mounted display;machine learning;mathematical model;naruto shippuden: clash of ninja revolution 3;norton zone;real-time clock;region of interest;requirement;stereopsis;stereoscopy;user experience;video post-processing;virtual reality	Tamay Aykut;Stefan Lochbrunner;Mojtaba Karimi;Burak Cizmeci;Eckehard G. Steinbach	2017		10.1145/3126686.3126751	computer vision;machine vision;stereoscopy;artificial intelligence;perception;computer science;virtual reality;stereopsis;motion sickness	Visualization	-39.26451193080081	-41.06493124524861	31698
251ae5aac4b0ad025899be4e4b2292e40dfd94ba	the sound manifesto	speech processing	Computing practice today depends on visual output to drive almost all user interaction. Other senses, such as audition, may be totally neglected, or used tangentially, or used in highly restricted specialized ways. We have excellent audio rendering through D-A conversion, but we lack rich general facilities for modeling and manipulating sound comparable in quality and flexibility to graphics. We need coordinated research in several disciplines to improve the use of sound as an interactive information channel. Incremental and separate improvements in synthesis, analysis, speech processing, audiology, acoustics, music, etc. will not alone produce the radical progress that we seek in sonic practice. We also need to create a new central topic of study in digital audio research. The new topic will assimilate the contributions of different disciplines on a common foundation. The key central concept that we lack is sound as a general-purpose information channel. We must investigate the structure of this information channel, which is driven by the cooperative development of auditory perception and physical sound production. Particular audible encodings, such as speech and music, illuminate sonic information by example, but they are no more sufficient for a characterization than typography is sufficient for a characterization of visual information. To develop this new conceptual topic of sonic information structure , we need to integrate insights from a number of different disciplines that deal with sound. In particular, we need to coordinate central and foundational studies of the representational models of sound	consistency model;general-purpose modeling;graphics;lambda calculus;quantum channel;speech processing	Michael J. O'Donnell;Ilia Bisnovatyi	2000	CoRR		computer vision;speech recognition;systems modeling;visualization;acoustics;sonic interaction design;computer science;graphics;machine learning;signal processing;speech processing;graphical user interface;linguistics;auditory imagery;user interface;sound	HCI	-46.548182168760356	-34.23332626046896	31735
4faed36494e27baf36ea3fb6adc9ccb4faf161b5	the creation of simulated activity datasets using a graphical intelligent environment simulation tool	conference;hidden markov models;monitoring;statistics;avatars;switches;sociology;data models	The availability of datasets capturing the performance of activities of daily living is limited by difficulties associated with the collection of such data. Software solutions can mitigate these limitations, providing researchers with the ability to rapidly generate simulated data. This paper describes the use of IE Sim to create a simulated intelligent environment within which activities of daily living can be performed using a virtual avatar. IE Sim has been demonstrated to facilitate the generation of datasets capturing normal activity performance in addition to overlapping activities and abnormal activities such as hazardous scenarios.	activities of daily living (activity);avatar (computing);departure - action;digitorenocerebral syndrome;graphical user interface;intelligent environment;physical object;rapid prototyping;simulation;smart environment;solutions;tea tree oil;web page;accelerometers;sensor (device)	Jonathan Synnott;Lei Chen;Chris D. Nugent;Ginevra L Moore	2014	2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2014.6944536	data modeling;simulation;human–computer interaction;network switch;computer science;artificial intelligence;machine learning;data mining;hidden markov model;statistics	Visualization	-35.56583725350135	-37.5352358762937	31847
f0d2973d8ad94a37de738298785844c605488cb2	radiometric characterization of spectral imaging for textual pigment identification	categories and subject descriptors according to acm ccs i 3 3 computer graphics picture image generation digitizing and scanning;human vision;cultural heritage;imaging system;spectral imaging;digital image;hyperspectral image	Digital imaging of cultural heritage artifacts has become a standard practice. Typically, standard commercial cameras, often commodity rather than scientific grade cameras, are used for this purpose. Commercial cameras are optimized for plausible visual reproduction of a physical scene with respect to trichromatic human vision. However, visual reproduction is just one application of digital images in heritage. In this paper, we discuss the selection and characterization of an alternative imaging system that can be used for the physical analysis of artifacts as well as visually reproducing their appearance. The hardware and method we describe offers a middle ground between the low cost and ease of commodity cameras and the high cost and complexity of hyperspectral imaging systems. We describe the selection of a system, a protocol for characterizing the system and provide a case study using the system in the physical analysis of a medieval manuscript.	digital image;digital imaging;norm (social);pigment;visual instruction set	Min H. Kim;Holly E. Rushmeier	2011		10.2312/VAST/VAST11/057-064	computer vision;computer science;cultural heritage;imaging science;multimedia;spectral imaging;digital image;computer graphics (images)	HCI	-35.690554866641605	-30.147522325386085	31850
07f0ced2be45c6925f8368b505ef39b7a673eccc	heatmap explorer: an interactive gaze data visualization tool for the evaluation of computer interfaces	heatmap explorer;information visualization;user interface evaluation;eye gaze visualization	Eye gaze is an important source of information to evaluate computer interfaces. Typically, visualization of gaze data is performed using heatmaps and gaze scanpaths displayed on top of images of the interface, enhancing regions that have attracted the user's visual attention. Such tools work well for static interfaces but they are not appropriate to visualize dynamic interfaces where the object of interaction is always changing, such as games, web browsing, or even common applications that change the interface according to the status of the application. In this paper we introduce an interactive tool to explore the spatial-temporal distribution of visual attention called Heatmap Explorer (HME). HME allows the experimenter to control the visualization by selecting temporal intervals and adjusting filter parameters of the eye movement classification algorithm. We show results of three typical application scenarios and discuss how HME can be an effective usability evaluation tool.	algorithm;data visualization;eb-eye;experiment;eye tracking;graphical user interface;heat map;information source;interactive visualization;key frame;online and offline;open-source software;printing;sampling (signal processing);tag (metadata);usability;window function	Antonio Diaz Tula;Andrew T. N. Kurauchi;Flávio Luiz Coutinho;Carlos Hitoshi Morimoto	2016		10.1145/3033701.3033725	computer vision;visual analytics;information visualization;human–computer interaction;multimedia	HCI	-34.679124260750626	-50.070155006146656	31866
35d784874614fbac62f279a79e3560570caca002	interacting with live preview frames: in-picture cues for a digital camera interface	interaction;digital camera;computer vision;digital photography;visual cues;next generation	We present a new interaction paradigm for digital cameras aimed at making interactive imaging algorithms accessible on these devices. In our system, the user creates visual cues in front of the lens during the live preview frames that are continuously processed before the snapshot is taken. These cues are recognized by the camera's image processor to control the lens or other settings. We design and analyze vision-based camera interactions, including focus and zoom controls, and argue that the vision-based paradigm offers a new level of photographer control needed for the next generation of digital cameras.	algorithm;camera interface;digital camera;image processor;interaction;live preview;programming paradigm;snapshot (computer storage)	Steven R. Gomez	2010		10.1145/1866218.1866250	smart camera;computer vision;digital photography;interaction;sensory cue;computer science;multimedia;computer graphics (images)	HCI	-42.9129456123857	-37.88173047726917	31870
8fa4a296a64ae7f8d40972c2baf25d6dd3325554	enhanced turning point displays facilitate drivers' interaction with navigation devices	pnd;user interface;advanced turn by turn display;spatial turning sound;ivts;turn by turn;leading tones for turning;turning point;gui;aui	Recently, the use of in-vehicle navigation devices, such as PNDs (Personal or Portable Navigation Devices) has become pervasive, and the device functions have been rapidly expanded and updated. Unfortunately, drivers often have considerable difficulty using these complex technologies. To improve and optimize PND user interfaces, the present study suggested several display improvements for the turning point, which is one of the critical usability issues. Advanced Turn-By-Turn Display and Spatial Turning Sound were suggested to facilitate the preparation of the next turns. Leading Tones for Turning was also presented to help drivers tune the timing of their turns. We evaluated these new concepts with domain experts in three countries, and improved the details of the functions. We are currently implementing those features and looking forward to demonstrating new displays on the real product in our presentation at the Automotive User Interface conference.	subject-matter expert;usability;user interface	Myounghoon Jeon;Kyoungjun Park;Ubeom Heo;Jongmin Yun	2009		10.1145/1620509.1620536	computer vision;simulation;engineering;engineering drawing	HCI	-47.39489771607104	-42.18433782722502	31895
8f2fa65cb08d76e9fd127bfea81e9665bc5e0829	ai characters and directors for interactive computer games	3d virtual environment;synthetic character;game engine;unreal tournament;interactive storytelling;computer game	We are creating an environment for investigating the role of advanced AI in interactive, story-based computer games. This environment is based on the Unreal Tournament (UT) game engine and the Soar AI engine. Unreal provides a 3D virtual environment, while Soar provides a flexible architecture for developing complex AI characters. This paper describes our progress to date, starting with our game, Haunt 2, which is designed so that complex AI characters will be critical to the success (or failure) of the game. It addresses design issues with constructing a plot for an interactive storytelling environment, creating synthetic characters for that environment, and using a story director agent to tell the story with those characters.	artificial intelligence;game engine;interactive storytelling;interactivity;pc game;soar (cognitive architecture);synthetic intelligence;unreal tournament;virtual reality	Brian Magerko;John E. Laird;Mazin Assanie;Alex Kerfoot;Devvan Stokes	2004			simulation;multimedia;video game development;computer graphics (images)	AI	-48.75323124848852	-32.51394646547458	31924
08818fe42ec4763e393ce0b966fe09ad7bf912c7	assessing a shape descriptor for analysis of mesoamerican hieroglyphics: a view towards practice in digital humanities		Technological advances in digitization, automatic image analysis and information management are enabling the possibility to analyze, organize and visualize large cultural datasets. As one of the key visual cues, shape feature has been used in various image analysis tasks such as handwritten character recognition [1, 5], sketch analysis [4], etc. We assess a shape descriptor, within the application domain of Maya hieroglyphic analysis. Our aim is to introduce this descriptor to the wider Digital Humanities (DH) community, as a shape analysis tool for DH-related applications. The Maya civilization is one of the major cultural developments in ancient Mesoamerica. The ancient Maya language infused art with uniquely pictorial forms of hieroglyphic writing, which represents an exceptionally rich legacy [10]. Most Maya texts were written during the Classic period (AD 250-900) of the Maya civilization on various media types, including stone monuments. A special class of Maya texts was written on bark cloths as folding books from the Post-Classic period (AD 1000-1519). Only three such books (namely the Dresden, Madrid and Paris codices) are known to have survived the Spanish Conquest. A typical Maya codex page contains icons, main sign glyph blocks, captions, calendric signs, etc. Fig. 1 illustrates an example page segmented into main elements [6]. In this paper, we are interested in the main signs. Maya hieroglyphic analysis requires epigraphers to spend a significant amount of time browsing	algorithm;application domain;autodesk maya;book;conquest;digital humanities;glyph;handwriting recognition;image analysis;information management;optical character recognition;shape analysis (digital geometry)	Rui Hu;Jean-Marc Odobez;Daniel Gatica-Perez	2016			humanities;library science;geography	Vision	-48.978241181920055	-26.970005750455563	32012
9ae559527a79690322fe33b58e1fbef1f5c0fcf2	issues of designing gestures into online interactions: implications for communicating in virtual environments	instant messaging;deictics;verbal and non verbal communication;mediated communication;proxemics;computer supported cooperative work;online interaction;collaboration;virtual environments;three dimensional;online community;medium affordances;medium naturalness theory;gestures;computer mediated communication;interactive virtual environment;medium richness;virtual environment;collaborative computing;synchronous interaction;virtual worlds;non verbal communication	This paper describes the use and design of gestures for online virtual environment interactions. Virtual Environments (VEs), are three dimensional virtual worlds that combine sound, text, and gestures in computer-mediated communication (CMC) processes. In VEs, communication can be done through text-based CMC such as instant messaging and chat, but also through additional gestural, visual, proxemic and deictic non-verbal channels. For this reason, VEs require an understanding of the impact of gestures in various contexts. This paper will outline common CMC medium affordances as related to VEs and discuss the importance of using gestures in various online communication contexts. In doing so, this paper will explore the issues that may influence the design and use of gestures for specific professional communication purposes. Because gestures may serve in addressing intercultural contexts and have informational and cognitive communicative benefits within virtual environments, this paper will end with an exploration of specific gesture designs that may help facilitate their use by professional communicators.	computer-mediated communication;gesture recognition;instant messaging;interaction;text-based (computing);virtual reality;virtual world	Gustav Verhulsdonck	2007		10.1145/1297144.1297151	nonverbal communication;three-dimensional space;deixis;human–computer interaction;proxemics;computer science;virtual machine;computer-supported cooperative work;multimedia;gesture;computer-mediated communication;collaboration	HCI	-58.92925991886104	-39.26729535683248	32022
4c053c0335ff40e7b982de762c8da2617d5727e8	fitmersive games: fitness gamification through immersive vr	fitness;virtual reality;immersion;gamification	The decreasing hardware cost makes it affordable to pair Immersive Virtual Environments (IVR) visors with treadmills and exercise bikes. In this paper, we discuss the application of different gamification techniques in IVR for supporting physical exercise. We describe both the hardware setting and the design of Rift-a-bike, a cycling fitmersive game (immersive games for fitness). We evaluate the effectiveness of such techniques through a user study, which provides different insights on their effectiveness in designing such applications.	gamification;interactive voice response;usability testing	Elena Tuveri;Luca Macis;Fabio Sorrentino;Lucio Davide Spano;Riccardo Scateni	2016		10.1145/2909132.2909287	simulation;human–computer interaction;computer science;virtual reality;multimedia;immersion	HCI	-51.088643447233466	-44.969208752503846	32039
db1ab47621d1b37f7712206f2786535788be9075	turning me on, turning me off		Interactive graphics are an effective way of communication and information delivery, especially for complex domains. However, domain experts are rarely aware of the potentials of interactive visual displays and which interaction principles can be in charge for communication and teaching purposes. In this paper we extend a pattern language for interactive information graphics and present four patterns. These patterns are all variations of buttons that can switch between two visual states. Each pattern describes the consequences and special fields of application related to the chosen button type in an educational setting.	graphics;infographic;interactivity;pattern language	Christian Kohls;Tobias Windbrake	2008			systems engineering;computer science	HCI	-53.985081689940664	-37.10511188087339	32091
1cf9ca32553708e43574a33653cf767360bac39d	an accelerometer-based approach to evaluate 3d unistroke gestures		This paper, presents an evaluation of Three Dimensional (3D) unistroke human arm gestures. Our scheme employs an accelerometer-based approach by using NintendoTM Wiimote as a gesture device. The system takes acceleration signals from Wiimote in order to, classify different gestures. It deals with numeric gestures, i.e., digits from 0 to 9 and simple mathematical operator gestures for addition, subtraction, multiplication and division. Two techniques, Dynamic Time Warping (DTW) and 2D trajectories are used to recognize and classify gestures. Successful recognition rates indicate that performing 3D gestures using accelerometer-based devices is intuitive and provides an effective means of interaction with	dvd player;dynamic time warping;gesture recognition;input device;palm os;wii	Tahir Mustafa Madani;Muhammad Tahir;Sheikh Ziauddin;Syed Shadab Raza;Mirza Ahmed	2015	Int. Arab J. Inf. Technol.		computer vision;speech recognition;computer science	HCI	-37.78604346813198	-43.88647745753383	32121
d124dae5d913612adc365ff1d784843444793e95	humanoid audio–visual avatar with emotive text-to-speech synthesis	modelizacion;animacion por computador;3 d face modeling and animation;mimica;lenguaje natural;text to speech synthesis;interfase usuario;interfaz grafica;reseau social;representation graphique;concepcion ingenieria;engineering design;sistema experto;human computer interaction;tts audio visual avatar emotive speech synthesis human computer interaction multimodal system 3 d face modeling and animation;multimedia;base de connaissances;guidage;speech synthesis;facies;graphical interface;user interface;conception ingenierie;systeme modulaire;mimique;gesture;prototypes;synthese par regle;rule based;lip;government;langage naturel;virtual reality;natural sounding emotive speech;speech articulation;sistema modular;guiado;humanoid audio visual avatar;human human communication;integrated design;concepcion integrada;virtual computer agents;man machine system;audio visual avatar;modelisation;social network;sintesis por regla;articulation parole;levre;emotive text to speech synthesis;texto hacia palabra;tts;emotion emotionality;articulacion palabra;natural language;modular system;rule synthesis;facial animation;face modeling;intelligent agent;generic 3 d avatar face model humanoid audio visual avatar emotive text to speech synthesis virtual computer agents human machine interaction human human communication natural sounding emotive speech;text to speech;avatars;grafo curva;guidance;sistema hombre maquina;multimodal system;labio;base conocimiento;emotion emotivite;avatars speech synthesis facial animation humans face government man machine systems synthesizers prototypes intelligent agent;audio visual;interface utilisateur;face;generic 3 d avatar face model;synthetiseur;humans;sintesis palabra;texte a parole;emocion emotividad;systeme expert;facial expression;emotive speech synthesis;computer animation;synthesizers;sintetizador;modeling;interface graphique;geste;man machine systems;conception integree	Emotive audio-visual avatars are virtual computer agents which have the potential of improving the quality of human-machine interaction and human-human communication significantly. However, the understanding of human communication has not yet advanced to the point where it is possible to make realistic avatars that demonstrate interactions with natural-sounding emotive speech and realistic-looking emotional facial expressions. In this paper, We propose the various technical approaches of a novel multimodal framework leading to a text-driven emotive audio-visual avatar. Our primary work is focused on emotive speech synthesis, realistic emotional facial expression animation, and the co-articulation between speech gestures (i.e., lip movements) and facial expressions. A general framework of emotive text-to-speech (TTS) synthesis using a diphone synthesizer is designed and integrated into a generic 3-D avatar face model. Under the guidance of this framework, we therefore developed a realistic 3-D avatar prototype. A rule-based emotive TTS synthesis system module based on the Festival-MBROLA architecture has been designed to demonstrate the effectiveness of the framework design. Subjective listening experiments were carried out to evaluate the expressiveness of the synthetic talking avatar.	automatic sounding;avatar (computing);biconnected component;database;experiment;human–computer interaction;intelligibility (philosophy);logic programming;mbrola;multimodal interaction;netware file system;prototype;semantic prosody;speech synthesis;synthetic intelligence;virtual machine	Hao Tang;Yun Fu;Jilin Tu;Mark Hasegawa-Johnson;Thomas S. Huang	2008	IEEE Transactions on Multimedia	10.1109/TMM.2008.2001355	face;computer vision;speech recognition;facies;computer science;graphics;computer animation;multimedia;natural language;user interface;gesture;speech synthesis;facial expression;government;social network	Visualization	-39.877480672557475	-48.66453409885789	32131
eb29a84820303af08cccda371e763969c72ca3bd	from zombies to cyborg bodies: exoskeleton, extra ear and avatars	extra ear	Issues of identity and alternate, intimate and involuntary experiences of the body, as well as the telematic scaling of experience, will be discussed through a presentation of recent performances. The stimbod so&ware makes possible the remote choreography of the body using a touch-screen interfaced muscle stimulation system. Exoskeleton is a pneumatically powered six-legged walking machine actuated by arm gestures. The extra ear is a proposed project to surgically construct an ear that,connected to a modem and wearable computer, becomes an intemet antenna able to hear real audio sounds to augment the local sounds it hears with its actual ears. Movatar is an inverse motion capture system an intelligent avatar that will be able to perform in the real world by accessing and actuating a body, whereas in previous performances the artist has attached prosthetic devices to augment the body. Now the body itself becomes a prosthesis possessed by an avatar to perform in the physical world. As well as showing visual documentation, he will also demonstrate his third hand and muscle stimulation system. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that topics hear this notice and the full citation on the first page. To copy otherwise, to rcpuhlish, to post on servers or to redistribute to lists. requires prior specific permission and/or a fee. Creativity & Cognition 99 Loughborough UK Copyright ACM 1999 I-581 13-078-3/99/10...$5.00 Stelarc is an Australian performance artist who has lived in Japan for 20 years. He is an Honorary Professor of Art and Robotics at Carnegie-Mellon University in Pittsburgh, USA. At present he is a Visiting Research Scholar at Nottingham Trent University in the UK. .He has received a three-year Fellowhsip fi-om the Visual Arts/Crafts Board of the Australia Council. This year he has done presentations, performances and participated in exhibitions in Australia, the UK, Italy, Austria, Germany, the Netherlands, Switzerland, the Czech Republic and Japan. He has used medical imaging, prosthetics, robotics, virtual reality systems and the intemet to explore and extend the parameters of the body. He is known for the internal filming of his body, the twenty-seven suspension events using insertions into the skin, and constructing an electronic sculpture inserted inside the stomach cavity. As artist-inresidence for Hamburg City in 1998 he completed his Exoskeleton Walking Machine project.	avatar (computing);cognition;cyborg;documentation;image scaling;medical imaging;modem;motion capture;performance;robotics;skin (computing);switzerland;telematics;touchscreen;virtual reality;wearable computer	Stelarc	1999		10.1145/317561.317566	multimedia;human–computer interaction;exoskeleton;computer science	HCI	-51.85221799949318	-24.806684813190277	32132
577d1523a1d0acc081a3fea214924a9db52c971d	head-guided wheelchair control system	wheelchair;joystick;control system;headset;servomotor;accelerometer;control;module	Many individuals using a wheelchair do not have the use of their hands, thereby impeding their ability to control a motorized wheelchair. Available systems that accomplish wheelchair control utilize potentially inhibitive peripheral devices. The purpose of this project was to design a system that allows individuals to control a motorized wheelchair with simple head movements. This system consists of a self-designed program, headset, and a control module that interfaces with a motorized wheelchair joystick.	control system;control unit;headset (audio);joystick;peripheral	John B. Hinkel	2010		10.1145/1878803.1878888	module;embedded system;simulation;computer science;control system;operating system;joystick;accelerometer;servomotor;scientific control	Robotics	-40.942640987221665	-44.9661184078203	32161
b09dc5367ef25c73d447783e44fbc41034099972	enhancement of usability for farmers: user interface for rural community		This research covers how to empower or improve the role of technology and bridge rural digital divide via ICT solutions in the agriculture sector of rural belt of Pakistan and suggests some new ideas like interconnection of web communities with e-boards and mobile phones for sake of giving access to all latest agricultural updates and news. Farmers will be encouraged towards the use of technology for their betterment, ease and efficient output in simple way while using HCI techniques.	usability;user interface	Muhammad Faraz Khokhar;Hassan Ejaz;Tayyab Asif Butt;Shahzaib Iftikhar;Umar Muzaffer;Abbas Ilyas;Faizan ul Mustafa;Adeel Mushtaq;Usman Ahmad;Usman Asghar	2014		10.1007/978-3-319-07635-5_55	human–computer interaction;knowledge management;multimedia	HCI	-59.897429500770905	-39.40643906081679	32166
0557cff070878c8693f43bb8aa19310e4dccd2ee	apple computer's human interface group: advanced technology group	human interface	Group in Apple Computer’s Group is an interdisciplinary team that focuses on interface imovation. The group, one of several Human Interface Groups at Apple, is managed by S. Joy Mountford. The mission of the group is to take a user-centered approach to designing and prototyping future human-computer interfaces. Many of our projects – such as our work in online help, information rvtrieval, and media interfaces progress to become actual Apple products.	human–computer interaction;timeline of apple inc. products;user interface;user-centered design	Kathleen M. Gomoll	1991		10.1145/108844.109009	human–computer interaction;computer hardware;computer science;human interface device	HCI	-49.67067928599527	-25.607619038183035	32247
2f2014c0d95a56ba58928b396f58de2d3d64ab34	speculative metaphors: a design-led approach to the visualisation of library collections	conference proceeding	We are at a critical moment when a library patron’s first, and sometimes only, point of access to library collections is an interface. The relationship we have with physical collections can not be discounted but it also can not be re-created within the screen space. There is a need to understand not only how interfaces operate and how they can be ‘usable’ but also how they shape our relationship with library collections. There is a need to understand how dominant orders of classification are reinforced through their visual representation within collection interfaces and how this shapes the way in which we come to know things. Johanna Drucker notes: “Digital technology depends on visual presentation for much of its effectiveness...but critical understanding of visual knowledge production remains oddly underdeveloped” [7]. We have an opportunity to rethink how we encounter collections through the physical act of browsing and through an interface; an opportunity that is not being addressed. What does each of these experiences afford? How can we reimagine the library collection? In this dissertation I will explore these opportunities through a practice-based approach to the development of a set of speculative prototypes. I will seek to re-imagine the collection through an exploration of the role of metaphor in the visual language of library interfaces and our experience of library collections.	experience;glossary of computer graphics;speculative execution;visual language	Georgina Hibberd	2015	TCDL Bulletin		computer science;multimedia;world wide web	HCI	-57.23397873590182	-31.967769087331153	32251
6ecfb7706b25f04cc7791b1f2dcd833745e7135a	acm siggraph user experience initiatives	special interest groups;computer graphic;user experience;cooperating societies;ux;special interest group	There has been a substantial growth in the number of educational and networking opportunities for professionals in the computer graphics and related fields in the last three years. One of the fastest areas of growth is in the field of computer user experience and the development of cultural communities through the advent of portal technologies, blogs, and wikis.	acm siggraph;blog;computer graphics;fastest;user (computing);user experience;wiki	Barbara Helfer	2005		10.1145/1056808.1056823	special interest group;user experience design;simulation;human–computer interaction;computer science;multimedia;world wide web	Visualization	-52.23334213003405	-30.872095938718854	32263
0e5fd63eb3f0ff9f258d187963d68e9917e2c2b9	magicphone: pointing & interacting	mobile phone;magnetometer;pointing interaction;gestural control;point interaction	Mobile phones are becoming a kind of must-have portable devices for people. This video demonstrates a mobile phone that can sense what you are pointing to and can act as a physical ubiquitous interaction device in real world, called MagicPhone. If you want to interact with an appliance around you, you just simply point the MagicPhone to it and then operate. The MagicPhone uses both the built-in accelerometer and magnetometer to sense the pointing orientation. Using MagicPhone, you only need to point to a device and sliding your finger, to show a picture on a display, to send a document to a laptop, to share slides on a projector, and to print a photo. In addition, MagicPhone can control a selected device with accelerometer-based gestures, e.g. changing TV channels. It also can serve as a mouse to draw a picture or play clicking games.	canonical account;interaction;laptop;mobile phone;personal digital assistant;video projector	Jiahui Wu;Gang Pan;Daqing Zhang;Shijian Li;Zhaohui Wu	2010		10.1145/1864431.1864483	magnetometer;simulation;multimedia	HCI	-44.584385445865216	-41.57251237684608	32281
65ce6ddf505e2240aa2c25555381f8f810497161	mathematical methods for protein structure analysis and design		Spend your few moment to read a book even only few pages. Reading book is not obligation and force for everybody. When you don't want to read, you can get punishment from the publisher. Read a book becomes a choice of your different characteristics. Many people with reading habit will always be enjoyable to read, or on the contrary. For some reasons, this mathematical methods for protein structure analysis and design advanced lectures 1st edition tends to be the representative book in this website.		Sorin Istrail;Pavel A. Pevzner	2003		10.1007/b11686	mathematical structure	PL	-60.84056621238319	-24.242956753025375	32285
27d219af08ff90aa01c7a4311b5ade3f8f0d3dac	virtual acoustic reconstruction of the church at the lost monastery of santa maria de la murta		Archeological acoustics is a part of acoustics that studies ancient environments which were dedicated (completely or partly) to sound performing. The combination of this acoustic area in conjunction with room acoustics enables the study of the acoustic evolution of existing buildings (in terms of the historical documentation) or even, make the acoustic reconstruction of rooms which were destroyed or they are in a bad state or in ruins. In this work, an acoustical reconstruction of the church of the Hieronymites’ monastery of the Murta from the XIV century, in Alzira, Spain has been developed. This building was abandoned in 1836 and now is in ruins. The work develops a geometrical model, together with a proposal of materials which were common in the churches of the same area, an acoustical study and finally an auralization of a choral performance has been done (as this monastery had its own musical chappele).	acoustic cryptanalysis;documentation;linear algebra;nonlinear acoustics;sonification	Ana Planells;Jaume Segura;Arturo Barba;Salvador Cerdá;Alicia Giménez;Rosa M. Cibrian	2014		10.1007/978-3-319-13969-2_37	visual arts;art;archaeology	HCI	-51.65469677134764	-28.158378886827677	32297
89e992ee60e9e990e9174464bf5850aa82d50acd	communication visibility in shared virtual worlds	distributed heterogeneous clients communication visibility shared virtual worlds research urban world spaces spatial subdivision logical subdivision city level optimizations for virtual environments cloves mit city scan automated urban geometry acquisition graduated visibility set interest management;city level optimizations for virtual environments;groupware;town and country planning;spatial subdivision;data subscription;gvs;image databases;town and country planning virtual reality groupware user interfaces client server systems;interest management;computer graphics;mit city scan;logical subdivision;geometry;virtual reality;graduated visibility set;shared virtual worlds;client server systems;automated urban geometry acquisition;collaborative virtual environments;cities and towns image databases spatial databases solid modeling buildings computer graphics virtual environment geometry predictive models object oriented modeling;cloves;visibility;solid modeling;spatial databases;cities and towns;communication visibility;predictive models;urban world spaces;virtual environment;distributed heterogeneous clients;collaborative virtual environment;user interfaces;object oriented modeling;buildings;virtual worlds	Though the service of shared virtual worlds is an active area of research, little effort has been made to optimize such systems for urban world spaces. Tracking the motion, action, and communication of thousands of users in a city requires a application of visibility for spatial and logical subdivision of updates. We propose herein the City-Level Optimizations for Virtual Environments (CLOVES) substrate for the MIT City Scan (automated urban geometry acquisition) project. CLOVES includes a generalized spatial subdivision optimized for visibility; a Graduated Visibility Set (GVS) generator; associated interest management techniques; and model service to distributed heterogeneous clients.	space partitioning;subdivision surface;virtual world	Michael V. Capps;Seth J. Teller	1997		10.1109/ENABL.1997.630812	simulation;visibility;computer science;virtual machine;operating system;database;distributed computing;virtual reality;predictive modelling;multimedia;solid modeling;computer graphics;user interface;world wide web;computer graphics (images)	Visualization	-35.52602947058111	-31.51474997677042	32347
5dc7beec215456da999c137b85c1c99bb922e78d	applying direct manipulation interfaces to customizing player character behaviour	iterative method;interfaz grafica;representation graphique;online game;graphical interface;direct manipulation;customization;divertissement;personnalisation;long terme;long term;metodo iterativo;largo plazo;methode iterative;graphical representation;personalizacion;grafo curva;jeu ordinateur;computer games;interface graphique;entertainment;graphics	The ability to customize a players avatar (their graphical representation) is one of the most popular features of online games and graphical chat environments. Though customizing appearance is a common ability in most games, creating tools for customizing a character’s behaviour is still a difficult problem. We propose a methodology, based on direct manipulation, that allows players to specify the type of behaviour they would like in a given context. This methodology is iterative, with the player performing a number of different customizations in different contexts. Players are also able to continue customizing their character during play, with commands that can have long term and permanent effects.	avatar (computing);direct manipulation interface;graphical user interface;iterative method	Marco Gillies	2006		10.1007/11872320_21	simulation;computer science;multimedia;computer graphics (images)	HCI	-42.14574044511294	-32.16782276870073	32383
51aaeb797ba66075a42a5e48518a5d35da9057eb	intelligent physiotherapy through procedural content generation		This paper describes an avenue for artificial and computational intelligence techniques applied within games research to be deployed for purposes of physical therapy. We provide an overview of prototypical research focussed on the application of motion sensor input devices and virtual reality equipment for rehabilitation of motor impairment: an issue typical of patients of traumatic brain injuries. We highlight how advances in procedural content generation and player modelling can stimulate development in this area by improving quality of rehabilitation programmes and measuring patient		Shabnam Sadeghi Esfahlani;Tommy Thompson	2016	CoRR		simulation;engineering;multimedia;biological engineering	HCI	-39.556505282999716	-46.769832893821935	32390
33f148ea06afbb92c8404fbca142fb3538e00dbc	simulation language features in 1976: existing and needed	computer graphic;technical report;computer simulation	It is now 1976, more than 20 years since computer simulation was first recognized as an area which had a need for special purpose computer languages and languages features. Many of the signs of maturity are present in the field: Two annual simulation conferences, courses in most major universities, and a number of standard textbooks and bibliographies[l,6,4]. In addition, there have been several surveys of simulation languages, e.g., [7] and we have no wish to repeat such a survey here. Rather, we attempt to describe a set of features that one should be able to “expect” any simulation language to have, and to discuss a set of features which are still needed - especially in the growing worlds of interactive computing and computer graphics and data bases.	capability maturity model;computer graphics;computer language;computer simulation;database;embedded system;formal language;interactive computing;simulation language	Louis W. Miller;Howard Lee Morgan	1976			computer simulation;simulation;computer science;engineering;technical report;multimedia;world wide web;simulation language	Graphics	-47.975379669493535	-28.58754200928585	32418
25b8d3a3a6ee5cf4b55630aa144e690c76896cc3	the empty museum. multi-user interaction in an immersive and physically walkable vr space	art;wireless network;virtual reality;multi user;virtual art gallery multi user interaction vr space virtual reality wireless system wireless network;motion capture;wireless lan;wireless lan virtual reality art;virtual reality head hardware displays design engineering visualization irrigation graphics portable computers wireless networks;wireless systems;virtual space	Until quite recently, virtual reality systems consisted of fixed devices which enabled the user to feel immersed in a spot of the virtual space by means of the adequate hardware. The recent emergence of wireless systems for motion capture, together with the increase in graphic power of laptops, and the generalisation of wireless networks has allowed the appearance of the first systems in which at last the user is able to walk physically within a given space framed in the real one, and containing the objects and elements of the virtual space [2][5][6]. Some examples of this hybrid space have already been accomplished worldwide. However, beyond the technical problems with the development of these systems, we must bear in mind the types of contents to be shown, making the most of the possibilities offered by the fact that the user him/herself is the pointer in this kind of virtual reality, while the space itself is the interface. The authors have recently developed a system similar to the ones described. This is a totally immersive, walkable and wireless system called the Empty Museum [7]. This paper outlines its enlargement with the purpose of making it simultaneously usable by several persons. Besides, an example of content is provided which has been specifically designed in order to be experienced in multi-user mode with this equipment: the Virtual Art Gallery.	emergence;human–computer interaction;immersion (virtual reality);laptop;motion capture;multi-user;pointer (computer programming);user space;virtual art;virtual reality	Luis Hernández;Javier Taibo;Antonio Seoane;Rubén López;Rocío López	2003		10.1109/CYBER.2003.1253488	computer vision;computer-mediated reality;motion capture;simulation;computer science;artificial intelligence;kernel virtual address space;operating system;wireless network;virtual reality;mixed reality;multimedia;computer graphics (images)	Visualization	-43.81519260719201	-38.87232014774601	32449
12654ec45ccffaa9b07a438c1a03e54395b8bc6f	analysis of electromyography and skin conductance response during rubber hand illusion	scr electromyography skin conductance response rubber hand illusion brain mechanism self body recognition rhi emg measurement;brain;skin;electromyography thyristors rubber time measurement skin educational institutions position measurement;skin brain electromyography;electromyography	Recently, the rubber hand illusion (RHI), which is one of phenomena that the sense of ownership is extended to the objects over the external area, attracts much attention to explain the brain mechanism of self body recognition of human. However, most previous research have only focused on the conditions for the occurrence of the RHI. In this study, we measured the electromyography (EMG) of the arm and the skin conductance response (SCR) of the end of the finger when the strong blow with a hammer would be given to the fake hand in order to examine whether the RHI is in fact occurred to the subject at a certain time during the experiment. As a result, we showed that the measurement of the EMG could satisfy above requirement and it is implied that the measurement of EMG gets closer to the tendency of introspection report than that of SCR.	conductance (graph);electromyography;introspection	Takuma Tsuji;Hiroshi Yamakawa;Atsushi Yamashita;Kaoru Takakusaki;Takaki Maeda;Motoichiro Kato;Hiroyuki Oka;Hajime Asama	2013	2013 IEEE Workshop on Advanced Robotics and its Social Impacts	10.1109/ARSO.2013.6705511	computer science;artificial intelligence;skin	Robotics	-45.39838967066244	-51.00305495508607	32530
5ef094199f4c5649cf91959df4a14d3fb100bafe	league of legends music: curse of the sad mummy		Every child in Valoran has heard the tale before, about the cursed mummy boy who felt his heart no more.		Justin Kranzl	2015		10.1145/2745234.2767013	league;computer graphics (images);humanities;curse;computer science	Theory	-54.127339169797814	-26.713362013555862	32573
d201bd381fab99faa237a92a2fb3f7a754fa3821	an interactive surface solution to support collaborative work onboard ships	collaborative work;mobility;interactive surface display;field studies;ships;industrial applications	Industrial environments are notoriously known as difficult places to gain access to conduct any type of contextual inquiry work, and marine vessels are no exception. But once this initial hurdle is overcome, these environments reveal interesting research directions. Challenges faced onboard ships range from issues with communication links, to the lack of support for current work practices. Based on findings from an earlier field study, the work presented in this paper focuses on several challenges involving collaboration, communication, information sharing such as video and images, and tracking task completion of crew members. This paper therefore presents a prototype which consists of a Microsoft surface, mobile phones, and PCs to enable crew members onboard ships to effectively communicate and collaborate with their colleagues.	contextual inquiry;field research;mobile phone;prototype;video	Veronika Domova;Elina Vartiainen;Saad Azhar;Maria Ralph	2013		10.1145/2512349.2512804	simulation;engineering;operations management;operations research	HCI	-61.29140921177829	-40.23440318912443	32599
2743a31675107e254ac71e41024f337c3287fbfa	active environments: sensing and responding to groups of people	computer supported cooperative work;intelligent environment;intelligent environments;social issues;ubiquitous computing;smart object	"""Most environments are  passive  -- deaf, dumb and blind, unaware of their inhabitants and unable to assist them in a meaningful way. However, with the advent of ubiquitous computing - ever smaller, cheaper and faster computational devices embedded in a growing variety of """"smart"""" objects -- it is becoming increasingly possible to create  active  environments: physical spaces that can sense and respond appropriately to the people and activities taking place within them. Most of the early UbiComp applications focus on how  individuals  interact with their environments as they work on  foreground  tasks. In contrast, this paper focuses on how  groups  of people affect and are affected by  background  aspects of their environments."""		Joseph F. McCarthy	2000		10.1145/633292.633325	simulation;human–computer interaction;computer science;social issues;computer-supported cooperative work;multimedia;ubiquitous computing	HCI	-48.9875562076638	-40.4174835637199	32614
64daa704b901a795cfaa8b3839f81020e9b9d1ce	co-operative minimally invasive robotic surgery	robotic surgery;posicionamiento;hospital;minimal invasive surgery;minimally invasive;handling;dexterity;remote handling;hombre;robotics;manutention;manutencion;remote operation;hopital;positioning;planificacion;remote handling devices;dexterite;teleaccion;chirurgie;human;surgery;robotica;arm;planning;cirugia;bras;brazo;robotique;planification;remote handled;destreza;telemanipulation;teleoperation;homme;positionnement;design methodology	Purpose – The purpose of this paper is to consider surgical robotics, with a focus on technology and design issues for remote‐mode operation assistance. The investigation leads to the definition of the technical characteristics of a co‐robotic positioning device (CRPD), to be developed in support of a split‐duty approach to planning. The expected characteristics and advantages are outlined, including the operation potential of special‐purpose devices (e.g. an automatic changer for surgical tools) and of scope‐driven enhancers (e.g. the exploration of the intervention theatre).Design/methodology/approach – The paper addresses example developments based on projects performed with the co‐operation of other robot laboratories in Munich and Paris. The CRPD concept is applied in relation to the DLR KineMedic® arm (developed by the Munich laboratory), and with the LRP prototype mini‐arm (built by the Paris laboratory).Findings – Minimally‐invasive surgery deserves increasing attention to reduce post‐operative ho...	robot	Rinaldo C. Michelini;Roberto P. Razzoli	2008	Industrial Robot	10.1108/01439910810876445	planning;teleoperation;simulation;robotic surgery;design methods;computer science;engineering;artificial intelligence;biological engineering;robotics;arm architecture	Robotics	-39.0764701012851	-47.71263078954034	32669
89aaaecd75d20a0886a685c01e18bbd3a3325325	social networking 2.0	mobile phone;social network;social networks;facebook;bluetooth;online social network;mobile phones	In this paper we describe the development of a platform that enables us to systematically study online social networks alongside their real-world counterparts. Our system, entitled Cityware, merges users' online social data, made available through Facebook, with mobility traces captured via Bluetooth scanning. Furthermore, our system is constantly growing, since it enables users to contribute their own mobility traces. In addition to describing Cityware's architecture, we discuss the type of data we are collecting, and the analyses we intend to carry out.	bluetooth;social network;tracing (software)	Vassilis Kostakos	2008		10.1145/1358628.1358861	computer science;internet privacy;world wide web;social network;online participation	HCI	-56.4256074560739	-41.92969378331288	32682
9de95d90b657c478d4242c2b558d1c78326696cf	supporting co-evolution of users and systems by the recognition of interaction patterns	visual sentence;system observation;visual interaction;co evolution;interactive system;interaction pattern;pattern recognition;visual interfaces;visual interface design	This paper presents an approach to support the designer of Visual Interactive Systems (VISs) in adapting a VIS to the evolution of its users. This process is called co-evolution of users and systems. The approach is based on the identification of the patterns of interaction between the user and an interactive system and on their use for the evolution of the system to facilitate novel usages introduced by the user. The approach is focused on WIMP systems and is based on the recently introduced PCL (Pictorial Computing Laboratory) model of interaction, within which we provide a novel definition of interaction pattern. The proposal assumes that the VIS is observed by an external system called SIC (Supporting Interaction Co-evolution), which is in charge of recording the interactions between the user and the VIS and of analyzing the relevant interaction patterns. In particular, SIC exploits a UML-based statechart specification of the VIS in order to associate observed user activities with the states of the interactive process. This information provides a useful basis for a variety of pattern recognition techniques. Two techniques called usual state and recurrent sequence recognition are illustrated and the results of a first experiment are discussed.	evolution;image;interaction design pattern;interactivity;pattern recognition;simplified instructional computer;state diagram;unified modeling language;visual instruction set;wimp (computing)	Stefano Arondi;Pietro Baroni;Daniela Fogli;Piero Mussio	2002		10.1145/1556262.1556291	interactive systems engineering;human–computer interaction;coevolution;computer science;data mining;multimedia;world wide web	HCI	-55.21841013541704	-44.97368339272126	32698
eeb0952e0e6dc30bb0ce76df46308bf5d6c23343	coherence in the learning system k-med	learning system;guided tour	The advantages of a hypermedia learning system are the possibility to adapt the content to the learner and to maintain the content easily due to the modular structure. The disadvantages are the well-known problem of the cognitive overhead, and the less discussed problem of little local and nearly no global coherence. The k-med approach is to describe the resources of the system by metadata, connect them by relations and have an ontology containing all relevant concepts. These descriptions enable the system to generate dynamically individual guided tours and table of contents for orientation and navigation according to the preferences of the users. The relations are also used to add small pieces of static text between two resources to connect them.	hypermedia;knowledge base;overhead (computing);requirement;whole earth 'lectronic link	Ralf Steinmetz;Cornelia Seeberg;Achim Steinacker	2001			speech recognition;computer science;coherence (physics)	AI	-39.4693885666689	-24.929438008307148	32708
8f58d9ea6da6e0977a46bd0edcdb47c90735e12f	screen play: film and the future of interactive entertainment	screen play;interactive entertainment;virtual environment	This paper looks at existing computer games and virtual environments from the perspective of film theory and practice. From this, we will draw conclusions about the ways in which the designers of computer games and virtual environments can use what has been discovered in the study of film to build more interesting, engaging and entertaining interactive narratives.	pc game;virtual reality	Andy Clarke;Grethe Mitchell	2001			human–computer interaction;computer science;multimedia;interactive media;computer graphics (images)	HCI	-57.46386845485912	-34.383676842916024	32735
87e6c6bec60646bbb7a8bbbd8159bc50ac67ebc9	exploring effective advertising strategies: the roles of formats, content relevance and shopping tasks on ad recognition		The widespread application of Web-based technology has contributed not only to the content of advertising but also to the improvement of presentation formats. Animation has become a powerful presentation format on the Web. Despite its potential benefits, however, animation is no panacea. Practitioners and academics have been paying increased attention to the exploration of effective advertising strategies in the e-commerce environment. Drawing on theories in cognitive psychology, this study, by using a laboratory experiment, investigates the roles of presentation formats, content relevance and shopping tasks on ad recognition. The results show that abrupt and looming formats are effective formats in improving ad recognition. There is a significant three-way interaction between the formats, content relevance and shopping tasks on ad recognition. Specifically, the interaction effect of animation formats and content relevance is observed in browsing tasks, but not in searching tasks.	e-commerce;relevance;theory;world wide web	Chunping Jiang;Kai H. Lim;Yongqiang Sun	2009				HCI	-39.04058078239898	-51.6823650989494	32749
b09f12327c5121dfbe1ae05b175e4ba9a9931a28	using image processing to teach cs1 and cs2	digital image processing;image processing;computer science curriculum;visual feedback	The use of digital image processing techniques in undergraduate computer science curriculum has advantages in terms of motivating student interest and immediate, visual feedback of executed code. Although the standard Java distribution includes support for basic image processing operations, including the display of images, the complexity of the package renders it unsuitable for inexperienced programmers. This paper presents an extension to the built-in image processing package that is suitable for use in CS1 and CS2 courses and suggests ways that the package can be used to teach topics in these courses.	canonical account;computer science;digital image processing;experience;grayscale;java package;programmer;rendering (computer graphics)	Kenny Hunt	2003	SIGCSE Bulletin	10.1145/960492.960535	image analysis;image processing;computer science;digital image processing;multimedia;computer graphics (images)	Graphics	-47.16339843980718	-29.077909692612195	32754
d1e5fd15c594051637696cf93c47d6c0a224a9e9	player acceptance of human computation games: an aesthetic perspective	acceptance;aesthetic experience;information quality;human computation games;usability	Human computation games (HCGs) are applications that use games to harness human intelligence to perform computations that cannot be effectively done by software systems alone. Despite their increasing popularity, insufficient research has been conducted to examine the predictors of player acceptance for HCGs. In particular, prior work underlined the important role of game enjoyment in predicting acceptance of entertainment technology without specifying its driving factors. This study views game enjoyment through a taxonomy of aesthetic experiences and examines the effect of aesthetic experience, usability and information quality on player acceptance of HCGs. Results showed that aesthetic experience and usability were important contributors of player acceptance. Implications of our study are discussed.	human-based computation	Xiaohui Wang;Dion Hoe-Lian Goh;Ee-Peng Lim;Adrian Vu	2014		10.1007/978-3-319-12823-8_24	simulation;usability;computer science;multimedia;information quality;world wide web	AI	-59.580972954315385	-45.906942455932224	32777
6c1516f81de843b167776944a9b9f92cca88191b	face me! head-tracker interface evaluation on mobile devices	mobile hci;user study;evaluation;head tracker interface	The integration of front cameras on mobile devices and the increase on processing capacity has opened the door to head-tracker interfaces on mobile devices. However, research mostly focus on the development of new interfaces and their integration into prototypes without analyzing human performance. In this work, we present a head-tracker interface for mobile devices and its evaluation from the point of view of Human-Computer Interaction. Nineteen participants performed position-select tasks using their nose's movement. User performance was measured with different device orientations and combining different gain and target width. Based on the obtained results, two design recommendations were made for those designers using the developed interface. In addition, we confirmed that device orientation, a particular feature for mobile devices that does not affect desktop computers, has no effect on the user's performance.	desktop computer;human reliability;human–computer interaction;java platform, micro edition;mobile device;point of view (computer hardware company)	Maria Francesca Roig-Maimó;Javier Varona;Cristina Manresa-Yee	2015		10.1145/2702613.2732829	user interface design;simulation;human–computer interaction;evaluation;multimedia	HCI	-46.88069612006488	-42.36358867291846	32791
66e195469f5966c10d6338d4514db9f2c97b2317	textile interfaces: embroidered jog-wheel, beaded tilt sensor, twisted pair ribbon, and sound sequins	yarn;sound sequins;sensors;spatial variables measurement;embroidered jog wheel;wearable computers;twisted pair ribbon;production engineering computing;wearable computers fabrics production engineering computing sensors spatial variables measurement textile technology;sound sequins textile interfaces embroidered jog wheel beaded tilt sensor twisted pair ribbon;sensors yarn fabrics capacitance wheels films;fabrics;beaded tilt sensor;capacitance;capacitive sensing embroidered jog wheel beaded tilt sensor twisted pair ribbon sound sequins electronic textiles e textiles electronics computing fabric e textile interfaces electronic textile interface swatch book e textile toolkit multiuse jog wheel multilayer embroidery pvdf film hanging bead;textile technology;films;wheels;textile interfaces	Electronic textiles (or e-textiles) attempt to integrate electronics and computing into fabric. In our efforts to create new e-textile interfaces and construction techniques for our Electronic Textile Interface Swatch Book (an e-textile toolkit), we have created a multi-use jog wheel using multilayer embroidery, sound sequins from PVDF film and a tilt sensor using a hanging bead, embroidery and capacitive sensing. In order to make capacitive sensing over long leads possible on the body, we have constructed twisted pair ribbon and demonstrated its effectiveness over more typical sensing techniques. We detail construction techniques and lessons learned from this technology exploration.	capacitive sensing;e-textiles;jog dial;twisted pair	Clint Zeagler;Scott M. Gilliland;Halley Profita;Thad Starner	2012	2012 16th International Symposium on Wearable Computers	10.1109/ISWC.2012.29	embedded system;wearable computer;human–computer interaction;computer science;sensor;capacitance	HCI	-42.36917525922951	-41.55376492617004	32794
2f0b3c94f936d3d491d195d00a78f8b7a9287816	interactive visualiztion of 3d-vector fields using illuminated streamlines				Malte Zöckler;Detlev Stalling;Hans-Christian Hege	1996				Robotics	-53.19930198801628	-31.820001128034797	32822
b6226be7eac36690024cfb7422153bb9032e5757	a case study on interactive exploration and guidance aids for visualizing historical data	four-dimensional space;particular event;guidance aid;exploration technique;historical data;interactive exploration;data acquisition;interaction concept;fixed dimension;animation generation;historical data visualization;case study;history;animation;visualization techniques;data visualization;data visualisation;virtual environment;visualization;interaction;virtual reality	"""In this paper, we address the problem of historical data visualization. We describe the data acquisition, preparation, and visualization. Since the data contain four dimensions, the standard 3D exploration techniques have to be extended or appropriately adapted in order to enable interactive exploration. We discuss in detail two interaction concepts: (1) navigation with one fixed dimension, and (2) quasi 4D navigation allowing to simultaneously explore the four-dimensional space. In addition, we also present a picture-in-picture display mode, enabling the user to interactively view the data, while """"flying with"""" a particular event, tracking its motion in time and space. Finally, we present a technique for guided exploration and animation generation, allowing for a vivid gain of insight into the historical data."""	computer display standard;data acquisition;data visualization;interactivity;norm (social);picture-in-picture	Stanislav L. Stoev;Wolfgang Straßer	2001	Proceedings Visualization, 2001. VIS '01.		anime;computer vision;interaction;visualization;computer science;virtual machine;multimedia;creative visualization;data acquisition;data visualization;statistics;computer graphics (images)	Visualization	-41.75115299488203	-35.600372693173476	32852
b6edefe7292a3fd0915fa7dca434a36fed2ef8aa	fingervoice: a syllable based input system via fingers touching		"""This paper proposes a novel input system via fingers touching, called FingerVoice. Instead of traditional letter based input system, it is based on syllables. Fingers on the left hand stand for consonants while fingers on the right hand for vowels. Fingers from both hands touch each other to form a syllable. We use a pair of gloves with conductive finger caps to detect the touch. As an example, the implementation of Chinese is presented in details. By connecting it to a smart phone via Bluetooth, a utility speaking system is implemented to help the speech-impaired to """"speak"""" via smart phone. Our experimental result shows that this syllable based input device is faster than letter based input devices."""	bluetooth;input device;input/output;java caps;smartphone;syllable;wired glove	Wenjie Chen;Yangyang Ma;ZhiLei Chai;Mingsong Chen	2017		10.1145/3132525.3134806	phone;input device;computer science;syllable;speech recognition;bluetooth	HCI	-43.705024813639305	-43.919018781988264	32858
9c6c35b9a81ed9e8c75a71f20c6cc40f72221ab1	a presentation authoring tool for media devices distributed environments	surround speakers;preauthor;multimedia systems;3d vrml model;multichannel presentation;presentation playback;hyperslide synchronization;presentation authoring tool;multimedia device visual representations;graphical user interfaces;live video;presentation organization;media devices distributed environments;audio-visual systems;authoring systems;gui;technical presentation;plasma displays;synchronisation;rendered image sequence;conference room multiple multimedia devices;user interfaces;augmented reality;distributed environment;virtual environment;computer science;videoconference	"""Many conference rooms are now equipped with multiple multi-media devices, such as plasma displays and surrounding speakers, to enhance presentation quality. However, most existing presentation authoring tools are based on the one-display-and-one-speaker assumption, which makes it difficult to organize and playback a presentation dispatched to multiple devices, thus hinders users from taking full advantage of additional multimedia devices. In this paper, we propose and implement a tool to facilitate authoring and playback of a multi-channel presentation in a media devices distributed environment. The tool, named PreAuthor, provides an intuitive and visual way to author a multi-channel presentation by dragging and dropping """"hyper-slides"""" on corresponding visual representations of various devices. PreAuthor supports """"hyper-slide"""" synchronization among various output devices during preview and playback. It also offers multiple options for the presenter to view the presentation in a rendered image sequence, live video, 3D VRML model, or real environment."""	3d modeling;drag and drop;output device;plasma display;vrml	Hangjin Zhang;Qiong Liu;Surapong Lertsithichai;Chunyuan Liao;Don Kimber	2004	2004 IEEE International Conference on Multimedia and Expo (ICME) (IEEE Cat. No.04TH8763)		computer vision;augmented reality;human–computer interaction;computer science;operating system;graphical user interface;multimedia;computer graphics (images)	Visualization	-42.86997826404659	-35.33053334775352	32903
2f2c5457abab9d664e2f165eec5826ae43ff9d9d	learning to predict engagement with a spoken dialog system in open-world settings	spoken dialog system;false positive rate;machine learning;visual analysis	We consider the challenge of predicting the engagement of people with an open-world dialog system, where one or more participants may establish, maintain, and break the communication frame. We show in particular how a system can learn to predict an intention to engage from multiple observations that are extracted from a visual analysis of people coming into the proximity of a system.	dialog system;open world;predictive modelling;robot;sensor;simplified molecular-input line-entry system;situated;spoken dialog systems;system time	Dan Bohus;Eric Horvitz	2009		10.3115/1708376.1708411	natural language processing;speech recognition;computer science;communication	HCI	-51.22114363747555	-48.90663387820554	32914
26804256c03b8775e6bc38b531ef711c599287c8	people-to-people-to-geographical-places: the p3 framework for location-based community systems	communication system;online interaction;design space;system design;context aware systems;virtual space	In this paper we examine an emerging class of systems that link People-to-People-to-Geographical-Places; we call these P3-Systems. Through analyzing the literature, we have identified four major P3-System design techniques: People-Centered systems that use either absolute user location (e.g. Active Badge) or user proximity (e.g. Hocman) and Place-Centered systems based on either a representation of people’s use of physical spaces (e.g. ActiveMap) or on a matching virtual space that enables online interaction linked to physical location (e.g. Geonotes). In addition, each feature can be instantiated synchronously or asynchronously. The P3-System framework organizes existing systems into meaningful categories and structures the design space for an interesting new class of potentially context-aware systems. Our discussion of the framework suggests new ways of understanding and addressing the privacy concerns associated with location aware community system and outlines additional socio-technical challenges and opportunities.	context-aware pervasive systems;location awareness;privacy;sociotechnical system;synchronization (computer science);virtual reality	Quentin Jones;Sukeshini A. Grandhi;Loren G. Terveen;Steve Whittaker	2004	Computer Supported Cooperative Work (CSCW)	10.1007/s10606-004-2803-7	real-time computing;simulation;system of systems;telecommunications;computer science;data mining;communication;world wide web;communications system;systems design	HCI	-58.696382721167495	-39.36066050464669	32980
f587fa55c71bc4645f90e5944d309c5d375e31b3	book review: the handbook of task analysis for human-computer interaction by d. diaper and n. stanton	human computer interaction;task analysis		handbook;human–computer interaction;task analysis	Yong Gu Ji	2005	Int. J. Hum. Comput. Interaction	10.1207/s15327590ijhc1802_7	human–computer interaction;computer science;artificial intelligence;task analysis;cognitive science	NLP	-51.858561828662864	-34.57635143125718	32987
a0330a5c89815e0b03b3a5e5d87e7034cc538598	the mysteries of lisp - i: the way to s-expression lisp		Despite its old age, Lisp remains mysterious to many of its admirers. The mysteries on one hand fascinate the language, on the other hand also obscure it. Following Stoyan but paying attention to what he has neglected or omitted, in this first essay of a series intended to unravel these mysteries, we trace the development of Lisp back to its origin, revealing how the language has evolved into its nowadays look and feel. The insights thus gained will not only enhance existent understanding of the language but also inspires further improvement of it.	lisp;look and feel;s-expression	Hong-Yi Dai	2015	CoRR		s-expression;computer science;lisp;look and feel;artificial intelligence	PL	-57.79430019631658	-27.521760527618124	32991
72733550c675d2981835a3eb451e5e44fe089f1f	a dagstuhl seminar looks beyond virtual and augmented reality	meetings;augmented reality	I n early February 2017, we—along with Anind Dey from Carnegie Mellon University, Jonna H€ akkil€ a from the University of Lapland, and Jun Rekimoto from University of Tokyo— organized a five-day seminar to discuss the future of interactive media. Thirty researchers from Europe, Asia, and the US gathered in picturesque Dagstuhl, a small village in rural Germany, to rethink virtual and augmented reality (see the sidebar). Schloss Dagstuhl, a nonprofit center for computer science research, hosted the seminar, which was motivated by the third wave of VR and AR technologies we’re currently witnessing. These technologies are the result of researchers broadly looking into VR and AR again, focusing on topics ranging from using AR to mitigate skills gaps and understanding user interaction with commercial AR games, to using focus depth as an input modality for VR, understanding the effect of gender in VR, and using sensory augmentation in cars. The goal of the seminar was to take a step back from technical research to look at the fundamental aspects of interactive media. Sharing experiences and knowledge has always been essential for human development, enabling empathy and the transfer of skills. Throughout history, humans have transitioned from oral traditions to cultures of writing. The ongoing digital revolution is removing hurdles to sharing knowledge and experiences. For example, people can now store 24/7 video recordings of their lives, creating massive collections of data. However, this can actually make it even more challenging to share experiences and knowledge with others in meaningful ways. We need to rethink and redefine experience sharing and skill transfer in light of current technological advances, including affordable consumer-grade VR and AR systems, new sensesharing technologies (focused on eye gaze, haptics, odors, and so on), and devices offering real-life tracking of physical and cognitive activities and emotional states. At the same time, we must account for research in the fields of cognitive science, education, and psychology, which is providing us with an increased understanding of individual and group behaviors and of empathy and the fundamentals of learning.	augmented reality;cognitive science;computer science;digital revolution;experience;humans;interactive media;modality (human–computer interaction);real life	Niels Henze;Kai Kunze	2017	IEEE MultiMedia	10.1109/MMUL.2017.35	simulation;human–computer interaction;computer science;multimedia	HCI	-55.69939850977854	-35.73012838072151	33021
875eac99d7b7fae67d628ce66c53c869084131bb	whose line is it anyway? enabling creative appropriation of television	creative appropriation	creative appropriation	television	Erik Blankinship;Pilapa Esara	2005		10.1007/1-4020-2967-5_26	human–computer interaction;engineering;knowledge management;multimedia	HCI	-53.608869863359	-34.02637394487436	33023
821eeb05dcbba18da5317162a4568320433ead2a	ifeeling: vibrotactile rendering of human emotions on mobile phones	electrical engineering electronic engineering information engineering;signalbehandling;computer and information science;vibrotactile rendering;lip tracking;mobile phone;datoriserad bildanalys;emotion estimation;natural sciences;engineering and technology;signal processing;mobile communication;datavetenskap datalogi;computer science;computerized image analysis;telekommunikation;telecommunications;tactile coding	Today, the mobile phone technology is mature enough to enable us to effectively interact with mobile phones using our three major senses namely, vision, hearing and touch. Similar to the camera, which adds interest and utility to mobile experience, the vibration motor in a mobile phone could give us a new possibility to improve interactivity and usability of mobile phones. In this chapter, we show that by carefully controlling vibration patterns, more than 1-bit information can be rendered with a vibration motor. We demonstrate how to turn a mobile phone into a social interface for the blind so that they can sense emotional information of others. The technical details are given on how to extract emotional information, design vibrotactile coding schemes, render vibrotactile patterns, as well as how to carry out user tests to evaluate its usability. Experimental studies and users tests have shown that we do get and interpret more than one bit emotional information. This shows a potential to enrich mobile phones communication among the users through the touch channel.	mobile phone	Shafiq ur Réhman;Li Liu	2008		10.1007/978-3-642-12349-8_1	mobile search;human–computer interaction;engineering;multimedia;communication	HCI	-47.15259830292218	-42.24419288194274	33055
99469bba5728946fbbd3b38ba6d5c3f984adc3e3	web3d in ocean science learning environments: virtual big beef creek	virtual geography;virtual environments;three dimensional;video game;virtual reality modeling language;science learning;interface paradigms;software development;vrml;world wide web;virtual environment;collaborative virtual environment;graduate student;virtual worlds	The Virtual Reality Modeling Language (VRML), Java 3D software development packages, and World Wide Web (the Web) offer great potential for delivering three-dimensional, collaborative virtual environments to broad, on-line audiences. These capabilities have significant potential in ocean sciences, so a visualization environment was developed to explore these possibilities. The University of Washington's Virtual Big Beef Creek (VBBC) project has been continuously refined since its initial implementation in April 1999. VBBC affords users the ability to navigate through a data-rich representation of a physical world estuary on Washington State's Olympic Peninsula. One important project goal is to give users a better sense of the overall watershed before they venture out to experience it in person. A second significant goal is to provide a single on-line repository for geo-referenced data obtained through fieldwork (both quantitative and qualitative). The research team has gained insight into application improvements through the participation of Ocean Sciences graduate students, video game enthusiasts, and the general public. In this paper, research challenges, project successes, and project shortcomings are discussed that may inform the larger Web3D community.	collaborative virtual environment;field research;java 3d;modeling language;online and offline;software development;vrml;virtual reality;watershed (image processing);web3d;world wide web	Bruce Campbell;Paul Collins;Hunter Hadaway;Nick Hedley;Mark Stoermer	2002		10.1145/504502.504517	simulation;human–computer interaction;engineering;instructional simulation;multimedia	HCI	-60.64061832270179	-32.2330018079121	33063
2f8dfb45adc1f90c56bfa5af4f28e9dd8cae5cc1	fast gaze typing with an adjustable dwell time	words per minute;longitudinal study;text entry;dwell time;gaze typing;gaze input;error rate	Previous research shows that text entry by gaze using dwell time is slow, about 5-10 words per minute (wpm). These results are based on experiments with novices using a constant dwell time, typically between 450 and 1000 ms. We conducted a longitudinal study to find out how fast novices learn to type by gaze using an adjustable dwell time. Our results show that the text entry rate increased from 6.9 wpm in the first session to 19.9 wpm in the tenth session. Correspondingly, the dwell time decreased from an average of 876 ms to 282 ms, and the error rates decreased from 1.28% to .36%. The achieved typing speed of nearly 20 wpm is comparable with the result of 17.3 wpm achieved in an earlier, similar study with Dasher.	dasher;experiment;words per minute	Päivi Majaranta;Ulla-Kaija Ahola;Oleg Spakov	2009		10.1145/1518701.1518758	real-time computing;simulation;word error rate;computer science;dwell time	HCI	-47.12350997055131	-45.524707330580455	33075
72e9312288e5683668bb6173b597f21cf6ae7f13	a new production platform for authoring object-based multiscreen tv viewing experiences		Multiscreen TV viewing refers to a spectrum of media productions that can be watched on TV screens and companion screens (e.g., smartphones and tablets). TV production companies are now promoting an interactive and engaging way of viewing TV by offering tailored applications for TV programs. However, viewers are demotivated to install dozens of applications and switch between them. This is one of the obstacles that hinder companion screen applications from reaching mass audiences. To solve this, TV production companies need a standard process for producing multiscreen content, allowing viewers to follow all kinds of programs in one single application. This paper proposes a new object-based production platform for authoring programs for multiscreen. The platform consists of two parts: the preproduction tool and the live editing tool. To evaluate whether the proposed workflow is appropriate, validation interviews were conducted with professionals in the TV broadcasting industry. The professionals were positive about the proposed new workflow, indicating that the platform allows for preparations at the preproduction stage and reduces the workload during the live broadcasting. They see as well its potential to adapt to the current production workflow.	multi-screen video;object-based language;smartphone;tv tuner card;tablet computer	Jie Li;Thomas Röggla;Maxine Glancy;Jack Jansen;Pablo César	2018		10.1145/3210825.3210834	workload;multimedia;workflow;broadcasting;computer science;new production	HCI	-48.49596686954683	-29.862763386627226	33076
63e6241d77075feae9651ee580e27440ecd91e8b	sensitive chair: a force sensing chair with multimodal real-time feedback via agent	computer program;real time;user study;work environment;repetitive strain injury;multimodal user interface;posture;agent;real time feedback;sensor technique;visual display terminal;system integration;user satisfaction	Motivation -- The paper presents an alternative prevention system for Repetitive Strain Injuries (RSI) in visual display terminal (VDT) workers. The system integrates an ergonomically adjustable chair with a computer program that provides multimodal real-time feedback through an agent.  Research approach -- A user study was conducted to test the use of the sensitive chair in a natural work environment. Data regarding user satisfaction, awareness provided by the system, and actions taken in response to system were gathered.  Findings/Design -- Findings of the user study show that the participants were positive about the sensitive chair and that they became much more aware of their sitting posture.  Take away message -- The use of sensors and an agent to give real-time feedback about an adequate sitting posture was positively evaluated and has a strong effect on the awareness of sitting posture in VDT workers.	awareness;computer monitor;computer program;computer terminal;computer user satisfaction;electronic visual display;human factors and ergonomics;multimodal interaction;poor posture;real-time clock;real-time transcription;repetitive strain;sensor;status message (instant messaging);usability testing;x86	I. Daian;Annemieke van Ruiten;Albertine Visser;S. Zubic	2007		10.1145/1362550.1362583	embedded system;simulation;engineering;multimedia;system integration	HCI	-48.87896958845087	-48.24480017318314	33081
de892b92ba53f213151786be15d803e9b0369965	multi-agents based virtual environments for cultural heritage		This paper focuses on the integration of artificial intelligence methods in multi-agent virtual environments for cultural heritage applications and especially virtual museums and exhibitions. Multi-agent systems are considered here as being autonomous social organizations capable of developing dynamic personalized virtual environments. In this respect, some of the most common artificial intelligence methods in intelligent virtual environments are discussed. Three different sub-domains of intelligent virtual environments are identified and presented, aiming at an enhanced virtual reality experience. The paper concludes by highlighting the importance of artificial intelligence applications in cultural heritage, and the dynamics of the virtual museums with personalized content, based on the profile of the user and the need of the case.	applications of artificial intelligence;autonomous robot;multi-agent system;personalization;virtual reality	Chairi Kiourt;George Pavlidis;Anestis Koutsoudis;Dimitrios Kalles	2017	2017 XXVI International Conference on Information, Communication and Automation Technologies (ICAT)	10.1109/ICAT.2017.8171602	social organization;multimedia;applications of artificial intelligence;cultural diversity;exhibition;virtual reality;cultural heritage;engineering;multi-agent system	AI	-33.934515569203235	-24.545155117031516	33091
bf96f33504df97c356f4a924be5df6032346a28a	adaptive video techniques for informal learning support in workplace environments		Learning at the workplace is largely informal and there is a high potential to make it more effective and efficient by means of technology, especially by using the power of multimedia. The main challenge is to find relevant information segments in a vast amount of multimedia resources for a particular objective, context and user. In this paper, we aim to bridge this gap using a personalized and adaptive video consumption strategy for professional communities. Our solution highlights relevant concepts within segments of video resources by means of collaborative semantic annotations, analyzes them based on the user’s learning objectives and recomposes them anew in a personalized way. As the preferred adaptation may be context dependent, the user has the opportunity to select a predefined adaptation strategy or to specify a new one easily. The approach uses a Web-based system that outputs a relevant mix of information from multiple videos, based on the user preferences and existing video annotations. The system is open source and uses an extendable approach based on micro-services. The performed evaluation investigated the usability and usefulness of the approach. It showed that effectiveness and especially efficiency of such informal learning could be indeed better with adaptive video techniques applied. On the other hand, collected ideas on how to improve the usability of the system show opportunities for its further improvements. These results suggest that personalization and adaptive techniques applied on video data are a good direction to proceed in facilitating informal learning in workplace environments.	extensibility;microservices;open-source software;personalization;usability;user (computing);utility;world wide web	Milos Kravcik;Petru Nicolaescu;Aarij Siddiqui;Ralf Klamma	2016		10.1007/978-3-319-52836-6_57	simulation;knowledge management;multimedia	HCI	-40.97656740402934	-25.58076817872268	33162
5021a930e60371d01536136afadc382aed513914	growkit: a kit for organisms to promote personalized steam learning		Hybrid systems between biology and computation to study living organisms have demonstrated potential in promoting children's science experience and better understanding of their actions on the environment. However, these systems offer limited interactions between the user and the biological subject caused by inflexible equipment and missing possibilities to interfere with the biological subject through an interface. We present GrowKit, a digital/physical construction kit for living organisms that enables children to personalize their own experiments in biology. Our findings suggest that the comprehensive scaffolding offered by storytelling cards, experimental building blocks and remote lab software allows young learners to explore a broad range of biological ideas and conduct personally meaningful experiments, and promotes engagement and curiosity in children. We present GrowKit, a digital/physical construction toolkit for biology that provides young learners with playful STEM experience of designing, making, and conducting experiments.	computation;experiment;interaction;personalization	Helene Steiner;Seokbin Kang;Asta Roseway;Richard Banks	2018		10.1145/3170427.3186605	human–computer interaction;curiosity;hybrid system;storytelling;software;computation;scaffold;computer science	Comp.	-56.96268715362292	-50.9307993364798	33196
68c49251273a8aa37cbc13de0f0a9b8c0d472287	assets: where do we go from here?	old age;universal design;design and development;design method;assistive technology	There are enormous opportunities for the Assets community in many of the significant changes which have occurred in the social, legal, demographic and economic landscape over the past ten to fifteen years. These changes will have a significant impact on the design and development of systems for older and disabled people. This keynote will bring together a number of proposals to improve both specialist and mainstream design methods in the field, and encourage a debate about the concepts of Universal Design and the future of design for older and disabled people.		Alan F. Newell	2002		10.1145/638249.638251	simulation;universal design;design methods;human–computer interaction	HCI	-62.2071817465926	-29.418801325853423	33212
69e3d139ea75628799aa1fd0613c611fc269519b	germic: application of gesture recognition model with interactive correction to manual grading tasks		Gesture-based recognition is one of the most intuitive methods for inputting information and is not subject to cumbersome operations. Recognition is performed on human’s consecutive motion without reference to retrial or alternation by user. We propose a gesture recognition model with a mechanism for correcting recognition errors that operates interactively and is practical. We applied the model to a setting involving a manual grading task in order to verify its effectiveness. Our system, named GERMIC, consists of two major modules, namely, handwritten recognition and interactive correction. Recognition is materialized with image feature extraction and convolutional neural network. A mechanism for interactive correction is called on-demand by a user-based trigger. GERMIC monitors, track, and stores information on the user’s grading task and generates output based on the recognition information collected. In contrast to conventional grading done manually, GERMIC significantly shortens the total time for completing the task by 24.7% and demonstrates the effectiveness of the model with interactive correction in two real world user environments.	gesture recognition	Kohei Yamamoto;Fumiya Kan;Kazuya Murao;Masahiro Mochizuki;Nobuhiko Nishio	2018		10.1007/978-3-319-90740-6_6	gesture recognition;convolutional neural network;computer security;grading (education);computer science;machine learning;feature extraction;handwriting recognition;artificial intelligence;gesture	HCI	-38.83178958686257	-41.53173171056724	33226
cdf9f1f5daf92035323921846fe2b8615b74426a	the v-city project	categories and subject descriptors according to acm ccs h 5 1 hci multimedia information systems	3D geoinformatics have entered the digital age, hesitantly in some areas, and rampantly in others. Google Earth and Microsoft Virtual Earth are household names. However, these projects are limited to textured 3D landscapes, aerial 2D images and a few boxy building envelopes. The V-City project is a European research initiative to surpass these limitations, and create a system for intuitively exploring large urban areas with a high degree of detail. Bringing together technologies from geoinformatics, virtual reality, computer graphics, and computer vision, the system constructs detailed 3D city models from geopositioned aerial images and building footprints. For networked browsing, city models are compressed and streamed for interactive viewing of entire landscapes. A unique tactile table has also been developed to let multiple users visualize the same city model in stereo 3D, and interact with it simultaneously using hand gestures.	3d city models;aerial photography;bing maps platform;computer graphics;computer vision;data compression;geoinformatics;google earth;multi-user;streaming media;usability testing;virtual reality	Jesse Himmelstein;Olivier Balet;Fabio Ganovelli;Enrico Gobbetti;Matthias Specht;Pascal Müller;Chris Engels;Luc Van Gool;Jean-Baptiste de la Rivière;Armando Cavazzini	2011		10.2312/PE/VAST/VAST11S/057-060	human–computer interaction;computer science;multimedia	HCI	-49.113215705530855	-30.75998177196673	33243
5e8f1a7b94f6b820ab73859351a952d002161061	graphical user interface syle guide for mobile communication services	goal orientation;mobile communication;graphic user interface;mobile user	All applications used by a mobile user will have different looks unless the developers have all referred to a common style guide on building the interface. This will create a common look and feel, and for the user it means using a homogeneous set of tools. These will be even easier to use if they work according to man's logic. We need intuitive interfaces, hence our goal oriented approach in the style guide. The end-user must know what he wants, but not how to obtain it (all the intermediate steps performed for actually accomplishing the desired action).	graphical user interface;mobile phone	Martine Abramovici;Niels Klußmann	1994		10.1007/BFb0013401	user interface design;10-foot user interface;user experience design;mobile search;mobile web;shell;human–computer interaction;mobile database;multimedia;mobile station;natural user interface;user interface;mobile computing;world wide web	HCI	-42.516124006776515	-30.6235833577238	33262
bafc90e102b9b1e3dc36c76893bcfb971027f3fe	leveraging mobile technology to support goal setting and strategies of college students		The purpose of this study is to understand (1) college students' goals and the strategies needed to achieve these goals through a bottom-up approach and (2) the use of mobile technology to support such strategies. To do this, we conducted a user study with a total of 295 undergraduate and graduate students. We identified four primary goals and eight strategies. We mapped each strategy to mobile sensor data and derived specific use scenarios. By analyzing and visualizing the collected sensor data, we developed a smartphone app which is expected to help students achieve their goals.	mobile app;smartphone;top-down and bottom-up design;usability testing	Bogoan Kim;So Young Rhim;Kyungsik Han;Seok-Won Lee	2018		10.1145/3267305.3267653	human–computer interaction;multimedia;mobile technology;computer science;goal setting	HCI	-61.829027732834184	-44.190854241182805	33310
e1ededf606af7c9bb7885fff75ff0145d935ff9c	event perception in mobile interaction: toward better navigation history design on mobile devices	mobile user interfaces;mobile device;individual difference;information navigation;mobile interaction	This article explores how people perceive interactive activities in order to inform navigation history design on mobile devices. Following event segmentation method, 12 participants were asked to break 6 episodes of mobile interaction into segments, organize the segments, and identify those deemed representative. Three findings emerged. First, when making sense of mobile interaction, users concentrate on the content objects on which actions are performed. This indicates the value of content-centric designs in navigation history and other mobile user interface designs. The content objects are data objects and their collections meaningful to the person dealing with it, for example, photos, messages, or albums. Second, users tend to employ two-level hierarchies in grouping segments, and use the similarity in content objects and applications as a reference. They deem the segments as representative where objects are created or changed, or where sharing or querying acts take place. These findings indicate how a navigation history design should organize and prioritize mobile interaction events. Finally, event perception shows relatively low interparticipant consensus, which indicates that navigation history designs have to accommodate large individual differences.	digital history;mobile device;mobile interaction;user interface	Yanqing Cui;Antti Oulasvirta;Lingyi Ma	2011	Int. J. Hum. Comput. Interaction	10.1080/10447318.2011.552058	computer vision;mobile search;mobile interaction;human–computer interaction;mobile database;computer science;operating system;mobile device;multimedia;world wide web;mobile robot navigation	HCI	-55.60587397081795	-43.032337517094504	33341
1239191ca4f921ab63e2fbd1a8b0901a778a20af	an architecture approach for 3d render distribution using mobile devices in real time	tecnologias generalidades;tecnologias	— Nowadays, video games such as Massively Multiplayer Online Game (MMOG) have become cultural mediators. Mobile games contribute to a large number of downloads and potential benefits in the applications market. Although processing power of mobile devices increases the bandwidth transmission, a poor network connectivity may bottleneck Gaming as a Service (GaaS). In order to enhance performance in digital ecosystem, processing tasks are distributed among thin client devices and robust servers. This research is based on the method 'divide and rule', that is, volumetric surfaces are subdivided using a tree-KD of sequence of scenes in a game, so reducing the surface into small sets of points. Reconstruction efficiency is improved, because the search of data is performed in local and small regions. Processes are modeled through a finite set of states that are built using Hidden Markov Models with domains configured by heuristics. Six test that control the states of each heuristic, including the number of intervals are carried out to validate the proposed model. This validation concludes that the proposed model optimizes response frames per second, in a sequence of interactions.	cloud gaming;digital ecosystem;heuristic (computer science);hidden markov model;interaction;markov chain;massively multiplayer online role-playing game;mobile device;thin client	Holman Bolívar Barón;John Alexander Velandia;Jenny Natalia Torres;Elena Giménez de Ory	2015	IJIMAI	10.9781/ijimai.2015.337	simulation;computer science;theoretical computer science;distributed computing	AI	-34.41543918066794	-37.30875197673944	33379
077ed476a6398df6fd4c7d2985a7bce2419438f9	barehands: implement-free interaction with a wall-mounted display	image processing;touch screen;user interface;touch interaction;hand posture;interactive user interface;infrared;interaction technique	"""We describe Barehands, a free-handed interaction technique, in which the user can control the invocation of system commands and tools on a touch screen by touching it with distinct hand postures. Using behind-screen infrared (IR) illumination and a video camera with an IR filter, we enable a back-projected SMARTBoard (a commercially available, 61 3/8"""" x 47"""" touch-sensing display) to identify and respond to several distinct hand postures. Barehands provides a natural, quick, implement-free method of interacting with large, wall-mounted interactive surfaces."""	interaction technique;smart board;touchscreen	Meredith Ringel Morris;Henry Berg;Yuhui Jin;Terry Winograd	2001		10.1145/634067.634284	computer vision;infrared;human–computer interaction;image processing;computer science;multimedia;user interface;interaction technique;computer graphics (images)	HCI	-43.599769043147084	-41.01225620557394	33398
8b965e416d915707c40f826abe117ed023d58014	tanagra: an intelligent level design assistant for 2d platformers	level design;procedural content generation;design tools;level generation	We present a demonstration of Tanagra, an intelligent level design assistant for 2D platformers. Tanagra integrates reactive planning and constraint programming to automatically generate levels and respond to designer changes in realtime. A reactive planning language, ABL, allows us to easily express hierarchical patterns of level geometry, from simple patterns such as a jump over a gap, to more complex patterns such as staircases and pits. A model of player movement and constraints within and between geometry components ensure that the levels created by our system are always playable.	constraint programming;level design;openedge advanced business language (abl);reactive planning;tanagra	Gillian Smith;E. James Whitehead;Michael Mateas	2010			simulation;level design;computer science;artificial intelligence	AI	-37.65784908478963	-34.226358539214665	33431
de9379f924def70a93d89552f9a0ac62ac7cba20	explorative analysis of recommendations through interactive visualization		Even though today’s recommender algorithms are highly sophisticated, they can hardly take into account the users’ situational needs. An obvious way to address this is to initially inquire the users’ momentary preferences, but the users’ inability to accurately state them upfront may lead to the loss of several good alternatives. Hence, this paper suggests to generate the recommendations without such additional input data from the users and let them interactively explore the recommended items on their own. To support this explorative analysis, a novel visualization tool based on treemaps is developed. The analysis of the prototype demonstrates that the interactive treemap visualization facilitates the users’ comprehension of the big picture of available alternatives and the reasoning behind the recommendations. This helps the users get clear about their situational needs, inspect the most relevant recommendations in detail, and finally arrive at informed decisions.	algorithm;book;interactive visualization;interactivity;prototype;recommender system;systems theory;treemapping	Christian Richthammer;Günther Pernul	2016		10.1007/978-3-319-53676-7_4	recommender system;visualization;computer science;data mining;interactive visualization;comprehension	HCI	-58.33267334282704	-31.29251943892319	33519
e045b2da7927caae66d8ccc8c80570fce938e900	dirty desktops: using a patina of magnetic mouse dust to make common interactor targets easier to select	mouse;bottom up;magnetic field;user interface;prior knowledge;human factors;pointer;adaptation;graphic user interface;snapping	A common task in graphical user interfaces is controlling onscreen elements using a pointer. Current adaptive pointing techniques require applications to be built using accessibility libraries that reveal information about interactive targets, and most do not handle path/menu navigation. We present a pseudo-haptic technique that is OS and application independent, and can handle both dragging and clicking. We do this by associating a small force with each past click or drag. When a user frequently clicks in the same general area (e.g., on a button), the patina of past clicks naturally creates a pseudo-haptic magnetic field with an effect similar to that ofsnapping or sticky icons. Our contribution is a bottom-up approach to make targets easier to select without requiring prior knowledge of them.	accessibility;bottom-up proteomics;desktop computer;drag and drop;graphical user interface;haptic technology;library (computing);operating system;pointer (computer programming);sticky bit;top-down and bottom-up design	Amy Hurst;Jennifer Mankoff;Anind K. Dey;Scott E. Hudson	2007		10.1145/1294211.1294242	simulation;pointer;magnetic field;human–computer interaction;computer science;human factors and ergonomics;operating system;top-down and bottom-up design;graphical user interface;user interface;world wide web;adaptation	HCI	-46.183393320299565	-41.39422334908596	33524
bfaa05b84d77ccbf412da508304d1916e53aee46	user engineering principles for interactive systems	new pattern;self-consistent pattern;successive action;interactive system;user engineering principle;good composer	The 'feel' of an interactive system can be compared to the impressions generated by a piece of music. Both can only be experienced over a period of time. With either, the user must abstract the structure of the system from a sequence of details. Each may have a quality of 'naturalness' because successive actions follow a logically self-consistent pattern. Finally, a good composer can write a new pattern which will seem, after a few listenings, to be so natural the observer wonders why it was never done before.	age of wonders ii: the wizard's throne;interactivity	Wilfred J. Hansen	1971		10.1145/1479064.1479159	simulation;engineering;multimedia;communication	DB	-48.191747879854084	-36.04171003848418	33534
08377a4de672b0486516417c8fbb21d7db335810	using a plush toy to direct synthetic characters	3d virtual environment;wireless sensor;input device;virtual characters;synthetic character;young children;sympathetic interface;plush toy;physically based interface;synthetic characters;animated character;virtual worlds	We introduce the concept of a sympathetic inter$ace forcontrolling an animated synthetic character in a 3D virtualenvironment. A plush doll embedded with wireless sensors is used tomanipulate the virtual character in an iconic and intentionalmanner. The interface extends from the novel physical input devicethrough interpretation of sensor data to the behavioral brain ofthe virtual character. We discuss the design of the interface andfocus on its latest instantiation in the Swamped! exhibit atSIGGRAPH 98. We also present what we learned from hundreds ofcasual users, who ranged from young children to adults.	3d film;armature (computer animation);digital life;embedded system;game engine;haptic technology;jed;robot;sensor;synthetic intelligence;toys;universal instantiation	Michael Patrick Johnson;Andrew D. Wilson;Bruce Blumberg;Christopher Kline;Aaron F. Bobick	1999		10.1145/302979.303028	computer science;operating system;multimedia;input device;computer graphics (images)	HCI	-46.03713463997735	-36.01456879646748	33561
9bd23d9a06477b4c9bd2d96d3fdf7aa6173a6d1f	beyond bookmarks: enriching web information. a demonstration of the nestor web browser and cartographer	navigation		cartography;nestor (encryption)	Romain Zeiliger	2000			engineering;web navigation;web page;database;client-side scripting;world wide web;information retrieval	Web+IR	-43.249425014234916	-23.98410796535976	33593
e20e16a9a50d21e4de4e75db7e0fc6e9fe3a0837	presentation of human action information via avatar: from the viewpoint of avatar-based communication	systeme temps reel;animacion por computador;mesure deplacement;mouvement corporel;activite humaine;realite virtuelle;realidad virtual;degree of freedom;real time;serveur informatique;symbolization;virtual reality;cache memory;simbolizacion;intelligence artificielle;cuerpo humano;corps humain;antememoria;antememoire;limu;displacement measurement;interactive system;human motion;human body;temps reel;rhp;tiempo real;servidor informatico;artificial intelligence;symbolisation;real time system;medicion desplazamiento;actividad humana;sistema tiempo real;inteligencia artificial;virtual environment;computer animation;movimiento corporal;human activity;body movement;computer server;animation par ordinateur	This paper describes techniques to present human action information on an avatar-based interaction system, using real-time motion sensing and human action symbolization. Avatar-based interaction systems with computer-generated virtual environments have difficulties in acquiring user's information, i.e., enough information to represent the user as if he/she were in the environment. This mainly comes of high degrees of freedom of human body and causes the lack of reality. Since it is almost impossible to acquire all the detailed information of human actions or activities, we, instead, recognize, or estimate, what kind of actions have occurred from sensed human motion information and other available information and re-generate detailed and natural actions from the estimated results. In this paper, we describe our approach, Real-time Human Proxy, especially on representing human actions. Also we present experimental results.	viewpoint	Daisaku Arita;Rin-ichiro Taniguchi	2005		10.1007/11553939_125	human body;simulation;cpu cache;computer science;virtual machine;artificial intelligence;operating system;virtual reality;computer animation;degrees of freedom;server	Vision	-35.32519361276281	-26.75995373115517	33611
0c98a80e69370d3960bbd51865d6eb2686db5d05	an information acquiring channel - lip movement	interfase usuario;user interface;lip;man machine system;lecture labiale;levre;cognition;speaker dependent;cognicion;sistema hombre maquina;labio;lectura labial;interface utilisateur;speaker;locutor;locuteur;lip reading;systeme homme machine	This paper is to prove that lip-movement is an available channel for information acquiring. The reasoning is given by describing two kinds of valid applications, which are constructed on lip movement information only. One is lip-reading, the other is lip-movement utterance recognition. The accuracy of the former system with speaker-dependent could achieve 68%, and of the latter achieves over 99.5% for testindependent (TI) and nearly 100% for test-dependent (TD) in experiments till now. From this conclusion, it could be easily got that lip-reading channel is an effective one and can be applied independently.	experiment	Xiaopeng Hong;Hongxun Yao;Qinghui Liu;Rong Chen	2005		10.1007/11573548_30	loudspeaker;speech recognition;cognition;computer science;artificial intelligence;operating system;user interface	Crypto	-39.558403779660594	-49.02174158234762	33715
dbad30582bddfd799e3b3166326285d28745186f	from doing to being: getting closer to the user experience	task performance;human computer interaction;media quality;skin conductance;heart rate;quality evaluation;user experience;blood volume pulse;psychophysiological measurements;subjective assessment;user satisfaction;physiological response	The research by Scheirer et al. (2002) is pivotal in promoting the use of psychophysiological measures in HCI. We argue that rather than inferring users’ emotional states from the data, which is difficult to do reliably, the signals can be used as an indicator of user cost by monitoring changes in users’ physiological responses. We applied this approach by monitoring Skin Conductance, Heart Rate and Blood Volume Pulse (as well as task performance and user satisfaction) to investigate the impact of media quality degradations on users. Five studies were conducted utilising this approach. Results show that psychophysiological data show responses to audio and video degradations: users respond to specific degradations with increased levels of arousal. In addition, psychophysiological responses do not always correlate with each other and subjective and physiological measures do not always concur, which means that psychophysiological data may detect responses that users are either not aware of or cannot recall at post-session subjective assessment. We thus conclude that psychophysiological measures have a valuable role to play in media quality evaluation. q 2004 Elsevier B.V. All rights reserved.	conductance (graph);human–computer interaction;pulse;user experience	Gillian May Wilson;M. Angela Sasse	2004	Interacting with Computers	10.1016/j.intcom.2004.06.001	user experience design;simulation;human–computer interaction;computer science;skin conductance;multimedia	HCI	-57.140298428236655	-47.05741967631748	33718
28fd2362db8a21dfd4e47e3ef4988bd6f79478b6	an object-oriented architecture for intelligent virtual receptionists	object oriented approoch;user profile;virtual receptionist;intelligent agents;object oriented approach;customized marketing	Because of the rapid proliferation of Web sites, the question of how a Web site can attract repeat visits has become an important problem. One approach is to treat each visitor to the site as a unique individual, much as a human receptionist would treat each client-customer. This paper proposes an object-oriented architecture for developing virtual receptionists for this purpose. All necessary object classes, their interactions, and the database schema are defined, and an example is provided. This object-oriented design proVides a foundation that allows the virtual agent to be easily modified.	database schema;interaction	Hsiangchu Lai	2000	Int. J. Electronic Commerce	10.1080/10864415.2000.11518372	simulation;computer science;artificial intelligence;multimedia;world wide web;intelligent agent	DB	-42.12608979181139	-27.581165766266615	33731
d5f9a11e316e4ba13ef273609d45a22606108e92	supporting creative work in educational digital libraries	web based applications;story telling;digital library;educational digital libraries;creative work	Educational digital libraries have become an effective source of sharing information and dissemination of knowledge. Like paintings, personal stories containing pictures, music, and text will exist for a long time to come and will be enjoyed long after their creation. In this demo, we present a system called Creaza Education. The system offers an engaging suit of are user-friendly web-based applications where users can use their imagination by creating, publishing and sharing digital stories.	digital library;image;library (computing);usability;web application	Naimdjon Takhirov	2011		10.1145/1998076.1998186	web application;digital library;computer science;multimedia;world wide web	HCI	-47.059742675562035	-24.23770631770126	33743
03d16018a30b7b3033cc356278beb234a6e9a396	mediating performance through virtual agents	ucl;virtual reality;discovery;theses;conference proceedings;digital web resources;ucl discovery;open access;ucl library;book chapters;open access repository;virtual agent;ucl research	This paper presents the process of creation of virtual agents used in a virtual reality performance. The performance aimed to investigate how drama and performance could inform the creation of virtual agents and also how virtual reality could raise questions for drama and performance. The virtual agents were based on the performance of 2 actors. This paper describes the process of preparing the actors, capturing their performances and transferring them to the virtual agents. A second set of agents was created using non-professional ‘näıve performers’ rather than actors.	experiment;intelligent agent;performance;virtual reality	Gabriella Giannachi;Marco Gillies;Nick Kaye;David Swapp	2009		10.1007/978-3-642-04380-2_48	human–computer interaction;computer science;artificial intelligence;instructional simulation;virtual reality;multimedia	Visualization	-53.48022112792595	-48.02740004161204	33795
c5912af5d1383b865b77b928368ded7cf9915f1d	diminishing chat confusion by multiple visualizations	chat tool;chat transcript;multiple visualizations;threading;modular architecture	In this article, we address the problem of confusion and co-text-loss in chat communication, identify requirements for a solution, discuss related work and present a new approach for addressing co-text loss in text-based chats. We report about first experiences with our solution and give an outlook on future work directions. The core idea of our solution MuViChat (multiple-visualization chat) is to support multiple visualizations of referenced chat transcripts in which users can choose their preferred view. The multiple visualizations offer different possibilities to follow and understand a communication and thereby diminish chat confusion which often occurs in standard chat systems. By enabling the recording and replaying of chat discussions and an extensible modular architecture we are supporting evaluation and further integration of advanced visualization concepts.	data logger;experiment;internet access;java;microsoft outlook for mac;open-source license;open-source software;requirement;text-based (computing);xml	Torsten Holmer;Stephan Lukosch;Verena Kunz	2009	J. UCS	10.3217/jucs-015-16-3139	human–computer interaction;threading;computer science;multimedia;world wide web	HCI	-53.01414249487756	-40.643454757971604	33844
a9f443bd5c521b8b05b684b94320ba15fb21c8f0	glassclass: exploring the design, implementation, and acceptance of google glass in the classroom		Google Glass is worn like a pair of eye-glasses and is controlled with a small screen, touchpad, and microphone. A variety of Augmented Reality and Mixed Reality Glassware applications are available for Glass. However, due to the size and position of the screen, it is hard for onlookers to discern what the user is doing while using these applications. Additionally, the user can surreptitiously take pictures and record videos of nearby people and things, resulting in privacy concerns. We hypothesized that use of Glassware in a specific domain, where onlookers were apprised of the use of the Glassware, would be better accepted than the more generic use of Glassware. This paper reports on our design, implementation and evaluation of several Glass applications to enhance communication between teachers and students in the classroom and presents results from a study that suggests that students accept the use of Glassware in this environment.	augmented reality;eye of horus;glass;image;microphone;mixed reality;privacy;television;touchpad	Dave A. Berque;James T. Newman	2015		10.1007/978-3-319-21067-4_25	simulation;engineering;knowledge management;multimedia	HCI	-47.559046190104986	-43.57546514042013	33887
f97cb64b2905425dc49bd9601d4ce8b8566fb3fe	effect of physical workload on navigation task performance by high-fit young males	cybernetics;training;navigation;time factors;estimation;analysis of variance;conferences	Many occupations require both physical exertion and the ability to navigate in an environment, simultaneously. This study investigated how intensity of physical activity influences direction determination and distance estimation. Thirty high fit young males participated in a lab study. Results showed that while high fit young males were accurate in determining direction across levels of physical exertion, they were significantly less accurate in distance estimation under high exertion intensity. Although physical activity level did not influence direction determination accuracy, response time was significantly shorter when participants were subject to low physical loading in comparison to medium and high loading. In addition, we found that distance estimation response time increased as physical workload increased. Findings of this study can be used to enhance presentation of navigation information in occupations that require concurrent physical activity and navigation.	response time (technology)	Maryam Zahabi;Wenjuan Zhang;Carl Pankok;Mei Ying Lau;James Shirley;David B. Kaber	2016	2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2016.7844265	estimation;navigation;real-time computing;simulation;analysis of variance;cybernetics;computer science;statistics	HCI	-47.42906894889474	-50.085186833931004	33893
6baffa7efed6ddbb9605b8c81e01066277546003	scenario-based requirements engineering facilitating interaction design	requirements engineering;usability;usage scenarios;user interfaces;interaction design	When the requirements and the interaction design of a system are separated, they will most likely not fit together, and the resulting system will be less than optimal. Even if all the real needs are covered in the requirements and also implemented, errors may be induced by human-computer interaction through a bad interaction design and its resulting user interface. Such a system may even not be used at all. Alternatively, a great user interface of a system with features that are not required will not be very useful as well. Therefore, the primary motivation of this tutorial is to improve system development in practice both regarding requirements engineering and interaction design, especially facilitating the latter. We argue for combined requirements engineering and interaction design, primarily based on usage scenarios in the sense of sequences of actions aimed at accomplishing some task goal. However, scenario-based approaches vary especially with regard to their use, e.g., employing abstract use cases or integrating scenarios with functions and goals in a systematic design process. So, the key issue to be addressed is how to combine different approaches, e.g., in scenario-based development, so that the result is an overall useful and useable system. In particular, scenarios are very helpful for purposes of usability as well.	human–computer interaction;interaction design;requirement;requirements engineering;usability;user interface	Hermann Kaindl	2011		10.1007/978-3-642-23768-3_126	requirements analysis;simulation;interactive systems engineering;usability;human–computer interaction;computer science;requirement;interaction design;requirements engineering;user interface;non-functional requirement	HCI	-52.76710112438278	-37.66060118542282	33915
a7231b70a7ab8dfccf849e50f7f58a3140178337	interpreting situated dialogue utterances: an update model that uses speech, gaze, and gesture information		In situated dialogue, speakers share time and space. We present a statistical model for understanding natural language that works incrementally (i.e., in real, shared time) and is grounded (i.e., links to entities in the shared space). We describe our model with an example, then establish that our model works well on nonsituated, telephony application-type utterances, show that it is effective in grounding language in a situated environment, and further show that it can make good use of embodied cues such as gaze and pointing in a fully multi-modal setting.	entity;modal logic;natural language;situated;statistical model	Casey Redd Kennington;Spyros Kousidis;David Schlangen	2013			natural language processing;speech recognition;computer science;communication	NLP	-34.27688661957229	-41.648780990298306	33923
1737970e800a7541f27c1829a6749bcc408fafcf	pain matters: outliers in new tribes and territories	art as research;virtual reality;interdisciplinary collaboration;chronic pain	ABSTRACTThis paper is an account of research collaborations and an interdisciplinary community that is being forged among experts who have few traditions in common, but who bring diverse expertise to the seemingly unreachable ‘moonshot’1 problem that is chronic pain. It provides analyses of how such collaborations among outliers—from health research, pain medicine, art, design, music, computer science and engineering—arose, and how these collaborators found support in usual and unlikely places, and created a small but vibrant and growing community of researchers. Specifically, this account focuses on artists and designers in this collaborative context; what was required of them, their roles in changing the very conditions of research, how their relationships to their originating (or ‘legacy’) discipline changed, and how their commitments profoundly redefined their very notions of what art can mean.		Diane Gromala	2016	Digital Creativity	10.1080/14626268.2016.1250013	visual arts;human–computer interaction;computer science;knowledge management;artificial intelligence;virtual reality;sociology;management;social psychology	HCI	-61.031461406857645	-34.77725149132524	33932
76493dc24ac100afa2a670024748e200eab55f27	practice system for controlling cutting pressure for paper-cutting		We describe a system for paper-cutting with a knife with a blade attached to the tip of the stylus on a drawing display. The purpose of this research is to support controlling cutting pressure for novices. Novices tend to cut paper with an unstable pressure stronger than necessary. Therefore, some instructors teach practicing to control the pressure to novices We have developed a device to support cutting practice by a stylus and drawing display. We measured the difference between pressures of novices and experts. In addition, we developed a system that encourages appropriate pressure based on expert pressure. This system shows the pressure difference between the user and the experts in color and sound. We experimented to compare the effectiveness of the system. As a result, the novices practiced with the system, the range of pressure and variation improved than the existing practice method.	color;control theory;stylus (computing)	Takafumi Higashi;Hideaki Kanai	2018		10.1145/3279778.3281457	engineering drawing;stylus;computer science	HCI	-40.937459232986185	-41.11538416691046	33939
21200f473d5d7f08377e331b271df6235739c831	a two-step click interaction for mobile internet on smartphone		Mobile Internet gains popularity due to the increasing use of smartphones having wireless network capabilities. However, the current click interaction method (hereafter, CC) hinders user experience when the size of the target hyperlink to be selected is small. The present study developed a two-step click interaction method (called Press and Flick; hereafter PF) for smartphone and evaluated its effectiveness by GOMS model. GOMS results indicate that the PF has a substantial benefit compared to the CC when a click error is occurred. The PF can enhance usability and user experience (UX) by reducing click error and providing a joyful interaction.	goms;hyperlink;smartphone;usability;user experience;user interface design	Kihyo Jung;Jinah Jang	2013		10.1007/978-3-642-39473-7_27	mobile search;mobile web	HCI	-50.80502699559777	-42.25971850122492	34008
f3105b277f3a18da8900c71b0dd6c150e9ff7c18	magnetic marionette: magnetically driven elastic controller on mobile device	tui;mobile device;conference;pseudo sensor;magnetometer;sensor repurposing;pattern recognition;tangible controller	In this paper, we present the Magnetic Marionette, a magnetically driven elastic controller that enables tangible interaction on mobile devices. This technique can determine eight different gestures in excess of 99% accuracy by sensing and tracking the magnets embedded on the controller. The advantage of this technique is that it is lightweight, battery-free, and inexpensive because it uses a magnetometer, which is already embedded in smart phones today. This simple and noble technique allows users to achieve richer tactile feedback, expand their interaction area, and enhance expressiveness without the need for hardware modification.	embedded system;mobile device;offset binary;smartphone;tangible user interface	Sungjae Hwang;Myungwook Ahn;KwangYun Wohn	2013		10.1145/2451176.2451207	embedded system;magnetometer;real-time computing;simulation;computer science;operating system;mobile device	HCI	-43.323328314193496	-41.75031324669863	34071
bf6b3d0ba5e94377d9521fb736b5e915660365f0	new research perspectives on ambient intelligence	social intelligence;ambient intelligence;user centered design;natural interaction;article letter to editor;experience research	Ten years of AmI research have led to many new insights and understandings about the way highly interactive environments should be designed to meet the requirement of being truly unobtrusive and supportive from an end-user perspective. Probably the most revealing finding is the fact that, in addition to cognitive intelligence and computing, also elements from social intelligence and design play a dominant role in the realization of the vision. In this paper we discuss these novel insights and their resulting impact on the AmI research landscape. We introduce a number of new AmI research perspectives that are related to social intelligence and in addition we argue that new ways of working are required applying the concept of Experience Research resulting in a true user-centered approach to Ambient Intelligence.	ambient intelligence;artificial intelligence;cobham's thesis;programming paradigm;threat (computer);unobtrusive javascript;user-centered design	Emile H. L. Aarts;Boris E. R. de Ruyter	2009	JAISE	10.3233/AIS-2009-0001	intelligence cycle;user-centered design;simulation;ambient intelligence;human–computer interaction;computer science;artificial intelligence;social intelligence	AI	-56.24318552723312	-36.21091958510645	34104
d44da799da439baf0755f43ab07ebaf9447c0e44	altering speed perception through the subliminal adaptation of music within a vehicle		We consider the potential for novel in-vehicle user-interfaces that alter speed perception at a subliminal level through the spatial adaption of music. In a fixed-base simulator, twenty-six participants drove on a motorway and were asked to maintain a speed of 70mph. At specific points, the speedometer was turned off. Music at a constant tempo was played throughout but periodically changed in balance from a 50:50 front:rear speaker split to a 25:75 ratio. Without the speedometer, participants drove significantly slower after the music had faded from the front to rear speakers (mean speed 71.5mph) compared to when no change occurred (mean speed 73.1mph). Post study interviews revealed that participants were not aware of alterations in the spatial positioning of the music. Such results suggest drivers naturally slowed when the music faded from front to rear speakers in an unconscious attempt to re-envelope themselves within the sound bubble.	simulation	Gary E. Burnett;Adrian Hazzard;Elizabeth Crundall;David Crundall	2017		10.1145/3122986.3122990	simulation;communication;perception;subliminal stimuli;psychology;speedometer	HCI	-46.75241447247054	-50.412700049803654	34113
cd505cd059b41170d49f3d3189f37df6644d1fcc	predictive text input in a mobile shopping assistant: methods and interface design	user interface;user study;recommendations;text input;interface design;adaptive;association rule;error rate;usability;user satisfaction	The fundamental nature of grocery shopping makes it an interesting domain for intelligent mobile assistants. Even though the central role of shopping lists is widely recognized, relatively little attention has been paid to facilitating shopping list creation and management. In this paper we introduce a predictive text input technique that is based on association rules and item frequencies. We also describe an interface design for integrating the predictive text input with a web-based mobile shopping assistant. In a user study we compared two interfaces, one with text input support and one without. Our results indicate that, even though shopping list entries are typically short, our technique makes text input significantly faster, decreases typing error rates and increases overall user satisfaction.	association rule learning;mobile payment;predictive text;usability testing;user interface design;web application	Petteri Nurmi;Andreas Forsblom;Patrik Floréen;Peter Peltonen;Petri Saarikko	2009		10.1145/1502650.1502714	user interface design;association rule learning;usability;human–computer interaction;word error rate;computer science;interface design;adaptive behavior;multimedia;user interface;world wide web	HCI	-47.43460199384173	-45.32784190893338	34134
4866649be732436bfdd1911a26309711c36697e0	usability evaluation of information technology in disaster and emergency management		Apart from technical reliability, usability is one of the major criteria for safe and efficient usage of interactive information technology in disaster and emergency management. However, in this setting, usability evaluation is difficult due to the heterogeneity and unpredictability of operation conditions, as well as the difficult, usually mobile, context. However, there are ways to conduct usability evaluations in disaster and emergency settings. Thus, in this paper, advantages and disadvantages of empirical and analytical usability evaluation methods for interactive systems in disaster and emergency management are discussed. The importance of formative evaluation measures within an iterative human-centered design process is emphasized. It is illustrated by two case studies dealing with paramedics’ and emergency physicians’ usage of mobile and wearable devices in mass casualty incidents.	usability	Tilo Mentler;Henrik Berndt;Daniel Wessel;Michael Herczeg	2016		10.1007/978-3-319-68486-4_5	formative assessment;engineering management;mass-casualty incident;emergency management;wearable technology;usability engineering;information technology;usability;construction engineering;user-centered design;computer science	HCI	-62.17541703705837	-51.3235399129083	34158
537ed79ab31abb0bf8142978423d812420f9289e	physical rendering with a digital airbrush	article	Airbrush painting is an expressive art form that allows for unrepeatable spray patterns and unique ink staining. Artists utilize these properties while painting, expressing subjective style and artistic intentions. We present an augmented airbrush device that acts both as a physical spraying device and an intelligent digital guiding tool, that maintains both manual and computerized control. We demonstrate our custom designed hardware and numerous algorithms that control it through hands-on usage examples of a human-computer collaborative of a physical painting effort.	algorithm;hands-on computing	Roy Shilkrot;Pattie Maes;Amit Zoran	2014		10.1145/2619195.2656328	iterative reconstruction;computer vision;simulation;computer science;artificial intelligence;operating system;mobile technology;image sensor;multimedia;interactivity;computer graphics (images)	HCI	-42.44617664286445	-37.149641737995594	34165
2604457fe5fe5a25768230dcb01a9ca055e251d0	voice coil actuators for percussion robotics		Percussion robots have successfully used a variety of actuator technologies to activate a wide array of striking mechanisms. Popular types of actuators include solenoids and DC motors. However, the use of industrial strength voice coil actuators provides a compelling alternative given a desirable set of heterogeneous features and requirements that span traditional devices. Their characteristics such as high acceleration and accurate positioning enable the exploration of rendering highly accurate and expressive percussion performances.	performance;requirement;robot;robotics;voice coil	Robert Van Rooyen;W. Andrew Schloss;George Tzanetakis	2017			human–computer interaction;voice coil;actuator;computer science;percussion;artificial intelligence;robotics	Robotics	-45.126816855001294	-35.67572414900145	34173
067b8db4eee3efee54e133126a630e6284d14c95	monitor-based tracking system for wide augmented reality environments	tracking system;categories and subject descriptors according to acm ccs multimedia information systems h 5 1 artificial augmented and virtual realities;pattern recognition i 5 2 design methodology pattern analysis;pattern recognition i 5 5 implementation interactive systems;augmented reality		augmented reality;tracking system	Guido Maria Re;Giandomenico Caruso;Paolo Belluco;Monica Bordegoni	2010		10.2312/LocalChapterEvents/ItalChap/ItalianChapConf2010/153-158	computer vision;augmented reality;simulation;computer science;multimedia	HCI	-46.62622067806024	-32.14970342099803	34175
f2d6b9712356e5662a3fb9bf27f52e24cb53619c	play as a modelling system - a semiotic analysis of the overreaching prestige of games		This paper aims at using the tools of semiotic analysis and semiotics of play in order to re-organise and specify the terminology surrounding the concept of gamification. In the first part, we propose an overview on the different theories and ideologies surrounding the topic, and we will underline the strengths and contradictions of the different approaches. In the second part we will propose a semiotic approach to the topic, drawing from Lotman's semiotics of culture, in order to redefine more precisely the different actions and metaphors related to the implementation of game and play mechanics in ordinary life contexts.	gamification;semiotics;theory	Min-Zhi Jiang	2017				AI	-59.48264898218297	-31.853639183046017	34242
b9e8a5733c512de0ca1988ea58f9b9246aaa1d45	high art: visualising damage on a heritage ceiling				Lindsay W. MacDonald;Ian Gibb;Stuart Robson;Constantina Vlachou-Mogire	2012			ceiling (aeronautics);archaeology;visual arts;art	NLP	-51.80163450557723	-27.939193836309332	34243
50cd9e0e7055adc0be7a045169de48d8b7c52b30	a realistic approach towards users' simulation		Simulation has been proposed and utilised widely in the field of the evaluation of information retrieval (IR) and interactive IR (IIR) systems. It can significantly reduce costs, make experiments easier to reproduce and save time to users and researchers. The question of how realistic these simulations are remains, to a great extent, unexplored. This is due to the fact that searching for information is a self-directed activity, and varies among users in terms of their information seeking behaviours (ISB) and their relevance judgments. Such variations are affected by a number of attributes describing users, tasks, and systems and their interactions. By identifying these attributes researchers could design more effective user models and realistic simulations. This paper presents a user-centric evaluation methodology based on user profiles and ISBs.	experiment;infinite impulse response;information retrieval;information seeking;interaction;relevance;simulation;user profile	Maram Barifah;Monica Landoni;Fabio Crestani	2017			computer science	Web+IR	-36.470489635367684	-51.904543551277754	34324
5ee7d73079c589556cea1c60b6df36a728a11b4a	the design of banking websites: lessons from iterative design	design process;usability testing;software prototyping;user testing banking website design iterative design design experiences design rationale design process usability techniques interactive graphic elements navigation embedded multimedia usability training salespeople customer feedback;multimedia systems internet software prototyping bank data processing user interfaces human factors interactive systems;website design;bank data processing;multimedia systems;human factors;internet;user testing;iterative design;design rationale;world wide web;interactive graphics;interactive systems;user interfaces;web page design banking process design usability prototypes guidelines graphics navigation feedback testing	Iterative design is usually considered in the context of prototyping, evaluating, and improving a single product. We had the interesting opportunity to work on three distinct products which had strong similarities, enabling us to carry many lessons over from one project to the next. This paper reveals our design experiences and discusses various design tradeoffs involved in the sequential development of the three banking websites. We discuss our design rationale, the elaboration of our design process, and the results of our evaluations. Our approach with these websites and others has been to try using new usability techniques with each project, and to incorporate successful techniques in every subsequent project. With this general approach, we improve the design of our websites, develop additional website design guidelines, and improve our design process. Through our experience with banking websites, we've developed guidelines for interactive graphic elements, navigation, and embedded multimedia. Our design process has also benefited primarily through: 1. providing usability training for our salespeople, 2. incorporating more effective mechanisms for customer feedback, and 3. conducting user testing.	audio signal processing;conformance testing;design rationale;documentation;embedded system;graphics;iterative design;iterative method;software prototyping;usability testing;user research;web design	Tom Brinck;Darren Gergle	1998		10.1109/APCHI.1998.704475	iterative design;the internet;design process;human–computer interaction;computer science;human factors and ergonomics;multimedia;user interface;world wide web;design rationale	HCI	-52.842504205643124	-40.98807814239866	34325
80345fbb6bb6bcc5ab1a7adcc7979a0262b8a923	soft biometrics for a socially assistive robotic platform		This work is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 License. Paladyn, J. Behav. Robot. 2015; 6:71–84 Research Article Open Access Pierluigi Carcagnì*, Dario Cazzato, Marco Del Coco, Pier Luigi Mazzeo, Marco Leo, and Cosimo Distante Soft Biometrics for a Socially Assistive Robotic Platform Abstract: In this work, a real-time system able to automatically recognize soft-biometric traits is introduced andused to improve the capability of a humanoid robot to interact with humans. In particular the proposed system is able to estimate gender and age of humans in images acquired from the embedded camera of the robot. This knowledge allows the robot to properly react with customized behaviors related to the gender/age of the interacting individuals. The system is able to handle multiple persons in the same acquired image, recognizing the age and gender of each person in the robot’s field of view. These features make the robot particularly suitable to be used in socially assistive applications.	algorithm;assistive technology;coco/r;download;embedded system;face detection;hoc (programming language);humanoid robot;human–robot interaction;illumination (image);leo (computer);nop;nao (robot);real-time computing;real-time locating system;soft biometrics;statistical model	Pierluigi Carcagnì;Dario Cazzato;Marco Del Coco;Pier Luigi Mazzeo;Marco Leo;Cosimo Distante	2015	Paladyn		simulation;computer science;biometrics;soft biometrics;human–robot interaction	Robotics	-38.198103753759796	-41.19744697129956	34394
7ea519ae30a164d54a709a4ac218faf5b03586fb	an agent-based personalized search on a multi-search engine based on internet search service	feedback mechanism;search engine;red www;agent based;information filtering;personalized search;interfase;recherche personalisee;motor investigacion;internet;personal information systems;interface;systeme information personnel;world wide web;reseau www;moteur recherche	The Internet Search Service (ISS) was proposed to support an uniform interface for searching on the World Wide Web. Based on this service, a multi-search engine named Octopus had been built. In order to provide more services, such as personal functionality, we provide personalized search for users. In this paper, the policies of personalized search are described. In addition, in order to keep the advantages of the ISS, a personal information-filtering agent is added into the Octopus instead of modifying the architecture or interface of ISS. The feedback mechanism is in coopcratiori with the filfering nrechanism to achieve the functionally of personalized search in a search engine.	personalized search;web search engine	Min-Huang Ho;Yue-Shang Chang;Shyan-Ming Yuan;Winston Lo	2000		10.1007/3-540-44491-2_59	the internet;simulation;computer science;interface;feedback;database;multimedia;search analytics;world wide web;search engine	Web+IR	-38.10696690329314	-25.001708842899305	34410
2bf32ef52092aae661c307defeae44c32476f054	interactive and dynamic graphics for data analysis - with r and ggobi		Make more knowledge even in less time every day. You may not always spend your time and money to go abroad and get the experience and knowledge by yourself. Reading is a good alternative to do in getting this desirable knowledge and experience. You may gain many things from experiencing directly, but of course it will spend much money. So here, by reading interactive and dynamic graphics for data analysis with r and ggobi 1st edition, you can take more advantages with limited budget.	graphics;money	Dianne Cook;Deborah F. Swayne	2007		10.1007/978-0-387-71762-3	computer graphics metafile;real-time computer graphics;graphics software;computer graphics;3d computer graphics	AI	-55.55108778833556	-27.62817060809574	34419
1f5102ad827f99a2631beadd79d23bf76464ea89	gaze point estimation on curved display by using session level calibration for flat screen displays		The advantages of curved screen displays find their place in populations everyday life, therefore it is important to adapt the existing eye tracking systems for estimating the gaze point on a curved screen. In this paper we present a curved screen based gaze point estimation model. This model provides methods for estimating the gaze point position by eye rotation angles, and also transforms flat screen based relative coordinates into curved screen based relative coordinates. The model was validated by using a flat screen based eye tracking system for a gaze point estimation on a curved screen. The eye rotation angle, distance to the screen and gaze point location’s influence were modelled in this paper as well. The results of these experiments prove the up to 4% increase of gaze point estimation accuracy by using the proposed model, by comparing the flat screen mode with the curved screen. However the results found are significant only if the distance to the screen is less than 0.5 of curved screen radius.	curved screen;experiment;eye tracking;flat panel display;point location;population;tracking system;usability;vesa bios extensions	Simona Ramanauskaite;Antanas Cenys;Egle Radvile;Nerijus Ramanauskas	2017	Multimedia Tools and Applications	10.1007/s11042-017-4616-y	gaze;computer vision;computer science;calibration;point location;artificial intelligence;computer graphics (images);point estimation;eye tracking	HCI	-44.9683920671548	-46.77113990608654	34437
6dd8d11d619b573d967263dbfe98d16cec64e9d8	immersion music: a progress report	gestural interaction;computer music;interactive computer music systems;conductor s jacket;digital baton	This paper describes the artistic projects undertaken at Immersion Music, Inc. (www.immersionmusic.org) during its three-year existence. We detail work in interactive performance systems, computer-based training systems, and concert production.	immersion (virtual reality)	Teresa Marrin Nakra	2003			visual arts;simulation;human–computer interaction;computer science;multimedia;computer music;programming language	AI	-47.876625523391304	-33.22765000278844	34461
a5d6ff9d7f2e9272d0a2f1a61e839eaa24e828a5	virtual art galleries: a new kind of cultural objects?	games of skill virtual reality exhibitions image resolution image texture;painting;art;mathematics;game engine technology;image resolution;high resolution images;prototypes;3d installations;virtual painting galleries;game engine;virtual reality;image texture;subspace constraints;texture files;game engines;engines;high quality textures;games of skill;displays;exhibitions;subspace constraints cultural differences art painting virtual reality engines displays atmosphere prototypes mathematics;next generation;high resolution imager;virtual art galleries;mapping effects;atmosphere;mapping effects virtual art galleries cultural objects game engine technology spatial layout high quality textures virtual painting galleries 3d installations high resolution images game engines texture files;spatial layout;cultural objects;cultural differences	We describe the development of a virtual art gallery based on a game engine's technology. In the specific case of painting, the development of a virtual gallery comprises both the design of the spatial layout and the editing of high-quality textures from original images. Besides recreating the original gallery atmosphere, virtual painting galleries enable new aesthetic experiences, such as the construction of 3D installations. This ability to interact with high-resolution images should be furthered in the near future, as the next generation of game engines will support larger texture files and sophisticated mapping effects.		Marc Cavazza;Steven J. Mead	2001		10.1109/ICIP.2001.959085	image texture;computer vision;image resolution;painting;computer science;atmosphere;virtual reality;multimedia;computer graphics (images)	Vision	-39.92223381396701	-33.86137053507951	34474
765807e2d8397ba0e6309bcaee8f7d4941367c59	caba2l a bliss predictive composition assistant for aac communication software	symbolic prediction.;hidden markov model;accessibility to disabled users;intelligent user interface;aac languages;human computer interaction;input device;auto regressive;behavior modeling	In order to support the residual communication capabilities of verbal impaired peoples softwares allowing Augmentative and Alternative Communication (AAC) have been developed. AAC communication software aids provide verbal disables with an electronic table of AAC languages (i.e. Bliss, PCS, PIC, etc.) symbols in order to compose messages, exchange them via email, or vocally synthetize them, and so on. A current open issue, in thins kind of software, regards human-computer interaction in verbal impaired people suffering motor disorders. They can adopt only ad-hoc input device, such as buttons or switches, which require an intelligent automatic scansion of the AAC symbols table in order to compose messages. In such perspective we have developedCABAL an innovative composition assistant exploiting an user linguistic behavior model adopting a semantic/probabilistic approach for predictive Bliss symbols scansion. CABAL is based on an original discrete implementation of auto-regressive hidden Markov model called DAR-HMM and it is able to predict a list of symbols as the most probable ones according to both the previous selected symbol and the semantic categories associated to the symbols. We have implemented the composition assistant as a component of BLISS2003 an AAC communication software centered on Bliss language and experimentally validated it with both synthetic and real data.	advanced audio coding;bliss;behavior model;email;experiment;hidden markov model;hoc (programming language);human–computer interaction;input device;markov chain;network switch;synthetic intelligence	Nicola Gatti;Matteo Matteucci	2004			behavioral modeling;simulation;speech recognition;computer science;operating system;autoregressive model;hidden markov model;input device	AI	-43.70676146660551	-44.743602722842354	34481
d84afa013516e5c22421afae96fb7b4dc1f4c4a0	automatic construction of 3d animatable facial avatars	facial animation	Rigging for facial animation is an important but time-consuming task, which generally requires experienced artists with knowledge of facial anatomy. In this paper, we investigate whether it is possible to produce a good animatable avatar automatically, given only a 3D static triangle mesh of the head. An automatic mechanism is devised for constructing multilayer animatable facial avatars for unseen faces. We evaluate our technique with a variety of models, and give a quantitative analysis of the constructed results. We also designed and conducted a user study for evaluating the perceived quality of the generated expressive animations. The results demonstrate that ourmethod is an appropriate tool for naı̈ve users to customize their personal 3D avatars. Copyright # 2010 John Wiley & Sons, Ltd.	avatar (computing);computer animation;expressive power (computer science);john d. wiley;multilayer perceptron;triangle mesh;usability testing	Yujian Gao;Qinping Zhao;Aimin Hao;T. Metin Sezgin;Neil A. Dodgson	2010	Journal of Visualization and Computer Animation	10.1002/cav.340	computer vision;simulation;computer facial animation;computer science;multimedia;computer graphics (images)	Graphics	-39.11167389671626	-35.649108078171466	34489
64e84e65b34673c4ac795c8787871c0f850ca955	toward an organology of virtual instruments in computer music		In this paper, we will first take assess of 25 years of interactive real-time music, and introduce the problem of preservation of this music for the future generations, that is to say its ability to be re-performed, and not only to preserve the recordings. We present the state of the art in the field of active preservation of real-time works. We then give an overview of the solutions developed by IRCAM and its partners Grame, Armines ParisTech and CIEREC, in the framework of the ASTREE project, and explain the possibilities envisioned in a case study that is En Echo by Philippe Manoury.	real-time cmix;real-time clock	Jérôme Barthélemy;Yann Orlarey;Alain Bonardi;Serge Lemouton;Raffaele Ciavarella;Karim Barkati	2010			multimedia;organology;computer music;computer science	AI	-47.09150632024636	-33.87280134216593	34528
f07d1256e3c6d5a3f83f7b41e1db8e3679753385	specifying mpeg-4 body behaviors	virtual reality computer animation;high level functionality;application software;computer graphics;low level animation parameters;real time;virtual reality;rich nonverbal behaviors;real time algorithmic animations;body animation script;complex bodily behaviors;mpeg 4 body behaviors;mpeg 4 standard;embodied agents;avatar markup language;animation;markup languages;aml;algorithm animation;intelligent agent;avatars;body animation;intelligent software agents;humans;virtual environment;computer animation;script format;markup language;3d graphics;aml mpeg 4 body behaviors mpeg 4 standard low level animation parameters body animation high level functionality avatars embodied agents script format complex bodily behaviors body animation script real time algorithmic animations intelligent software agents rich nonverbal behaviors avatar markup language;verbal behavior;intelligent software agent;mpeg 4 standard animation avatars intelligent agent markup languages virtual environment application software computer graphics humans laboratories	The MPEG-4 standard specifies a set of low-level animation parameters for body animation, but does not provide any high-level functionality for the control of avatars or embodied agents. In this paper, we discuss the required features for a script format allowing designers to easily specify complex bodily behaviors, and describe a system and its associated syntax Body Animation Script (BAS) which fulfills these requirements in a flexible way. The described architecture allows the organization and parametrization of predefined MPEG-4 animations and their integration with real-time algorithmic animations, such as pointing at a specific location or walking. This system has been implemented at EPFL in the framework of the EU SoNG project, in order to allow intelligent software agents to control their 3D graphical representation and end-users to trigger rich nonverbal behaviors from an online interface. It has been integrated into AML the Avatar Markup Language.	3d computer graphics;arc macro language;avatar (computing);broadcast auxiliary service;codec;embodied agent;form-based authentication;high- and low-level;intelligent agent;markup language;multimodal interaction;netware file system;parsing;real-time locating system;requirement;software agent;virtual reality;xml	Anthony Guye-Vuillème;Daniel Thalmann	2002		10.1109/CA.2002.1017520	simulation;computer science;multimedia;computer graphics (images)	HCI	-42.31499166430353	-33.4512819921268	34548
9df9866d0a7f7c0969d12a1d441d3c8ea062b8f2	interactive music studio: the soloist		In this paper, we present and demonstrate Samsung’s new concept music creation engine and music composer application for mobile devices such as touch phones or MP3 players, ‘Interactive Music Studio : the soloist’.	creation engine;mp3;mobile device	Hyun-Soo Kim;Je-Han Yoon;Moon-Sik Jung	2010			multimedia;human–computer interaction;music visualization;computer science;mobile device;studio	HCI	-48.03817666489564	-37.756588136568055	34565
7cb080a7feec49a758f24ab868f4395e2f72653f	emergence in interactive artistic visualization	creativity;emergence;disability;interactive art;interaction design;artistic visualization;experience design	This research draws on theories of emergence to inform the creation of an artistic and direct visualization. This is an interactive artwork and drawing tool for creative participant experiences. Emergence is characteristically creative and many different models of emergence exist. It is therefore possible to effect creativity through the application of emergence mechanisms from these different disciplines. A review of theories of emergence and examples of visualization in the arts, is provided. An art project led by the author is then discussed in this context. This project, Iterative Intersections, is a collaboration with community artists from Cerebral Palsy League. It has resulted in a number of creative outcomes including the interactive art application, Of me with me. Analytical discussion of this work shows how its construction draws on aspects of experience design, fractal and emergent theory to effect perceptual emergence and creative experience as well as to facilitate self-efficacy.	emergence	Jen Seevinck	2015	International Journal of Software Engineering and Knowledge Engineering	10.1142/S0218194015400070	simulation;experience design;computer science;engineering;artificial intelligence;interaction design;multimedia;creativity;emergence	SE	-58.491313391967694	-34.127927435302155	34583
dce88e33f2de61430dc8839f921635a8d9287dd4	introducing intelligent environments, agents and autonomy to users	user centred design computer animation multi agent systems surveying ubiquitous computing;adjustable autonomy intelligent environments demonstrating video intelligent agents;artificial intelligence pervasive computing intelligent agents programming smart homes conferences educational institutions;adjustable autonomy;multi agent systems;intelligent agents;intelligent environments;surveying;ubiquitous computing;online survey intelligent agents autonomy user centric systems pervasive computing intelligent environments animated video;user centred design;computer animation;demonstrating video	There is a long-standing debate over the role that intelligent agents should play and how much autonomy they should be given in user-centric systems such as intelligent environments and other pervasive computing technologies. A recent online survey has been conducted to assess people's opinions of the use of autonomy in intelligent environments. In order to conduct this survey, an animated video was created to explain the necessary concepts of intelligent environments to the survey participants. This short paper gives an overview of the online survey and discusses the explanation video.	autonomy;intelligent environment;software agent;ubiquitous computing	Matthew Ball;Victor Callaghan	2011	2011 Seventh International Conference on Intelligent Environments	10.1109/IE.2011.67	agent architecture;simulation;intelligent decision support system;human–computer interaction;engineering;multimedia;intelligent agent	Robotics	-53.96969045631307	-37.703571724245634	34586
001a8e0ec44e9ec5cc8e18387cadcbb1530d42eb	feathers for mystical creatures: creating pegasus for clash of the titans	real time;gpu;antialiasing	One of the major challenges for Clash of the Titans was to develop a highly sophisticated feather system used on several flying characters in over a hundred shots. The system had to reliably provide photo-realistic results over a large range of detail levels from characters in the background to those passing just in front of the camera. The feather system was tightly integrated with MPC's existing pipeline and allowed for a large amount of artistic freedom and control efficiently, while requiring almost no technical background to use.	pegasus;pipeline (computing)	James Leaning;Damien Fagnou	2010		10.1145/1837026.1837094	simulation;computer science;computer graphics (images)	Graphics	-38.957041898408825	-34.081582125508206	34587
be502bcd0ad1ebcb5bdc0352e191b125e92ca293	interactive multi-scale visualizations of tonal evolution in musa.rt opus 2	music visualization;music performance;data stream;real time;computational music cognition;multiple scales;tonal analysis;interactive system;computer music;geometric model;geometric models;interactive art;tonality models;concurrent process;harmonic analysis	The purpose of this article is to describe MuSA.RT Opus 2, an interactive system for tonal visualization of music at multiple scales, and to present examples of the types of musical features and attributes that can be abstracted and visualized by the system. MuSA.RT aims to create an environment by which musical performances can be mapped in real-time to a concrete and visual metaphor for tonal space, wherein we can see the establishment and evolution of the tonal context. In this environment, expert musicians will be able to see the tonal structures of what they play, initiated listeners will be able to visually follow the structures that they hear, and novices can learn to hear the structures that they see. MuSA.RT is both an interactive art installation that can convert musical performances to mathematically elegant graphics and a scientific tool for visualizing the inner workings of tonal induction and tracking algorithms. In this article we describe the mapping strategies for transforming a MIDI stream into tonal structures in 3D space, and our solution for overcoming the challenge of real-time concurrent processing of data streams; we will also give examples and present case studies of visual mappings of music by Pachelbel, Bach, and Barber.	algorithm;computer multitasking;graphics;interactive art;interactivity;midi;performance;real-time clock;real-time locating system;libopus	Elaine Chew;Alexandre R. J. François	2005	Computers in Entertainment	10.1145/1095534.1095545	speech recognition;computer science;artificial intelligence;geometric modeling;machine learning;harmonic analysis;music visualization;multimedia;computer music;programming language	HCI	-46.275136717457364	-34.49869732908834	34653
4e57e551d05e4839b3f25832f601bce6e8f55f48	the turning, stretching and boxing technique: a step in the right direction	boxing technique;graphical process;effect presence;intelligent virtual agent;right direction	This paper describes a combination of three graphical processes for rendering a 3D avatar for an intelligent virtual agent (IVA) on a 2D display that increases the perceived presence of the avatar in the viewer's environment. The results of an experiment that shows that these processes positively effect presence are presented and analysed.	boxing	M C M Dunne;Brian Mac Namee;John D. Kelleher	2012		10.1007/978-3-642-33197-8_37	simulation;multimedia;computer graphics (images)	Crypto	-41.99564295879393	-38.043279765214784	34658
51e0959cc62e23e54c258758b9df27754df16fcb	motivated selective attention during political ad processing	motivated attention;dynamic processing;time series;cardiac somatic coupling;emotional political ads;psychophysiology	This study examines the dynamic, real-time interplay between the emotional content of political television ads and individuals’ political attitudes during ad processing based upon the Dynamic Motivational Activation (DMA) theoretical framework. Time-series cross-sectional models were developed to test the effects of three motivational inputs of emotional ads (arousing content, positivity, and negativity) and viewers’ evaluation of the featured candidates on four psychophysiological responses (heart rate, skin conductance level, corrugator electromyography, and zygomatic electromyography). As predicted by the DMA, physiological responses during ad viewing were affected by their own first- and second-order dynamic system feedback effects. These results not only support the predicted dynamic nature of the physiological system but also help disentangle message effects from the moderating and accumulating effects of the physiological system itself. Also as predicted, message motivational inputs interacted with...		Zheng Wang;Alyssa C. Morey;Jatin Srivastava	2014	Communication Research	10.1177/0093650212441793	psychology;psychophysiology;developmental psychology;time series;advertising;communication;social psychology;statistics	Robotics	-55.61797299630204	-50.98236846943781	34724
96a78fd848f3a78aa1337a6679aee3b0697f4b9a	physicality and interaction	hci;physicality;design;article	We live in an increasingly digital world yet our bodies and minds are naturally designed to interact with the physical. The products of the 21st century are and will be a synthesis of digital and physical elements embedded in new physical and social environments. As we design more hybrid physical/digital products, the distinctions for the user become blurred. It is therefore increasingly important that we understand what we gain, lose or confuse by the added digitality. Digitally augmented physical artefacts can be tailored and adapted to operate within a wide range of ecological settings. However, they also become more complex and require a fairly intensive design process to make them not simply practical and functional but also engaging. As a result, the need becomes even more pressing to comprehend the underlying computational intricacies, the physical form, properties and behaviour, the physical and social contexts, and the issues of aesthetics and creativity. This special issue of Interacting with Computers arose out of series of workshops on the issue of Physicality (see http:// www.physicality.org/). These attracted a wide range of participants: artists and architects, designers and dancers, programmers and philosophers; all in different ways seeking to understand and exploit the physical nature of the world, things within the world and the human body itself. They also reflected intersections with other topical areas including ubiquitous computing and tangible interaction. The importance of issues surrounding physicality and materiality is clear. At the same time that the call for this special issue was issued there were two other journal special issue calls in closely related areas. However, the level of interest is such that despite this ‘competition’, the call for this issue was in fact heavily oversubscribed. The editors’ own interest in the issue of physicality cuts across a number of areas including: understanding the way physical artefacts act as prompts or triggers for action; using the placement of objects in the environment as a resource for the analysis of human activity; studying digital and electrical appliances in order to understand the role of physical form and behaviour in enhancing usability and user experience; and exploring the role of physical tools and models during the design process and how this affects the designers themselves and users testing early prototypes. The papers in this special issue also demonstrate the wide range of domains where issues of physicality are important. Antle, Corness and Droumeva in ‘‘What the Body Knows: Exploring the Benefits of Embodied Metaphor in Hybrid Physical Digital Environments” look at the use of physical metaphors to drive the design of tools for creating music. Khoo, Merritt and Cheok in ‘‘Designing Physical and Social Intergenerational Family Entertainment” focus on physical	command-line interface;database trigger;digital data;embedded system;materiality (digital text);mind;overselling;programmer;tangible user interface;ubiquitous computing;usability;user experience	Devina Ramduny-Ellis;Alan J. Dix;Steve Gill;Joanna Hare	2009	Interacting with Computers	10.1016/j.intcom.2008.10.003	design;human–computer interaction;computer science	HCI	-58.74981059645149	-32.18713949331982	34814
a9ec573e7bafc74ed020a02eeb15fb094de39039	the persuasiveness of ambient intelligence	cognitive science;ambient intelligence;large scale integration;point of view;smart environment;embedded computing	Ambient intelligence (AmI) is a novel concept for embedded computing that builds on the large-scale integration of electronic devices into peoples’ surroundings and the ubiquitous availability of digital information to the users of such environments. The concept however is not only concerned with the integration of computing in the background but, as a direct result of the disappearing computer and the corresponding interaction technologies, it calls for novel means of control that support the natural and intelligent use of such smart environments, emphasizing predominantly social aspects. As the familiar box-like devices are replaced by hidden functions embedded in the surroundings, the classical meaning and implication of security and trust needs to be revisited in the context of ambient intelligence. In this chapter, we briefly revisit the foundations of the AmI vision by addressing the role of AmIware, which refers to the basic and enabling AmI technologies, and by presenting some basic definitions of ambient intelligence. Next we discuss the meaning and role of persuasion on the basis of models and theories for motivation originating from cognitive science. Notions such as compliance and ambient journaling are used to develop an understanding of the concept of ambient persuasion. We also address the ethics of ambient intelligence from the point of view of a number of critical factors such as trust and faith, crossing boundaries, and changing realities. The chapter concludes with a summary of findings and some final remarks.	ambient intelligence;cognitive science;digital data;embedded system;experience;integrated circuit;programming paradigm;smart environment;theory	Emile H. L. Aarts;Panos Markopoulos;Boris E. R. de Ruyter	2007		10.1007/978-3-540-69861-6_24	ambient intelligence;human–computer interaction;computer science;multimedia;world wide web	HCI	-60.24378227831034	-33.88433502705218	34823
d689fbb016658f634d455cc1bc6149446431a664	towards the autonomous animation of multiple human figures	animation humans kinematics expert systems legged locomotion communication channels computer graphics logic programming computer architecture production;constraint logic programs;blackboard architecture;mixed initiative system;expert systems;spatial reasoning;legged locomotion;computer graphics;mixed initiative;declarative knowledge base;animation system;kinematics;explicit knowledge;high level tools;computer architecture;agents;agents autonomous animation multiple human figures high level tools explicit knowledge declarative knowledge base animation system expert system blackboard architecture reasoning mixed initiative system constraint logic programming knowledge based animation;logic programming;expert systems computer animation blackboard architecture spatial reasoning;animation;multiple human figures;knowledge based animation;autonomous animation;production;procedural knowledge;humans;constraint logic programming;reasoning;computer animation;communication channels;human animation;knowledge base;expert system	"""High level tools to support the animation of multiple human figures make use of knowledge in a number of ways. Explicit knowledge, in the form of keyframes is supplied directly by the animator and procedural knowledge for repetitive movements like walking or grasping is built into the algorithms. However, the interaction of multiple figures in a complex environment requires a declarative knowledge base of rules and constraints. The most obvious way to add declarative knowledge to an animation system is to choose a well developed expert system and to set up communication channels between the two systems, but this """"two monoliths"""" approach can be very inefficient. To avoid the problems associated with distinct expert and animation system, we are implementing a blackboard architecture which allows integration of reasoning with the graphics algorithms. The result is a mixed initiative system where autonomously produced motion paths for multiple human figures are edited and constrained interactively by the animator. A partial implementation is being evaluated."""	algorithm;autonomous robot;blackboard system;expert system;graphics;human–computer interaction;interactivity;key frame;knowledge base	Thomas W. Calvert;Russell Ovans;Sang Mah	1994		10.1109/CA.1994.324005	computer vision;simulation;computer science;artificial intelligence	Graphics	-38.60118357332866	-33.25260435344508	34824
c717b704216aabfb55b3edaf5153bbd5538f7f07	keynote: language adaptation		As we all know, more and more of life is now manifested online, and many of the digital traces that are left by human activity are increasingly recorded in natural-language format. This availability offers us the opportunity to glean user-modeling information from individual users’ linguistic behaviors. This talk will discuss the particular phenomenon of individual language adaptation, both in the short term and in the longer term. We’ll look at connections between how people adapt their language to particular conversational partners or groups, on the one hand, and on the other hand, those people’s relative power relationships, quality of relationship with the conversational partner, and propensity to remain a part of the group.	digital footprint;natural language;tracing (software);user modeling	Lillian Lee	2014			psychology;artificial intelligence;communication;social psychology	HCI	-58.85659734590122	-38.500384810443734	34837
a1937a18efe248f373e414e84d1927b7b76b0a2b	designing effective pictures: is photographic realism the only answer? (panel session)	orthopedic surgery;computer aided design;hand therapy;computer graphics;hand surgery;ct and mr imaging			James F. Blinn;Donald P. Greenberg;Margaret A. Hagen	1988		10.1145/54852.378559	orthopedic surgery;computer aided design;multimedia;computer graphics;hand surgery;computer graphics (images)	Theory	-47.62043128357479	-30.935799358992995	34838
45832e819d092865c3dc3cb75b17d6970f8af24d	the effects of interactive latency on exploratory visual analysis	visual analytics data visualization visualization interactive services image color analysis;interactive analysis tools interactive latency effects exploratory visual analysis user behavior knowledge discovery think aloud protocols verbal data analysis;user performance;interaction;interactive visualization;verbal analysis;exploratory analysis;interactive services;visualization;image color analysis;data visualization;latency;scalability;visual analytics;interactive systems data analysis data mining	To support effective exploration, it is often stated that interactive visualizations should provide rapid response times. However, the effects of interactive latency on the process and outcomes of exploratory visual analysis have not been systematically studied. We present an experiment measuring user behavior and knowledge discovery with interactive visualizations under varying latency conditions. We observe that an additional delay of 500ms incurs significant costs, decreasing user activity and data set coverage. Analyzing verbal data from think-aloud protocols, we find that increased latency reduces the rate at which users make observations, draw generalizations and generate hypotheses. Moreover, we note interaction effects in which initial exposure to higher latencies leads to subsequently reduced performance in a low-latency setting. Overall, increased latency causes users to shift exploration strategy, in turn affecting performance. We discuss how these results can inform the design of interactive analysis tools.	exploratory testing;generalization (psychology);interactive visualization;interrupt latency;protocols documentation;think aloud protocol	Zhicheng Liu;Jeffrey Heer	2014	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2014.2346452	latency;visual analytics;interaction;scalability;visualization;interactive visualization;human–computer interaction;interactive visual analysis;computer science;data mining;multimedia;world wide web;data visualization;statistics	Visualization	-42.00817198680804	-48.72337861091669	34845
c742d7a92a8486926bc26bd50c11dd7add11a8fa	evaluation of domain-specific rule generation framework based on usability criteria			common criteria;usability	Neel Mani;Shastri L. Nimmagadda;Markus Helfert;Torsten Reiners	2018			knowledge management;user experience design;usability;computer science	DB	-62.65096511498942	-46.29309800150203	34882
2f3f65e6f2315d73d5790f197c90103227da5778	games for games: manipulating contexts in human computation games	human based computation;data gathering;data engineering;computational method;games;hbc;human computation;cost effectiveness;computer games;context;computer game;hc	The present work and demonstration system aims at finding an efficient and cost-effective human computation method to expand the linguistic capabilities of interactive games that need it to respond appropriately to the language based input of their users. As a showcase scenario for the experiments conducted, we took interactive fiction applications and examined how the human computation game design and scoring approaches affects the quality of the data gathered. The ensuing analysis of the data confirms our initial hypothesis that game approaches can provide both the qualitative and quantitative data needed for the corresponding interactive games.	experiment;human-based computation;norm (social)	Aneta Takhtamysheva;Robert Porzel;Markus Krause	2009		10.1145/1600150.1600164	combinatorial game theory;games;simulation;cost-effectiveness analysis;information engineering;turns, rounds and time-keeping systems in games;computer science;theoretical computer science;game mechanics;machine learning;data mining;multimedia;statistics;data collection	HCI	-57.447380882718335	-47.423067619307105	34901
60a57db6ef046956e5ffcb495e1fe002f3bb07f2	modelling interaction with economic models of search	search strategy;search behaviour;search behavior;retrieval strategies	Understanding how people interact when searching is central to the study of Interactive Information Retrieval (IIR). Most of the prior work has either been conceptual, observational or empirical. While this has led to numerous insights and findings regarding the interaction between users and systems, the theory has lagged behind. In this paper, we extend the recently proposed search economic theory to make the model more realistic. We then derive eight interaction based hypotheses regarding search behaviour. To validate the model, we explore whether the search behaviour of thirty-six participants from a lab based study is consistent with the theory. Our analysis shows that observed search behaviours are in line with predicted search behaviours and that it is possible to provide credible explanations for such behaviours. This work describes a concise and compact representation of search behaviour providing a strong theoretical basis for future IIR research.	infinite impulse response;information retrieval	Leif Azzopardi	2014		10.1145/2600428.2609574	artificial intelligence;data mining	HCI	-36.13340884613065	-52.00058905216345	34956
dd0143291a649b5a79f7286f141ee7f226b68b9d	a multi-user 3-d virtual environment with interactive collaboration and shared whiteboard technologies	shared whiteboard;information retrieval;speech processing;transparent communication;multi user;multimodal interaction;facial expression;virtual environment;avatar;face to face;animation techniques;eye gaze;3 d virtual environment;interactive collaboration	A multi-user 3-D virtual environment allows remote participants to have a transparent communication as if they are communicating face-to-face. The sense of presence in such an environment can be established by representing each participant with a vivid human-like character called an avatar. We review several immersive technologies, including directional sound, eye gaze, hand gestures, lip synchronization and facial expressions, that facilitates multimodal interaction among participants in the virtual environment using speech processing and animation techniques. Interactive collaboration can be further encouraged with the ability to share and manipulate 3-D objects in the virtual environment. A shared whiteboard makes it easy for participants in the virtual environment to convey their ideas graphically. We survey various kinds of capture devices used for providing the input for the shared whiteboard. Efficient storage of the whiteboard session and precise archival at a later time bring up interesting research topics in information retrieval.	3d modeling;anomalous experiences;archive;avatar (computing);human–computer interaction;immersion (virtual reality);immersive technology;information retrieval;interactivity;multi-user;multimodal interaction;pen computing;speech processing;user interface;virtual reality	Wing Ho Leung;Tsuhan Chen	2003	Multimedia Tools and Applications	10.1023/A:1023466231968	human–computer interaction;eye tracking;computer science;virtual machine;multimodal interaction;speech processing;multimedia;facial expression	HCI	-47.97892395690497	-36.240041254565384	34958
7ffb62157f52f41ede98afbb7ca69bba4425fa87	an ontology for description of emotional cues	modelizacion;interfase usuario;ontologie;theoretical model;user interface;structure sandwich;abstraction;abstraccion;device independence;man machine system;modelisation;sandwich structure;emotion emotionality;levels of abstraction;cognition;cognicion;sistema hombre maquina;emotion emotivite;ontologia;interface utilisateur;emocion emotividad;estructura sandwich;modeling;ontology;affective computing;systeme homme machine	There is a great variety of theoretical models of emotions and implementation technologies which can be used in the design of affective computers. Consequently, designers and researchers usually made practical choices of models and develop ad-hoc solutions that sometimes lack flexibility. In this paper we introduce a generic approach to modeling emotional cues. The main component of our approach is the ontology of emotional cues. The concepts in the ontology are grouped into three global modules representing three layers of emotions' detection or production: the emotion module, the emotional cue module, and the media module. The emotion module defines emotions as represented with emotional cues. The emotional cue module describes external emotional representations in terms of media properties. The media module describes basic media properties important for emotional cues. Proposed ontology enables flexible description of emotional cues at different levels of abstraction. This approach could serve as a guide for the flexible design of affective devices independently of the starting model and the final way of implementation.	affective computing;computer;hoc (programming language);personalization;principle of abstraction;unified modeling language;usability;xml	Zeljko Obrenovic;Nestor Garay-Vitoria;Juan Miguel López;Inmaculada Fajardo;Idoia Cearreta	2005		10.1007/11573548_65	computer vision;systems modeling;cognition;computer science;artificial intelligence;machine learning;ontology;affective computing;abstraction;user interface	HCI	-37.70106936097853	-26.406345409944883	34985
a190e797e35017bf112d723bfb29eb4663ff970b	multimodal interaction and believability: how can we design and evaluate the next generation of ipa?			multimodal interaction;next-generation network	Jiajia Li;Yeuk Yu Lee	2017		10.14236/ewic/HCI2017.17	human–computer interaction;multimodal interaction;computer science	HCI	-50.81145159774776	-34.743456859501755	35049
7eeee8d8a1ad0f9c4535048d866be1873177eff9	development of a mobile robot which embodies a remote instructor	mobile robot;computer aided instruction;mobile robots;mobile robots communication system control medical treatment systems engineering and theory art laboratories mobile communication robot vision systems cameras optical control;satisfiability;natural interaction;time delay;atm networks;satellite links;atm network mobile robot remote instructor gestural information gestureman laser pointer video cameras satellite link;video cameras;laser pointer;field of view;telerobotics;video cameras telerobotics mobile robots computer aided instruction satellite links;high speed	"""A system which supports remote instruction for physical tasks in the real world should support following requirements: 1) flexibility of the body arrangement; 2) awareness of each other's orientations; and 3) communication of gestural information. To satisfy these requirements, we developed a system named """"GestureMan"""". GestureMan is a mobile robot mounted with three cameras and a remotely controlled laser pointer on it. An instructor controls the robot remotely with a joystick and gives instruction to an operator. Two preliminary experiments using a low-speed Satellite link and high-speed ATM network respectively show that GestureMan has an ability to support the remote instruction. Furthermore, gigabit network enabled the natural interaction between the instructor and the operator by alleviating limitations such as time delay and narrow field of view."""	mobile robot	Shinya Oyama;Hideaki Kuzuoka;Keiichi Yamazaki;Mamoru Mitsuishi;Kenji Suzuki	2000		10.1109/IROS.2000.893152	mobile robot;embedded system;computer vision;simulation;computer science;artificial intelligence	Robotics	-41.89287682730951	-45.183636272498454	35088
60f11642dfb63630df5a0b5b2f1200e5d0fe9efc	examining the effectiveness of gamification in human computation	engagement;acceptance;effectiveness;human computation;gamification	AbstractWithin the human computation paradigm, gamification is increasingly gaining interest. This is because an enjoyable experience generated by game features can be a powerful approach to attract participants. Although potentially useful, there has been little research conducted into understanding the effectiveness of gamification in human computation. In this experimental study, we operationalized effectiveness as perceived engagement and user acceptance, and examined it by comparing the performance of a gamified human computation system against a non-gamified version. We also investigate the determinants of acceptance and how their effects differ between these two systems. Analysis of our data found that participants experienced more engagement and showed higher behavioral intentions towards the gamified system. Moreover, perceived output quality and perceived engagement were significant determinants of acceptance of the gamified system. In contrast, determinants for acceptance of the non-gamified sy...	gamification;human-based computation	Xiaohui Wang;Dion Hoe-Lian Goh;Ee-Peng Lim;Adrian Vu;Alton Yeow-Kuan Chua	2017	Int. J. Hum. Comput. Interaction	10.1080/10447318.2017.1287458	human–computer interaction;operationalization;human computation;computer science	Web+IR	-59.711325558955934	-45.88277932490668	35198
8e67f2e8b15a1b0b6b0fa1bf7ca6a72b5d0a51e9	ai painting: an aesthetic painting generation system		There are many great works done in image generation. However, it is still an open problem how to generate a painting, which is meeting the aesthetic rules in specific style. Therefore, in this paper, we propose a demonstration to generate a specific painting based on users' input. In the system called AI Painting, we generate an original image from content text, transfer the image into a specific aesthetic effect, simulate the image into specific artistic genre, and illustrate the painting process.	glossary of computer graphics;simulation	Cunjun Zhang;Kehua Lei;J. J. Jia;Yihui Ma;Zhiyuan Hu	2018		10.1145/3240508.3241386	engineering drawing;multimedia;open problem;computer science;painting	AI	-39.508278392165764	-33.16296435610585	35199
2afe39050a415ba471ebdde1229ae6f34e4f2111	crosswatch: a camera phone system for orienting visually impaired pedestrians at traffic intersections	real time;computer vision;camera phone;visual impairment;off the shelf	"""Urban intersections are the most dangerous parts of a blind or visually impaired person's travel. To address this problem, this paper describes the novel """"Crosswatch"""" system, which uses computer vision to provide information about the location and orientation of crosswalks to a blind or visually impaired pedestrian holding a camera cell phone. A prototype of the system runs on an off-the-shelf Nokia camera phone in real time, which automatically takes a few images per second, uses the cell phone's built-in computer to analyze each image in a fraction of a second and sounds an audio tone when it detects a crosswalk. Tests with blind subjects demonstrate the feasibility of the system and its ability to provide useful crosswalk alignment information under real-world conditions."""	audio media;camera phone;canonical account;cellular phone;computer vision;mobile phone;prototype;visually impaired persons	Volodymyr Ivanchenko;James M. Coughlan;Huiying Shen	2008	Computers helping people with special needs : ... International Conference, ICCHP ... : proceedings. International Conference on Computers Helping People with Special Needs	10.1007/978-3-540-70540-6_168	computer vision;simulation;engineering;computer graphics (images)	Robotics	-40.20306043347354	-43.235803940539824	35223
413683c60d0db315750c5cfb5e559bb9cc0bb0e0	implementing wave particles for real-time water waves with object interaction	object interaction;real-time water wave;implementing wave particle;real time;water waves	We present the implementation details of our real-time simulation system explained in the SIGGRAPH 2007 paper, Wave Particles, and shown in the SIGGRAPH 2007 Computer Animation Festival under the same title. Figure 1 shows sample frames captured from our real-time simulation system. The method is based on the new concept of wave particles, which offers a simple, fast, and unconditionally stable approach to surface wave simulation of globally flowless fluids. We also developed a fast object interaction technique that connects our wave simulator to a rigid body simulation system for two-way interactions with the fluid (both object to fluid and fluid to object coupling). In this sketch, we elaborate on the implementation details of our approach and present how to use wave particles to achieve high-performance simulations of fluid surface waves and their interactions with floating objects on a standard PC.	apache wave;computer animation;interaction technique;personal computer;real-time clock;siggraph;simulation;surface wave	Cem Yuksel;Donald H. House;John Keyser	2007		10.1145/1280720.1280728	dispersion;computer science;mechanical wave	Graphics	-42.38816997449684	-34.74915017956321	35228
f7d4ea9f9ae304b6be25b22d922be4db9ce4e4b7	interactive computer graphics applied to the theoretical aircraft/store separation problem	computer program;computer graphic;flow field;early development;interactive graphics	Recently, a computer program was developed which computes the theoretical trajectory of a store in the complex flow field after release from an aircraft flying at subsonic speeds. However, the engineer still had the exceptionally difficult problem of preparing the source and sink models which are part of the computer program input.Views of the interactive graphics console illustrate how an interactive graphics computer program (SOURCE) is used to prepare source and sink theoretical aerodynamic models for the aircraft fuselage, nacelles, fuel tanks, and store bodies. Use of this interactive graphics computer program reduces the calendar time required to develop a source and sink model from approximately 2 weeks to one hour. This reduction in required calendar time permits theoretical store separation analyses to be conducted for a store in the early development program when the store shape is frequently being changed.	computer graphics;computer program;programmer;subsonic	Harold R. Spahr;Hugh A. Sumlin	1974	Computers & Graphics	10.1145/563182.563224	simulation;computer hardware;computer science;operating system;real-time computer graphics;3d computer graphics;computer graphics (images)	Graphics	-49.151526504515815	-28.516266858544235	35252
7c3c3848b1686ae19e16a8e2d1e2aaec3f38eac5	interactive diversity optimization of environments		The design of a building requires an architect to balance a wide range of constraints: aesthetic, geometric, usability, lighting, safety, etc. At the same time, there are often a multiplicity of diverse designs that can meet these constraints equally well. Architects must use their skills and artistic vision to explore these rich but highly constrained design spaces. A number of computer-aided design tools use automation to provide useful analytical data and optimal designs with respect to certain tness criteria. However, this automation can come at the expense of a designer’s creative control. We propose μDOME, a user-in-the-loop system for computer-aided design exploration that balances automation and control by e ciently exploring, analyzing, and ltering the space of environment layouts to better inform an architect’s decision-making. At each design iteration, μDOME provides a set of diverse designs which satisfy user-de ned constraints and optimality criteria within a user de ned parameterization of the design space. The user then selects a design and performs a similar optimization with the same or di erent parameters and objectives. This exploration process can be repeated as many times as the designer wishes. Our user studies indicates that μDOME, with its diversity-based approach, improves the e ciency and e ectiveness of even novice users with minimal training, without compromising the quality of their designs.	approximation algorithm;computer-aided design;crowd simulation;dynamic problem (algorithms);iteration;mathematical optimization;multi-objective optimization;multidisciplinary design optimization;naruto shippuden: clash of ninja revolution 3;spatial analysis;usability testing	Glen Berseth;Mahyar Khayatkhoei;M. Brandon Haworth;Muhammad Usman;Mubbasir Kapadia;Petros Faloutsos	2018	CoRR		human–computer interaction;automation;filter (signal processing);systems engineering;optimal design;computer science;usability;parametrization	EDA	-36.78726086696724	-33.91657109005973	35282
9f1dd1b23339b1e2ff4e059893fa00dc36a1155b	make it matter: the opportunities and responsibilities of pervasive computing research	context awareness;biographies;pervasive computing;health education;medical services;engineering profession;programming profession;ubiquitous computing;space technology;pervasive computing space technology ubiquitous computing context awareness engineering profession programming profession medical services biographies;human centric view pervasive computing ubiquitous computing;human centric view	Many technologists have been wooed by the vision of pervasive or ubiquitous computing. We must recall that much of the initial visions of this field urged a human-centric view of the challenges and opportunities. Much of the initial efforts in our field have addressed important technical challenges, but for the overall health and longevity of pervasive computing, we need to think beyond our own relatively small communities. In this talk, I will try to show how I think our field must turn outward and think beyond the formulation of problems that we alone understand. I will show some examples that use health, education and the home as case studies.	pervasive informatics;ubiquitous computing	Gregory D. Abowd	2009		10.1109/PERCOM.2009.4912779	context-aware pervasive systems;human–computer interaction;computer science;knowledge management;end-user computing;multimedia;space technology;ubiquitous computing	HCI	-59.10041706735421	-41.60847804253859	35293
549f6f9e6388127ebf9ae0db7a4c250a047e223e	partial-autonomous frenzy: driving a level-2 vehicle on the open road		Partial-autonomous vehicles are among us and represent a prominent testing ground for assessing the human interaction with autonomous vehicles. One main limitation of the studies investigating would-be users’ attitude toward partial to full autonomous driving stems from their indirect experience with such technology. In this study, participants drove a partial-autonomous vehicle on the open road and interacted with both Adaptive Cruise Control (ACC) and Lane Keeping Assist (LKAS) systems. Preliminary results show participants rating level-2 autonomous features as possible sources of stress. Participants had issues engaging these systems with denser traffic and thought these systems to be more beneficial in traffic-free driving. Compared to ACC, engaging LKAS and monitoring its functioning represented a more challenging task and participants’ ratings of stress toward this system increased over time. Findings obtained in this study are of importance for exploring user interaction with future highly-autonomous vehicles and designing effective countermeasures to make the human-machine interface of these systems more informative and easier to use.	autonomous robot	Francesco Biondi;Rachel Goethe;Joel M. Cooper;David L. Strayer	2017		10.1007/978-3-319-58475-1_25	human–machine interface;simulation;business;countermeasure;cruise control	Robotics	-47.14175647791662	-52.0688630887231	35319
b0164d77bb7e535b12a7bdc3ac716580d72bce9f	charanisml: a flexible virtual actor control interface	3d;animation language;embodied conversational agents;believable agents;digital storytelling;three dimensional;game based learning;scenejo;avatar animation;character animation;2d;embodied conversational agent;charanisml;virtual actor;interactive digital storytelling	In this submission we present a first step for an author-centric interface to believable agents. Based on a number of approaches for the description of 3D content, we developed CharanisML, the Character Animation System Meta Language. It is applicable for controlling both 2D and 3D avatars. To demonstrate this, we implemented two different clients in 2D and 3D that are able to interpret CharanisML. Also, they can be adapted as animation engines for interactive digital storytelling engines like Scenejo, that are used in the fields of entertainment as well as game-based learning. Using CharanisML it is possible for an author to control characters independently from both storytelling engines and twoor three-dimensional representation.	virtual actor	Sebastian A. Weiß;Florian Berger;Alexander Marbach;Wolfgang Müller	2009		10.1007/978-3-642-03364-3_16	character animation;three-dimensional space;2d computer graphics;simulation;virtual actor;computer facial animation;skeletal animation;embodied agent;computer science;computer animation;multimedia;3d computer graphics	Graphics	-42.21331356040824	-33.13330362948145	35339
6755b22aa0b912e2301475c698666c5e18e6e96e	shareme: a metaphor-based authoring tool for multimedia environments	multimedia authoring;user interface;interactive method;multimedia information system;design and implementation;information system;authoring tool	This paper investigates requirements on tools for the authoring of multimedia information systems. These requirements can be divided into those of the authors who create the information system, and those of the users who interact with these environments. The goal of multimedia authoring tools is to allow the author easily build new information environments, which can subsequently support the user with a variety of interaction methods. It will be recommended that the design and implementation tasks of multimedia environments should be integrated within the authoring tool, and that both the author and users should be supported by the same appropriate user interface metaphors.		Kaisa Väänänen	1993		10.1007/3-540-57312-7_57	human–computer interaction;multimedia;world wide web	HCI	-40.627268004295864	-27.454577816447927	35352
8b0a1a08e092870a9b1abbc556c667336dd9ed79	encouraging witting participation and performance in digital live art	unwitting;performer;poi;participant;wittingness;spectator;digital live art;maori art;exertion interface;ipoi;performance framing;peer to peer	We describe a framework for characterizing people’s behavior with Digital Live Art. Our framework considers people’s wittingness, technical skill, and interpretive abilities in relation to the performance frame. Three key categories of behavior with respect to the performance frame are proposed: performing, participating, and spectating. We exemplify the use of our framework by characterizing people’s interaction with a DLA iPoi. This DLA is based on the ancient Maori art form of poi and employs a wireless, peer-to-peer exertion interface. The design goal of iPoi is to draw people into the performance frame and support transitions from audience to participant and on to performer. We reflect on iPoi in a public performance and outline its key design features.	apache poi;drive letter assignment;exemplification;peer-to-peer	Jennifer G. Sheridan;Nick Bryan-Kinns;Alice Bayliss	2007		10.1145/1531294.1531297	psychology;simulation;multimedia;social psychology	HCI	-58.522685310065455	-35.38156879835232	35410
197e7e02043d4bcd32d9435ccc36a7d3890c536b	reflections on designing networked exertion games	videoconferencing;social interaction;tangible;physically active;sports;exertion interface;physical	Research in human-computer interaction has begun to acknowledge the benefits of physicality in the way people interact with computers. However, the role of physicality is often understood in terms of the characteristics of physical smart objects and their digital augmentation. We are stressing that the physicality lies within the interaction, not the object, and use a subset of bodily actions, exertion interactions, as an example to demonstrate our point. Emerging game designs have shown that supporting such exertion interactions can enable beneficial experiences between geographically distant participants. Based on several designs from our own work as well as others in this area we articulate reflections for the design of systems that support and facilitate bodily aspects of physicality in networked environments. We believe our work can serve as guidance for designers who are interested in creating future systems that support networked exertion interactions.	amiga reflections;computer;human–computer interaction;reflection (computer graphics);smart objects	Florian Mueller;Martin R. Gibbs;Frank Vetere	2013		10.1145/2513002.2513020	simulation;human–computer interaction;engineering;multimedia	HCI	-56.764283802276346	-38.21553951955844	35411
6b53261562dd329e54f577c5d8c6b9dd6ea2dace	personal robots in cs1: implementing the myro api in java	myro;cs1 curricula;personal robots;ipre;java	Personal robots, where each student has access to her/his own robot to use both in and out of class, are becoming popular platforms to use in CS1 courses. The Myro API developed by the Institute for Personal Robots in Education (IPRE) is a Python-based API and curriculum used at many colleges and universities. This paper describes the author's implementation of the Myro API in Java.	application programming interface;institute for personal robots in education;java;python;robot;web api	Douglas Harms	2011		10.1145/2023607.2023699	simulation;computer science;artificial intelligence;software engineering;multimedia;programming language;java;personal robot	Robotics	-48.241925157483806	-27.298589549243484	35528
dd6be3ed1ed5a0559adb10891292866d49a06d63	a database-based framework for gesture recognition	american sign language;sign language;online interaction;hand pose estimation;indexing methods;indexing method;community computing;indexation;image and video databases;embeddings;video database;gesture recognition;human computer interface;pose estimation;time constraint	Gestures are an important modality for human–machine communication. Computer vision modules performing gesture recognition can be important components of intelligent homes, assistive environments, and human–computer interfaces. A key problem in recognizing gestures is that the appearance of a gesture can vary widely depending on variables such as the person performing the gesture, or the position and orientation of the camera. This paper presents a database-based approach for addressing this problem. The large variability in appearance among different examples of the same gesture is addressed by creating large gesture databases, that store enough exemplars from each gesture to capture the variability within that gesture. This database-based approach is applied to two gesture recognition problems: handshape categorization and motion-based recognition of American Sign Language signs. A key aspect of our approach is the use of database indexing methods, in order to address the challenge of searching large databases without violating the time constraints of an online interactive system, where system response times of over a few seconds are oftentimes considered unacceptable. Our experiments demonstrate the benefits of the proposed database-based framework, and the feasibility of integrating large gesture databases into online interacting systems.	categorization;computer vision;database index;experiment;gesture recognition;interaction;interactivity;modality (human–computer interaction);spatial variability	Vassilis Athitsos;Haijing Wang;Alexandra Stefan	2009	Personal and Ubiquitous Computing	10.1007/s00779-009-0276-x	computer vision;speech recognition;pose;sign language;computer science;gesture recognition	AI	-35.09365243825623	-44.07779926111146	35549
9f274c780abc3fc5a15adcb6a77fe617bb62d9d6	virtually living together: a design framework for new communication media	interpersonal communication;telepresence;emotional communication;human computer interaction;manniska datorinteraktion interaktionsdesign;scenario based design;inter family communication;blue sky research;holistic approach;new media;interaction design;everyday life;industrial design	In this paper, we discuss the possibility of a holistic approach in the design of new media for interpersonal communication. The key argument is that if we base our design on daily practice, this may inhibit truly innovative ideas from taking form, and, on the contrary, if we design using pure intuition and visions, the design is likely to fail due to a lack of connection to daily practice. Scenario-based design was hence used to makes us envision new media while field observations such as ethnographic studies, become a tool to retain the ties with everyday life. This duality can also be implemented, as we will describe, in a design that makes a bi-language/bi-levelled understanding of a product possible.	holism;new media	Konrad Tollmar;Stefan Junestrand;Olle Torgny	2000		10.1145/347642.347670	simulation;industrial design;new media;human–computer interaction;computer science;communication design;engineering;artificial intelligence;interaction design;management;interpersonal communication	HCI	-60.024207866473354	-35.35854380919305	35572
0ff68e2e0b7443a3913077e00a6aa5e9f698c358	application of fitts' law to eye gaze interaction	fitts law;dwell time;cursor control;movement time;eye tracking;eye gaze;interaction technique;human computer interface	An experiment is described comparing the performance of an eye tracker and a mouse in a simple pointing task. Subjects had to make rapid and accurate horizontal movements to targets that were vertical ribbons located at various distances from the cursor's starting position. The dwell-time protocol was used for the eye tracker to make selections. Movement times were shorter for the mouse than for the eye tracker. Fitts' Law model was shown to predict movement times using both interaction techniques equally well. The model is thus seen to be a potential contributor to design of modern multimodal human-computer interfaces.	cursor (databases);eye tracking;fitts's law;interaction technique;multimodal interaction	Darius Miniotas	2000		10.1145/633292.633496	computer vision;simulation;human–computer interaction;eye tracking;computer science;fitts's law	HCI	-46.15485708977037	-46.56164712263301	35591
62a25e77d06941701f9737c08f69818afc32c5a4	a document browser based on a book-style interface with augmented reality	document handling;booklet;human computer interaction;motion pictures;ar marker;easyto use interface;prototypes;gesture interaction;users gesture recognition;users gesture recognition document browser document browsing method book style interface office environment augmented reality ar technology easyto use interface multimedia contents booklet ar marker motion sensor kinect;graphical user interface;motion sensor kinect;multimedia systems;multimedia contents;book style interface;three dimensional displays prototypes tracking multimedia communication augmented reality motion pictures;three dimensional displays;multimedia communication;multimedia systems augmented reality document handling gesture recognition human computer interaction;document browsing method;document browser;augmented reality;graphical user interface augmented reality book style interface gesture interaction;ar technology;gesture recognition;office environment;tracking	We propose a document browsing method based on a book-style interface usable in a normal office environment. Augmented reality (AR) technology is applied to realize an easy-to-use interface. In the proposed method, documents and multi-media contents are projected on a real booklet including an AR marker printed on each page. Users can feel that they are reading a book consisting of document files, presentation files, movies, and images. Furthermore, we utilize the motion sensor Kinect to recognize the users' gestures. The detected gestures can be used to activate some functions such as picking a page up and placing it in a real work space. In this paper, we describe the proposed book-style interface, its implementation method, and evaluate and discuss the effectiveness of the proposed method.	augmented reality;book;e-book;experiment;image scaling;kinect;motion detector;operability;page up and page down keys;printing;workspace	Masatoshi Nishimura;Tsuneo Kagawa;Hiroaki Nishino;Kouichi Utsumiya	2013	2013 Seventh International Conference on Complex, Intelligent, and Software Intensive Systems	10.1109/CISIS.2013.133	human–computer interaction;computer science;multimedia;computer graphics (images)	HCI	-42.96612160769851	-39.899572369713354	35599
c7485ab57beccaa64579ff050aa6c46e709e03bc	php - pocket reference: creating dynamic web pages: covers php 4 (2. ed)	web pages		dynamic web page;handbook;php	Rasmus Lerdorf	2002			static web page;human–computer interaction;computer science;dynamic web page;database;world wide web	Web+IR	-43.73406181183415	-24.065519213582093	35667
aa7582e80d14faaaf5cad278a5e1e58bb7edb9d8	retail user assistant: evaluation of a user-adapted performance support system	user adaptation;adaptive interface;support system;point of sale	This paper describes the Retail User Assistant (RUA), a prototype system that provides continual adaptive help for the use of an existing retail point-of-sale device. A preliminary study compared performance of three groups: one using the retail device alone, one using the device with a nonadaptive version of the RUA, and one with the adaptive RUA. Trends in the results imply that performance support, both adaptive and nonadaptive, improved accuracy but slowed performance. The RUA's adaptation, which gave less detailed help as users learned the task, was readily accepted by users and seems to have improved users' performance times. This result is discussed in the context of previous research on adaptive help and adaptive interfaces.		Beth Meyer	1994		10.1007/3-540-58648-2_25	user interface design;user;simulation;user modeling;human–computer interaction;computer science;multimedia;point of sale	HCI	-47.5113074077553	-45.4071539045836	35680
716b9747795b44cc15724b20bf81ed4ade6d49fb	web-chat php based system using mysql database as interactive communication media on daskom on-line			mysql;php;web chat	Anak Agung Putri Ratna;J. Samuel;Luhur Bayuaji	2002			world wide web;database;computer science	NLP	-43.91987991048209	-24.09396421775198	35687
3dfc91963ec74dec953914d6db99b44ad4b90ace	user-centered and analytic-based approaches to generate usable gestures for individuals with quadriplegia	spinal cord injury sci assistive technologies hand gesture based interfaces laban space;user centred design gesture recognition handicapped aids injuries;standards;manifolds;trajectory standards feature extraction transforms manifolds yttrium interviews;sci user centered approach analytic based approach hand gesture based interface quadriplegia upper limb motor impairment spinal cord injury;trajectory;yttrium;feature extraction;transforms;interviews	Hand gesture-based interfaces have become increasingly popular as a form to interact with computing devices. Unfortunately, standard gesture interfaces are not very usable by individuals with upper limb motor impairments, including quadriplegics due to spinal cord injury (SCI). The objective of this paper is to convert an existing interface to be usable by users with motor impairments. The key idea is to project existing patterns of gestural behavior to match those exhibited by users with quadriplegia due to common cervical SCIs. Two complementary approaches (a user-centered and an analytic approach) have been developed and validated to provide both subjective and quantitative solutions to interface design. The feasibility of the proposed methodology was validated through user-based experimental paradigms. Through this study, subjects with upper extremity motor impairments preferred (gave a significantly lower Borg scale) the use of alternative constrained gestures generated by the proposed approach rather than the standard gestures.	borg (star trek);user-centered design	Hairong Jiang;Bradley S. Duerstock;Juan Pablo Wachs	2016	IEEE Transactions on Human-Machine Systems	10.1109/THMS.2015.2497346	simulation;speech recognition;interview;manifold;feature extraction;computer science;artificial intelligence;trajectory;yttrium;machine learning;gesture recognition;quantum mechanics	HCI	-48.217382369515896	-49.80438598921663	35768
70ef177ddcf5a9a45a81a489798fe807aa5cfdb7	what video can and cannot do for collaboration: a case study	user interface;computer supported cooperative work;collaborative system;desktop video conferencing;conversation;shared space;face to face;user interfaces;remote collaboration	As multimedia become an integral part of collaborative systems, we must understand how to design such systems to support the user's rich set of existing interaction skills, rather than requiring people to adapt to arbitrary constraints of technology-driven designs. To understand how we can make effective use of video in remote collaboration, we compared a small team's interactions through a desktop video conferencing prototype with face-to-face interactions and phone conversations. We found that, compared with audio-only, the video channel of our desktop video conferencing prototype adds or improves the ability to show understanding, forecast responses, give nonverbal information, enhance verbal descriptions, manage pauses, and express attitudes. These findings suggest that video may be better than the phone for handling conflict and other interaction-intense activities. However, the advantages of video depend critically on the nearly-instantaneous transmission of audio, even if it means getting out of sync with the video image. Nonetheless, when compared with face-to-face interaction, it can be difficult in video interactions to notice peripheral cues, control the floor, have side conversations, point to things or manipulate real-world objects. To enable rich interactions fully, video should be integrated with other distributed tools that increase the extent and type of shared space in a way that enables natural collaborative behaviors within those environments.		Ellen Isaacs;John C. Tang	1994	Multimedia Systems	10.1007/BF01274181	human–computer interaction;computer science;operating system;multimedia;user interface;mobile collaboration;world wide web	HCI	-53.67802467969264	-43.80082378425116	35787
2c498f5203da28bd9c85186af240c2a4e0af0823	urbanhermes: social signaling with electronic fashion	electronic media;virtual networks;wearables;mobile device;urban space;online community;access to information;accessories;signaling theory;identity and expression;clothing;face to face;mobile devices;fashion;social signals	Humans use fashion signals to indicate access to information. While fashion is typically associated with clothing, fashion also transpires within the domain of electronic media: weblogs, discussion lists, and online communities teem continuously with fresh, digestible content. A fashionable status - well-informed and well-connected - is demonstrated through a consistent, timely, and meaningful display of newly acquired information. While production constraints of material-based fashions limit the signal refresh rate, ephemeral electronic fashions can cycle as quickly as the flow of information. The challenge we present is to develop physical objects that can go beyond the limitations of their materiality, and to signal with the rapidity of electronic fashions. We introduce the design of urbanhermes as a communicative accessory that integrates the fresh, dynamic, fluid nature of electronic-based fashion signals within the tactile, face-to-face environment of a physical space. This paper presents the design discussion within the framework of fashion as a social signal.	blog;freedom of information laws by country;humans;materiality (digital text);online community;refresh rate	Christine M. Liu;Judith S. Donath	2006		10.1145/1124772.1124902	human–computer interaction;computer science;operating system;mobile device;multimedia;world wide web	HCI	-58.05580020249152	-36.93989722866585	35825
aa4070e6784d1518a3a93b7fd258aa9142b98f28	toward a user-centric digital ecosystem	social network services;social networking services;information technology keywords total experience design txd user centric design;information technology;keywords total experience design;colloborative tools;user centered design;txd;internet;digital communication;web design;ecosystems;user centric design;interface states;user centred design internet;web applications user centric digital ecosystem data processing machine human centered design digital strategy;user centred design;user centered design digital communication ecosystems colloborative tools social network services web design design methodology interface states;experience design;design methodology	Digital platforms are transforming into a series of personal companions that are as much an extension of the user as they are a data processing machine. To build a cohesive digital brand that truly connects with and influences users, designers and developers must apply human-centered design to reshape their digital strategy, whether that strategy is to develop Web applications, manage and tailor content, or launch new services.	digital ecosystem;digital strategy;user-centered design;web application	Mile Corrigan;H. Gilbert Miller	2011	IT Professional	10.1109/MITP.2011.72	digital transformation;user-centered design;human–computer interaction;computer science;multimedia;law;information technology;world wide web	HCI	-44.27986420559156	-24.47547122485973	35850
a3365b34d6a05964d893fab9269fbcb2371b15d6	effects of stereoscopic presentation, image motion, and screen size on subjective and objective corroborative measures of presence	post-test subjective rating;subjective presence rating;subjective judgment;stereoscopic presentation;20-inch stereoscopic screen;lateral postural response;screen size;image motion;objective corroborative measures;group subjective measure;significant effect	Recently, we reported that group subjective measures of presence as well as observers' postural responses are sensitive to increasing the realism of a display with motion content, by the addition of stereoscopic information, using a 20-inch stereoscopic screen with an effective horizontal field of view of 28 deg. (Freeman, Avons, Meddis, Pearson, & IJsselsteijn, 2000). The experiment presented here employed a large projection display with a 50 deg. horizontal field of view showing a rally car traversing a curved track at speed. The independent variables included image motion and stereoscopic presentation as within-subjects factors and screen size as a between-subjects factor. Dependent variables included subjective measures of presence, vection, involvement, and sickness, as well as observers' lateral postural responses, which served as a candidate objective corroborative measure of presence. Results demonstrated a noisy yet positive effect of stereoscopic presentation on the lateral postural responses. Post-test subjective ratings revealed a significant effect of stereoscopic presentation on the subjective judgments of presence, but not on those of vection, involvement, or sickness. Image motion had a large and significant effect on the subjective judgments of presence, vection, and involvement. The effect of image motion was considerably larger than that of stereoscopic viewing. By comparing results between experiments, a large effect of screen size on subjective presence ratings could be demonstrated, but only for the video stimulus that contained motion. The postural response measure did not differentiate between screen sizes, thus limiting its utility as an objective corroborative measure of presence, although further research is required to be able to be more firm in our conclusion regarding this issue.	display size;experiment;lateral thinking;stereoscopy;utility	Wijnand A. IJsselsteijn;Huib de Ridder;Jonathan Freeman;Steve E. Avons;Don Bouwhuis	2001	Presence: Teleoperators & Virtual Environments	10.1162/105474601300343621	computer vision;simulation	HCI	-43.674829450735345	-49.31390059684958	35871
1485b72151d53895a1e251d2e42d84f78c1a5009	multiple view geometry in computer vision	cybernetics;publication;robotics;machine vision;artificial intelligence	Downloading the book in this website lists can give you more advantages. It will show you the best book collections and completed collections. So many books can be found in this website. So, this is not only this multiple view geometry in computer vision. However, this book is referred to read because it is an inspiring book to give you more chance to get experiences and also thoughts. This is simple, read the soft file of the book and you get it.	book;computer vision;download;experience;jargon file	Bernhard P. Wrobel	2001	KI		computer vision;theoretical computer science;mathematics;geometry	Vision	-59.8292597190228	-24.380678108345464	35921
531ca9d42330f40ead06d26bdd9d09b77aba464a	transformtable: a self-actuated shape-changing digital table	spatial behaviors;tabletop display;group interaction	This paper proposes TransformTable, an interactive digital table, whose shape can be physically and dynamically deformed. Shape transformations are mechanically and electrically actuated by wireless signals from a host computer. TransfomTable represents digital information in a physically changeable screen shape and simultaneously produces different spatial arrangements of users around the table. This provides visual information while changing the physical workspace to allow users to effectively handle their tasks. We implemented the first TransformTable prototype that can deform from/into one of three typical shapes: round, square, or rectangular. We also discuss implementation methods and further application designs and scenarios. A preliminary study shows fundamental and potential social impacts of the table transformation on users' subjective views in a group conversation.	digital data;host (network);prototype;shape context;workspace	Kazuki Takashima;Naohiro Aida;Hitomi Yokoyama;Yoshifumi Kitamura	2013		10.1145/2512349.2512818	simulation;engineering;multimedia;communication	HCI	-46.97598095953333	-38.96279728277094	35932
d426da948841ea3c6ff41e0113def961a9d8e7ca	bringing haptics and physical simulation together: haptic travel through physical worlds	rigid body;haptic device;virtual environments;force feedback;navigation;haptic rendering;virtual environment;weed management;animated character;haptic interaction;physical simulation	Abstract#R##N##R##N#This paper describes our efforts in bringing haptics closer to current dynamic virtual environments (VE). These interactive 3D worlds make more and more use of physical simulations in order to increase realism. As a first step in closing the gap, we propose haptic travel that allows users to feel how their virtual representation navigates through the simulated world. In this work, we show how we coupled stable haptic rendering to physical simulation in order to achieve this. By generating a force feedback field, based on the user's input in combination with collision information provided by a rigid body simulator, we managed to provide the user with useful information on what is happening to its virtual representation. A humanoid animated character, which represents the user, is coupled to the rigid body object that represents the user in physical space. This character is animated according to the travel motions that the physical object makes, depending on user input from the haptic device. Our approach is suitable for a whole set of applications and input devices and can reduce the number of devices necessary to interact in VEs. Copyright © 2006 John Wiley & Sons, Ltd.	dynamical simulation;haptic technology	Pieter Jorissen;Joan De Boeck;Wim Lamotte	2006	Journal of Visualization and Computer Animation	10.1002/cav.121	stereotaxy;simulation;computer science;artificial intelligence;multimedia;haptic technology;computer graphics (images)	Visualization	-41.31893914797304	-37.725608303956804	35942
fbc4abbfe040f2c7c90d024b7027c818a4b134be	analyzing perceptual representations of complex, parametrically-defined shapes using mds	three dimensional;multidimensional scaling;similarity;shape parameter;visual perception;haptic perception;psychophysics	In this study we show that humans are able to form a perceptual space from a complex, three-dimensional shape space that is highly congruent to the physical object space no matter if the participants explore the objects visually or haptically. The physical object space consists of complex, shell-shaped objects which were generated by varying three shape parameters. In several psychophysical experiments participants explored the objects either visually or haptically and performed similarity ratings. Multidimensional scaling (MDS) analyses showed high congruency of the visual and haptic perceptual space to the physical object space. Additionally, visual and haptic exploration resulted in very similar MDS maps providing evidence for one shared perceptual space underlying both modalities.	experiment;haptic technology;image scaling;map;microsoft outlook for mac;modality (human–computer interaction);multidimensional scaling;randomized algorithm;simplicial complex;tuple space	Nina Gaißert;Christian Wallraven;Heinrich H. Bülthoff	2008		10.1007/978-3-540-69057-3_31	computer vision;mathematics;communication;social psychology	HCI	-42.14649165355363	-51.503762966890356	35978
d706458bd54603df84bc5c21bf47fbfa78abae00	user performance of gestural and non-gestural operations in a mixed computer task under different multi-touch screen configurations		Multi-touch screens are the fastest growing main stream technology for human-computer interaction and gestural operation usually accompanies their use. Research has shown that configuring single-touch screens at greater tilting angles might optimize manual operation. Furthermore, gestural operation, given its direct-manipulating nature, may enable better performance in object manipulation tasks. The current study recruited 20 volunteer participants to perform a mixed computer task by 4 modes of operations on a multi-touch screen at 2 display positions. The results showed that display position optimized for manual operation did not improve user performance. KM mode (participants used keyboards/mice) generated the best overall performance. Gesture mode enabled participants to have comparable precision to that of KM mode in sizing the picture, while Touch mode completed the task in a shorter time. All mode that allowed mixed use of all types of operations did not outperform using traditional keyboards/mice.	multi-touch;pointing device gesture	Cheng-Jhe Lin	2016		10.1007/978-3-319-40542-1_19	simulation;human–computer interaction;computer science;multimedia	HCI	-46.49288281872664	-45.91247476699328	35993
8fe2c7522b014fb7f73cc4cb0ef802dbefa0a938	intent imitation using wearable motion capturing system with on-line teaching of task attention	intent imitation;human behavior;motion capture;education humanoid robots learning systems humans artificial intelligence intelligent robots character generation trajectory pattern recognition emulation;wearable motion capturing system;humanoid robots;humanoid robots task attention intent imitation wearable motion capturing system online teaching;user interfaces artificial intelligence humanoid robots;artificial intelligence;online teaching;user interfaces;task attention	In order for humanoids to imitate humans behavior, it is important to extract a needful parameter for target of imitation. Especially in daily-life environment, only simple joint angles are insufficiency because position and posture of hands and remarkable point of target object are needed for intent imitation. In this paper, we describe a development methods of motion capturing system with interactive teaching of task attention, and show its feasibility in daily-life environments	automated planning and scheduling;constraint algorithm;convergence insufficiency;hidden markov model;humanoid robot;motion capture;online and offline;poor posture;sensor;wearable computer	Tetsunari Inamura;Naoki Kojo;Tomoyuki Sonoda;Kazuyuki Sakamoto;Kei Okada;Masayuki Inaba	2005	5th IEEE-RAS International Conference on Humanoid Robots, 2005.	10.1109/ICHR.2005.1573611	computer vision;motion capture;simulation;computer science;humanoid robot;artificial intelligence;user interface;human behavior	Robotics	-36.99640329433403	-42.94577502370266	36046
7466f7c750e96850659d5692e3b81d808e1c9eac	learning from demonstration to be a good team member in a role playing game	learning from demonstration;case based reasoning;computer games	We present an approach that uses learning from demonstration in a computer role playing game to create a controller for a companion team member. We describe a behavior engine that uses case-based reasoning. The behavior engine accepts observation traces of human playing decisions and produces a sequence of actions which can then be carried out by an artificial agent within the gaming environment. Our work focuses on teambased role playing games, where the agents produced by the behavior engine act as team members within a mixed human-agent team. We present the results of a study we conducted, where we assess both the quantitative and qualitative performance difference between human-only teams compared with hybrid human-agent teams. The results of our study show that human-agent teams were more successful at task completion and, for some qualitative dimensions, hybrid teams were perceived more favorably than human-only teams.	case-based reasoning;digital footprint;intelligent agent;reinforcement learning;tracing (software)	Michael Silva;Silas McCroskey;Jonathan Rubin;Michael Youngblood;Ashwin Ram	2013			case-based reasoning;simulation;computer science;knowledge management;artificial intelligence;team effectiveness	AI	-53.95372060120507	-48.40801990913638	36089
ae13b2961685365aae9b919d7111d28cf335687c	sonic interaction design	inf 01 informatica;interaction design	It is quite obvious that sounds can induce actions. Auditory alerts are explicitly designed to induce actions of different kinds and urgency, such as picking up a phone call or searching for a power supply. It is perhaps less obvious that sounds are continuously mediating human– object interactions, even though eminent scholars recently wrote about the role of sound in the experience of everyday objects such as orange juicers (Buxton, 2007) or kettles (Norman, 2007). In reviewing Norman’s book, Torenvliet (2008) writes that ‘‘in a world where everything beeps he would like to see designers experiment with a richer palette of sounds’’. To describe his favorite juicer, Buxton tightly couples sound, proprioception, and action when saying that ‘‘There is a cadence in the action that is almost musical’’. Sounds can be functional to the use of products, and they can contribute to the overall appreciation of an object by participating to the perception–action loop. Sonic Interaction Design is the activity of shaping the relation between humans and objects by means of sound. It explores ways in which sound can be used to convey information, meaning, aesthetic and emotional qualities in interactive contexts. Sonic Interaction Design is an emergent discipline which is the result of efforts that come from different directions. Within human–computer studies, auditory display and sonification have been topics of interest for a couple of decades (Kramer, 1994; Kramer and Walker, 2005). In sound and music computing (Polotti and Rocchesso, 2008), there has been a growing interest into principles and methods to design and evaluate sonic interactive systems, and this is considered to be one of the most promising areas for research and experimentation (The S2S Consortium, 2007). Among scholars in perception and cognition there has been a shift in attention, from the human as a receiver of auditory stimuli, to the perception–action loops that are mediated by acoustic signals (Leman, 2007). In interaction design, the availability of physical computing resources has put the construction of interactive sonic objects among the favorite activities of many practitioners. The emergence of the discipline of Sonic Interaction Design is facilitated by the increasing possibilities offered by sensors and actuators	as-interface;acoustic cryptanalysis;action potential;amiga walker;auditory display;comefrom;cognition;computer;consortium;emergence;information;interactivity;kramer graph;noise shaping;palette (computing);physical computing;power supply;sensor;sonic interaction design;sonification;sound and music computing	Davide Rocchesso;Stefania Serafin	2009	Int. J. Hum.-Comput. Stud.	10.1016/j.ijhcs.2009.09.009	computer science;interaction design	HCI	-54.27901614466022	-37.30260951409819	36142
6dffeecf0ce29af3f90142628d6f0baae5a9ebab	infoscout: an interactive, entity centric, person search tool	web investigation;professional search;qa75 electronic computers computer science;open source intelligence;entity search;people searching	Individuals living in highly networked societies publish a large amount of personal, and potentially sensitive, information online. Web investigators can exploit such information for a variety of purposes, such as in background vetting and fraud detection. However, such investigations require a large number of expensive man hours and human effort. This paper describes InfoScout, a search tool which is intended to reduce the time it takes to identify and gather subject centric information on the Web. InfoScout collects relevance feedback information from the investigator in order to re-rank search results, allowing the intended information to be discovered more quickly. Users may still direct their search as they see fit, issuing ad-hoc queries and filtering existing results by keywords. Design choices are informed by prior work and industry collaboration.	hoc (programming language);relevance feedback;world wide web	Sean McKeown;Martynas Buivys;Leif Azzopardi	2016		10.1145/2911451.2911468	computer science;machine learning;data mining;multimedia;world wide web;information retrieval;search engine	Web+IR	-36.755246266565095	-49.147230428734034	36185
234dfb0a5a49627d90fdcf040fcc858c26cbb02f	orchestrated informal care coordination: designing a connected network of tools in support of collective care activities for informal caregivers	connected home;qualitative methods;caregiver;internet of things;caregiving;field study	"""Often, family caregivers experience difficulties in coordinating older adults' health care because it requires not only a lot of time but also a diverse set of responsibilities to coordinate care for their loved ones. While many can reduce their individual burden by sharing care tasks with other family members, there are still many challenges to overcome in maintaining the quality of care when they work together. As they increase their informal care network, it becomes more difficult for them to stay informed and coordinated. Coordination breakdowns caused by having multiple caregivers who are cooperating to care for the same care recipient result in reduced quality of care. I explored opportunities for """"Internet of Things (IoT)"""" technologies to help informal caregivers better coordinate and communicate care with each other for their loved ones. Based on identified design opportunities, I propose the concept of CareBot, a smart home platform consisting of interactive tools in support of collective care activities of family caregivers. \"""	home automation;internet of things	Kyungmin Youn	2016		10.1145/2984751.2984752	human–computer interaction;computer science;qualitative research;internet of things;field research	HCI	-59.48326988920439	-42.507632924678845	36206
56259fd6f66bca49916f47fcbd4e2e448c2dc716	image presentation in space and time: errors, preferences and eye-gaze activity	visual information browsing;rapid serial visual presentation;eye gaze tracking;space time;space time trade off;time domain;resource availability;visual interfaces;visual interface design;interaction design;eye gaze	Rapid Serial Visual Presentation (RSVP) is a technique that allows images to be presented sequentially in the time-domain, thereby offering an alternative to the conventional concurrent display of images in the space domain. Such an alternative offers potential advantages where display area is at a premium. However, notwithstanding the flexibility to employ either or both domains for presentation purposes, little is known about the alternatives suited to specific tasks undertaken by a user. As a consequence there is a pressing need to provide guidance for the interaction designer faced with these alternatives.We investigated the task of identifying the presence or absence of a previously viewed image within a collection of images, a requirement of many real activities. In experiments with subjects, the collection of images was presented in three modes (1) 'slide show' RSVP mode; (2) concurrently and statically -- 'static mode'; and (3) a 'mixed' mode. Each mode employed the same display area and the same total presentation time, together regarded as primary resources available to the interaction designer. For each presentation mode, the outcome identified error profiles and subject preferences. Eye-gaze studies detected distinctive differences between the three presentation modes.	eb-eye;experiment;interaction design	Robert Spence;Mark Witkowski;Catherine Fawcett;Brock Craft;Oscar de Bruijn	2004		10.1145/989863.989884	computer vision;simulation;human–computer interaction;eye tracking;time domain;computer science;interaction design;space time;multimedia	HCI	-47.92696694554212	-47.832767502435196	36212
93fc7b32619852406d9dfcf907ff0dcbf4347dae	fit your hand: personalized user interface considering physical attributes of mobile device users	physical attribute;mobile user interfaces;mobile device;user interface;personalization;proof of concept;physical characteristic;software architecture;machine learning;intelligent interaction;touch input pattern;adaptive user interface	We present a mobile user interface which dynamically reformulates the layout based on the touch input pattern of users. By analyzing the touch input, it infers users' physical characteristics such as handedness, finger length, or usage habits, thereby calculates the optimal touch area for the user. The user interface is gradually adapted to each user by automatically rearranging graphic objects such as application icons to the most easy-to-touch positions. To compute the optimal touch area, we designed software architecture and implemented an Android application which analyzes touch input and determines the touch frequency in specific screen areas, the handedness and hand size of users. As proof of concept, this research prototype shows acceptable performance and accuracy. To decide which items should be placed in the optimal touch area, we plan to integrate our machine learning algorithm which prioritizes applications according to the context of users into the proposed system.	algorithm;android;circular polarization;machine learning;mobile device;personalization;prototype;software architecture;user interface	Hosub Lee;Young Sang Choi	2011		10.1145/2046396.2046422	user interface design;software architecture;10-foot user interface;human–computer interaction;computer science;operating system;personalization;mobile device;multimedia;natural user interface;user interface;proof of concept;world wide web	HCI	-46.065458958056105	-42.79741484164143	36284
f78c5bf1682fe81d5aea82aad00cfc80233c8418	are we there yet?: the role of gender on the effectiveness and efficiency of user-robot communication in navigational tasks	pen gesture;gender difference;human robot interaction;command system;paper based interface;navigation system;article;virtual worlds;spatial navigation	Many studies have identified gender differences in communication related to spatial navigation in real and virtual worlds. Most of this research has focused on single-party communication (monologs), such as the way in which individuals either give or follow route instructions. However, very little work has been reported on spatial navigation dialogs and whether there are gender differences in the way that they are conducted. This article will address the lack of research evidence by exploring the dialogs between partners of the same and of different gender in a simulated Human-Robot Interaction study. In the experiments discussed in this article, pairs of participants communicated remotely; in each pair, one participant (the instructor) was under the impression that s/he was giving route instructions to a robot (the follower), avoiding any perception of gendered communication. To ensure the naturalness of the interaction, the followers were given no guidelines on what to say, however, each had to control a robot based on the user's instructions. While many monologe-based studies suggest male superiority in a multitude of spatial activities and domains, this study of dialogs highlights a more complex pattern of results. As anticipated, gender influences task performance and communication. However, the findings suggest that it is the interaction—the combination of gender and role (i.e., instructor or follower)—that has the most significant impact. In particular, pairs of female users/instructors and male “robots”/followers are associated with the fastest and most accurate completion of the navigation tasks. Moreover, dialoge-based analysis illustrates how pairs of male users/instructors and female “robots”/followers achieved successful communication through “alignment” of spatial descriptions. In particular, males seem to adapt the content of their instructions when interacting with female “robots”/followers and employ more landmark references compared to female users/instructors or when addressing males (in male-male pairings). This study describes the differences in how males and females interact with the system, and proposes that any female “disadvantage” in spatial communication can disappear through interactive mechanisms. Such insights are important for the design of navigation systems that are equally effective for users of either gender.	experiment;fastest;gps navigation device;human–robot interaction;robot;spatial navigation;virtual world	Theodora Koulouri;Stanislao Lauria;Robert D. Macredie;Sherry Y. Chen	2012	ACM Trans. Comput.-Hum. Interact.	10.1145/2147783.2147787	human–robot interaction;spatial memory;simulation;human–computer interaction;computer science;multimedia	HCI	-48.44193290169397	-48.62084827740822	36297
629fda9e7934ef903a9da47a72c24b425ae5471f	qualifying ontology-based visual query formulation		This paper elaborates on ontology-based end-user visual query formulation, particularly for users who otherwise cannot/do not desire to use formal textual query languages to retrieve data due to the lack of technical knowledge and skills. Then, it provides a set of quality attributes and features, primarily elicited via a series of industrial enduser workshops and user studies carried out in the course of an industrial EU project, to guide the design and development of successor visual query	industrial pc;list of system quality attributes;query language;usability testing	Ahmet Soylu;Martin Giese	2015		10.1007/978-3-319-26154-6_19	query expansion	HCI	-41.39635550346054	-26.15175943841505	36298
c37a515623b56dd92d585c5fde82eaf8cc7cf8fd	fun with your alarm clock: designing for engaging experiences through emotionally rich interaction	engaging experience;alarm clock;rich interaction	To strive for the incorporation of fun in product use is to design for engaging experiences. In order to do this we should respect all human skills in human-product interaction. So while most current electronic products appeal to cognitive skills, we believe that a person’s perceptual-motor and emotional skills should be taken into account as well. One way of opening up such an experience is to allow people to use their natural expressive powers by permitting them to use their perceptual-motor skills. Most current products do not tap into these skills because their functionality is accessible in just one way, and often a very poor way indeed.		Stephan Wensveen;Kees C. J. Overbeeke	2018		10.1007/978-3-319-68213-6_38	engineering;multimedia;communication;social psychology	HCI	-55.52847113119727	-45.48838079032557	36383
3540bec1cf57b3d9f6e43e7a6d4455e1616d3555	guest editorial: interactive media: technology and experience		The shifting balance between lean-back passive TV/Web-based media experience and leanforward interactivity has led to new forms of collaborative content creation. This allows controlling media with a companion screen and more advanced forms of audiovisual content interaction. Based on such developments, new media formats and consumption paradigms that allow for new types of interactivity have emerged. This special issue focuses on interactive media experiences and presents articles on recent advances regarding interaction with audiovisual content, both recorded and live. The issue brings together articles from the area of interactive media around topics of interest like enabling technologies, experiences, user interaction, and content. It shows best practices in all these areas as well as future challenges. The special issue received 31 submissions showing its widely-gained attention. After two rounds of revision, a total of 9 manuscripts were accepted. The manuscripts address the following issues in the field:	best practice;interactive design;interactive media;interactivity;new media	Britta Meixner;Rene Kaiser;Joscha Jäger;Wei Tsang Ooi;Harald Kosch	2016	Multimedia Tools and Applications	10.1007/s11042-016-4307-0	artificial intelligence;interactive media;computer science;computer vision;multimedia	HCI	-52.365080821312084	-30.949569578294028	36390
3e0a83df32ad4eb96b8b268e59e3373853622ee5	developing a networked public display system	mobile;multimedia information systems;project management;pervasive computing;hci;internet web technologies;stakeholders;software engineering;software defined networking;human factors;public displays;software development;networking;bulletin boards;informatics	"""Engaging stakeholders in the design of networked public display systems is a key factor for a successful long-term deployment. Previous work has identified these stakeholders as display providers, content producers, and content viewers. Yet reports on stakeholder engagement and their influence on system design and overall deployment processes are rare. Here, the authors describe the process of developing and installing a display network at a university over the course of three years. Their work uncovers the fuzziness of stakeholder roles and illustrates the compromises and practical decisions made during the design and deployment process when working within a university setting. Their experiences with the design and deployment of a public display network contribute to the knowledge of building and deploying systems """"in the wild."""" This article is part of a special issue on pervasive displays."""	pervasive informatics;software deployment;systems design	Nemanja Memarovic;Ivan Elhart;Elisa Rubegni	2016	IEEE Pervasive Computing	10.1109/MPRV.2016.59	project management;stakeholder;human–computer interaction;computer science;knowledge management;software development;operating system;mobile technology;multimedia;software-defined networking;informatics;software deployment;ubiquitous computing	HCI	-60.13790934026159	-40.788168263231455	36407
438e77b761e32e6b62f58fda103c0cbc42f746bf	process-oriented evaluation of user interactions in integrated system analysis tools	analytical models;visualization process oriented evaluation user interactions integrated system analysis tools computer based tools complex systems future tool development space exploration logistics tools model transparency hierarchical structure;usability testing;measurement;testing;data visualisation;usability propulsion measurement testing logistics interviews analytical models;logistics;analysis tools;software development;interviews;complex systems;user interfaces data visualisation software tools;propulsion;software tools;usability;user interfaces;analysis tools complex systems software development usability testing space logistics;space logistics	When computer-based tools are used for analysis of complex systems, the design of user interactions and interfaces becomes an essential part of development that determines the overall quality. The objective of this study is to investigate the processes and results of user interactions with integrated analysis tools to synthesize design implications for future tool development. In this study, two space exploration logistics tools are compared in a controlled user experiment. Through a comparative usability analysis, this study evaluated user performance and perception to provide design implications for future integrated analysis tools. For a comprehensive evaluation, multiple methods were used for data collection, including observation, questionnaire and interview. In addition to a result-oriented performance analysis, a process-oriented approach was used for analyzing patterns in user behaviors and errors. Results are presented with reference to the related features embedded in the interfaces of the two tools. Based on the comparative results, synthesized design insights for hierarchical structure, model transparency, automation, and visualization and feedback are discussed for integrated analysis tools in general.	complex systems;embedded system;interaction;logistics;profiling (computer programming);system analysis;usability	Chaiwoo Lee;Paul T. Grogan;Olivier L. de Weck	2012	2012 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/ICSMC.2012.6377840	complex systems;user experience design;simulation;usability;human–computer interaction;computer science;user interface	SE	-61.578014840835976	-45.28886668052965	36437
8dbac4940040355f51d1b0d87cc8e10e5d39b1e1	design of image processing embedded systems using multidimensional data flow		Excellent book is always being the best friend for spending little time in your office, night time, bus, and everywhere. It will be a good way to just look, open, and read the book while in that time. As known, experience and skill don't always come with the much money to acquire them. Reading this book with the PDF design of image processing embedded systems using multidimensional data flow will let you know more things.	dataflow;embedded system;image processing;money;portable document format	Joachim Keinert;Jürgen Teich	2011		10.1007/978-1-4419-7182-1	parallel computing;computer science;theoretical computer science	Embedded	-60.62378682084737	-25.50481657401358	36482
a92e39dac5db37ae8da87a3bf3dfaf4dbd0b4061	providing timely examples improves the quantity and quality of generated ideas	creativity;examples;collective intelligence;ideation	Emerging online ideation platforms with thousands of example ideas provide an important resource for creative production. But how can ideators best use these examples to create new innovations? Recent work has suggested that not just the choice of examples, but also the timing of their delivery can impact creative outcomes. Building on existing cognitive theories of creative insight, we hypothesize that people are likely to benefit from examples when they run out of ideas. We explore two example delivery mechanisms that test this hypothesis: 1) a system that proactively provides examples when a user appears to have run out of ideas, and 2) a system that provides examples when a user explicitly requests them. Our online experiment (N=97) compared these two mechanisms against two baselines: providing no examples and automatically showing examples at a regular interval. Participants who requested examples themselves generated ideas that were rated the most novel by external evaluators. Participants who received ideas automatically when they appeared to be stuck produced the most ideas. Importantly, participants who received examples at a regular interval generated fewer ideas than participants who received no examples, suggesting that mere access to examples is not sufficient for creative inspiration. These results emphasize the importance of the timing of example delivery. Insights from this study can inform the design of collective ideation support systems that help people generate many high quality ideas.	baseline (configuration management);display resolution;theory	Pao Siangliulue;Joel Chan;Krzysztof Z. Gajos;Steven Dow	2015		10.1145/2757226.2757230	simulation;human–computer interaction;ideation;computer science;artificial intelligence;collective intelligence;creativity;management;world wide web	HCI	-59.10236407196871	-45.49579767799324	36491
5fe3139bb69cb83d7762d90fcd475fdec51d5283	human-computer interaction for bci games: usability and user experience	psychophysiological computing;brain;human computer interaction;psychology brain computer interfaces computer games human computer interaction physiology;brain computer interface;physiological computing;training;multimodal interaction human computer interaction bci game brain computer interface physiological computing psychophysiological computing;games brain human computer interaction training brain computer interfaces accuracy electroencephalography;bci game;psychology;accuracy;physiology;games;multimodal interaction;psychophysiological signals;brain computer interfaces;electroencephalography;computer games;games brain computer interfaces physiological computing psychophysiological signals affective computing multimodal interaction;affective computing	Brain-computer interfaces (BCI) come with a lot of issues, such as delays, bad recognition, long training times, and cumbersome hardware. Gamers are a large potential target group for this new interaction modality, but why would healthy subjects want to use it? BCI provides a combination of information and features that no other input modality can offer. But for general acceptance of this technology, usability and user experience will need to be taken into account when designing such systems. This paper discusses the consequences of applying knowledge from Human-Computer Interaction (HCI) to the design of BCI for games. The integration of HCI with BCI is illustrated by research examples and showcases, intended to take this promising technology out of the lab. Future research needs to move beyond feasibility tests, to prove that BCI is also applicable in realistic, real-world settings.	brain–computer interface;human–computer interaction;modality (human–computer interaction);usability;user experience	Danny Plass-Oude Bos;Boris Reuderink;Bram van de Laar;Hayrettin Gürkök;Christian Mühl;Mannes Poel;Dirk Heylen;Anton Nijholt	2010	2010 International Conference on Cyberworlds	10.1109/CW.2010.22	brain–computer interface;simulation;human–computer interaction;computer science;affective computing;multimedia	HCI	-50.62217149085334	-45.7968671993762	36515
e571169e970192b69b794c981ec83a587de7db75	gis cartography: a guide to effective map design		Only for you today! Discover your favourite gis cartography a guide to effective map design second edition book right here by downloading and getting the soft file of the book. This is not your time to traditionally go to the book stores to buy a book. Here, varieties of book collections are available to download. One of them is this gis cartography a guide to effective map design second edition as your preferred book. Getting this book b on-line in this site can be realized now by visiting the link page to download. It will be easy. Why should be here?	cartography;download;geographic information system;goto;line level;link page;online and offline	Daniel G. Cole	2010	Cartographica	10.3138/carto.45.2.159	data mining;geography;cartography	HCI	-44.364288607091616	-25.299652346457687	36550
1295c72d2856960105d4afff7b3aa7538d6fb744	companion robots for elderly people: using theatre to investigate potential users' views	geriatrics;robots senior citizens interviews educational institutions cognition human computer interaction;service robots;mobile robots;medical disorders;conference paper;handicapped aids;entertainment companion robots elderly people user views theatre production ethical aspects social aspects robot functionality residential care home audience elderly residents carers mental disabilities physical disabilities care robots social interaction;service robots entertainment ethical aspects geriatrics handicapped aids home automation medical disorders mobile robots;ethical aspects;entertainment;home automation	A theatre production of a play illustrating the functionality, social and ethical aspects of robots helping with aspects of elderly care was presented at a residential care home. The audience consisted of mainly elderly residents and carers. The residents suffered from various physical and mental disabilities which impaired their ability to provide responses through standard questionnaires. Therefore, additional structured interviews were used to gain insight into their views on the theatre scenario. Both carers and residents were generally positive towards the idea of using robots to help with care. Residents in particular stressed the desire for care robots to also provide social interaction and entertainment.	aibo;interaction;robot	Michael L. Walters;Kheng Lee Koay;Dag Sverre Syrdal;Anne Campbell;Kerstin Dautenhahn	2013	2013 IEEE RO-MAN	10.1109/ROMAN.2013.6628393	mobile robot;home automation;entertainment;simulation;computer science;multimedia;geriatrics	HCI	-55.602700555267134	-50.45850973079202	36552
1ccb038f5125687d006304fe3a8ca2ebcf3f7292	trajectory-based skill learning using generalized cylinders		In this article, we introduce Trajectory Learning using Generalized Cylinders (TLGC), a novel trajectory-based skill learning approach from human demonstrations. To model a demonstrated skill, TLGC uses a Generalized Cylinder—a geometric representation composed of an arbitrary space curve called the spine and a surface with smoothly varying cross-sections. Our approach is the first application of Generalized Cylinders to manipulation, and its geometric representation offers several key features: it identifies and extracts the implicit characteristics and boundaries of the skill by encoding the demonstration space, it supports for generation of multiple skill reproductionsmaintaining those characteristics, the constructed model can generalize the skill to unforeseen situations through trajectory editing techniques, our approach also allows for obstacle avoidance and interactive human refinement of the resulting model through kinesthetic correction. We validate our approach through a set of real-world experiments with both a Jaco 6-DOF and a Sawyer 7-DOF robotic arm.		Seyed Reza Ahmadzadeh;Sonia Chernova	2018	Front. Robotics and AI	10.3389/frobt.2018.00132	simulation;robot learning;computer science;cylinder;encoding (memory);computer vision;trajectory;robotic arm;obstacle avoidance;artificial intelligence;human–robot interaction;kinesthetic learning	Robotics	-37.04875999091049	-37.96991269599452	36582
19ace8c87e14264fde338f6fb622d2cb7a066baa	that robot touch that means so much: on the psychological effects of human-robot touch			robot	Laura Hoffmann	2017				HCI	-52.95090379154282	-50.13917490980826	36605
47d81039292450d5f191b64123899ddb8779e1ae	bringing playfulness to disabilities	smart textiles;robot companion;robot design;smart textile design;inclusive games;physical impairment;visual quality;social exchange;self esteem;modular design;mild cognitive impairment	"""This article presents the design case of a robot companion targeted at children who are prevented from playing normally, due to cognitive, developmental or physical impairments. The robot design presents some distinctive qualities. From an instrumental viewpoint it reflects inclusiveness and social exchange. It enables inclusive play activities that promote confidence and self-esteem. All children blossom as children with different abilities, including """"fully able"""" children, collaboratively achieve success, in games that are fun for all. A specific effort in the design was spent in creating consistency between the form, visual qualities, and the behaviours of the robot, in order to enable play scenarios that were specifically targeted at autistic, mild cognitively-impaired and severely motor-impaired children."""	robot	Patrizia Marti	2010		10.1145/1868914.1869046	simulation;computer science;operating system;modular design;social exchange theory	HCI	-57.22450304844122	-51.25882253411477	36624
c17f9682d5db57705bedb3e7398c7bedbe080107	from local to global illumination and back	global illumination	The following being musings about illumination problems and illumination answers, more particularly about the evolution and the interplay between local and global illumination concerns.	black hole;global illumination;graphics;illumination (image);workstation	Alain Fournier	1995		10.1007/978-3-7091-9430-0_13	computer science;global illumination	Vision	-52.330204499340184	-27.422867145205878	36678
6addf12b95161aa85f0a6b4cc352134cd103ace2	people, personal data and the built environment		Personal data is increasingly important in our lives. We use personal data to quantify our behaviour, through health apps or for 'personal branding' and we are also increasingly forced to part with our data to access services. With a proliferation of embedded sensors, the built environment is playing a key role in this developing use of data, even though this remains relatively hidden. Buildings are sites for the capture of personal data, such as ID card gateways or wifi hotspots. This data is used to adapt buildings to people's behaviour, and increasingly, organisations use this data to understand how buildings are occupied and how communities develop. This workshop will bring together a community of researchers and practitioners interested in personal informatics and the design of interactive buildings and environments to foster critical discussion on the future role of personal data in interactions with the built environment.	embedded system;hotspot (wi-fi);informatics;interaction;personal branding;personally identifiable information;quantified self;sensor	Holger Schnädelbach;Nils Jäger;Sara Nabil;Nick Dalton;David S. Kirk;Elizabeth Churchill	2017		10.1145/3064857.3064864	human–computer interaction;engineering;multimedia;built environment;architecture;informatics;personal information manager;personal information management	HCI	-56.66110985153837	-41.852797248733275	36715
3f5c45261554d7c0e2a5f06869acd5b39e69e5ea	a multimedia authoring-in-the-large environment to support complex product documentation	hand held device;incremental composition;incremental hyperlinking;multimedia authoring;hyperlink browser;authoring in the large;life cycle;user interface;product manual configuration specification;component document;automatic hyperlinking;product model;hyperlink specification;large scale;product manual composition;document database;multimedia document mailer;large scaled document browsing;aiu extraction;document transformation;form based document browsing;aiu structure specification;multimedia document annotator;network based document delivery;document evaluation;sgml conversion;card based document browsing;hyperlink management	In this paper, we present a new paradigm for multimedia document authoring to support large-scaled industrial technical documentation. An industrial-strength multimedia authoring environment requires a high degree of automation support for producing a large amount of high-quality technical documentation efficiently and effectively, and provides a consistent user interface to facilitate viewing and browsing of large-scaled technical contents. Product documentation includes technical information in all media for all aspects of a product during the life cycle of the product. Product documents are highly cross-referenced and often shared by a family of related product models. Previous authoring paradigms have their limitations in supporting such complex technical documentation required for today's sophisticated products. Our approach is based on an authoring-in-the-large paradigm by adopting formal configuration specifications for automatically assembling machine-specific product manuals from component documents, and formal hyperlink specifications for systematically creating hyperlinks in highly cross-referenced technical documents. Integrated media-specific viewers are provided to support viewing, browsing and navigation of large-scaled hyperlinked multimedia contents in a consistent manner for various product-related applications such as operation, maintenance and training on various platforms such as UNIX, PC/Windows, laptops and hand-held devices locally and over network.	hyperlink;laptop;microsoft windows;mobile device;programming paradigm;technical documentation;unix;user interface	Liang H. Hsu;Peiya Liu;Tim Dawidowsky	1999	Multimedia Tools and Applications	10.1023/A:1009643213925	biological life cycle;common source data base;computer science;operating system;technical documentation;database;multimedia;user interface;world wide web	DB	-42.07063401203432	-26.74606902922942	36745
cbece84f633b748b82558563223e90724e9ae223	learning and execution of object manipulation tasks on humanoid robots				Mirko Wächter	2018			humanoid robot;simulation;computer science	Robotics	-35.65239269731625	-39.1858225774457	36780
f5629b5bbfdafd7a62f3173cb632a382d13a1d2f	adaptcontrol: an adaptive mobile touch control for games	apple metal;compute;gpgpu mobile;nvidia cuda;google renderscript;opencl	The key aspect that defines the experience when playing a video game is the effectiveness and intuitiveness of gameplay, allowing an unexperienced player to quickly learn the main aspects of an interactive game and start playing immediately. While older games were usually simple and could be operated with 1 or 2 buttons with a quick learning curve, modern games allow a wide variety of actions that demands a more complex control scheme, sometimes with 10 or 15 buttons, resulting in unintuitive controls. Other methods of interaction, like touch, motion controls and voice, presented a more intuitive way to play games, but never reached the same level of precision found in regular controllers. To create an easier way to interact with games but at the same time, maintain the precision and quick response, delivering the best from both worlds, this work proposes the AdaptControl: a virtual controller based on an Android touchscreen device that communicates to a PC and works as a regular joystick to control a game, that can display only the amount of buttons needed for a game in a simplified interface. But this flexibility creates another challenge: the lack of physical feedback to the user. To solve this issue, the AdaptControl uses machine learnings algorithms to detect when the user is missing buttons and correct its position and size to an optimal configuration. And this kind of intelligence applied to the controller will bring another benefit: despite starting with a generic configuration for one game, the controller will be capable of changing its own layout to match each users' ergonomic need, resulting in a personal controller that matches the player's needs.	algorithm;android;human factors and ergonomics;joystick;touchscreen	Leonardo Torok;Mateus Pelegrino;Jefferson Lessa;Daniela Gorski Trevisan;Esteban Walter Gonzalez Clua	2014		10.1145/2669062.2669081	computer vision;real-time computing;simulation;computer science;artificial intelligence;operating system;machine learning;multimedia;computer graphics (images)	HCI	-45.01808033585382	-42.93681925899322	36846
ddb1346a866702b48f12dae32a381618e7efe590	cnc milling machine simulation in engineering education	engineering education;simulation;computer numerical control	In this work an effective simulator for a CNC milling machine is presented. It has been developed in EMC2, a free Opens Source NC software running in Linux environment, developed by an international community. It can be installed on a common PC and is able to: control a CNC machine; read part programs; display the tool path; send instructions to the CNC machine for the cutting process. In this work a new feature has been implemented, which can both display a 3D model of the machine and simulate all the motions of the movable parts of a real 3 axis end milling machine. This simulator lets the users not only verify the toolpath but also detect any possible collision by using the very computer which controls the milling machine. This system is very efficient and easy to use as powerful tool in Engineering education.	simulation	Ernesto LoValvo;Roberto Licari;Alessio Adornetto	2012	iJOE		embedded system;simulation;engineering education;engineering;operating system;numerical control;engineering drawing;virtual finite-state machine;manufacturing engineering	AI	-38.43421484533188	-30.468622171894907	36884
05d011c5bbad9d4326a977f33dc3649f5ac20755	relations between eye gaze and cognitive factors in a transportation task using remote control	transportation task;remote control;cognitive factor;magnetic heads;transportation robot kinematics eyes remote monitoring displays cognitive robotics human robot interaction communication system control anthropometry man machine systems;telecontrol cognition interactive devices;skill acquisition;eye gaze factor;data mining;eye movement;skill acquisition eye gaze factor cognitive factor transportation task remote control joysticks;robots;cognition;telecontrol;partial discharges;humans;vehicles;joysticks;eye gaze;cameras;interactive devices	We investigated eye movement in a transportation task using remote control. Participants controlled two sets of joysticks to move a toy excavator and a toy truck monitoring the task process through video displays. They repeated the task ten times. At an early stage of skill acquisition, the participants tended to look at both task related objects (e.g., material to transport and the shovel) and the course (e.g., ground and wall), but did not look much at obstacles (trees). At a late stage of skill acquisition, the participants tended to look at the course, other than related objects or obstacles. However, fixations to related objects did not decrease in “look ahead” fixations or fixations prior to manipulation of the joystick. It can be said that as humans acquire a skill, the positions of the related objects are learned so they did not actually look at these objects when they manipulate joystick. We came to a temporal conclusion that the participants might allocate “covert attention” or attention without gaze to these related objects and obstacles for better performance.	excavator (microarchitecture);humans;joystick;remote control	Harumi Kobayashi;Tetsuya Yasuda;Shuichi Nakata;Satoshi Suzuki	2009	RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication	10.1109/ROMAN.2009.5326225	robot;computer vision;simulation;cognition;eye tracking;computer science;artificial intelligence;joystick;dreyfus model of skill acquisition;remote control;eye movement	Robotics	-45.30342672631677	-50.00938153728787	36895
79fd3b61af88622211995e98d084924bcdb8f425	usable, aesthetic, sociable and motivating interface for students' online knowledge sharing	online knowledge sharing;motivating interface;gamification			Prasanna Ramakrisnan;Azizah Jaafar	2016		10.1007/978-3-319-39483-1_50	human–computer interaction;knowledge management;multimedia	HCI	-62.70636056709363	-37.74043084128322	36900
6e221e9fb40e74681b9ec2717c43eaaab44d1251	liquitouch: liquid as a medium for versatile tactile feedback on touch surfaces	interactive surfaces;touch;liquid;tactile feedback;water	On interactive surfaces such as touch screens, tabletops or interactive walls, the addition of active tactile feedback has been shown to greatly improve user performance and subjective evaluation of the interaction. However, common electromechanical solutions to enable tactile stimuli on flat touch displays entail the use of costly, complex and cumbersome actuator technology. This especially holds true for solutions which try to address the full complexity of our sense of touch, i.e. our ability to experience warmth, coolness, pressure, stickiness or smoothness. In this paper, we propose the use of liquid as a medium for versatile tactile feedback.  We present LiquiTouch, a first prototype which emits actively generated water jets in order to communicate state, function and material properties of virtual touchscreen elements. We discuss the design implications and illustrate the potentials of using liquid to enrich and improve the interaction with touch surfaces.	prototype;touchscreen	Hendrik Richter;Felix Manke;Moriel Seror	2013		10.1145/2460625.2460678	computer vision;simulation;engineering;communication	HCI	-44.961727366614475	-39.731786519100105	36976
6fbcaf3b718e9346fd47c2338baf47e9dcc31d32	the clutterpalette: an interactive tool for detailing indoor scenes	clutter;modeling tools;computer graphics;training;training data;computational modeling;three dimensional displays;suggestive user interfaces;solid modeling;indoor scenes;clutter three dimensional displays training computational modeling computer graphics solid modeling training data;real world scenes clutterpalette interactive tool indoor scenes small scale items;scene modeling;scene understanding;interactive 3d modeling;suggestive user interfaces interactive 3d modeling scene modeling scene understanding indoor scenes modeling tools;natural scenes indoor environment interactive systems	We introduce the Clutterpalette, an interactive tool for detailing indoor scenes with small-scale items. When the user points to a location in the scene, the Clutterpalette suggests detail items for that location. In order to present appropriate suggestions, the Clutterpalette is trained on a dataset of images of real-world scenes, annotated with support relations. Our experiments demonstrate that the adaptive suggestions presented by the Clutterpalette increase modeling speed and enhance the realism of indoor scenes.	cumulative trauma disorders;experiment;image;level design;level of detail;physical object;silo (dataset);video games;virtual studio	Lap-Fai Yu;Sai Kit Yeung;Demetri Terzopoulos	2016	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2015.2417575	computer vision;training set;scene statistics;computer science;clutter;solid modeling;computer graphics;computational model	Visualization	-37.35698987369884	-34.93350558268332	36995
50fd3eb2db5d218ee3c5e9550185b8291597adf8	design of a behavior of robot that attracts the interest of the mildly demented elderly	interaction;care and nursing home;dementia;older adult(s);robots and robotics	In this study, using the unexpected intervention overturning the interaction amount of the field and the mental model, an interaction of a robot system that enables sustained nonverbal communication with the mildly demented elderly was proposed and its effectiveness was shown in the group home of the mildly demented elderly.	mental model;mild mental retardation;nonverbal communication;robot	Misato Nihei;Natsuki Sakuma;Hiroyuki Yabe;Minoru Kamata;Takenobu Inoue	2017	Studies in health technology and informatics	10.3233/978-1-61499-798-6-492	cognitive psychology;developmental psychology;robot;psychology	HCI	-55.636321180299724	-51.87092506337323	37034
ea146683152531588dda8c7e41e8932c903f66c2	rmt: a dialog-based research methods tutor with or without a head	sistema interactivo;systeme tutoriel base dialogue;animated agent;systeme tutoriel intelligent;realite virtuelle;realidad virtual;systeme tuteurage;educational software program;virtual reality;didacticiel;research method;systeme conversationnel;user assistance;assistance utilisateur;interactive system;research method tutor;asistencia usuario;intelligent tutoring systems;tools and techniques;talking head;programa didactico;modular architecture;tutoring system	RMT (Research Methods Tutor) is a dialog-based tutoring system that has a dual role. Its modular architecture enables the interchange and evaluation of different tools and techniques for improving tutoring. In addition to its research goals, the system is intended to be integrated as a regular component of a term-long Research Methods in Psychology course. Despite the significant technical challenges, this may help reduce our knowledge gap about how such systems can help students with long-term use. In this paper, we describe the RMT architecture and give the results of an initial experiment that compared RMT’s animated agent “talking head” with a text-only version of the system.	elegant degradation;modular programming;natural language understanding;text-based user interface;type class;virtual economy;dialog	Peter M. Wiemer-Hastings;David Allbritton;Elizabeth Arnott-Hill	2004		10.1007/978-3-540-30139-4_58	simulation;human–computer interaction;computer science;artificial intelligence;virtual reality;multimedia	HCI	-53.77669213455336	-47.00200264998768	37043
9e0cc7de3f039380c3c3a1983d7f2f048e8afd62	scandinavian human-centred systems design: theoretical reflections and challenges	computability theory;interaction;research paradigm;complexity;human centredness;system design;research paradigms;participatory design;scandinavian school;design theory;theoretical foundation;communication;computer theory	Currently there is a clear trend towards questioning the traditional sovereign human self which for two hundred years has had an undisputed central status within European culture and philosophy. This challenges the tradition of anthropocentrism which in a Scandinavian computer science context has had two theoretical foundations: the workoriented design theory represented by the Scandinavian participatory design philosophy, and the idea of the computer to a rather passive medium for human communication. The process, reducing the computer to a rather passive medium for human communication. The paper firstly examines these two theoretical anthropocentric positions. Secondly it outlines the trend towards challenging the status of the human self within different research contexts. This trend represents a challenge for Human-centred systems design. Finally, it discusses the new demands for conceptualising basic IT research phenomena created by this development, with particular focus on the issue of human-centredness as a systems design strategy.	amiga reflections;computer science;systems design	Lars Qvortrup	1996	AI & SOCIETY	10.1007/BF01205280	complexity;interaction;social science;computability theory;computer science;artificial intelligence;management science;sociology;designtheory;operations research;systems design	HCI	-62.33929218158553	-30.73757480045609	37089
f75abc7192b2cba8f0b39cde9c53552ca74961bf	an overview of open-source chatbots social skills		This paper aims to analyze and compare some of the most known open source chatbot technologies focusing on their potential to model a conversational agent able to show a form of “social intelligence”. The main features and drawbacks of each system will be examined. Then, we will discuss their flexibility to produce more realistic social conversational scenarios adopting as the reference the social practice theory.	artificial intelligence;dialog system;open-source software	Agnese Augello;Manuel Gentile;Frank Dignum	2017		10.1007/978-3-319-77547-0_18	chatbot;social practice;knowledge management;social intelligence;social skills;computer science;dialog system	NLP	-51.584118780803465	-38.2766606125749	37097
6f398c80b46056fc6091336803c951140b037960	carsketch: a collaborative sketching table with self-propelled tangible objects for automotive applications		We present CarSketch, a concept and prototype of a collaborative sketching table that supports interdisciplinary development teams during the early development phase of driver assistance systems. Due to the high costs caused by the use of physical prototypes, simulation is a common approach. Yet, the operation of state-of-the-art simulations is restricted to specialists, leaving the majority of stakeholders as passive observers. Our system for a collaborative and multi-perspective communication tool enables all participants to interact with the simulation. In particular, it (1) structures the ideation and development by providing a distraction-free environment with an easy-to-use drawing interface, (2) which is used by self-propelled tangibles to monitor and influence the simulation. (3) Additional information is provided by personal augmentation and (4) the simulation can be replayed in an immersive 3D environment. We expect the tool to be useful for multidisciplinary teams in fostering the ideation phase and finding conceptual mistakes more efficiently.	personal computer;prototype;self-propelled particles;simulation	Ludwig Trotter;Christian Mai;Florian Alt	2017		10.1145/3131726.3131749	advanced driver assistance systems;engineering;human–computer interaction;immersion (virtual reality);simulation;ideation;multidisciplinary approach;automotive industry	HCI	-61.4597643427826	-39.616312922301105	37105
8a57f61500be09bfa5c4d8f4a86f4d0af1e7ae8c	affective haptics in emotional communication	analytical models;real time messaging affective haptics emotional communication online communication ifeel_im affect analysis model emotion recognition innovative haptic device;emotional communication;ifeel_im;vibrations;haptic device;real time;affective haptics;text analysis;emotion recognition;innovative haptic device;belts;online community;affect analysis model;electronic messaging;humans;real time messaging;text analysis electronic messaging emotion recognition haptic interfaces;haptic interfaces;online communication;heart beat	In the paper we are proposing a conceptually novel approach to reinforcing (intensifying) own feelings and reproducing (simulating) the emotions felt by the partner during online communication through specially designed system, iFeel_IM!. The core component, Affect Analysis Model, automatically recognizes nine emotions from text. The detected emotion is stimulated by innovative haptic devices integrated into iFeel_IM!. The implemented system can considerably enhance emotionally immersive experience of real-time messaging.	affective haptics;cognition;computer-mediated communication;haptic technology;human factors and ergonomics;online and offline;real-time transcription;simulation;usability testing	Dzmitry Tsetserukou;Alena Neviarouskaya;Helmut Prendinger;Naoki Kawakami;Susumu Tachi	2009	2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops	10.1109/ACII.2009.5349516	psychology;text mining;simulation;computer science;artificial intelligence;vibration;multimedia;haptic technology;communication	Robotics	-53.31307470663945	-47.16775833406732	37124
9255c74706fdc4fe2fb1b5c1633867ace857056d	principles, techniques, and ethics of stage magic and their application to human interface design	hci design;magician;theater;response time;time;ethics;human interface;magic;illusion;dissimulation;misdirection;anthropomorphism;principle;technique;human computer interface;characters	Magicians have been designing and presenting illusions for 5000 years. They have developed principles, techniques and ethical positions for their craft that this paper argues are applicable to the design of human/computer interfaces. The author presents a number of specific examples from magic and discusses their counterparts in human interface design, in hopes that human interface practitioners and researchers will, having recognized the applicability of magic, go further on their own to explore its domain.	user interface	Bruce Tog Tognazzini	1993		10.1145/169059.169284	ethics;human–computer interaction;computer science;principle;magic;response time;illusion;human interface device	HCI	-54.939241444901484	-36.540564878637895	37211
c5d7b6963fa9407a24253c61d470a7422188820c	children's explanations as a window into their intuitive theories of the social world	explanation;social categorization;naive sociology;intuitive theories	Social categorization is an early emerging and robust component of social cognition, yet the role that social categories play in children's understanding of the social world has remained unclear. The present studies examined children's (N = 52 four- and five-year olds) explanations of social behavior to provide a window into their intuitive theories of how social categories constrain human action. Children systematically referenced category memberships and social relationships as causal-explanatory factors for specific types of social interactions: harm among members of different categories more than harm among members of the same category. In contrast, they systematically referred to agents' mental states to explain the reverse patterns of behaviors: harm among members of the same category more than harm among members of different categories. These data suggest that children view social category memberships as playing a causal-explanatory role in constraining social interactions.	behavior;categories;categorization;category theory;causality;interaction;mental state;social cognition;social reality;explanation	Marjorie Rhodes	2014	Cognitive science	10.1111/cogs.12129	psychology;social relation;developmental psychology;social competence;social;social psychology	HCI	-59.92066536255316	-33.33278038999617	37226
c9ceded7dd6eae67b09cf146fece26ac920f2d58	effects of multi-symbols on enhancing virtual reality based collaborative	representations;virtual reality collaboration;social presence	Applying virtual reality (VR) technologies to enhance learning becomes more and more popular. This research intends to investigate how multi-symbolic representations could help users being aware of collaborative context and partner's needs to enhance completing haptics-based collaborative tasks in a co-located/distant virtual environment. This study evaluates the performance of collaboration including the completing time and the number of failure in completing a task. To make users being aware of context, multisymbolic representations in forms of color and text are provided as well as haptics and audio feedback in the virtual environment. Participants in the experiment were separated into four groups with the combinations of two variables: w/o multi-symbols and co-located/distant. The results show that multisymbols significantly helped users reduce the time in completing a task in the case of co-located collaborative virtual environment. However, there was no significant improvement in performance in the case of distant collaborative virtual environment. From our on-site observations, while operating the task collaboratively, several interesting behaviors existing between participants, such as strategy toward task success or failure, were found. First, after a few trials of completing the task in the beginning, instead of pinching and lifting the virtual cube directly toward the cone-like target, participants first push the cube and slide it to the underneath of the cone-like target, and then lifted it upward till reaching the target. Namely, participants were able to develop a good strategy with less completing time or errors therefore to complete the task more efficiently and successfully even though it was in a virtual environment. Second, the failure in completing the task was generally caused by inconsistent and incoordinate movements or force between partners. While intending to pinch the virtual cube, a balanced force output from left side and right side is required to prevent the sliding. Furthermore, to investigate how multi-symbols could affect user's perceptions, we investigate the perceived awareness, presence and social presence of our proposed system and its influence on perceived usefulness, ease of use and playfulness based on Technology Acceptance Model. The results showed that awareness, presence and social presence significantly influenced perceived usefulness, ease of use and playfulness. Therefore, our proposed multi-symbols virtual reality system has potentials to help collaborative learning.	virtual reality	Shih-Ching Yeh;Wu-Yuin Hwang;Jing-Liang Wang;Yuin-Ren Chen	2011		10.1007/978-3-642-23456-9_6	simulation;human–computer interaction;computer science;mental representation;multimedia;world wide web	Visualization	-54.43822024565096	-44.57960334320837	37337
f6177a9f5f00782eeac7a906fe92a354dbe7ab6f	exploring asynchronous online discussions through hierarchical visualisation	discussion visualisation;asynchronous online discussions;social network services;yarn navigation humans data visualization social network services blogs technological innovation sociology internet mood;yarn;pediatrics;technological innovation;customizable social visualization system;user interfaces data visualisation social sciences computing;data mining;online discussion;interactive capabilities;hierarchical visualisation;data visualisation;navigation;visualization;visual representations;internet;social visualization;visual representation;social sciences computing;slashdot org asynchronous online discussions hierarchical visualisation customizable social visualization system visual representations conversation threads interface interactive capabilities discussion visualisation;interface;data visualization;humans;communities;conversation threads;blogs;use case;mood;user interfaces;sociology;slashdot org	We introduce a highly customizable Social Visualization system for exploring online discussions through visual representations of conversation threads. The tool provides users with an interface that allows the navigation and exploration through the often intricate structure of online discussions.Apart from being useful for readers and participants of these forums, the interactive capabilities of our system makes it appealing for social researchers interested in understanding the phenomena and intrinsic structure of online conversations. We also show a use case where we applied the tool to visualize discussions from Slashdot.org,showing its capabilities to represent new, in this case Slashdot specific, metrics. The visual representation of discussion threads has arisen as a complement for supporting investigation,as it helps to understand such large amount of information and contributes to the generation of new research ideas.	online chat;slashdot	Victor Pascual-Cid;Andreas Kaltenbrunner	2009	2009 13th International Conference Information Visualisation	10.1109/IV.2009.14	human–computer interaction;computer science;multimedia;world wide web	HCI	-62.00155718340142	-39.54058548594709	37386
ae6ad43fcd6868cd6e51865acb4496f2bba009db	toward interactive timelapses		Timelapse was invented in order to record the progress of any project or phenomenon from its beginning to its completion. It is a type of video whose frames have been shot with a set time and using a photographic camera. Timelapse is continually developing and applied more and more in many fields such as education, construction, the observation of natural phenomenon, experiments etc. However, the timelapse viewer remains passive throughout the duration of the video without the possibility of intervention. The aim of this paper is to propose interactivity on timelapses and the implementation of a related application that showcases the interactive features. Interactive timelapse gives viewers the opportunity to choose the way they wish to view it. Furthermore, apart from photographic shots it also includes text, images, sound, music and video. This added information makes timelapse even more important and its field of application forever expanding. Interactive timelapse features are analyzed, some of which are included and used in this paper and the application. The subject of the application is the recently restored Thematic Gardens of Thessaloniki's New Coast. These gardens are a central attraction for the city, however, very little informational material is available regarding them. The application is directed to anyone who wishes to visit these gardens, especially students of all levels, as an accompaniment to their educational excursions and can be used before, during and after their visit. Primary evaluation results are encouraging and already inspire future work.	experiment;hypermedia;interactivity;video;wizard (software)	Filio Leventaki;Georgios D. Styliaras	2017	2017 8th International Conference on Information, Intelligence, Systems & Applications (IISA)	10.1109/IISA.2017.8316357	multimedia;interactivity;natural phenomenon;thematic map;attraction;phenomenon;wish;computer science	HCI	-49.723099964226435	-37.031971951943774	37387
800027aca5a3b56949b1f71ae89f47aaca5fb39f	introduction to artificial life	libraries;virtual machining;genetics;books;physics;rna;entropy books thermodynamics physics computer science genetics virtual machining rna libraries cd roms;thermodynamics;entropy;computer science;artificial life;cd roms	From the combination of knowledge and actions, someone can improve their skill and ability. It will lead them to live and work much better. This is why, the students, workers, or even employers should have reading habit for books. Any book will give certain knowledge to take all benefits. This is what this introduction to artificial life tells you. It will add more knowledge of you to life and work better. Try it and prove it.	artificial life;book	C. Adami	1998	IEEE Trans. Evolutionary Computation	10.1109/TEVC.1998.738989	entropy;rna;cd-rom;computer science;artificial intelligence;mathematics;algorithm;artificial life	AI	-58.83016565728271	-24.28123994971528	37393
279029b505cd437f71ae66bb246673aa1c032fda	an application of virtual reality for training and ranking operators of mobile robot		In the paper it is shown how candidates for mobile robot operators might be trained and ranked with the help of the specific game running on a computer in virtual reality. It is also shown how playing the game improves their skills and discloses their personal features.	mobile robot;virtual reality	Barbara Lukawska;Pawel Paduch;Krzysztof Sapiecha	2006	Annales UMCS, Informatica		artificial intelligence;computer science;computer vision;operator (computer programming);simulation;mobile robot;ranking;virtual reality	Robotics	-36.46254212274327	-39.20670079448686	37413
b9b6ca09aa42f13bf8d1890347d9715b87edae6a	the international and the personal: designing local interfaces for global understanding				W. Ray Webster	2004			human–computer interaction;engineering;knowledge management	HCI	-53.39730854868796	-34.30180640889502	37482
35f2ffe69a2a1562ce3b9d2c3f7163eaf8f18418	views from the ground floor		Jess Loseby is an established net and digital artist from the UK. Her primary medium is the internet. She exhibits in national and international projects both online and offline. Her work ranges from small and intimate online installations to large-scale digital projections and video. Loseby's unashamedly low-tech net installations and video build comparisons of the network and digitality into their frustrations, attention to triviality, and repetition as absurdly compatible to the female domestic routine. Themes dealing with individuality and cyber identity reoccur frequently, as do the faces of her children, who seem to be bound up irrevocably with her digital self. Jess Loseby is young(ish), has three children, one husband, and no time!	internet;jess;online and offline;video projector	Jessica Loseby	2004		10.1145/1185884.1185979		Networks	-54.9648622850005	-31.716676533387144	37484
5328b5d3375edb3ef7f997b98439beafed3a4e1f	trigger-action programming in the wild: an analysis of 200, 000 ifttt recipes	internet of things iot;ifttt;end user composition;trigger action programming;end user programming	While researchers have long investigated end-user programming using a trigger-action (if-then) model, the website IFTTT is among the first instances of this paradigm being used on a large scale. To understand what IFTTT users are creating, we scraped the 224,590 programs shared publicly on IFTTT as of September 2015 and are releasing this dataset to spur future research. We characterize aspects of these programs and the IFTTT ecosystem over time. We find a large number of users are crafting a diverse set of end-user programs---over 100,000 different users have shared programs. These programs represent a very broad array of connections that appear to fill gaps in functionality, yet users often duplicate others' programs.	ecosystem;end-user development;ifttt;programming paradigm	Blase Ur;Melwyn Pak Yong Ho;Stephen Brawner;Jiyun Lee;Sarah Mennicken;Noah Picard;Diane Schulze;Michael L. Littman	2016		10.1145/2858036.2858556	human–computer interaction;computer science;operating system;data mining;multimedia;world wide web;internet of things	Metrics	-47.28316718592083	-24.73630857658989	37488
ef77aab28c1fc44e9b089c87e9fe7be836807b70	evaluation of experiments in social robotics: insights from the monarch project		The paper discusses the assessment of humanrobot interaction (HRI) experiments in social robotics. Some of the MOnarCH project experiments are analyzed, illustrating key ideas on performance indicators based in activation rates of micro-behaviors and environment models. The consistency of the results obtained indicates that the ideas are fully applicable to other experiments in social robotics.	biasing;data acquisition;experiment;human–robot interaction;norm (social);numerical analysis;robotics;social robot;time complexity	João S. Sequeira	2017	2017 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)	10.1109/ROMAN.2017.8172282	simulation;social robot;performance indicator;computer science;robot	Robotics	-49.460444875144354	-52.046702183379594	37623
7e96bf1832572c0bea8b4b5a1e13cae2c6ed2509	visualizing time-variant sets on a handheld device	mobile application development;time series data	"""We propose a radial method for visualizing time-series data on mobile devices. We display a single """"snapshot"""" of the data at a given moment in time, and allow the user to easily navigate forwards and backwards in time to view other snapshots. Transitions from one snapshot to another are smoothly animated to preserve context. As a proof of concept of this idea, we have implemented U.S. Supreme Court, a mobile app that lets users visualize the membership of the United States Supreme Court at any date in history."""	mobile app;mobile device;radial (radio);smoothing;snapshot (computer storage);time series	Iliesa S. K. Delai;Meilan Jin;Geoffrey M. Draper	2016		10.1145/2968220.2968231	computer science;data mining;internet privacy;world wide web	HCI	-44.77733385732524	-27.903988968551168	37637
0daac4ea6fb3745d386ea815cf0d7379b2f15d42	touchplates: low-cost tactile overlays for visually impaired touch screen users	blindness;accessibility;touchplates;visual impairments;touch screens;guides;hardware	"""Adding tactile feedback to touch screens can improve their accessibility to blind users, but prior approaches to integrating tactile feedback with touch screens have either offered limited functionality or required extensive (and typically expensive) customization of the hardware. We introduce touchplates, carefully designed tactile guides that provide tactile feedback for touch screens in the form of physical guides that are overlaid on the screen and recognized by the underlying application. Unlike prior approaches to integrating tactile feedback with touch screens, touchplates are implemented with simple plastics and use standard touch screen software, making them versatile and inexpensive. Touchplates may be customized to suit individual users and applications, and may be produced on a laser cutter, 3D printer, or made by hand. We describe the design and implementation of touchplates, a """"starter kit"""" of touchplates, and feedback from a formative evaluation with 9 people with visual impairments. Touchplates provide a low-cost, adaptable, and accessible method of adding tactile feedback to touch screen interfaces."""	3d printing;accessibility;cutter expansive classification;printer (computing);touchscreen	Shaun K. Kane;Meredith Ringel Morris;Jacob O. Wobbrock	2013		10.1145/2513383.2513442	computer vision;computer hardware;computer science;accessibility;multimedia;world wide web	HCI	-45.21881874564979	-39.174584973596566	37648
c19f267a2259dce07a512300cbdd7e82b8c17487	an analysis of the potential to utilize virtual worlds to enhance edutainment and improve the wellbeing of the ageing population	social isolation;ageing;design;edutainment;well being;virtual worlds	The contribution of this paper identifies the importance of developing and designing tailored virtual places which older members of society can utilize easily and safely within this context; with a focused application of addressing social isolation, enabling wellbeing and improving edutainment. The paper also explores the complexities associated with engaging older people with virtual world's experiences alongside the potential for new commercial approaches to design and delivery edutainment and wellbeing support initiatives in 3D immersive contexts through the implementation of next generational digital services.	educational entertainment	Ann Smith;Matthew Chilcott	2013	Trans. Edutainment	10.1007/978-3-642-37042-7_4	simulation;human–computer interaction;engineering;multimedia	HCI	-61.84838520807901	-41.696209438750024	37650
8179e431aa08c62dc60f05cfc55e6fd41527c52f	conferring human action recognition skills to life-like agents	user modelling;image recognition;virtual reality;software agents;autonomous agent;human factors;information exchange;action recognition;virtual environment;computer animation;real time systems	Most of today's virtual environments are populated with some kind of autonomous lifelike agents. Such agents follow a pre-programmed sequence of behaviours that exclude the user as a participating entity in the virtual society. In order to make inhabited virtual reality an attractive place for information exchange and social interaction, we need to equip the autonomous agents with some perception and interpretation skills. In this paper we present one skill: human action recognition. By opposition to human-computer interfaces (HCI) that focus on speech or hand gestures, we propose a full-body integration of the user. We present a model of human actions along with a real-time recognition system. To cover the bilateral aspect in human-computer interfaces, we also discuss some action response issues. In particular we describe a motion management library that solves animation continuity and mixing problems. Finally we illustrate our system with two examples and discuss what we have learned.	automaton;autonomous robot;bilateral filter;cognition;computer animation;estimation of signal parameters via rotational invariance techniques;eurographics;gesture recognition;hidden markov model;humans;human–computer interaction;information exchange;interactivity;international standard book number;knuth–morris–pratt algorithm;lotus improv;markov chain;mixing (mathematics);motion capture;perlin noise;population;real-time clock;real-time computing;real-time locating system;real-time transcription;scott continuity;springer (tank);switzerland;virtual reality	Luc Emering;Ronan Boulic;Daniel Thalmann	1999	Applied Artificial Intelligence	10.1080/088395199117379	simulation;information exchange;computer science;virtual machine;artificial intelligence;autonomous agent;software agent;virtual reality;computer animation;multimedia	AI	-37.46590069988573	-38.893323156091796	37727
fb880c67f67e665fa36d6087781d7771c70a2e3c	using monetary incentives and auctions to elicit user preferences between usability and aesthetics	user preferences;monetary incentives;aesthetics;auction;usability	INTRODUCTION Recent findings suggest that, at least in certain contexts, users’ evaluations of usability and aesthetics are positively correlated. Aesthetics was also found to be correlated with overall user preferences of the interactive system. One problem in evaluating the relative importance of these system attributes is that one cannot assume that stated preferences or evaluations will automatically be translated into actual choice. Evaluating an alternative favorably does not necessarily translate to choosing that alternative over other options. For example, a laboratory experiment showed that users’ evaluations of an application’s skins were inconsistent with the skins that they eventually chose. Thus, in a sense, studies about the relative importance of aesthetics or usability can only take us so far if true preferences, i.e., commitment to action, are not revealed. We present preliminary results from an experiment that employed a competitive setting to elicit user preferences of systems that differ along usability and aesthetics lines. We offered users financial incentives based on their performance. Users then competed in an auction for systems that would allow them to maximize their profits in the ensuing tasks. Thus, users’ bids on the systems were used as the measure for their true preferences.	interactivity;skin (computing);usability;user (computing)	Tamar A. Ben-Bassat;Joachim Meyer;Noam Tractinsky	2004		10.1145/985921.986145	usability;human–computer interaction;computer science	HCI	-49.231585076421894	-47.261194627147596	37730
0d324cd02769f88fa3ae099f24db5642184a5887	a method of touching and moving virtual shadows with real shadows	3dcg;virtual reality rendering computer graphics solid modelling;three dimensional displays image color analysis light sources entertainment industry art cameras shape;interaction;kinect;3d position virtual shadow real shadow light source 3d shape;shadow;kinect interaction shadow 3dcg	In this research, we propose a system to touch and move virtual shadows with real shadows. Shadows are easily generated with physical objects and a light source, and they are also used for entertainment such as shadow play and shadow art. In our method, the system scans physical objects in front of a light source, generates virtual shadows according to the scan data, and superimposes the virtual shadows to real shadows of the physical objects. The virtual shadows can be changed interactively according to the change of the real shadows, and it would realize a new type of entertainment that uses shadows. Our system scans the 3D shapes and the 3D positions of physical objects, and simulates shadows of the physical objects projected on the screen. Thus the positions and the sizes of physical objects for shadows are not limited. Virtual shadows are synthesized and moved based on the simulated shadows, and they are superimposed to real shadows of the physical objects. The user can feel as if interacting with virtual shadows by real shadows.	interaction;interactivity	Hiroko Iwasaki;Momoko Kondo;Rei Ito;Saya Sugiura;Yuka Oba;Shinji Mizuno	2015	2015 International Conference on Cyberworlds (CW)	10.1109/CW.2015.46	computer vision;shadow;interaction;self-shadowing;computer science;machine learning;multimedia;shadow mapping;statistics;computer graphics (images)	Graphics	-42.296099199566434	-38.32825425768998	37735
21466a30b71995c7e284326ac65935a4a109ea4c	distributed hide-and-seek	play;location sensing;grandparents;hide and seek;games;children;bluetooth	Grandchildren and grandparents are often separated by distance. The decline of the extended family, the pursuit of careers, global migration, divorce and family disputes can contribute to grandchildren growing up without much contact with their grandparents. Technological advances can provide new and creative ways to bring separated grandparent and grandchildren closer. This paper reports on a technological prototype based on the traditional game of hide-and-seek that seeks to re-connect intergenerational relatives. The prototype exploits Bluetooth technologies to sense location and create a distributed hide-and-seek experience.	bluetooth;prototype;traditional game	Frank Vetere;Mark Nolan;Raihaan Abdool Raman	2006		10.1145/1228175.1228235	games;computer science	HCI	-59.29692390688084	-39.53690735696749	37758
00d249fcff0f8afd48eb111dff871e39d6e75acd	art, art history, and the computer	computational linguistic			Kenneth C. Lindsay	1966	Computers and the Humanities	10.1007/BF00138353	studio art;digital art;contemporary art	Vision	-53.264855637667075	-28.514175040076434	37762
5248f666a30e6f8c5684e774b5ee10e333d79883	a reliable non-verbal vocal input metaphor for clicking	websearch;ikz124620;i 2 6 artificial intelligence learning parameter learning;i 3 7 computer graphics three dimensional graphics and realism virtual reality;h 5 2 information interfaces and presentation user interfaces voice i o;rwth publications	While experiencing an immersive virtual environment a suitable trigger metaphor is often needed, e.g. for the interaction with objects out of physical reach or system control. The BlowClick approach [35] that is based on non-verbal vocal input has been proven to be a valuable trigger technique in previous work. However, its original detection method is vulnerable to false positives and, thus, is limited in its potential use. Therefore, we extended the existing approach by adding machine learning methods to reliably classify blowing events. We found a support vector machine with Gaussian kernel performing the best with at least the same latency and more precision than before. Furthermore, we added acoustic feedback to the NVVI trigger, which increases the user's confidence and whose absence was also stated as a limitation of the previous work. With this extended technique, we repeated the conducted Fitts' law experiment with 33 participants and could confirm that it is possible to use NVVI as a reliable trigger as part of a hands-free point-and-click interface. Furthermore, we tested reaction times to measure the trigger's performance without the influence of pointing and calculated device throughputs to ensure comparability.	adding machine;audio feedback;computer accessibility;desktop metaphor;fitts's law;machine learning;mobile device;modal logic;point and click;pointing device;support vector machine;usability testing;virtual reality	Daniel Zielasko;Neha Neha;Benjamin Weyers;Torsten Kuhlen	2017	2017 IEEE Symposium on 3D User Interfaces (3DUI)	10.1109/3DUI.2017.7893316	simulation;computer science;artificial intelligence;multimedia	Security	-46.85467584119685	-44.907967213731716	37801
d499b770b096d3a7e18be4080b38d6c95f786cf0	presenting social media information on mobile devices using multiple contexts	mobile application design;drntu engineering computer science and engineering information systems information interfaces and presentation;journal article;information organization;user interface design;social media;context	Purpose – Research has shown that when presenting large amounts of social media information on small devices, design should consider multiple contexts which include user preferences, time, location, environment and so on. It should also take into account the purpose of use, for example, the kind of tasks undertaken by users. However, little research has been done on the organization of social media information by multiple context and tasks. The paper aims to discuss these issues. Design/methodology/approach – Using tourism as a domain, the authors conducted a user evaluation study with a prototype to investigate users’ preferred ways of organizing different types of social media information based on multiple contexts. Findings – In this paper, the authors present a sequence of context types for organizing four types of social media information (recommendations, events, friends and media elements). The study revealed that users preferred to view recommendations by location and environment context, events b...	mobile device;social media	Esther Meng-Yoke Tan;Dion Hoe-Lian Goh	2015	Aslib J. Inf. Manag.	10.1108/AJIM-09-2014-0124	human–computer interaction;computer science;multimedia;world wide web	AI	-56.2294650121115	-42.88791738944735	37810
7def47b371306d04a40a9fb47bef2238342a9df4	measurement of useful field of view during ocular following response		There have been numerous studies related to useful field of view with regard to ensuring safety during activities and preventing recognition failures that can result in human error. As a result, the form of the useful field of view has been determined and methods for its measurement have been proposed. Most studies have assumed a fixed gaze, however, thus failing to consider the useful field of view during eye movement. The present research takes an experimental approach toward discovering the effects of eye movement speed and direction on useful field of view, limiting eye movement speed to 30°/s. As a result, the direction of gaze movement, increases in speed, and the direction of the recognized object with respect to the focal point cause variation in the narrowing of the useful field of view.		Kimihiro Yamanaka;Atsushi Minochi	2013		10.1007/978-3-642-39473-7_91	human–computer interaction;computer science;human error;computer vision;gaze;useful field of view;focal point;artificial intelligence;limiting;eye movement	Robotics	-45.99087473687124	-51.737496219406566	37842
ee778639fb2dd7b2480f7860677aabdcf67126ef	steward robot: emotional agent for subtle human-robot interaction	steward robot;smart home;intelligent sweet home;virtual scenario;service robots;mobile robots;human robot interaction;human robot interaction intelligent robots psychology service robots medical robotics costs smart homes medical treatment mechanical engineering senior citizens;service agent;cost effectiveness;virtual scenario steward robot emotional agent human robot interaction service agent intelligent sweet home;service robots home automation man machine systems mobile robots;point of view;emotional agent;man machine systems;home automation	In this paper, we propose a new service agent, called a steward robot, which provides inhabitants with accessible, convenient, and cost effective interfaces as an intermediate agent between the user and a smart home environment. To implement more subtle emotional reaction of the agent, we adopt a novel emotional cue, sentiment relation, and address a problem of modeling the intensity and transition of emotion words, while previous researches have mainly focused on the selection of discrete emotion words from psychological point of view. We also discuss some issues of the proposed emotional model applying a virtual scenario to our Intelligent Sweet Home	home automation;human–robot interaction;robot	Youngmin Kim;Hyong-Euk Lee;Kwang-Hyun Park;Z. Zenn Bien	2006	ROMAN 2006 - The 15th IEEE International Symposium on Robot and Human Interactive Communication	10.1109/ROMAN.2006.314428	human–robot interaction;home automation;simulation;computer science;artificial intelligence	Robotics	-50.698830382971174	-50.401308381260456	37858
1cd20ed512eda576e167e33b425ad77d5f99cb00	a journey of integration : virtuality and physicality in a computer-mediated environment			virtuality (gaming)	Susan-Jane Thomas	2004				AI	-52.9656655766504	-34.31929041873684	37889
5c9972947dfe194746a0a23423fa79f0c8f39ffb	bringing impressionism to life with neural style transfer in come swim		Neural Style Transfer is a striking, recently-developed technique that uses neural networks to artistically redraw an image in the style of a source style image. This paper explores the use of this technique in a production setting, applying Neural Style Transfer to redraw key scenes in Come Swim in the style of the impressionistic painting that inspired the film. We present a case study on how the technique can be driven within the framework of an iterative creative process to achieve a desired look and propose a mapping of the broad parameter space to a key set of creative controls. We hope this study can provide insights for others who wish to use the technique in a production setting and guide priorities for future research.	artificial neural network;iteration	Bhautik J. Joshi;Kristen Stewart;David Shapiro	2017	CoRR	10.1145/3105692.3105697	parameter space;rendering (computer graphics);machine learning;computer science;artificial neural network;artificial intelligence;wish;painting	HCI	-57.50125162912754	-30.01200005669595	37900
641d2ce3be93eaf8a06126f9d8632e4749d9de3a	collaborative task assignment on tabletop computer	direct manipulation;face to face collaboration;tabletop surface computers;task assignment	his paper proposes the use of Tabletop Computers for use in Project Management activities like Task Assignment. Task Assignment is essentially collaborative, which ideally should be done at table discussion, now-a-days happens over network on personal devices even though there is no constraint on common time and space. Face-to-face collaboration is dwindling, even though it is faster in reaching consensus, richer in terms of quality of communication and tends to be more satisfying for the group (as compared to computer-mediated)[1]. Use of a tabletop computer, which combines the productivity benefit of a computer with the social benefits of around-the-table interaction, can potentially enhance the effectiveness of such collocated Task Assignment meeting without affecting the agility or disturbing the traditional settings.	surface computer;table computer	Chayan Kumar Deb	2013		10.1145/2512349.2514908	simulation;human–computer interaction;computer science;multimedia	HCI	-54.596750001471335	-43.99033728195551	37912
5414aa582144895d6c8f8d808ecbb2b38b9f4ded	comparing mouse and magic pointing for moving target acquisition	mouse;magic pointing;pilot study;input device;video analysis;eye gaze interaction;moving target acquisition	Moving target acquisition is a challenging and manually stressful task if performed using an all-manual, pointer-based interaction technique like mouse interaction, especially if targets are small, move fast, and are visible on screen only for a limited time. The MAGIC pointing interaction approach combines the precision of manual, pointer-based interaction with the speed and little manual stress of eye pointing. In this contribution, a pilot study with twelve participants on moving target acquisition is presented using an abstract experimental task derived from a video analysis scenario. Mouse input, conservative MAGIC pointing and MAGIC button are compared considering acquisition time, error rate, and user satisfaction. Although none of the participants had used MAGIC pointing before, eight participants voted for MAGIC button being their favorite technique; participants performed with only slightly higher mean acquisition time and error rate than with the familiar mouse input. Conservative MAGIC pointing was preferred by three participants; however, mean acquisition time and error rate were significantly worse than with mouse input.	computer mouse;computer user satisfaction;data acquisition;eye tracking;interaction technique;pointer (computer programming);video content analysis	Jutta Hild;Dennis Gill;Jürgen Beyerer	2014		10.1145/2578153.2578172	computer vision;simulation;computer science;communication	HCI	-46.13092597382866	-46.03939552564413	37929
cc695b47f06c1fab24d3aeb1c623f05fcd382ac3	wristwhirl: one-handed continuous smartwatch input using wrist gestures	smartwatch;one handed interaction;smartwatch input;gestural input;continuous input	We propose and study a new input modality, WristWhirl, that uses the wrist as an always-available joystick to perform one-handed continuous input on smartwatches. We explore the influence of the wrist's bio-mechanical properties for performing gestures to interact with a smartwatch, both while standing still and walking. Through a user study, we examine the impact of performing 8 distinct gestures (4 directional marks, and 4 free-form shapes) on the stability of the watch surface. Participants were able to perform directional marks using the wrist as a joystick at an average rate of half a second and free-form shapes at an average rate of approximately 1.5secs. The free-form shapes could be recognized by a $1 gesture recognizer with an accuracy of 93.8% and by three human inspectors with an accuracy of 85%. From these results, we designed and implemented a proof-of-concept device by augmenting the watchband using an array of proximity sensors, which can be used to draw gestures with high quality. Finally, we demonstrate a number of scenarios that benefit from one-handed continuous input on smartwatches using WristWhirl.	british informatics olympiad;display resolution;finite-state machine;gesture recognition;joystick;modality (human–computer interaction);sensor;smartwatch;usability testing	Jun Gong;Xing-Dong Yang;Pourang Irani	2016		10.1145/2984511.2984563	simulation;speech recognition;computer science;operating system;smartwatch	HCI	-46.183173449044396	-44.39182423266682	38052
c8d815043fd1e642e68d7933d493f14fb600e56e	evaluation of mental workload and familiarity in human computer interaction with integrated development environments using single-channel eeg		With modern developments in sensing technology it has become possible to detect and classify brain activity into distinct states such as attention and relaxation using commercially available EEG devices. These devices provide a low-cost and minimally intrusive method to observe a subject’s cognitive load whilst interacting with a computer system, thus providing a basis for determining the overall effectiveness of the design of a computer interface. In this paper, a single-channel dry sensor EEG headset is used to record the mental effort and familiarity data of participants whilst they repeat a task eight times in either the Visual Studio or Eclipse Integrated Development Environments (IDEs). This data is used in conjunction with observed behaviour and perceived difficulties reported by the participants to suggest that human computer interaction with IDEs can be evaluated using mental effort and familiarity data retrieved by an affordable EEG headset.	assignment (computer science);eclipse;electroencephalography;headset (audio);human computer;human–computer interaction;integrated development environment;linear programming relaxation;microsoft visual studio;user interface;while	Shahin Rostami;Alex Shenfield;Stephen Sigurnjak;Oluwatoyin Fakorede	2015			computer vision;simulation;computer science;communication	HCI	-48.89062574177868	-46.453452855742555	38063
3cee33d0149681ed7046c908277677dcae7bed37	simplifying documentation of digital reconstruction processes - introducing an interactive documentation system			documentation	Jonas Bruschke;Markus Wacker	2016		10.1007/978-3-319-47647-6_12	documentation;multimedia;computer science;systems engineering	HCI	-46.93022873721133	-29.586168107497528	38090
d12e5bc9561a63aa4f01095dc62d1c13606dcf99	navigator – a talking gps receiver for the blind	interfase usuario;navegacion informacion;user interface;navigation information;systeme gps;information browsing;blind;gps system;navigational aid;user assistance;assistance utilisateur;asistencia usuario;aide navigation;interface utilisateur;ayuda navegacion;ciego;sistema gps;aveugle	  This paper discusses an new navigation support device based on talking GPS information.  	gps navigation device	Ryszard Kowalik;Stanislaw Kwasniewski	2004		10.1007/978-3-540-27817-7_65	embedded system;simulation;global positioning system;telecommunications;computer science;operating system;user interface	HCI	-49.41099370040567	-38.446600976390144	38115
7fe0a9f3a6135e06636bb777d8b14ee0d96ebc1b	symbiosis and synergy? scenarios, task analysis and reuse of hci knowledge	design process;knowledge reuse;generic model;scenario based design;task model;task analysis;task modelling	This paper follows the scenarios and task models debate by reviewing the contributions of task modelling and scenario based approaches from a cognitive perspective. A framework of cognitive affordances is introduced to discuss the merits and limitations of each approach. An extension of the modelling theme, generic task models, is proposed to augment the contribution of knowledge reuse to the design process. The paper concludes by discussing how scenario based design might complement task analysis and reuse of task based knowledge.	cognitive tutor;human–computer interaction;synergy;task analysis;theme (computing)	Alistair G. Sutcliffe	2003	Interacting with Computers	10.1016/S0953-5438(03)00002-X	design process;human–computer interaction;computer science;knowledge management;task analysis;management science	NLP	-55.50770201631883	-34.33654849156509	38159
63dc71f2dff4422fe3f902c3654321870ddfb71b	social navigation for the spoken web	social navigation;empirical analysis;mobile phone;recommendation system;recommender system;collaborative filtering;knowledge sharing;developing regions;interactive voice response	This paper describes our experiences deploying a recommender system for a mobile phone-based knowledge sharing application to farmers in rural India. Users of the system record questions and call back for answers left by other users and experts. We used collaborative filtering to derive relevant content for each user based on historical navigation patterns of the community. An empirical analysis of behavioral and interview data reveals key issues for future mobile recommender systems in developing regions of the world.	collaborative filtering;mobile phone;recommender system	Robert G. Farrell;Nitendra Rajput;Rajarshi Das;Catalina Danis;Ketki A. Dhanesha	2010		10.1145/1864708.1864780	computer science;knowledge management;collaborative filtering;machine learning;multimedia;world wide web;recommender system	HCI	-56.236993647178444	-40.88919367458598	38175
7327c085b0363f23af27994408b6d1d1d42ad60a	advances in computer graphics i (tutorials from eurographics'84 and eurographics'85 conf.)	computer graphic		computer graphics;eurographics		1986			computer graphics (images);computer graphics;graphics software;computer science	Graphics	-47.45685256164466	-29.391400468459636	38189
094215475c64211483cec1f0a37e89744ef532b6	design of exhibition contents using swipe gesture recognition communication based on kinect	kinect swipe gesture depth information exhibition content design swipe gesture recognition communication touch panel user convenience depth information image color information hand movements kiosk;gesture recognition educational institutions sensors object detection flowcharts feature extraction shape;sensors;shape;image sensors gesture recognition image colour analysis;feature extraction;flowcharts;gesture recognition;object detection;3 dimensional coordinates exhibition contents kinect gesture recognition depth information	Exhibition content has been used a touch panel for the user's convenience. Touch panel are not good recognition. And errors may occur when you use a long time. In addition there occurs a malfunction due to contamination. In the present study, such problems want to improve. We designed the Exhibition contents to recognize the swipe gesture of Kinect. Gesture recognition is constructed in three steps of Modeling, analysis, and recognition. Such gesture recognition was implemented using Kinect. Kinect provides Depth information of the image, Color Information and dimensional coordinates of your joints. Using this information in Were aware of the Hand movements. The KIOSK was controlled by use in 4 kind(up, down, left, and right) swipe gestures In order to verify the performance of the Exhibition contents applying the Kinect swipe gesture depth information and RGB image were used. And, verified the performance of the swipe gesture recognition and exhibition contents operations.	gesture recognition;kinect;touchscreen	Yi-Yoen Kim;Min-Woo Lee;Joo-Yong Park;Soon-Ho Jung;Kyung-Hoon Kim;Jae-Sang Cha	2015	2015 International Conference on Information Networking (ICOIN)	10.1109/ICOIN.2015.7057909	computer vision;flowchart;feature extraction;shape;computer science;sensor;gesture recognition;multimedia	Robotics	-38.68255084885974	-43.372017087204604	38243
dfc867943419d695570a5fc9bbafc5641d08bd35	playful cognitive behavioral therapy apps: design concepts and tactics for engaging young patients	concept driven research;cognitive behavioral therapy;game design;ehealth;research through design;telepsychiatry;interaction design	"""As smart solutions for healthcare (eHealth) are becoming increasingly widespread, apps and other digital devices may effectively complement various forms of psychotherapy. We point at children and adolescents in psychological therapy as a yet-underserved public for similar solutions. Moreover, a shared design sensibility between interaction designers, game designers, and therapists is still lacking. The Games 4 Therapy initiative was launched to address this problem space through practical design explorations. We illustrate its design research agenda, we call for more attention to children and adolescents as important recipients of digitally-mediated psychological therapies, and we offer actionable concepts and game design tactics for interaction designers and psychotherapists. Finally, we discuss our findings by """"thinking through"""" a selection of conceptual design explorations, pointing at the characteristics and tactics we identified in our sketches."""	interaction design;problem domain	Gabriele Ferri;Wouter Sluis-Thiescheffer;Dries Booten;Ben A. M. Schouten	2016		10.1145/2930674.2930698	psychology;simulation;social psychology;clinical psychology	HCI	-61.24810431078693	-42.83741075719703	38309
f83508d92fb7ae2183b74b15f4f9629cfa503237	explorations in multiparty casual social talk and its relevance for social human machine dialogue		Much talk between humans is face-to-face, casual, multiparty, and of indefinite duration. Such casual conversation or social talk facilitates social bonding and mutual co-presence rather than strictly being used to exchange information in order to complete well-defined practical tasks. Artificial partners capable of participating as a speaker or listener in such talk would be useful for companionship, educational, and social contexts. However, to adequately model social talk, such applications require dialogue structure beyond simple question/answer routines. While there is a body of theory on multiparty casual talk, there is a lack of quantitative work in the area. Our work focuses on the anatomy of casual talk, in particular phases of chat, highly interactive dialogue exchanges, and chunks, longer contributions from single participants in the dialogue. We outline the current knowledge on the structure of casual talk and describe our investigations in this domain. Our research finds that distributions of the durations of chat and chunk phases vary with chat being shorter than chunk phases. Chat is also more common at the start of conversations, with chunks becoming more prominent as the conversation progresses. Laughter and overlap are more common in chat phases than chunk phases. We discuss how these insights can inform the design and implementation of truly social machine dialogue partners.	dialog system;dyadic transformation;relevance;speech technology;text corpus;time complexity	Emer Gilmartin;Benjamin R. Cowan;Carl Vogel;Nick Campbell	2018	Journal on Multimodal User Interfaces	10.1007/s12193-018-0274-2	conversation;human–computer interaction;interpersonal relationship;human–machine system;casual;computer science;laughter	NLP	-52.51408260979351	-47.94789231205117	38325
aa678f211278f168131b46146afe208d428fd226	an augmented reality nanomanipulator for learning nanophysics: the “nanolearner”  platform	multisensorial interfaces;virtual reality;nanomanipulator;nanophysics education;haptic interaction	"""The work presented in this paper focuses on the description and evaluation of an augmented reality nanomanipulator, called """"nanoLearner platform"""" used as educational tool in practical works of nanophysics. Through virtual reality associated to multisensory renderings, students are immersed in the nanoworld where they can interact in real time with a sample surface or an object, using their senses as hearing, seeing and touching. The role of each sensorial rendering in the understanding and control of the """"approach-retract"""" interaction has been determined thanks to statistical studies obtained during practicals. Conjointly, qualitative analyses of student reports revealed the NanoLearner platform efficiency, compared to the use of classical tools. Finally, we present extension of the use of this innovative tool for investigating nano effects in living organisms and for allowing grand public to have access to a natural understanding of nanophenomena."""	augmented reality;gnu nano;virtual reality	Florence Marchi;Sylvain Marlière;Jean-Loup Florens;Annie Luciani;Joël Chevrier	2008	2008 International Conference on Cyberworlds	10.1007/978-3-642-14484-4_14	simulation;human–computer interaction;engineering;multimedia	Visualization	-54.03772094309874	-35.320884534360005	38351
2d0a483002de65350020d9880997e9bc6160c6e8	building better systems for learning and training: bringing the entertainment industry and simulation technology together	virtual reality;southern california;artificial intelligent;leadership development;institute for creative technologies	  In 1999, at the University of Southern California the Institute for Creative Technologies (ICT) was established. The ICT was  intended to explore a question: what would happen if researchers who understood the technology of simulation and virtual reality  worked in close collaboration with people from the entertainment industry who understood how to create compelling stories  and engaging characters? What synergies would emerge? Would it be possible to create much more immersive simulation systems  for training and learning? In the brief period since the opening of the ICT, we are starting to see the answers to these questions  and understand the promise of this approach. In this keynote talk, I will describe some of the insights that have emerged  from this collaboration, the major research efforts we have undertaken in areas such as graphics, artificial intelligence  and sound, and the integrating virtual reality applications we have produced in areas such as training and leadership development.    	simulation	William R. Swartout	2004		10.1007/978-3-540-28643-1_3	simulation;engineering;knowledge management;multimedia	HCI	-61.516097811080016	-32.848339515350034	38359
7202fd1622e165b21df19bd454203a3df0867e77	blogalpha: home automation robot using ontology in home environment	blog;robot;ontology;home automation	BlogAlpha is a home automation system that uses a home automation robot ApriAlpha to integrate the legacy appliances in a home. It provides a blog interface to receive the user’s request remotely in a natural language and show the state of the home. A robot works as intelligent glue that connects and automates legacy appliances, allowing users to introduce an intelligent home environment in their home at much lower cost. BlogAlpha uses the ontologies about the commodities in a home, the locations where these are placed and the tasks the robot can achieve. By using these ontologies, the robot can achieve the proper action to respond to a wide variety of user requests.	blog;home automation;natural language;ontology (information science);robot;web ontology language	K. Cho;Takahiro Kawamura	2007			home automation;simulation;engineering;social robot;multimedia;world wide web	Robotics	-34.45344233392593	-41.25116149562168	38361
1ccdf81cdbe80ec74733315c10cab1f1ae7e8550	imposing constraints on fragmented body motion for synthesis	fuzzy c mean;hdri;motion capture;human motion;computer game	The generation of realistic motion of human figures for computer games and animation has been under active research in recent years. Since the nature of human motion is complex, motion capture is commonly used to obtain high-quality motion. Although capturing motions directly from human can acquire life-like motion, these motions cannot be easily reused. In order to maximize usage of captured motions, existing approaches attempt to reuse by combining motions of specific parts of the body [Ikemoto and Forsyth 2004] or by splicing actions with locomotion [Heck et al. 2006] to create new motions. While these approaches explore conditions for combining specific body parts to generate convincing full body motions, we lack a generalized representation for animators to specify the conditions under which motions from which body parts can be synthesized to produce convincing full body motions. With a generalized representation, it is possible for animators to make use of a motion library of body parts to synthesize a wide variety of motions.	forsyth–edwards notation;kinesiology;motion capture;pc game	William Wai-Lam Ng;Clifford Sze-Tsan Choy	2007		10.1145/1280720.1280816	computer vision;motion capture;simulation;computer science;computer graphics (images)	Graphics	-39.281521691739094	-35.6534146908587	38383
706021ac625faca0703e920353f43b20f3819250	graphical behaviors and animated agents			computer animation;graphical user interface	Norman I. Badler	1993	Comput. Graph. Forum		theoretical computer science;human–computer interaction;computer science	HCI	-42.395147657790794	-32.08502911620842	38398
878505cfd65f10a149df82ed1eae31af96ae303a	using bio-electrical signals to influence the social behaviours of domesticated robots	computers;stress;software;brain computer interaction bio electrical signals domesticated robot social behaviours computer devices electro corticographic signals skin biopotential facial muscle tension domestic robot direct motion command emotional stress emotional parameter robot behaviour human stress;skin;emotional instrument robots bio electric signals emotional control ocz nia irobot roomba;skin bioelectric potentials brain computer interfaces human factors human robot interaction motion control psychology service robots;irobot roomba;robots stress muscles software computers humans skin;bio electric signals;robots;ocz nia;humans;social behaviour;emotional control;emotional instrument;muscles	Several emerging computer devices read bio-electrical signals (e.g., electro-corticographic signals, skin biopotential or facial muscle tension) and translate them into computer- understandable input. We investigated how one low-cost commercially-available device could be used to control a domestic robot. First, we used the device to issue direct motion commands; while we could control the device somewhat, it proved difficult to do reliably. Second, we interpreted one class of signals as suggestive of emotional stress, and used that as an emotional parameter to influence (but not directly control) robot behaviour. In this case, the robot would react to human stress by staying out of the person's way. Our work suggests that affecting behaviour may be a reasonable way to leverage such devices.	british informatics olympiad;domestic robot	Paul Saulnier;Ehud Sharlin;Saul Greenberg	2009	2009 4th ACM/IEEE International Conference on Human-Robot Interaction (HRI)	10.1145/1514095.1514167	robot;simulation;social behavior;computer science;artificial intelligence;skin;stress;neural impulse actuator	Robotics	-50.53606189394671	-49.52988713171349	38422
678a879461b8fbe4661e9abcbb6710f41c0d3d88	automatic speed adjustment for travel through immersive virtual environments based on viewpoint quality	manuals;cognitive resources automatic speed adjustment immersive virtual environments viewpoint quality indoor scenes;h 5 1 information interfaces and presentation multimedia information systems artificial;geometry;ikz080012;virtual reality;virtual environments;websearch;augmented;indexes;visualization;smoothing methods;i 3 6 computer graphics methodology and techniques interaction technqiues h 5 1 information interfaces and presentation multimedia information systems artificial augmented and virtual realities;and virtual realities;manuals indexes optical buffering geometry smoothing methods virtual environments visualization;i 3 6 computer graphics methodology and techniques interaction technqiues;optical buffering;rwth publications	When traveling virtually through large scenes, long distances and different detail densities render fixed movement speeds impractical. However, to manually adjust the travel speed, users have to control an additional parameter, which may be uncomfortable and requires cognitive effort. Although automatic speed adjustment techniques exist, many of them can be problematic in indoor scenes. Therefore, we propose to automatically adjust travel speed based on viewpoint quality, originally a measure of the informativeness of a viewpoint. In a user study, we show that our technique is easy to use, allowing users to reach targets faster and use less cognitive resources than when choosing their speed manually.	algorithm;automatic taxonomy construction;usability testing;virtual reality	Sebastian Freitag;Benjamin Weyers;Torsten Kuhlen	2016	2016 IEEE Symposium on 3D User Interfaces (3DUI)	10.1109/3DUI.2016.7460033	simulation;computer science;multimedia;computer graphics (images)	Visualization	-43.144370800014016	-47.594578950364074	38425
a4dbfe342af24e7c15f215456458344b894381f9	the development of conversational agent based interface			agent-based model;dialog system	Inese Vira;Andrejs Vasiljevs	2014		10.3233/978-1-61499-442-8-46	human–computer interaction;dialog system;computer science	HCI	-50.593096412394104	-34.39445781014652	38433
dcc5f106cdf2b650c8cc7cae561c58e6613981e2	understanding meeting capture and access	longitudinal study;meeting capture and access;research agenda;capture and access;ubiquitous computing;longitudinal studies;distributed work;work practice	Meeting capture has been a common subject of research in the ubiquitous computing community for the past decade. However, the majority of the research has focused on technologies to support the capture but not enough on the motivation for accessing the captured record and the impact on everyday work practices based on extended authentic use of a working capture and access system. Our long-term research agenda is to build capture services for distributed workgroups that provide appropriate motivation and to further understand how access of captured meetings impacts work practices. To do this, we have developed a testbed for meeting capture as part of a larger distributed work system called TeamSpace. We will put this system into real use in a variety of settings.	testbed;ubiquitous computing;work systems	Heather Richter Lipford	2002		10.1145/506443.506480	simulation;human–computer interaction;computer science;knowledge management;ubiquitous computing	HCI	-59.824334200955576	-40.86140813244902	38492
6b7ad546c74868e22c0991664b2ecd1111fab82d	experiments in mobile spatial audio-conferencing: key-based and gesture-based interaction	mobile hci;gesture interaction;mobile phone;support group;multimodal interaction;spatial audio	In this paper we describe an exploration into the usability of spatial sound and multimodal interaction techniques for a mobile phone conferencing application. We compared traditional keypad based-interaction to that of a newer approach using the phone itself as a device to navigate within a virtual spatial auditory environment. While the traditional keypad interaction proved to be more straightforward to use, there was no significant impact on task completion times or number of interaction movements made between the techniques. Overall, users felt that the spatial audio application supported group awareness while aiding peripheral task monitoring. They also felt it aided the feeling of social connectedness and offered enhanced support for communication.	interaction technique;mobile phone;multimodal interaction;peripheral;surround sound;usability	Christina Dicke;Shaleen Deo;Mark Billinghurst;Nathan B P Adams;Juha Lehikoinen	2008		10.1145/1409240.1409251	human–computer interaction;computer science;multimodal interaction;multimedia	HCI	-47.43647388476774	-43.12300934519299	38500
b6335d99592a09e5e8da9203bdacd4c6e1fc7e4e	living book of anatomy (lba) project: see your insides in motion!	embodiment;real time;motion capture;anatomy learning;augmented human;augmented reality ar	The complexity of human anatomy makes learning and understanding it a difficult task. Using the Embodiment Theory as foundation we present the living book of anatomy project, an AR system for teaching anatomy, specifically anatomy in motion.	ar (unix);bmc remedy action request system	Armelle Bauer;Ali-Hamadi Dicko;Olivier Palombi;François Faure;Jocelyne Troccaz	2015		10.1145/2818466.2818470	real-time computing;motion capture;simulation;computer science;multimedia;computer graphics (images)	Robotics	-49.088661643439345	-31.760340770479782	38502
00b692a2ffe40b0d1b702bd9f6fafdafd6f80bb9	authoring transformations by direct manipulation for adaptable multimedia presentations	incremental transformations;document model;multimedia;xslt;direct manipulation;xml;world wide web;multimedia presentation;authoring tool;authoring tools	In this paper, we present a method for authoring generic and adaptable multimedia presentations. This method relies on document transformations. For the currently available tools, designing the XML content and the transformation sheets is a tedious and error prone experience. We propose a framework based on an incremental transformation process. Incremental transformation processors represent a better alternative to help in the design of both the content and the transformation sheets. We believe that such authoring tools are a first step toward fully interactive transformation-based authoring environments. In this paper, we focus on the authoring of transformation sheets by direct manipulation. In particular, we study the authoring of transformations for the XSLT language defined at the World Wide Web Consortium.	central processing unit;cognitive dimensions of notations;consortium;direct manipulation interface;world wide web;xml;xslt	Lionel Villard	2001		10.1145/502187.502206	xml;xslt;computer science;database;multimedia;programming language;world wide web	DB	-42.111894920675454	-26.800268526955108	38505
a74a78340f4df3791e7e57bc3a08d8960ed86487	using augmented reality in urban context: georeferenced system for business localization using google glass	wearable computers augmented reality geographic information systems user interfaces;5g mobile communication;yttrium;usability test augmented reality google glass micro interactions georeferenced systems;5g mobile communication yttrium;usability test augmented reality georeferenced system business localization user interaction google glass explorer community microinteraction based interface wearable gadget	Developing new paradigms of user interaction is always challenging. The introduction of the Google Glass platform presents a novel way to deliver content to users. Clearly, the Glass platform is not going to become a mainstream consumer electronics product as it is; however it was an experimental program from which important practical lessons can be learned. We, as part of the Google Glass Explorer Community, present this study as a contribution to the practical understanding of products that can be core for the development of micro-interaction-based interfaces for wearable gadgets in urban contexts. Throughout this paper we detail the development process of this kind of application by focusing on the challenges presented, the implementation and design decisions, and the usability tests we performed. The main results were that the use of the app is intuitive in general, but the users have problems identifying several components that were adapted for the size of the screen and the concept of the device.	augmented reality;glass;usability;wearable computer	Leonardo Ferrer;Jesus Garcia-Mancilla;Victor M. Gonzalez;Santiago Bermúdez;Pedro Bleier;Carlos Prieto	2015	2015 IEEE First International Smart Cities Conference (ISC2)	10.1109/ISC2.2015.7366157	smartglasses;augmented reality;computer-mediated reality;human–computer interaction;engineering;multimedia;world wide web	Visualization	-48.39824016672648	-41.29778893933463	38527
04473a84ce5d07853522be820258636c24bdb3c5	position tracking for virtual reality using commodity wifi		Today, experiencing virtual reality (VR) is a cumbersome experience which either requires dedicated infrastructure like infrared cameras to track the headset and hand-motion controllers (e.g. Oculus Rift, HTC Vive), or provides only 3-DoF (Degrees of Freedom) tracking which severely limits the user experience (e.g. Samsung Gear VR). To truly enable VR everywhere, we need position tracking to be available as a ubiquitous service. This paper describes WiCapture, a novel approach which leverages commodity WiFi infrastructure, which is ubiquitous today, for tracking purposes. We prototyped WiCapture using off-the-shelf WiFi radios and demonstrated that it achieves an accuracy of 0.88 cm compared to sophisticated infrared-based tracking systems like the Oculus Rift, while providing much higher range, resistance to occlusion, ubiquity and ease of deployment.	htc vive;headset (audio);motion controller;samsung gear vr;software deployment;tracking system;user experience;virtual reality	Manikanta Kotaru;Sachin Katti	2018		10.1145/3264877.3264882	software deployment;headphones;wireless;headset;tracking system;computer science;user experience design;virtual reality;commodity;embedded system	Vision	-43.08327419368716	-41.82150282331968	38529
28191d73d872f4472090142c6ba2d26e717b42e0	an end-to-end echronicling system for mobile human surveillance	biomedical monitoring;sensor technology;sensor systems;information representation tool;surveillance information retrieval mobile computing sensor fusion;surveillance;information retrieval;speech processing;speech analysis;mobile computer;wearable sensors;humans surveillance speech analysis navigation wearable sensors information analysis sensor systems biomedical monitoring speech processing information retrieval;multi dimensional;navigation;multisensory data;information processing;humans;sensor fusion;multisensory data end to end electronic chronicling system mobile human surveillance mobile computing device sensor technology information processing tool information representation tool;end to end electronic chronicling system;system architecture;mobile computing;information processing tool;information analysis;mobile human surveillance;mobile computing device	Rapid advances in mobile computing devices and sensor technologies are enabling the capture of unprecedented volumes of data by individuals involved in field operations in a variety of applications. As capture becomes ever more rich and pervasive the biggest challenge is in developing information processing and representation tools that maximize the utility of the captured multi-sensory data. The right tools hold the promise of converting captured data into actionable intelligence resulting in improved memory, enhanced situational understanding, and more efficient execution of operations. These tools need to be at least as rich and diverse as the sensors used for capture, and need to be unified within an effective system architecture. This paper presents our initial attempt at such a system and architecture that combines several emerging sensor technologies, state of the art analytic engines, and multi-dimensional navigation tools, into an end-to-end electronic chronicling solution for mobile surveillance by humans.	end-to-end encryption;information processing;mobile computing;pervasive informatics;sensor;systems architecture	Gopal Sarma Pingali;Ying-li Tian;Shahram Ebadollahi;Jason W. Pelecanos;Mark Podlaseck;Harry Stavropoulos	2007	2007 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2007.383527	embedded system;computer vision;navigation;simulation;computer science;sensor;sensor fusion;data analysis;mobile computing	Vision	-37.121277296965815	-42.02811421512116	38571
36ec5ce3a7bbb3d873b2ce1dbd32de57507d39e9	semi-autonomous virtual valet parking	older driver;parking;remote driving;user interaction;driver vehicle interaction	Despite regulations specifying parking spots that support wheelchair vans, it is not uncommon for end users to encounter problems with clearance for van ramps. Even if a driver elects to park in the far reaches of a parking lot as a precautionary measure, there is no guarantee that the spot next to their van will be empty when they return. Likewise, the prevalence of older drivers who experience significant difficulty with ingress and egress from vehicles is nontrivial and the ability to fully open a car door is important. This work describes a method and user interaction for low cost, short-range parking without a driver in car. This will enable ingress/egress without the doors being blocked by neighboring cars.	autonomous robot;device driver;egress filtering;semiconductor industry;wheelchair accessible van	Arne Suppé;Luis E. Navarro-Serment;Aaron Steinfeld	2010		10.1145/1969773.1969798	simulation;engineering;parking guidance and information;transport engineering;computer security	HCI	-50.282459717354165	-42.02656529052108	38610
366e5a13234bf58c83945550f3c7b7d473d9df1f	integration of haptics with web3d using the sai	parametric functions;implicit functions;force feedback;3d web visualization;x3d;physical properties;function based shape modeling;haptic interaction;human computer interface;open source;collaborative platform;virtual worlds	Haptics force-feedback technology is fast becoming a consumer product and no longer only found in research laboratories. The emergence of the budget Falcon device (Novint Technologies, Inc., USA) represents a key step in the dissemination of haptics technology as it offers this functionality to home users, in particular to games players. Haptics has the potential to revolutionise the Human Computer Interface if novel and creative software solutions can be found to utilise it. Currently developing for haptics requires low level programming knowledge, which is often a barrier to uptake. This paper looks at how haptics support can be integrated into an X3D authored virtual world using an open source haptics library via the Scene Authoring Interface (SAI). We supply a partial implementation of a Java wrapping to the HAPI open-source haptics library and provide a demonstration of its use within the Xj3D browser through SAI. This work is intended to contribute to a possible future haptics extension of the ISO X3D standard.	emergence;falcon;haptic technology;human computer;human–computer interaction;java;low-level programming language;novint technologies;open-source software;virtual world;web3d;wrapping (graphics);x3d	Liam Kurmos;Nigel W. John;Jonathan C. Roberts	2009		10.1145/1559764.1559768	simulation;human–computer interaction;computer science;multimedia	HCI	-42.7244296361289	-33.04141228154461	38612
c1ed5d5f2764cffc2b60ee43d55dadca3d76168e	swiss-cheese extended: an object recognition method for ubiquitous interfaces based on capacitive proximity sensing	3d interaction;object recognition;ubiquitous interfaces;object tracking;capacitive sensing;capacitive proximity sensing	Swiss-Cheese Extended proposes a novel real-time method for recognizing objects with capacitive proximity sensors. Applying this technique to ubiquitous user interfaces, it is possible to detect the 3D-position of multiple human hands in different configurations above a surface that is equipped with a small number of sensors. The retrieved object configurations can significantly improve a user's interaction experience or an application's execution context, for example by detecting multi-hand zoom and rotation gestures or recognizing a grasping hand. We emphasize the broad applicability of the proposed method with a study of a multi-hand gesture recognition device.	gesture recognition;outline of object recognition;real-time clock;sensor;user interface	Tobias Alexander Große-Puppendahl;Andreas Braun;Felix Kamieth;Arjan Kuijper	2013		10.1145/2470654.2466186	embedded system;computer vision;computer science;cognitive neuroscience of visual object recognition;video tracking;capacitive sensing	HCI	-43.711878337524055	-42.77904317120404	38649
4fc2cd6e9103388c3914f456f7144ef48533ddb8	shared design space: sketching ideas using digital pens and a large augmented tabletop setup	interfase usuario;proceso concepcion;design process;conference contributions published;realite virtuelle;realidad virtual;modelo autorregresivo;user interface;virtual reality;information space;design space;autoregressive model;preparacion serie fabricacion;realite augmentee;realidad aumentada;analisis regresion;analyse regression;interface utilisateur;regression analysis;process planning;ingenierie simultanee;ingenieria simultanea;augmented reality;modele autoregressif;preparation gamme fabrication;concurrent engineering;processus conception	Collaborative Augmented Reality (AR) setups are becoming increasingly popular. We have developed a collaborative tabletop environment that is designed for brainstorming and discussion meetings. Using a digital pen, participants can annotate not only virtual paper, but also real printouts. By integrating both forms of physical and digital paper, we combine virtual and real 2d drawings, and digital data which are overlaid into a single information space. In this paper, we describe why we have integrated these devices together in a unique way and how they can be used efficiently during a design process.	augmented reality;digital data;digital paper;digital pen	Michael J Haller;Peter Brandl;Daniel Leithinger;Jakob Leitner;Thomas Seifried;Mark Billinghurst	2006		10.1007/11941354_20	augmented reality;simulation;design process;human–computer interaction;computer science;virtual reality;autoregressive model;user interface;regression analysis;statistics;concurrent engineering;computer graphics (images)	HCI	-35.43011577049713	-27.592214961059057	38652
4f5180e71681b423ebde8482b7d2b92d3df40909	evaluation and prediction of evoked emotions induced by image manipulations	image manipulation;machine learning;emotion;evaluation;crowdsourcing	Various image editing tools make our pictures more attractive, and at the same time, evoke different emotional responses. With powerful and easy-to-use imaging applications, capturing, editing and then sharing pictures have become daily life for many. This paper investigates the influence of several image manipulations on evoked emotions for different types of images. To do so, various types of images clustered in different categories, were collected from Instagram and subjective evaluations were conducted via crowdsourcing to gather the emotional responses on different manipulations as perceived by subjects. Evaluation results show that certain image manipulations can induce different evoked emotions on transformed pictures when compared to the original ones. However, such changes in image emotions due to manipulation are highly content dependent. Then, we conducted a machine learning based experiment, in attempt to predict the emotions of a manipulated image given its original version and the desired manipulation method. Experimental results present a promising performance of such a prediction model, which could pave the road to automatic selection or recommendation of image editing tools that can efficiently transform or emphasize desired emotions in pictures. Introduction Thanks to wide spread popularity of smart mobile devices with high-resolution cameras, as well as user-friendly imaging and social networking applications, taking pictures, then editing and sharing, have become part of everyday life for many. Photo sharing has been used as a way to share not only stories but also current moods with friends, family and public at large. Modern photo sharing applications equipped with advanced and easy-touse image editing tools, such as Instagram, provide consumers with very convenient solutions to make their pictures more attractive, and more importantly, to arouse stronger emotional resonances. Different types of image content generate different emotions. Using different photographic techniques, visual filters or editing tools, pictures of the same scene can also evoke different emotions. Motivated by these facts, we attempt to change an original picture’s evoked emotion and transform it to new emotions (stronger, weaker, or completely different) by image manipulation. To achieve this goal, we first need to understand the emotional responses evoked by different image manipulations when applied to pictures. This paper investigates the influence of image manipulations on evoked emotions, and tries to find the potential pattern between image manipulation and generated emotions. To do so, we conducted subjective experiments based on online crowdsourcing. Different types of images were collected from Instagram, and manipulated by a number of typical image editing tools. Crowdsourcing subjects were then exposed to each, and questioned regarding the emotions pictures induced on them. Using the crowdsourced data as groundtruth, we trained and evaluated a model based on machine learning for predicting evoked emotions, taking an original image and desired manipulation as input. The rest of the paper is structured as follows. The next section introduces the related works by other researchers, followed by a section describing the data collection and user study. Then we analyze and interpret emotional responses obtained from subjects, and report the experiments of emotion prediction upon image manipulation in the followed two sections. Finally, the last section concludes the paper and discusses future work. Prior Work Image aesthetic quality estimation, emotion recognition and classification have been largely studied in the field of computer vision [1, 2, 3, 4, 5]. Most previous works use image features for affective image classification and emotion prediction [2, 3, 6, 7, 5]. Such features include color, texture, composition, edge and semantic information. A few researchers have worked on transforming image emotions by editing images. In [8], Wang et al. associate color themes with emotion keywords depending on art theory and transform the color theme of an input image to the desired one. However, in their work, only a few cartoon-like images are used. Peng et al. [9] propose a framework to change an image’s emotion by randomly sampling from a set of possible target images, but only show a few examples. Jun et al. [10] show that changing brightness and contrast of an image can affect the pleasure and excitement felt by observers. However, only a limited variation of an input image can be produced by changing the two features. Peng et al. [11] change the color tone and texture related features of an image to transfer the evoked emotion distribution, with experiments conducted on only limited types of image content. Evaluating image’s evoked emotions after image manipulation is not a trivial task. Many well-established image manipulation and editing tools have been widely used in online photo sharing and social networks, as ways for users to enhance their image content either to draw better attention or to evoke stronger emotions. Popular image editing tools include image enhancement [12], grayscale conversion, vintage processing, cartoonizing [13], and more recently addition of stickers1 [14]. However, most image manipulation methods have been studied merely from the perspective of image processing and not so much on their emotional impact. 1https://www.facebook.com/help/1597631423793468 (a) Original (b) Cartoon (c) Emoji (d) Enhance (e) Halo (f) Gray (g) Grunge (h) Old paper Figure 1. Example image manipulated by different methods. Several affective image databases have been created in previous works, including artistic photos or abstract paintings used in [2], International Affective Picture System (IAPS) [15], The Geneva affective picture database (GAPED) [16] and Emotion6 [11]. In our research, we are more interested in the emotions of everyday photographs, especially those images that are widely shared by online users. Unfortunately, most existing affective image datasets contain either extremely emotional images, or images without much natural high-level semantic features like human face. All those types of images do not fit our requirements. Therefore we decided to collect our own dataset using Instagram, one of the most popular online photo sharing services. To measure emotions, different types of models have been designed by psychologists. One of the most popular is the valence-arousal (VA) model (proposed by Russell [17]), characterizing emotions in two dimensions, where valence measures attractiveness in a scale from positive to negative, while arousal indicates the degree of excitement or stimulation. In terms of categorization of emotions, Ekman’s six basic emotions (anger, disgust, fear, joy, sadness and surprise) [18] are widely known. In our work, we used both models similar to works in [16, 11]. Image Dataset and User Study This section describes in detail the image dataset creation and crowdsourcing experiment. Image Collection and Processing We collected images from Instagram. According to a previous study by Hu et al. [19], images shared within Instagram can be classified into the following eight basic categories in terms of their content: Friends, Food, Gadget, Captioned photo, Pet, Activity, Selfie and Fashion. Therefore, we collected image dataset by searching for the eight category keywords or their synonyms via Instagram #tag. This was mainly motivated in order to have a wider variety of image content. At the end 13 color images were selected manually for each category resulting in 104 images in total. All selected images have the same size of 640×640 pixels. For each image, seven different manipulations were applied to create different visual effects. We will refer to these manipulations as the following names: • Cartoon: Applies a cartoon effect to an image. • Emoji: Adds an Emoji on top-right corner of an image. • Enhance: Applies brightness/contrast/colorization enhancement on an image via LAB colorspace. • Halo: Applies a circular halo effect to an image. • Gray: Converts an image to gray scale. • Grunge: Applies a classic vintage effect with a grunge background to an image. • Old paper: Applies another heritage style vintage effect with an old paper background to an image. The reason of selecting the seven particular manipulations is that the changes of an image caused by these operations cover different aspects of image information, e.g. color, texture, composition, and higher-level image semantics. The emoji sticker “Tear of Joy” was selected as it has been in the top 10 most popular emojis on Emojipedia for all of 20152, and the emotion it expresses is not that obvious. The seven manipulations were implemented by using ImageMagick software3. An example image processed by the 7 different manipulations is illustrated in Figure 1. Summing up, a grand total of 832 (104× 8) images were generated, including the original versions of each image. The image dataset is publicly accessible at http://mmspg.epfl.ch/ emotion-image-datasets. User Study We used Microworkers4 platform to collect emotional responses from subjects. A questionnaire was designed where four emotion-related questions are asked for each image. The first two questions are about the valence and arousal ratings respectively, where a 9-point scale was used, same as [11, 15]. For valence, 1, 5, and 9 mean very negative, neutral, and very positive emotions respectively, in terms of attractiveness. For arousal, 1 and 9 mean emotions with very low and very high stimulating effects respectively. In the questionnaire, instead of directly asking subjects to provide VA scores, questions were rephrased to be similar as in [11]. The third question is about the emotion distribution of the image, based on Ekman’s six basic emotions [18]. Similar to [11], 7 emotion keywords (Ekman’s six basic emotions and “Neutral”) were used and subjects wer	categorization;color space;computer vision;crowdsourcing;database;emoji;emoticon;emotion recognition;experiment;generalized valence bond;grayscale;high- and low-level;hoare logic;image editing;image processing;image resolution;imagemagick;instagram;machine learning;mobile device;pixel;randomness;requirement;sadness;sampling (signal processing);selfie;smart device;social network;texture mapping;usability testing;visual effects	Lin Yuan;Touradj Ebrahimi	2017		10.2352/ISSN.2470-1173.2017.14.HVEI-150	natural language processing;computer vision;computer science;machine learning	HCI	-34.44795033307314	-48.62329776031666	38700
c48e7b3da3e395aecde6d872de5ff226c5ee357f	attitudes towards vertical farming at home: a user study	autonomy and control;ambient intelligence;sustainability;intelligent environments;technology acceptance;vertical farming	Vertical farming is a promising new technology for increasing crop yields per square meter. However, little research has been done so far in people's perception of this technology. The aim of this project was to gain a better understanding of consumers' attitude on small scale vertical farming at home. This was achieved by developing a prototype that uses sensor and LED technology for growing food at home and deploying it in a user study. The prototype was built to give users a genuine feeling of what it would be like to use a small scale vertical farming system. The user study showed that the attitudes towards the system were mostly positive. However, a fully autonomous system is not desirable and there are concerns regarding food safety.	autonomous system (internet);gold farming;prototype;sensor;usability testing	Guido Jansen;Nazli Cila;Marije Kanis;Yanti Slaats	2016		10.1145/2851581.2892474	simulation;ambient intelligence;human–computer interaction;computer science;sustainability;vertical farming	HCI	-58.17356288504716	-43.370214401340206	38708
cd414d922157246aa5e298d652a7309bbd53262b	content-enriched communication: social uses of interactive tv				Konstantinos Chorianopoulos	2008		10.1081/E-EWMC-120043174	interactive television;multimedia;computer science	HCI	-53.052634994360105	-32.808184978028585	38720
a7fb73b196317ac96996c5fe2d902d20b641b10b	antisedentary beigeless computing	information access;satisfiability;system design;information system;information system design;human computer interface	Four decades of sporadic invention and experimentation of and with non-traditional human-computer interface schemes have congealed (somewhat abruptly though not without a few clear-sighted antecedents) into a new field of information system design, here calledAntisedentary Beigeless Computing, that consciously rejects the traditional conception of isolated tete-a-tete between the human and the box-CRT-keyboardmouse. ABC systems instead favour the complementary directions away from this notion of an immobile info-shrine: more personal, intimate, and portable information access; and more diffuse, environmentally-integrated information access. Consideration of ABC projects to date seems to suggest that no single instance can alone express the full generality required of a ‘working’ information system, so that (on the one hand) system design must acknowledge that a complex set of trade-offs involving capabilities, universality, specificity, personalization, and generality is inescapable; while (on the other hand) an ideal, eventual ‘information environment’ will inevitably comprise the careful interweaving of some number of individual ABC systems. Taxonomies and classification schema can rarely hope to be found complete or flawless before the collection of items that they purport to describe have themselves reached the evolutionary stasis of ‘adulthood’ — that is, there is typically some threshold of development or growth beyond which few enough surprises lurk that an encompassing taxonomy can be constructed and observed to reliably encompass, in the longer term. The domain of ABC thought is still quite nascent, and so we would be foolish to assume that all its extremities of form and connotation are now visible, but to the extent that we can already see the outlines of a ‘field’ it is reasonable to make a first run at an analytic taxonomy. The ‘independent character axes’ approach presented here seems broad and loose enough to accommodate any number of additions to the basic stable of ABC systems. It is, further, a taxonomy amenable to significant revision as may be found necessary: axes can be added, deleted, reconstrued, etc. as time and consideration clarify our understanding of ABC. However, it should also be anticipated that the field will eventually coalesce around a much smaller number of better-defined ‘axes’ and thus permit taxonomic reversion to the more hierarchical (and finally more satisfying) ‘Linnean’ scheme we'd originally imagined establishing.	abc;cathode ray tube;cloud computing;consciousness;human–computer interaction;information access;information system;null (sql);personalization;reversion (software development);sensitivity and specificity;single-instance storage;systems design;taxonomy (general);thinking outside the box;universality probability	John Underkoffler	1997	Personal Technologies	10.1007/BF01317886	simulation;human–computer interaction;computer science;artificial intelligence;information system;satisfiability;systems design	HCI	-56.020015959996506	-30.13808689721969	38730
13c0df0bc636859fbb88ba8fbc5456705c8b621d	looking into meta-emotions	meta emotion;humanidades;paul ekman;higher order emotion;filosofia etica;harry frankfurt;emotion;facs;metaemotion;facial actions;psychic unity;desire	There are many psychic mechanisms by which people engage with their selves. We argue that an important yet hitherto neglected one is self-appraisal via meta-emotions. We discuss the intentional structure of meta-emotions and explore the phenomenology of a variety of examples. We then present a pilot study providing preliminary evidence that some facial displays may indicate the presence of meta-emotions. We conclude by arguing that meta-emotions have an important role to play in higher-order theories of psychic harmony and that Frankfurt-style accounts, which explain a person’s “reflective self-endorsement” exclusively in terms of volitional hierarchies, are inchoate and need to be augmented by a theory of meta-emotions.	first-order predicate;meta-emotion;pervasive informatics;theory	Christoph Jäger;Eva Bänninger-Huber	2014	Synthese	10.1007/s11229-014-0588-x	emotion;philosophy;epistemology	HCI	-59.639454425332254	-33.67192911856424	38780
3c61555c6bff16001db4c8ee2b86177192aefdb9	creation of interactive media content by the reuse of images	presentation software;interactive media;video clip	By using video clips taken from a “Japanese sumo wrestling digest” TV program, we introduces an example of interactive media content production on the viewer side.	cryptographic hash function;interactive media;video clip	Tsutomu Miyasato	2000		10.1145/336296.336407	computer science;multimedia;interactive media;world wide web;computer graphics (images)	HCI	-44.28390531023542	-27.625937644708333	38786
0b59990a137b1fb849d07807f33ea9bdc6a9ae31	overview of the visex task at ntcir-9		Interactive Visual Exploration (VisEx) is a pilot task at NTCIR-9 for establishing an efficient and effective framework for objectively evaluating interactive and explorative information access environments. It aims to acquire more useful and richer evaluation data based on empirical user studies, by adopting a common framework for the environments and conducting sophisticated experiments. Four teams participated in this task. Although it was harder to understand the results and draw a clear conclusion than expected, we learned much and have made useful progress.	experiment;information access	Tsuneaki Kato;Mitsunori Matsushita;Hideo Joho	2011			management science;information access;computer science	ML	-61.631829247634485	-45.19857266240366	38822
288e1964bf6a4901087184dccd0e99269f399788	city landmark as an interactive installation: experiences with stone, water and public space	hci;ubiquitous art;natural element user interfaces;urban user interfaces;stone;tangible interaction;water	In this paper, we describe our demonstration utilizing an urban landmark, a monument in a city center, for interaction. The City Mouse is an interactive media installation, where the participants rotate a 3D model of the Earth presented on a screen by rolling a large stone ball resting on a water fountain in a stone slab. During a public trial, approximately a hundred people interacted with the system. We demonstrate how an existing landmark in urban spaces can be used in an interactive experience, and report the experiences with the installation. We present how the communal ownership, associations and existing practices were connected to the successful design of the media installation.	experience;interactive media;slab allocation	Jonna Häkkilä;Olli Koskenranta;Maaret Posti;Yun He	2014		10.1145/2540930.2540980	human–computer interaction;engineering;civil engineering;cartography	HCI	-49.53299603810308	-35.08653620818424	38830
624723f4d0fbd96e0e3627f9b77b806d9d395b6f	pripref broadcaster: enabling users to broadcast privacy preferences in their physical proximity	privacy signaling;privacy preferences;mobile devices	While privacy is often treated as an information centric issue, privacy issues in ubiquitous and mobile computing also encompass physical or territorial aspects, i.e., the right to be left alone or undisturbed. Disturbances that affect privacy often stem from persons nearby and their mobile devices, e.g., ringing phones, loud phone calls, or sounds of mobile games. We propose PriPref Broadcaster, a smartphone-based approach for communicating personal privacy preferences to persons in physical proximity. Our approach further supports automatic adaptation of mobile device settings based on the dominating preferences in the current environment. Results from a usability study and a five-day field trial with 28 participants show that broadcasting privacy preferences is perceived as meaningful and has the potential to support privacy signaling in many everyday situations.	adobe flash player;mobile computing;mobile device;mobile game;mobile phone;privacy;ringing (signal);smartphone;usability testing	Bastian Könings;Sebastian Thoma;Florian Schaub;Michael Weber	2014		10.1145/2677972.2677978	privacy software;information privacy;computer science;operating system;mobile device;internet privacy;world wide web;computer security	HCI	-57.307782905220215	-42.078077352231155	38888
3420e768e3a15d58b78b5b6bb053f7d45f21ad73	'slidingmap': introducing and evaluating a new modality for map interaction	map interaction;mobile computer;tablet pc;inclination modality;mobile systems;digital mapping	In this paper, we describe the concept of a new modality for interaction with digital maps. We propose using inclination as a means for panning maps on a mobile computing device, namely a tablet PC. The result is a map which is both physically transportable as well as manipulable with very simple and natural hand movements. We describe a setup for comparing this new modality with the better known modalities of pen-based and joystick-based interaction. Apart from demonstrating the new modality we plan to perform a short evaluation.	joystick;mobile computing;modality (human–computer interaction);pen computing;tablet computer	Matthias Merdes;Jochen Häußler;Matthias Jöst	2004		10.1145/1027933.1027989	computer vision;digital mapping;computer science;operating system;multimedia;computer graphics (images)	Vision	-45.30615032554436	-41.3125322631068	38914
ad3c0c84a4242fafece37950ef67303e4da54c9f	guest editor's introduction		The premiere issue of IEEE Computer Graphics and Applications contained a review of graphic display technologies,* which indicated the current dominance of directed-beam refresh and direct-view storage tube displays in computer-aided design and drafting. This dominance is reflected in three articles in this issue, which describe inhouse graphic design and documentation applications developed by Western Electric, Lawrence Livermore National Laboratory, and IBM. These applications are in operation today and were initiated in the mid-to-late 1970's; thus, they predate the current strong interest in color raster displays. Merging the high interactivity of directed-beam refresh technology and the greater information content ofDVST technology with the greater flexibility of raster-scan is a challenge for the 1980's.	computer graphics;computer-aided design;direct-view bistable storage tube;display device;documentation;interactive media;interactivity;raster graphics;raster scan;self-information	Frank Schweitzer	2002	Advances in Complex Systems	10.1142/S0219525902000560		Graphics	-49.890959541522804	-26.250072385328235	38920
1101bff6dc838bf17ad39bf19324987b7119436b	analyzing musical expressivity with a soft computing approach	musical expressivity;soft computing;articulo;musical analysis;analysis tools;classical guitar;computing process;audio analysis	In this paper we present our research on the design of a tool to analyze musical expressivity. Musical expressivity is a human activity difficult to model computationally because of its nature: implicitly acquired by musicians through a long process of listening and imitation. We propose the use of soft computing techniques to deal with this problem. Specifically, from a collection of sound features obtained by using state of the art audio analysis algorithms, we apply a soft computing process to generate a compact and powerful representation. Moreover, we have designed a graphical user interface to provide a flexible analysis tool. We are using the analysis tool in the guitarLab project, focused on the study of musical expressivity of classical guitar.	soft computing	Josep Lluís Arcos;Enric Guaus;Tan Hakan Özaslan	2013	Fuzzy Sets and Systems	10.1016/j.fss.2012.01.019	classical guitar;speech recognition;computer science;artificial intelligence;soft computing;audio analyzer;musical analysis	NLP	-35.10380962037712	-45.89315510434893	38923
68de5771b4dccd8cffb96b28d1fc0ecdca577901	twin worlds: augmenting, evaluating, and studying three-dimensional digital cities and their evolving communities	communaute virtuelle;reseau communication;web pages;social interaction;modelo 3 dimensiones;interaction sociale;visualizacion;congres international;implementation;modele 3 dimensions;congreso internacional;data collection;virtual community;library and information science;three dimensional model;information visualization;international conference;three dimensional;ejecucion;visualization;internet;interaccion social;visualisation;design and implementation;3 dimensional;ville numerique;user interaction;red de comunicacion;communication network;comunidad virtual;virtual worlds	New approaches and tools are required to inform the design and implementation of 3-dimensional (3-D) digital cities and to steer the growth of their virtual communities. This paper argues to apply information visualization techniques and to utilize Twin Worlds – pairs of virtual worlds in which one world is devoted to visualize user interaction data collected in the other world – to augment, evaluate, and research the digital cities of tomorrow. The approach is exemplified by means of an abstract scholarly digital city: A 3-D collaborative Memory Palace – a shared resource of online documents (web pages, papers, images, videos, software demos) for faculty and students at the School of Library and Information Science at Indiana University – and its twin, Mirror Garden – a second 3-D world that visualizes user interaction data collected in	information visualization;library and information science;smart city;virtual community;virtual world;web page	Katy Börner	2001		10.1007/3-540-45636-8_20	three-dimensional space;simulation;information visualization;visualization;human–computer interaction;computer science;artificial intelligence;database;computer security	HCI	-60.70959353848582	-31.898742273164924	38931
1b45a23b381e6a6a8491567828d766fc6a0186ca	augmenting bicycles and helmets with multimodal warnings for children		Child cyclists are often at greater risk for traffic accidents. This is in part due to the development of children's motor and perceptual-motor abilities. To facilitate road safety for children, we explore the use of multimodal warning signals to increase their awareness and prime action in critical situations. We developed a bicycle simulator instrumented with these signals and conducted two controlled experiments. We found that participants spent significantly more time perceiving visual than auditory or vibrotactile cues. Unimodal signals were the easiest to recognize and suitable for encoding directional cues. However, when priming stop actions, reaction time was shorter when all three modalities were used simultaneously. We discuss the implications of these outcomes with regard to design of safety systems for children and their perceptual-motor learning.	bi-directional text;experiment;multimodal interaction;simulation;the machine's child	Andrii Matviienko;Swamy Ananthanarayan;Shadan Sadeghian Borojeni;Yannick Feld;Wilko Heuten;Susanne Boll	2018		10.1145/3229434.3229479	priming (psychology);system safety;simulation;encoding (memory);computer science	HCI	-47.71197070592604	-52.03783771349973	38935
43048d9264543967de6a5c589b4df91192fb35fc	emerging topics on personalized and localized multimedia information systems	multimedia information systems;life logging;personalization;check ins;location aware applications and services;geographic popularity	We are experiencing an era with a rapid increase of data relevant to different aspects of users' daily life. On the one hand, such data contains personal information of each individual user. On the other hand, it also reflects user behaviors related to the society as data of more users is aggregated. These data could not only be very beneficial for studying various lifestyle patterns, but also be used to generate more descriptive and explanatory analysis across the landscape of diverse multimedia data. Using personal mobile devices and web services to systematically explore interesting aspects of people world has attracted much attention recently. This is a full-day tutorial that addresses emerging topics on personalized and localized multimedia technologies and applications and emphasizes knowledge sensing and discovery in multimedia landscape. This tutorial aims to deliver anoverall introduction to multimedia landscapes with multimedia processing, contextual data acquisition, people activity logs, data analytics, geographic-aware multimedia sharing and delivery, and serves as an important lecture on fundamental and advanced research areas of personalized and localized multimedia information systems.	data acquisition;information systems;information system;mobile device;personalization;personally identifiable information;web service	Yi Yu;Kiyoharu Aizawa;Toshihiko Yamasaki;Roger Zimmermann	2014		10.1145/2647868.2654850	computer science;data mining;personalization;multimedia;world wide web;lifelog	DB	-56.79249428573999	-42.074357012960306	38946
144f6542f211974c9113f3ad3eed595f732fe543	sidelock: authentication on mobile device sides	mobile authentication;tangible interaction;gesture recognition;grasp pattern	This paper presents Sidelock, a tangible authentication prototype on mobile devices. Grasp events and finger gestures are sensed by twenty capacitive sensors on left and right sides. They were used in a two-phase authentication process, in which grasp pattern wakes up devices and a 1-D template-based gesture recognizer verifies if input matches pre-defined password templates. Compared with other popular authentication approachers like PINs or grid lock, Sidelock has much larger password space and approximate recognition speed, with only 5.3% critical errors. The prototype could be minimized and embedded broadly in many common mobile devices. The user feedback suggests it is an memorable and acceptable authentication method.	approximation algorithm;authentication;embedded system;finite-state machine;gesture recognition;mobile device;password;prototype;sensor;two-phase commit protocol	Zhenkun Zhou;Jiangqin Wu	2013		10.1145/2512349.2514912	embedded system;engineering;communication;computer security	HCI	-43.85020169008003	-43.57119790350411	38955
b7682634c8633822145193242e7a3e3739042768	musicmixer: computer-aided dj system based on an automatic song mixing	computer aided dj system;automatic song mixing;human content interaction	In this paper, we present MusicMixer, a computer-aided DJ system that helps DJs, specifically with song mixing. MusicMixer continuously mixes and plays songs using an automatic music mixing method that employs audio similarity calculations. By calculating similarities between song sections that can be naturally mixed, MusicMixer enables seamless song transitions. Though song mixing is the most fundamental and important factor in DJ performance, it is difficult for untrained people to seamlessly connect songs. MusicMixer realizes automatic song mixing using an audio signal processing approach; therefore, users can perform DJ mixing simply by selecting a song from a list of songs suggested by the system, enabling effective DJ song mixing and lowering entry barriers for the inexperienced. We also propose personalization for song suggestions using a preference memorization function of MusicMixer.	audio signal processing;computer;experience;midi;personalization;seamless3d	Tatsunori Hirai;Hironori Doi;Shigeo Morishima	2015		10.1145/2832932.2832942	speech recognition;acoustics;computer science;communication	HCI	-47.70403693498983	-37.284802053984244	38988
c9cdf84587cc9d75bde5f83c72e537c5ccd0ab4d	cluemaker: a language for approximate record matching	source code;decision tree;machine learning	We introduce ClueMaker, the first language designed specifically for approximate record matching. Clues written in ClueMaker predict whether two records denote the same thing based on the values of the records’ attributes. For example, a clue may predict match if the records have identical values for the first name attribute. The values of the clues can then be used as input to a machine-learning technique to compute a match probability. ClueMaker is based on Java and is compiled to Java source or byte code. Therefore, ClueMaker is easily accessible to many programmers, allows the integration of any Java class, runs on virtually any platform, supports UNICODE, and is more easily accepted by IT departments who try to minimize the number of distinct languages in use. ChoiceMaker Technologies has used ClueMaker successfully over the past two years in a variety of approximate record matching tasks.	approximation algorithm;byte;compiler;java;machine learning;programmer;unicode	Martin Buechi;Andrew Borthwick;Adam Winkel;Arthur Goldberg	2003			data control language;object language;specification language;language primitive;low-level programming language;machine learning;high-level programming language;artificial intelligence;cache language model;pattern recognition;object code;computer science	PL	-47.232470541228	-25.538209488736506	38996
af52044b38945999979f0ee336a6c6de642bf222	good textbook for computer graphics students : interactive computer graphics: functional, procedural and device-level methods p burger and d gillies addison-wesley (1989) 504 pp	computer graphic		computer graphics;procedural programming	Jarke J. van Wijk	1990	Computer-Aided Design	10.1016/0010-4485(90)90098-W	human–computer interaction;computer science;mathematics;computer graphics (images)	Graphics	-46.8010127083198	-29.719406956663896	38998
e28c4839c514cc3d9ada534ec6aece28d3dc7c61	fractal computation in step with real-time dance	real time;cluster computing	This paper describes a collaboration among computer scientists, dancers, and musicians on a production entitled “Fibonacci and Phi.” Thematically, the production explored mathematical concepts that have aesthetic appeal, capturing the ways in which mathematical beauty gives shape to nature and art, and expressing the human response to these forms. Technically, the production tested the limits of parallel cluster computation in real-time multimedia and performance art. The result was a dance performance that wove together science and art in a way intended to draw new audience members into both realms.	computation;computer scientist;digital media;fractal;graphical user interface;parallel computing;real-time clock;real-time transcription;realms;supercomputer	Jennifer J. Burg;Tim Miller	2004			fibonacci number;multimedia;computation;computer cluster;dance;mathematical beauty;computer science;fractal	Graphics	-53.42057724940031	-29.043977926700656	39051
9adbe92dc0d72c78bb354a9fb1a6ce3ec44bce05	advanced interface productization: lessons learned	adaptive interfaces;internet chat;mobile device;user interface;experience;web interface;adaptive interface;mobile web;lessons learned;table top displays;visual language;ubiquitous computing;comics;automatic illustration generation;mobile internet;user interfaces;mobile devices;multi touch interfaces;productization;object sensing interfaces;product development	Over the years I have managed several research projects relating to advanced interfaces and have turned them into shipping products. Microsoft Comic Chat uses automatic illustration generation and the visual language of comics to present online conversations. Microsoft's Mobile Internet Toolkit renders web interfaces on a variety of mobile devices. Microsoft Surface is a multi-touch, object-sensing table-top display. More recently I have been advising start-ups in the mobile content/ubiquitous computing area. Juggling the competing interests of research and product development has been often interesting, sometimes painful, and always challenging. In my invited talk, I will discuss lessons learned while productizing interface technology, including selecting the product to ship, balancing research and product requirements, navigating management's whims of the day, setting goals, and evaluating the results, as well as what has worked and what has not, and why certain efforts have been more successful than others. Here I present a sampling of these lessons.	internet;mobile device;multi-touch;new product development;online chat;rendering (computer graphics);requirement;sampling (signal processing);ubiquitous computing;user interface;visual language	David Kurlander	2012		10.1145/2254556.2254559	human–computer interaction;computer science;operating system;multimedia;user interface;world wide web;ubiquitous computing	HCI	-50.81071385802263	-35.83968072718736	39090
18ed9bb1e0fe6398e47a5cea4802512c42138b67	supporting personal narrative for children with complex communication needs	scanning selection;single switch input;cerebral palsy;personal narrative;augmentative and alternative communication;disability;accessibility;language development;assistive technology	Children with complex communication needs who use voice output communication aids seldom engage in extended conversation. The “How was School today...?” system has been designed to enable such children to talk about their school day. The system uses data-to-text technology to generate narratives from sensor data. Observations, interviews and prototyping were used to ensure that stakeholders were involved in the design of the system. Evaluations with three children showed that the prototype system, which automatically generates utterances, has the potential to support disabled individuals to participate better in interactive conversation. Analysis of a conversational transcript and observations indicate that the children were able to access relevant conversation and had more control in the conversation in comparison to their usual interactions where control lay mainly with the speaking partner. Further research to develop an improved, more rugged system that supports users with different levels of language ability is now underway.	interaction;prototype;rugged computer	Rolf Black;Annalu Waller;Ross Turner;Ehud Reiter	2012	ACM Trans. Comput.-Hum. Interact.	10.1145/2240156.2240163	simulation;human–computer interaction;computer science;accessibility;multimedia;world wide web	HCI	-51.24812464163609	-43.76803689206351	39099
ab4a9e3f3113abac9d51435db7ec4a6ca7cb9236	tcp/ip and onc/nfs - internetworking in a unix environment (2. ed.)		Well, someone can decide by themselves what they want to do and need to do but sometimes, that kind of person will need some tcp ip and onc nfs internetworking in a unix environment references. People with open minded will always try to seek for the new things and information from many sources. On the contrary, people with closed mind will always think that they can do it by their principals. So, what kind of person are you?	internet protocol suite;internetworking;mind;open network computing remote procedure call;unix	Michael Santifaller	1994				Web+IR	-61.26034931658947	-25.160743426181423	39110
926d6cec3c7097e11367be56ee4ebaf75e765d61	cooperative game play with avatars and agents: differences in brain activity and the experience of play	cooperative play;videogames;eeg	The current study sought to identify the impact of whether teammates in a cooperative videogame were controlled by other humans (avatars) or by the game (agents). The impact on player experience was explored through both subjective questionnaire measures and brain wave activity measurement (electroencephalography). Play with human teammates was associated with a greater sense of relatedness, but less competence and flow than play with other computer-controlled teammates. In terms of brain activity, play with human teammates was associated with greater activity in the alpha, theta and beta power bands than play with computer-controlled teammates. Overall, the results suggest that play with human teammates involves greater cognitive activity in terms of 'mentalising' than play with computer-controlled teammates. Additionally, the associations between subjective measures of player experience and brain activity are described. Limitations of the current study are identified and key directions for future research are discussed.	avatar (computing);electroencephalography;humans;intelligent agent;neural oscillation	Daniel M. Johnson;Peta Wyeth;Madison Clark;Christopher N. Watling	2015		10.1145/2702123.2702468	simulation;electroencephalography	HCI	-53.884626512092275	-50.28960370838659	39140
88d57ac7859daa33cccee9f627d8b50bcc0e6b0d	invisible connections: investigating older people's emotions and social relations around objects	internet of things;social relations;socio material relations;ageing;objects;tangible interaction;conference proceeding	The advent of the Internet of Things creates an interest in how people might interrelate through and with networks of internet enabled objects. With an emphasis on fostering social connection and physical activity among older people, this preliminary study investigated objects that people over the age of 65 years viewed as significant to them. We conducted contextual interviews in people's homes about their significant objects in order to understand the role of the objects in their lives, the extent to which they fostered emotional and social connections and physical activity, and how they might be augmented through internet connection.  Discussion of significant objects generated considerable emotion in the participants. We identified objects of comfort and routine, objects that exhibited status, those that fostered independence and connection, and those that symbolized relationships with loved ones. These findings lead us to consider implications for the design of interconnected objects.	contextual inquiry;internet access;internet of things	Kate Vaisutis;Margot Brereton;Toni Robertson;Frank Vetere;Jeannette Durick;Bjorn Nansen;Laurie Buys	2014		10.1145/2556288.2557314	ageing;social relation;human–computer interaction;computer science;object;multimedia;internet of things	HCI	-58.550884712805036	-40.741815826601446	39196
80537233367453289a93b6c0d302cd5d66b572b7	experimental facility for evaluation of the user interface in the pbx environment	user interface		user interface	S. Rabie;R. Bloedon;D. Dockendorff;Sorin Cohn-Sfetcu	1984			user interface design;shell (computing);human–computer interaction;computer science;user interface	HCI	-43.003780730997335	-29.83698655104324	39208
96d8ddb5a4dcf4d7fdda221b23b211c748e58532	a phrase-driven grammar system for interactive data visualization	polarisation optique;interfaces;man machine dialogue;0130c;langage naturel;pregunta documental;natural languages;systeme conversationnel;0705r;grammars;visualization;graphical user interfaces;optical polarization;grammaire;natural language;data visualization;query;graphic user interface;dialogo hombre maquina;visualisation donnee;interactive systems;interface graphique;requete;dialogue homme machine	A Phrase-Driven Grammar System (PDGS) is a novel GUI for facilitating the visualization of data. The PDGS integrates data source applications and external visualization tools into its framework and functions as a middle-layer application to coordinate their operations. It allows users to formulate data query and visualization descriptions by selecting graphical icons in a menu or on a map. To specify data query and visualization intuitively and efficiently, we designed Graphical User Interface and a natural-language-like grammar, Phrase-Driven Grammar (PDG). The formulation of PDG data query and visualization descriptions is a constrained natural-language phrase building process. PDG phrases produce graphical visualizations of the data query, allowing users to interactively explore meaningful data relationships, trends, and exceptions.	experience;graphical user interface;graphics;human computer;human–computer interaction;interactive data visualization;interactivity;natural language;parsing;program dependence graph;prototype;sql;scripting language;simulation;usability testing	Sang Yun Lee;Ulrich Neumann	2008		10.1117/12.766907	natural language processing;information visualization;computer science;theoretical computer science;graphical user interface;natural language;programming language;data visualization	HCI	-37.784962664521146	-28.07940503258056	39223
429185748883fb262eaef3a1d8e071a6fedf7ee3	guided procedural modeling	mass spring system;building block;top down control;procedural modeling;computer graphic;viewing algorithms;message passing;i 3 3 computer graphics picture image generation	Procedural methods present one of the most powerful techniques for authoring a vast variety of computer graphics models. However, their massive applicability is hindered by the lack of control and a low predictability of the results. In the classical procedural modeling pipeline, the user usually defines a set of rules, executes the procedural system, and by examining the results attempts to infer what should be changed in the system definition in order to achieve the desired output. We present guided procedural modeling, a new approach that allows a high level of top-down control by breaking the system into smaller building blocks that communicate. In our work we generalize the concept of the environment. The user creates a set of guides. Each guide defines a region in which a specific procedural model operates. These guides are connected by a set of links that serve for message passing between the procedural models attached to each guide. The entire model consists of a set of guides with procedural models, a graph representing their connection, and the method in which the guides interact. The modeling process is performed by modifying each of the described elements. The user can control the high-level description by editing the guides or manipulate the low-level description by changing the procedural rules. Changing the connectivity allows the user to create new complex forms in an easy and intuitive way. We show several examples of procedural structures, including an ornamental pattern, a street layout, a bridge, and a model of trees. We also demonstrate interactive examples for quick and intuitive editing using physics-based mass-spring system.	blackwell (series);color graphics adapter;compiler;computer graphics;eurographics;graph (discrete mathematics);graphical user interface;graphics hardware;high- and low-level;high-level programming language;ibm notes;interactivity;l-system;mathematical model;message passing;multifractal system;procedural modeling;procedural programming;top-down and bottom-up design;usability testing	Bedrich Benes;Ondrej Stava;Radomír Mech;Gavin S. P. Miller	2011	Comput. Graph. Forum	10.1111/j.1467-8659.2011.01886.x	computer vision;message passing;simulation;computer science;artificial intelligence;operating system;programming language;effective mass;procedural modeling;algorithm;computer graphics (images)	Graphics	-38.458488741553936	-32.28208649888005	39247
9a87e965ceace7aaefcdbb849dd6f19f18e0b6e3	customizing urban pattern through an agent-based approach	swarm behaviour;space customization;evolutionary image generation;agent based approach	This paper discusses the 3D space customization of design concepts within self-generated sculpture as an instigator for design of urban pattern. Appropriating from the concept of computer fuzzy logic, fuzzy design prods serve as exemplars of naturally occurring swarm behaviors. The hybridization of design through the 'mistake' and the different material vocabularies serve as departure points for the conceptualization of image breeding in 2D and for 3D grouping within urban pattern. Additive and eroding material processes spawn rule-based agent behaviors that assist the designers/artists to conceive and to enhance appearance and place. In an iterative process, swarm entities physically augment forms in an organic manner. The designer becomes the voyeur of their own creative input as swarm behaviors influence the placement and grouping of architecture/sculpture within the urban pattern of cities. In particular, this paper focuses on the agent-based approach whereby swarm behavior classifies residential, commercial and green spaces within urbanized areas.		Salman Khalili Araghi;Afshin Esmaeili;Gerald Hushlak;Anna Hushlak	2014	IJSIR	10.4018/ijsir.2014100103	swarm behaviour;simulation;artificial intelligence	Vision	-37.35748512406193	-33.06945151979833	39251
0760bc64025962142b8c312849d31eb7f974b43b	do we need to walk for effective virtual reality navigation? physical rotations alone may suffice	virtual reality;head mounted display	Physical rotations and translations are the basic constituents of navigation behavior, yet there is mixed evidence about their relative importance for complex navigation in virtual reality (VR). In the present experiment, 24 participants wore head-mounted displays and performed navigational search tasks with rotations/translations controlled by physical motion or joystick. As expected, physical walking showed performance benefits over joystick navigation. Controlling translations via joystick and rotations via physical rotations led to better performance than joystick navigation, and yielded almost comparable performance to actual walking in terms of search efficiency and time. Walking resulted, however, in increased viewpoint changes and shorter navigation paths, suggesting a rotation/translation tradeoff and different navigation strategies. While previous studies have emphasized the importance of full physical motion via walking (Ruddle & Lessels, 2006, 2009), our data suggests that considerable navigation improvements can already be gained by allowing for full-body rotations, without the considerable cost, space, tracking, and safety requirements of free-space walking setups.	gps navigation device;head-mounted display;joystick;microsoft outlook for mac;motion planning;requirement;simulation;viewing cone;virtual reality;word lists by frequency	Bernhard E. Riecke;Bobby Bodenheimer;Timothy P. McNamara;Betsy Williams Sanders;Peng Peng;Daniel Feuereissen	2010		10.1007/978-3-642-14749-4_21	computer vision;simulation;engineering;computer graphics (images)	HCI	-45.25350829235122	-48.38365063676965	39290
251c055157da3f6dc792f874a22f2f81f8cbf7af	pictures worth a thousand words: reflections on visualizing personal blood glucose forecasts for individuals with type 2 diabetes		Type 2 Diabetes Mellitus (T2DM) is a common chronic condition that requires management of one's lifestyle, including nutrition. Critically, patients often lack a clear understanding of how everyday meals impact their blood glucose. New predictive analytics approaches can provide personalized mealtime blood glucose forecasts. While communicating forecasts can be challenging, effective strategies for doing so remain little explored. In this study, we conducted focus groups with 13 participants to identify approaches to visualizing personalized blood glucose forecasts that can promote diabetes self-management and understand key styles and visual features that resonate with individuals with diabetes. Focus groups demonstrated that individuals rely on simple heuristics and tend to take a reactive approach to their health and nutrition management. Further, the study highlighted the need for simple and explicit, yet information-rich design. Effective visualizations were found to utilize common metaphors alongside words, numbers, and colors to convey a sense of authority and encourage action and learning.	amiga reflections;color;focus group;google analytics;heuristic (computer science);nsa product types;personalization;self-management (computer science)	Pooja M. Desai;Matthew E. Levine;David J. Albers;Lena Mamykina	2018		10.1145/3173574.3174112	human–computer interaction;type 2 diabetes mellitus;type 2 diabetes;health communication;predictive analytics;chronic condition;applied psychology;computer science;focus group;diabetes mellitus;heuristics	HCI	-59.03235758915897	-51.41246292512711	39340
f0b4a51602e18d04cda97a33697bef8a6ec88c20	ict and health care development in western europe	health care	An image display device includes an image carrier, an image forming station for forming an erasable image on the image carrier, a display station for visually displaying the image formed on the image carrier, and a read station for reading out the image on the image carrier and converting the image to an image signal.		Angela Di Tommaso	2009	Egyptian Computer Science Journal		multimedia;display device;electronic engineering;engineering;health care;information and communications technology	Logic	-38.89086108759122	-40.3004489360254	39373
927258c829dc1d6058c69eea11fa106640dedf90	cgal arrangements and their applications - a step-by-step guide		Why should wait for some days to get or receive the cgal arrangements and their applications a step by step guide book that you order? Why should you take it if you can get the faster one? You can find the same book that you order right here. This is it the book that you can receive directly after purchasing. This cgal arrangements and their applications a step by step guide is well known book in the world, of course many people will try to own it. Why don't you become the first? Still confused with the way?	cgal;purchasing;the hitchhiker's guide to the galaxy	Efi Fogel;Dan Halperin;Ron Wein	2012		10.1007/978-3-642-17283-0		Theory	-61.37603965960575	-24.534053705604745	39374
54bee5ca7d3865971764c1b36a7bfc6842f616bf	preparation of media object presentation and sensory effect rendering in mulsemedia applications		Nowadays multimedia applications are employed in different fields as entertainment, education, government services, health and e-commerce. Moreover, some applications incorporate sensory effects along with traditional multimedia content, stimulating other human senses beyond sight and hearing to convey information. Those applications are called mulsemedia applications, where the maintenance of synchronization among media objects and sensory effects is a key point for user quality of experience. In order to minimize delays or failures on content reproduction, this paper proposes a new operation on media objects and sensory effects that can be offered to authors of mulsemedia applications. This operation enables the preparation of media object presentation or the preparation of sensory effect rendering, considering device limitations on which the application will be executed. As proof of concept, our proposal is implemented on multimedia applications using the NCL language, through specification of a new NCL event type, named preparation event. Furthermore, a new NCL media property is presented, allowing the control of continuous media reproduction. Finally, three use cases are defined to demonstrate the use of the preparation event in multimedia and mulsemedia applications.		Marina Josué;Débora C. Muchaluat-Saade;Marcelo Ferreira Moreno	2018		10.1145/3243082.3243098	sight;rendering (computer graphics);multimedia;proof of concept;synchronization;entertainment;use case;quality of experience;sensory system;computer science	HCI	-46.62126629558945	-33.85565932709546	39388
183adb1a2948f1c07086b3d8ddc61a3671513dd8	intelliprompter: speech-based dynamic note display interface for oral presentations		The fear of forgetting what to say next is a common problem in oral presentation delivery. To maintain good content coverage and reduce anxiety, many presenters use written notes in conjunction with presentation slides. However, excessive use of notes during delivery can lead to disengagement with the audience and low quality presentations. We designed IntelliPrompter, a speech-based note display system that automatically tracks a presenter’s coverage of each slide’s content and dynamically adjusts the note display interface to highlight the most likely next topic to present. We developed two versions of our intelligent teleprompters using Google Glass and computer screen displays. The design of our system was informed by findings from 36 interviews with presenters and analysis of a presentation note corpus. In a within-subjects study comparing our dynamic screen-based and Google Glass note display interfaces with a static note system, presenters and independent judges expressed a strong preference for the dynamic screen-based system.	computer monitor;glass;teleprompter;text corpus	Reza Asadi;Ha Trinh;Harriet J. Fell;Timothy W. Bickmore	2017		10.1145/3136755.3136818	human–computer interaction;anxiety;disengagement theory;multimedia;forgetting;computer science	HCI	-37.64710066216464	-49.476545561945386	39393
37145cbaed062c8b5c3ddb95683b69f2cc6b21ce	development of an open source motion capture system		Motion capture (MoCap) has been one of the leading and most useful tools within the field of animation to capture fluid and detailed motion. However, it can be quite expensive for animators, game developers and educators on a tight budgets. By using Raspberry Pi Zeros, with NoIR cameras and IR LED light rings, the cost of a four-camera system can potentially be reduced to less than 1000 USD. The research described should lead to an effective and useful system, able to detect multiple markers, record their coordinates, and keep track of them as they move. With a setup of three or more cameras, one would be able to triangulate the data on a low-cost host computer. All software and hardware designs will be disseminated open source, providing anyone who is interested in MoCap, whether it be for hobbyist, semi-professional, or educational purposes, a system for a fraction of the typical cost.	host (network);motion capture;open-source software;raspberry pi 3 model b (latest version);semiconductor industry	Paul Canada;George Ventura;Christopher Iossa;Orquidia Moreno;William J. Joel	2018		10.1145/3230744.3230811	computer graphics (images);triangulation;software;animation;host (network);multiple markers;motion capture;computer science;game developer	HCI	-45.892454494458825	-30.990325525858108	39439
2be91daa54d6ae9399a8835a850922369be0830e	effects of haptic and 3d audio feedback on operator performance and workload for quadrotor uavs in indoor environments	behavioral entropy;haptics;force feedback;multimodal interaction;unmanned aerial vehicles;3d audio	Effects of Haptic and 3D Audio Feedback on Operator Performance and Workload for Quadrotor UAVs in Indoor Environments Robert M. Philbrick Department of Mechanical Engineering, BYU Master of Science Indoor flight of unmanned aerial vehicles (UAVs) has many applications in environments in which it is undesirable or dangerous for humans to be, such as military reconnaissance or searching for trapped victims in a collapsed building. However, limited visual feedback makes it difficult to pilot UAVs in cluttered and enclosed spaces. Haptic feedback combined with visual feedback has shown to reduce the number of collisions of UAVs in indoor environments; however, it has increased the mental workload of the operator. This thesis investigates the potential of combining novel haptic and 3D audio feedback to provide additional information to operators of UAVs in order to improve performance and reduce workload. Many haptic feedback algorithms, such as Time to Impact (TTI) [1], have been developed to help pilot UAVs. This thesis compares TTI with two new haptic feedback algorithms: OmniDirectional Dynamics Springs (ODDS) and Velocity Scaled Omni-Directional Dynamic Springs (VSODDS). These novel algorithms are based on the idea that dynamic springs are attached to the haptic controller in all directions. This thesis is unique by augmenting visual and haptic feedback with real-time 3D audio feedback. Continuous Directional Graded Threshold (CDGT) and Discrete Directional Graded Threshold (DDGT) are two novel algorithms that were developed to provide 3D audio warning cues to operators. To reduce sensory overload, these algorithms play a graded audio alert cue in the direction of velocity and when within a threshold distance of an obstacle. In order to measure operator workload, many researchers have used subjective measures, which suffer from subject bias, preconceptions, and ordering. Instead of using a subjective measure, experimental data is used to objectively measure operator workload using behavioral entropy, which works on the idea that humans work to reduce entropy by skilled behavior [2]. QuadSim, a robust and versatile indoor quadrotor simulator, was developed as a test bed for visual, haptic, and 3D audio feedback. Using QuadSim, a human subject experiment was performed to determine the effectiveness of haptic and 3D audio feedback on operator performance and workload. The results of the study indicate that haptic feedback significantly reduced the number of collisions and collision length. Operator workload was decreased in the side-to-side direction by VSODDS but was adversely increased by TTI. Overall, VSODDS outperformed the other haptic algorithms. Unlike haptic feedback, audio feedback proved to be neither helpful nor harmful in improving performance or reducing workload.	aerial photography;algorithm;audio feedback;haptic technology;real-time clock;simulation;testbed;unmanned aerial vehicle;velocity (software development)	Robert M. Philbrick;Mark B. Colton	2014	JRM	10.20965/jrm.2014.p0580	control engineering;computer vision;simulation;engineering	HCI	-45.176448941304486	-51.65472815896792	39459
5307b9d2ffdb3a1a0a6c5725650447f5cfdc5b36	momo: enabling hybrid museums	research centre;museo;tecnologia electronica telecomunicaciones;informatique mobile;calculateur embarque;computacion informatica;tourisme;musee;pocketpc platform;tourist attraction;cultural heritage;customization;divertissement;personnalisation;notebook computers humanities mobile computing network operating systems;partial prediction matching algorithm;learning environment;patrimoine culturel;tourism;hybrid museums;net compact framework;ciencias basicas y experimentales;patrimonio cultural;boarded computer;personalizacion;momo;society cultural heritage preservation;preservation;attraction;museum;tecnologias;atraccion;turismo;mobile computing;preservacion;entertainment;calculador embarque;partial prediction matching algorithm momo hybrid museums society cultural heritage preservation learning environment research centre tourist attraction wireless personal digital device net compact framework pocketpc platform;wireless personal digital device	Present-day museums are not mere passive institutions for the preservation of a society’s cultural heritage. They have become instead learning environments, research centres and even tourist attractions. The paper introduces the notion of a hybrid museum (HM) in which wireless personal digital devices (PDAs) are used to tailor digital contents to the visitor to enrich both the learning and entertainment experience. The paper describes a fully functional hybrid museum infrastructure (MoMo) implemented with the .NET compact framework running on the PocketPC platform. Several research challenges that had to be faced during the implementation of the system such as the exploration of large sets of information on PDAs are also presented; and the customisation and personalisation of the displayed contents using a modified partial prediction matching algorithm are also discussed.	algorithm;breath of fire iii;graphical user interface;markov chain;markov model;personal digital assistant;personalization;pocket pc	Javier Jaén Martínez;José M. Esteve;José A. Mocholí;José H. Canós	2005	IEE Proceedings - Software	10.1049/ip-sen:20045029	entertainment;simulation;computer science;engineering;cultural heritage;operating system;multimedia;mobile computing;tourism;preservation	HCI	-50.68496850676688	-31.092394923339317	39473
086584f6cd26ded3093a7897efdb3692a94852a6	learn to adapt based on users' feedback	assistive interactive robots users feedback adaptive behavior personalized behavior human robot interactive systems interaction traces robot behavior robot actions adaptation rule extraction learning algorithms;history;diabetes;brightness;hidden markov models;robots adaptation models hidden markov models markov processes brightness diabetes history;robots;markov processes;adaptation models;learning artificial intelligence human robot interaction interactive systems	Adaptive and personalized behavior is becoming essential and desirable in Human-Robot Interactive systems. We are interested in adaptive robots that learn from interaction traces (previous interactions with users). Our proposal is based on types of interactions where users express their level of satisfaction through feedback. Indeed, depending on the situation of interaction and the user himself, the robot behavior should adjust, and therefore can be judged, differently. From interaction traces (including robot actions and users' feedback), we aim to extract adaptation rules that give the dependencies between certain attributes of the interaction situation and/or the user profile, and the level of user satisfaction. We propose two learning algorithms to learn these adaptation rules. The first algorithm is direct, certain and optimal but slow to converge. The second is able to detect the importance of certain attributes in the adaptation process. It generalizes adaptation rules on unknown situations and to first time users, which makes it an approach with risk. We detail in this paper, our proposed model, both learning algorithms, and an evaluation of the learned rules from both algorithms by simulations and through a scenario with real users.	adaptive filter;algorithm;converge;experiment;feedback;human–robot interaction;machine learning;personalization;robot;sensor;simulation;tracing (software);user profile	Abir-Beatrice Karami;Karim Sehaba;Benoît Encelle	2014	The 23rd IEEE International Symposium on Robot and Human Interactive Communication	10.1109/ROMAN.2014.6926322	robot;simulation;computer science;artificial intelligence;machine learning;markov process;brightness;hidden markov model	Robotics	-49.43852413477726	-51.23758078440858	39505
6b9454e5db24581413f4608f8ff72af4fe458ae0	video shooting navigation system by real-time useful shot discrimination based on video grammar	real time systems;real time;projection method;broadcasting;navigation	In this paper, we propose a video shooting navigation system by real-time useful shot discrimination based on video grammar to support users shooting nice shots for later editing work. In this system, the processing speed must be very high so that we use a gray value projection method to extract the camerawork parameters in real-time. From the result of camerawork and gray value analysis, the shots are classified into 16 states and the system issues alarms and instructions about the usefulness and uselessness of the shots in real-time just after shooting the shot. Thereby, users can retake a shot necessary for later editing efficiently	real-time clock;real-time locating system;real-time transcription	Kuniaki Uehara;Miki Amano;Yasuo Ariki;Masahito Kumano	2004	2004 IEEE International Conference on Multimedia and Expo (ICME) (IEEE Cat. No.04TH8763)		computer vision;navigation;computer science;multimedia;projection method;broadcasting;computer graphics (images)	Robotics	-39.831880508190615	-38.72483386303091	39568
f718f2d3b19ee735c33b9ca4376cbc73aff11b69	comparing voice with touch screen for controlling the instructor’s operating station of a flight simulator	voice driven interface touch screen operating station control flight simulator;touch sensitive screens;touch screen;aerospace simulation aircraft propulsion speech analysis computer aided engineering fingers analytical models performance analysis costs security guidelines;user interfaces aerospace simulation speech based user interfaces touch sensitive screens;flight simulator;aerospace simulation;speech based user interfaces;voice driven interface;operating station control;user interfaces	Flight simulators are expensive devices that airlines use to train their pilots. Currently, the instructor interact with the simulator through touch screens. We analyzed how a voice driven interface can improve the trainer's interaction time efficiency and fluency with the simulator. Real training scenarios were analyzed and 12 representative tasks were chosen for this study. Time comparisons between the voice driven interface and two touch screen interfaces are reported. Twenty voice commands have been derived from the 12 tasks. The analysis of task completion time for touch screen is based on a model-based approach that relieves us from having users performing tasks with the interfaces, the KLM-GOMS model. Results show an average execution time gain of 33.8% using voice commands compared to touch screen commands. However, even though the majority of commands have faster input time for the voice activated interface, some are faster to enter through the touch screen, which suggests that an interface that allows both types of interaction mode might be best.	flight simulator;goms;run time (program lifecycle phase);simulation;speaker recognition;speech recognition;touchscreen	Joël Migneault;Jean-Marc Robert;Michel C. Desmarais;Sylvain Caron	2007	2007 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2007.4414002	embedded system;simulation;human–computer interaction;computer science;flight simulator;user interface	Visualization	-47.085802485678244	-47.04655346143268	39665
10d73bce99d9582a783b4ab40b64efacf4582e6e	a spatial immersive office environment for computer-supported collaborative work: moving towards the office of the future	multiprojector immersive environment office of the future multitouch surface virtual camera;prototypes;three dimensional displays hardware computational modeling rendering computer graphics real time systems cameras prototypes;computational modeling;three dimensional displays;rendering computer graphics;cameras;hardware;real time systems	In this paper, we present our work in building a prototype office environment for computer-supported collaborative work, that spatially — and auditorially — immerses the participants, as if the augmented and virtual generated environment was a true extension of the physical office. To realize this, we have integrated various hardware, computer vision and graphics technologies from either existing state-of-the-art, but mostly from knowledge and expertise in our research center. The fundamental components of such an office of the future, i.e. image-based modeling, rendering and spatial immersiveness, are illustrated together with surface computing and advanced audio processing, to go even beyond the original concept.	computer vision;computer-supported cooperative work;graphics;office of the future;partial template specialization;prototype;real-time transcription;surface computing	Maarten Dumont;Sammy Rogmans;Steven Maesen;Karel Frederix;Johannes Taelman;Philippe Bekaert	2011	Proceedings of the International Conference on Signal Processing and Multimedia Applications		computer vision;simulation;image-based modeling and rendering;computer science;prototype;multimedia;computational model;software rendering;computer graphics (images)	Visualization	-43.51879464348159	-36.36870856193704	39668
46983ab30bd287147ef9fb0a0c6d48788e05a438	talk and draw: bundling speech and graphics	speech synthesis;computer graphics;input output;aerospace computing;airborne warning and control system;user interfaces aerospace computer control aerospace computing computer graphics military computing speech recognition speech synthesis;speech recognition;man machine interface;system architecture;development issues airborne warning control system workstation voice i o man machine interface mmi prototype development vocabulary grammar itt vrs 1280 speech recognizer dec talker speech synthesizer system architecture graphical input awacs interface prototype;graphics prototypes speech analysis workstations user interfaces control systems performance analysis vocabulary speech recognition speech synthesis;user interfaces;military computing;aerospace computer control	A description is given of a workstation with integrated graphical and voice I/O (input/output), to improve the man-machine interface (MMI) of the Airborne Warning and Control System (AWACS). Incremental prototype development is used to improve the MMI for AWACS. Operator tasks were analyzed to identify functions that would be better performed with voice I/O, and a 70-word vocabulary and corresponding grammar were developed for an ITT VRS-1280 speech recognizer. Responses for a DEC Talker speech synthesizer were also developed. The system architecture for integrating voice and graphical input in the AWACS interface prototype and its implementation are described and development issues are examined. Future developments are considered.<<ETX>>	control system;finite-state machine;graphical user interface;graphics;input/output;prototype;speech recognition;speech synthesis;systems architecture;vocabulary;workstation	Mark W. Salisbury;Joseph H. Hendrickson;Terence L. Lammers;Caroline Fu;Scott A. Moody	1990	Computer	10.1109/2.56872	human–machine interface;natural language processing;input/output;speech recognition;computer science;operating system;programming language;computer graphics;user interface;speech synthesis	Robotics	-46.8130045001436	-27.062402722281004	39670
7bed93aa30916d960428580c421cee589aa0bac6	materialization of interactive stereoscopic artwork based on hand-painted images	stereoscopic art;interactive art;hand painting	This paper presents interactive stereoscopic artwork and an algorithm for natural artistic expression using hand-painted images expressed by the artist’s manual brush strokes. The system proposes a new interactive method that allows a viewer to experience the painting process representing the consecutive process of an actual artist’s oil painting. The combination of analog and digital techniques stimulates emotions of the audience. The system architecture is composed of the Kinect sensor, which recognizes the movement of the user, a module that generates real-time stereoscopic images, and a projection module that displays the generated image. The survey is conducted to evaluate the effects of the 3D modeling method and the artistic modeling method. The statistical result show that the proposed hand-painted method provides more artistic satisfaction to the viewers than the 3D modeling method.	3d modeling;algorithm;graphical user interface;interactive media;kinect;real-time clock;real-time locating system;semiotics;stereoscopic video game;stereoscopy;systems architecture	SangHun Nam;Youngeun Kim;YangMi Lim	2016	Multimedia Tools and Applications	10.1007/s11042-016-4235-z	computer vision;computer science;multimedia;computer graphics (images)	Graphics	-41.51103698834129	-35.9617670329013	39679
f19d120941de2c5435180be8b98ee8da16c85d50	android-based intelligent mobile robot for indoor healthcare	intelligent mobile robot android based intelligent mobile robot indoor healthcare multiple wireless communication internet wlan robot movement cloud speech recognition offline speech recognition technologies simple man machine conversation camera remote real time transmission video sound hardware compatibility sound hardware expansibility;wireless lan android operating system health care human robot interaction intelligent robots internet medical robotics mobile robots speech recognition;speech recognition cloud computing smart phones robot sensing systems mobile robots wireless communication	The intelligent mobile robot platform based on Android features multiple wireless communication functions, which, via Internet and WLAN, may control the movement of the robot. The platform integrates cloud speech recognition and offline speech recognition technologies to control the movement of the robot and have simple man-machine conversation. In addition, with a camera, this platform may realize the remote real-time transmission of video. Since the platform is characterized with sound hardware compatibility and expansibility, we may conduct the research of the robot and rapidly develop an intelligent mobile robot which applies to a specific application scenario on the platform.	android;mobile operating system;mobile robot;online and offline;real-time clock;speech recognition;vii	Yujun Ma;Dengming Xiao;Ran Li;Ruan Hang;Shan Zhao;Junlong Zhao;Yin Zhang	2015	2015 17th International Conference on E-health Networking, Application & Services (HealthCom)	10.1109/HealthCom.2015.7454548	mobile robot;embedded system;simulation;engineering;multimedia;ubiquitous robot;mobile robot navigation;personal robot	Robotics	-37.47259961318007	-43.12220114218084	39683
49a1cbd6bdebb685bacab27c132d7b33493f815b	audio and video extensions to graphical user interface toolkits	object oriented model;multimedia systems;multimedia data;graphic user interface	This paper describes audio and video extensions to graphical user interface (GUI) toolkits for multimedia systems. These extensions are based on a new object-oriented model for handling multimedia data. The introduction of temporal glue, and a mechanism for constructing composite multimedia data hierarchically in the model, make it quite easy to edit and reuse composite multimedia data. Programs created on the basis of this model are versatile in the range of their potential application and highly adaptable to changes in hardware. The model can help answer the need for toolkits that will ease the development of multimedia systems and the editing of multimedia data.	graphical user interface;list of toolkits	Rei Hamakawa;Hidekazu Sakagami;Jun Rekimoto	1992		10.1007/3-540-57183-3_39	user interface design;10-foot user interface;shell;human–computer interaction;computer science;operating system;graphical user interface;multimedia;natural user interface;user interface;world wide web;graphical user interface testing	HCI	-41.90873726112256	-30.407434350739276	39688
897ff5792276625141b888099b0bea683a0ca8b9	alternative wheelchair control involving intentional muscle contractions	hands free control;revolution speed controller;intelligent wheelchair;muscle contraction;intentional muscle contractions	This paper deals with various ways of controlling an electrically powered wheelchair beyond the usual method involving a manual joystick. The main focus is on the newest version of HaWCoS – the “HAnds-free Wheelchair COntrol System” – allowing persons with severe disabilities to reliably navigate a power wheelchair without the need to use the hands. All the user has to do is to produce a sequence of tiny contractions of an arbitrary muscle, e.g., by raising the eyebrow. The working prototype of the system, which has been realized as a stand-alone device, is introduced in detail, together with a closer look at the muscle-based control principle and a brief description of a PC-based simulator. The advantages and the drawbacks of the system are discussed on the basis of a rather simple real-life experiment. The paper also elaborates on possible approaches to improve HaWCoS (by reducing or eliminating its problems) in the future. In addition to a quick software solution and a controller implementation involving supplemental sensory information, planned “improvements” include the development of an “intelligent wheelchair” with HaWCoS being some sort of a prototype for the User Interface component.	autonomous robot;computer simulation;content-control software;contraction mapping;control system;control unit;countermeasure (computer);experiment;input device;interference (communication);joystick;laptop;microsoft windows;nl (complexity);nan;numerical aperture;overhead (computing);piezoelectricity;prototype;real life;robustness (computer science);sensor;user interface;wheels	Torsten Felzer;Bruno Strah;Rainer Nordmann;Sebastian Miglietta	2009	International Journal on Artificial Intelligence Tools	10.1142/S0218213009000226	embedded system;simulation	Robotics	-40.76997856223529	-45.00128485271875	39713
b846cbb52e8b28a71d3ff75e1ccf18707a1370ca	domain-adaptive conversational agent with two-stage dialogue management	lenguaje natural;interfase usuario;lenguaje documental;multiagent system;usability testing;agent based;user interface;langage naturel;intelligence artificielle;langage documentaire;pattern matching;agent intelligent;natural language;inferencia;intelligent agent;conversational agent;utilisabilite;information language;artificial intelligence;interface utilisateur;agente inteligente;inteligencia artificial;concordance forme;intencion;usabilidad;sistema multiagente;usability;intention;dialogue manager;inference;systeme multiagent	The conversational agent understands and provides users with proper information based on natural language. Conventional agents based on pattern matching have much restriction to manage various types of real dialogues and to improve the answering performance. For the effective construction of conversational agents, we propose a domain-adaptive conversational agent that infers the user’s intention with two-stage inference and incrementally improves the answering performance through a learning dialogue. We can confirm the usefulness of the proposed method with examples and usability tests.	dialog system;natural language;pattern matching;usability testing	Jin-Hyuk Hong;Sung-Bae Cho	2004		10.1007/978-3-540-30549-1_115	natural language processing;usability;computer science;artificial intelligence;intelligent agent	AI	-36.756718858463245	-25.966301199009244	39749
6d5ac8a2a7380451ea00b7eaad85c3d5ede74923	interaction, usability and aesthetics: what influences users' preferences?	usability evaluation;engagement;metaphors;interaction;interaction style;website design;design evaluation;aesthetics;information quality;person perception	In this paper we describe an evaluation of two websites with the same content but different interface styles (traditional menu-based and interactive metaphors). A formative usability evaluation was carried out with heuristic assessment of aesthetics, and questionnaire assessment of aesthetics, content, information quality, usability and post-test memory. The study revealed that perception of information quality is affected by the interaction style implemented in the interface, in a manner resembling the halo effect in person perception. Implications for website design and evaluation are discussed.	heuristic;information quality;usability;web design	Antonella De Angeli;Alistair G. Sutcliffe;Jan Hartmann	2006		10.1145/1142405.1142446	pluralistic walkthrough;web usability;cognitive walkthrough;interaction;usability;human–computer interaction;computer science;engineering;usability engineering;multimedia;information quality;heuristic evaluation;usability lab;social perception	HCI	-59.67702663126665	-46.66037642396868	39788
0354ef0342cb4e2e6b006e57303f1a65b4e8f01c	on abstraction in scenarios	system engineering;costs and benefits;use case	ing to text throws away essentially all the dimensions of sensory experience, such as sight, sound, touch, smell, taste, acceleration; all subjective impressions like surprise, fear, pleasure, complexity, boredom; and all dimensions of space. Even time is reduced to a symbolic indication of sequence, by some device such as numbering. Without becoming too philosophical, we seem to be discarding a dozen dimensions here, replacing them all with a stream of symbols encoded in compressed form with the strange metaphors of natural language. Think, for example, what pictures are being referred to when we say ‘interrogate the user’, ‘enter a PIN number’, ‘capture a requirement’ – these verbs may conjure up images of vigorous questioning, going into a house, grabbing a hostage. Text has many virtues, among them familiarity, being a shared medium, expressiveness, flexibility and brevity; but it is both imprecise and skimpy on detail. Some of text’s odd properties can be understood by remembering that it is a representation of speech – reading a text out loud means decoding it and translating it into speech again. Speech has evolved to carry meaning across a very narrow and noisy channel – air full of other sounds – to people with different intentions from ourselves. It is also designed to be persuasive, so that we can get other people to share our point of view, make plans together and then act together. 3. Why the Right Amount of Abstraction Matters Text is inherently less persuasive than speech because it lacks the natural non-verbal accompaniments to speech: body language, gesture, the urgency of the shared situation, facial expression, tone of voice and so on. In partial compensation, book designers have at their disposal an artificial range of non-verbal ways to express tone, style and importance, such as choice of typeface, font size, boldness, line spacing and page layout. I observe that we engineers seem to rarely consider whether visual devices might not subtly influence the readers of our scenarios, though Potts [8] argues convincingly for the power of visual metaphor. Our usual attitude is, I imagine, that it is hard enough to get the basic facts on to paper, let alone worrying about the non-verbals. But the comments I get back from clients about, say, Use Case Models that I’ve asked them to review are quite often like ‘The numbering makes it hard to understand’, or ‘What does a number like UC-123 mean?’ or ‘Where do I start?’ In other words, they don’t find the actual scenarios difficult, but the presentation matters enormously. As they say, you only get one chance to make a first impression. Creating effective scenarios is a skill and practical knowledge, which as Heron argues [9] must be based on understanding, that is, on propositional knowledge which can be written down. That in turn is based on presentational knowledge, which can be communicated in drawing, storytelling, drama, dance and imagery. That On Abstraction in Scenarios 253	heron;imagine: babyz;natural language;noisy channel model;noisy-channel coding theorem;numbering (computability theory);point of view (computer hardware company);potts model	Ian F. Alexander	2002	Requirements Engineering	10.1007/s766-002-8404-8	use case;simulation;systems engineering;engineering;knowledge management;cost–benefit analysis;software engineering	HCI	-56.69974534366631	-28.8841754972545	39814
9e3c51bdf0e902f0aba8d9a3bf0f232dfe325ba2	exploring the challenges of making data physical	tangible user interface;shape changing display;data physicalization;information visualisation	Physical representations of data have existed for thousands of years. However, it is only now that advances in digital fabrication, actuated tangible interfaces, and shape-changing displays can support the emerging area of 'Data Physicalization' [6]: the study of computer-supported, physical representations of data and their support for cognition, communication, learning, problem solving and decision making. As physical artifacts, data physicalizations can tap more deeply into our perceptual exploration skills than classical computer setups, while their dynamic physicality alleviates some of the main drawbacks of static artifacts by facilitating their crafting, supporting adaptation to different data, and encouraging sharing between different users.	cognition;digital modeling and fabrication;physicalization;problem solving	Jason Alexander;Yvonne Jansen;Kasper Hornbæk;Johan Kildal;Abhijit Karnik	2015		10.1145/2702613.2702659	simulation;information visualization;human–computer interaction;computer science;multimedia	HCI	-59.23470354402709	-36.90920160859531	39849
7beae53c6572acf7f7804e00c1158ae9df1f5bf8	designing noticeable bricklets by tracking users' eye movements	viewing behavior;visual hierarchy;images of faces;web pages;green products;bepress selected works;prototypes;prototypes visualization image color analysis tracking green products web pages;user centered design;web pages user eye movements websites visual hierarchy eye tracking;attention;fixations;visualization;web design;time to first fixation user centered design visual hierarchy eye tracking viewing behavior;time to first fixation;image color analysis;usability user experience eye tracking web design attention images of faces fixations news pages;user experience;eye movement;news pages;information design;eye tracking;usability;tracking	To be successful, websites must not only contain useful information, but provide that information in a quickly and easily accessible manner. One method of delivering this experience is to design websites that effectively guide users' attention to key information on the page. Grounded in the model of visual hierarchy, this study examines several attributes that can affect users' attention to key information. Using an eye tracking device, users' eye movements were recorded while completing tasks on web pages. The results provide partial support for the model of visual hierarchy and indicate that tracking users' eye movement is an effective method for informing design.	effective method;eye tracking;tracking system;visual hierarchy;web page	Soussan Djamasbi;Marisa Siegel;Thomas S. Tullis	2012	2012 45th Hawaii International Conference on System Sciences	10.1109/HICSS.2012.200	computer vision;user experience design;user-centered design;visualization;attention;usability;web design;eye tracking;computer science;web page;prototype;tracking;multimedia;information design;world wide web;fixation;eye movement;computer graphics (images)	HCI	-35.0857191556407	-49.20287805527783	40052
5b87749cba67f750a91a5dcf689c906261f9cacb	accessible optical wireless pedestrian-support systems for individuals with visual impairment	weak eyesight;pedestrians assisted living handicapped aids optical communication;optical transmitters;legged locomotion;prototypes;light emitting diodes;pedestrian support;receivers;image color analysis;weak eyesight bollard pedestrian support visible light communication;bollard;visible light communication;image color analysis modulation prototypes optical transmitters light emitting diodes legged locomotion receivers;temperature 2000 k optical wireless pedestrian support system visible light communication self illuminated bollards luminescent color 4ppm modulation format visually impaired people;modulation	We construct a pedestrian support system using visible-light communication with self-illuminated bollards. The system is designed for the requirements of people with weak eyesight and the elderly to notify them of danger and obstacles such as steps and signals. The luminescent color that the color temperature of 2000 K, that is orange, is suitable for communication. And we demonstrate prototype models of visible-light communication with 4PPM modulation format. Moreover, we carry out evaluation experiments with visually impaired people to determine the best distance for danger notification.	a dark room;experiment;modulation;optical wireless;prototype;requirement	Hirotoshi Kii;Yusuke Murata;Saeko Oshiba;Yuki Nagai;Hiroki Watanabe;Shunsuke Iki;Yoji Kitani;Noriaki Kuwahara;Kazunari Morimoto	2014	2014 IIAI 3rd International Conference on Advanced Applied Informatics	10.1109/IIAI-AAI.2014.156	computer vision;telecommunications;engineering;optics	Robotics	-40.70434924355645	-42.837216204565756	40057
b949e11a3e36e997b84e0dc5889fd092d975473d	interactive gaming reduces experimental pain with or without a head mounted display	analgesia;electric stimulation;positive emotion;gaming;virtual reality;emotion;electrical stimulation;virtual reality environment;head mounted display	While virtual reality environments have been shown to reduce pain, the precise mechanism that produces the pain attenuating effect has not been established. It has been suggested that it may be the ability to command attentional resources with the use of head mounted displays (HMDs) or the interactivity of the environment. Two experiments compared participants’ pain ratings to high and low levels of electrical stimulation while engaging in interactive gaming with an HMD. In the first, gaming with the HMD was compared to a positive emotion induction condition; and in the second experiment the HMD was compared to a condition in which the game was projected onto a wall. Interactive gaming significantly reduced numerical ratings of painful stimuli when compared to the baseline and affect condition. However, when the two gaming conditions were directly compared, they equally reduced participants’ pain ratings. These data are consistent with past research showing that interactive gaming can attenuate experimentally induced pain and its effects are comparable whether presented in a head mounted display or projected on a wall.	backward induction;baseline (configuration management);experiment;functional electrical stimulation;head-mounted display;interactivity;numerical analysis;virtual reality	Nakia S. Gordon;Junaid Merchant;Catherine A. Zanbaka;Larry F. Hodges;Paula Goolkasian	2011	Computers in Human Behavior	10.1016/j.chb.2011.06.006	psychology;simulation;emotion;computer science;virtual reality;multimedia;communication;social psychology	HCI	-47.79236616555339	-50.09980333149326	40073
754e686ee5196fb71186ef1cb2863a68fd89d159	first-person view animation editing utilizing video see-through augmented reality	art;virtual reality;cave2;storytelling;digital humanities;interactive installation	In making 3D animation with traditional method, we usually edit 3D objects in 3-dimension space on the screen; therefore, we have to use input devices to edit and to observe 3D models. However, those processes can be improved. With the improvement in gesture recognition nowadays, virtual information operations are no longer confined to the mouse and keyboard. We can use the recognized gestures to apply to difficult operations in editing model motion. And for observing 3D model, we would use head tracking from external devices to improve it. It would be easy to observe the interactive results without complicated operation because the system will accurately map the real world head movements.	3d modeling;augmented reality;computer animation;gesture recognition;input device;motion capture	Liang-Chen Wu;Jia-Ye Li;Yu-Hsuan Huang;Ming Ouhyoung	2015		10.1145/2787626.2787656	computer vision;digital humanities;simulation;computer science;virtual reality;multimedia;computer graphics (images)	HCI	-41.133321994069576	-36.82265298078158	40085
277ae5b56a25782d51bc0a40026e70329d5476d5	the landscape's apprentice: lessons for place-centred design from grounding documentary	felt life;spatiality;habitus;rural;spatial infrastructure;traditional knowledge;conservation;indexicality;user experience;indexation;indigenous people;video;fire;use case	We propose that grounding documentaries can help designers to respond to non-western, non-urban spatial infrastructures. We describe locally-produced, in vivo video methods developed by indigenous Elders in Australia to persist and transfer their Traditional Knowledge and the specific use-case of a documentary on fire. The culturally-situated nature of the documentary exposes subtleties in a dialectic between models of space. The ontology embedded in the methods, and expressed by the documentary, has a spatiality and a belonging to place that profoundly differs from that typifying HCI's urban focus and many video methods used by designers to understand useage contexts. Grass-roots driven documentaries ground subsequent design by engaging designers in otherwise inaccessible truths about remote places, partly through the designer's sense of their own felt-life. The fire documentary reveals many general insights for design, such as the need to escape a singularly anthropocentric spatio-temporal approach in order to respond to the plurality of user experience.	embedded system;human–computer interaction;situated;user experience;video-in video-out	Nicola J. Bidwell;Peta-Marie Standley;Tommy George;Vicus Steffensen	2008		10.1145/1394445.1394455	use case;habitus;user experience design;traditional knowledge;simulation;video;indexicality;conservation;human–computer interaction;computer science;engineering;artificial intelligence;fire;multimedia;management;rural area;mechanical engineering	HCI	-53.54702890592202	-32.25259108367315	40099
3e03db18d1b55e1ac086f84c900082f81b5faef2	parvai — hvs aware adaptive display power management for mobile games	energy conservation;organic light emitting diodes;kwaak3 parvai hvs aware adaptive display power management mobile games human visual system aware algorithms oled display power consumption reduction organic light emitting diode quake iii;green products;smart phones;led displays;image color analysis games organic light emitting diodes smart phones power demand green products brightness;brightness;power aware computing;smart phones computer games energy conservation led displays organic light emitting diodes power aware computing power consumption;image color analysis;games;power consumption;computer games;power demand	Displays are one of the major system energy consumers in smartphones. Power consumption of Organic Light-Emitting Diode (OLED) displays have direct relationship with the colour and intensity of the contents being displayed. Games are one of the most popular smartphone applications that consume relatively more resources and power. In this paper, we present Human Visual System (HVS) aware algorithms and techniques to reduce OLED display power consumption up to 45% while retaining the perceived quality of game content. We have implemented and evaluated our solution in the game Quake III and its Android port Kwaak3.	algorithm;android;diode;framebuffer;human visual system model;mobile app;mobile game;oled;power management;real-time transcription;smartphone;virtual world	Bhojan Anand;Li Kecen;Akkihebbal L. Ananda	2014	2014 Seventh International Conference on Mobile Computing and Ubiquitous Networking (ICMU)	10.1109/ICMU.2014.6799052	embedded system;simulation;engineering;multimedia	Mobile	-48.84218655856839	-39.27017129990764	40133
8b53369ce7f9e7ecfe6b1565d8288be5b0f988bd	an overview of interval research corporation	interval research corporation	This short paper describes the background, philosophy, organization, staff, and business model of Interval Research Corporation. Several of Interval’s research directions are briefly presented, including field ethnography, media manipulation, immersive narratives, cultural play, and virtual communities.	immersive design;virtual community	David E. Liddle;Meg Withgott;Debby Hindus	1994		10.1145/259963.260194	human–computer interaction;computer science;corporation	DB	-52.98735765896037	-25.50848067791834	40140
679c0bf88670b640e2e88b60438dca174c841c0f	summary of the second international workshop on network and operating system support for digital audio and video	operating system	On November 18 and 19, 1991, 60 researchers participated in the Second International Workshop on Net work and Operating System Support for Digital Audio and Video, held in cooperation with AC M SIGCOMM and SIGOPS at the IBM European Networking Center in Heidelberg, Germany . This worksho p was the second in a series started in November 1990 at the International Computer Science Institute (ICSI ) and the University of California at Berkeley. It brought together again researchers in networks and operating systems to discuss the needs of multimedia applications and how they may be satisfied . The trend towards powerful workstations and high-speed networks has enabled applications to communicate and manipulate digital audio and video (continuous media) . These new media differ from traditional discrete media such as text and graphics in that they have stringent delay and bandwidth requirements . The mechanisms used to transport ordinary data over networks and many of today's communication protocol s are insufficient to communicate continuous media . Special operating system support must also be provide d to meet the requirements of both discrete and continuous media in future multimedia applications . The workshop dealt with virtually all system aspects of multimedia support . Among the topics discusse d were :	bsd;communications protocol;computer science;graphics;new media;operating system;requirement	Ralf G. Herrtwich	1992	Computer Communication Review	10.1145/141800.141807	embedded system;real-time computing;human–computer interaction;computer science	Arch	-47.776735338039174	-25.391693353845483	40146
9e62d7054aedf6f126a0d8bb626da63224cfe473	morphing tactile display for haptic interaction in vehicles		This work describes the design, fabrication and characterization of a morphing (actuated) tactile display based on an array of 32 electromagnetic actuators and a capacitive touch deformable layer. A two-stage locking device allows for reliable pattern creation when the users’ finger is placed on the interface. Taxel (tactile pixel) displacements up to 1.9 mm and a holding force of 1.25 N at 9 W electrical power have been measured. This device integrated in the centre console of vehicles is designed to work in combination with a LCD display mounted in the dashboard to control secondary vehicle systems.	haptic technology;morphing	Christian Bolzmacher;Gerard Chalubert;Olivier Brelaud;Jean-Philippe Alexander;Moustapha Hafez	2014		10.1007/978-3-662-44193-0_42	capacitive sensing;pixel;dashboard (business);computer hardware;haptic technology;liquid-crystal display;morphing;actuator;computer science	HCI	-42.496548631744176	-40.622532965190786	40151
779e1f112a1e4decf285d2114d8d1f614fb6d4e5	rightontime: the role of timing and unobtrusiveness in behavior change support systems	interruptions;behavior change;user experience;timing strategies;persuasive systems design;unobtrusiveness;persuasive systems	Influencing people's behavior by means of technology is achievable under many technological guises from websites to mobile devices to activity trackers, making them practically ubiquitous. The timing of persuasive messages has been found to be influential in itself, but the omnipresence of modern computing also raises questions about the effects of randomly timed interruptions and perceived obtrusiveness. The presented research is an explorative experiment regarding the unobtrusiveness of a behavior change support system. The study compares two timing strategies: random timing and user-defined timing for system interaction. While the results of the mixed ANOVA analyses in this pilot study did not yield statistically significant results between the timing strategies, the correlations found do seem to indicate that random interruptions are perceived as more obtrusive than user-timed interaction. Furthermore, a correlation was observed between task success, task satisfaction and perceived unobtrusiveness.		Piiastiina Tikka;Harri Oinas-Kukkonen	2016		10.1007/978-3-319-31510-2_28	psychology;user experience design;simulation;computer science;behavior change;psychotherapist;communication;social psychology	HCI	-59.55751568969	-46.042729931708365	40156
139fb96a687f9855c0281477301782b99c5c4596	hiding spaces: a cave of elusive immateriality	visual simulation;natural phenomena;virtual worlds	Hiding Spaces is an immersive VR Cave artwork which pushes past the limitations of physical media by exploring the new ambiguities that can delight the viewer in the virtual world. By using innovative tools developed especially for creative work within the Cave environment, in combination with more established digital methods and artistic practice, the authors collaborated to produce a work which transgresses the usual borders of 2D and 3D, including those that are common even in VR environments.	cave story;spaces;virtual world	Cynthia Beth Rubin;Daniel F. Keefe	2002		10.1145/1242073.1242205	computer science;metaverse;multimedia;computer graphics (images)	HCI	-54.712309989517635	-29.616931871342363	40171
3e62578edca3ec93530ef0fcef4968d2eff3db71	a taxonomy of strategies for multimodal persuasive message generation	human computer interaction;preventive medicine;social relation;beliefs desires and intentions;intelligent system;social action;social psychology;domain specificity;emotional expression	Future intelligent systems will have contextual goals to pursue. As opposed to more traditional scenarios of human computer interaction, intelligent persuasive systems may also aim to induce the user or, in general, the audience, to perform some actions in the real world. Some scenarios of application are dynamic advertisement, preventive medicine, social action, and edutainment. As a step in this direction, a prototype called Promoter was developed for the production of persuasive messages. In modeling persuasion, the cognitive state of the participants (beliefs, desires, and intentions) is taken into account, as well as their social relations, their emotions, and the context of interaction. In this article, a taxonomy of persuasive strategies and the meta-reasoning model that works on this taxonomy is described. The taxonomy is built by taking into consideration studies coming both from social psychology and philosophy, and from the area of natural argumentation. The taxonomy is not domain specific and it helps to bridge persuasion strategies and rhetorical relations, a fundamental element in text planning. The use of this taxonomy also permits reasoning on the basis of emotion expression in accordance with persuasion strategies for multimodal message generation.	multimodal interaction	Marco Guerini;Oliviero Stock;Massimo Zancanaro	2007	Applied Artificial Intelligence	10.1080/08839510601117169	social relation;emotional expression;knowledge management;artificial intelligence;persuasive technology	AI	-53.20404468229151	-46.71153427628348	40177
a667e4748ef2f7d7b05becf589f1bf97926bbc4d	pinocchio: conducting a virtual symphony orchestra	ewatch;motor skills;neural networks;virtual symphony orchestra;device independence;qa75 electronic computers computer science;wii remote;conducting;3d video;classical music;gesture recognition;3d audio;neural network;driver assistance system	We present a system that allows users of any skill to conduct a virtual orchestra. Tempo and volume of the orchestra's performance are influenced with a baton. Pinocchio works with several types of batons, differing in tracking method and in algorithms for gesture recognition. The virtual orchestra can be configured, allowing the muting, hiding and positioning of individual musicians or instrument groups in 3D space. The audio and video material is based on a professional recording session with the Bavarian symphony orchestra. Pinocchio's long-term goal is the creation of a multi-modal, device independent framework for gesture-based applications which require motor skills or the control and operation of a complex set of sensors in intelligent house or car driver assistance systems. In this paper, we describe the current development status of the project, detail its usage and finally give an overview over our future project goals.	algorithm;baton;device independence;gesture recognition;home automation;modal logic;sensor;symphony	Bernd Brügge;Christoph Teschner;Peter Lachenmaier;Eva Fenzl;Dominik Schmidt;Simon Bierbaum	2007		10.1145/1255047.1255132	simulation;engineering;multimedia;communication	HCI	-46.85553882584724	-36.97039703114792	40187
f7301e01a4f4867aa97bdc469402e2d7095ba50b	a lightweight 3d visualization and navigation system on handheld devices	frames per second;visibility algorithms;3d visualization;embedded system;mobile phone;handheld devices;graphical application;handheld device;navigation system;3d graphics;octrees	This work presents a lightweight 3D visualization and navigation system we have proposed and implemented on handheld devices, using the Open Graphics Library for Embedded Systems (OpenGL ES API). The visibility algorithms view-frustum culling, backface culling (this one available in the OpenGL ES API), and a combination of view-frustum culling and backface culling, associated to different depth levels of Octrees (used to partition the 3D scene) were implemented and used to optimize the processing time required to render 3D graphics. The system was then tested using these combinations of algorithms and performance analyses were conducted for situations where the camera walks through an environment containing 6199 polygons. The results show that navigation at interactive rates of 10.07 and 30.61 frames per second can be obtained using the Pock-etPC iPaq hx2490b and the mobile phone Nokia n82, respectively.	3d computer graphics;algorithm;application programming interface;back-face culling;embedded system;frustum;glossary of computer graphics;graphics library;hidden surface determination;ipaq;mobile device;mobile phone;opengl es	Wendel Bezerra Silva;Maria Andréia F. Rodrigues	2009		10.1145/1529282.1529318	embedded system;computer hardware;computer science;operating system;mobile device;3d computer graphics;computer graphics (images)	Visualization	-38.9524216628239	-39.76764243374916	40257
5cfc7795003f249691e91b70795372188e3c223c	fast hands-free writing by gaze direction	arithmetic coding;language model	We describe a method for text entry based on inverse arithmetic coding that relies on gaze direction and which is faster and more accurate than using an on-screen keyboard. These benefits are derived from two innovations: the writing task is matched to the capabilities of the eye, and a language model is used to make predictable words and phrases easier to write. For people who cannot use a standard keyboard or mouse, the direction of gaze is one of the few ways of conveying information to a computer. Many systems for gaze-controlled text entry provide an onscreen keyboard whose buttons are ‘pressed’ by staring at them. But eyes did not evolve to push buttons, and this method of writing is exhausting. Moreover, on-screen keyboards are inefficient because typical text has considerable redundancy.1 Although a partial solution to this defect is to include word-completion buttons as alternative buttons alongside the keyboard, a language model’s predictions can be better integrated into the writing process. By inverting an efficient method for text compression – arithmetic coding2 – we have created an efficient method for text entry, one that is also well matched to the eye’s natural talent for search and navigation. One way to write a piece of text is to go into the library that contains all possible books,3 and find the book that contains exactly that text. Writing thus becomes a navigational task. In our idealized library, the ‘books’ are arranged alphabetically on one enormous shelf. As soon as the user looks at a part of the shelf, the view zooms in continuously on the point of gaze. To write a message that begins ‘hello’, one first steers towards the section of the shelf marked h, where all the books beginning with h are found. Within this section are sections for books beginning ha, hb, hc, etc.; one enters the he section, then the hel section within it, and so forth. To make the writing process efficient we use a language model, which predicts the probability of each letter’s occurring in a given context, to allocate the shelf-space for each letter of the alphabet (figure 1a). When the language model’s predictions are accurate, many successive characters can be selected by a single gesture. We previously evaluated this system, which we call Dasher, with a mouse as the steering device.4 Novices rapidly learned to write and an expert could write at 34 words per minute. All users made fewer errors than when using a standard qwerty keyboard. Figure 1b shows an evaluation of Dasher driven by an eyetracker, compared with an on-screen keyboard. After an hour of practice, Dasher users could write at up to 25 words per minute, whereas on-	arithmetic coding;book;dasher;data compression;eye tracking;language model;mega city (the matrix);push-button;software bug;virtual keyboard;words per minute	David J. Ward;David J. C. MacKay	2002	CoRR		arithmetic coding;language model	HCI	-48.91680231153323	-43.85451816098856	40263
46ef30f4706513333d9801286e176982d6a73a9a	tv series and social media: powerful engagement factors in mobile video games				Fernando de Rada;Asunción Mochón;Yago Saez	2018	IJIMAI	10.9781/ijimai.2018.11.002	multimedia;data mining;social media;computer science	HCI	-53.92884549647131	-33.35313480587246	40307
98495033d72ca4810edac3c1e78b735ce156f9da	xml3d physics: declarative physics simulation for the web	categories and subject descriptors according to acm ccs i 3 6 computer graphics methodology and techniques standards		declarative programming;simulation;world wide web	Kristian Sons;Philipp Slusallek	2011		10.2312/PE/vriphys/vriphys11/055-063	computer science;theoretical computer science;programming language	Theory	-41.996981012231636	-27.036803166385816	40361
62ce8113e8e94600f15aa61e617545cffbfb76b5	an instructable office clerk	graphical user interfaces office automation software agents knowledge based systems business data processing generalisation artificial intelligence learning by example;postal services information systems computer science educational robots human robot interaction workstations operating systems problem solving manuals concrete;software agents;graphical user interfaces;learning by example;x window system;business data processing;knowledge based system instructable office clerk clerk office automation software agent generalization playback record user commentary demonstration learning by example x window system;generalisation artificial intelligence;office automation;knowledge based systems	The paper reports our progress o n building a n automated o f i ce Clerk agent f o r a n instructable system that can record, generalize, and play back tasks demonstrated by a user. The user is able to provide a simple, helpful commentary t o a demonstration, and the Clerk learns f rom both. The Clerk is implemented in the X window system within a general purpose record/playback tool.	x window system	Pierre Baril;István Hernádi;Bruce A. MacDonald	1995		10.1109/ANNES.1995.499479	simulation;human–computer interaction;computer science;artificial intelligence;software agent;knowledge-based systems;machine learning;graphical user interface	AI	-33.77474862540458	-38.440402347030776	40365
1915959e7ba07ac9912a9cb9a147ec6135dea987	designing interfaces for multiple-goal environments: experimental insights from in-vehicle speech interfaces	driving;user engagement;selected works;driving simulator;interface design;task prioritization;in vehicle speech interfaces;multiple voices;text to speech;bepress;text complexity;distraction;e mailing	Designing computer-human interfaces for multiple-goal environments is challenging because people pursue multiple goals with conflicting priorities. Safety-critical environments, such as driving, aggravate the need for a more nuanced understanding of interfaces that may reconcile conflicting tasks. Speech interfaces are prime examples of such interfaces. In this article, we investigate how design variations of an in-vehicle speech interface influence performance of a primary task (driving safely) and a secondary task (e-mailing). In a controlled experiment, we test the performance implications of using single computer-generated Text-To-Speech (TTS) voice and multiple matching TTS voices while users respond to e-mails with varying levels of complexity during driving. Our results indicate that the number of voices used has a significant effect on both driving performance and handling e-mail--related activities. We discuss potentially unintended consequences of making the interface too naturalistic and too engaging for the driver and conclude with theoretical and practical implications.	computer-generated holography;driving simulator;email;netware file system;speech synthesis;unintended consequences	Sergej Truschin;Michael Schermann;Suparna Goswami;Helmut Krcmar	2014	ACM Trans. Comput.-Hum. Interact.	10.1145/2544066	simulation;human–computer interaction;computer science;interface design;multimedia;speech synthesis	HCI	-48.423379942617146	-46.96242074851932	40371
2bacf3666b7cc12753b1e21b9b2ce8cb401baf00	evaluation of a web-based and mobile ski touring application for gps-enabled smartphones		In the project “TourGuide”, an integrated web and mobile information system for hiking tours and ski tours was developed and evaluated with a group of test users. The web portal provides detailed information about tours and offers functionality to search, explore and discover tours that suit a user’s individual needs. While the web portal enables users to plan a trip from home, the mobile client application offers a range of dedicated location based services and navigation instructions that support users while on the move. The mobile client application was developed for Java based smartphones and includes a map-viewer component that displays a user’s current location, the selected tour, as well as multimedia information about objects of interest along the route. Besides gaining valuable information about how the usability of the system may be improved, the results of the evaluation show that integrated solutions can lead to a high level of user satisfaction.	global positioning system;smartphone	Elisabeth Haid;Günter Kiechle;Nicolas Göll;Martin Soutschek	2008		10.1007/978-3-211-77280-5_28	simulation;mobile web;multimedia;advertising	Mobile	-51.110630902675986	-40.90000173699532	40476
fe4f42def304dc563335ca47afe1a771857ca5d0	extracting 3d objects from photographs using 3-sweep		We introduce an interactive technique to extract and manipulate simple 3D shapes in a single photograph. Such extraction requires an understanding of the shape's components, their projections, and their relationships. These cognitive tasks are simple for humans, but particularly difficult for automatic algorithms. Thus, our approach combines the cognitive abilities of humans with the computational accuracy of the machine to create a simple modeling tool. In our interface, the human draws three strokes over the photograph to generate a 3D component that snaps to the outline of the shape. Each stroke defines one dimension of the component. Such human assistance implicitly segments a complex object into its components, and positions them in space. The computer reshapes the component to fit the image of the object in the photograph as well as to satisfy various inferred geometric constraints between components imposed by a global 3D structure. We show that this intelligent interactive modeling tool provides the means to create editable 3D parts quickly. Once the 3D object has been extracted, it can be quickly edited and placed back into photos or 3D scenes, permitting object-driven photo editing tasks which are impossible to perform in image-space.	algorithm;cognition	Tao Chen;Zhe Zhu;Shi-Min Hu;Daniel Cohen-Or;Ariel Shamir	2016	Commun. ACM	10.1145/3007175	computer vision;simulation;computer science;computer graphics (images)	Graphics	-38.256007340053095	-33.08065801683721	40493
61ba88ccba1bc17ffd040f4a4ae92ae5b5953335	enhancing task classification in human-machine collaborative teleoperation systems by real-time evaluation of an agreement criterion	virtual assistant;interactive forces;suboptimal assistance;human machine collaborative teleoperation system;hidden markov model;erroneous task classification;real time;humans hidden markov models transportation haptic interfaces teleoperators safety force;teleoperators;force;hmm based classifier;human operator human machine collaborative teleoperation system virtual assistant hmm based classifier agreement criterion interactive forces suboptimal assistance erroneous task classification unsuitable assistance;telerobotics man machine systems pattern classification task analysis;hidden markov models;task analysis;transportation;safety;human operator;pattern classification;telerobotics;humans;haptic interfaces;agreement criterion;man machine systems;unsuitable assistance;haptic interface	Human-machine collaborative teleoperation systems were introduced to overcome limitations of state-of-the-art teleoperation systems by using a virtual assistant that supports the human operator in the execution of a task. Since assistances are highly task-dependent a correct classification of the currently performed task is paramount. In this paper, we present a novel approach for improving task classification for a human-machine collaborative teleoperation system. Starting from a classical HMM-based classifier implemented in our previous research, we introduce a method for correcting erroneous task classifications by evaluating an agreement criterion. This criterion is based on interactive forces and is used to distinguish between situations in which human and assistant agree/disagree in their execution of the task. Using disagreement as indicator for the activation of an unsuitable/suboptimal assistance, erroneous task classifications are identified and the original classification result is revised. The proposed approach shows significant improvements in task classification coming along with a comparable low implementation effort.	fundamental interaction;hidden markov model;multi-user;real-time clock;usability testing	Carolina Passenberg;Nikolay Stefanov;Angelika Peer;Martin Buss	2011	2011 IEEE World Haptics Conference	10.1109/WHC.2011.5945535	telerobotics;computer vision;transport;simulation;computer science;artificial intelligence;task analysis;haptic technology;force;hidden markov model	Robotics	-37.070362737635875	-45.00002562294312	40517
8ba86f29ef51b4e786e652449ae449fa68a625ee	making the case for query-by-voice with echoquery	natural language interfaces;data exploration	Recent advances in automatic speech recognition and natural language processing have led to a new generation of robust voice-based interfaces. Yet, there is very little work on using voice-based interfaces to query database systems. In fact, one might even wonder who in her right mind would want to query a database system using voice commands! With this demonstration, we make the case for querying database systems using a voice-based interface, a new querying and interaction paradigm we call Query-by-Voice (QbV). We will demonstrate the practicality and utility of QbV for relational DBMSs using a using a proof-of-concept system called EchoQuery. To achieve a smooth and intuitive interaction, the query interface of EchoQuery is inspired by casual human-to-human conversations. Our demo will show that voice-based interfaces present an intuitive means of querying and consuming data in a database. It will also highlight the unique advantages of QbV over the more traditional approaches, text-based or visual interfaces, for applications where context switching is too expensive, too risky or even not possible at all.	context switch;database;natural language processing;programming paradigm;speech recognition;text-based (computing)	Gabriel Lyons;Vinh Tran;Carsten Binnig;Ugur Çetintemel;Tim Kraska	2016		10.1145/2882903.2899394	natural language processing;computer science;database;programming language	DB	-50.392194119711135	-44.28197768953301	40532
90413f65ac5335d11a1a545034b33d189d091beb	controllable water particle display	three dimensional;three dimensional scanning display;water drop;volumetric display	"""We propose the new concept,"""" Controllable particle display"""", there is no display in space and the space itself is a display. Moreover realizing the concept, we also propose a system of using a set of water drops designed to form a plane surface and developed a prototype system. In this paper, for long-term use, we improved this system to have the enough strength with standing long-term use."""	prototype	Shin'ichiro Eitoku;Kotaro Hashimoto;Tomohiro Tanikawa	2006		10.1145/1178823.1178867	computer vision;simulation;volumetric display;engineering;computer graphics (images)	HCI	-42.37493417708653	-39.85593068286854	40593
dfaca0245b5b6a24e7331b3df611d2be9fb31029	robot compliant behaviour with mixed-initiative interaction in an obstacle avoidance scenario		Walking is considered extremely important for elderly individuals who need to maintain mobility and cope with the decline of functional capacity. Based on a previous feasibility study, we designed a humanoid walking trainer for this specific category of users. In this paper, we present and test the motion control system of our robot which is guided by the human user, though it takes initiative in case of obstacle and suggests how to avoid it through head gaze and speed change. This first experiment aims at observing the moment of switching control between the human user and the robot, and how the user interprets these social cues exhibited by the robot.	obstacle avoidance;robot	Chiara Piezzo;Bruno Leme;Masakazu Hirokawa;Kenji Suzuki	2017		10.1007/978-3-319-70022-9_71	social cue;gaze;robot;motion control;obstacle;human–computer interaction;compliant behaviour;obstacle avoidance;computer science;trainer	Robotics	-46.03589865186103	-50.58195036086637	40596
b616ed1d404231b348a997b60850dcae5c2f802e	perceptually transparent vibration rendering using a vibration motor for haptic interaction	human computer interaction;vibrations;virtual reality;internal structure;haptic interfaces rendering computer graphics actuators virtual reality vibration control frequency size control voltage control psychology prototypes;input output;human factors;prototype graphical vibration editor perceptually transparent vibration rendering method vibration motor haptic interaction virtual reality human computer interaction computer gaming absolute magnitude estimation paradigm inverse i o relation user percept;magnitude estimation;haptic interfaces;rendering computer graphics;vibrations haptic interfaces human factors rendering computer graphics;haptic interaction	The vibration motor is an actuator widely used for generating vibrations in haptic interaction, virtual reality, human-computer interaction, and gaming. Whereas the vibration motor has many advantages such as low price and small size, its internal structure brings a critical limitation: its output has correlated amplitude and frequency both controlled by voltage applied to the motor. Using the absolute magnitude estimation paradigm in psychophysics, we have previously obtained an Input-Output (I/O) relation from the applied voltage to vibration magnitude perceived by the user. Based on its inverse I/O relation, this paper addresses a vibration rendering method that allows the user to command the vibration motor in a perceptually transparent manner. Using the rendering method, the effects of vibration commands (in voltage) on the user's percept can be correctly understood, allowing more effective vibration design for haptic interaction. We have also implemented a prototype graphical vibration editor into which the perceptually transparent vibration rendering method is incorporated. The vibration editor can assist the user to easily design vibration effects for the vibration motor as s/he manipulates sounds with a graphical audio editor and to clearly predict the user's percept induced from the resulting vibrations as well.	audio engineer;graphical user interface;haptic technology;human–computer interaction;input/output;programming paradigm;prototype;virtual reality	Jonghyun Ryu;Jaehoon Jung;Seojoon Kim;Seungmoon Choi	2007	RO-MAN 2007 - The 16th IEEE International Symposium on Robot and Human Interactive Communication	10.1109/ROMAN.2007.4415100	input/output;computer vision;simulation;computer science;vibration;virtual reality;computer graphics (images)	Visualization	-42.80863723896949	-47.89327241867528	40640
e12d5e2de32e7dfd2916ecbf49729f35be7f27b4	definition and evaluation of an interaction model for three-dimensional interface		We have elaborated a new 3D virtual workspace for distant meetings. 2D/3D documents can be integrated. The interaction must be generic so that the users can manipulate all documents in the same way. From real-time computer graphics techniques and users’abilities in a 3D virtual environment, we have defined a model on interaction together with the spatial organisation of the workspace. We emphasize the interaction based on two-handed direct manipulation, the use of two 3D input devices, simple metaphors, suggestive visual cues. Three experimentation tested our model. The first concerned the structure of the visual field and motor performances. The results show the latter is influenced by the visual context. The second one studies what are the relevant perceptive hints to enhance the pointing task performance according to the kind of input device used. The results show that isotonic devices are superior and that shadows are helpful to guide the action. The last one investigates the effect of shape of the shadow. The results suggest that the shape shadow was processing as a action's affordance. A 3D INTERACTION MODEL We define navigation as changes in the user's point of view. Interaction refers to how the user acts in the scene: the user manipulates objects without changing his overall point of view of the scene. Navigation and interaction are intrinsically linked; in order to interact with the interface the user has to be able to move within the interface. Unfortunately, the existence of a third dimension creates new problems with positioning and with user orientation (Hinckley, 1994); these need to be dealt with in order to avoid disorienting the user. This is especially true for our interface, where the main objective is not to navigate within the interface, but rather to act on the interface. This entails designing a coordinate frame where navigation within a restricted space is adequate and easy. With an 3D isotonic input device (Zhai, 1998) like AscensionTM trackers, translation of the dominant hand's movement is immediately reflected in the interface by the pointer (figure 1). In a real life situation, users cannot go search for documents or tools without getting up from the table. With our room metaphor, the user does not have to navigate to find objects, he can select them directly with the pointer, it can be moved throughout the entire meeting room. Although the appropriate input device is available to the user he may still lose his pointer when moving around in the interface. There are several ways of dealing with this problem. First, pointer orientation is used to indicate any change in direction and to enhance the impression of movement. Secondly, we use shading effects and the pointer's shadow is projected onto the floor. This helps the user to perceive meeting room depth accurately and to get his bearings quickly and easily (Kersten, 1997). Figure 1: pointer on an H20 molecule, with progressive bounding-boxes surrounding objects Our model uses visual cues to show that an object has been selected or that it can be selected (figure 1): a graphical representation of a box appears progressively around the object. The closer the pointer is to the object, the more the surrounding box is visible and closer to the object. This progressive bounding box system greatly simplifies the manipulation of the pointer. Once an object is selected, the user may want to manipulate it. In order to maintain direct manipulation and to avoid widgets, we use an isometric device, a 3D trackball, in the non-dominant hand to apply actions to the object (like rotation, etc.). So, our model uses bimanual interaction (Kabbash, 1994), because the user has more interaction possibilities and it simulates reality. It should reduce dominant hand movement and thus increase precision for object manipulation (with the isometric device).	direct manipulation interface;function pointer;graphical user interface;input device;interaction;isometric projection;isotonic regression;minimum bounding box;music tracker;performance;pointer (computer programming);pointing device;real life;real-time computer graphics;real-time locating system;shading;trackball;virtual reality;workspace	Cédric Dumas;Patricia Plénacoste;Catherine Demarey	1999				HCI	-42.759058912205745	-39.64056520306533	40720
7d4591bdbcda10f3eba2fa0c76453bf7b3973365	flippable user interfaces for internationalization	institutional repositories;dynamic change;fedora;user interface;arabic language;direct manipulation;user interface layout;vital;internationalization;left to right;adaptation;graphic user interface;vtls;reading direction;cultural background;ils	The language reading direction is probably one of the most determinant factors influencing the successful internationalization of graphical user interfaces, beyond their mere translation. Western languages are read from left to right and top to bottom, while Arabic languages and Hebrew are read from right to left and top to bottom, and Oriental languages are read from top to bottom. In order to address this challenge, we introduce flippable user interfaces that enable the end user to change the reading direction of a graphical user interface by flipping it into the desired reading direction by direct manipulation. This operation automatically and dynamically changes the user interface layout based on a generalized concept of reading direction and translates it according to the end user's preferences.	direct manipulation interface;graphical user interface;right-to-left	Iyad Khaddam;Jean Vanderdonckt	2011		10.1145/1996461.1996524	human–computer interaction;computer science;multimedia;world wide web	HCI	-49.89153767487591	-44.015363935196916	40731
7344953e7189ef49eb3dade81a2bef7dd01cdf08	intact: instant interaction with 3d printed objects		The INTACT platform enables the instant addition of interactivity to any 3D printed object, with the aim of enriching and enhancing the interaction design process by eliminating the need to incorporate potentially obstructive sensor technologies. Using a system of force sensing combined with a digital model of the object; the system is capable of detecting a single touch, its position and the corresponding force of that touch anywhere on the object. In order to aid and encourage other practitioners to use such technology we demonstrate how to build such a system and describe two illustrative examples.	3d printing;interaction design;interactivity;sensor	Charles Hudin;Sabrina A. Panëels;Steven Strachan	2016		10.1145/2851581.2892351	computer vision;simulation;human–computer interaction;3d printing;computer science;interaction design;multimedia	HCI	-44.921441469553656	-39.39730510627194	40830
932d6f1275741aa10b6d0e1bb1d19baf34e65034	image-based lightweight tree modeling	tree;rule based;image based modeling;large scale;3d model;lightweight modeling;vrml;l system	This paper presents a novel lightweight tree modeling approach for constructing large scale online virtual forestry on Web. It firstly recovers 3D skeleton of the visible trunk from two source images of a tree, then extracts the rules and parameters of tree L-system from the recovered skeleton, and parses the parametric L-system into very lightweight tree Web3D files. Comparing with rule based tree modeling methods e.g. L-system and AMAP, our method is more convenient for users without requiring botany expertise. Furthermore, our method inherits the merits of both image based tree modeling and rules based tree modeling. Comparing with such 3D modelers as 3DMAX and MAYA, our method is more efficient and economical for users to avoid their heavily manual modeling labors. More important, it can generate very lightweight Web3D tree files even with 1K-2K, which are photorealistic in shape and structure, Experimental results show that the feasibility and perspective of our proposed method in WebVR applications.	amap;autodesk 3ds max;l-system;web3d;webvr	Ruoxi Sun;Jinyuan Jia;Hongyu Li;Marc Jaeger	2009		10.1145/1670252.1670258	rule-based system;simulation;vrml;computer science;artificial intelligence;theoretical computer science;incremental decision tree;data mining;l-system;fractal tree index;tree;k-d-b-tree;computer graphics (images)	Robotics	-37.507537817038475	-32.85444923192356	40912
1254dd0089d7d34eed24da42ef55ade3f9b08bf6	'sketchy wives' and 'funny heroines' - doing and undoing gender in art games		Gender analysis of video games has increased its public visibility through the Gamergate controversy. We examine several casual art games in order to explore the diversity of both conventional and counter-stereotypical gender representations. We find significant reliance on stereotypical presentations, especially in ‘sketchy wife’ characters. Such tropes may offer rhetorical resources to communicate, in brief lapses of gameplay, messages about life, death and the human condition. We also find creative ways of tackling gender displays through character description and game mechanics. Art games may thus serve as a laboratory for experimenting with doing and possibly un-doing gender.	experiment;game mechanics;gamergate controversy	Cosima Rughinis;Elisabeta Toma	2015		10.1007/978-3-319-20916-6_59		HCI	-60.71440417101835	-32.77256053357202	40943
4614257ea857b36444197e6d236bb744ff252d0b	research environment for developing and testing object tracking algorithms	interfaces;user interface;detection and tracking algorithms;computer programming;visualization;prototyping;inertial navigation systems;object tracking;algorithms;video	We present an integrated research environment (RAVEN) that we have developed for the purpose of developing and testing object tracking algorithms. As a Windows application, RAVEN provides a user interface for loading and viewing video sequences and interacting with the segmentation and object tracking algorithms, which are included at run time as plug-ins. The plug-ins interact with RAVEN via a programming interface, enabling algorithm developers to concentrate on their ideas rather than on the user interface. Over the past two years, RAVEN has greatly enhanced the productivity of our researchers, enabling them to create a variety of new algorithms and extend RAVEN’s capabilities via plug-ins. Examples include several object tracking algorithms, a live-wire segmentation algorithm, a methodology for the evaluation of segmentation quality, and even a mediaprocessor implementation of an object tracker. After implementing an algorithm, RAVEN makes it easy to present the results since it provides several mask display modes and output options for both image and video. We have found that RAVEN facilitates the entire research process, from prototyping an algorithm to visualization of the results to a mediaprocessor implementation.	algorithm;application programming interface;computer display standard;encoder;interaction;matlab;media processor;microsoft windows;plug-in (computing);programming tool;prototype;run time (program lifecycle phase);scripting language;user interface	Todd N. Schoepflin;Christopher Lau;Rohit Garg;Donglok Kim;Yongmin Kim	2000		10.1117/12.411845	computer vision;simulation;video;visualization;computer science;video tracking;computer programming;user interface;computer graphics (images)	HCI	-44.62615704886945	-29.727857930040468	40996
f2a9c8cbcd5155d32e8096df72a5453127022158	narrative paradox and the design of alternate reality games (args) and blogs	cross media narratives and alternate reality games args;interactive fiction;hyperfiction alternate reality game blogs narrative paradox interactive fiction pervasive game;real time;virtual reality;data mining;media;virtual reality computer games;media space;games;hyperfiction;pervasive game;narrative paradox;alternate reality game;interactive environment;blogs space technology environmental factors industrial relations toy industry art cybernetics linearity cultural differences environmental economics;cross media narratives and alternate reality games args pervasive games playable media;computer games;blogs;context;playable media;films;real time systems;pervasive games	This article explores the narrative paradox and how we can design interactive fictions where actions and narrations can contribute for a more meaningful experience within an interactive setting. In this paper we argue that pervasive and alternate reality games (ARGs) and blogs, where participants can cooperate and compete in the real application of tactics and strategies as they play the game online and offline, can contribute to create a more engaging environment where players are able to build their own stories. Player made content and hyperfiction are useful tools to better understand alternate reality games and blogs. The analysis of playable fiction can contribute to recombine action and narration in digital ecologies and we may consider that implosive stories, in which everything happens simultaneously, present in digital spaces allow us to better understand the problem of interactive environments that use mixed technologies. We consider that ARGs are inclusive spaces where gamers can learn how to deal with different media, different communities and different genders. In this mix media spaces gamers can play with action and narrative in order to design their own fictions and stories in real time. In this article we consider some ARGs, I'm Trying to Believe, I Love Bees, Uncle Roy All Around You, among others, in order to argue that this persistent games can be useful to better understand the narrative paradox.	alternate reality game;blog;epr paradox;ecology;hypertext fiction;love me or hate me;media space;online and offline;pervasive informatics;real-time computing	Patrícia Gouveia	2009	2009 International IEEE Consumer Electronics Society's Games Innovations Conference	10.1109/ICEGIC.2009.5293585	simulation;computer science;multimedia;advertising	HCI	-58.16499402346054	-33.4400341641161	41001
88aab5cd7e6a595dd188d8d7b5a2451dd79dfec7	ptz control with head tracking for video chat	ptz control;user interface;user study;panning;head tracking;hci;vision based interaction;zooming;tilting;field of view;wide field of view video chat	This paper describes a user interface for video chat that is capable of panning, tilting, and zooming (PTZ) operation using head tracking. The approach is to map a captured 3D position from head tracker to PTZ parameters of a remote camera so that a user can intuitively change the view just as people change their sight by moving their head. The preliminary user study gave encouraging results and clarified the point for further improvement.	motion capture;pan–tilt–zoom camera;usability testing;user interface	Kota Yamaguchi;Takashi Komuro;Masatoshi Ishikawa	2009		10.1145/1520340.1520594	computer vision;field of view;computer science;operating system;multimedia;panning;user interface;computer graphics (images)	HCI	-43.91153656524352	-40.91951487757802	41164
2f7283cd99efb42bb8252b114ca62ee0dbf4d934	functionality preserving shape style transfer	shape synthesis;style analysis;style vs function	When geometric models with a desired combination of style and functionality are not available, they currently need to be created manually. We facilitate algorithmic synthesis of 3D models of man-made shapes which combines user-specified style, described via an exemplar shape, and functionality, encoded by a functionally different target shape. Our method automatically transfers the style of the exemplar to the target, creating the desired combination. The main challenge in performing cross-functional style transfer is to implicitly separate an object's style from its function: while stylistically the output shapes should be as close as possible to the exemplar, their original functionality and structure, as encoded by the target, should be strictly preserved. Recent literature point to the presence of similarly shaped, salient geometric elements as a main indicator of stylistic similarity between 3D shapes. We therefore transfer the exemplar style to the target via a sequence of element-level operations. We allow only compatible operations, ones that do not affect the target functionality. To this end, we introduce a cross-structural element compatibility metric that estimates the impact of each operation on the edited shape. Our metric is based on the global context and coarse geometry of evaluated elements, and is trained on databases of 3D objects. We use this metric to cast style transfer as a tabu search, which incrementally updates the target shape using compatible operations, progressively increasing its style similarity to the exemplar while strictly maintaining its functionality at each step. We evaluate our framework across a range of man-made objects including furniture, light fixtures, and tableware, and perform a number of user studies confirming that it produces convincing outputs combining the desired style and function.	3d modeling;algorithm;database;generative model;high-level synthesis;holographic principle;interaction;natural language;structural element;tabu search;usability testing;virtual fixture	Zhaoliang Lun;Evangelos Kalogerakis;Rui Wang;Alla Sheffer	2016	ACM Trans. Graph.	10.1145/2980179.2980237	computer vision;artificial intelligence;machine learning	Graphics	-37.04093310805159	-34.27311191307019	41184
d6006c3c8b45967add897ce40e5032add9242495	ideas about vr&ar as a new genre in fine arts	fine arts;future;aesthetic theory;digital art;virtual and augmented reality;new media	Virtual and augmented reality is a specific part of computer arts. Computer art is a wide term, which subsumes a few branches that have found their artistic expression, like Software art, Net art, Algorithmic art. Does a VR&AR art exist? VR&AR come from an artistic deflection of a technology. Combination of immersion, interactivity and virtual world gives opportunity for specific expressions of artistic themes like mixing of illusion and truth, blurring borders between virtuality and reality, imaginary and realistic representations, questioning nature, the world, the relationship between humans and their power, contemporaries techniques and modes of perceptions, and typically the uncanny valley limit.	algorithmic art;augmented reality;imaginary time;immersion (virtual reality);interactivity;internet art;uncanny valley;virtual world;virtuality (gaming)	Suzanne Beer;Judith Guez	2013		10.1145/2466816.2466834	visual arts;art methodology;art;digital art;mixed reality;multimedia;computer graphics (images)	Visualization	-53.9486530368268	-29.395294626805747	41213
963005010bdd5f635435b90259743eda8bb9be81	navigating in 3d space with a handheld flexible device		Abstract   Prototypes for handheld, flexible devices are becoming popular in the research community. We explore opportunities in the domain of mobile gaming with flexible devices, by focusing on deformable inputs to control navigation in 3D virtual environments. We compare two sets of bend gestures to control a first person camera in a 3D maze, one inspired by console game controllers, and the other inspired by PC game controls (i.e. mouse and keyboard). Our results shows that users prefer the set inspired by the console controller: moving forward and backwards mapped to the top left corner, turning to the top right corner, and strafing to the bottom right corner. This results in lower wall collisions and an overall better user experience. We propose design recommendations to create deformable game controls in 3D spaces.	handheld game console	Meagan Leflar;Audrey Girouard	2014	Entertainment Computing	10.1016/j.entcom.2014.07.002	simulation;human–computer interaction;operating system;multimedia;computer graphics (images)	HCI	-45.26616785931423	-41.68255227155834	41221
418881782c95c13b961de4d215991b7a4e3415c0	the social media ecology: user perceptions, strategies and challenges	boundary management;media ecology;content sharing;social media	"""Many existing studies of social media focus on only one platform, but the reality of users' lived experiences is that most users incorporate multiple platforms into their communication practices in order to access the people and networks they desire to influence. In order to better understand how people make sharing decisions across multiple sites, we asked our participants (N=29) to categorize all modes of communication they used, with the goal of surfacing their mental models about managing sharing across platforms. Our interview data suggest that people simultaneously consider """"audience"""" and """"content"""" when sharing and these needs sometimes compete with one another; that they have the strong desire to both maintain boundaries between platforms as well as allowing content and audience to permeate across these boundaries; and that they strive to stabilize their own communication ecosystem yet need to respond to changes necessitated by the emergence of new tools, practices, and contacts. We unpack the implications of these tensions and suggest future design possibilities."""	categorization;ecosystem;emergence;experience;information needs;media ecology;mental model;social media	Xuan Zhao;Cliff Lampe;Nicole B. Ellison	2016		10.1145/2858036.2858333	social media;human–computer interaction;computer science;knowledge management;media ecology;world wide web	HCI	-58.66573650638723	-38.528212221485894	41258
727cc441155b87c6e052e4231ea2e611d62c8e1e	the use of character sets and character mappings in icon				Ralph E. Griswold	1980	Comput. J.	10.1093/comjnl/23.2.107	theoretical computer science;character encoding;computer science;icon	Theory	-44.832888767685425	-30.094498765015548	41276
7e4a7d2bea1b5958685a8f69d9924416eb52e6e3	conception and realization of a multi-sensory interactive mobile office guide	speech synthesis system multisensory interactive mobile office guide modified pioneer 2 robot vision based multi person tracker carmen bidirectional video conference system;image processing;speech synthesis;mobile robot;video conference;mobile robots;teleworking mobile robots machine vision speech synthesis robot vision systems human robot interaction videoconference image processing computer vision collision avoidance;robot vision;system integration;speech synthesis interactive systems mobile robots sensor fusion office environment robot vision;sensor fusion;interactive systems;office environment	This paper describes the conception and realization of a multi-sensory interactive mobile office guide. We designed a mobile robot based on a modified pioneer-2 robot, which is able to welcome visitors in our department and guide them to the desired staff member. The main components of the system are a vision based multi-person-tracker and the existing navigation toolkit CARMEN. Furthermore, the robot provides a bidirectional video conference system and a speech synthesis system. Experimental results show, that the implemented multi-person-tracker is accurately able to track an unknown number of persons in real-time and guide them to the respective people, while keeping an eye on the interaction partner.	mobile robot;real-time locating system;speech synthesis	Christian Martin;Hans-Joachim Böhme;Horst-Michael Groß	2004	2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)	10.1109/ICSMC.2004.1401047	mobile robot;computer vision;simulation;image processing;computer science;artificial intelligence;social robot;robot control;multimedia;speech synthesis;mobile robot navigation;personal robot	Robotics	-36.181668563593135	-42.227008203585974	41293
09e30cfd88480e974147525a68eb54dbbf4551d0	"""paving the way or pushing at open doors? a comment on abramo and d'angelo """"farewell to size-independent indicators"""""""		The article by Abramo and D’Angelo (in this issue; hereafter AA) is an interesting piece in the debate about indicators, ut it is a bit of a caricature. Firstly, a few words on the context. If we define scientometrics as the study of science actors and networks, and “output bibliometrics” as the particular etworks involving knowledge media, it is the nature of large disciplines with powerful conceptual frames, such as maintream economics and evolutionary economics (as it would be for the various strong streams of sociology), to take over ibliometrics as soon as they address technology and science. For economics, science output came later in the game than atents, since scientific knowledge was basically considered as a public good (among others Arrow, 1962) albeit gradually ppearing as less pure. Stephan for example, considers it as a mix of public goods in the information process and rival goods, restige and priority, in the scientific competition (Stephan, 2012). Once publication was recognized as a legitimate object, conomics thinking stimulated reflections not only on the cost side of research, but also on benefits and incentives of acaemic publication (on the value of publications and citations, a landmark is Diamond, 1986). More generally, economics of cience (as sociology of science) rightly claims science networks, the core of scientometrics, as important objects, with two mplications:	bibliometrics;computer performance;image scaling;recurrent neural network;reflection (computer graphics);scientometrics;soundness (interactive proof)	Michel Zitt	2016	J. Informetrics	10.1016/j.joi.2016.04.003		Theory	-60.18422879364299	-27.529735780213304	41321
3835cd2d81e58685ac721f8096ffc51dc81cd39d	sliders versus storyboards - investigating interaction design for mobile video browsing	mobile video browsing;interactive multimedia;mobile interfaces	We present a comparative study of two different interfaces for mo- bile video browsing on tablet devices following two basic concepts - storyboard designs representing a video's content in a grid-like arrangement of static im- ages extracted from the file, and slider interfaces enabling users to interactively skim a video's content at random speed and direction along the timeline. Our results confirm the usefulness and usability of both designs but do not suggest a clear benefit of either of them in the direct comparison, recommending - among other identified design issues - an interface integrating both concepts.	browsing;interaction design;storyboard	Wolfgang Hürst;Miklas Hoet	2015		10.1007/978-3-319-14442-9_11	computer science;multimedia;interactive media;world wide web;computer graphics (images)	HCI	-46.024299005605805	-41.3286325881838	41335
be497b933bc6434cf611e3103f86eb0b2ddbc87f	empathic robots for long-term interaction - evaluating social presence, engagement and perceived support in children		As a great number of robotic products are entering people’s lives, the question of how can they behave in order to sustain long-term interactions with users becomes increasingly more relevant. In this paper, we present an empathic model for social robots that aim to interact with children for extended periods of time. The application of this model to a scenario where a social robot plays chess with children is described. To evaluate the proposed model, we conducted a long-term study in an elementary school and measured children’s perception of social presence, engagement and social support.	interaction;social presence theory;social robot;social support	Iolanda Leite;Ginevra Castellano;André Pereira;Carlos Martinho;Ana Paiva	2014	I. J. Social Robotics	10.1007/s12369-014-0227-1	simulation	HCI	-53.41543889648972	-50.63627649281046	41355
199a164c354646a6215e1727e95466627614161a	survey on second screen systems		Today's consumers demand personalized experience for live events. The standard, single-for-all entertainment perspective fails to satisfy consumer expectations on broadcast of real-time information from various sources. Future entertainment environments should presume spectators equipped with mobile devices that can be involved in adapting to the entertainment and personal interests. This article surveys solutions with the ability to provide personalized perspectives to the audience. Such second screen applications may involve use of variety of devices, such as mobile phones equipped with WiFi, venue screens in hotel rooms, meeting halls, arenas, elevators, etc. Furthermore, functionality of such applications is considered regarding to interaction among with the remote spectators and the event.	acm international collegiate programming contest;internet access;mobile device;mobile phone;personalization;real-time data;second screen;venue (sound system);web application	Tomás Cerný;Michael J. Donahoo	2016	2016 6th International Conference on IT Convergence and Security (ICITCS)	10.1109/ICITCS.2016.7740374	simulation;engineering;multimedia;advertising	Mobile	-52.344774612246745	-39.9996055581033	41357
bea5b27ab3e8c609f74a0cdf37002d47647ef3b1	narrative use of sign language by a virtual character for the hearing impaired	virtual characters;sign language;hearing impaired	Abstract#R##N##R##N#This paper describes the concept and control of a 3d virtual character system with facial expressions and gesturesas a conversational user interface with narrative expressiveness for the hearing impaired. The gestures and facialexpressions are based on morphing techniques. The system allows the generation of sign language and mouthmotion in real time from text at the level of lip reading quality. The concept of Narrative Extended Speech Acts(NESA) is introduced, based on Interactive Storytelling techniques and the concepts of Narrative Conflict andSuspense Progression. We define a choice of annotation tags to be used with NESAs. We use the NESAs to classifyconversation fragments and to enhance computer generated sign language. We note, how the sign language gesturesare generated and show the possibilities for editing sign language gestures. Furthermore, we give details onhow the NESAs are mapped to gestures. We show the possibilities of controlling the virtual character's behaviourand gestures in a human-oriented way and provide an outlook on future work.#R##N##R##N##R##N##R##N#Categories and Subject Descriptors (according to ACM CCS): 1.3.6 [Computer Graphics]: Methodology and Techniques		Thomas Rieger;Norbert Braun	2003	Comput. Graph. Forum	10.1111/1467-8659.t01-2-00713	natural language processing;speech recognition;sign language;computer science;gesture	NLP	-43.46933412806664	-32.702998626226915	41377
85fd6116d74ad5283f7b9136cc07ceb4bcdeccb1	design and usability of digital libraries: case studies in the asia pacific	digital library	A separator electrolytic cell is disclosed which comprises a first electrode compartment surrounded with a releasable wall plate and a glove-shaped finger and a second electrode compartment surrounded with remaining cell walls, a cell top cover and the glove-shaped finger. A sheet-like separator is immediately installed to the cell without any peculiar processing and the cell facilitates assembly and disassembly and provides tight sealing.	digital library;library (computing);usability	Judy P. Bolstad	2007	JASIST	10.1002/asi.20500	digital library;computer science;multimedia;world wide web	PL	-42.31598697560033	-40.47477770692248	41397
4d356f347ab6647fb3e8ed8c2154dbd359e479ed	extracting and associating meta-features for understanding people’s emotional behaviour: face and speech	vocal and facial emotional expressions;mathematical models;communicative signals;speech and image processing	Emotion is a research area that has received much attention during the last 10 years, both in the context of speech synthesis, image understanding as well as in automatic speech recognition, interactive dialogues systems and wearable computing. There are promising studies on the emotional behaviour of people, mainly based on human observations. Only a few are based on automatic machine detection due to the lack of Information Technology and Engineering (ITE) techniques that can make available a deeper and large-scale noninvasive analysis and evaluation of people’s emotional behaviour and provide tools and support for helping them to overcome social barriers. The present paper reports a study for extracting and associating emotional meta-features to support the development of emotionally rich man–machine interfaces (interactive dialogue systems and intelligent avatars).	computer vision;dialog system;speech recognition;speech synthesis;user interface;wearable computer	Nikolaos G. Bourbakis;Anna Esposito;Despina Kavraki	2010	Cognitive Computation	10.1007/s12559-010-9072-1	psychology;speech recognition;mathematical model;communication;social psychology;statistics	HCI	-52.72989104873887	-46.37708132588682	41416
3a026eaedfb3e404cbb6a8e46de4bb301d76538a	a new proof-manager and graphic interface for the larch prover	management system;graphical interface;graphics system	We present plp, a proof management system and graphic interface for the Larch Prover (LP). The system provides additional support for interactive use of LP, by letting the user control the order in which goals are proved. We offer improved ways to investigate, compare and communicate proofs by allowing independent attempts at proving a goal, a better access to the information associated with goals and an additional script mechanism. All the features are accessible through a graphic system that makes the proof structure accessible to the user.		Frédéric Voisin	1997		10.1007/BFb0030648	shell;human–computer interaction;computer science;theoretical computer science;programming language;graphical user interface testing	HCI	-42.18868422882825	-29.940945336637007	41454
b53e293271b53df9c56a12ae1e524f2ec761dc8c	enhancing the restaurant dining experience with an nfc-enabled mobile user interface		We provide a field study of a smartphone application and server-side software for improving the on-premises restaurant experience. To increase customer engagement, the smartphone client uses Near-Field Communication (NFC) to bootstrap the communication with the server and to localize customers to specific tables. The application further pro- vides a complete end-to-end experience by allowing users to select food items, read micro-reviews of dishes, submit their order, and be alerted when the food is ready. We deployed our prototype system in a cafe-style restaurant in the dining commons of Stanford University and observed that the system was able to successfully streamline and enhance the din- ing experience for both customers and restaurant staff.	near field communication;user interface	Diego Argueta;Yu-Ta Lu;Jing Ma;Diego Rodriguez;Yuan-Hung Yang;Thomas Phan;Won Jong Jeon	2013		10.1007/978-3-319-05452-0_29	engineering;marketing;multimedia;advertising	SE	-52.50891512263144	-42.59813398495377	41504
3b56c30b6b880b4bffe31c3c9f967287e5089d56	technical and human engineering problems in connecting terminals to a time-sharing system	interactive use;user community;human engineering problem;increasing number;user performance;computer system;time-sharing system;system capability;interactive user;human engineering;human engineered user-system interface;non-interactive use;time sharing	Today, an increasing number of computer systems are used interactively by their user communities. Interactive use of computers, involving more prolonged man-machine contact than non-interactive use, requires a well human engineered user-system interface. The interactive user's performance---his rate of doing work and his ability and desire to utilize system capability---is a sensitive function of the success of this human engineering. In turn, the computer system's effectiveness depends on achieving a satisfactory level of user performance with reasonable efficiency.	computer;human factors and ergonomics;interactivity;time-sharing	Joseph F. Ossanna;Jerome H. Saltzer	1970		10.1145/1478462.1478513	simulation;interactive systems engineering;human–computer interaction;systems engineering;engineering;user interface	HCI	-45.45426151930313	-42.56102947657542	41522
0422a55e3e18e716fb44a6ab085f72b0f891be8b	virtual handcrafting: building virtual wood models using tooldevice	user performance virtual handcrafting virtual wood models tooldevice system 3d modeling software tweezersdevice knifedevice hammerdevice mixed reality 3d modeling system virtual objects wood materials;virtual reality;virtual reality solid modelling user interfaces;virtual reality augmented reality human computer interaction tangible device;user interfaces;solid modelling;augmented reality solid modeling computational modeling three dimensional displays virtual reality magnetic sensors light emitting diodes	Most 3-D modeling software are difficult for beginners to learn. The operations are often complicated, and the user is required to have prior mathematical knowledge. Therefore, we developed a simple modeling system using ToolDevice to simplify such operations. ToolDevice consists of a set of interaction devices, which use metaphors of real-life hand tools to help users recognize each device's unique functions. Using TweezersDevice, KnifeDevice, and HammerDevice, we developed a mixed reality (MR) 3-D modeling system that imitates real-life woodworking. In the system, TweezersDevice is used to pick up and move objects, while KnifeDevice and HammerDevice are, respectively, used to cut and join virtual objects represented as wood materials. In this study, we describe the motivation for developing the system, the available interactions, and the procedures for creating 3-D models in the system. We also present the results of a user study in which we compare user performance in our system and a common 3-D modeling software. Finally, we discuss the contributions and limitations of this study and future work.	3d modeling;interaction;mixed reality;real life;usability testing	Ryan Arisandi;Mai Otsuki;Asako Kimura;Fumihisa Shibata;Hideyuki Tamura	2014	Proceedings of the IEEE	10.1109/JPROC.2013.2294243	augmented reality;computer-mediated reality;simulation;human–computer interaction;computer science;engineering;instructional simulation;virtual reality;mixed reality;multimedia;user interface	HCI	-39.95691580805799	-34.91900352359954	41621
012f534785bac4c0bd9436fbc2db4e249422a828	hci meets material science: a literature review of morphing materials for the design of shape-changing interfaces		With the proliferation of flexible displays and the advances in smart materials, it is now possible to create interactive devices that are not only flexible but can reconfigure into any shape on demand. Several Human Computer Interaction (HCI) and robotics researchers have started designing, prototyping and evaluating shape-changing devices, realising, however, that this vision still requires many engineering challenges to be addressed. On the material science front, we need breakthroughs in stable and accessible materials to create novel, proof-of-concept devices. On the interactive devices side, we require a deeper appreciation for the material properties and an understanding of how exploiting material properties can provide affordances that unleash the human interactive potential. While these challenges are interesting for the respective research fields, we believe that the true power of shape-changing devices can be magnified by bringing together these communities. In this paper we therefore present a review of advances made in shape-changing materials and discuss their applications within an HCI context.	expectation propagation;flexible display;human computer;human–computer interaction;morphing;next-generation network;plan;robotics	Isabel P. S. Qamar;Rainer Groh;David Holman;Anne Roudaut	2018		10.1145/3173574.3173948	human–computer interaction;affordance;computer science;smart material;flexible display;morphing;robotics;artificial intelligence	HCI	-54.297162199378214	-36.05902478680788	41633
27a628923577329535c1092a0383dae3163b2eda	remote automated user testing: first steps toward a general-purpose tool	usability testing;user participation;data collection;user perception;laboratory tests;user testing;error rate;user behavior;field study	In this paper, we discuss our work towards self-healing property specification of an autonomic behavior in the Distributed Modular Audio Recognition Framework (DMARF) by using the Autonomic System Specification Language (ASSL). ASSL aids in enhancing DMARF with an autonomic middleware that enables it to perform in autonomous systems that theoretically require less-to-none human intervention. Here, we add an autonomic middleware layer to DMARF by specifying the core four stages of the DMARF’s pattern-recognition pipeline as autonomic elements managed by a distinct autonomic manager. We devise the algorithms corresponding to this specification.	algorithm;autonomic computing;autonomous system (internet);general-purpose modeling;middleware;pattern recognition;specification language;usability testing	Chandan Sarkar;Candace Soderston;Dmitri Klementiev;Eddy Bell	2010		10.1007/978-3-642-13273-5_3	simulation;user modeling;human–computer interaction;word error rate;computer science;operating system;world wide web;field research;data collection	SE	-43.384992955551276	-28.876182926935876	41653
fbc9356d3353f82fb2b010d369c365c5d17eaf45	racing to the finish line: effects challenges on cars 3		"""The world of Disney Pixar's Cars 3 finds our hero Lightning McQueen on a journey to reconnect to the roots of """"real"""" racing as he struggles to stage a comeback in a sport which is quickly evolving past him. Over the course of the film, our characters race on beaches alongside lapping waves, in abandoned ghost tracks, through winding mountain forests, and even in a raucus, muddy demolition derby. Providing a sense of believable interaction between our characters and these varied environments in over 600 shots was one of the key responsibilities of our FX team on Cars 3.  In order to achieve the scope and scale of this work efficiently, we built on sequence-wide workflows and independent """"clustered"""" simulations presented last year in (Reisch et al. 2016), extending these strategies to effects unique to the show. Creating a common shared core to our simulation and effects-asset rigs provided artists with a familiar starting point regardless of whether they were working on volumetric dust, rigid-body debris, point-based dynamics sand, or even viscous mud simulations. A focus on stability, artist experience, and optimized workflows which scaled to take advantage of our render farm, allowed our team to achieve visually consistent, high quality results on an accelerated schedule."""	display resolution;lightning (connector);list of amd fx microprocessors;render farm;simulation	Stephen Marshall;Tim Speltz;Greg Gladstone;Krzysztof Rost;Jon Reisch	2017		10.1145/3084363.3085160	demolition;computer graphics (images);rendering (computer graphics);computer vision;hero;artificial intelligence;render farm;computer science;workflow	HCI	-50.02424418533154	-29.511559533211784	41677
64d0eb1c37e5f9b8a8c41a72de9db3ba0a177d15	haptic carillon: a computationally enhanced mechanical performing instrument	physical modelling and music;musical force feedback;haptics;musical instruments;carillon design;carillon	This paper describes the development and user-testing of a model for emulating the haptic dynamics of a carillon, specifically the National Carillon in Canberra, Australia. The carillon is one of only a few instruments that elicit a sophisticated haptic response from the amateur and professional player alike. Force-feedback varies widely across the range of the instrument and developing an intuition for the heaviness of different bells is a critical part of carillon pedagogy. Unfortunately, rehearsal time available to individual carillonneurs is limited by competition from other carillonneurs and environmental factors like civic noise limits and carillon maintenance schedules. Rehearsal instruments do exist but they do not accurately display the haptic dynamics of the real carillon. Our device couples the notions of  entertainment  and  cultural  computing; while musical instruments are now regularly digitised for purposes of entertainment the haptic carillon is motivated by an awareness of the musicianship of carillonneurs and the public cultural space they inhabit with their instrument.	haptic technology	Mark Havryliv;Fazel Naghdy;Greg Schiemer;Timothy Hurd	2013	Entertainment Computing	10.1016/j.entcom.2012.10.003	simulation;human–computer interaction;computer science;artificial intelligence;multimedia;haptic technology;social psychology	AI	-57.29799765548023	-34.92644906455064	41683
e7105ea0b8fa80fea95d37f0ce21672f3432437e	multimedia interface design.	interface design	Interface design plays a vital role in the production of multimedia displays. To a large degree, the plan and layout of multimedia screens determines how visitors perceive the key ideas in the display as well as how to travel from one idea to the next. At its best, effective interface design will interest, engage and thrill a visitor. Effective design also integrates each multimedia element text, graphics, animation, video into a seamless whole where the information is clearly presented, where visitors have a chance to perform interesting activities, and where the visitors understand how to navigate through the interactive at a glance.	graphics;seamless3d	Tom Wujec	1993			user interface design;shell;network interface;interface control document;user interface	HCI	-49.227673701753744	-36.99609330228092	41707
ba0e9693dfa6004753b9d86a8c99165a4fe8c56b	a discussion of the role of user trails in web applications	web modelling;navigation pattern;navigation assistance;user trails	Designing, implementing and maintaining Web application is a challenging task. Moreover, driven by some of the characteristics of Web applications, such as multiculturalism, continuous change, fast pace and competitiveness, there is an increasing need to use mechanisms that automatically adapt Web applications to new environments.#R##N##R##N#Trails, built from information about the users' browsing paths and activities, are an establishment approach to assist users in navigating vast information spaces and finding appropriate information. In this article we investigate how Web applications can profit from the integration of the concept of user trails -- implemented as navigation pattern -- in the Web modelling process. Furthermore, we investigate how trails can be applied to the various catagories of Web applications. The results of our research show that trails are particularly suitable for those Web applications which exhibit a high degree of user interaction.	web application	Erich Gams;Siegfried H Reich	2006	J. Web Eng.		web service;web application security;web development;web modeling;simulation;data web;web analytics;web mapping;web-based simulation;web design;human–computer interaction;web accessibility initiative;web standards;computer science;web navigation;social semantic web;web intelligence;web engineering;world wide web;mashup	Web+IR	-42.38531878404218	-24.29939021906934	41778
b62b8fcbc8504e2a0b683347ebdbf0a4022e723d	the poetics of design fiction	poetics;design fiction;utopianism	Design fiction is an emergent field within HCI and interaction design the understanding of which ultimately relies, so we argue, of an integrative account of poetics and design praxis. In this paper we give such an account. Initially, a precise definition of design fiction is given by drawing on the theory of possible worlds found within poetics. Further, we offer a method of practicing design fiction, which relies on the equal integration of literary practice with design practice. The use of this method is demonstrated by 4 design projects from a workshop set up in collaboration with a Danish author. All of this substantiates our notion of a poetics of practicing design fiction, and through our critical examination of related work we conclude on how our approach contribute to HCI and interaction design.	accessibility relation;altran praxis;biological anthropology;emergence;futures and promises;human–computer interaction;indeterminacy in concurrent computation;interaction design;interconnection;network model;possible world	Thomas Markussen;Eva Knutz	2013		10.1145/2513506.2513531	psychology;computer science;engineering;fiction theory;sociology;poetics	HCI	-62.09699197146376	-31.03353277781234	41793
509d0e0eb5b7ca0359a3802033c62b643b9d41a0	crowd powered media delivery: facilitating ubiquitous device-to-device file transfers	interface design;smart irrigation;precision agriculture;developing countries	Device-to-device file transfers are pervasive in many emerging markets, but users typically only share content with close friends or informal media vendors. We seek to facilitate ubiquitous device-to-device file transfers beyond one's immediate social network. This extended abstract outlines the research challenges we need to address and our initial plans for addressing them.	pervasive informatics;social network	Colin Scott	2016		10.1145/3001913.3006640	engineering;internet privacy;world wide web;computer security	HCI	-55.40970697893101	-40.03216335848911	41855
70750f73c2a546b212b9105f384cf0839a3b99e5	sensing pollution on online social networks: a transportation perspective	sensors;social networks;transportation;traffic;smart objects;human perception	Transportation policy and planning strategies, as well as Intelligent Transportation Systems (ITS), can all play important roles in decreasing pollution levels and their negative effects. Interestingly, limited effort has been devoted to exploring the potential of social network analysis in such context. Social networks provide direct feedback from people and, hence, potentially valuable information. A post telling how a person feels about pollution at a given time at a given location, could be useful to policy-makers, planners or environmentally-aware ITS designers. This work verifies the feasibility of sensing air pollution from social networks and of integrating such information with real sensors feeds, unveiling how people advertise such phenomenon, acting themselves as smart objects, and how online posts relate to true pollution levels. This work explores a new dimension in pollution sensing for the benefit of environmental and transportation research in future smart cities, confronting over 1,500,000 posts and pollution readings obtained from governmental on-the-field sensors over a one-year span.	expect;heap pollution;observable;randomness;sensor;smart city;smart objects;social network analysis	Rita Tse;Yubin Xiao;Giovanni Pau;Serge Fdida;Marco Roccetti;Gustavo Marfia	2016	MONET	10.1007/s11036-016-0725-5	transport;simulation;telecommunications;sensor;perception;computer security;social network	HCI	-59.175435265869766	-29.195987364900354	41874
4d74a7d373a4280acc7e282a3406aaffe15e3342	designing accessible visualizations: the case of designing a weather map for blind users	visualizations;sonification;weather;spatial sound;multi sensory;universal usability;accessibility;auditory information seeking principle	Major strides have been made to improve the accessibility of text-based documents for blind users, however, visualizations still remain largely inaccessible. The AISP framework represents an attempt to streamline the design process by aligning the information seeking behaviors of a blind user with those of a sighted user utilizing auditory feedback. With the recent popularity of touch-based devices, and the overwhelming success of the talking tactile tablet, we therefore suggest that the AISP framework be extended to include the sense of touch. This research-in-progress paper proposes such an extended design framework, MISD. In addition, the article also presents the preliminary work done in designing an accessible weather map based on our theory-driven design. A discussion and an outline of future work conclude the manuscript.		Dustin Carroll;Suranjan Chakraborty;Jonathan Lazar	2013		10.1007/978-3-642-39188-0_47	simulation;human–computer interaction;engineering;multimedia	HCI	-55.045692723962084	-38.26294173676328	41903
f666f2ab00501b5de67657b57c8c3495b5602c19	outline of the mx standard	digital object;catalogue metadata;music symbol;different encoding format;mx standard;computer-driven performance;audio track;single music piece;comprehensively heterogeneous music content;heterogeneous content;single mx file	MX is a new XML-based format to describe comprehensively heterogeneous music contents. In a single MX file, music symbols, printed scores, audio tracks, computer-driven performances, catalogue metadata, text and graphic contents related to a single music piece are linked and mutually synchronised within the same framework. Heterogeneous contents are organised in a multilayered structure that supports different encoding formats and a number of digital objects for each layer. 1 A New Standard for Music MX, an acronym which stands for Musical application using XML, is the code-name for a new file format whose international standardisation is in progress. Its development follows the guidelines of IEEE P1599, Recommended Practice Dealing with Applications and Representations of Symbolic Music Information Using the XML Language. MX is the result of research efforts at the Laboratorio di Informatica Musicale, or LIM, of the Università degli Studi di Milano. P1599, the IEEE Standard, is sponsored by the Computer Society Standards Activity Board and was launched by the Technical Committee on Computer Generated Music (IEEE CS TC on CGM) [1]. In 2002, a prototypal version of the format was released, originally known as Musical Application using XML, or MAX [2]. This format was discussed at IEEE MAX 2002 international conference. The IEEE final evaluation process, known as balloting, is currently being performed, with the aim of making MX/P1599 an international standard. The most recent version is MX Release Candidate 1, whose DTD and documentation can be downloaded from http://www.mx.dico.unimi.it. Tools for music visualisation [3], content-based retrieval [4], and automatic segmentation [5] are currently available. 2 Key Features of MX MX is an XML-based format. There are many advantages in choosing XML to describe information in general, and music information in particular. For instance, an Outline of the MX Standard 197 XML-based language allows inherent readability, extensibility and durability. It is open, free, easy to read by humans and computers, and can be edited by common software applications. Moreover, it is strongly structured, it can be extended to support new notations and new music symbols, and it can thus become a means of interchange for music with software applications and over the Net. Most of these topics have been treated in [6] and [7]. A comprehensive description of music must support heterogeneous materials. Thanks to the intrinsic capability of XML to provide structures for information, such representations can be organised in an effective and efficient way. MX employs six different layers to represent information, as explained in [8] and shown in Figure 1: • General – music-related metadata, i.e. catalogue information about the piece; • Logic – the logical description of score symbols; • Structural – identification of music objects and their mutual relationships; • Notational – graphical representations of the score; • Performance – computer-based descriptions and executions of music according to performance languages; • Audio – digital or digitised recordings of the piece. Not all layers must, or can, be present for a given music piece. Of course, the higher their number, the richer the musical description. Richness has been mentioned in regard to the number of heterogeneous types of media description, namely symbolic, logic, audio, graphic, etc. But the philosophy of MX allows one extra step, namely that each layer can contain many digital instances. For example, the Audio layer could link to several audio tracks, and the Structural layer could provide many different analyses for the same piece. The concept of multilayered description – as many different types of descriptions as possible, all correlated and synchronised – together with the concept of multi-instance support – as many different media objects as possible for each layer – provide rich and flexible means for encoding music in all its aspects. It is possible to adopt some ad hoc encoding in addition to already existing formats to represent information. In fact, while a comprehensive format to represent music is not available, popular existing standards must be taken into account. This is a not a contradiction because of the two-sided approach of MX to music representation, which is: keep intrinsic music descriptions inside of the MX file – in XML format – and media objects outside of the MX file – in their original format. The symbols that belong to the score, such as chords and notes, are described in XML, in the Logic layer. On the contrary, MP3 files and other audio descriptions are not translated into XML format, rather they are linked and mapped inside the corresponding MX layer, the Audio layer. It should be clear that the description provided by an MX file is flexible and rich, both in regard to the number and to the type of media involved. In fact, thanks to this approach, a single file can contain one or more descriptions of the same music piece in each layer. For example, in the case of an operatic aria, the MX file could house: the catalogue metadata about the piece, its author(s) and genre; the corresponding portion of the libretto; scans of the original manuscript and of a number of printed scores; several audio files containing different performances; related iconographic contents, such as sketches, on-stage photographs, and playbills. Thanks to the	compact disc digital audio;computer;documentation;durability (database systems);extensibility;graphical user interface;hoc (programming language);ieee 1471;mp3;max;music visualization;performance;printing;software release life cycle;sound card;wai-aria;xml	Luca A. Ludovico	2007		10.1007/978-3-540-77051-0_21	computer science;database;multimedia;world wide web	DB	-46.67805082825895	-25.045994536615886	41904
17873bbebbb94560daea3699bf7aa3974ededfd8	programmable plaid: the search for seamless integration in fashion and technology.	smart textiles;plaid;image processing;wearable technology;fibre optics;wireless communication;wearable electronics;wearable computing;light emitting textiles	The work presented celebrates traditional tartan patterns and their design parameters, as well as the numerous technological achievements in woven textiles. The designers propose a woven textile (as applied in the garment presented) with the capability to illuminate threads in both warp and weft directions. The work described investigates the creation of garments for day-to-day use, which is a use-case probably never considered when developing off-the-shelf electronic components. In developing a new tartan pattern and fabrication technique, the authors respond to the lack of purpose-made electronic components by choosing to evolve tradition into technology. Manufacturing techniques and components for the proposed textile have been carefully considered, researched, and contrasted to previous work by the authors, as well as recent trends in the field of wearable technology. The garment is a component of a tartan pattern generating system which searches the space of all registered tartans in order to find relevant, but yet unregistered tartan color and thread count combinations from a variety of inputs either embedded on the dress, or from a mobile phone.	common gateway interface;database;electronic component;embedded system;limited availability;mobile phone;optical fiber;seamless3d;wearable technology	Elizabeth Esther Bigger;Luis Edgardo Fraguada	2016		10.1145/2968219.2971343	embedded system;simulation;wearable computer;telecommunications;computer science;optical fiber;wearable technology;wireless	EDA	-45.266941699493294	-37.884321897137326	41911
fe8e90cecfbeda2eb74ab342aff31c8e3b75c97b	enhancing the familiarity for humanoid robot pepper by adopting customizable motion		Most of the works use sensor to recognize human gesture and teleoperate robot in real time for different usage. One of the purpose is social learning which human learn behavior and take place in the social context. Our research aims to let user create customizable gesture and motion for proper condition. This paper presented the method to replicates human gesture for robot imitation. If robot can adapt human like behavior to interact with human, it is potentially helpful to build intimate between human and robot. Human gesture were recorded and unnecessary noise was removed to smooth robot's motion. Furthermore, by changing motion speed it is possible to create different feeling so that it might be arouse human's interest and spice up the interaction. The experiment results show the parameter pair to implement motion on robot.	humanoid robot;learning disorders;occur (action);population parameter;spice 2;spices	Wei Fen Hsieh;Eri Sato-Shimokawara;Toru Yamaguchi	2017	IECON 2017 - 43rd Annual Conference of the IEEE Industrial Electronics Society	10.1109/IECON.2017.8217492	social learning;simulation;control engineering;humanoid robot;social environment;robot;feeling;imitation;engineering;robot kinematics;gesture	Robotics	-50.5594899280673	-50.17539211209027	41950
46d391481b3ff02676748ee7179a111c64c8db66	conference preview: uist 2001: the 14th annual acm symposium on user interface software and technology		UIST is the premier forum for innovations in developing human-computer interfaces. The symposium brings together user-interface researchers and practitioners with an interest in techniques, tools, and technology for constructing high-quality, innovative user interfaces. The intimate size, single track, and comfortable surroundings make this symposium an ideal opportunity to exchange research results and implementation experiences. An exciting addition this year is the User Interface Design Contest, which will be held during the opening reception. Contestants will design and implement an interface to a real-time game application prior to the symposium , then use their interface to play the game in a tournament for prizes worth an estimated $2,500. The goal of the contest is to encourage participants to explore interface software and technology in an applied setting , and to provide an opportunity for participants to showcase their work to the UIST community in an exciting and entertaining format. The following is a preliminary program:	acm symposium on user interface software and technology;real-time transcription;user interface design	Marisa E. Campbell	2001	Interactions	10.1145/384076.384088	human–computer interaction;multimedia;software;engineering;user interface	Embedded	-50.86359602267333	-35.79835774975723	41954
6d3faeac9ca42d8dc543f7274c652adef7340c6d	please touch: object properties that invite touch	touch based properties and capabilities of the human user;psychology;surface texture;glass;purchasing;perception and psychophysics;visualization;microstructure;purchasing consumer behaviour haptic interfaces product design psychology;shape;cognition;touch based properties and capabilities of the human user cognition perception and psychophysics psychology;consumer behaviour;product design;correlation;haptic interfaces;shape microstructure visualization haptic interfaces correlation glass;microstructures;structural object attributes surface textures shape complexity stimulus factor specific hand movements touch ability ratings need for touch scale product design object properties marketing purchasing psychophysical literature;haptic interface	Touch has received increasing interest in marketing, given research indicating that contact with products influences evaluation and the tendency to purchase. However, little is known from the marketing or psychophysical literature about visible attributes of objects that elicit touch for hedonic purposes. In these studies, participants rated the tendency of pictured objects to invite touch, or “touch-ability.” Rated touch-ability varied reliably with structural attributes of objects, and the structural influences were distinct from those on other ratings such as attractiveness and apparent expense. Although the trends varied across object sets, touch-ability generally declined as surface textures became markedly rough and shape complexity became extreme. Holding stimulus factors constant, touch-ability also varied with the specific hand movements that were anticipated. Finally, mean touch-ability ratings were correlated across participants with the “Need for Touch” scale, which measures an individual's tendency to touch products. The studies point to touch-ability as a potential factor that might be incorporated into product design.	movement;physical object;rating (action)	Roberta L. Klatzky;Joann Peck	2012	IEEE Transactions on Haptics	10.1109/TOH.2011.54	computer vision;microstructure;computer science;product design	Visualization	-43.42588984810949	-51.403526281047085	41966
ab8fabc028006176fb55c8de25061a08f6be7250	embedding interface sketches in code	design tool;development;user interface;ascii art;development environment;visual representation;design;plug in;documentation	This paper presents a user interface (UI) design tool, GUIIO, which uses ASCII text as its medium for rendering interface components. Like other UI design tools, GUIIO allows individuals to create and manipulate UI components as first-class objects. However, GUIIO has the advantage that its UI designs can be embedded directly within the program code itself. We implemented GUIIO as an extension to an existing development environment. As a result, developers can fluidly transition from editing code to editing the UI mock-up, with the text editor automatically switching its mode from code editing to UI editing as a function of the location of the cursor. By rendering UIs as ASCII art, GUIIO fills an important gap in the design, implementation, and revision of UIs by providing a highly portable and immediately accessible visual representation of the UI that embeds with the code itself.	ascii art;cursor (databases);design tool;embedded system;mock object;text editor;user interface design	James Simpson;Michael A. Terry	2011		10.1145/2046396.2046438	ascii art;ui data binding;design;human–computer interaction;documentation;computer science;operating system;development environment;user interface;world wide web;computer graphics (images)	HCI	-41.65567269487333	-30.63409159003174	41991
cb0ae74b3d1699a33ecea962721f3e477a2b8749	practices in creating videos with mobile phones	mobile multimedia;user created content;user study;user studies;mobile phone;mobile phones;mobile video	Mobile phones with integrated video cameras have become ubiquitous tools that people use both to document everyday surroundings and to express themselves artistically. In this paper we report the findings of a user study on user created mobile videos, where the actions of 11 active mobile video users were documented for 2 weeks, the collected material including diaries, device logs, and altogether 255 videos. We describe the patterns related to the creation, sharing and consuming mobile videos, revealing characteristics of both context and content of the video material.	mobile phone;usability testing	Arto Puikkonen;Jonna Häkkilä;Rafael Ballagas;Jani Mäntyjärvi	2009		10.1145/1613858.1613862	mobile search;mobile web;gsm services;mobile database;computer science;mobile technology;multimedia;internet privacy;mobile computing;world wide web	HCI	-54.33156004992513	-41.8247268419094	42009
17cb7a3ac2240a385a1f7983caca35b82b84cf3b	enhancing exercise experience with individual multi-emotion provoking game elements		In this work we present a new affective game design method that includes multi-emotion provoking and continuous game elements. Evaluated with our physical cycling exergaming system it extends our previously designed single-emotion game scene approach. In addition we introduce new entertaining content that includes game elements to provoke both physical and mental stress as well as emotions with a controlled intensity to provide an exciting but not taxing exercise. Our previously developed analysis method for facial expressions and physiological data has been enhanced by a near real-time emotion evaluation, to allow the system to adapt the gameplay dynamically and in this work new multi-emotion provoking game elements are introduced. A case study with 25 participants have been conducted to showcase and evaluate the enhancing exercise experience by individual multi-emotion provoking game elements.	cluster analysis;electronic design automation;emotion recognition;real-time clock;real-time computing	Larissa Müller;Arne Bernin;Kai von Luck;Andreas Kamenz;Sobin Ghose;Qi Wang;Christos Grecos;Florian Vogt	2017	2017 IEEE Symposium Series on Computational Intelligence (SSCI)	10.1109/SSCI.2017.8280929	task analysis;cognitive psychology;facial expression;game design;continuous game;affect (psychology);stress management;emotional intelligence;psychology	HCI	-55.80488802989586	-48.70250510069719	42047
500a3bf2718d1a43161cc6ce1d6da12f7b58c6af	how spatial layout, interactivity, and persistent visibility affect learning with large displays	learning;controlled experiment;learning activities;cognitive process;use of space;spatial distribution;interactivity;spatial representation;multiple monitors;large displays;spatial information;memory	Visualizations often use spatial representations to aid understanding, but it is unclear what properties of a spatial information presentation are most important to effectively support cognitive processing. This research explores how spatial layout and view control impact learning and investigates the role of persistent visibility when working with large displays. We performed a controlled experiment with a learning activity involving memory and comprehension of a visually represented story. We compared performance between a slideshow-type presentation on a single monitor and a spatially distributed presentation among multiple monitors. We also varied the method of view control (automatic vs. interactive). Additionally, to separate effects due to location or persistent visibility with a spatially distributed layout, we controlled whether all story images could always be seen or if only one image could be viewed at a time. With the distributed layouts, participants maintained better memory of the associated locations where information was presented. However, learning scores were significantly better for the slideshow presentation than for the distributed layout when only one image could be viewed at a time.	cognition;interactivity;multi-monitor	Eric D. Ragan;Alex Endert;Doug A. Bowman;Francis K. H. Quek	2012		10.1145/2254556.2254576	computer vision;simulation;cognition;computer science;spatial analysis;multimedia;memory;interactivity	HCI	-47.35684814103313	-48.32197170046895	42103
dc830386248085cff5b011b9e2430c4da70795ca	emerging trends in cybertherapy introduction to the special issue	distributed virtual reality;cybertherapy;input device;brain computer interface;ambient intelligence;real time;mobile communication system;virtual reality;knowledge society;information and communication technology;mobile communication;3rd generation;biosensors;health care	"""According to the recent reports presented by IST Advisory Group (ISTAG) the evolutionary technology scenarios in support of the Knowledge Society of the 2010s will be rooted within three dominant trends: (a) Ambient Intelligence, the pervasive diffusion of intelligence in the space around us; (b) B3G, """"Beyond 3rd Generation"""" mobile communication system; (c) Shared Virtual Reality, with the increase of the range, accessibility and comprehensiveness of communications. The convergence of these trends manifests itself as the next frontier of Information and Communication Technologies. This convergence stimulates a change in the way health care is carried out, making it an embodied experiential process in which communication and collaboration of geographically dispersed users may also play a key role. In this special issue we will try to outline this process and its potential for the future of cybertherapy. We suggest that a key role will be played by the attainment of """"Immersive Virtual Telepresence"""" (IVT). In IVT tools, distributed virtual reality systems are combined with wireless multimedia facilities real-time video – and innovative input devices – tracking sensors, biosensors, brain-computer interfaces"""	accessibility;ambient intelligence;brain–computer interface;cyberpsychology;embodied cognition;emoticon;input device;knowledge society;pervasive informatics;real-time locating system;sensor;virtual reality	Giuseppe Riva;Brenda K. Wiederhold	2006	PsychNology Journal		brain–computer interface;information and communications technology;simulation;mobile telephony;ambient intelligence;human–computer interaction;computer science;artificial intelligence;operating system;virtual reality;multimedia;world wide web;input device;health care;biosensor;mechanical engineering	HCI	-56.800742433548606	-37.730606237550376	42147
fce5ca961b4f84648c84301fbf5f36e0c4c77bdf	for the love of information: motivations and affective dynamics of surfing the web for pleasure	affective dynamics;information behavior;pleasure surfing;motivations	The World Wide Web (the Web) is ubiquitous in people’s lives today and is used on a daily basis to get information on a variety of topics. While information seeking behavior has been studied extensively in a Web setting, many of these studies assume a task-oriented search for information even though much of the information seeking on the Web is not necessarily done with previously stated information needs or goals. Some Web information seeking activity is done for purposes of entertainment, or for simply connecting with other people (for example, via social media websites), oftentimes with an unstructured, serendipitous approach. Information seeking in general, and information acquisition, in particular, is a pleasurable activity in and of itself. Surfing the Web for pleasure is a prominent activity and worthy of our attention as researchers. To do so, we need an appropriate framework. This paper borrows ideas from a social-cognitive approach to the uses and gratifications paradigm in order to investigate the suitability of that framework in a Web information access setting. More specifically, we use the answers from questions we asked to 180 survey respondents about their motivations for surfing the Web for pleasure and their affective states during and after their surfing sessions. The findings indicate that the social-cognitive approach to uses and gratifications is a useful baseline. We make enhancements to it and propose our own framework for research in this area. We present data to bolster the notion that the seeking and acquisition of information brings pleasure and we propose a framework to study user motivations and affective states both during and after their “surfing the Web for pleasure” sessions.	baseline (configuration management);emergence;granular computing;information access;information needs;information retrieval;information seeking behavior;internet;programming paradigm;recommender system;social media;software framework;speeded up robust features;theme (computing);world wide web	Ziad Matni;Chirag Shah	2014		10.1002/meet.2014.14505101057	motivation;multimedia;social psychology;world wide web	Web+IR	-60.875014049579704	-41.95645135533289	42160
29672236c11e4e2dbb51670f7f07a04c05adf035	canonical abstract prototypes for abstract visual and interaction	sistema interactivo;interfase usuario;user interface;systeme conversationnel;interactive system;interface utilisateur	"""Abstract user interface prototypes offer designers a form of representation for specification and exploration of visual and interaction design ideas that is intermediate between abstract task models and realistic or representational prototypes. Canonical Abstract Prototypes are an extension to usage -centered design that provides a formal vocabulary for expressing visual and interaction designs without concern for details of appearance and behavior. A standardized abstract design vocabulary facilitates comparison of designs, eases recognition and simplifies description of common design patterns, and lays the foundations for better software tools. This paper covers recent refinements in the modeling notation and the set of Canonical Abstract Components. New applications of abstract prototypes to design patterns are discussed, and variations in software tools support are outlined. Usage-Centered Design Usage-centered design is a model-driven approach to the presentation design and interaction design of software [1] and Web-based applications [2]. It is a robust and adaptable process with a proven record of success over nearly a decade of application on a wide variety of projects ranging from small, XP-style applications programming [3] to large-scale industrial tools development [4]. It has proved particularly effective for complex problems in which the efficiency and dependability of user performance is critical, such as in medical informatics [5] or industrial automation applications programming, where it has led to radical improvements in user task and problemsolving performance with award-winning results [4, 6]. In usage-centered design, user interface designs derive directly and systematically from a series of core models. The final presentation and interaction designs are based directly on models of interface contents, which derive in a straightforward fashion from models of user tasks, which are based in turn on models of the roles users play in relation to the planned system. The core of the process is a robust, fine -grained task model comprising a collection of interrelated use cases expressed in so-called essential form [7, 8]. The close fit of the user interface design to this task model can yield dramatic improvements in ease of learning as well as more reliable and efficient user task performance. Constantine & Lockwood, Ltd. Constantine: Canonical Abstract Prototypes page 2 Usage-centered design was originally developed and has continued to evolve driven primarily by pragmatic concerns with supporting the design process of practicing designers working on real-world projects. The objective has always been to facilitate a design process that is simultaneously efficient, reproducible, and creative. The models it incorporates were devised to provide the most conceptual and creative leverage for the least amount of effort on the part of analysts and designers. To this end, usagecentered design differs in both philosophy and practice from many mainstream usercentered design approaches [2, 9]. For example, although task models in one form or another are elements of many design processes and are widely used in practice, the task cases employed in usage-centered design are specifically constructed to be the simplest and most compact expression of a given set of tasks, thereby promoting the identification of simpler designs to support those tasks. Similarly, and unlike more elaborate methods such as the Rational Unified Process [10, 11], usage-centered design seeks to reduce the number and complexity of design artifacts to the absolute minimum required for an orderly and effective process. Content Models and Modeling Content models represent the contents of user interfaces and their various constituent sections or parts independent of details of appearance and behavior. Content models thus abstract from more realistic representations of user interface designs, such as the paper prototypes or mockups commonly sketched by designers, and for this reason, they are sometimes referred to as abstract prototypes [12]. As abstractions, they can serve as an intermediate bridge between task models and realistic designs, smoothing, simplifying, and systematizing the design process. Content models help clarify and define what a user interface must contain and how its contents are partitioned before its design is worked out in detail. They encourage reasoning and experimentation with how component parts of a user interface are combined and distributed to form a coherent, understandable, and usable whole. User interface prototypes can be arrayed on a continuum of abstraction. The simplest and most abstract content models, called content inventories, consist of simple lists inventorying the information and controls to be collected within a given interaction context, such as, a window, dialog box, page, or screen. The visual content inventories using sticky notes that were first introduced in usage-centered design [1, 12] also incorporate position or spatial relationship among interface contents. So-called wireframe schematics outline (with """"wire frames"""") the areas occupied by the various interface contents. Figure 1 illustrates a single Web page as described by a content inventory and a wire -frame schematic. At the most realistic and least abstract end of the spectrum are low-fidelity paper prototypes or rough sketches, high-fidelity paper prototypes, and, finally, accurate mockups or even working or partially functional simulations. The more abstract models facilitate solving problems in user interface organization, navigation, or overall architecture, leaving aside the details, while realistic prototypes help resolve detail design decisions in layout, visual presentation, and component selection, as well as fine points in interaction design and interface behavior. Indeed, skilled, disciplined designers tend to work from higher-level abstract representations toward progressively more realistic and detailed representations as the design evolves and is refined. Constantine: Canonical Abstract Prototypes page 3 Other Publications Pages Links Publications Page Content Inventory Primary Sections Links Publications List (sortable) Full-Site"""	automation;coherence (physics);content inventory;dependability;design pattern;informatics;interaction design;interaction technique;model-driven architecture;paper prototyping;rational unified process;schematic;simulation;smoothing;sticky bit;triune continuum paradigm;usage-centered design;user interface design;vocabulary;web application;web page;wire-frame model;dialog	Larry L. Constantine	2003		10.1007/978-3-540-39929-2_1	simulation;human–computer interaction;computer science;artificial intelligence;operating system;database;programming language;user interface;algorithm	HCI	-41.67508923672663	-29.789638527768357	42167
b2251192d9676d840fbde34945a038ada54dc943	humanoid robots	utilitarian counterpart;everyday life;humanoid robot;technological advantage	The future promises lots of robots in our everyday lives; some, perhaps many, of them could look and behave like people but only if being humanoid represents a technological advantage over their relatively utilitarian counterparts.	robot	Rodney A. Brooks	2002	Commun. ACM	10.1145/504729.504751	simulation;artificial intelligence	HCI	-57.11002648840457	-36.90995681285286	42201
9b741257682aa96ea25601bc9366bc21a6303336	guest editorial		As the boundaries of autonomous agents and multi-agent systems continue to expand, there is an increasing need for agents to interact with humans. To date, the field of multi-agent systems has matured from conceptual models to real-world applications (e.g., energy and sustainability, disaster management, or health care). One significant challenge that arises when transitioning from conceptual models to applications is addressing how people will interact with these systems. To this end, this special issue examines major challenges at the intersection of human–agent systems. In particular, we focus on the challenges of designing and modelling human–agent interaction. Design challenges typically take a human-centric view of human–agent systems and focuses on human–agent coordination mechanisms, trust issues in human–agent interaction, interaction techniques, and human activity recognition. Modelling challenges are concerned with finding better models of human behaviour in a variety of settings so that autonomous andmulti-agent systems can appropriately interactwith human agents (e.g., agent–human negotiation strategies or health care agents encouraging physical therapy for a variety of recovering patients). In order to bring together the research community that is addressing these issues, in 2008 we initiated the workshop series on Human–Agent Interaction Design and Models (HAIDM) co-located with the International Joint Conference on Autonomous/Agents and Multi-Agent Systems (AAMAS). This special issue invited contributions from both Human– Computer Interaction (HCI) and agents communities.We received an overwhelming response (22 submissions) from researchers from a number of fields (both HCI andAgents). Following	activity recognition;autonomous agents and multi-agent systems;autonomous robot;human–computer interaction;interaction design;interaction technique;international conference on autonomous agents and multiagent systems;multi-agent system;norm (social)	Sarvapali D. Ramchurn;Avi Rosenfeld;Joel E. Fischer	2015	Autonomous Agents and Multi-Agent Systems	10.1007/s10458-015-9295-3		AI	-56.44492046555737	-36.23132724138265	42222
b16147d467777cb59732932a168e6d63ef2ec7e0	storytelling by the storycake visualization		On the issue of how to explore the complex relationships of entities in a story, Storyline visualization has proven to be a useful approach. The traditional Storyline visualization technology only applies to a linear continuous story and has the limitation of reflecting the development of the plots in a story. In this paper, we propose a hierarchical plot visualization method called StoryCake to improve this problem. First, according to the story elements, inter-sessions are divided into groups, whereby the layout of the entities will be optimized in individual groups. Then, the hierarchical relationships of entities in polar coordinates can be confirmed. Finally, we use several skills, e.g., interaction, labels and a fan-shaped visualization view, to enable people better understand how story evolves and track the hierarchical relationship more conveniently. Experimental results show that the proposed hierarchical plot visualization method in polar coordinates can reveal discontinuous events and nonlinear stories. The results also show that our algorithm presents the narrative structure and development of the story.	algorithm;entity;nonlinear system	Qiang Lu;Bingjie Chai;Haibo Zhang	2017	The Visual Computer	10.1007/s00371-017-1409-2	artificial intelligence;computer vision;visualization;computer science;storytelling;narrative;polar coordinate system;nonlinear system;scenario analysis	Visualization	-38.011607989919725	-36.569082033441035	42288
c96daa71ac52b8def2c63ac083b2d80d522269b7	captap: combining capacitive gesture recognition and acoustic touch detection	capacitive proximity sensors;hidden interaction;gesture recognition	Capacitive sensing is a common technology for finger-controlled touch screens. The variety of proximity sensors extends the range, thus supporting mid-air gesture interaction and application below any non-conductive materials. However, this comes at the cost of limited resolution for touch detection. In this paper, we present CapTap, which uses capacitive proximity and acoustic sensing to create an interactive surface that combines mid-air and touch gestures, while being invisibly integrated into living room furniture. We introduce capacitive imaging, investigating the use of computer vision methods to track hand and arm positions and present several use cases for CapTap. In a user study we found that the system has average localization errors of 1.5cm at touch distance and 5cm at an elevation of 20cm above the table. The users found the system intuitive and interesting to use.	acoustic cryptanalysis;algorithm;alveolar rhabdomyosarcoma;body part;capacitive sensing;cardiomyopathies;coat of arms;computer vision;evaluation;gesture recognition;image processing;knee;mask (computing);masks;physical object;stereo camera;stereo cameras;touchscreen;tracking system;usability testing;anatomical layer;sensor (device)	Andreas Braun;Sebastian Zander-Walz;Stefan Krepp;Silvia Rus;Reiner Wichert;Arjan Kuijper	2016		10.1145/2948963.2948969	embedded system;computer vision;engineering;communication	HCI	-43.027790036331105	-42.27881208541192	42321
657e005c299f86ff7464b7c99150200264657cea	prism: pose registration for integrated semantic mapping		Many robotics applications involve navigating to positions specified in terms of their semantic significance. A robot operating in a hotel may need to deliver room service to a named room. In a hospital, it may need to deliver medication to a patient's room. The Building-Wide Intelligence Project at UT Austin has been developing a fleet of autonomous mobile robots, called BWIBots, which perform tasks in the computer science department. Tasks include guiding a person, delivering a message, or bringing an object to a location such as an office, lecture hall, or classroom. The process of constructing a map that a robot can use for navigation has been simplified by modern SLAM algorithms. The attachment of semantics to map data, however, remains a tedious manual process of labeling locations in otherwise automatically generated maps. This paper introduces a system called PRISM to automate a step in this process by enabling a robot to localize door signs – a semantic markup intended to aid the human occupants of a building – and to annotate these locations in its map.		Justin W. Hart;Rishi Shah;Sean Kirmani;Nick Walker;Kathryn Baldauf;Nathan John;Peter Stone	2018	2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2018.8593681	computer vision;computer science;artificial intelligence;robot;semantic mapping;human–computer interaction;lecture hall;semantic html;semantics;mobile robot;robotics	Robotics	-33.96451229362093	-41.47258339955203	42332
46053fe3e8213224660b15fcb8c1c9d5d03cbd6d	bone-grass boy: the secret banks of the conejos river	secret bank;conejos river			Ken Gonzales-Day	1996		10.1145/253607.253639	computer graphics (images);computer vision;artificial intelligence;computer science	Crypto	-52.02803637123251	-27.248243490529255	42363
2c502d058a54b8a202f8468f07fda9d203eb42f8	susan schneider (ed): science fiction and philosophy	science fiction	The rising popularity of science fiction during the past decade or two can be explained by Toffler’s (1970) vision of the future shock: that mass disorientation which results from suddenly transporting an entire society into the reality of the twenty-first century. Science fiction classics such as 1984 and Neuromancer became so popular perhaps for no reason more then because they have offered several generations the only medicine for the global future shock. Elaborating social, technological, or scientific extrapolations, these classics depict worlds that are rationally and empirically plausible, thereby helping us prepare for ‘‘a storm of change’’ and an accelerating process of turning witchcraft into reality. Pulp fiction, space operas, and Hollywood special effects helped this important genre break the confines of the neurotic and nerdy. Science fiction’s popularity brought to the fore other aspects of the genre. Susan Schneider’s edited volume is about works of art that, whether aided by poetic narrative or expensive cinematic effects, seek to unpack and flesh out thought experiments. As I understand it, Schneider’s central thesis is that the timeless pieces of Isaac Asimov, Arthur C. Clark, and Ray Bradbury are spectacular exercises of gedankenexperiments. I, Robot and Nightfall examine traditional philosophical problems such as the nature of free will and the nature of humankind. Solaris, The Matrix, and The 13th Floor question the fabric of reality, driving home questions about ‘‘the very fundamental nature of things’’. Old questions of personal identity that were raised in Mary Shelly’s Frankenstein receive a deeper and more technologically informed treatment in the TV series Dollhouse. Blade Runner, 12 Monkeys and Brain in a Vat open a window to unexpected yet seemingly inevitable by-products of scientific progress which are otherwise indistinguishable from magic, such as posthumanism and time travel. Hence ‘science fiction’ (and	blade runner;experiment;hollywood;i, robot;nerd;neuromancer;susan schneider (philosopher);the matrix	Amnon Eden	2010	Minds and Machines	10.1007/s11023-010-9195-x	science fiction fandom;sci-fi;computer science;artificial intelligence;fantasy	NLP	-58.07020397212563	-24.58613553745433	42393
561eeac18f93b53cb7ece53e99df128578ef4db5	workshop on situation awareness in automotive evaluation & design	user interface;requirements;system design;situation awareness;goal directed cognitive task analysis;situation awareness measures	In recent decades, situation awareness (SA) has gained importance as a promising cognitive theory, which can support both driver behavior research and advanced driver assistance system (ADAS) design. SA theory can be used to both determine and assess drivers' dynamic knowledge needs under different contexts. Additionally, based on an understanding of SA requirements, designers can develop automotive user interfaces (UI) that supply the needed information to the driver in the most useful format to directly support their SA [1]. This workshop aims to provide a high level overview of SA as it relates to the automotive industry - starting with an understanding of SA and SA information needs (requirements), continuing on to the application of several useful SA measures, and finally exploring the application of SA to ADAS design.	architecture design and assessment system;high-level programming language;information needs;requirement;user interface	Yu Zhang;Linda Angell;Bobbie Seppelt;Lee Skrypchuk;Cheryl Bolstad	2016		10.1145/3004323.3005683	simulation;systems engineering;engineering;knowledge management	HCI	-61.51799040705227	-45.297607410057395	42394
9cb495f9148dfd0a5daa0f5c8799d5a34e6fdbdb	extension of reward-attention circuit model: alcohol's influence on attentional focus and consequences on autism spectrum disorder				Karine Guimarães	2019	Neurocomputing	10.1016/j.neucom.2018.10.034		ECom	-55.5719869231378	-51.28833061390153	42431
fbb838a7b6ca43d9217a3b8d24e78d719b506385	directions for multimedia development considering the aspect of art analysis	software;virtual computer reality multimedia development art analysis;art;art subspace constraints virtual reality humans information analysis hardware software multimedia systems multimedia communication psychology;virtual computer reality;multimedia development;virtual reality;psychology;art analysis;multimedia systems;subspace constraints;multimedia computing;multimedia communication;humans;information analysis;virtual reality art multimedia computing;hardware	The proposed work is studying the principles of creating multimedia works for creative activities of artists. Definition of computer culture aesthetic principles is given, classification of virtual computer reality as a multimedia derivative is done. Setting these new rules of conceptual creation and design of multimedia works is the basic stage for their analyzing and classifying.		Olga Yatsiouk	2007		10.1109/CBMI.2007.385423	simulation;human–computer interaction;computer science;virtual reality;multimedia;data analysis	HCI	-57.43888142792107	-32.95389657196542	42455
2b00b7881b95c62b7a03ca30a2d84707ab378445	a gesture-based door control using capacitive sensors		In public places sanitary conditions are always of concern, particularly of surfaces that are touched by a multitude of persons, such as door handles in rest rooms. Similar issues also arise in medical facilities. Doors that open based on presence are common in environments such as shopping malls; however they are not suited for sensitive areas, such as toilet stalls. Capacitive proximity sensors detect the presence of the human body over a distance and can be unobtrusively applied in order to enable hidden gesture-based interfaces that work without touch. In this paper we present a concept for a gesture controlled automated door based on this sensor technology. We introduce the underlying technology and present the concept and electronic components used in detail. Novel interaction patterns and data processing methods allow to open, close, lock and unlock the door using simple gestures. A prototype device has been created and evaluated in a user study.	electronic component;interaction technique;prototype;sim lock;sensor;usability testing	Steeven Zeiß;Alexander Marinc;Andreas Braun;Tobias Alexander Große-Puppendahl;Sebastian Beck	2014		10.1007/978-3-319-07788-8_20	capacitive sensing;embedded system;lock (computer science);toilet;doors;computer science;proximity sensor;gesture;electronic component	HCI	-43.49349672238337	-42.856102635739475	42479
9ccbe8389f2f8905d133e82f338cb328395b9c09	health 2.0	technology;ehealth	When I first began my journalism training, the buzz word at the time was ‘web 2.0’. So much so in fact, that my online tutor went so far as to say that in the future no one would buy newspapers or magazines. A worrying thought for all those whose careers hang in the balance of the printed word.	health 2.0	Benedikt Buchner	2011	Datenschutz und Datensicherheit - DuD	10.1007/s11623-011-0196-6	health policy;health administration;health promotion;public health;occupational health nursing;health;occupational safety and health;global health;health care;health education	HCI	-62.51123304154212	-24.683792599953012	42483
db44ac46f3f7dbf7027223e919933e47f18baa37	interacting with semantic musical features: enhanced human-computer interactions based on musical “meaning”	musical meaning semantic musical features human computer interactions;human computer interaction;music human computer interaction;music analysis;feature extraction semantics computers media human computer interaction music presses;music	Semantic musical features are proposed to reflect the understanding of the music, instead of the music itself, and serve as idea interfaces for musical “meaning” based human-computer interactions. The proposed semantic musical features are based on reductive music analysis and musical expressive features.	human–computer interaction	Ren Gang;Gregory Bocko;Justin Lundberg;Stephen Roessner;Mark F. Bocko;Dave Headlam	2012	2012 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2012.6161747	natural language processing;new interfaces for musical expression;music psychology;musical syntax;music;music theory;multimedia;music and emotion;musicality	Robotics	-52.035478447331066	-46.52403588146013	42489
28d3729ce4f3548dab9dfb3a49a334e8915061d0	slice: drop: collaborative medical imaging in the browser		This project demonstrates real-time rendering and sharing of standard medical data formats between WebGL-enabled browsers across multiple devices. Any linked browser can interact with and update the display, which propogates to all other linked browsers.	medical imaging;real-time transcription;webgl	Daniel Haehn	2013		10.1145/2503541.2503645	multimedia;internet privacy;world wide web	Graphics	-44.36074587369802	-27.054905702338363	42508
16b3bea90b5485a0313e65c9be3ba101e264723a	the gnome canvas: a generic engine for structured graphics	application programmer;main display engine;gnome application;structured graphics;particular need;interactive graphics display;high performance rendering;generic engine;gnome canvas;basic functionality	The GNOME canvas is a generic engine for structured graphics that offers a rich imaging model, high performance rendering, and a powerful high-level API. Application programmers can use the canvas to create interactive graphics displays easily. Many GNOME applications use the canvas as their main display engine, some of them using the basic functionality provided by the canvas, and others by extending it for their particular needs. This paper describes the architecture of the canvas in detail and examines the way it is used in several GNOME applications.	canvas element;gnome;graphics	Federico Mena-Quintero;Raph Levien	2000			multimedia;computer graphics (images)	Crypto	-41.328888905810466	-31.08144959229729	42513
785c2bff2974566b4222178689c9e2d490899f88	design for user empowerment	social issues;user interfaces	have noticed that some of the best work comes when there are people with disabilities on the design and development team, contributing to all aspects of the design and implementation, not just as participants in user studies. I call this strong engagement by users design for user empowerment, meaning, in its strongest sense, that the users of the technology are empowered to solve their own accessibility problems. Here, I will try to explain, mostly using examples, why this approach is so powerful. Although I am not disabled, I am very fortunate to have lived around disabled people my entire life. My parents were deaf college graduates and successful professionals. I learned from them that disability is not a tragedy, but rather simply part of the diversity of life. I also learned from them the power of technology to be transformational, not in any kind of medical sense but in a purely social sense. When they and their friends got TTYs, surplus Western Union Teletypewriters attached to acoustic modems, in the early 1970s, they could finally communicate easily at a distance with friends and colleagues around the country by typing, or what we now call texting or instant Insights → It is important for designers of technology for people with disabilities to engage with people with disabilities to achieve the most usable designs. → Better yet is empowering people with disabilities to design and build the technologies themselves. → Two features of design for user empowerment are self-determination and technical expertise. Design for User Empowerment	accessibility;acoustic cryptanalysis;modem;usability testing	Richard E. Ladner	2015	Interactions	10.1145/2723869	user experience design;knowledge management	HCI	-57.77218682547346	-41.17706139140636	42536
1999c79022377fbb9d4320cd6aeccac2b96cb9bd	multimodal synchronization of image galleries		This paper describes our contribution to the MediaEval 2014 task on the Synchronization of multi-user Event Media (SEM). We propose two multimodal approaches that employ both visual and time information for the synchronization of different images galleries and for the detections of sub-events. The methods prove robustness in the determination of time offsets with accuracy of up to 87%.	multi-user;multimodal interaction;sensor	Maia Zaharieva;Michael Riegler;Manfred del Fabro	2014			computer vision;art;multimedia;communication	NLP	-33.758753084439356	-43.527948749072536	42552
f7ab6f2774b4541f3ea7361c741bb312f06fc8c0	testing strategies for evaluation of user interfaces in soa-based systems	user interface;hci;soa;usability;working paper	This paper presents a research that was conducted to determine the best techniques and testing strategies for user interface evaluation of SOA-based systems. Although we can find many articles on, for example, the design of web services, few guidelines exist on user interfaces and their usability for SOA-based systems. To achieve our goal we have performed various tests to determine which evaluation techniques are most suitable for such systems. The tests were conducted on a system which was created at our university -- Platel. As a result we came up with a set of most effective techniques for user interface evaluation, and some guidelines for testing strategies, for SOA-based systems.	service-oriented architecture;usability;user interface;web service	Piotr Chynal	2013		10.1145/2500342.2500364	user interface design;usability;human–computer interaction;computer science;systems engineering;service-oriented architecture;user interface;heuristic evaluation;world wide web	HCI	-62.731288739863054	-46.37277495197325	42553
d8b061417ea56567d54f354152b1db642cc50246	new approaches to efficient rendering of complex reconstructed environments	solid;software systems;g500 information systems;and object representations;interactive display;viewing algorithms i 3 5 computer graphics curve;level of detail;categories and subjectdescriptors according to acmccs i 3 3 computer graphics display algorithms;surface;multi resolution;real time rendering	The creation of complete reconstructions of populated urban environments are technically difficult tasks primarily due to economic constraints in the modeling phase: complex models need to keep rendering aspects in mind in order to warrant interactive rendering speeds which makes this kind of work a labor-intensive task for highly skilled personel. Specialized modelling tools, which exploit knowledge of the types of object being modelled by working in the application domain, can be used to create appealing virtual reconstructions quickly. At the same time, the structural information from the modeller gives essential hints to the interactive renderer to determine efficient interactive display strategies through the use of level-of-detail and culling techniques. Even more important, only a shift in the modeling paradigm from ’just in case’ to ’just in time’ can solve the problem applications are faced in real-time rendering. In this paper we discuss the way in which polygonal and multi-resolution surface techniques can complement one another in the effective rendering of complex reconstructed environments. We also draw more general conclusions which apply to other software systems that share the same objectives.	application domain;level of detail;modeller;on the fly;population;programming paradigm;real-time locating system;rendering (computer graphics);run time (program lifecycle phase);software system	Sven Havemann;Dieter W. Fellner;Andrew M. Day;David B. Arnold	2003		10.2312/VAST/VAST03/185-194	computer vision;tiled rendering;simulation;image-based modeling and rendering;rendering;computer science;operating system;level of detail;parallel rendering;solid;real-time rendering;surface;alternate frame rendering;software rendering;software system;computer graphics (images)	Graphics	-37.193075199488824	-33.09960954826782	42564
60997f2128dfb46db49fc5c2c105a3f6ab8ab35b	a distributed memory hierarchy and data management for interactive scene navigation and modification on tiled display walls	distributed system;biological patents;biomedical journals;memory management;life cycle;text mining;rendering computer graphics distributed databases data models navigation layout three dimensional displays memory management;europe pubmed central;citation search;collaboration;layout;citation networks;distributed memory hierarchy data density distribution data management technique object space adjacency screen space adjacency secondary storage devices interactive rendering applications scd node infrastructure storage compute display nodes pc cluster lcd panel tiled multidisplays simultaneous interactive navigation distributed data management interactive out of core rendering performance data layout repeated data edits massive 3d models simultaneous modification tiled display walls interactive scene navigation;navigation;research articles;three dimensional displays;abstracts;management of computing and information systems;open access;life sciences;distributed databases;clinical guidelines;computer science;storage management distributed memory systems interactive devices liquid crystal displays rendering computer graphics solid modelling;full text;rendering computer graphics;rest apis;project and people management;orcids;europe pmc;biomedical research;data models;bioinformatics;the computing profession miscellaneous ethics;literature search	Simultaneous modification and navigation of massive 3D models are difficult because repeated data edits affect the data layout and coherency on a secondary storage, which in turn affect the interactive out-of-core rendering performance. In this paper, we propose a novel approach for distributed data management for simultaneous interactive navigation and modification of massive 3D data using the readily available infrastructure of a tiled display. Tiled multi-displays, projection or LCD panel based, driven by a PC cluster, can be viewed as a cluster of storage-compute-display (SCD) nodes. Given a cluster of SCD node infrastructure, we first propose a distributed memory hierarchy for interactive rendering applications. Second, in order to further reduce the latency in such applications, we propose a new data partitioning approach for distributed storage among the SCD nodes that reduces the variance in the data load across the SCD nodes. Our data distribution method takes in a data set of any size, and reorganizes it into smaller partitions, and stores it across the multiple SCD nodes. These nodes store, manage, and coordinate data with other SCD nodes to simultaneously achieve interactive navigation and modification. Specifically, the data is not duplicated across these distributed secondary storage devices. In addition, coherency in data access, due to screen-space adjacency of adjacent displays in the tile, as well as object space adjacency of the data sets, is well leveraged in the design of the data management technique. Empirical evaluation on two large data sets, with different data density distribution, demonstrates that the proposed data management approach achieves superior performance over alternative state-of-the-art methods.	3d modeling;anatomic node;anemia, sickle cell;areal density (computer storage);auxiliary memory;binary space partitioning;cache;clustered file system;computer cluster;computer data storage;data management;data access;data model;data system;display resolution;distributed computing;distributed memory;dynamic data;equilibrium;glossary of computer graphics;glycogen storage disease type i;greater;handling (psychology);load balancing (computing);lucid;memory disorders;memory hierarchy;movement;multi-user;navigation;node - plant part;numerous;out-of-core algorithm;particle system;programming paradigm;sample variance;simulation;small;software framework;tuple space;user interface device component;video display terminals;virtual reality;walls of a building	Duy-Quoc Lai;Behzad Sajadi;Shan Jiang;Meenakshisundaram Gopi;Aditi Majumder	2015	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2015.2398439	layout;data modeling;biological life cycle;computer vision;navigation;text mining;computer science;data mining;database;world wide web;memory management;computer graphics (images);collaboration	Visualization	-35.192664122238746	-33.51593832653782	42597
347b1451cda2264cd34efeeaa6de006b0f9a5f40	my place or yours: use and abuse of research facilities	research facility	"""You fought hard for funding, the contractors have finished, and the paint is almost dry. Naturally you want to make sure that your new usability lab earns it keep. But don't get too enthusiastic about having a full schedule if it means that your development process will suffer as a consequence. Here's the thing: while audio and video recording of test sessions, focus groups, and interviews in a controlled setting can be very useful , so can ethnographic research and candid feedback. Unfortunately, these last two activities (and several others) cannot be done in a usability lab. Ethnographic research in particular must be done in the field. Hauling participants into your test facility, no matter how charmingly decorated, will not produce the same results at all. We need to see how users really work, in their own environments, with their own equipment, materials and other resources. This is especially true of third-party interactions, where aside from the computer and its immediate user, there is a customer, passenger, patient, or other """" ultimate """" user. These complex interactions simply will not be the same in an artificial environment as they will in the real world. Furthermore, because you have much easier access to your organi-zation's own staff (than to, say, your customers) there is a very real concern that the Web-based system you are developing will target the wrong users. Let's take an example. Suppose that you are in the travel business, developing a new customer-facing system for use by your staff. While it is very important that this system be highly usable by staff, it would be to no avail if it did not at the same time meet the needs of customers. And you really don't have much choice here. Either you go out into the field and observe real customers with real needs or you become a playwright, drafting scripts to be enacted on this stage that is your lab environment. Of course, the problem is not limited to three-party interactions. In ethnographic observation we are : / 45 i n t e r a c t i o n s / m a y + j u n e 2 0 0 4 hci and the web hci and the web"""	focus group;interaction;usability lab;video;world wide web	William Hudson	2004	Interactions	10.1145/986253.986270	human–computer interaction;multimedia;engineering	HCI	-61.838895603865346	-27.827288646751605	42670
61023908639c9982694ba99d21e1d0acb58667b7	remote touch: humanizing social interactions in technology through multimodal interfaces		Waves, pokes, and tugs are simple social gestures that can benefit from more thoughtful design when translated onto mobile devices and computers. Haptics provide an additional mode of conveyance that is frequently forgotten about in development of mobile technologies, but incorporating it can have significant positive impact on user experience. Combining advanced vibrotactile haptics, location, and multimodally congruent feedback, our prototype creates a simple experience that connects people through non-verbal information to deliver a meaningful gesture and playful interaction.	multimodal interaction;remote touch	Alexia Mandeville;David Birnbaum;Chad Sampanes	2017		10.1007/978-3-319-57987-0_10	wearable computer;human–computer interaction;mobile technology;user experience design;haptic technology;gesture;mobile device;multimedia;ibeacon;computer science	HCI	-47.28456497733307	-43.03314179243082	42679
e4cf39287e699a2fce231ccd74eddbc409dea85e	modified relt for display and navigation of large hierarchy on handheld touch-screen devices	humanoid robot;hierarchical structure;hierarchy;androids;touch sensitive screens;touch screen;query processing;display devices;relt radial edgeless tree;smart phones;layout;tree data structures;vegetation;data visualisation;navigation;visualization;shape;humanoid robots;radial edgeless tree handheld touch screen devices hierarchical structure smartphone screen laptop screens pc screen width restriction visualization technique user query information display relt navigation;visualization technique;visualization layout vegetation navigation shape androids humanoid robots;tree data structures data visualisation display devices query processing smart phones touch sensitive screens;visualization hierarchy tree structure relt radial edgeless tree;tree structure	Visualizing and exploring a hierarchical structure on a smartphone screen is challenging. On larger screens of PCs and laptops these types of structures are often shown in a tabular view. Due to the width restriction, tabular view is not feasible on smaller screens. This paper presents a visualization technique that displays multiple levels of a hierarchical structure on a single view for small screens and allows users to explore the structure rapidly through touch input. The visualization technique makes full use of the available space and distributes the space fairly among nodes. The system allows selection and display of relevant information based on user query or user habit, while hiding less important items from the view.	laptop;smartphone;table (information)	Abhishek P. Chhetri;Kang Zhang	2012	2012 IEEE/ACIS 11th International Conference on Computer and Information Science	10.1109/ICIS.2012.73	computer vision;simulation;computer science;computer graphics (images)	Visualization	-45.38936042134568	-40.85924344818804	42725
e9f5289c98a407a21ea0ca30d56e9fe41e4a546a	fantasy birds in yazi's dream		Yazi's Dream celebrates the life of Yazi, our first cat. This project uses the dizi, a Chinese transverse flute that often imitates bird songs, to make tremolos for the fantasy birds used in Yazi’s Dream.		Lydia Ayers	2006			psychology;aesthetics	NLP	-54.1473087712771	-26.972106505888906	42734
de64f7c9e6efdfaf2372a0328f17c7c34df949d2	taking the 'a' out of 'ar': play based low fidelity contextual prototyping of mobile augmented reality		Taking the u0027Au0027 out of u0027ARu0027 means implementing the augmented elements of an interface and contextual elements of reality in a more controlled context to allow for proof of concept evaluations. This paper proposes a prototyping technique that bridges the gap between traditional paper prototyping methods used for interface design and evaluation, and the challenges associated with the development of visual, context-aware augmented reality (AR) applications. An initial evaluation of this technique was conducted through the examination of a small-scale case study of user evaluation sessions of a mobile application.	augmented reality;mobile app;paper prototyping;prototype	Alexandra Thompson;Leigh Ellen Potter	2018		10.1145/3270316.3271518	proof of concept;interface design;multimedia;fidelity;augmented reality;paper prototyping;computer science	HCI	-55.026131158639174	-38.5823837380002	42753
1f08009b9ff79fc1461ca7273fec44bb49c0960f	susceptibility to periodic vibrotactile guidance of human cadence	legged locomotion frequency measurement atmospheric measurements particle measurements synchronization haptic interfaces portable computers;pedestrians;continuous control system periodic vibrotactile guidance human cadence periodic vibrotactile cues walker susceptibility pvg stride frequency vibrotactile display spatiotemporal guidance systems pedestrians speed;spatiotemporal phenomena;spatiotemporal phenomena haptic interfaces pedestrians;haptic interfaces	In this paper we introduce a new guidance method that employs periodic vibrotactile cues to help users walk at a desired speed. We also explore walker's susceptibility to periodic vibrotactile guidance (PVG): specifically, adjustments of their stride frequency in response to cues that are clearly perceived; and finally, how long users can maintain their stride frequency after the guidance cue stops. While wearing a vibrotactile display on one wrist, each participant was given five vibrotactile tempos, logarithmically spaced across the participant's walking frequency range. We compared realtime stride frequency with cue tempo under conditions that varied cue tempo and presence / absence. Our results suggest that most individuals (here, 13 of 15) can synchronize their cadence with a vibrotactile cue with 95% accuracy (mean error, all participants: -1.5%, SD = 8.1) for a guidance tempo within their physical ability. Once a tempo was matched, walkers could maintain it for at least 30 seconds after the cue was turned off, showing promise for intermittent guidance as a solution to stimulus adaptation and annoyance. This finding informs design of spatiotemporal guidance systems, by showing how the informationally narrow but nevertheless underused haptic channel may have utility in guiding pedestrians' speed, without a need to learn abstracted signals, and through a continuous control system.	amiga walker;control system;frequency band;guidance system;haptic technology;institute for operations research and the management sciences	Idin Karuei;Karon E. MacLean	2014	2014 IEEE Haptics Symposium (HAPTICS)	10.1109/HAPTICS.2014.6775446	computer vision;simulation;engineering;communication	HCI	-46.287657886333044	-50.05021242635242	42783
d027b04f9e8dca02d8fef799d5a905f5725afb4a	level of abstraction structured text	evaluation performance;traitement texte;documentation manuals text processing stress error correction costs application software computer aided instruction history pain;performance evaluation;learning;etude experimentale;evaluacion prestacion;system documentation;aprendizaje;apprentissage;levels of abstraction;tratamiento textos;task performance user requirements computer documentation level of abstraction structured text text frames last system;estudio experimental;word processing	Computer users' negative reactions to printed computer documentation are attributed to the linear nature of the print medium, coupled with the varied information needs of different classes of users. A document system, called LAST (level of abstraction structured text), that meets the requirements of nonlinear traversal and automatic adjustment to users' requirements for type of information and level of detail is described. This system is a series of internally consistent text frames, organized by level of abstraction. It is shown that LAST system users made significantly fewer errors on task performance measures such as number of typing mistakes, correctly set margins and tabs, and correctly formatted text body. LAST users' subjective acceptance ratings of the documentation system were found to be higher than those given to the printed documentation. >	structured text	James H. Watt	1988	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.17367	speech recognition;computer science;artificial intelligence;multimedia	Embedded	-60.18090507575336	-48.306622704271724	42788
27a086ea22941ffe90208eae429764683d687027	tools for designing experience: repurposing design resources for the emerging experience economy		Experience Design, the planning and production of these experience-centred, immersive productions, is a new and essentially interdisciplinary academic field meaning that almost all current practitioners originally trained in a different (albeit related) single area (e.g. architecture, theatre, UI design etc.) and have developed their interdisciplinary expertise through a process of individual research, experience and reflection. This necessarily limits the availability of suitably skilled practitioners and there is a growing sense that appropriate training needs to be developed to support the continued expansion of the sector.	experience design;user interface design	Ian Willcock	2017		10.14236/ewic/EVA2017.48	systems engineering;repurposing;engineering;knowledge management	HCI	-62.46667472466613	-34.064977325349915	42843
86193b7d7fbc1929245219e70f1323993b8ac8c0	special issue on advances in image and video processing techniques	video processing	This special issue of the Multimedia Tools and Applications Journal contains extended papers selected from the image and video processing sessions and multimedia sessions of ELMAR symposia held in Zadar, Croatia, in 2008 and 2009 (more information about ELMAR symposia can be found here: www.elmar-zadar.org). Eight selected papers provide a coverage of recent advances in image and video processing. Papers were selected by the Guest Editors of this special issue in September 2009, and the respective authors were invited to submit the extended versions of their papers, along with further research they conducted on the subject. Multimedia is all around us and we are often reminded that we live in the age of multimedia. It is in products and services like digital TV, IPTV, video and image databases, social networking etc. Time between research and final consumer product has never been shorter and this is one of the reasons why multimedia is still an attractive and widely researched area. Given the wide area of multimedia tools and applications usage in real life, it is necessary to research and develop algorithms and methods in various fields, and this special issue reflects this fact in its wide range of topics covered. Multimed Tools Appl (2010) 49:405–408 DOI 10.1007/s11042-009-0459-5	algorithm;database;iptv;real life;video processing	Mislav Grgic;Marta Mrak;Kresimir Delac	2009	Multimedia Tools and Applications	10.1007/s11042-009-0459-5	computer science;digital image processing;video tracking;video processing	HPC	-48.351125498425525	-24.942169141624287	42844
5d7cdbf7fb6d5ebc4c7679ffd2026e9774201b69	mashing up visual languages and web mash-ups	text;mashups;complexity theory;feeds;visual programming;visualization;mashups programming visualization ecosystems communities feeds complexity theory;visual languages;ecosystems;web services;visual language;visual programming visual languages web mash ups human centered computing;human centered computing;communities;programming;web services visual languages visual programming;web mash ups	Research on Web mashups and visual languages share an interest in human-centered computing. Both research communities are concerned with supporting programming by everyday, technically inexpert users. Visual programming environments have been a focus for both communities, and we believe that there is much to be gained by further discussion between these research communities. In this paper we explore some connections between web mashups and visual languages, and try to identify what each might be able to learn from the other. Our goal is to establish a framework for a dialog between the communities, and to promote the exchange of ideas and our respective understandings of human-centered computing.	human-centered computing;knowledge spillover;mash-1;mashup (web application hybrid);visual programming language;dialog	M. Cameron Jones;Elizabeth F. Churchill;Michael Twidale	2008	2008 IEEE Symposium on Visual Languages and Human-Centric Computing	10.1109/VLHCC.2008.4639075	computer science;multimedia;programming language;world wide web	Visualization	-47.169103113827525	-24.87729038415581	42862
