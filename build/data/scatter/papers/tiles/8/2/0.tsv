id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
61c74a1c72f05eacf9e652ca4a7f4d79fa68f367	hong kong english: linguistic and sociolinguistic perspectives	selected works;bepress	Hong Kong English (HKE) has been the subject of a growing body of research over the last few decades. This article presents an overview of research into HKE from both linguistic and sociolinguistic perspectives. The first section reviews the linguistic features of HKE at phonological, grammatical and lexical levels as documented in the literature. In the second section of the article, the development of HKE as a ‘new’ variety of English is discussed from a sociolinguistic viewpoint, with a view to addressing the question of in which phase HKE is currently situated according to Schneider’s (2007) DynamicModel of evolution for New Englishes. A review of the literature suggests that HKE displays a number of phonological, grammatical and lexical features which makes HKE distinguishable from other varieties of English, with the majority of these features attributable to the inf luence of Cantonese, the first language of the majority of HKE speakers. With reference to Schneider’s model, the article suggests that HKE can be situated in phase 3 and is considered an ‘emergent’ variety with its norms still in the process of developing. While there are signs of increasing awareness of HKE as a distinct variety in the local community, ambivalent attitudes towards the acceptability of HKE as the linguistic norm still exist. It remains to be seen whether HKEwill eventually reach phase 4 and become an autonomous variety of English in the near future. This review ends by suggesting several further research directions with respect to the study of HKE.	autonomous robot;in-phase and quadrature components;lexicon;situated	Chit Cheung Matthew Sung	2015	Language and Linguistics Compass	10.1111/lnc3.12142	psychology;linguistics;communication	NLP	-34.067793396068296	-83.05538860438129	29453
57c624d8c758675a06128482bd5e2d9fcece9bbb	conditions for effectively deriving a q-matrix from data with non-negative matrix factorization. best paper award		The process of deciding which skills are involved in a given task is tedious and challenging. Means to automate it are highly desirable, even if only partial automation that provides supportive tools can be achieved. A recent technique based on Non-negative Matrix Factorization (NMF) was shown to offer valuable results, especially due to the fact that the resulting factorization allows a straightforward interpretation in terms of a Q-matrix. We investigate the factors and assumptions under which NMF can effectively derive the underlying high level skills behind assessment results. We demonstrate the use of different techniques to analyse and interpret the output of NMF. We propose a simple model to generate simulated data and to provide lower and upper bounds for quantifying skill effect. Using the simulated data, we show that, under the assumption of independent skills, the NMF technique is highly effective in deriving the Q-matrix. However, the NMF performance degrades under different ratios of variance between subject performance, item difficulty, and skill mastery. The results corroborates conclusions from previous work in that high level skills, corresponding to general topics like World History and Biology, seem to have no substantial effect on test performance, whereas other topics like Mathematics and French do. The analysis and visualization techniques of the NMF output, along with the simulation approach presented in this paper, should be useful for future investigations using NMF for Q-matrix induction from data.	best practice;high-level programming language;non-negative matrix factorization;simulation	Michel C. Desmarais	2011			artificial intelligence;machine learning;computer science;q-matrix;non-negative matrix factorization	AI	-41.38906965745696	-80.27166550237094	38570
13d2825a3c1095ded8c332daec826d12a6d865d1	a revised algorithm for latent semantic analysis	intelligent tutoring system;natural language understanding;natural language;la tent semantic analysis;latent semantic analysis	Particle physics experiments, like the Large Hadron Collider in Geneva, can generate thousands of data points listing detected particle reactions. An important learning task is to analyze the reaction data for evidence of conserved quantities and hidden particles. This task involves latent structure in two ways: first, hypothesizing hidden quantities whose conservation determines which reactions occur, and second, hypothesizing the presence of hidden particles. We model this problem in the classic linear algebra framework of automated scientific discovery due to Valdés-Pérez, Żytkow and Simon, where both reaction data and conservation laws are represented as matrices. We introduce a new criterion for selecting a matrix model for reaction data: find hidden particles and conserved quantities that rule out as many interactions among the nonhidden particles as possible. A polynomial-time algorithm for optimizing this criterion is based on the new theorem that hidden particles are required if and only if the Smith Normal Form of the reaction matrix R contains entries other than 0 or 1. To our knowledge this is the first application of Smith matrix decomposition to a problem in AI. Using data from particle accelerators, we compare our algorithm to the main model of particles in physics, known as the Standard Model: our algorithm discovers conservation laws that are equivalent to those in the Standard Model, and indicates the presence of a hidden particle (the electron antineutrino) in accordance with the Standard Model.	algorithm;data point;electron;experiment;interaction;large hadron collider;latent semantic analysis;linear algebra;polynomial;subatomic particle;time complexity	Xiangen Hu;Zhiqiang Cai;Max M. Louwerse;Andrew Olney;Phanni Penumatsa;Arthur C. Graesser	2003			natural language processing;semantic interpretation;speech recognition;latent semantic analysis;computer science;artificial intelligence;machine learning;linguistics;natural language	ML	-34.49084004725966	-84.42401391511505	41393
943ba7742522761b5747240ebecc76fd0deae749	review: connectives as discourse landmarks, by agnes celle and ruth huart (eds.).		In the volume under review, papers by eleven authors are included which were presented at the international conferenceConnectives asDiscourse Landmarks (University of Paris-Diderot, May 2005). emain point of interest of present studies are syntactic, semantic and pragmatic functions of several discourse connectives in English. e term ‘connective’ is being used in a broad sense, without specific theoretical restrictions, herewith opening space for different treatments of discourse. In the book, this term covers not only traditional connective items like conjunctions (and, but) and relative pronouns (which), but also discourse adverbials (rather, still, yet), phrasal constructions (aer all) as well as whole sentential frames (the fact is that; it’s not that; A because B so A’) andmeans of contact (well, you know). As this set of connectives shows, the studies deal with two large aspects of discourse research: with questions of syntax, semantics and lexicology, understanding connectives as items expressing the relations between sentences (abstract objects, events), andwith pragmatics where connectives are understood as units linking the speaker and the hearer. e editors’ introduction describes briefly the historical context of present-day discourse studies, making short references to RichardG.Warner, Deborah Schiffrin and Jan-OlaÖstman. Further, it explains the development of the discourse terminology and touches upon some open questions of the discourse research, namely the level of grammaticalization of connectives, the issue, whether for the meaning of connectives, their core lexical sense is more important or rather pragmatic sense variance in different contexts and, finally, the relation between the form (conjunctive, subjunctive) and themeaning of connectives. Aer general remarks ondiscourse, the main points of the studies included are shortly summarized. In the “Part I. Connectives andmodality”, Raphael Salkie (“Connectives, modals and prototypes: A study of rather”) focuses on common features of different senses of rather (connective,	calculus of constructions;logical connective;point of interest	Sárka Zikánová	2007	Prague Bull. Math. Linguistics		linguistics;computer science	NLP	-34.17925237980369	-82.40268166252733	46939
aa57c7cf6df7f530854b8feb5ade8eea756381ef	self-assimilation for solving excessive information acquisition in potential learning			data assimilation	Ryotaro Kamimura;Tsubasa Kitago	2018	J. Artif. Intell. Soft Comput. Res.	10.1515/jaiscr-2018-0001	machine learning;computer science;artificial intelligence;assimilation (phonology)	NLP	-35.99934605242858	-83.09850328717707	53828
ad77c782cab713b7fc838460c452874c128c7069	entropy-based assessment of written albanian language	english language;frequency analysis;text processing;upper bound;probability distribution	Abstract The entropy analysis of a printed language is vital for a wide range of applications, such as compression, written text recognition, text processing, and encryption. In this paper, we present empirical results on the entropy of written Albanian language based on a frequency analysis of letters, digrams, trigrams, and for orthographic words of length one, two and three characters. This work represents the first attempt at calculating some statistics for the Albanian language. Our sample consists of more than 1.86 million letters with a total of 433,366 words obtained electronically from various sources – newspapers, journals, books, and websites – and comes from different applications such as business, politics, health, religion, sports, literature, and history. Because the real probability distribution of Albanian language n-grams is not known, we consider our calculated (trigram) entropy of 3.2972 as an upper bound estimate for the true entropy of the Albanian language. We established that words...		Saif alZahir;Arber Borici	2011	Journal of Quantitative Linguistics	10.1080/09296174.2011.533592	natural language processing;language identification;probability distribution;speech recognition;computer science;english;mathematics;linguistics;frequency analysis;upper and lower bounds;statistics	NLP	-36.955453167163	-81.72397008413242	60913
9d6b98edcb5b6d45113d6e7f570c5da8648aad44	the times and the man as predictors of emotion and style in the inaugural addresses of u.s. presidents	political power;discourse semiotics;stylistique;rhetoric;discours public;analyse du discours;president;discours politique;etats unis;rhetorique;stylistics;etude quantitative;emotion;semiotique discursive;pouvoir politique;political discourse;discourse analysis;quantitative study;public discourse	Intercorrelations among stylistic and emotional variables and constructvalidity deduced from relationships to other ratings of U.S. presidentssuggest that power language (language that is linguistically simple,emotionally evocative, highly imaged, and rich in references to Americanvalues) is an important descriptor of inaugural addresses. Attempts topredict the use of power language in inaugural addresses from variablesrepresenting the times (year, media, economic factors) and the man(presidential personality) lead to the conclusion that time-basedfactors are the best predictors of the use of such language (81%prediction of variance in the criterion) while presidential personalityadds at most a small amount of prediction to the model. Changes in powerlanguage are discussed as the outcome of a tendency to opt for breadthof communication over depth.	the times	Cynthia Whissell;Lee Sigelman	2001	Computers and the Humanities	10.1023/A:1017569003556	stylistics;social science;emotion;rhetoric;philosophy;discourse analysis;linguistics;sociology;literature	NLP	-35.084187211909374	-81.93298485167071	67513
9f9e7850dcbc87046e6d403cd3c14d2ac15562ad	information structure in subordinate and subordinate-like clauses	information structure;topic focus;subordinate clause;utterance;discourse structure;theme rheme	While information structure has traditionally been viewed as a single partition of information within an utterance, there are opposing views that identify multiple such partitions in an utterance. The existence of alternative proposals raises questions about the notion of information structure and also its relation to discourse structure. Exploring various linguistic aspects, this paper supports the traditional view by arguing that there is no information structure partition within a subordinate clause.	claire;european summer school in logic, language and information;information theory;mark steedman;mathematical morphology;recursion;semantic prosody	Nobo Komagata	2003	Journal of Logic, Language and Information	10.1023/A:1024158621568	natural language processing;dependent clause;philosophy;mathematics;linguistics	NLP	-33.75487188108996	-81.90546352133002	74559
de4dcb4b11a9a7a5e45a1eac7d8046f8a5442fda	a multimedia presentation system for the remediation of sentence processing deficits				Martin Beveridge;Alison Crerar	1999			environmental remediation;multimedia;computer science;natural language processing;artificial intelligence;speech recognition;sentence processing	NLP	-36.60243979112194	-83.5598557218255	96122
3846e4f18182fd8991197422ad22865d990e8ae8	memory-based context-sensitive spelling correction at web scale	eigenvalues and eigenfunctions;radio networks;traffic dynamics;random correlations;cross correlation;network traffic application;traffic dynamics feature extraction random matrix theory cross correlation matrix null hypothesis random correlations eigenvalue spectra eigenvectors network traffic application network congestion control;feature extraction eigenvalues and eigenfunctions communication system traffic control traffic control machine learning system testing statistics statistical distributions internet application software;telecommunication congestion control;network congestion control;matrix algebra;eigenvalues;eigenvalues and eigenvectors;network traffic;feature extraction;eigenvalue spectra;null hypothesis;telecommunication congestion control eigenvalues and eigenfunctions feature extraction matrix algebra radio networks;random matrix theory;network congestion;cross correlation matrix;eigenvectors	We study the problem of correcting spelling mistakes in text using memory-based learning techniques and a very large database of token n-gram occurrences in web text as training data. Our approach uses the context in which an error appears to select the most likely candidate from words which might have been intended in its place. Using a novel correction algorithm and a massive database of training data, we demonstrate higher accuracy on correcting real- word errors than previous work, and very high accuracy at a new task of ranking corrections to non-word errors given by a standard spelling correction package.	algorithm;calo;carnegie mellon cylab;context-sensitive grammar;grams;ibm notes;instance-based learning;mitchell corporation;n-gram;norman margolus;pal;scalability;spell checker;tom	Andrew Carlson;Ian Fette	2007	Sixth International Conference on Machine Learning and Applications (ICMLA 2007)	10.1109/ICMLA.2007.50	mathematical optimization;combinatorics;multivariate random variable;eigenvalues and eigenvectors;computer science;theoretical computer science;random function;mathematics	DB	-39.192014247570164	-80.56275619191868	103462
08b16af2940b78986ed87df53fffa21f35ee7ac1	modelling control in generation	control structure;different decision;generation tree;advanced wide-coverage generation system;stylistic control;natural language generation;approach factors control;modelling control;different control strategy;control aspect;control strategy	In this paper we present a view of natural language generation in which the control structure of the generator is clearly separated from the content decisions made during generation, allowing us to explore and compare different control strategies in a systematic way. Our approach factors control into two components, a ‘generation tree’ which maps out the relationships between different decisions, and an algorithm for traversing such a tree which determines which choices are actually made. We illustrate the approach with examples of stylistic control and automatic text revision using both generative and empirical techniques. We argue that this approach provides a useful basis for the theoretical study of control in generation, and a framework for implementing generators with a range of control strategies. We also suggest that this approach can be developed into tool for analysing and adapting control aspects of other advanced wide-coverage generation systems.	algorithm;control flow;map;natural language generation	Roger Evans;David J. Weir;John A. Carroll;Daniel S. Paiva;Anja Belz	2007			natural language processing;computer science;artificial intelligence	AI	-35.023943306635154	-83.40125243477708	105600
1271e5a28be29edbee4007ef828629e55c91b107	measuring syntactic variation in dutch dialects	dutch;dialectometrie;techniques informatiques;computacion informatica;dialectology;computational techniques;historia y critica literaria;filologias;grupo de excelencia;universiteitsbibliotheek;variation syntaxique;linguistica;ciencias basicas y experimentales;neerlandais;grupo a;dialectometry;dialectologie;syntactic variation	This research applies dialectometric methods to purely syntactic dialect data. It will be shown that there is geographic cohesion in syntactic variation when viewed in the aggregate. The amount of syntactic variation which can be accounted for by geography will be determined. Dialectometric techniques will be used to develop an additive measure of syntactic differences. Multidimensional scaling will be applied to visualise the geographic distribution of the Dutch dialects with respect to syntactic variation in the aggregate. The Dutch dialect map based on a syntactic measure will be compared with a dialect map based on subjective judgements and a dialect map based on pronunciation differences to put the syntactic measurement results into perspective. An alternative way to measure syntactic distance will be presented and will provide indications for future research to more accurately quantify syntactic variation.	aggregate data;american and british english pronunciation differences;cohesion (computer science);image scaling;multidimensional scaling;utility functions on indivisible goods	Marco R. Spruit	2006	LLC	10.1093/llc/fql043	natural language processing;dialectology;philosophy;linguistics;sociology	NLP	-34.96386451982054	-81.89311458935978	108267
dd2400fe5d016f6802e04004fdf9a6b1985eada4	perspectives on language as a source of social markers		Because linguistic forms vary between groups of speakers, language serves as a source of social markers, allowing people to distinguish between those who do and do not belong to the same social group. This review surveys interdisciplinary perspectives on four issues concerning social markers: (a) the role of interand intra-individual variation, (b) the purpose of social markers, (c) language as an especially good source of social markers and (d) social marking as a source of new dialects. In July 2006, a year after the 7/7 suicide bombings in London, a video statement by one of the bombers was broadcast by Al-Jazeera. A report in The Times newspaper included the following sentence: Shehzad Tanweer, 22, a university dropout from Beeston, Leeds, is shown speaking with a Yorkshire accent, wearing a Muslim headscarf and jabbing his finger at the camera. (Fresco, McGrory, and Norfolk 6 July 2006) Given that The Times is aimed at educated British readers who are well aware that Leeds is in Yorkshire, it is striking that Tanweer’s accent was considered worthy of mention. The implication is that British people expected an Islamic terrorist to sound a particular way and that, by sounding like Alan Bennett, Tanweer did not conform to this expectation. It is interesting to note that, earlier in the same year, BBC had received complaints for hiring a continuity announcer with a Jamaican accent, who did not conform to some radio listeners’ expectations of how the BBC should sound (Kirby 1 April 2006). While the two cases are clearly rather different, both illustrate the fact that linguistic variability is a source of social markers. That is, language can be seen as a collection of variables, and different realisations of these variables – such as particular lexical items, or particular phonetic values of phonemes – become associated with different social groups. This means that when people use language, they reveal social information about themselves that others use to guide their interactions with them. Unsurprisingly, this behaviour has not been ignored by linguists. (Indeed, since Labov’s foundational 1963 study of socially motivated sound change in Martha’s Vineyard, it has been a central focus of the discipline of sociolinguistics.) However, because of its relevance to such issues as intergroup conflict, cooperation, and cultural evolution, it has also attracted interest from researchers in a number of other fields, especially psychology, biology and anthropology. This paper will review interdisciplinary perspectives on four closely related issues that have attracted particular attention: 1. The role of interand intra-individual variation in social marking. 2. The purpose of social markers. 3. Language as a good source of social markers. 4. Social marking as a source of new dialects.	automatic sounding;dropout (neural networks);emoticon;interaction;item unique identification;relevance;scott continuity;spatial variability;the times	Gareth Roberts	2013	Language and Linguistics Compass	10.1111/lnc3.12052	psychology;social group;developmental psychology;communication;social psychology	NLP	-36.31575048705595	-85.21170890002715	119003
d565b015026d5c4e82ef15db8d04f0de70a7196a	recognizing head movement	grammaire minimaliste;linguistica matematica;logique mathematique;logica matematica;minimalist grammar;mathematical logic;linguistique mathematique;computational linguistics	"""Previous studies have provided logical representations and eÆcient recognition algorithms for a simple kind of \minimalist grammars."""" This paper extends these grammars with head movement (\incorporation"""") and aÆx hopping. The recognition algorithms are elaborated for these grammars, and logical perspectives are brie y considered. Michaelis (1998) showed how the derivations of a simple kind of minimalist grammar (MG) (Stabler, 1997) correspond to derivations of exactly the same strings in a multiple context free grammar (MCFG) (Seki et al., 1991). MGs build structures by merging pairs of expressions, and simplifying single expressions by moving a subexpression in them. The basic idea behind the correspondence with MCFGs can be traced back to Pollard (1984) who noticed, in e ect, that when a constituent is going to move, we should not regard its yield as included in the yield of the expression that contains it. Instead, the expression from which something will move is better regarded has having multiple yields, multiple components { the \moving"""" components have not yet reached their nal positions. In a completed derivation, the component strings of all the categories are eventually ordered to yield a sentence. This conception behind the Michaelis result relies on the fact that for any MG, there is a xed, nite bound on the number of categories and rules, and on the number of components that any constituent will have. These recent results led to eÆcient parsing methods (Stabler, 1999; Harkema, 2000), to connections with multimodal logics (Cornell, 1999; Vermaat, 1999; Lecomte and Retor e, 1999), and to a succinct reformulation of MGs (Stabler and Keenan, 2000) in which the multiple components of expressions are explicit. The parsing methods, the logics, and the succinct reformulation were provided for MGs with only overt, phrasal movement (as explained below). This paper extends these results with two kinds of head movement, providing an eÆcient parsing method and some preliminary observations about bringing this work into a logical perspective like that proposed by (Lecomte and Retor e, 1999). And for illustration, in the course of the paper, we present small example grammars for (G1) subject-auxiliary inversion and (G3) aÆx-hopping in English, and (G2) object clitics in French."""	algorithm;context-free grammar;frequency-hopping spread spectrum;koopmans' theorem;mg (editor);minimalist grammar;multimodal interaction;open road tolling;parsing;programming language;source-to-source compiler;surface hopping;theory	Edward P. Stabler	2001		10.1007/3-540-48199-0_15	natural language processing;mathematical logic;phrase structure grammar;computer science;artificial intelligence;computational linguistics;linguistics;algorithm	NLP	-34.16718136482694	-82.48893951987394	119698
528ad983dd090cae4cb024c9a1dda032399043d2	entropy rate of thai text and testing author authenticity using character combination distribution	authorisation;authentication;text analysis authorisation electronic publishing entropy natural languages statistical distributions;text analysis;natural languages;testing;joints;statistical distributions;estimation;probability distribution;thai text sources author authenticity testing character combination distribution entropy rate estimation probability distribution based text authentication information theoretic quantities digital books numerical values entropy based method;entropy probability distribution joints writing testing estimation authentication;entropy rate;writing;entropy;electronic publishing;information theoretic	This paper has two main goals. The first goal is to estimate the entropy rate of Thai text which is found to be roughly 2 bits/character. The second goal is to come up with methods for text authentication based on probability distribution and information theoretic quantities. Using proposed methods, we found that digital books composed by the same author give close numerical values, while those from different authors give much higher differences. Among the three techniques under consideration, we found that the entropy-based method provides the best test. Thirty Thai text sources of various styles are tested to increase reliability of the study. Additionally, the comparison of the effectiveness of proposed methods is shown here.	authentication;book;e-book;entropy rate;information theory;navier–stokes equations;numerical analysis	Theerawat Kiatdarakun;Prapun Suksompong	2012	2012 Second International Conference on Digital Information and Communication Technology and it's Applications (DICTAP)	10.1109/DICTAP.2012.6215415	probability distribution;speech recognition;computer science;machine learning;pattern recognition;data mining;mathematics;electronic publishing;statistics	DB	-37.002129992584216	-81.75630110308978	127559
ea3d5af7400e33f0f4fcdb3edbd930be55bdb6f1	on the notion of contrast in information structure and discourse structure	contraste;information structure;pragmatics;concept linguistique;linguistic concept;filologias;connector;semantics;connecteur;semantique;discourse structure;linguistica;pragmatique;anglais;theme rheme;structure de l information;english;conjonction de coordination;grupo a;structure discursive;coordinating conjunction	The idea of contrast plays an important role in the analysis of information structure and discourse structure. In the literature on information structure, we encounter the concept of contrastive focus and of contrastive topic, and there is also the opinion that focus in general establishes a contrast. In the literature on discourse structure, it is commonly held that there is a discourse relation of contrast which is indicated by, e.g., English but. However, these notions of contrast differ considerably with respect to what is meant by their conception of contrast.	discourse relation	Carla Umbach	2004	J. Semantics	10.1093/jos/21.2.155	philosophy;english;semantics;linguistics;sociology;pragmatics	NLP	-33.728114413930676	-81.7625270213738	128137
11ea866095a0ca430591c439724ce14a5fb9fa71	a comparison of natural (english) and artificial (esperanto) languages. a multifractal method based analysis	word frequency;cantor set;time series;data analysis;pacs numbers;word length;characteristic function	We present a comparison of two english texts, written by Lewis Carroll, one (Alice in wonderland) and the other (Through a looking glass), the former translated into esperanto, in order to observe whether natural and artificial languages significantly differ from each other. We construct one dimensional time series like signals using either word lengths or word frequencies. We use the multifractal ideas for sorting out correlations in the writings. In order to check the robustness of the methods we also write (!) (consider? ) the corresponding shuffled texts. We compare characteristic functions and e.g. observe marked differences in the (far from parabolic) f(α) curves, differences which we attribute to Tsallis non extensive statistical features in the frequency time series and length time series. The esperanto text has more extreme vallues. A very rough approximation consists in modeling the texts as a random Cantor set if resulting from a binomial cascade of long and short words (or words and blanks). This leads to parameters characterizing the text style, and most likely in fine the author writings.	approximation;cantor set;carroll morgan (computer scientist);multifractal system;parabolic antenna;project looking glass;sorting;time series;word lists by frequency	J. Gillet;Marcel Ausloos	2008	CoRR		natural language processing;characteristic function;speech recognition;cantor set;computer science;artificial intelligence;machine learning;time series;word lists by frequency;linguistics;data analysis;algorithm;statistics	NLP	-36.96614301778195	-81.74637163765073	130878
60ed3d69d2a1cd319dd0aec236dea1b389264fbc	catching the cheshire cat	information retrieval;cheshire cat;local syntactic constraint;text-to-speech system;modified measure;useful phrase;mutual information ratio;text to speech;mutual information	"""I N T R O D U C T I O N In Alice's Adventures in Wonderland by Lewis Carrel many of Alice's friends have names that consists of two words, for example: the March Hare, the Mock Turtle, and the Cheshire Cat. '['he individual words in these combinations, if we ignore capitalisation, might be quite common. Individual words usually mean different things when they am free. l:or example, in """"The March against Apartheid"""", and """"The March I tare"""", """"march"""" means totally different things. There is obviously a strong link between """"the"""" and """"march"""", but the link between """"march"""" and """"hare"""" is definitely stronger, at least in Lt;wis Carrol's text. The goal of this paper is to propose a statistic that measures the strength ol7 such glue between words in a sampled text. Finding tile names {)17 Alice's friends can be done by searching for two adjacent words with initial capit~d letters. ()no use of statistical associations could he to find translatable concepts and phrases, that might be expressed with a different number of words in another language. Another possibly interesting use of statistical associations is to predict whether words constitute new or given information in speech. It has been proposed (e.g. H o r n e & Johansson, 1993) that the stress of words in speech is highly dependent on the informational content of the word. Also, statistical associations are not incompatible with the first stages of the """"hypothesis space"""" proposed by Processability Theory (personal communication with Manfred Pienemann of Sydney University, see also Meisel & al., 1981). There are different methods of calculating statistical associations. Yang & Chute (1992) showed that a linear least square mapping of natural language to canonical terms is both feasible, and a way of detecting synonyms. Their method does not seem to detect dependencies in the order of words however. To do this we need a measure that is sensitive to the order between words. In this paper we will use a variant of mutual infi)rmation that derives from Shannon's theory of information. (as discussed in e.g., Salton & McGill, 1983) Definit ions and assumpt ions The definition of a word in a meaninglul way is [:ar from easy, but a working definition, for technical purposes, is to assume that a word equals a string of letters. These 'words' are separated by non-letters. The case of letters is ignored, i.e. converted into lower case. For example: """"there's"""" are two 'words': """"there"""" and ~IS"""". A collocation consists of a word and the word that immediate@ follows. Index I will refer to the first word and 2 to the second word. Index 12 will refer to word 1 followed by word2, and similarly for 2 I. Another assumption is that natural language is morn predictive in the (left-to-right) temporal order, than in tile reversed order. This is motiwtted by the simple obserwttion that speech comes into the system through the ears serially. For example: consider the French phrase """"un ben viu hlanc"""" (Lit. """"a good wine white""""). """"Ben"""" can (relatively often) be followed by """"vin"""", but usually not """"vin"""" by """"ben"""". The same kind of link exists between """"vin"""" and """"bhmc"""", but not between """"blanc"""" and """"vin"""". This linking affects the intonation of French phrases, and also that intonation supports these kinds of links. Note, that this is not an explana-. tion of either intonation or syntax: we mosl likely have to consider massive interaction be-. tween different modalities of language."""	collocation;definition;gerard salton award;ising model;mock object;natural language;schrödinger's cat;sensor;shannon (unit);statistical model;turtle (robot);vehicle identification number;yang	Christer Johansson	1994			natural language processing;variation of information;computer science;pattern recognition;mutual information;speech synthesis;interaction information;information retrieval;pointwise mutual information	NLP	-33.747843733596994	-84.42847750277248	136038
ab425f92f5661ed3ff9715055b02549255873e2a	an experimental evaluation of delimiters in a command language syntax	langage commande;command language;syntax;programming language;delimiteur;syntaxe;langage programmation;experimental evaluation;langage specialise;special purpose language;control language	Abstract   This article examines the effects of delimiters on the cognitive processes involved in command construction and interpretation. After a theoretical analysis of the impact of delimiters on commands, two techniques of delimiter usage were examined: single-level delimiters (space) and two-level hierarchical delimiters (space and comma). Using a within-subject experimental design with 20 subjects, we found that hierarchical delimiters improved the ability of individuals to distinguish fields and identify the grammatical correctness of the commands. This study suggests that the ability to orgainize elements of commands mentally is enhanced when they consist of fields separated by one delimiter and items within fields by another delimiter. Other factors, not examined in this study, should have an impact on production. They may include the name of the command, the order in which operands are presented, and the experience level of the user.		M. L. Schneider;K. Hirsh-Pasek;S. Nudelman	1984	International Journal of Man-Machine Studies	10.1016/S0020-7373(84)80027-9	natural language processing;syntax;computer science;artificial intelligence;linguistics;programming language	Arch	-38.63352675951863	-81.98978402737065	145530
1de2b1a3881cb4c3b26fc37e609580c12e924fbf	some computational properties of tree adjoining grammars	following area;closure result;head grammar;new result;basic notion;tree adjoining grammars;tree adjoining grammar;parsing complexity;natural language grammar;linguistic relevance;detailed investigation;computational property	Tree Adjoining Grammar (TAG) is u formalism for natural language grammars. Some of the basic notions of TAG's were introduced in [Jo~hi,Levy, mad Takakashi I~'Sl and by [Jo~hi, l ~ l . A detailed investigation of the linguistic relevance of TAG's has been carried out in IKroch and Joshi,1985~. In this paper, we will describe some new results for TAG's, espe¢ially in the following areas: (I) parsing complexity of TAG's , (2) some closure results for TAG's, and (3) the relationship to Head grammars. 1. I N T R O D U C T I O N lnvestigatiou of constrained grammatical s y s t e m from the point of view of their linguistic &leqnary and their computational tractability has been a mnjor concern of computational linguists for the last several years. Generalized Phrase Structure grammars (GPSG), Lexical Functional grunmmm (LFG), Phrm~ Linking grammars (PLG), and Tree Adjoining grammars (TAG) are some key examples of grammatical systems that have been and still continue to be investignted along theme lines. Some of the bask notions of TAG's were introduced in [Joahi, Levy, and Takahashi,1975] and [Jo~hi,198,3 I. Some pretiminav/ investigations of the linguistic relevance and some computational properties were also carried out in [Jo~hi, l~S3 I. More recently, a detailed iuvestigution of the linguistic relevance of TAG's were carried out by [Kro~h and Joshi, 19851. In this paper, we will des¢ribe some new results for TAG's, especially in the following areas: (I) parsing complexity of TAG's , (2) some closure results for TAG's, and (3) the relationship to Head grammar*. These topics will be covered in Sections 3, 4, and $ respectively. In section 2, we will give an introduction to TAG's. In section 6, we will s ta te some properties not discussed here. A detailed exposition of these results is given in [ V i j a y S b u h ~ and Joahi,1985[. *This work w u p t r t i s J~ su.~ported by NSP Gr~u~* Mk'TS-4~IOII6.~'~R, MCS42-07.~94. We wta t to thank Cl r | Pol!ard. Kelly Rozeh, David S e ~ t ad David Weu'. We have beDeflt~l enormously I:y v*/uablo di~*eo~iotc with them. 82 2. T R E E A D J O I N I N G G R A M M A R S T A G ' s We now introduce tree adjoining grammars (TAG's). TAG's are more powerful than CFG's, botb weakly and strongly, l TAG's were first introduced in [Joshi, Levy, and Takahashi,1975J and [Joehi,1983 I. We include their description in this ~*ction to make the paper ~lf-contalned. We can define a tree adjoining grammar as follows. A tree adjoining grammar G is a paw (i,A) where i is a set of initial trees, and A is a set of auxiliary trees. A tree a ls an initial tree if it is of the form	computation;computational linguistics;context-free grammar;emoticon;formal grammar;generalized phrase structure grammar;head grammar;kelly criterion;kleene star;lexical functional grammar;natural language;parsing;relevance;tree (data structure);tree-adjoining grammar	K. Vijay-Shanker;Aravind K. Joshi	1985			natural language processing;tree-adjoining grammar;l-attributed grammar;computer science;linguistics;communication	NLP	-34.31224161315667	-82.41277262707032	154536
ff6cca218cade88cfb823c96372a302cefa125cb	incrementality and the dynamics of routines in dialogue		We propose a novel dual processing model of linguistic routi nisation, specifically formulaic expressions (from relatively fixed idioms, all the way through to looser collocational phenomena). This model is formalised using the Dynamic Syntax (DS) forma l account of language processing, whereby we make a specific extension to the core DS lexical arc hitecture to capture the dynamics of linguistic routinisation. This extension is inspired by wo rk within cognitive science more broadly. DS has a range of attractive modelling features, such as full incrementality, as well as recent accounts of using resources of the core grammar for modelling a range of dialogue phenomena, all of which we deploy in our account. This leads to not only a fully i ncremental model of formulaic language, but further, this straightforwardly extends to rout inised dialogue phenomena. We consider this approach to be a proof of concept of how interdisciplina ry work within cognitive science holds out the promise of meeting challenges faced by modellers of d ialogue and discourse. c ©2011 Andrew Gargett Submitted 1/2010; Accepted 11/2010; Published online 5/201 1	cognitive science;emergence;lexicon;loose coupling;programming idiom	Andrew Gargett	2011	D&D		natural language processing;computer science;artificial intelligence;communication	NLP	-34.46297958125152	-81.2896763981645	160799
fa021f7437384d4e3436492ac651dd8c0d88510d	autumn: a general pitch-extraction wave-to-midi transcription system		Automatic Music Transcription is an area of active research at the intersection of Computer Science and Music, of interest for both theoretical and practical applications. We propose two Wave-to-MIDI transcription systems: Basic AUTUMN and V-AUTUMN. Both systems achieve their goal by performing window-by-window frequency-domain pitch extraction. Minimal assumptions are made on the content of the input signal. Listening tests show that AUTUMN performs acceptably well, with test subjects giving the system a rating of roughly 5 on a 9-point scale across a comprehensive range of musical examples. Quantitative analysis shows that AUTUMN can achieve success rates as high as 95%, with under 5% false-hit rates. Stability is the most important remaining problem to address.		Kevin J. Di Filippo;Andrew Horner;Eric Fung;Jenny Lim;Lydia Ayers	2006			computer hardware;midi;transcription (biology);computer science	ML	-41.36136316368943	-80.38711976949473	163578
302859a8d9522e623dc9da76386affa053f24e58	the prague historical collection of tuning forks: a surviving replica of the koenig tonometre		Despite the copious advances in acoustic techniques and devices for measuring and displaying acoustic parameters, a steady component of a phonetician’s scientific method remains to be – to this very day – our hearing. This paper therefore focuses on a historical device shared between acoustics and auditory phonetic analysis: Rudolph Koenig’s Grand Tonometre, consisting of more than a hundred tuning forks, which occupies an exceptional position among the vast array of instruments manufactured by Koenig. One copy of the original apparatus survived the Second World War untouched, and is now stationed at the Institute of Phonetics in Prague. The paper describes in detail the Prague collection of tuning forks, with photographs illustrating the device, and provides a commentary on the use of tuning forks in phonetic research of that time. In addition, the intricate history of the Prague tonometre is discussed, pertaining to the efforts of Josef Chlumský, the founder of the Czech phonetics institute, to obtain the device from the French colleagues.	acoustic cryptanalysis;days gone;experiment;fork (software development);josef čapek	Pavel Sturm	2015			speech recognition;replica;computer science;tuning fork	HCI	-40.881653326836464	-81.61829496391569	185507
4daf9092ddc33595c99a934ba1a5d295854109ba	game of tropes: exploring the placebo effect in computational creativity		Twitter has proven itself a rich and varied source of language data for linguistic analysis. For Twitter is more than a popular new channel for social interaction in language; in many ways it constitutes a whole new genre of text, as users adapt to its new limitations (140 character messages) and to its novel conventions such as retweeting and hash-tagging. But Twitter presents an opportunity of another kind to computationally-minded researchers of language, a generative opportunity to study how algorithmic systems might exploit linguistic tropes to compose novel, concise and re-tweetable texts of their own. This paper evaluates one such system, a Twitterbot named @MetaphorMagnet that packages its own metaphors and ironic observations as pithy tweets. Moreover, we use @MetaphorMagnet, and the idea of Twitterbots more generally, to explore the relationship of linguistic containers to their contents, to understand the extent to which human readers fill these containers with their own meanings, to see meaning in the outputs of generative systems where none was ever intended. We evaluate this placebo effect by asking human raters to judge the comprehensibility, novelty and aptness of texts tweeted by simple and sophisticated Twitterbots. Tropes: Containers of Meaning A mismatch between a container and its contents can often tell us much more than the content itself, as when a person places the ashes of a deceased relative in a coffee can, or sends a brutal death threat in a Hallmark greeting card. The communicative effectiveness of mismatched containers is just one more reason to be skeptical of the Conduit metaphor (Reddy, 1979) – which views linguistic constructs as containers of propositional content to be faithfully shuttled between speaker and hearer – as a realistic model of human communication. Language involves more than the faithful transmission of logical propositions between information-hungry agents, and more effective communication – of attitude, expectation and creative intent – can often be achieved by abusing our linguistic containers of meaning than by treating them with the sincerity that the Conduit metaphor assumes. Consider the case of verbal irony, in which a speaker deliberately chooses containers that are pragmatically illsuited to the conveyance of their contents. For instance, the advertising container “If you only see one [X] this year, make it this one” assumes that [X] denotes a category of event – such as “romantic comedy” or “movie about superheroes” – with a surfeit of available members for a listener to choose from. When [X] is bound to the phrase “comedy about Anne Frank” or “musical about Nazis”, the container proves too hollow for its content, and the reader is signaled to the presence of playful irony. Though such a film may well be one-of-a-kind, the illfitting container suggests there are good reasons for this singularity that do not speak to X’s quality as an artistic event. Yet if carefully chosen, an apparently inappropriate container can communicate a great deal about a speaker’s relationship to the content conveyed within, and as much again about the speaker’s relationship to their audience. As more practical limitations are placed on the form of linguistic containers, the more incentive one has to exploit or abuse containers for creative ends. Consider the use of Twitter as a communicative medium: writers are limited to micro-texts of no more than 140 characters to convey both their meaning and their attitude to this meaning. So each micro-text, or tweet, becomes more than a container of propositional content: each is a brick in a larger edifice that comprises the writer’s online personae and textual aesthetic. Many Twitter users employ irony and metaphor to build this aesthetic and thus build up a loyal audience of followers for their world view. Yet Twitter challenges many of our assumptions about irony and metaphor. Such devices must be carefully modulated if an audience is to perceive a speaker’s meaning in the playful (mis)match of a linguistic container to its contents. Failure to do so can have serious repercussions when one is communicating to thousands of followers at once, with tweets that demand concision and leave little room for nuance. It is thus not unusual for even creative tweets to come packaged with an explicit tag such as #irony, #sarcasm or #metaphor. Metaphor and irony are much-analysed phenomena in social media, but this paper takes a generative approach, to consider the production rather than the analysis of creative linguistic phenomena in the context of a fullyautonomous computational agent – a Twitterbot – that crafts its own metaphorical and ironical tweets from its own knowledge-base of common-sense facts and beliefs. How might such a system exhibit a sense of irony that human users will find worthy of attention, and how might this system craft interesting metaphoric insights from a knowledge-base of everyday facts that are as banal as they are uncontentious? We shall explore the variety of linguistic containers at the disposal of this agent – a real computational system named @MetaphorMagnet – to better understand how such containers can be playfully exploited to convey ironic, witty or thought-provoking views on the world. With @MetaphorMagnet we aim to show that interesting messages are not crafted from interesting contents, or at least not necessarily so. Rather, effective tweets emerge from an appropriate if nonobvious combination of familiar linguistic containers with unsurprising factual fillers. In support of this view, we shall present an empirical analysis of the assessment of @MetaphorMagnet’s uncurated outputs by human judges. Just as one can often guess the contents of a physical container by its shape, one can often guess the meaning of a linguistic container by its form. We become habituated to familiar containers, and just as we might imagine our own uses for a physical container, we often pour our own meanings into suggestive textual forms. For in language, meaning follows form, and readers will generously infer the presence of meaning in texts that are well-formed and seemingly the product of an intelligent entity, even if this entity is not intelligent and any meaning is not intentional. Remarkably, Twitter shows that we willingly extend this generosity of interpretation to the outputs of bots that we know to be unthinking users of wholly aleatoric methods. Twitterbots exploit this placebo effect – wherein a wellformed linguistic container is presumed to convey a wellfounded semantic content – by serving up linguistic forms that readers tacitly fill with their own meanings. We aim to empirically demonstrate here that readers do more than willingly suspend their disbelief, and that a well-packaged linguistic form can seduce readers into seeing what it not there: a comprehensible meaning, or at least an intent to be meaningful. We do this by evaluating two metaphorgenerating bots side-by-side: a rational, knowledge-based Twitterbot named @MetaphorMagnet vs. an aleatoric and largely knowledge-free bot named @MetaphorMinute. Digital Surrealists: La Règle Du Jeu Most Twitterbots are simple, rule-based systems that use stochastic methods to explore a loosely-defined space of texual forms. Such bots are high-concept, low-complexity text-production mechanisms that transplant the aleatoric techniques of the surrealist poets – from André Breton to William Burroughs and Brion Gysin – into the realms of digital content, social networking and online publishing. Each embodies a language game with its own generative rules, or what Breton called “la règle du jeu.” Yet Breton, Burroughs and Gysin viewed the use of aleatorical rules as merely the first stage of a two-stage creation process: at this first stage, random recombinant methods are used to confect candidate texts in ways that, though unguided by meaning, are also free of the baleful influence of cliché; at the second stage, these candidates are carefully filtered by a human, to select those that are novel and interesting. Most bots implement the first stage and ignore the second, pushing the task of critiquing and filtering candidate texts onto the humans who read and selectively re-tweet them. Nonetheless, some bots achieve surprising effects with the simplest language tools. Consider @Pentametron, a bot that generates accidental poetry by re-tweeting pairs of random tweets of ten syllables apiece (for an iambic pentameter reading) if each ends on a rhyming syllable. When the meaning of each tweet in a couplet coheres with the other, as in “Pathetic people are everywhere” |“Your web-site sucks, @RyanAir”, the sum of tweets produces an emergent meaning that is richer and more resonant than that of either tweet alone. Trending social events such as the Oscars or the Super Bowl are especially conducive to just this kind of synchronicity, as in this fortuitous pairing: “So far the @SuperBowl commercials blow.” | “Not even gonna watch the halftime show.” In contrast, a bot named @MetaphorMinute wears its aleatoric methods on its sleeve, for its tweets – such as “a haiku is a tonsil: peachblow yet snail-paced” – are not so much random metaphors as random metaphor-shaped texts. Using a strategy that stresses quantity over quality, this bot instantiates that standard linguistic container for metaphors – the copula frame “X is a Y” – with mostly random word choices every two minutes. Interestingly, its tweets are as likely to provoke a sense of mystification and ersatz profundity as they are total incomprehension. Yet bots such as @Pentametron and @MetaphorMinute do not generate their texts from the semantic-level up; rather, they manipulate texts at the word-level, and thus lack any sense of the meaning of a tweet, or any rationale for why one tweet might be better – which is to say, more interesting, more apt or more re-tweetable – than others. The Full-FACE poetry generator of Colton 	computation;computational creativity;conduit metaphor;design rationale;digital recording;emergence;error-tolerant design;frank soltis;generative systems;gon;haiku;irony;linear algebra;mediawiki;microprinting;modulation;numerical aperture;realms;reblogging;recombinant dna;rule-based system;social media;syllable;synchronicity;well-formed element	Tony Veale	2015			placebo;computational creativity;psychology;social psychology	NLP	-33.81300781596454	-84.3536636335874	186690
6c0a736da68f6726393aea6fd287b862b607d835	representation and storage of motion data	candidate keys;motion;null values;motion byte;binary representation	This research proposes a model for representation and storage of motion data. Such representation will enable one to communicate, store, and analyze patterns of motion in a way similar to that with spoken and written languages. The basic problem is the lack of a machine readable motion alphabet. We thus set out to define the elemental components and building blocks of motion, coming up with a Motion Byte. Such building blocks are suggested as basis for a motion language that has words, phrases, and sentences. The binary-based model, significantly different from the common â€œkey framesâ€ approach, is also a method of storing motion data. Comparison of our method with a standard motion system, based on key frames, indicates a significant advantage for the binary model.		Roy Gelbard;Israel Spiegler	2002	J. Database Manag.	10.4018/jdm.2002070104	binary number;computer science;theoretical computer science;motion;motion estimation;data mining;database;motion field;candidate key;algorithm	DB	-34.32941645577765	-85.8955899462983	192097
2d0b3ec13f0f7303f7db0cc32b5960058849df5b	semeval-2010 task 13: evaluating events, time expressions, and temporal relations (tempeval-2)	time expression;semeval-2010 evaluation exercise;temporal expression;temporal relation;semeval-2010 task;distinct subtasks;event expression;anchoring event;tempeval-2 task	We describe the TempEval-2 task which is currently in preparation for the SemEval-2010 evaluation exercise. This task involves identifying the temporal relations between events and temporal expressions in text. Six distinct subtasks are defined, ranging from identifying temporal and event expressions, to anchoring events to temporal expressions, and ordering events relative to each other.	markup language;natural language;regular expression;semeval;temporal expressions;temporal logic;timeml	James Pustejovsky;Marc Verhagen	2009			real-time computing;data mining;mathematics;communication	NLP	-35.49180082341072	-80.71874316310173	192225
499ae9db8ac70a6caedc0f7a555fe92f74974e17	quantification and acd: evidence from real-time sentence processing	quantification acd quantifier raising sentence processing;article	Quantifiers, unlike proper names or definite descriptions, cannot be given the semantics of referring expressions. This fact has triggered a long-standing debate in formal semantics and syntax as to the combinatorial means by which quantifiers are integrated into a sentence. The present paper contributes to this debate through an investigation of quantifier comprehension during real-time sentence processing. We present evidence showing that two potentially independent processes—the integration of a quantifier in object position and the resolution of antecedent-contained deletion (ACD)—are linked. Our data show, more specifically, that the resolution of a downstream ACD site is facilitated during real-time sentence processing if the upstream DP hosting the ACD site is quantificational but not if it is definite. We discuss these findings in the context of a QUANTIFIER RAISINGbased approach and a type-shifting-based approach to quantifier integration. We argue that facilitation of ACD resolution by an upstream quantifier is only expected by theories, such as the QUANTIFIER RAISING approach, which employ the same mechanism for both processes. We then compare the QUANTIFIER RAISING-based account with a non-grammatical experience-based approach to our data, which attempts to explain the findings in terms of corpus frequencies. Although we cannot rule out such an alternative at this stage, we offer reasons to believe that an account that exploits QUANTIFIER RAISING has an explanatory	downstream (software development);quantifier (logic);real-time clock;real-time transcription;regular expression;semantics (computer science)	Martin Hackl;Jorie Koster-Hale;Jason Varvoutis	2012	J. Semantics	10.1093/jos/ffr009	philosophy;linguistics	AI	-33.84785654337276	-81.5170675160502	192475
fe20684c7af6845e164cc2f9c8e7c377f386f4e4	the vocal tract of newborn humans and neanderthals: acoustic capabilities and consequences for the debate on the origin of language. a reply to lieberman (2007a)	vocal tract	In most cases authors are permitted to post their version of the article (e.g. in Word or Tex form) to their personal website or institutional repository. Authors requiring further information regarding Elsevier's archiving and manuscript policies are encouraged to visit:	acoustic cryptanalysis;archive;humans;microsoft word for mac;personal web page;tract (literature)	Louis-Jean Boë;Jean-Louis Heim;Kiyoshi Honda;Shinji Maeda;Pierre Badin;Christian Abry	2007	J. Phonetics	10.1016/j.wocn.2007.06.006	psychology;vocal tract;acoustics;philosophy;linguistics;sociology;communication	NLP	-36.26118351048555	-85.23298725331546	199465
