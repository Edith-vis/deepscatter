id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
d55111da42cf87d98d44e0125f1d23347863ddc3	an iot monitoring system for precision viticulture		This article presents a hardware and software platform for remote monitoring vineyards in the Internet of Things (IoT) era. The system proposed is a Wi-Fi based wireless sensor network composed by autonomous and self-powered nodes deployed throughout a vineyard. Such nodes comprise sensors that allow for obtaining exhaustive knowledge on viticulture processes. Since its deployment in 2016, the system has been generating alerts that warn vine growers about the measures that have to be taken and it has stored the historical weather data gathered from various locations of the vineyard. Collected data can then be displayed in a web-based interface that can be accessed through the Internet by using desktop or mobile devices. Its modular design enables the easy addition of alarms, sensors, and actuators. Furthermore, it supports different epidemiological models in order to prevent certain diseases existing in an area.	autonomous robot;desktop computer;esp8266;ecology;email;experiment;internet of things;microcontroller;mobile device;modular design;requirement;scalability;sensor;smartphone;software deployment;tablet computer;usability;web application;world wide web	Josman P. Pérez-Expósito;Tiago M. Fernández-Caramés;Paula Fraga-Lamas;Luis Castedo	2017	2017 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)	10.1109/iThings-GreenCom-CPSCom-SmartData.2017.104	wireless sensor network;software deployment;embedded system;the internet;precision viticulture;mobile device;modular design;software;computer science;vineyard	Embedded	-30.68088651837251	18.919854203805134	13495
81cb15a0158e4867439f93b430938c59966a8e0d	an infinitary rewriting interpretation of coinductive types		We introduce an infinitary rewriting semantics for strictly positive nested higher-order (co)inductive types. This may be seen as a refinement and generalization of the notion of productivity in term rewriting to a setting with higher-order functions and with data specifed by nested higher-order inductive and coinductive definitions. We prove an approximation theorem which essentially states that if a term reduces to an arbitrarily large finite approximation of an infinite object in the interpretation of a coinductive type, then it infinitarily reduces to an infinite object in the interpretation of this type. We introduce a sufficient syntactic correctness criterion, in the form of a type system, for finite terms decorated with type information. Using the approximation theorem we show that each well-typed term has a well-defined interpretation in our infinitary rewriting semantics. This gives an operational interpretation of typable terms which takes into account the “limits” of infinite reduction sequences.		Lukasz Czajka	2018	CoRR		mathematics;discrete mathematics;coinduction;correctness;arbitrarily large;semantics;generalization;syntax;rewriting	PL	-8.891398880250986	15.18784686549897	13515
6d4fbe97de371d63383b3e3c3c101f566c4c9eb6	evaluation of relational algebras incorporating the time dimension in databases	base relacional dato;query language;valid time;relation algebra;transaction time;time variation;query optimization;snapshot relation;relational database;variation temporelle;satisfiability;design space;lenguaje interrogacion;chronon;historical relation;base donnee relationnelle;time varying data;langage interrogation;systeme gestion base donnee;sistema gestion base datos;database management system;variacion temporal;homogeneity;aggregate	The relational algebra is a procedural query language for relational databases. In this paper we survey extensions of the relational algebra that can query databases recording time-varying data. Such an algebra is a critical part of a temporal DBMS. We identify 26 criteria that provide an objective basis for evaluating temporal algebras, Seven of the criteria are shown to be mutually unsatisfiable, implying there can be no perfect temporal algebra, Choices made as to which of the incompatible criteria are satisfied characterize existing algebras Twelve time-oriented algebras are summarized and then evaluated against the criteria. We demonstrate that the design space has in some sense been explored in that all combinations of basic design decisions have at least one representative algebra. Coverage of the remaining criteria provides one measure of the quality of each algebra We argue that all of the criteria are independent and that the criteria identified as compatible are indeed so, Finally, we list plausible properties proposed by others that are either subsumed by other criteria, are not well defined, or have no objective basis for being evaluated. The algebras realize many different approaches to what appears initially to be a straightforward design task.	query language;relational algebra;relational database	L. Edwin McKenzie;Richard T. Snodgrass	1991	ACM Comput. Surv.	10.1145/125137.125166	query optimization;aggregate;homogeneity;relational database;computer science;transaction time;relation algebra;database;chronon;valid time;algorithm;query language;satisfiability	DB	-26.746337172967245	10.740925893108713	13518
a8d75c28cef51494f603daf8addadee2fce453f2	tensed modalities		In the past twenty-five years a number of formal systems concerned with some such notion as the realisation or truth at a time of tensed or chronologically indetite propositions have been constructed.1 Although these have been discussed in connection with theories of temporal modalities (e.g. the Megarian in which ‘necessarily p’ is interpreted as ‘p is true at all times’)s, and interpretutions have been given them in terms of ordinary modal systems such as Lewis’s S5,s the question has not been raised as to the possibility of combining them with modal systems: their tensed or chronologically indefinite propositions are always non-modal in form. After making some introductory remarks I wish to explore the idea of tensed mod&ties. In Section III I shall suggest a formal system for them. Briefly, the basic idea of these systems is that a tensed or chronologically indelinite proposition is one which may be true or realised at one time and not at another. For example, my doorbell may be ringing at one time, tl, and not at another, tz. Thus if an operator ‘JC (read as ‘ . . . is realised at.. .‘), with one place for a time and one for a proposition, is introduced the proposition that my doorbell is ringing at tl may be rendered ‘Rtlpl’ this latter proposition being untensed. It is supposed that the &operator can be applied not merely to a time (e.g. tl) and a tensed proposition (e.g. pi), but also to a time (e.g. tJ and an already untensed proposition (e.g. Rtlpl) the result being the also untensed proposition, Rt2Rtlpl. One such system, chosen here for convenience, is that of Prior4 which Rescher s call ‘P5’. Given the propositional calculus and some elementary quantification theory, s it has as axioms, whose propositional variables take as values tensed and untensed propositions indifferently:	first-order logic;formal system;modal logic;propositional calculus;ringing (signal);transform, clipping, and lighting	Roger S. Woolhouse	1973	J. Philosophical Logic	10.1007/BF00129611		AI	-12.263903625209869	5.968527157916506	13642
2d38b68ab283dcd1436f092ebb69c41f2e397f8b	classes of structures with no intermediate isomorphism problems		We say that a theory T is intermediate under effective reducibility if the isomorphism problems among its computable models is neither hyperarithmetic nor on top under effective reducibility. We prove that if an infinitary sentence T is uniformly effectively dense, a property we define in the paper, then no extension of it is intermediate, at least when relativized to every oracle on a cone. As an application we show that no infinitary sentence whose models are all linear orderings is intermediate under effective reducibility relative to every oracle on a cone.	computable function;hyperarithmetical theory	Antonio Montalbán	2016	J. Symb. Log.	10.1017/jsl.2014.55	combinatorics;discrete mathematics;mathematics;algorithm	Theory	-8.302626140962895	15.535617380580776	13665
697b8e34d59d33c60bf28a7d4e76d073f4950154	translating c programs to msvl programs		C language is one of the most popular languages in system programming and applications written in C have been widely used by different industries. In order to improve the safety and reliability of these applications, a runtime verification tool UMC4MSVL based on Modeling, Simulation and Verification Language (MSVL) is employed. To do so, C programs have to be translate into MSVL programs. This paper presents an algorithm to achieve the translation from a C program to an equivalent MSVL program in one-to-one manner. The proposed algorithm has been implemented in a tool called C2M . A case study is given to show how the approach works.		Meng Wang;Cong Tian;Nan Zhang;Zhenhua Duan;Chenguang Yao	2018	CoRR		software system;programming language;theoretical computer science;equivalence (measure theory);compiler;runtime verification;existential quantification;expression (mathematics);syntax;computer science;goto	PL	-20.85245750766145	26.665211114284748	13704
b8b5d832627fc993628bef2ce862cf66bf6f78d6	a type system for pspace derived from light linear logic		We present a polymorphic type system for lambda calculus ensuring that well-typed programs can be executed in polynomial space: dual light affine logic with booleans (DLALB). To build DLALB we start from DLAL (which has a simple type language with a linear and an intuitionistic type arrow, as well as one modality) which characterizes FPTIME functions. In order to extend its expressiveness we add two boolean constants and a conditional constructor in the same way as with the system STAB in [7]. We show that the value of a well-typed term can be computed by an alternating machine in polynomial time, thus such a term represents a program of PSPACE (given that PSPACE = APTIME ([5])). We also prove that all polynomial space decision functions can be represented in DLALB. Therefore DLALB characterizes PSPACE predicates.	abstract machine;input/output;lambda calculus;linear logic;modality (human–computer interaction);ph (complexity);pspace;polynomial hierarchy;time complexity;type system	Lucien Capedevielle	2011		10.4204/EPTCS.75.4	combinatorics;discrete mathematics;pspace;mathematics;algorithm	PL	-9.419804609314992	18.108339627530956	13732
2a696d314d8d1c89b839159044d44d049e704e0b	when is a fallacy valid? reflections on backward reasoning			backward chaining;reflection (computer graphics)	Romane Clark	1982	Notre Dame Journal of Formal Logic	10.1305/ndjfl/1093883560	formal fallacy;deductive fallacy;base rate fallacy;circular reasoning;fallacy of four terms;mathematical fallacy	Logic	-14.464201460647596	11.266311058333871	13773
b0d93f14e0cee4e214ed77b3b5d8de2d1371945f	proof systems for lattice theory	lattice theory;computacion informatica;ciencias basicas y experimentales;matematicas;grupo a	A formulation of lattice theory as a system of rules added to sequent calculus is given. The analysis of proofs for the contraction-free calculus of classical predicate logic known as G3c extends to derivations with the mathematical rules of lattice theory. It is shown that minimal derivations of quantifier-free sequents enjoy a subterm property: all terms in such derivations are terms in the endsequent.An alternative formulation of lattice theory as a system of rules in natural deduction style is given, both with explicit meet and join constructions and as a relational theory with existence axioms. A subterm property for the latter extends the standard decidable classes of quantificational formulas of pure predicate calculus to lattice theory.		Sara Negri;Jan von Plato	2004	Mathematical Structures in Computer Science	10.1017/S0960129504004244	discrete mathematics;lattice;proof theory;mathematics;proof calculus;natural deduction;algebra	Logic	-11.506051631409543	16.12148090737135	13783
7a4aaaac2f77f985aa4b4c288b9c760eca21297b	algebraic specification and automatic generation of compilers	lenguaje programacion;algebraic specification;compilateur;programming language;generacion lenguaje;sistema informatico;computer system;specification programme;compiler;methode algebrique;automatic generation;algebraic method;langage programmation;systeme informatique;metodo algebraico;generation langage;program specification;especificacion programa;compilador;language generation	Abstract#R##N##R##N#This paper proposes an algebraic specification method for the compiler and presents a method which automatically generates a compiler from the specification. In the proposed description, the specification of the compiler is described by regarding the compiler as a function on an algebraic domain that contains syntactic domains of the source and the target languages as the subdomain. This approach provides a natural description to the essential part of the compiler, such as the assignment of the area to store the variables within the algebraic framework. An experiment is executed and the result is discussed, where the specification of the compiler of the programming language PL/0 is actually described, and the compiler is generated automatically from the specification.	algebraic specification;compiler	Masahiko Sakai;Toshiki Sakabe;Yasuyoshi Inagaki	1992	Systems and Computers in Japan	10.1002/scj.4690230201	manifest expression;compiler;common subexpression elimination;compiler correctness;interprocedural optimization;computer science;loop optimization;theoretical computer science;compiler construction;optimizing compiler;bootstrapping;programming language;static single assignment form;inline expansion;intrinsic function;functional compiler;algorithm	HPC	-22.98395053053223	28.081862041983385	13811
0fdf5b1efb4640c6ece6d07aa36997c22667ec7f	snark: a language to represent declarative knowledge and an inference engine which uses heuristics.				Jean-Louis Laurière;M. Vialatte	1986			natural language processing;computer science;machine learning;programming language	AI	-18.8518420344398	12.027645215972939	13857
c75991e1d8a6f400dfd8df02ecc82192361b65c1	meta-programming through typeful code representation	meta programming	By allowing the programmer to write code that can generate code at run-time, meta-programming offers a powerful approach to program construction. For instance, meta-programming can often be employed to enhance program efficiency and facilitate the construction of generic programs. However, meta-programming, especially in an untyped setting, is notoriously error-prone. In this paper, we aim at making meta-programming less error-prone by providing a type system to facilitate the construction of correct meta-programs. We first introduce some code constructors for constructing typeful code representation in which program variables are represented in terms of deBruijn indexes, and then formally demonstrate how such typeful code representation can be used to support meta-programming. With our approach, a particular interesting feature is that code becomes first-class values, which can be inspected as well as executed at run-time. The main contribution of the paper lies in the recognition and then the formalization of a novel approach to typed meta-programming that is practical, general and flexible.	metaprogramming	Chiyan Chen;Hongwei Xi	2005	J. Funct. Program.	10.1017/S0956796805005617	metaprogramming;computer science;theoretical computer science;redundant code;programming language;algorithm;code generation;source code	PL	-22.556903139833107	27.036300522495207	13874
3fbb2c2b2928cce3b4414679fd7e5ac8bf8ec214	simulation fundamentals	monte carlo methods;digital simulation;statistical analysis;monte carlo simulation;r statistical language;built-in functions;dice game craps;learning curve;simulation 101 workshop;simulation libraries;single-server queue;software suite;winter simulation conference	This paper provides an introduction to simulation fundamentals via the Monte Carlo and next-event approaches to simulation, supported by two representative programs from the software suite written specifically for the Winter Simulation Conference's Simulation 101 workshop. The simulation libraries and functions associated with workshop are written using the R statistical language. R is suitable for an introduction to simulation because of its easy learning curve and its wide range of statistical procedures, built-in functions, and graphics capabilities. This paper begins with general instructions for downloading, compiling, and executing the software. This is followed by explanations of the Monte Carlo and next-event approaches, using two examples: craps() uses Monte Carlo simulation to estimate the probability of winning the dice game Craps, and ssq3() uses a next-event approach to estimate several measures of performance associated with a single-server queue.	compiler;download;graphics;library (computing);monte carlo method;r language;server (computing);simulation;software suite	Barry Lawson;Lawrence Leemis	2009	Proceedings of the 2009 Winter Simulation Conference (WSC)		computational science;simulation;computer science;statistics;monte carlo method	EDA	-32.663847331113594	27.26915312713696	13905
9c7c8df9fb6e37a7da357443f07453ca6137f253	term graphs for computing derivatives in imperative languages	programming language;term graph;rule based;computational graph;automatic differentiation;fortran	Automatic differentiation is a technique for the rule-based transformation of a subprogram that computes some mathematical function into a subprogram that computes the derivatives of that function. Automatic differentiation algorithms are typically expressed as operating on a weighted term graph called a linearized computational graph. Constructing this weighted term graph for imperative programming languages such as C/C++ and Fortran introduces several challenges. Alias and definition-use information is needed to construct term graphs for individual statements and then combine them into one graph for a collection of statements. Furthermore, the resulting weighted term graph must be represented in a language-independent fashion to enable the use of AD algorithms in tools for various languages. We describe the construction and representation of weighted term graphs for C/C++ and Fortran, as implemented in the ADIC 2.0 and OpenAD/F tools for automatic differentiation.	imperative programming	Paul D. Hovland;Boyana Norris;Michelle Mills Strout;Jean Utke	2007	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2006.10.029	automatic differentiation;rule-based system;lattice graph;null model;graph product;null graph;graph property;graph labeling;computer science;clique-width;theoretical computer science;abstract semantic graph;mathematics;voltage graph;graph;programming language;algorithm;graph rewriting	Theory	-23.185809238446613	24.751890032844397	13923
79d0703e8bb1f0947eaaf22aa50aa5e5037ad517	theorem proving in higher order logics		ion and Refinement in Higher Order Logic . . . . . . . . . . . . . . . . . . . . . 201 Matt Fairtlough, Michael Mendler, Xiaochun Cheng A Framework for the Formalisation of Pi Calculus Type Systems in Isabelle/HOL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217 Simon J. Gay Representing Hierarchical Automata in Interactive Theorem Provers . . . . . . 233 Steffen Helke, Florian Kammüller Refinement Calculus for Logic Programming in Isabelle/HOL . . . . . . . . . . . . 249 David Hemer, Ian Hayes, Paul Strooper Predicate Subtyping with Predicate Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265 Joe Hurd A Structural Embedding of Ocsid in PVS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281 Pertti Kellomäki A Certified Polynomial-Based Decision Procedure for Propositional Logic . 297 Inmaculada Medina-Bulo, Francisco Palomo-Lozano, José A. Alonso-Jiménez Finite Set Theory in ACL2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313 J Strother Moore The HOL/NuPRL Proof Translator (A Practical Approach to Formal Interoperability) . . . . . . . . . . . . . . . . . . . . . 329 Pavel Naumov, Mark-Oliver Stehr, José Meseguer Formalizing Convex Hull Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346 David Pichardie, Yves Bertot Experiments with Finite Tree Automata in Coq . . . . . . . . . . . . . . . . . . . . . . . . 362 Xavier Rival, Jean Goubault-Larrecq Mizar Light for HOL Light . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378 Freek Wiedijk Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395 JavaCard Program Verification	acl2;algorithm;convex hull algorithms;coq (software);decision problem;first-order logic;formal verification;gnu hurd;hol (proof assistant);hol light;hayes microcomputer products;interactive theorem proving (conference);interoperability;isabelle;j strother moore;jean;java card;logic programming;mizar;nuprl;polynomial;proof assistant;propositional calculus;refinement (computing);refinement calculus;set theory;tree automaton;π-calculus	Andrew Adams;Martin Dunstan;Hanne Gottliebsen;Tom Kelsey;Ursula Martin;Sam Owre	2001		10.1007/3-540-44755-5	discrete mathematics;automated theorem proving;pure mathematics;full employment theorem;mathematics	Logic	-20.248414272774124	18.751210926250113	13982
43ef8552b8ef0fbc49e68dc56636cc7701b56485	domain-specific languages: an annotated bibliography	domainn specific language;terminologie;software;lenguaje programacion;compilacion;terminologia;programming language;logiciel;implementation;software systems;semantics;bibliografia;bibliography;conception;program library;semantica;semantique;universiteitsbibliotheek;specification language;ejecucion;bibliographie;diseno;domain specific language;bibliotheque programme;langage programmation;compilation;logicial;design;terminology;lenguaje especificacion;langage specification;biblioteca programa;design methodology	We survey the literature available on the topic of domain-specific languages as used for the construction and maintenance of software systems. We list a selection of 75 key publications in the area, and provide a summary for each of the papers. Moreover, we discuss terminology, risks and benefits, example domain-specific languages, design methodologies, and implementation techniques.	domain-specific language;software maintenance;software system	Arie van Deursen;Paul Klint;Joost Visser	2000	SIGPLAN Notices	10.1145/352029.352035	natural language processing;design;specification language;computer science;semantics;bibliography;programming language;implementation	PL	-25.681167368079418	23.931750068879396	14059
7894be50808308fe4b58ffbaf55dc90e5b552677	factorisation of petri net solvable transition systems		In recent papers, general conditions were developed to characterise when and how a labelled transition system may be factorised into non-trivial factors. These conditions combine a local property (strong diamonds) and a global one (separation), the latter being of course more delicate to check. Since one of the aims of such a factorisation was to speed up the synthesis of Petri nets from such labelled transition systems, the problem arises to analyse if those conditions (and in particular the global one) could be simplified, or even dropped, in the special case of Petri net solvable behaviours, i.e., when Petri net synthesis is possible. This will be the subject of the present paper.	decision problem;monoid factorisation;petri net	Raymond R. Devillers;Uli Schlachter	2018		10.1007/978-3-319-91268-4_5	discrete mathematics;transition system;computer science;petri net;speedup;special case;local property;factorization	Logic	-9.939558055902205	23.34095192553697	14062
6bf02dcb2266816abfc094ff6888871dba00caea	embedding the hypersequent calculus in the display calculus		The difficulty in finding analytic Gentzen sequent calculi for non-classical logics has lead to the development of many new proof frameworks (proof systems) that have been used to give analytic calculi for these logics. The multitude and diversity of such frameworks has made it increasingly important to identify their interrelationships and relative expressive power. Hypersequent and Display calculi are two widely-used proof frameworks employed to present analytic calculi for large classes of logics. In this article, we show how any hypersequent calculus can be used to construct a display calculus for the same logic. The display calculus we obtain preserves proof-theoretic properties of the original calculus including cutelimination and the subformula property. Since the construction applies to any hypersequent calculus, this result shows that in terms of presenting logics the display calculus formalism subsumes the hypersequent calculus formalism.	expressive power (computer science);formal grammar;semantics (computer science);sequent calculus;theory;well-formed formula	Revantha Ramanayake	2015	J. Log. Comput.	10.1093/logcom/exu061	lambda cube;natural deduction	Logic	-12.868100195707305	16.41729649770754	14070
45bb9a638f566fb9b4481cd984636817b39f7ba7	towards succinctness in mining scenario-based specifications	succinct mined specification scenario based specification mining system execution trace machine readable representation human readable representation live sequence chart classical sequence diagram equivalence relation isomorphic embedding delta discriminative measure statistical metrics;scenario based specifications live sequence charts specification mining succinctness;specification mining;formal specification;measurement;delta discriminative measure;bepress selected works;statistical analysis data mining equivalence classes formal specification;semantics;machine readable representation;scenario based specifications;live sequence charts scenario based specifications specification mining succinctness;live sequence charts;succinctness;data mining;statistical metrics;scenario based specification mining;automata;equivalence relation;redundancy;statistical analysis;concrete data mining redundancy semantics measurement automata context;isomorphic embedding;succinct mined specification;classical sequence diagram;equivalence classes;human readable representation;system execution trace;context;concrete;live sequence chart	Specification mining methods are used to extract candidate specifications from system execution traces. A major challenge for specification mining is succinctness. That is, in addition to the soundness, completeness, and scalable performance of the specification mining method, one is interested in producing a succinct result, which conveys a lot of information about the system under investigation but uses a short, machine and human-readable representation. In this paper we address the succinctness challenge in the context of scenario-based specification mining, whose target formalism is live sequence charts (LSC), an expressive extension of classical sequence diagrams. We do this by adapting three classical notions: a definition of an equivalence relation over LSCs, a definition of a redundancy and inclusion relation based on isomorphic embeddings among LSCs, and a delta-discriminative measure based on an information gain metric on a sorted set of LSCs. These are applied on top of the commonly used statistical metrics of support and confidence. A number of case studies show the utility of our approach towards succinct mined specifications.	chart;human-readable medium;kullback–leibler divergence;mined;scalability;semantics (computer science);sequence diagram;statistical model;tracing (software);turing completeness	David Lo;Shahar Maoz	2011	2011 16th IEEE International Conference on Engineering of Complex Computer Systems	10.1109/ICECCS.2011.30	equivalence class;concrete;computer science;theoretical computer science;data mining;formal specification;semantics;automaton;redundancy;equivalence relation;programming language;algorithm;measurement	SE	-13.838968581395442	25.876620780103355	14110
d6e28731cc8919b3e0b3509c5a83fc5debc2fcbd	implementation methodology for using concurrent and collaborative approaches for theorem provers, with case studies of sat and lcf style provers	thesis or dissertation;theorem provers;abstractions;parallel theorem provers;lcf;sat;functional;parallelisation	Theorem provers are faced with the challenges of size and complexity, fueled by the increasing range of applications. The use of concurrent/ distributed programming paradigms to engineer better theorem provers merits serious investigation, as it provides: more processing power and opportunities for implementing novel approaches to address theorem proving tasks hitherto infeasible in a sequential setting. Investigation of these opportunities for two diverse theorem prover settings with an emphasis on desirable implementation criteria is the core focus of this thesis. Concurrent programming is notoriously error prone, hard to debug and evaluate. Thus, implementation approaches which promote easy prototyping, portability, incremental development and effective isolation of design and implementation can greatly aid the enterprise of experimentation with the application of concurrent techniques to address specific theorem proving tasks. In this thesis, we have explored one such approach by using Alice ML, a functional programming language with support for concurrency and distribution, to implement the prototypes and have used programming abstractions to encapsulate the implementations of the concurrent techniques used. The utility of this approach is illustrated via proof-of-concept prototypes of concurrent systems for two diverse case studies of theorem proving: the propositional satisfiability problem (SAT) and LCF style (first-order) theorem proving, addressing some previously unexplored parallelisation opportunities for each, as follows:. SAT: We have developed a novel hybrid approach for SAT and implemented a prototype for the same: DPLL-Stalmarck. It uses two complementary algorithms for SAT, DPLL and Stalmarck’s. The two solvers run asynchronously and dynamic information exchange is used for co-operative solving. Interaction of the solvers has been encapsulated as a programming abstraction. Compared to the standalone DPLL solver, DPLL-Stalmarck shows significant performance gains for two of the three problem classes considered and comparable behaviour otherwise. As an exploratory research effort, we have developed a novel algorithm, Concurrent Stalmarck, by applying concurrent techniques to the Stalmarck algorithm. A proof-of-concept prototype for the same has been implemented. Implementation of the saturation technique of the Stalmarck algorithm in a parallel setting, as implemented in Concurrent Stalmarck, has been encapsulated as a programming abstraction. LCF: Provision of programmable concurrent primitives enables customisation of concurrent techniques to specific theorem proving scenarios. In this case study, we have developed a multilayered approach to support programmable, sound extensions for an LCF prover: use programming abstractions to implement the concurrent techniques; use these to develop novel tacticals (control structures to apply tactics), incorporating concurrent techniques; and use these to develop novel proof search procedures. This approach has been implemented in a prototypical LCF style first-order prover, using Alice ML. New tacticals developed are: fastest-first; distributed composition; crossTalk: a novel tactic which uses dynamic, collaborative information exchange to handle unification across multiple sub-goals, with shared meta-variables; a new tactic, performing simultaneous proof-refutation attempts on propositional (sub)goals, by invoking an external SAT solver (SAT case study), as a counter-example finder. Examples of concrete theorem proving scenarios are provided, demonstrating the utility of these extensions. Synthesis of a variety of automatic proof search procedures has been demonstrated, illustrating the scope of programmability and customisation, enabled by our multilayered approach.		G Sriipriya	2013			discrete mathematics;theoretical computer science;mathematics;algorithm	EDA	-18.560126287548385	26.448010544097126	14173
e86a4e5008807a8c0c9ba517a7d18e881b3c5f31	design and implementation of efficient storage schemas and low-level storage manager for gml documents	databases;document handling;pediatrics;gml document;geographic information;information retrieval;storage manager;component;probability density function;storage management;lbs component storage schema storage manager gml document;xml document handling geographic information systems information retrieval storage management;data mining;ogc;indexes;design and implementation;storage schemas;geographic information systems;spatial databases;low level storage manager;xml markup languages geographic information systems spatial databases information retrieval mobile communication application software telematics engineering management design engineering;lbs;xml;xml document;spatial networks;markup language;storage schema;geographic markup language;open gis consortium;gml documents;geographic markup language storage schemas low level storage manager gml documents geographic information ogc open gis consortium	GML is a markup language presented as exchange standard for geographic information by the OGC (Open GIS Consortium). In spatial network databases, researches for supporting GML (Geographic Markup Language) can be divided into parsing, storing and retrieval of GML documents. Among them, the study on the storage of GML documents is essential for their efficient retrieval. However there is little research on the storing of GML documents whereas there have been a lot of researches on the storing of XML documents. Because the storage schema designed for XML documents are not appropriate for geographic information, we, in this paper, propose three storage schemas for efficiently storing GML documents including geographic information in order to solve the problem that the XML storage schema store duplicate data and need to search many tables for obtaining elements. In addition, we design and implement a storage manager which can store GML documents using proposed GML storage schema.	consortium;data redundancy;geographic information system;geography markup language;high- and low-level;in-memory database;location-based service;network model;oracle database;parsing;relational database management system;spatial network;telematics;xml	Yong Ki Kim;You-Jin Jang;Jae-Woo Chang	2009	2009 Third International Conference on Research Challenges in Information Science	10.1109/RCIS.2009.5089265	xml;geography markup language;computer science;data mining;database;geographic information system;information retrieval;web feature service	DB	-32.89178201787919	7.986962315007889	14192
eb38ebf850ed09b656252040c7052c7a6377997b	on the revision of preferences and rational inference processes	conditional revision;rational closure;rational agent;conditional bases;rational inferences;rationality;non monotonic logics;preferences;knowledge base	Orderings and inference relations can be successfully used to model the behavior of a rational agent. This behavior is indeed represented either by a set of ordered pairs that reflect the agent’s preferences, or by a rational inference relation that describes the agent’s internal logics. In the finite case where we work, both structures admit a simple representation by means of logical chains. The problem of revising such inference processes arises when it appears necessary to modify the original model in order to take into account new facts about the agent’s behavior. How is it then possible to perform the desired modification ? We study here the possibilities offered by the technique of ’chain revision’ which appears to be the easiest way to treat this kind of problem: the revision is performed through a simple modification of the logical chain attached to the agent’s behavior, and the revision problem boils down to adding, retracting or modifying some of the links of the original chain. This perspective permits an effective treatment of the problems of both simple and multiple revision. The technique developed can also be used in some limiting cases, when the agent’s inference process is only partially known, encoded by an incomplete set of preferences or a conditional knowledge base.	conditional entropy;encode;knowledge base;logical connective;ordered pair;rational agent	Michael Freund	2004	Artif. Intell.	10.1016/S0004-3702(03)00116-4	rational agent;knowledge base;discrete mathematics;rationality;computer science;artificial intelligence;mathematics;algorithm	AI	-16.396770762372554	6.75591040475641	14248
7e17fca71ea664e3566e8a89c69ef88f067758eb	about the implementability and the power of equationally defined data abstractions	data abstraction	Abstract   The question of implementability and expressive power of equational axiom definitions of data abstractions is faced in the paper from the point of view of computability theory.  A definition of implementable algebra is given, which looks reasonable and very general. With respect to the given definition it is proved that, if the least congruence semantics is accepted, an equationally defined data algebra is implementable if and only if the least congruence on terms induced by the equational definition is decidable. Moreover, the paper shows that there are: (a) equationally defined data algebras that cannot be implemented; (b) implementable algebras that cannot be expressed in any way by equational axioms.		G. Callegarin;Giuliano Pacini	1981	Theor. Comput. Sci.	10.1016/0304-3975(81)90048-7	combinatorics;discrete mathematics;computer science;mathematics;algorithm	ECom	-8.959667253269068	15.46780080962436	14250
572851ab2eb12ba8d1fedf7f499bc9826ea51a04	considerations on information storage and retrieval systems for recycling objects	internal data structuring information storage systems information retrieval systems recycling management systems information storage architecture recycling objects storage organization;recycling management systems;management system;storage management;storage management information retrieval systems;internal data structuring;information storage architecture;storage organization;information retrieval systems;data structure;information retrieval recycling waste management information management memory engineering management manufacturing database systems cultural differences raw materials;information storage and retrieval;information storage systems;recycling objects	This paper considers some requirements needed for recycling management systems, especially on their information storage architecture and retrieval capability of recycling objects. Their realization scheme adapting these requirements will also be discussed. Improving lower level storage organization or internal data structuring strategy for storing recycling objects can provide handling efficiency or retrieval efficiency rather independently of the kind of recycling object.	computer data storage;data structure;information retrieval;requirement	Takayuki Tsuchida;Tatsuo Tsuji;Ken Higuchi	2007	2007 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2007.4413856	data structure;computer science;knowledge management;management system;database;information repository;information retrieval	DB	-33.18980911294839	10.355266278983047	14266
b41f98a7b00c77e571726517c0a78fcd654a7c09	rewrite systems with abstraction and beta-rule: types, approximants and normalization	satisfiability;first order;rewrite systems;intersection types	In this paper we define and study intersection type assignment systems for first-order rewriting extended with application, -abstraction, and -reduction ( ). One of the main results presented is that, using a suitable notion of approximation of terms, any typeable term of a that satisfies a general scheme for recursive definitions has an approximant of the same type. From this result we deduce, for different classes of typeable terms, a head-normalization and a normalization theorem.		Steffen van Bakel;Franco Barbanera;Maribel Fernández	1996		10.1007/3-540-61055-3_50	computer science;normalization property;first-order logic;algorithm;satisfiability	Logic	-10.944980285469123	17.22052754861089	14278
8eba038692f9b0a4766ef26aee9d851bf9ca05da	compositional proof systems for model checking infinite state processes	operational semantics;model checking;finite state processes	We present the rst compositional proof system for checking processes against formulas in the modal-calculus which is capable of handling general innnite-state processes. The proof system is obtained in a systematic way from the operational semantics of the underlying process algebra. A non-trivial proof example is given, and the proof system is shown to be sound in general, and complete for nite-state processes.	context-free language;gs/os;modal logic;model checking;np-completeness;operational semantics;process calculus;proof calculus;recurrence relation;recursion;roberto busa;simpson's rule;weak value	Mads Dam	1995		10.1007/3-540-60218-6_2	model checking;combinatorics;discrete mathematics;computer science;mathematics;programming language;operational semantics;abstraction model checking;algorithm	Logic	-10.445219135246075	23.387434656493493	14293
a2960bb369c0414c59be420907e8c547bfbb30fb	completeness theorems for reactive modal logics	temporal logic;03b45;03b44;modal logic	1 Overview This paper gives completeness theorems for some basic reactive Kripke models and semantics. This section will 1. Introduce reactivity 2. Discuss and compare the kind of Kripke semantics we get with reactivity 3. Explain the challenges in obtaining completeness theorems 1.1 Fibring and reactivity Our starting point is an ordinary Kripke model for modal logic. This has the form m = (S, R, a) where S is a nonempty set of worlds, a ∈ S is the initial (actual) world and R ⊆ S × S is the accessibility relation. The model also has an assignment h, giving for each atomic q a subset h(q) ⊆ S. We shall focus on R.	accessibility relation;kripke semantics;modal logic	Dov M. Gabbay	2012	Annals of Mathematics and Artificial Intelligence	10.1007/s10472-012-9315-9	modal logic;dynamic logic;t-norm fuzzy logics;normal modal logic;modal μ-calculus;discrete mathematics;linear temporal logic;description logic;higher-order logic;relevance logic;temporal logic;interval temporal logic;computer science;intermediate logic;artificial intelligence;axiom s5;mathematics;kripke semantics;accessibility relation;substructural logic;multimodal logic;algorithm	AI	-12.515259017766326	12.938858059128588	14322
9c4889829a4c0c76fe204f4d2402820643a63453	reducing probabilistic timed petri nets for asynchronous architectural analysis	performance measure;energy delay optimization;transistor sizing;time petri net;architecture analysis;petri net;large classes	This paper introduces structural reductions of probabilistic timed Petri nets that preserve a large class of performance measurements. In particular, the paper proposes a class of reductions that preserve efficiently computable bounds of statistics of time-separation of events (TSEs). It identifies two specific reductions within this class. It demonstrates the utility of these reductions by reducing a detailed Petri net describing the four-phase protocol of a well-known asynchronous pipeline template into a simpler two-phase architectural-level Petri net model. The benefit of this reduced model is that the run-time of subsequent TSE analysis can be greatly improved.	computable function;petri net;two-phase commit protocol	Shinwon Kim;Sunan Tugsinavisut;Peter A. Beerel	2002		10.1145/589411.589441	real-time computing;stochastic petri net;computer science;distributed computing;process architecture;petri net;algorithm	EDA	-10.918674645997188	29.185312632724	14352
4bcf419236c08a605b2a2f7a812851ea4758802c	stalnaker's thesis in context	indicatives;stalnaker s thesis;contextualism;conditionals;conditional probability;adams thesis	In this paper I present a precise version of Stalnaker’s thesis and show that it is both consistent and predicts our intuitive judgments about the probabilities of conditionals. The thesis states that someone whose total evidence is E should have the same credence in the proposition expressed by ‘if A then B’ in a context where E is salient as they have conditional credence in the proposition B expresses given the proposition A expresses in that context. The thesis is formalised rigorously and two models are provided that demonstrate that the new thesis is indeed tenable within a standard possible world semantics based on selection functions. Unlike the Stalnaker-Lewis semantics the selection functions cannot be understood in terms of similarity. A probabilistic account of selection is defended in	possible world	Andrew Bacon	2015	Rew. Symb. Logic	10.1017/S1755020314000318	conditional probability;epistemology;artificial intelligence;mathematics;algorithm	NLP	-14.73021350177621	5.5029623379956565	14365
366ee70438d66a1c0ddd5d48bd8186080069da65	combination of disjoint theories: beyond decidability	disjoint finitely axiomatized theory;assumed procedure;stably infinite;underlying theory;decision procedure;refutationally complete procedure;disjoint theory;nelson-oppen framework;disjoint decidable stably infinite;satisfiability modulo theory;arbitrary first-order theory	Combination of theories underlies the design of satisfiability modulo theories (SMT) solvers. The Nelson-Oppen framework can be used to build a decision procedure for the combination of two disjoint decidable stably infinite theories. We here study combinations involving an arbitrary first-order theory. Decidability is lost, but refutational completeness is preserved. We consider two cases and provide complete (semi-)algorithms for them. First, we show that it is possible under minor technical conditions to combine a decidable (not necessarily stably infinite) theory and a disjoint finitely axiomatized theory, obtaining a refutationally complete procedure. Second, we provide a refutationally complete procedure for the union of two disjoint finitely axiomatized theories, that uses the assumed procedures for the underlying theories without modifying them.	algorithm;black box;cardinality (data modeling);decision problem;first-order logic;first-order predicate;heuristic (computer science);modulo operation;predicate (mathematical logic);satisfiability modulo theories;solver;terminate (software);theory;turing completeness;unary operation	Pascal Fontaine;Stephan Merz;Christoph Weidenbach	2012		10.1007/978-3-642-31365-3_21	combinatorics;discrete mathematics;mathematics;algorithm	Logic	-12.71703772199544	20.482573149210953	14374
a18483dcc5865a5549a8763e87684ef0d9829c1f	elaborating inductive definitions		We present an elaboration of inductive definitions down to a universe of datatypes. The universe of datatypes is an internal presentation of strictly positive types within type theory. By elaborating an inductive definition – a syntactic artefact – to its code – its semantics – we obtain an internalised account of inductives inside the type theory itself: we claim that reasoning about inductive definitions could be carried in the type theory, not in the meta-theory as it is usually the case. Besides, we give a formal specification of that elaboration process. It is therefore amenable to formal reasoning too. We prove the soundness of our translation and hint at its completeness with respect to Coq’s Inductive definitions. The practical benefits of this approach are numerous. For the type theorist, this is a small step toward bootstrapping, i.e. implementing the inductive fragment in the type theory itself. For the programmer, this means better support for generic programming: we shall present a lightweight deriving mechanism, entirely definable by the programmer and therefore not requiring any extension to the type theory. In a dependent type theory, inductive types come in various shapes and forms. Unsurprisingly, we can define data-types à la ML, following the sum-of-product recipe: we offer a choice of constructors and, for each constructor, comes a product of arguments. An example is the vintage List datatype: data List [A :Set] :Set where ListA 3 nil | cons (a :A)(as :ListA) For the working semanticist, this brings fond memory of a golden era: this syntax has a trivial categorical interpretation in term of signature functor, here LAX = 1 + A × X. Without a second thought, we can brush away the syntax, mapping the syntactic representations of sum and product to their categorical counterpart. Handling parameters comes at a minor complexity cost: we merely parameterise the functor itself, for instance with A here. We ought to make sure that our language of data-type is correct, let alone semantically meaningful. Indeed, if we were to accept the following definition data Bad [A :Set] :Set where Bad A 3 ex (f :BadA→A) we would make many formal developments a lot easier to prove! To ban these bogus definitions, theorem provers such as Agda (Norell, 2007) or Coq (The Coq Development Team) rely on a positivity checker to ensure that all recursive arguments are in a strictly positive position. The positivity checker is therefore part of the trusted computing base of the theorem prover. Besides, by working on the syntactic representation of datatypes, it is a non negligible piece of software that is a common source of frustration: it either stubbornly prevents perfectly valid definitions – as it sometimes is the case in Coq – or happily accepts obnoxious definitions – as Agda users discover every so often. While reasoning about datatypes is at a functor away, we seem stuck with these clumsy syntactic presentations: quoting Harper and Stone (2000), “the treatment of datatypes is technically complex,	agda;automated reasoning;automated theorem proving;calculus of constructions;coherence (physics);coq (software);dependent type;disjunctive normal form;expectation propagation;formal specification;generic programming;high- and low-level;inductive type;linear algebra;programmer;recursion;recursive definition;spell checker;trusted computing base;type system;visual artifact	Pierre-Évariste Dagand;Conor McBride	2012	CoRR		computer science;programming language;algorithm	PL	-15.598719536002951	19.293420116089838	14382
270aecffdf0ac9cb386d4e9675bc4b1543d17c53	finally, safely-extensible and efficient language-integrated query	sql;tagless final;language integrated query;edsl;linq	Language-integrated query is an embedding of database queries into a host language to code queries at a higher level than the all-to-common concatenation of strings of SQL fragments. The eventually produced SQL is ensured to be well-formed and well-typed, and hence free from the embarrassing (security) problems. Language-integrated query takes advantage of the host language's functional and modular abstractions to compose and reuse queries and build query libraries. Furthermore, language-integrated query systems like T-LINQ generate efficient SQL, by applying a number of program transformations to the embedded query. Alas, the set of transformation rules is not designed to be extensible. We demonstrate a new technique of integrating database queries into a typed functional programming language, so to write well-typed, composable queries and execute them efficiently on any SQL back-end as well as on an in-memory noSQL store. A distinct feature of our framework is that both the query language as well as the transformation rules needed to generate efficient SQL are safely user-extensible, to account for many variations in the SQL back-ends, as well for domain-specific knowledge. The transformation rules are guaranteed to be type-preserving and hygienic by their very construction. They can be built from separately developed and reusable parts and arbitrarily composed into optimization pipelines. With this technique we have embedded into OCaml a relational query language that supports a very large subset of SQL including grouping and aggregation. Its types cover the complete set of intricate SQL behaviors.	abstract syntax;concatenation;database schema;digital subscriber line;embedded system;entity framework;error detection and correction;extensibility;functional programming;in-memory database;language integrated query;library (computing);mathematical optimization;nosql;ocaml;pipeline (computing);program optimization;program transformation;programmer;programming language;query language;relational database;sql;semantics (computer science);type system;well-formed document;well-formed formula	Kenichi Suzuki;Oleg Kiselyov;Yukiyoshi Kameyama	2016		10.1145/2847538.2847542	data transformation services;sargable;data definition language;query optimization;sql;.ql;sql injection;stored procedure;data manipulation language;computer science;query by example;user-defined function;data mining;autocommit;database;sql/psm;rdf query language;language integrated query;programming language;view;null;query language;object query language;spatial query	PL	-23.652071580933768	25.900933459324033	14385
8bb3b2639406fb1fdfb81772b27b95a9acefc122	a set-theoretical approach for abox reasoning services (extended version)		In this paper we consider the most common ABox reasoning services for the description logic DL〈4LQS〉(D) (DL D , for short) and prove their decidability via a reduction to the satisfiability problem for the set-theoretic fragment 4LQS. DL D is a very expressive description logic admitting various concept and role constructs, datatypes, and it allows one to represent rule based languages such as SWRL. Decidability results are achieved by defining a generalized version of the conjunctive query answering problem, called HOCQA (Higher Order Conjunctive Query Answering), that can be instantiated to the most widespread ABox reasoning tasks. Then, a KE-tableau based procedure is defined to calculate the answer set from a DL D knowledge base and from a higher order DL D conjunctive query. The system is an extension of a KE-tableau based decision procedure for the CQA problem introduced in a previous work and allows one to reason on several well known ABox reasoning tasks.	ambit;abox;artificial intelligence;book;boolean satisfiability problem;computational logic;conference on automated deduction;conjunctive query;consortium;decision problem;decision tree;description logic;expressive power (computer science);ext js javascript framework;first-order logic;first-order predicate;int (x86 instruction);iswc;international joint conference on automated reasoning;jack lutz;knowledge base;knowledge engineering;lecture notes in computer science;literal (mathematical logic);method of analytic tableaux;natural deduction;network address translation;ontology (information science);performance;propositional calculus;query language;semantic web rule language;set theory;springer (tank);stable model semantics;star filler;stratified sampling;type system;ver (command);web ontology language;world wide web	Domenico Cantone;Marianna Nicolosi Asmundo;Daniele Francesco Santamaria	2017	CoRR		abox;discrete mathematics;data type;description logic;conjunctive query;boolean satisfiability problem;decidability;mathematics	AI	-18.880076900838027	11.351440942386205	14392
9469576b4af18cce7909d1cac31254273f19c458	automatic generation of quality specifications	bad quality;binary-tagged computation;rm ltl;quality operator;real value;quality specification;satisfaction value;representative computation;quality constant;different satisfaction possibility;automatic generation;general case	The logic LTL extends LTL by quality operators. The satisfaction value of an LTL formula in a computation refines the 0/1 value of LTL formulas to a real value in [0, 1]. The higher the value is, the better is the quality of the computation. The quality operator Oλ, for a quality constant λ ∈ [0, 1], enables the designer to prioritize different satisfaction possibilities. Formally, the satisfaction value of a subformula Oλφ is λ times the satisfaction value of φ. For example, the LTL formula G(req → (Xgrant ∨ O 1 2 F grant)) has value 1 in computations in which every request is immediately followed by a grant, value 1 2 if grants to some requests involve a delay, and value 0 if some request is not followed by a grant. The design of an LTL formula typically starts with an LTL formula on top of which the designer adds the parameterized O operators. In the Boolean setting, the problem of automatic generation of specifications from binary-tagged computations is of great importance and is a very challenging one. Here we consider the quantitative counterpart: an LTL query is an LTL formula in which some of the quality constants are replaced by variables. Given an LTL query and a set of computations tagged by satisfaction values, the goal is to find an assignment to the variables in the query so that the obtained LTL formula has the given satisfaction values, or, if this is impossible, best approximates them. The motivation to solving LTL queries is that in practice it is easier for a designer to provide desired satisfaction values in representative computations than to come up with quality constants that capture his intuition of good and bad quality. We study the problem of solving LTL queries and show that while the problem is NP-hard, interesting fragments can be solved in polynomial time. One such fragment is the case of a single tagged computation, which we use for introducing a heuristic for the general case. The polynomial solution is based on an analysis of the search space, showing that reasoning about the infinitely many possible assignments can proceed by reasoning about their partition into finitely many classes. Our experimental results show the effectiveness and favorable outcome of the heuristic.	computation;euler–maclaurin formula;heuristic;linear temporal logic;np-hardness;polynomial;time complexity;well-formed formula	Shaull Almagor;Guy Avni;Orna Kupferman	2013		10.1007/978-3-642-39799-8_32	artificial intelligence;theoretical computer science;algorithm	Logic	-14.604387625934352	24.99474805810081	14458
1ebe5c7c78cdfb5a4a62c4f70b462e69367fe38e	supporting domain-specific state space reductions through local partial-order reduction	state space reduction;protocols;message passing system;software systems;safety critical software concurrency control formal verification java message passing protocols;formal verification;model checking;safety critical software;concurrency control;message passing protocol domain specific state space reduction local partial order reduction complex concurrent software systems java pathfinder model checker java based lpor library;partial order reduction;message passing;optimization system recovery computational modeling algorithm design and analysis heuristic algorithms complexity theory java;state space explosion;domain specificity;java	Model checkers offer to automatically prove safety and liveness properties of complex concurrent software systems, but they are limited by state space explosion. Partial-Order Reduction (POR) is an effective technique to mitigate this burden. However, applying existing notions of POR requires to verify conditions based on execution paths of unbounded length, a difficult task in general. To enable a more intuitive and still flexible application of POR, we propose local POR (LPOR). LPOR is based on the existing notion of statically computed stubborn sets, but its locality allows to verify conditions in single states rather than over long paths. As a case study, we apply LPOR to message-passing systems. We implement it within the Java Pathfinder model checker using our general Java-based LPOR library. Our experiments show significant reductions achieved by LPOR for model checking representative message-passing protocols and, maybe surprisingly, that LPOR can outperform dynamic POR.	algorithm;experiment;java pathfinder;liveness;locality of reference;message passing;model checking;partial order reduction;spin;software system;state space	Péter Bokor;Johannes Kinder;Marco Serafini;Neeraj Suri	2011	2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011)	10.1109/ASE.2011.6100044	model checking;partial order reduction;communications protocol;parallel computing;message passing;real-time computing;formal verification;computer science;concurrency control;distributed computing;programming language;java;software system	SE	-18.35874317897122	29.437642424474486	14500
531149295ddc930eb65a7a4cda8494994b93a3c0	extending siena to support more expressive and flexible subscriptions	content based networks knowledge based networks;semantics;conference paper;publish subscribe;ontologies;bags;content based networks;knowledge based networks;knowledge base	This paper defines and discusses the implementation of two novel extensions to the Siena Content-based Network (CBN) to extend it to become a Knowledge-based Network (KBN) thereby increasing the expressiveness and flexibility of its publications and subscription. One extension provides ontological concepts as an additional message attribute type, onto which subsumption relationships, equivalence, type queries and arbitrary ontological subscription filters can be applied. The second extension provides for a bag type to be used that allows bag equivalence, sub-bag and super-bag relationships to be used in subscription filters, possibly composed with any of the Siena subscription operators or the ontological operators previously mentioned. The performance of this KBN implementation has also been explored. However, to maintain scalability and performance it is important that these extensions do not break Siena's subscription aggregation algorithm. We also introduce the necessary covering relationships for the new types and operators and examine the subscription matching overhead resulting from these new types and operators.	algorithm;overhead (computing);scalability;subsumption architecture;turing completeness	John Keeney;Dominik Roblek;Dominic Jones;David Lewis;Declan O'Sullivan	2008		10.1145/1385989.1385995	knowledge base;computer science;ontology;artificial intelligence;data mining;database;distributed computing;semantics;publish–subscribe pattern	PL	-22.98882532147143	8.942247280934602	14511
64b013e1f3e014763949d796d04a540ba8d2aa1a	argotrics - automated triangle construction solver	wernick s list;search procedure;triangle construction problems;automated deduction in geometry	In this paper a method for automatically solving a class of straightedge-and-compass construction problems is proposed. These are the problems where the goal is to construct a triangle given three located points. The method is based on identifying and systematizing geometric knowledge, a specific, restricted search and handling redundant or locus dependent instances. The proposed method is implemented and the current implementation can solve a large number of triangle construction problems. To our knowledge this is the first systematic automated construction solver focused on solving problems from the corpus given. This is also the first approach that consider proving correctness of generated constructions (by using external automated theorem provers).	automated theorem proving;correctness (computer science);locus;redundancy (engineering);solver	Vesna Marinkovic	2017	J. Exp. Theor. Artif. Intell.	10.1080/0952813X.2015.1132271	algorithm	AI	-18.70132404221382	17.26498766647334	14550
21ae3f3a9d3c5d2d8a2eddcbbe647e2ebc27351a	petri net modelling for the analysis of the ordering of actions in computer games	petri net		pc game;petri net	Stéphane Natkin;Liliana Vega	2003			petri net;theoretical computer science;stochastic petri net;distributed computing;computer science	Vision	-22.934237494590132	17.475866871730364	14566
98579ccb6842c8fc6d02bde02fdc83db0c391292	equivalences between logics and their representing type theories	type theory	We propose a new framework for representing logics, called LF + and based on the Ed-inburgh Logical Framework. The new framework allows us to give, apparently for the rst time, general deenitions which capture how well a logic has been represented. These deen-itions are possible since we are able to distinguish in a generic way that part of the LF + entailment which corresponds to the underlying logic. This distinction does not seem to be possible with other frameworks. Using our deenitions, we show that, for example, natural deduction rst-order logic can be well-represented in LF + , whereas linear and relevant logics cannot. We also show that our syntactic deenitions of representation have a simple formulation as indexed isomorphisms, which both connrms that our approach is a natural one and provides a link between type-theoretic and categorical approaches to frameworks.	indexed grammar;logical framework;natural deduction;type theory	Philippa Gardner	1995	Mathematical Structures in Computer Science	10.1017/S0960129500000785	discrete mathematics;topology;computer science;mathematics;programming language;type theory;algebra	Logic	-12.269170260736066	16.509085508388793	14572
f7d29e58a079aea8683f94d88ad2a13fb089a461	wafe - an x toolkit based frontend for application programs in various programming languages	programming language	Wafe provides a flexible and easy to use interf ace to theX Toolkit (Xt) and theAthenawidget set (Xaw) using the embeddable command language Tcl [1]. It allows access to Xt’ s functionality from all compiler and interpreter languages, pro vided that the y can communicate o ver stdout andstdin via unbuffered I/O. A typical Wafe application consists of a frontend process and an application program, which is e x cuted as a child process of the frontend. W afe provides a relatively high level interface to the X T oolkit and widget programming, where the user interf ace can be interacti vely developed without an y need to program in C. W afe can be used as a rapid prototyping tool and allo ws easier migration from e xisting ASCII based programs to X W indow applications.	automatic computing engine;child process;command language;compiler;high-level programming language;rapid prototyping;standard streams;tcl;ver (command);x athena widgets;x toolkit intrinsics	Gustaf Neumann;Stefan Nusser	1993			programming language;compiler;command language;ascii;rapid prototyping;computer science;interpreter;child process	PL	-30.97032907200171	27.502197566003986	14573
c756779c0802ae6bf347d24b61072704260b30dc	uncountable limits and the lambda calculus	domain model;laboratory for foundations of computer science;lambda calculus;domain equations;countable non determinism;function space;denotational semantic;denotational semantics;article;semantics of programming languages	In this paper we address the problem of solving recursive domain equations using uncountable limits of domains. These arise for instance, when dealing with the ω1-continuous function-space constructor and are used in the denotational semantics of programming languages which feature unbounded choice constructs. Surprisingly, the category of cpo’s and ω1-continuous embeddings is not ω0-cocomplete. Hence the standard technique for solving reflexive domain equations fails. We give two alternative methods. We discuss also the issue of completeness of the λβη-calculus w.r.t reflexive domain models. We show that among the reflexive domain models in the category of cpo’s and ω0-continuous functions there is one which has a minimal theory. We give a reflexive domain model in the category of cpo’s and ω1-continuous functions whose theory is precisely the λβη theory. So ω1-continuous λ-models are complete for the λβη-calculus. CR Classification: F.3.2, F.4.1, D.3.3	acm computing classification system;denotational semantics;domain model;function type;lambda calculus;navier–stokes equations;programming language;recursion;semantics (computer science)	Pietro Di Gianantonio;Furio Honsell;Gordon D. Plotkin	1995	Nord. J. Comput.		apply;discrete mathematics;normalisation by evaluation;function space;computer science;domain theory;domain model;lambda calculus;mathematics;programming language;denotational semantics of the actor model;denotational semantics;algorithm	Logic	-12.456582160888686	17.431614103417562	14614
1b4ff11edf747a002f853a8543f5f16519f5598c	abstraction mechanisms in the beta programming language	combinators;object oriented language;programming language;loops;distributed computing system;smalltalk;common property	The BETA programming language is developed as part of the BETAproject. The purpose of this project is to develop concepts,constructs and tools in the field of programming and programminglanguages. BETA has been developed from 1975 on and the variousstages of the language are documented in [BETA a]. The application area of BETA is programming of embedded as wellas distributed computing systems. For this reason a major goal hasbeen to develop constructs that may be efficiently implemented.Furthermore the BETA language is intended to have a few number ofbasic but general constructs. It is then necessary that theabstraction mechanisms are powerful in order to define morespecialized constructs. BETA is an object oriented language like SIMULA 67([SIMULA]) and SMALLTALK ([SMALLTALK]). By this is meant that aconstruct like the SIMULA class/subclass mechanism is fundamentalin BETA. In contrast to SMALLTALK, BETA is a language in the ALGOL60 ([ALGOL]) family. SIMULA 67 is a system description and a programming language.The DELTA language ([DELTA]) is a system description language only,allowing description of full concurrency, continuous change andcomponent interaction, developed from a SIMULA conceptual platform.BETA started from the system concepts of DELTA, but is aprogramming language, drawing upon a large number of contributionsto programming research in the 1970s. A basic idea in BETA is tobuild the language upon one, general abstraction mechanism --- thepattern ([BETA a 77]) --- covering both data, procedural andcontrol abstractions, substituting constructs like class,procedure, function and type. Correspondingly objects, procedure activation records andvariables are all regarded as special cases of the basic buildingblock of program executions: the entity. A pattern thusdescribes a category of entities with identical structure.An entity consists of a set of attributes and anaction-part. An attribute may be a data-item or apattern. The action-part is a sequence of imperatives that may beexecuted. A data-item may be an entity or a reference to an entity.A pattern may be used in a procedure like manner in the sense thatan entity (procedure activation record) described by the patternmay be generated and executed as a part of the action sequence ofanother entity. A pattern may be used to generate entities thatexecute their action-part in concurrency with other entities. Suchentities may also execute their actions interleaved in a coroutinelike manner. Entities may be organized hierarcically by means of ageneralization of the SIMULA subclass mechanism. This givespossibilities for grouping common properties of entities ofdifferent patterns. In SIMULA 67 a class may have virtual attributes(procedures, labels, and switches). This is a powerful parametermechanism that gives the possibility to delay the specification ofan attribute to a subclass specification. However, SIMULA 67 lacksthe possibility to have virtual class attributes. Furthermore it isnecessary to have a runtime check on the parameters of virtualprocedures, since it is not possible to specify the parameter listof a virtual procedure. The virtual patterns of BETA is ageneralization of the virtual concept in SIMULA 67. In this paper the sequential part of BETA will be presented. Themain purpose is to demonstrate the use of the pattern/subpatternmechanism with virtual patterns as a powerful abstractionmechanism. In addition, a further generalization of the virtualconcept based on syntactic categories will be described. Work has been initiated to design and implement an integratedprogramming system for BETA. The approach to separatecompilation of BETA modules is described in [BETA c]. This paper is organised as follows: Section 2 describesentities, patterns and imperatives. Section 3 describes thesubpattern mechanism. Virtual patterns are described in section 4.Section 5 describes the generalization of the virtual concept. Insection 6 the remaining elements of BETA not mentioned in theprevious sections are described. Finally the syntax of BETA isgiven in the appendix. Each section with a brief introduction ofthe relevant language elements whereafter a number of examples aregiven. Most of the examples are extended versions of Hoare'sSmallIntSet [Hoare 72]. There is a distinction between the base language (calledbasic BETA) and standard BETA. Standard BETA is basicBETA extended with a number of commonly used constructs. Theseadditional constructs may all be regarded as patterns in basicBETA, but will often be given a special syntax. This paper willmainly focus on basic BETA. Occasionally we shall use parts of astandard BETA, but this will be stated at the appropiate place.	algol 60;apl;beta;call stack;concurrency (computer science);distributed computing;embedded system;entity;hoare logic;network switch;programming language;simula;smalltalk	Bent Bruun Kristensen;Ole Lehrmann Madsen;Birger Møller-Pedersen;Kristen Nygaard	1983		10.1145/567067.567094	combinatory logic;computer science;programming language;object-oriented programming;algorithm	PL	-26.10893177103113	27.31510388848865	14639
207259df32812720ac7f1a769ba1d928035e96fa	using systemverilog assertions to relate non-cycle-accurate to cycle-accurate designs	systemverilog;non cycle accurate;assertion;temporal abstraction;cycle accurate;portable document format;ieee xplore;register transfer level;temporal abstraction systemverilog assertion cycle accurate non cycle accurate transaction level register transfer level;transaction level	The problem of the exact temporal relationship between a non-cycle-accurate and a cycle-accurate design is discussed. A non-cycle-accurate design is transformed into a set of equivalent SystemVerilog assertions. An interpretation schema is defined for the translation of these assertions into the time-dimension of the cycle-accurate design. The assertions are then co-simulated or verified. The conditions are identified under which the method is compositional.	systemverilog	Hans Eveking;Tobias Dornes;Martin Schweikert	2011	2011 IEEE International High Level Design Validation and Test Workshop	10.1109/HLDVT.2011.6114161	parallel computing;assertion;computer science;database;programming language;register-transfer level	EDA	-15.76890775077758	29.13909751807822	14721
14cdaa51e020fc79483940fcf3bc2512fbb7fc12	interactive inference of join queries		We investigate the problem of inferring join queries from user interactions. The user is presented with a set of candidate tuples and is asked to label them as positive or negative depending on whether or not she would like the tuples as part of the join result. The goal is to quickly infer an arbitrary n-ary join predicate across two relations by keeping the number of user interactions as minimal as possible. We assume no prior knowledge of the integrity constraints between the involved relations. This kind of scenario occurs in several application settings, such as data integration, reverse engineering of database queries, and constraint inference. In such scenarios, the database instances may be too big to be skimmed. We explore the search space by using a set of strategies that let us prune what we call “uninformative” tuples, and directly present to the user the informative ones i.e., those that allow to quickly find the goal query that the user has in mind. In this paper, we focus on the inference of joins with equality predicates and we show that for such joins deciding whether a tuple is uninformative can be done in polynomial time. Next, we propose several strategies for presenting tuples to the user in a given order that lets minimize the number of interactions. We show the efficiency and scalability of our approach through an experimental study on both benchmark and synthetic datasets. Finally, we prove that adding projection to our queries makes the problem intractable.	benchmark (computing);constraint inference;data integrity;database;dhrystone;experiment;information;interaction;join (sql);reverse engineering;scalability;time complexity	Angela Bonifati;Radu Ciucanu;Slawomir Staworko	2014		10.5441/002/edbt.2014.41	theoretical computer science;data mining;database;sort-merge join	DB	-24.891866240732654	4.7484558499779626	14781
da5ca560ee57d662b2b2916b79890c9bf67564fe	floating point verification in hol light: the exponential function	developpement logiciel;exponential function;ingenieria logiciel;logical programming;program verification;software engineering;verificacion programa;formal verification;programmation logique;desarrollo logicial;software development;genie logiciel;floating point;coma flotante;floating point arithmetic;verification programme;programacion logica;virgule flottante	Since they often embody compact but mathematically sophisticated algorithms, operations for computing the common transcendental functions in floating point arithmetic seem good targets for formal verification using a mechanical theorem prover. We discuss some of the general issues that arise in verifications of this class, and then present a machine-checked verification of an algorithm for computing the exponential function in IEEE-754 standard binary floating point arithmetic. We confirm (indeed strengthen) the main result of a previousl published error analysis, though we uncover a minor error in the hand proof and are forced to confront several subtle issues that might easily be overlooked informally.	hol (proof assistant);hol light	John Harrison	1997		10.1007/BFb0000475	computer science;floating point;artificial intelligence;theoretical computer science;operating system;mathematics;programming language;algorithm	Crypto	-17.488273138143366	23.03889235407141	14791
10b8cc6e6d162380ac92a9d334df1de7c57d2dfb	rewrite, rewrite, rewrite, rewrite, rewrite	abstract data type;term rewrite system;functional programming language	The theory of term rewriting systems has important applications in abstract data type specifications and functional programming languages. We begin here a study of properties of systems that are not necessarily terminating, but allow for infinite derivations that have a limit. In particular, we give conditions for the existence of a limit and for its uniqueness.	abstract data type;functional programming;newman's lemma;programming language;rewriting	Nachum Dershowitz;Stéphane Kaplan	1989		10.1145/75277.75299	computer science;normalization property;programming language;functional programming;abstract data type;algorithm;rippling	PL	-12.18311921190381	18.620863234245423	14831
a20ebda02f31e63f5fae263c3ed60a0945b20c1b	automated implementation of petri nets on plcs with oop	software;pn design petri nets plc oop manufacturing systems programmable logic controllers object oriented programming program modification;heating;reactive power iec standards arrays xml software heating object oriented modeling;programmable controllers manufacturing systems object oriented programming petri nets;arrays;iec standards;xml;object oriented modeling;reactive power	In this paper a method is proposed to implement controllers and supervisors for manufacturing systems designed by Petri Nets (PNs) on Programmable Logic Controllers (PLCs) using Object Oriented Programming (OOP). In the resulting program each instruction is directly related to the evaluation of a transition, to the update of the marking, or to the enabling of a transition. This preserves the structure of PNs and entails reusability since it allows the user to easily modify the program directly, starting from the modifications made to the PN design.	complex systems;item unique identification;petri net;programmable logic device	Francesco Basile;Pasquale Chiacchio;Jolanda Coppola;Diego Gerbasio	2014	Proceedings of the 2014 IEEE Emerging Technology and Factory Automation (ETFA)	10.1109/ETFA.2014.7005287	embedded system;real-time computing;xml;computer science;operating system;ac power;programming language	EDA	-33.5545202304188	29.506777815966664	14838
e1dbdaad33a2d31c7081f5b99bebed96a01b1296	innovative power operating center management exploiting big data techniques	outdoor environments;multi agent based models;intelligent environments;cloud based solutions;mobile technologies	The problem of accurately predicting the energy production from renewable sources has recently received an increasing attention from both the industrial and the research communities. It presents several challenges, such as facing with the rate data are provided by sensors, the heterogeneity of the data collected, power plants efficiency, as well as uncontrollable factors, such as weather conditions and user consumption profiles. In this paper we describe Vi-POC (Virtual Power Operating Center), a project conceived to assist energy producers and decision makers in the energy market. In this paper we present the Vi-POC project and how we face with challenges posed by the specific application. The solutions we propose have roots both in big data management and in stream data mining.	big data;data mining;sensor	Michelangelo Ceci;Nunzio Cassavia;Roberto Corizzo;Pietro Dicosta;Donato Malerba;Gaspare Maria;Elio Masciari;Camillo Pastura	2014		10.1145/2628194.2628231	simulation;computer science;mobile technology;data mining;database	ML	-32.58360933038198	19.547350294639497	14845
dd3c4c5300f130abccfcab02ad4fd76401a5dddd	representation of the event bush approach in terms of directed hypergraphs		The paper discusses the relation between a novel approach of knowledge engineering, the event bush, and the formalism of directed hypergraph. Despite the seemingly obvious similarity, the relation appears to be far from transparent. However, if formulated accurately, it may give a handy demonstration tool for the event bush approach and open a new avenue of research in directed hypergraph theory.		Cyril A. Pshenichny;Dmitry Mouromtsev	2013		10.1007/978-3-642-35786-2_22	combinatorics;discrete mathematics;topology;mathematics	NLP	-5.4144525527861	10.326150873052228	14860
de01b18a0e63e1fd13210a9736a38eb707027d8a	modal system s4.4		The theses Gl and D2 are the proper axioms of the well-known systems S4.2 and S4.3 respectively, cf. [2], [l], [β], and [ l l j In [2], p. 263, Dummett and Lemmon have proved that Ml, i.e. their formula (8), does not hold in S4.3. Prior, [6], p. 139, pointed out that Geach showed that in the field of S4.2 theses Ml and Nl are equivalent. As one can easily notice βl and βZ verify also the following two formulas	emoticon;modal logic;nl (complexity);scott r. lemmon;whole earth 'lectronic link	Boleslaw Sobocinski	1964	Notre Dame Journal of Formal Logic	10.1305/ndjfl/1093957980		AI	-10.515465111006653	14.286323894249408	15002
b5ae25a800b38e5dd91153582969509b47ebf741	loop monotonic computations: an approach for the efficient run-time detection of races	usc;on the fly;it evaluation;static analysis	This paper describes an assertion based approach for the detection of races in parallel programs that exploit fork-join parallelism. Assertions are introduced following joins in the program and then verified at run-time to ensure the absence of races. In geneml, the detection of a race requires the monitoring of all memory references at run-time. However, using the notion of loop monotonicity, we identify situations in which efficiently verifiable assertions can be generated. An expression is considered to be loop monotonic if its evaluation, during successive iterations of the loop, results in a monotonically increasing/decreasing sequence of values. Static analysis techniques are presented to identify loop monotonic statements. Loop monotonicity is used to reduce the storage overhead associated with on-the-fly detection of races by reducing the amount of information which is collected at run-time for usc in verifying an assertion. The run-time overhead is decreased by simplifying the checks performed during the verification of an assertion. Monotonicity information can also be used to reduce the run-time overhead due to array bound checking. Given that a very large percentage of subscript expressions in scientific programs are monotonic, significant savings can be expected using these techniques. We conclude with a brief discussion of the applicability of our approach to the detection of other types of synchronization related errors.	assertion (software development);computation;deadlock;formal verification;infinite loop;iteration;overhead (computing);parallel computing;race condition;run time (program lifecycle phase);sensor;static program analysis;waits	Rajiv Gupta;Madalene Spezialetti	1991		10.1145/120807.120816	control engineering;real-time computing;simulation;computer science;static analysis	PL	-18.55574307384137	31.778702676046095	15004
6733c67e42f64557b31488a5e65998858868970a	type inference with extended pattern matching and subtypes	pattern matching;type inference		pattern matching;type inference	Lalita Jategaonkar Jagadeesan;John C. Mitchell	1993	Fundam. Inform.		computer science;type inference;pattern matching;programming language	Theory	-23.176695378184952	21.263006813033886	15054
119d29be70fe5e8954c5ee6211ea3a96a94e1e1e	continuations, processes, and sharing	processes;call by need;bisimulation;continuation passing style;effects;transforms;π calculus	Continuation-passing style (CPS) transforms have long been important tools in the study of programming. They have been shown to correspond to abstract machines and, when combined with a naming transform that expresses shared values, they enjoy a direct correspondence with encodings into process calculi such as the π-calculus. We present our notion of correctness and discuss the sufficient conditions that guarantee the correctness of transforms. We then consider the call-by-value, call-by-name and call-by-need evaluation strategies for the λ-calculus and present their CPS transforms, abstract machines, π-encodings, and proofs of correctness. Our analysis covers a uniform CPS transform, which differentiates the three evaluation strategies only by the treatment of function calls. This leads to a new CPS transform for call-by-need requiring a less expressive form of side effect, which we call constructive update.	abstract machine;artifact (software development);continuation;continuation-passing style;correctness (computer science);feedback;ibm notes;immutable object;lambda calculus;lazy evaluation;memoization;paul brainerd;process calculus;side effect (computer science);type system;ueli maurer (cryptographer);π-calculus	Paul Downen;Luke Maurer;Zena M. Ariola;Daniele Varacca	2014		10.1145/2643135.2643155	π-calculus;computer science;bisimulation;continuation-passing style;theoretical computer science;programming language;algorithm	PL	-18.2393929754471	23.62928056042321	15062
6cf1f4856bd902bc5a38c42e713d34bb3d6a96c1	acyclicity and coherence in multiplicative exponential linear logic	logique lineaire;logica exponencial;exponential logic;multiplicative exponential linear logic;logical programming;logica lineal;logique multiplicative;multiplicative logic;programmation logique;programacion logica;linear logic;logica multiplicativa;logique exponentielle	We give a geometric condition that characterizes MELL proof structures whose interpretation is a clique in non-uniform coherent spaces: visible acyclicity. We define the visible paths and we prove that the proof structures which have no visible cycles are exactly those whose interpretation is a clique. It turns out that visible acyclicity has also nice computational properties, especially it is stable under cut reduction.	clique (graph theory);coherence (physics);computation;linear logic	Michele Pagani	2006		10.1007/11874683_35	linear logic;discrete mathematics;computer science;mathematics;programming language;algorithm	Logic	-10.557055835984608	15.622643897758238	15139
17ef1fe1cba6fb768d63fad4d4f62997decc9ecf	aspects of language design for combinatorial computing	computer languages;boolean functions;programming profession computer languages marine vehicles differential equations indium tin oxide boolean functions automatic programming digital arithmetic resumes;automatic programming;resumes;indium tin oxide;marine vehicles;machine design;programming profession;language development;digital arithmetic;differential equations;language design	Experience in the area of combinatorial computing scription of programming notation for the manipulation acquired by the Maniac group at Los Alamos has motivated language of sets is given in Section III. Further useful combinadevelopment in this direction. Features incorporated into the MADt CAP language to aid in programming combinatorial calculations intonalla g developm esare pres nt iection clude notation for set synthesis and analysis, notation for variably IV Finally, Section V discusses the important relationnested iterations (backtracking) and notation for complex condiship of machine design to such language evolution and tional statements. The paper also examines the relationship of such to combinatorial computing in general. combinatorial language design to machine design. Prominent in this discussion are the bit manipulation (shifting, counting and searchI	backtracking;bit manipulation;iteration;maniac i;transition path sampling	Mark B. Wells	1964	IEEE Trans. Electronic Computers	10.1109/PGEC.1964.263845	fourth-generation programming language;first-generation programming language;electronic engineering;very high-level programming language;language primitive;indium tin oxide;programming domain;computer science;programming language implementation;engineering;electrical engineering;theoretical computer science;third-generation programming language;mathematics;programming paradigm;low-level programming language;boolean function;fifth-generation programming language;programming language theory;visual programming language;programming language;second-generation programming language;high-level programming language;differential equation;algorithm;algebra	PL	-27.327122460079497	23.661217592290154	15166
f6129bc43b886343dc9ffcc1bfb2abce88bd4686	parallel evolution programming language for data flow machines	general and miscellaneous mathematics computing and information science;programming language;parallel evolution;data flow processing;programming 990200 mathematics computers;data flow;parallel programs;parallel languages;parallel processing;programming languages	In the first part of this paper we make a survey of data-driven control concepts and their relation to non-von-Neumann and parallel programming. We then introduce informally a high-level data flow language, PEP, which implements these concepts in a simple and clear form, bringing them closer to practical use by the programmers.The second part of the paper (to appear) will be a PEP programmer's manual which provides more formal description of the language.	dataflow architecture;description logic;high- and low-level;parallel computing;programmer;programming language	Yury Litvin	1982	SIGPLAN Notices	10.1145/988152.988162	fourth-generation programming language;data flow diagram;parallel processing;first-generation programming language;declarative programming;very high-level programming language;language primitive;programming domain;reactive programming;computer science;theoretical computer science;third-generation programming language;parallel evolution;functional logic programming;programming paradigm;symbolic programming;low-level programming language;inductive programming;fifth-generation programming language;programming language theory;programming language;second-generation programming language;high-level programming language;control flow analysis;parallel programming model	PL	-24.702702846467627	23.00578990364131	15167
15534d0914095b97c87650cdf82ee504dc2affe9	reasoning with finite sets and cardinality constraints in smt		We consider the problem of deciding the satisfiability of quantifier-free formulas in the theory of finite sets with cardinality constraints. Sets are a common high-level data structure used in programming; thus, such a theory is useful for modeling program constructs directly. More importantly, sets are a basic construct of mathematics and thus natural to use when formalizing the properties of computational systems. We develop a calculus describing a modular combination of a procedure for reasoning about membership constraints with a procedure for reasoning about cardinality constraints. Cardinality reasoning involves tracking how different sets overlap. For efficiency, we avoid considering Venn regions directly, as done in previous work. Instead, we develop a novel technique wherein potentially overlapping regions are considered incrementally as needed, using a graph to track the interaction among the different regions. The calculus has been designed to facilitate its implementation within SMT solvers based on the DPLL(T ) architecture. Our experimental results demonstrate that the new techniques are competitive with previous techniques and can scale much better on certain classes of problems.	3d computer graphics;algorithm;benchmark (computing);boolean satisfiability problem;byzantine fault tolerance;cardinality (data modeling);computation;correctness (computer science);data structure;decision problem;expect;generative grammar;graph (discrete mathematics);high- and low-level;leon;modulo operation;philippe kruchten;quantifier (logic);relational operator;satisfiability modulo theories;set theory;simultaneous multithreading;solver	Kshitij Bansal;Clark W. Barrett;Andrew Reynolds;Cesare Tinelli	2018	Logical Methods in Computer Science	10.23638/LMCS-14(4:12)2018	discrete mathematics;cardinality;architecture;combinatorics;dpll algorithm;satisfiability;mathematics;finite set;modular design;venn diagram;data structure	AI	-12.92961944727363	20.625807369612474	15197
0801455ebacc367201f7fb3fc3681573154a8b39	on the complexity of counting in description logics	description logic;data model;satisfiability	Many Description Logics (DLs) allow for counting expressions of various forms that are important in many applications, e.g., for reasoning with semantic data models and for applications concerned with the configuration of technical systems. We present two novel complexity results for DLs that contain counting constructs: (1) We prove that concept satisfiability for ALCQI is decidable in PS PACE even if binary coding of numbers in the input is assumed. (2) We prove that TBox consistency for ALCQI with cardinality restrictions is NEXPTIME-complete. 1 The Complexity ofALCQI Qualifying number restrictions [10] are a common generalisation of both role-quantification and standard number restrictions that are present in almost all DL systems. They provide an expressive means to describe objects by the number of other objects they are related to and are necessary for reasoning with semantic data models [5]. In [16] we have shown that—at least for ALC—number restrictions can be replaced by qualifying number restrictions without increasing the (worst-case) complexity of the satisfiability problem. In this paper we extend this result to inverse roles. Definition 1 (The DL ALCQI) LetNC be a set ofatomic conceptsandNR a set ofatomic roles. The set ofALCQIrolesNR isNR[fR j R 2 NRg. ConceptsinALCQI are built inductively using the following rules: 1. everyA 2 NC is anALCQI-concept, and 2. if C;D1; D2 are ALCQI-concepts,n 2 N, andR 2 NR, then:C, D1 u D2, D1 t D2, (> n R C), and (6 n R C) areALCQI-concepts. We use./ as a placeholder for 6 and>. For an interpretation I = ( I ; I), we extend the usual semantics of ALCconcepts to qualifying number restrictions by setting (./ n R C)I :=fx 2 I j ]fy j (x; y) 2 RI ; y 2 CIg./ng; where] denotes the cardinality of a set. For roles names R, we define(R )I := f(y; x) j (x; y) 2 RIg. WithALCQ we denote the fragment of ALCQI that does not contain inverse roles. WithSAT(ALCQ) and SAT(ALCQI) we denote the set of all satisfiableALCQ-, resp.,ALCQI-concepts. Obviously, existential and universal restrictions can be expressed inALCQI with qualifying number restrictions using the equivalences 9R:C (> 1 R C) and8R:C (6 0 R :C). In order to avoid considering roles such as R , we define a functionInv that returns the inverse of a role by settingInv(R) := R if R 2 NR, andInv(R) := S if R = S for someS 2 NR. Reasoning with Qualifying Number Restrictions In [10] a tableaux algorithm is presented that decides S AT(ALCQ) in polynomial space, provided that only unary coding of numbers in the input is allowed. In [6] it is conjectured that binary coding of numbers makes S AT(ALCQ) EXPTIME-hard. Why does the coding of numbers seem to be of such an importance for the problem? The answer lies in the nature of the tableaux algorithms forALCQ: Like other tableaux algorithms, they decide the satisfiability of a concept C by trying to construct a model for it. For an instance x of a concept(> n R C), the algorithm in [10] generates n successors of x, and the correctness of the algorithms relies on the fact that they are kept in memorysimultaneously . Assuming unary coding of numbers in the input, this is can be done in polynomial space because the number n will consumen bits in the input and hence the amount of memory needed for the n successors is polynomial in the size of the input. This changes if we assume binary coding of numbers: then n consumes onlylog2 n bits in the input, making the amount of memory required to store n successors potentially exponential in the size of the input. In [16] we give an algorithm derived from the one presented in [10] that is capable of deciding S AT(ALCQ) in PSPACE, even if binary coding of numbers in the input is allowed. While still generatingn successors for a concept (> n R C), non-deterministic guessing of an assignment of relevant constraints to newly generated individuals is used to be able to generate them successively while re-using space. This determines the complexity of S AT(ALCQ) as PSPACEcomplete. As a result we may augment ALC with qualifying number restrictions without increasing the (worst-case) complexity of the satisfiability problem. In this paper we present an extension of the algorithm in [16] that can, additionally, deal with inverse roles and runs in polynomial space. The so-called “reset-restart” technique used to deal with concepts moving “backwards” in the completion tree due to thechoose-rule has already been used in [11] to handle inverse roles. Definition 2 An ALCQI-conceptC is in negation normal form (NNF) if negation occurs only in front of atomic concepts; we denote the NNF of :C by C. For a conceptC in NNF we define clos (C) to be the smallest set of ALCQIconcepts that contains C and is closed under sub-concepts and . A completion treefor an ALCQI-conceptD is a tree where each nodex is labelled with a setL(x) clos(D) and each edge hx; yi is labelled with a (possibly inverse) role nameL(hx; yi) = R for a role occurring in clos(D). Given a completion tree, a node y is called anR-successor of a nodex iff y is a successor of x andL(hx; yi) = R. A nodey is called anR-neighbourof x iff y is anR-successor of x, or if x is an Inv(R)-successor of y. Predecessors, ancestors, paths, etc., are defined as usual. A nodex in T is said to contain aclashif, for some atomic concept A, fA;:Ag L(x), or for some concept C, role R, andn 2 N, it holds that (6 n R C) 2 L(x) and ]RT(x;C) > n, where RT(x;C) := fy j y is R-neighbour ofx in T and C 2 L(y)g. A completion tree is called clash-freeiff none of its nodes contains a clash; it is calledcompleteiff none of the expansion rules in Figure 1 is applicable to any of its nodes. To test the satisfiability of an ALCQI-conceptD, the algorithm starts with a completion tree consisting of a single nodex0 with L(x0) = fDg. It applies the expansion rules, stopping when a clash occurs, and answers “ D is satisfiable” iff the completion rules can be applied in such a way that they yield a complete and clash-free completion tree. Correctness of the Algorithm In this paper we can only give the main ideas of the proofs, for details please refer to [17]. In order to prove the correctness of the algorithm it is necessary to show termination, soundness, and completeness. The following lemma states the termination of the algorithm an collects some facts that will be needed for the complexity analysis. Lemma 3 Let D be anALCQI-concept in NNF andT a completion tree that is generated for D by the tableaux algorithm. u-rule: if 1.C1 u C2 2 L(x) and 2.fC1; C2g 6 L(x) thenL(x) ! L(x) [ fC1; C2g t-rule: if 1.C1 t C2 2 L(x) and 2.fC1; C2g \ L(x) = ; thenL(x) ! L(x) [ fCg for someC 2 fC1; C2g chooseif 1. (./ n R C) 2 L(x) and rule: 2. there is anR-predecessor y of x with fC; Cg \ L(x) = ; thenL(y) ! L(y) [ fEg for someE 2 fC; Cg and deleteall descendants of y. >-rule: if 1. (> n R C) 2 L(x), none from the above rules is applicable tox or any of its ancestors, and 2. ]RT(x;C) < n then create a new node y with L(hx; yi) = R and L(y) = fC;E1; : : : ; Eng where fD1; : : : Dng = fD j (./ n R D) 2 L(x)g andEi 2 fDi; Dig. Figure 1: Tableaux expansion rules for ALCQI 1. ]clos(D) = O(jDj). 2. The length of a path in T is bounded byjDj. 3. The out-degree of T is limited by]clos(D) 2jDj. 4. For any concept D, the algorithm terminates. Proof. (1) is easily proved by observing that clos(D) = sub(D) [ f C j C 2 sub(D)g, wheresub(D) is the set of all subconcepts of D which contains at most jDj elements. (2) and (3) are simple consequence of the expansion rules. (4) is a consequence of the bounded size of the tree because each rule application either adds nodes to the tree or the adds concepts to the label of a node x which is always a subset of clos(D). The only problem is the deletion of nodes that is triggered by thechoose-rule. However, if this rule deletes a node from the tree, also the label of an ancestor of this node grows, which prevents an infinite sequence of rule applications. ut Lemma 4 (Soundness)If the rules can be applied to an ALCQI-conceptD such that they yield a complete and clash-free completion tree, then D is satisfiable. Proof. LetT be a complete and clash-free completion tree for D. A modelI = ( I ; I) for D can be defined by choosing I to be the nodes of T and by defining: AI = fx j A 2 L(x)g for all atomic conceptsA RI = fhx; yi j L(hx; yi) = R orL(hy; xi) = Inv(R)g: Inductively it can be shown that, for all x 2 I and allC 2 clos(D), C 2 L(x) impliesx 2 CI . SinceD 2 L(x0) it follows thatDI 6= ; and, henceI is a model ofD. ut Lemma 5 (Completeness)LetD be anALCQI-concept: If D is satisfiable, then the expansion rules can be applied in such a way that they yield a complete and clash-free completion tree forD. Proof. Let I = ( I ; I) be a model forD. We will useI to guide the application of the non-deterministic completion rules. For this we incremently define a function mapping the nodes inT to elements of I such that at any given stage the following holds: if C 2 L(x), then (x) 2 CI if L(hx; yi) = R; thenh (x); (y)i 2 RI if y; z areR-neighbours ofx with y 6= z; then (y) 6= (z) 9>>=>; ( ) CLAIM : Whenever( ) holds for a treeT and a function , and a rule is applicable to T then it can be applied in a way that maintains( ). The proof of this claim can be found in [17]. Lemma 5 is a consequence of this claim: obviously, ( ) holds for the initial tree with root nodex0 if we set (x0) := s0 for an elements0 2 DI (such an element must exist because I is a model forD). Lemma 3 yields that each sequence of rule applications must terminate, and also each tree for which ( ) holds is necessarily clash-free because: (a) it cannot contain a clash of the formfA;:Ag L(x) since this would imply (x) 2 AI and (x) 62 AI ; (b) it can neither contain a clash of the form(6 n R C) 2 L(x) and]RT(x;C) > n because is injective on the set of all R-neighbours of a node x and hence]RT(x;C) > n implies]RI(x;C) > n. This contradicts (x) 2 (6 n R C)I . ut Theorem 6 	algorithm;analysis of algorithms;best, worst and average case;binary number;boolean satisfiability problem;correctness (computer science);description logic;directed graph;exptime;emoticon;nexptime;negation normal form;numerical recipes;ps-algol;pspace;polynomial;rs-232;semantic data model;soundness (interactive proof);tbox;terminate (software);time complexity;tox;unary coding;unary operation	Stephan Tobies	1999			t-norm fuzzy logics;monoidal t-norm logic;data model;description logic;satisfiability;algorithm;mathematics	AI	-6.333801288342685	15.099201962449932	15203
0298e1a2ba5921f83ce6ae3ec4e08f71aa335879	stochastic local search for satisfiability modulo theories	bit vectors;stochastic local search;satisfiability modulo theories	Satisfiability Modulo Theories (SMT) is essential for many practical applications, e.g., in hardand software verification, and increasingly also in other scientific areas like computational biology. A large number of applications in these areas benefit from bit-precise reasoning over finite-domain variables. Current approaches in this area translate a formula over bit-vectors to an equisatisfiable propositional formula, which is then given to a SAT solver. In this paper, we present a novel stochastic local search (SLS) algorithm to solve SMT problems, especially those in the theory of bit-vectors, directly on the theory level. We explain how several successful techniques used in modern SLS solvers for SAT can be lifted to the SMT level. Experimental results show that our approach can compete with state-of-the-art bit-vector solvers on many practical instances and, sometimes, outperform existing solvers. This offers interesting possibilities in combining our approach with existing techniques, and, moreover, new insights into the importance of exploiting problem structure in SLS solvers for SAT. Our approach is modular and, therefore, extensible to support other theories, potentially allowing SLS to become part of the more general SMT framework.	algorithm;bit array;boolean satisfiability problem;bridging (networking);computation;computational biology;local search (optimization);relevance;satisfiability modulo theories;software verification;solver;standard sea level;theory	Andreas Fröhlich;Armin Biere;Christoph M. Wintersteiger;Youssef Hamadi	2015			machine learning;satisfiability modulo theories;algorithm	AI	-15.261104186278523	24.619676141385096	15242
283f98af1f7b5ec189b3880faa0dc47f05f664a9	ariadne: dominance checking of nonlinear hybrid automata using reachability analysis	dominance checking;approximation capability;approximation technique;reachability analysis;continuous behavior;computable analysis;hybrid system;hybrid automata analysis;nonlinear hybrid automaton;continuous environment;hybrid automaton	dominance checking;approximation capability;approximation technique;reachability analysis;continuous behavior;computable analysis;hybrid system;hybrid automata analysis;nonlinear hybrid automaton;continuous environment;hybrid automaton	approximation;backward compatibility;differential inclusion;hybrid automaton;hybrid system;nonlinear system;reachability;scalability	Luca Benvenuti;Davide Bresolin;Pieter Collins;Alberto Ferrari;Luca Geretti;Tiziano Villa	2012		10.1007/978-3-642-33512-9_8	discrete mathematics;theoretical computer science;mathematics;algorithm	Logic	-6.1534254381390765	25.347416466125267	15262
0a8ac4b1f97b92c489829794df18875c7fe94c2a	a method for solving nash equilibria of games based on public announcement logic	iterated admissibility algorithm;game theory;nash equilibria;rational agent;higher order;public announce logic;iterated admissibility algorithm rationality public announce logic demo;demo;rationality	Describing the interactional behavior of rational agents and seeking equilibria are two main domains of game theory. The epistemic foundation of the above two domains is based on the assumption that all players are rational. However, game theory itself cannot precisely model the higher-order information changes of mutual knowledge among players, so in the current studies of game theory the interpretations of rationality are vague. In this paper, a concept of the rationality is redefined through incorporating an epistemic ingredient. Then a method is proposed to solve and refine Nash equilibria which is grounded on public announcement logic, and it is proved that the iterated announcement of this rationality assertion characterizes the iterated admissibility algorithm in game theory, which offers a dynamic epistemic foundation for this algorithm. Finally, an implementation of this method, based on the extended DEMO, is shown to be correct.	algorithm;assertion (software development);backward induction;definition;epistemic modal logic;game demo;game theory;iterated function;iteration;nash equilibrium;rational agent;rationality;vagueness	Jianying Cui;Xiaojia Tang	2010	Science China Information Sciences	10.1007/s11432-010-4010-0	implementation theory;rational agent;game theory;mathematical optimization;best response;higher-order logic;ecological rationality;rationality;non-credible threat;mathematics;normal-form game;mathematical economics;algorithm;nash equilibrium	AI	-17.346574229752573	4.838378579643063	15267
2dd49b80fdf78c90d0dd16c8c89d82633a13e3bb	on perfect supercompilation	rewrite systems	We extend positive supercompilation to handle negative as well as positive information. This is done by instrumenting the underlying unfold rules with a small rewrite system that handles constraints on terms, thereby ensuring perfect information propagation. We illustrate this by transforming a näıvely specialised string matcher into an optimal one. The presented algorithm is guaranteed to terminate by means of generalisation steps.	algorithm;instrumentation (computer programming);metacompilation;rewrite (programming);rewriting;software propagation;terminate (software)	Jens P. Secher;Morten Heine Sørensen	1999		10.1007/3-540-46562-6_10	discrete mathematics;theoretical computer science;mathematics;algorithm	AI	-15.415503792756938	21.995711026853034	15308
a2bb4b116e688b0afd5736ddd58049c1e0697c74	adding semantic knowledge to a relational database system	relational database system	This chapter suggests two mechanisms for adding semantic knowledge to a data manager, namely inclusion of an AI oriented rules system and a particular use of abstract data types. Both topics are explored in the context of the INGRES relational database system.	relational database management system	Michael Stonebraker	1982			domain relational calculus;data definition language;sql;relational model;relational calculus;entity–relationship model;relational database;database normalization;database model;view;database schema;graph database;object-relational impedance mismatch;database design;spatiotemporal database	DB	-30.758657587825105	9.792543883722969	15323
c00b635169b8b992f1825f4e65daeb988ab08887	specialising the ground representation in the logic programming language gödel	ground representation.;meta-programming;partial evaluation;meta programming	Meta-programs form a class of logic programs of major importance. In the past it has proved very difficult to provide a declarative semantics for meta-programs in languages such as Prolog. These problems have been identified as largely being caused by the fact that Prolog fails to handle the necessary representation requirements adequately. The ground representation is receiving increasing recognition as being necessary to adequately represent meta-programs. However, the expense it incurs has largely precluded its use to date. The logic programming language Gödel is a declarative successor to Prolog. Gödel provides considerable support for meta-programming, in the form of a ground representation. Using this representation, Gödel meta-programs have the advantage of having a declarative semantics and can be optimised by program specialisation, to execute in a time comparable to equivalent Prolog meta-programs which use a nonground representation.	declarative programming;gödel;logic programming;metaprogramming;programming language;prolog;requirement	Corin A. Gurr	1993			metaprogramming;constraint programming;horn clause;computer science;theoretical computer science;datalog;fifth-generation programming language;programming language;logic programming;partial evaluation;algorithm	AI	-20.057442265449172	15.221703939578852	15379
86214d52a7b514d144717808184f18deb8697bb6	clustered computer architecture for automated database query generation system	cluster computing		computer architecture	Tae-Wan Ryu;Haklin Kimm	2000			database tuning;computer science;query expansion;rdf query language;data mining;query language;database;query by example;sargable;query optimization;view	Arch	-31.341418127038786	7.259627716349321	15446
bec246b2bc808e61364f0a7f2848c9481aaf098b	compositional derivation of symmetries for constraint satisfaction	modelizacion;abstraction;global constraint;abstraccion;constraint satisfaction;modelisation;satisfaction contrainte;datavetenskap datalogi;constraint satisfaction problem;satisfaccion restriccion;computer science;modeling	This paper reconsiders the problems of discovering symmetries in constraint satisfaction problems (CSPs). It proposes a compositional approach which derives symmetries of the applications from primitive constraints. The key insight is the recognition of the special role of global constraints in symmetry detection. Once the symmetries of global constraints are available, it often becomes much easier to derive symmetries compositionally and efficiently. The paper demonstrates the potential of this approach by studying several classes of value and variable symmetries and applying the resulting techniques to two nontrivial applications. The paper also discusses the potential of reformulations and high-level modeling abstractions to strengthen symmetry discovery.	constraint satisfaction problem;high- and low-level	Pascal Van Hentenryck;Pierre Flener;Justin Pearson;Magnus Ågren	2005		10.1007/11527862_17	constraint logic programming;mathematical optimization;constraint programming;combinatorics;discrete mathematics;systems modeling;constraint satisfaction;computer science;constraint graph;constraint satisfaction dual problem;mathematics;abstraction;complexity of constraint satisfaction;constraint;constraint satisfaction problem;algorithm;hybrid algorithm;local consistency;backtracking	AI	-14.21171695615183	22.648200279042733	15506
7647349cb36dd1e88b9201fc0002bf7d660a56de	an operator precedence parser for standard prolog text	operator precedence parser;standard prolog text	Prolog is a language with a dynamic grammar which is the result of embedded operator declarations. The parsing of such a language cannot be done easily by means of standard tools. Most often, an existing parsing technique for a static grammar is adapted to deal with the dynamic constructs. This paper uses the syntax definition as defined by the ISO standard for the Prolog language. It starts with a brief discussion of the standard, highlighting some aspects that are important for the parser, such as the restrictions on the use of operators as imposed by the standard in order to make the parsing deterministic. Some possible problem areas are also indicated. As output is closely related to input in Prolog, both are treated in this paper. Some parsing techniques are compared and an operator precedence parser is chosen to be modified to deal with the dynamic operator declarations. The necessary modifications are discussed and an implementation in C is presented. Performance data are collected and compared with a public domain Prolog parser written in Prolog. It is the first efficient public domain parser for Standard Prolog that actually works and deals with all the details of the syntax.	operator-precedence parser;prolog	Koen De Bosschere	1996	Softw., Pract. Exper.	10.1002/(SICI)1097-024X(199607)26:7%3C763::AID-SPE33%3E3.0.CO;2-L	natural language processing;input/output;parser combinator;syntax;parsing expression grammar;top-down parsing language;computer science;bottom-up parsing;operator;operating system;programming language;implementation;prolog;top-down parsing;algorithm;taxonomy	NLP	-25.046154609714836	24.071254222076412	15526
9227276c29a48e730940b23fc56a3270816176ea	combinatory abstraction bsing b, b' and friends	assignment;implicational logics;asignacion;combinatory;b;relacion orden;assignation;abstraction;ordering;reducibility;abstraccion;algorithme;algorithm;relation ordre;tipificacion;typing;typage;friends;abstractability;algoritmo	In this paper we characterise precisely the sets of terms whose abstractions can be defined using the following partial bases of combinators: {B, B′, I}, {B, B′, I, W}, {B, B′, I, K}, {B, T, I}, {B, T, I, W} and {B, T, I, I}. The reduction axioms for B′ and T are B′XYZY(XZ) TXYZYXZ. The first two B′-bases correspond via type-assignment to two interesting implicational logics. T has the re-ordering property of B′ but not its bracketing property, and turns out to be strictly stronger than B′ but strictly weaker than CI whose reduction axiom is CIXYYX.		P. Trigg;J. Roger Hindley;Martin W. Bunder	1994	Theor. Comput. Sci.	10.1016/0304-3975(94)90114-7	arithmetic;combinatorics;combinatory logic;order theory;computer science;assignment;mathematics;abstraction;programming language;algorithm	ECom	-8.078388156340853	18.732764753708	15546
69b9221559df47c6e3d5ec93c5fcec232f8467c3	bibel's matrix connection method in paraconsistent logic: general concepts and implementation	logic lattices;theorem proving formal logic;theorem proving;propositional logic;formal logic;classical logic;annotated propositional logic bibel s matrix connection method paraconsistent logic mechanized proof logical statements;paraconsistent logic	Bibel's matrix connection method is an alternative to resolution for the mechanized proof of logical statements. Bibel's method was originally defined for classical logic. In this work, an adaptation of the method for annotated propositional logic is given, followed by a simple case study. Some implementation details are also presented.	paraconsistent logic	Décio Krause;Emerson Faria Nobre;Martin A. Musicante	2001		10.1109/SCCC.2001.972644	predicate logic;dynamic logic;zeroth-order logic;discrete mathematics;classical logic;resolution;description logic;higher-order logic;paraconsistent logic;horn clause;many-valued logic;intuitionistic logic;intermediate logic;first-order logic;computational logic;mathematics;logic;term logic;second-order logic;algorithm;philosophy of logic;rule of inference;autoepistemic logic	Robotics	-13.907247043817472	13.281440612304966	15561
30610d7cd30d5c2b8979c1e1ad994519d3c17316	abstract interpretation of non-monotone bi-inductive semantic definitions	labelled transition system;operational semantics;abstract machine;denotational semantic;enriched categories;abstract interpretation;static program analysis	Divergence/nonterminating behaviors definitely need to be considered in static program analysis [13], in particular for typing [2,11].	abstract interpretation;inductive reasoning;monotone	Radhia Cousot	2008		10.1007/978-3-540-78163-9_1	computer science;theoretical computer science;abstract machine;programming language;denotational semantics of the actor model;operational semantics;static program analysis	NLP	-15.565129945015398	21.352558263540708	15562
075cc7940e13d888d058453cbe2607873fa2be59	on-the-fly token similarity joins in relational databases	string similarity;q grams;tokenize operator;set similarity join	Token similarity joins represent data items as sets of tokens, for example, strings are represented as sets of q-grams (substrings of length q). Two items are considered similar and match if their token sets have a large overlap. Previous work on similarity joins in databases mainly focuses on expressing the overlap computation with relational operators. The tokens are assumed to preexist in the database, and the token generation cannot be expressed as part of the query. Our goal is to efficiently compute token similarity joins on-the-fly, i.e., without any precomputed tokens or indexes. We define tokenize, a new relational operator that generates tokens and allows the similarity join to be fully integrated into relational databases. This allows us to (1) optimize the token generation as part of the query plan, (2) provide the query optimizer with cardinality estimates for tokens, (3) choose efficient algorithms based on the query context. We discuss algebraic properties, cardinality estimates, and an efficient iterator algorithm for tokenize. We implemented our operator in the kernel of PostgreSQL and empirically evaluated its performance for similarity joins.	algorithm;computation;grams;iterator;kernel (operating system);lexical analysis;linear algebra;mathematical optimization;postgresql;precomputation;query optimization;query plan;relational database;relational operator;substring;turing completeness	Nikolaus Augsten;Armando Miraglia;Thomas Neumann;Alfons Kemper	2014		10.1145/2588555.2610530	computer science;theoretical computer science;data mining;database;string metric	DB	-29.628204944731348	5.304940462919611	15572
0d55e79af372c91cebc1076e961726fc8612488b	simulating parity reasoning (extended version)		Propositional satisfiability (SAT) solvers, which typically operate using conjunctive normal form (CNF), have been successfully applied in many domains. However, in some application areas such as circuit verification, bounded model checking, and logical cryptanalysis, instances can have many parity (xor) constraints which may not be handled efficiently if translated to CNF. Thus, extensions to the CNF-driven search with various parity reasoning engines ranging from equivalence reasoning to incremental Gaussian elimination have been proposed. This paper studies how stronger parity reasoning techniques in the DPLL(XOR) framework can be simulated by simpler systems: resolution, unit propagation, and parity explanations. Such simulations are interesting, for example, for developing the next generation SAT solvers capable of handling parity constraints efficiently.	boolean satisfiability problem;conjunctive normal form;cryptanalysis;exclusive or;gaussian elimination;inference engine;model checking;simulation;software propagation;turing completeness;unit propagation	Tero Laitinen;Tommi A. Junttila;Ilkka Niemelä	2013	CoRR		combinatorics;discrete mathematics;mathematics;algorithm	AI	-17.802265699079182	15.198993744042692	15596
12d73635fa7936e41bec5544a53f990ae18fdbc7	contract soundness for object-oriented languages	fondation ouvrage;preconditionnement;syntax;object oriented language;foundations;hierarchized structure;specification;structure hierarchisee;preconditioning;object oriented programming;syntaxe;refinement method;fundacion obra;especificacion;object oriented;precondicionamiento;fiabilite logiciel;methode raffinement;fiabilidad logicial;sintaxis;programmation orientee objet;software reliability;metodo afinamiento;contract enforcement;estructura jerarquizada	Checking pre- and post-conditions of procedures and methods at runtime helps improve software reliability. In the procedural world, pre- and post-conditions have a straightforward interpretation. If a procedure's pre-condition doesn't hold, the caller failed to establish the proper context. If a post-condition doesn't hold, the caller failed to establish the proper context. If a post-condition doesn't hold, the procedure failed to compute the expected result.  In the object-oriented world, checking pre- and post-conditions for methods, often called contracts in this context, poses complex problems. Because methods may be overridden, it is not sufficient to check only pre- and post-conditions. In addition, the contract hierarchy must be checked to ensure that the contracts on overridden methods are properly related to the contracts on overriding methods. Otherwise, a class hierarchy may violate the substitution principle, that is, it may no longer be true that an instance of a class is substitutable for objects of the super-class.  In this paper, we study the problem of contract enforcement in an object-oriented world from a foundational perspective. More specifically, we study contracts as refinements of types. Pushing the analogy further, we state and prove a contract soundness theorem that captures the essential properties of contract enforcement. We use the theorem to illustrate how most existing tools suffer from a fundamental flaw and how they can be improved.	caller id;class hierarchy;design by contract;essence;flaw hypothesis methodology;ibm notes;instance (computer science);jackson;java;method (computer programming);method overriding;philippe kruchten;postcondition;precondition;programmer;run time (program lifecycle phase);shriram krishnamurthi;software reliability testing	Robert Bruce Findler;Matthias Felleisen	2001		10.1145/504282.504283	computer science;programming language;object-oriented programming;computer security;algorithm	PL	-23.093016642211943	28.863685706068317	15604
e2580c42b5b1dcc3ec3a23ed994c85734b3be6ec	legislation as logic programs	legislation;logic programs	The linguistic style in which legislation is normally written has many similarities with the language of logic programming. However, examples of legal language taken from the British Nationality Act 1981, the University of Michigan lease termination clause, and the London Underground emergency notice suggest several ways in which the basic model of logic programming could usefully be extended. These extensions include the introduction of types, relative clauses, both ordinary negation and negation by failure, integrity constraints, metalevel reasoning and procedural notation. In addition to the resemblance between legislation and programs, the law has other important similarities with computing. It needs for example to validate legislation against social and political specifications, and it needs to organise, develop, maintain and reuse large and complex bodies of legal codes and procedures. Such parallels between computing and law suggest that it might be possible to transfer useful results and techniques in both directions between these different fields. One possibility explored in this paper is that the linguistic structures of an appropriately extended logic programming language might indicate ways in which the language of legislation itself could be made simpler and clearer.	artificial intelligence;code;computational logic;data integrity;formal specification;knowledge representation and reasoning;logic programming;natural language;negation as failure;norm (social);parallels desktop for mac;programming language;underground	Robert A. Kowalski	1991		10.1007/3-540-55930-2_15	notice;natural language processing;deductive database;notation;relative clause;data integrity;algorithm;negation;computer science;logic programming;legislation;artificial intelligence	AI	-18.86039604745908	15.382400721142046	15611
0cffe6a476e51de97cd7c1d590333a5b78dd2ded	computation environments, an interactive semantics for turing machines (which p is not equal to np considering it)		To scrutinize notions of computation and time complexity, we introduce and formally define an interactive model for computation that we call it the computation environment. A computation environment consists of two main parts: i) a universal processor and ii) a computist who uses the computability power of the universal processor to perform effective procedures. The notion of computation finds it meaning, for the computist, through his interaction with the universal processor. We are interested in those computation environments which can be considered as alternative for the real computation environment that the human being is its computist. These computation environments must have two properties: 1-being physically plausible , and 2-being enough powerful. Based on Copeland' criteria for effective procedures, we define what a physically plausible computation environment is. By being enough powerful, we mean that the universal processor of the computation environment, must be in a way that the computist can carry out (via the universal processor) all logical deductions that the human being can carry out. We construct two physically plausible and enough powerful computation environments: 1-the Turing computation environment, denoted by E T , and 2-a persistently evolutionary computation environment, denoted by E e , which persistently evolve in the course of executing the computations. We prove that the equality of complexity classes P and NP in the computation environment E e conflicts with the free will of the computist. We provide an axiomatic system T for Turing computability and prove that ignoring just one of the axiom of T , it would not be possible to derive P = NP from the rest of axioms. We prove that the computist who lives inside the environment E T , can never be confident that whether he lives in a static environment or a persistently evolutionary one.	axiomatic system;complexity class;computability;computable function;evolutionary computation;np (complexity);p (complexity);p versus np problem;real computation;time complexity;turing machine	Rasoul Ramezanian	2012	CoRR		discrete mathematics;computer science;theoretical computer science;mathematics;algorithm	Theory	-8.462373541162208	4.597278767531528	15636
17c4097bbed82ea0873f082ce243f908cc0c4f50	a semantics for probabilistic quantifier-free first-order languages, with particular application to story understanding	first order	"""We present a semantics for interpreting prob-abilistic statements expressed in a first-order quantifier-free language. We show how this semantics places constraints on the probabilities which can be associated with such statements. We then consider its use in the area of story understanding. We show that for at least simple models of stories (equivalent to the script/plan models) there arc ways to specify reasonably good probabilities. Lastly, we show that while the semantics dictates seemingly implausibly low prior probabilities for equality statements, once they are conditioned by an assumption of spatio-temporal locality of observation the probabilities become """"reasonable."""""""	first-order predicate;locality of reference;quantifier (logic)	Eugene Charniak;Robert P. Goldman	1989			natural language processing;computer science;artificial intelligence;first-order logic;data mining;mathematics;well-founded semantics;operational semantics;algorithm	AI	-14.45060451847344	7.3233318858426895	15637
9ba870a74a0143e9d26668e4d2c8b5ca4dd00b77	a powerful and sql-compatible data model and query language for olap	query language;formal algebra;current olap data model;automatic aggregation;sql-compatible data model;correct aggregation;seamless integration;external xml data;olap data;data model;sqlm olap data model;business-to-business electronic commerce;data integration;query languages;data models;olap	In this paper we present the SQL OLAP data model, formal algebra, and query language that, unlike current OLAP data models and languages, are both powerful, meaning that they support irregular dimension hierarchies, automatic aggregation of data, and correct aggregation of data, and SQL-compatible, allowing seamless integration with relational technology. We also consider the requirements to the data model posed by integration of OLAP data with external XML data. The concepts are illustrated with a real-world case study from the Business-toBusiness electronic commerce (B2B) domain.	avg;aggregate data;aggregate function;cube;data model;e-commerce;high- and low-level;mathematical optimization;max;online analytical processing;query language;query optimization;relational algebra;requirement;sql;seamless3d;xml	Dennis Pedersen;Karsten Riis;Torben Bach Pedersen	2002			e-commerce;data modeling;data model;online analytical processing;computer science;database model;data mining;database;logical data model;information retrieval;query language	DB	-31.703727217478196	9.550771473699973	15652
1f6496dd903c8cdd3dc3912f580725ddeb1110e6	residuation and guarded rules for constraint logic programming	constraint logic programming;guarded rule;constraint propagation	A major difficulty with logic programming is combinatorial explosion: since goals are solved with possibly indeterminate (Le., branching) reductions, the resulting search trees may grow wildly. Constraint logic programming systems try to avoid combinatorial explosion by building in strong determinate (Le., non-branching) reduction in the form of constraint simplification. In this paper we present two concepts, residuation and guarded rules, for further strengthening determinate reduction. Both concepts apply to constraint logic programming in general and yield an operational semantics that coincides with the declarative semantics. Residuation is a control strategy giving priority to determinate reductions. Guarded rules are logical consequences of programs adding otherwise unavailable determinate reductions.	constraint logic programming;control theory;indeterminacy in concurrent computation;level of detail;operational semantics;property (philosophy);residuated lattice	Gert Smolka	1991			constraint logic programming;concurrent constraint logic programming;mathematical optimization;constraint programming;discrete mathematics;constraint satisfaction;mathematics;logic programming;algorithm	PL	-16.33361327538426	20.257398866755533	15668
1bc286bd00eb4c415481e6eb4b3caadf791a3c83	quantified equilibrium logic and foundations for answer set programs	proof theory;answer set programming;first order;intuitionistic logic;classical logic;disjunctive logic programming	QHT is a first-order super-intuitionistic logic that provides afoundation for answer set programming (ASP) and a useful tool foranalysing and transforming non-ground programs. We recall someproperties of QHT and its nonmonotonic extension, quantifiedequilibrium logic (QEL). We show how the proof theory of QHT can beused to extend to non-ground programs previous results on thecompleteness of θ-subsumption. We also establish a reductionof QHT to classical logic and show how this can be used to obtainand extend classical encodings for concepts such as the strongequivalence of programs and theories. We pay special attention to aclass of general (disjunctive) logic programs that capture alluniversal theories in QEL.	stable model semantics	David Pearce;Agustín Valverde	2008		10.1007/978-3-540-89982-2_46	predicate logic;dynamic logic;classical logic;higher-order logic;stable model semantics;many-valued logic;intuitionistic logic;computer science;intermediate logic;artificial intelligence;answer set programming;proof theory;first-order logic;computational logic;programming language;logic programming;logic;algorithm;philosophy of logic	AI	-14.275599119684882	14.906922101106865	15678
fcfd04a833737e870593d13329b98c56cbe428d0	semantical vacuity detection in declarative process mining		A large share of the literature on process mining based on declarative process modeling languages, like DECLARE, relies on the notion of constraint activation to distinguish between the case in which a process execution recorded in event data “vacuously” satisfies a constraint, or satisfies the constraint in an “interesting way”. This fine-grained indicator is then used to decide whether a candidate constraint supported by the analyzed event log is indeed relevant or not. Unfortunately, this notion of relevance has never been formally defined, and all the proposals existing in the literature use ad-hoc definitions that are only applicable to a pre-defined set of constraint patterns. This makes existing declarative process mining technique inapplicable when the target constraint language is extensible and may contain formulae that go beyond pre-defined patterns. In this paper, we tackle this hot, open challenge and show how the notion of constraint activation and vacuous satisfaction can be captured semantically, in the case of constraints expressed in arbitrary temporal logics over finite traces. We then extend the standard automata-based approach so as to incorporate relevance-related information. We finally report on an implementation and experimentation of the approach that confirms the advantages and feasibility of our solution.	automata theory;automaton;conformance testing;declarative programming;first-order predicate;heuristic (computer science);hoc (programming language);modeling language;process modeling;relevance;temporal logic;tracing (software)	Fabrizio Maria Maggi;Marco Montali;Claudio Di Ciccio;Jan Mendling	2016		10.1007/978-3-319-45348-4_10	systems engineering;theoretical computer science;extensibility;computer science;process mining;process modeling	DB	-19.553969546005547	28.389607584418965	15693
95f852e902b7812a883e381afbc10e1c7c4efd89	a reverse engineering tool for precise class diagrams	class diagram;reverse engineering	Developers use class diagrams to describe the architecture of their programs intensively. Class diagrams represent the structure and global behaviour of programs. They show the programs classes and interfaces and their relationships of inheritance, instantiation, use, association, aggregation and composition. Class diagrams could provide useful data during programs maintenance. However, they often are obsolete and imprecise: They do not reflect the real implementation and behaviour of programs. We propose a reverse-engineering tool suite, Ptidej, to build precise class diagrams from Java programs, with respect to their implementation and behaviour. We describe static and dynamic models of Java programs and algorithms to analyse these models and to build class diagrams. In particular, we detail algorithms to infer use, association, aggregation, and composition relationships, because these relationships do not have precise definitions. We show that class diagrams obtained semi-automatically are similar to those obtained manually and more precise than those provided usually. Copyright c © 2004 Yann-Gaël Guéhéneuc. Permission to copy is hereby granted provided the original copyright notice is reproduced in copies made. This work has been partly funded by IBM OTI Labs – 2670 Queensview Drive – Ottawa, Ontario, K2B 8K1 – Canada.	algorithm;class diagram;java;object composition;reverse engineering;semiconductor industry;universal instantiation	Yann-Gaël Guéhéneuc	2004		10.1145/1034914.1034917	real-time computing;computer science;operating system;class diagram;database;programming language;story-driven modeling;reverse engineering	SE	-25.64885037864953	29.04136618203937	15746
1c700efa4c0683d3c3ef389309cbce84b518a2f9	the bernays-schönfinkel-ramsey class for set theory: semidecidability	first order logic;cumulant;satisfiability;set theory		bernays–schönfinkel class;set theory	Eugenio G. Omodeo;Alberto Policriti	2010	J. Symb. Log.		zermelo–fraenkel set theory;effective descriptive set theory;descriptive set theory;universal set;combinatorics;discrete mathematics;class;general set theory;internal set;first-order logic;decision rule;mathematics;elementary class;structure;enumeration;recursive set;infinite set;computable analysis;set theory;satisfiability	Theory	-8.075707568180595	12.946947249708876	15761
2970fadf6a253a84f18feb7ed489df65a5d202b0	lattices, closures systems and implication bases: a survey of structural aspects and algorithms	minimal generators;dependence graph;galois lattice;canonical direct basis;implicational system;concept lattice;closure operator;closed set lattice;closure system;canonical basis;lattice	Concept lattices and closed set lattices are graphs with the lattice property. They have been increasingly used this last decade in various domains of computer science, such as data mining, knowledge representation, databases or information retrieval. A fundamental result of lattice theory establishes that any lattice is the concept lattice of its binary table. A consequence is the existence of a bijective link between lattices, contexts (via the table) and a set of implicational rules (via the canonical (direct) basis). The possible transformations between these objects give rise to relevant tools for data analysis.#R##N##R##N#In this paper, we present a survey of lattice theory, from the algebraic definition of a lattice, to that of a concept lattice, through closure systems and implicational rules; including the exploration of fundamental bijective links between lattices, reduced contexts and bases of implicational rules; and concluding with the presentation of the main generation algorithms of these objects.	algorithm	Karell Bertet;Christophe Demko;Jean-François Viaud;Clément Guérin	2018	Theor. Comput. Sci.	10.1016/j.tcs.2016.11.021	distributive lattice;combinatorics;discrete mathematics;lattice model;standard basis;congruence lattice problem;pure mathematics;lattice;mathematics;complemented lattice;free lattice;lattice;map of lattices;lattice miner;algorithm;lattice problem;closure operator;algebra	Theory	-6.17250059552964	16.240159621029644	15766
256b626c15ee4acd7ce74efcb9ca108d617294ba	constraint based mining of first order sequences in seqlog	first order;background knowledge	A logical language, SeqLog, for mining and querying sequential data and databases is presented. In SeqLog, data takes the form of a sequence of logical atoms, background knowledge can be specified using Datalog style clauses and sequential queries or patterns correspond to subsequences of logical atoms. SeqLog is then used as the representation language for the inductive database mining system MineSeqLog. Inductive queries in MineSeqLog take the form of a conjunction of a monotonic and an anti-monotonic constraint on sequential patterns. Given such an inductive query, MineSeqLog will compute the borders of the solution space. MineSeqLog uses variants of the famous level-wise algorithm together with ideas from version spaces to realize this. Finally, we report on a number of experiments in the domains of user-modelling that validate the approach.	algorithm;constraint logic programming;data mining;database;datalog;durchmusterung;experiment;feasible region;heikki mannila;hidden markov model;inductive logic programming;inductive reasoning;kramer graph;markov chain;programming language;refinement (computing);structure mining;subsumption architecture;universal instantiation;version space learning	Sau Dan Lee;Luc De Raedt	2002		10.1007/978-3-540-44497-8_8	theoretical computer science;data mining;mathematics;algorithm	DB	-21.062532960022587	12.623983443668386	15777
10c1a1f9d9d1d0f0c93c914a1bcac843a5b9f717	a sound and complete axiomatization of delimited continuations	axiomatization;delimited continuation;complete axiomatization;cps translation;continuation;power control	The shift and reset operators, proposed by Danvy and Filinski, are powerful control primitives for capturing delimited continuations. Delimited continuation is a similar concept as the standard (unlimited) continuation, but it represents part of the rest of the computation, rather than the whole rest of computation. In the literature, the semantics of shift and reset has been given by a CPS-translation only. This paper gives a direct axiomatization of calculus with shift and reset, namely, we introduce a set of equations, and prove that it is sound and complete with respect to the CPS-translation. We also introduce a calculus with control operators which is as expressive as the calculus with shift and reset, has a sound and complete axiomatization, and is conservative over Sabry and Felleisen's theory for first-class continuations.	axiomatic system;computation;delimited continuation;delimiter	Yukiyoshi Kameyama;Masahito Hasegawa	2003		10.1145/944705.944722	delimited continuation;power control;computer science;continuation;programming language;algorithm	PL	-12.562368482860617	17.496510404190282	15789
a880fb8c989a54ab9439a9cf34de605ab6986c07	classical logic, storage operators and second-order lambda-calculus	second order;lambda calculus;classical logic	We describe here a simple method in order to obtain programs from proofs in second-order classical logic. Then we extend to classical logic the results about storage operators (typed λ-terms which simulate call-by-value in call-by-name) proved by Krivine (1990) for intuitionistic logic. This work generalizes previous results of Parigot (1992).	sequent calculus;system f	Jean-Louis Krivine	1994	Ann. Pure Appl. Logic	10.1016/0168-0072(94)90047-7	dynamic logic;zeroth-order logic;typed lambda calculus;discrete mathematics;classical logic;binary lambda calculus;higher-order logic;many-valued logic;intuitionistic logic;intermediate logic;lambda calculus;simply typed lambda calculus;mathematics;proof calculus;minimal logic;situation calculus;curry–howard correspondence;programming language;church encoding;natural deduction;substructural logic;second-order logic;algorithm;algebra	Logic	-12.743571993523343	14.99442292995218	15813
51509fa5a642f982392dd5bd92a9146e0d644fd3	using a network of workstations to enhance database query processing performance	tratamiento paralelo;tiempo respuesta;reponse temporelle;database system;evaluation performance;performance evaluation;traitement parallele;query processing;information retrieval;evaluacion prestacion;database query processing;machine parallele;interrogation base donnee;interrogacion base datos;response time;temps reponse;internet;time response;recherche information;network of workstation;parallel machines;recuperacion informacion;systeme gestion base donnee;respuesta temporal;sistema gestion base datos;database management system;database query;parallel processing	Query processing in database systems may be improved by applying parallel processing techniques. One reason for improving query response time is to support the increased number queries when databases are made accessible	central processing unit;computational resource;computer cluster;database;esa;load balancing (computing);paging;parallel virtual machine;parallel computing;parallel database;response time (technology);scalability;shared nothing architecture;workstation	Mohammed Al Haddad;Jerome Robinson	2001		10.1007/3-540-45417-9_48	online aggregation;parallel processing;sargable;query optimization;query expansion;web query classification;the internet;database tuning;computer science;query by example;data mining;database;rdf query language;web search query;view;world wide web;response time;alias;query language;spatial query	DB	-28.001345101944267	5.854792973115766	15904
12e58a3a8376b45b92d48a81ad4ac7f4cd1fb8a3	specifying the odp trader: an introduction to e-lotos	odp trader	Details about the next meeting are still undecided. It is likely to happen between June and July 1997 as a WG7 plenary. MODULES It is agreed that the basis of modularity may be conceptualised with the instructions : « export SIGNATURE » and « import MODULE, include OBJECTS, exclude OBJECTS». There is a general agreement on the desired modules characteristics: a) module has by default a complete exportation signature b) importation of modules supports multiple views c) nested definitions of modules are not allowed d) generic modules will have a light syntax e) actualisation will have a light syntax f) renaming will exist in module actualisation and «duplication» g) sharing in generic operations is discarded It is agreed that the CD will incorporate modularity based on GR2.duplication built-in renaming (new) operator alias ? built-in (e.g. T ≡ T' iff T⊆T' AND T ⊇T') A choice needs to be made on a subset of characteristics of the language ; a solution providing all features simultaneously not seeming to exist : SET #1 SET #2 ============== ============= no overloading overloading dot notation flat names sub-typing (global) sub-typing (static) anonymous records extensible unions alias structural equality Set#1 is generally (but not unanimously) preferred. French and Romanian experts express a strong concern about the decision of having general sub-typing which will require run-time type checking and will result in complexity and loss of performance for implementations. A working plenary should be held around June-July of 1997. Thus, February should be a target date for the submission of a CD to ballot. A CD should be produced as soon as possible. It is generally acknowledged that the objectives set in Kansas City meeting have not all been met , although a very good part of them have, not to say most of them. (detailed discussion is postponed until the end of the meeting since it is foreseen that many of the highlighted topics are likely to be tackled before the end any way.) N1 : lack of integration == lack of convergence in KC, plus the lack of time for better coordination and collaboration N2 : structure of document is not aligned w.r.t. the format planned in KC N3 : extra syntactic sugar in core is meant for help, suggestions of enhancements The user and core level languages should be unified and duplication eliminated.	cd-rom;canonical account;e-lotos;emoticon;function overloading;rm-odp;signature;trader media east;type system	Giovanny F. Lucero;Juan Quemada	1997			operator (computer programming);e-lotos;theoretical computer science;typing;exception handling;synchronization;computer science;composition operator	PL	-25.245175312838	27.187122738539852	16005
79077cc788afd41292b8948abdedfc94534fd4dc	experiences with gentle: efficient compiler construction based on logic programming	compiler construction;two level grammar;logic programs	 Gentle [Schroer 89] is a compiler description languagein the tradition of two level grammars [Koster 71] andlogic programming [Warren 80]. It provides a commondeclarative notation for high level descriptionof analysis, transformation, and synthesis. Imperativeconstructs like global variables and dynamic arrays,needed for efficient compiler construction, areintroduced as well. A tool has been implemented tocheck the wellformedness of Gentle descriptions, andto generate very fast... 	compiler;gentle;logic programming	Jürgen Vollmer	1991		10.1007/3-540-54444-5_120	computer architecture;compiler;compiler correctness;interprocedural optimization;computer science;compiler construction;bootstrapping;programming language;attribute grammar;logic programming;inline expansion;intrinsic function;functional compiler;algorithm	EDA	-24.624787724079784	23.38896598331555	16009
99065b415b51e91a70d4f155b4265b9f3c7cc73c	leveraging aggregate constraints for deduplication	search space;constraint satisfaction;deduplication;entity resolution	We show that aggregate constraints (as opposed to pairwise constraints) that often arise when integrating multiple sources of data, can be leveraged to enhance the quality of deduplication. However, despite its appeal, we show that the problem is challenging, both semantically and computationally. We define a restricted search space for deduplication that is intuitive in our context and we solve the problem optimally for the restricted space. Our experiments on real data show that incorporating aggregate constraints significantly enhances the accuracy of deduplication.	aggregate data;aggregate function;constraint satisfaction problem;data deduplication;expectation–maximization algorithm;experiment	Surajit Chaudhuri;Anish Das Sarma;Venkatesh Ganti;Raghav Kaushik	2007		10.1145/1247480.1247530	data deduplication;constraint satisfaction;name resolution;computer science;theoretical computer science;data mining;database;programming language	DB	-24.95955073705896	4.579327065432918	16051
5374b2f4c560d159a8fdc835febf8c8635e3f9c3	modal definability in languages with a finite number of propositional variables and a new extension of the sahlqvist's class			modal logic;propositional calculus	Dimiter Vakarelov	2002			modal;discrete mathematics;algorithm;propositional variable;finite set;mathematics	Logic	-12.002281654243934	13.054848984977852	16065
69acd85a049fe71382827e5d450ed14266c7f046	computing argumentation in polynomial number of bdd operations: a preliminary report	input sentence;exponential complexity;binary decision diagram;argumentation process;bdd operation;preliminary report;argumentation-based reasoning;polynomial number;computing argumentation;argumentation theory	Many advances in argumentation theory have been made, but the exponential complexity of argumentation-based reasoning has made it impr actical to apply argumentation theory. In this paper, we propose a binary decisio n diagram (BDD) approach to argumentation-based reasoning. In the appr oach, sets of arguments and defeats are encoded into BDDs so that an argumentation p rocess can work on a set of arguments and defeats simultaneously in one BDD op eration. As a result, the argumentation can be computed in polynomial number o f BDD operations on the number of input sentences.	computation;diagram;experiment;fixed point (mathematics);heuristic (computer science);iterative method;model checking;numerical integration;polynomial;time complexity	Yuqing Tang;Timothy J. Norman;Simon Parsons	2010		10.1007/978-3-642-21940-5_16	theoretical computer science;mathematics;probabilistic argumentation;algorithm	AI	-15.262167255786359	14.717174378627362	16086
07f4500f148255beef6540a45b7105fc9ad58f49	a little knowledge goes a long way: simple knowledge-based derivations and correctness proofs for a family of protocols	transmission problem;correctness proof;knowledge base	A high-level, knowledge-based approach for deriving a family of protocols for the sequence transmission problem is presented. The protocols of Aho et al. [2, 3], the Alternating Bit protocol [5], and Stenning’s protocol [44] are all instances of one knowledge-based protocol that is derived. The derivation in this paper leads to transparent and uniform correctness proofs for all these protocols.	alternating bit protocol;correctness (computer science);high- and low-level;knowledge-based systems	Joseph Y. Halpern	1987		10.1145/41840.41863	correctness;knowledge base;discrete mathematics;computer science;mathematics;algorithm	Theory	-18.07118791993821	26.349487389618652	16147
6cfe09aa6e5da6ce98077b7a048cb1badd78cc76	interaction combinators	interaction combinators	It is shown that a very simple system of interaction com-binators, with only 3 symbols and 6 rules, is a universal model of distributed computation, in a sense that will be made precise. This paper is the continuation of the au-thor's work on interaction nets, inspired by Girard's proof nets for linear logic, but no preliminary knowledge of these topics is required for its reading.	combinatory logic;computation;continuation;distributed computing;linear logic	Yves Lafont	1997	Inf. Comput.	10.1006/inco.1997.2643	computer science;theoretical computer science;mathematics;programming language;algorithm	Logic	-7.1616242787288655	22.025266528550738	16154
2654836539eefc08dea254837ea49974e813533e	automatic verification of tla + proof obligations with smt solvers	automatic verification;elementary set theory;proof obligation;smt solvers;proof system tlaps;actions tla;new backend;certain interactive proof;corresponding proof obligation;proof language;proof manager	TLA is a formal specification language that is based on ZF set theory and the Temporal Logic of Actions TLA. The TLA proof system tlaps assists users in deductively verifying safety properties of TLA specifications. tlaps is built around a proof manager, which interprets the TLA proof language, generates corresponding proof obligations, and passes them to backend verifiers. In this paper we present a new backend for use with SMT solvers that supports elementary set theory, functions, arithmetic, tuples, and records. Type information required by the solvers is provided by a typing discipline for TLA proof obligations, which helps us disambiguate the translation of expressions of (untyped) TLA, while ensuring its soundness. Preliminary results show that the backend can help to significantly increase the degree of automation of certain interactive proofs.	algorithm;axiomatic system;combinatory logic;damien doligez;first-order logic;first-order predicate;formal specification;front and back ends;interactive proof system;isabelle;leslie speaker;preprocessor;proof calculus;simultaneous multithreading;solver;specification language;tla+;temporal logic of actions;type inference;type system;verification and validation;zermelo–fraenkel set theory	Stephan Merz;Hernán Vanzetto	2012		10.1007/978-3-642-28717-6_23	computer science;theoretical computer science;programming language;algorithm	Logic	-19.95716785677433	25.82924232518529	16159
a13563898c3be1019e59b05f37c20002d62fa383	operational semantics for positive r	semantica formal;logique mathematique;operational semantics;logica matematica;theorie modeles;formal semantics;mathematical logic;semantique formelle;teoria modelos;model theory	/ Two kinds of formal semantics for intensional logics It is convenient to begin with a few remarks about the distinction between model-theoretic (sometimes called 'set-theoretic') semantics and algebraic semantics for sentential logics containing non-truth-functional connectives. Both issue in definitions of validity on a structure or over a class of structures in terms of which completeness theorems are sought, to the effect that provability in this or that logic coincides with a certain such notion of validity. I take the hallmark of the model-theoretic approach to be that it characterizes the validity notion in question via an inductively defined notion of truth of a formula at a point in a model, while the algebraic approach features no such intermediate level of description. 1	operational semantics	Lloyd Humberstone	1988	Notre Dame Journal of Formal Logic	10.1305/ndjfl/1093637771	mathematical logic;discrete mathematics;classical logic;formal semantics;game semantics;proof-theoretic semantics;formal semantics;mathematics;programming language;well-founded semantics;operational semantics;denotational semantics;algorithm;model theory;algebra	Logic	-10.507722774683382	12.618586981874959	16213
003134b5c37169c904e571ede1eeaa584073a7f7	on denotational semantics of spatial-temporal consistency language -- stec	railways;high speed train denotational semantics spatial temporal consistency language stec cyber physical systems specification language abstract theorem china gaotie;semantics bismuth real time systems syntactics educational institutions electronic mail algebra;spatial temporal consistency;formal semantics;formal semantics cyber physical systems spatial temporal consistency specification language;cyber physical systems;specification languages railways;specification language;specification languages	In order to describe the requirement of spatial and temporal consistency of cyber-physical systems, a specification language called as STeC was proposed by Chen in [1]. In this paper, we focus on the theory of semantics of STeC. After simply restating the syntax and operational semantics, we mainly establish the denotational semantics of STeC. To investigate the reasonability of the denotational semantics, an abstract theorem is given to show the soundness and completeness of the denotational semantics. Finally, a simple case about China Gaotie (which means High-speed train) is given to show how to compute the operational and denotational semantics.	cyber-physical system;denotational semantics;entity–relationship model;operational semantics;specification language	Hengyang Wu;Yixiang Chen;Min Zhang	2013	2013 International Symposium on Theoretical Aspects of Software Engineering	10.1109/TASE.2013.16	normalisation by evaluation;formal semantics;action semantics;specification language;failure semantics;computer science;theoretical computer science;domain theory;formal semantics;programming language;denotational semantics of the actor model;cyber-physical system;operational semantics;denotational semantics;algorithm;language of temporal ordering specification;computational semantics	Logic	-15.517623125402404	11.775136931163907	16215
0303eed3e71257a8376f1979ed527cb0f52b3a09	correct blame for contracts: no more scapegoating	debugging;puesta a punto programa;fiabilidad;reliability;behavioral analysis;semantics;lambda calculus;program verification;contrato;satisfiability;semantica;semantique;higher order;debogage;blame assignment;verificacion programa;contract;fiabilite;analyse comportementale;lambda calculo;design;analisis conductual;contrat;higher order functions;behavioral contracts;verification programme;lambda calcul;languages;higher order programming	Behavioral software contracts supplement interface information with logical assertions. A rigorous enforcement of contracts provides useful feedback to developers if it signals contract violations as soon as they occur and if it assigns blame to violators with preciseexplanations. Correct blame assignment gets programmers started with the debugging process and can significantly decrease the time needed to discover and fix bugs.  Sadly the literature on contracts lacks a framework for making statements about the correctness of blame assignment and for validating such statements. This paper fills the gap and uses the framework to demonstrate how one of the proposed semantics for higher-order contracts satisfies this criteria and another semantics occasionally assigns blame to the wrong module.  Concretely, the paper applies the framework to the lax enforcement of dependent higher-order contracts and the picky one. A higher-order dependent contract specifies constraints for the domain and range of higher-order functions and also relates arguments and results in auxiliary assertions. The picky semantics ensures that the use of arguments in the auxiliary assertion satisfies the domain contracts and the lax one does not. While the picky semantics discovers more contract violations than the lax one, it occasionally blames the wrong module. Hence the paper also introduces a third semantics, dubbed indy, which fixes the problems of the picky semantics without giving up its advantages.	assertion (software development);correctness (computer science);debugging;higher-order function;indy (software);programmer;software bug	Christos Dimoulas;Robert Bruce Findler;Cormac Flanagan;Matthias Felleisen	2011		10.1145/1926385.1926410	contract;design;real-time computing;computer science;lambda calculus;reliability;semantics;programming language;debugging;computer security;algorithm;satisfiability	PL	-22.845494648529183	29.155323206024693	16246
a6dc53bee48d3afae070fed76f33b13d149f337b	a comparison of languages which operationalize and formalize kads models of expertise	article in monograph or in proceedings	In the field of Knowledge Engineering, dissatisfaction with the rapid-prototyping approach has led to a number of more principled methodologies for the construction of knowledgebased systems. Instead of immediately implementing the gathered and interpreted knowledge in a given implementation formalism according to the rapid-prototyping approach, many such methodologies centre around the notion of a conceptual model: an abstract, implementation independent description of the relevant problem solving expertise. A conceptual model should describe the task which is solved by the system and the knowledge which is required by it. Although such conceptual models have often been formulated in an informal way, recent years have seen the advent of formal and operational languages to describe such conceptual models more precisely, and operationally as a means for model evaluation. In this paper, we study a number of such formal and operational languages for specifying conceptual models. In order to enable a meaningful comparison of such languages, we focus on languages which are all aimed at the same underlying conceptual model, namely that from the KADS method for building KBS. We describe eight formal languages for KADS models of expertise, and compare these languages with respect to their modelling primitives, their semantics, their implementations and their applications. Future research issues in the area of formal and operational specification languages for KBS are identified as the result of studying these languages. The paper also contains an extensive bibliography of research in this area. Appeared in: The Knowledge Engineering Review, 9(2), 1994. Language Comparison 2	formal language;kad network;knowledge engineering;knowledge-based systems;operational semantics;problem solving;rapid application development;rapid prototyping;semantics (computer science);specification language	Dieter Fensel;Frank van Harmelen	1994	Knowledge Eng. Review	10.1017/S0269888900006767	natural language processing;computer science;artificial intelligence;ontology language	AI	-22.644222076102015	9.646900727853435	16264
8198b157f9d69c710341d9951ae39415ad824ee8	timed-automata-based verification of mitl over signals		It has been argued that the most suitable semantic model for real-time formalisms is the nonnegative real line (signals), i.e. the continuous semantics, which naturally captures the continuous evolution of system states. Existing tools like Uppaal are, however, based on ω-sequences with timestamps (timed words), i.e. the pointwise semantics. Furthermore, the support for logic formalisms is very limited in these tools. In this article, we amend these issues by a compositional translation from Metric Temporal Interval Logic (MITL) to signal automata. Combined with an emptiness-preserving encoding of signal automata into timed automata, we obtain a practical automata-based approach to MITL model-checking over signals. We implement the translation in our tool MightyL and report on case studies using LTSmin as the back-end. 1998 ACM Subject Classification F.4.1 Mathematical Logic, F.1.1 Models of Computation	automata theory;computation;formal verification;interval temporal logic;ltsmin;model checking;real-time locating system;timed automaton;uppaal	Thomas Brihaye;Gilles Geeraerts;Hsi-Ming Ho;Benjamin Monmege	2017		10.4230/LIPIcs.TIME.2017.7	theoretical computer science;semantic data model;real line;timestamp;encoding (memory);rotation formalisms in three dimensions;computer science;semantics;automaton;pointwise	Logic	-11.381953754858188	25.29435343451122	16291
a08bb5d477472394ae556c3d6fad283e920c3af8	what's in common between test, model checking, and decision procedures?	model checking;decision procedure;proof search	The interaction of proof search and counterexample search is at the heart of recent methods in decision procedures model checking. In this talk, we'll consider the relation between these methods and test.	model checking	Kenneth L. McMillan	2009		10.1007/978-3-642-04570-7_4	model checking;computer science;automated proof checking;programming language;algorithm	SE	-19.423869887195767	19.515077230632723	16305
044db4751818a4cfe097d0d0f69842f6d650f101	responsibility for inconsistency	responsibility;causality models;minimal inconsistent subsets;minimal correction subsets;inconsistency	Abstract It is desirable to identify the degree of responsibility of each part of a knowledge base for the inconsistency of that base to make some necessary trade-off decisions on restoring the consistency of that base. In this paper, we propose a measurement for the degree of responsibility of each formula in a knowledge base for the inconsistency of that base. This measurement is given in terms of minimal inconsistent subsets of a knowledge base. Moreover, it can be well explained in the context of causality and responsibility presented by Chockler and Halpern [1] .	algorithm;causality;counterfactual conditional;knowledge base;requirements engineering	Kedian Mu	2015	Int. J. Approx. Reasoning	10.1016/j.ijar.2015.04.007	discrete mathematics;mathematics	AI	-17.295151763974005	6.764740782880684	16313
503ea6bb520235afd19f07305d42ba152e5286fa	a brief survey of quantum programming languages	lenguaje programacion;programming language;logical programming;functional programming;programmation logique;langage programmation;programmation fonctionnelle;programacion logica;programacion funcional	This article is a brief and subjective survey of quantum programming language research. 1 Quantum Computation Quantum computing is a relatively young subject. It has its beginnings in 1982, when Paul Benioff and Richard Feynman independently pointed out that a quantum mechanical system can be used to perform computations [11, p.12]. Feynman’s interest in quantum computation was motivated by the fact that it is computationally very expensive to simulate quantum physical systems on classical computers. This is due to the fact that such simulation involves the manipulation is extremely large matrices (whose dimension is exponential in the size of the quantum system being simulated). Feynman conceived of quantum computers as a means of simulating nature much more efficiently. The evidence to this day is that quantum computers can indeed perform certain tasks more efficiently than classical computers. Perhaps the best-known example is Shor’s factoring algorithm, by which a quantum computer can find the prime factors of any integer in probabilistic polynomial time [15]. There is no known classical probabilistic algorithm which can solve this problem in polynomial time. In the ten years since the publication of Shor’s result, there has been an enormous surge of research in quantum algorithms and quantum complexity theory. 2 Quantum Programming Languages Quantum physics involves phenomena, such as superposition and entanglement, whose properties are not always intuitive. These same phenomena give quantum computation its power, and are often at the heart of an interesting quantum algorithm. However, there does not yet seem to be a unifying set of principles by which quantum algorithms are developed; each new algorithm seems to rely on a unique set of “tricks” to achieve its particular goal. One of the goals of programming language design is to identify and promote useful “high-level” concepts — abstractions or paradigms which allow humans 2 to think about a problem in a conceptual way, rather than focusing on the details of its implementation. With respect to quantum programming, it is not yet clear what a useful set of abstractions would be. But the study of quantum programming languages provides a setting in which one can explore possible language features and test their usefulness and expressivity. Moreover, the definition of prototypical programming languages creates a unifying formal framework in which to view and analyze existing quantum algorithm. 2.1 Virtual Hardware Models Advances in programming languages are often driven by advances in compiler design, and vice versa. In the case of quantum computation, the situation is complicated by the fact that no practical quantum hardware exists yet, and not much is known about the detailed architecture of any future quantum hardware. To be able to speak of “implementations”, it is therefore necessary to fix some particular, “virtual” hardware model to work with. Here, it is understood that future quantum hardware may differ considerably, but the differences should ideally be transparent to programmers and should be handled automatically by the compiler or operating system. There are several possible virtual hardware models to work with, but fortunately all of them are equivalent, at least in theory. Thus, one may pick the model which fits one’s computational intuitions most closely. Perhaps the most popular virtual hardware model, and one of the easiest to explain, is the quantum circuit model. Here, a quantum circuit is made up from quantum gates in much the same way as a classical logic circuit is made up from logic gates. The difference is that quantum gates are always reversible, and they correspond to unitary transformations over a complex vector space. See e.g. [3] for a succinct introduction to quantum circuits. Of the two basic quantum operations, unitary transformations and measurements, the quantum circuit model emphasizes the former, with measurements always carried out as the very last step in a computation. Another virtual hardware model, and one which is perhaps even better suited for the interpretation of quantum programming languages, is the QRAM model of Knill [9]. Unlike the quantum circuit model, the QRAM models allows unitary transformations and measurements to be freely interleaved. In the QRAM model, a quantum device is controlled by a universal classical computer. The quantum device contains a large, but finite number of individually addressable quantum bits, much like a RAM memory chip contains a multitude of classical bits. The classical controller sends a sequence of instructions, which are either of the form “apply unitary transformation U to qubits i and j” or “measure qubit i”. The quantum device carries out these instruction, and responds by making the results of the measurements available. A third virtual hardware model, which is sometimes used in complexity theory, is the quantum Turing machine. Here, measurements are never performed, and the entire operation of the machine, which consists of a tape, head, and finite control, is assumed to be unitary. While this model is theoretically equivalent	apply;compiler;computation;computational complexity theory;computer;fits;high- and low-level;integer factorization;logic gate;operating system;pp (complexity);polynomial;programmer;programming language theory;quantum turing machine;quantum algorithm;quantum circuit;quantum complexity theory;quantum computing;quantum entanglement;quantum gate;quantum indeterminacy;quantum mechanics;quantum programming;quantum superposition;quantum system;qubit;randomized algorithm;shor's algorithm;simulation;time complexity;virtual machine	Peter Selinger	2004		10.1007/978-3-540-24754-8_1	computer science;artificial intelligence;programming language;functional programming;algorithm	Theory	-20.626766726057948	14.132162424264632	16335
a8adc6a284e5d650abc47f34d9ea5b752ca28364	a web-based management system for h.264 live video broadcasting	introductory calculus education;mathematics computing;3d computer graphics;3d visualization;college students;computer graphic;introductory calculus class;data visualisation;virtual touch;computer haptics;visual impairment;calculus visualization haptic interfaces equations shape computer graphics computer science education educational institutions computational modeling computer simulation;mathematical equation;3d mathematical shape;mathematics computing courseware data visualisation haptic interfaces;mathematical equation introductory calculus education 3d visualization virtual touch 3d computer graphics introductory calculus class math software 3d mathematical shape computer haptics;haptic interfaces;math software;courseware;multi modal interaction	This paper presents a Web-based management system for H.264 live video broadcasting. The system consists of three functional parts, namely, the H.264 streaming control, the program management and the media management. It is created to remotely control and operate a P2P live video broadcasting overlay network using H.264 as the video compression codec. An extended MVC model with a newly created distributed service layer is designed and adopted in the system architecture to meet the demands for the scalability of multiple live video sources control and distributed video storage. The implementation of the system integrates many open source projects to make the system small in size, universal accessible, highly scalable, highly efficient and highly portable. Struts is used as the basic Web application framework, Hibernate and MySql are used for the efficient persistent data storage and RMI is used in the video control to facilitate distributed video collection. Those technologies and the scalability of the architecture make the system applicable for being deployed on low-cost PC servers or PC clusters.	apache struts;application framework;codec;computer cluster;computer data storage;h.264/mpeg-4 avc;management system;mysql;open-source software;overlay network;scalability;service layer;streaming media;systems architecture;web application	Yin Zhou;Xiaoyu Hua;Hui Xao	2007	Second International Multi-Symposiums on Computer and Computational Sciences (IMSCCS 2007)	10.1109/IMSCCS.2007.48	human–computer interaction;computer science;multimedia	OS	-32.69102229484124	17.051022276729206	16343
12b7d6c29bb99dcb59915a14a3d2fe360c9b9be9	mining association rules from xml data	extraction information;base relacional dato;extensible markup language;analisis datos;information extraction;xml language;relational database;data mining;association rule mining;data analysis;regle association;association rule;fouille donnee;decouverte connaissance;base donnee relationnelle;xml document;descubrimiento conocimiento;analyse donnee;busca dato;extraccion informacion;langage xml;lenguaje xml;knowledge discovery	The eXtensible Markup Language (XML) rapidly emerged as a standard for representing and exchanging information. The fastgrowing amount of available XML data sets a pressing need for languages and tools to manage collections of XML documents, as well as to mine interesting information out of them. Although the data mining community has not yet rushed into the use of XML, there have been some proposals to exploit XML. However, in practice these proposals mainly rely on more or less traditional relational databases with an XML interface. In this paper, we introduce association rules from native XML documents and discuss the new challenges and opportunities that this topic sets to the data mining community. More specifically, we introduce an extension of XQuery for mining association rules. This extension is used throughout the paper to better define association rule mining within XML and to emphasize its implications in the XML context.	association rule learning;data mining;markup language;relational database;xml;xquery	Daniele Braga;Alessandro Campi;Mika Klemettinen;Pier Luca Lanzi	2002		10.1007/3-540-46145-0_3	xml catalog;xml validation;binary xml;xml encryption;xml base;simple api for xml;xml;xml schema;streaming xml;computer science;document type definition;document structure description;xml framework;data mining;xml database;xml schema;database;xml signature;xml schema editor;cxml;information extraction;information retrieval;efficient xml interchange;sgml	DB	-33.14412892401431	6.754297375537176	16436
c1bc42595d14ab90cf7e7b269e0646e079ae7625	a fragment of first order logic adequate for observation equivalence	observational equivalence;first order logic	We present a logical characterization of the Milneru0027s notion of observation equivalence of processes (“at most one observable action at a time” variant) by using a restricted class of first order formulas. We use the game technique due to Ehrenfeucht as a means to achieve this characterization. First we extend the Ehrenfeucht game by introducing a pair of compatibility relations as a parameter to the game so that we can restrict the moves of the players on the basis of the previous moves. We then define the logic which corresponds to the extended game. Second we characterize the observation equivalence on a restricted class of labelled transition systems (with τ moves), called trace-unique labelled transition systems (t-τltsu0027s), as the equivalence induced by the games played on certain rcducts of the given t-τltsu0027s where the compatibility relations are defined in terms of bounded reachability in the t-τltsu0027s. Combining these two characterizations we get our main result.	first-order logic;turing completeness	Halit Oguztüzün	1991		10.1007/BFb0023774	discrete mathematics;computer science;first-order logic;mathematics;programming language;algorithm	Logic	-10.487192839180826	21.456555181878837	16458
c71534f38aefebc38a4800ef0bd56146f9161edf	automata-theoretic decision of timed games	temporal logic;tree automata;timed games	The solution of games is a key decision problem in the context of verification of open systems and program synthesis. Given a game graph and a specification, we wish to determine if there exists a strategy of the protagonist that allows to select only behaviors fulfilling the specification. In this paper, we consider timed games, where the game graph is a timed automaton and the specification is given by formulas of the temporal logics Ltl and Ctl. We present an automata-theoretic approach to solve the addressed games, extending to the timed framework a successful approach to solve discrete games. The main idea of this approach is to translate the timed automaton A, modeling the game graph, into a tree automaton AT accepting all trees that correspond to a strategy of the protagonist. Then, given an automaton corresponding to the specification, we intersect it with the tree automaton AT and check for the nonemptiness of the resulting automaton. Our approach yields a decision algorithm running in exponential time for Ctl and in double exponential time for Ltl. The obtained algorithms are optimal in the sense that their computational complexity matches the known lower bounds.	2-exptime;algorithm;computational complexity theory;decision problem;exptime;program synthesis;reachability;temporal logic;time complexity;timed automaton;tree automaton	Marco Faella;Salvatore La Torre;Aniello Murano	2014	Theor. Comput. Sci.	10.1016/j.tcs.2013.08.021	discrete mathematics;büchi automaton;temporal logic;computer science;theoretical computer science;two-way deterministic finite automaton;continuous automaton;deterministic automaton;timed automaton;pushdown automaton;algorithm	Logic	-6.990639255025274	23.976016667089894	16459
3fe13be606d9c01b73794d69747fdb5d9bc31ef2	deriving a correct logic program from the formal specification of a non-linear planner	formal specification		formal specification;logic programming	T. L. McCluskey	1988			formal specification;dynamic logic (modal logic);formal methods;language of temporal ordering specification;refinement;signature (logic);proof calculus;algorithm;computer science;formal verification	SE	-19.082228819763113	19.857584121889	16476
73163d1813bab3f9bcd866588224fe633e885180	on definition of a formal model for iec 61499 function blocks	signal image and speech processing;circuits and systems;control structures and microprogramming;formal model;function block;journal article;electronic circuits and devices	Formal model of IEC 61499 syntax and its unambiguous execution semantics are important for adoption of this international standard in industry. This paper proposes some elements of such a model. Elements of IEC 61499 architecture are defined in a formal way following set theory notation. Based on this description, formal semantics of IEC 61499 can be defined. An example is shown in this paper for execution of basic function blocks. The paper also provides a solution for flattening hierarchical function block networks.	formal language;loss function;programming language;prolog;semantics (computer science);set theory	Victor Dubinin;Valeriy Vyatkin	2008	EURASIP J. Emb. Sys.	10.1155/2008/426713	embedded system;computer science;theoretical computer science;operating system;programming language;algorithm	DB	-27.785570300637342	21.18875162219877	16486
471abe14b430bb94ac9a4c22eb31ee5c6fe52be8	signals and comonads	distributive law;semantic description;stream function	We propose a novel discipline for programming stream functions and for the semantic description of stream manipulation languages based on the observation that both general and causal stream functions can be characterized as coKleisli arrows of comonads. This seems to be a promising application for the old, but very little exploited idea that if monads abstract notions of computation of a value, comonads ought to be useable as an abstraction of notions of value in a context. We also show that causal partial-stream functions can be described in terms of a combination of a comonad and a monad.	causal filter;clock rate;comment (computer programming);computation;dataflow programming;intensional logic;lucid;lustre;monad (functional programming);programming paradigm;usability	Tarmo Uustalu;Varmo Vene	2005	J. UCS	10.3217/jucs-011-07-1311	distributive property;theoretical computer science;stream function;algorithm	AI	-22.35613275115428	22.733652966158065	16491
5a99344904f23a954c7849344e5428b26028ecda	bounded linear-time temporal logic: a proof-theoretic investigation	temporal logic;sequent calculus;modal logic;resolution	Abstract   Propositional and first-order bounded linear-time temporal logics (BLTL and FBLTL, respectively) are introduced by restricting Gentzen type sequent calculi for linear-time temporal logics. The corresponding Robinson type resolution calculi, RC and FRC for BLTL and FBLTL respectively are obtained. To prove the equivalence between FRC and FBLTL, a temporal version of Herbrand theorem is used. The completeness theorems for BLTL and FBLTL are proved for simple semantics with both a bounded time domain and some bounded valuation conditions on temporal operators. The cut-elimination theorems for BLTL and FBLTL are also proved using some theorems for embedding BLTL and FBLTL into propositional (first-order, respectively) classical logic. Although FBLTL is undecidable, its monadic fragment is shown to be decidable.	linear temporal logic;theory	Norihiro Kamide	2012	Ann. Pure Appl. Logic	10.1016/j.apal.2011.12.002	discrete mathematics;linear temporal logic;mathematics;algorithm	Logic	-12.0421260045135	14.572778180630406	16517
4ef3d9c5a7e8215ce7009787d7ed8a015d767a21	conditional logics of belief change	conditional logic;belief revision;belief change	The study of belief change has been an active area in philosophy and AI. In recent years two special cases of belief change, belief revision and belief update, have been studied in detail. Belief revision and update are clearly not the only possible notions of belief change. In this paper we investigate properties of a range of possible belief change operations. We start with an abstract notion of a belief change system and provide a logical language that describes belief change in such systems. We then consider several reasonable properties one can impose on such systems and characterize them axiomatically. We show that both belief revision and update fit into our classification. As a consequence, we get both a semantic and an axiomatic (proof-theoretic) characterization of belief revision and update (as well as some belief change operations that generalize them), in one natural framework.	belief revision;theory	Nir Friedman;Joseph Y. Halpern	1994			computer science;artificial intelligence;belief revision	AI	-15.73492195154478	6.8947391721684355	16520
bda4a3629ad09d1b8b2e454122ebfe90813adc35	partial evaluation of ghc programs based on the ur-set with constraints.	partial evaluation			Hiroshi Fujita;Akira Okumura;Koichi Furukawa	1988			mathematical optimization;partial evaluation;computer science	PL	-22.46928833175961	21.82319031355218	16531
125b42509024e5637c7e000e837cfc939bd6b707	a visual approach for modeling spatiotemporal relations	visual specification;programming language;digital tv;connector;visual representation;synchronization;declarative languages;spatiotemporal relations;ncl;sbtvd	Textual programming languages have proven to be difficult to learn and to use effectively for many people. For this sake, visual tools can be useful to abstract the complexity of such textual languages, minimizing the specification efforts. In this paper we present a visual approach for high level specification of spatiotemporal relations. In order to accomplish this task, our visual representation provides an intuitive way to specify complex synchronization events amongst media. Finally, to validate our work, the visual specification is mapped to NCL (Nested Context Language), the standard declarative language of the Brazilian Terrestrial Digital TV System.	declarative programming;high-level programming language;nested context language;terrestrial television	Rodrigo Laiola Guimarães;Carlos de Salles Soares Neto;Luiz Fernando Gomes Soares	2008		10.1145/1410140.1410202	natural language processing;synchronization;computer science;database;programming language	SE	-32.81154965159918	23.839523238043252	16540
852232645d1e38f96e71037ed88c146a13e01963	declarative diagnosis of wrong answers in constraint functional-logic programming	programmation logique avec contrainte;diagnostico;programacion logica con restriccion;logical programming;programmation logique;error diagnosis;constraint logic programming;functional logic programming;abstract interpretation;diagnosis;programacion logica;generic programming;diagnostic	Debugging tools are a practical need for diagnosing the causes of erroneous computations. Declarative programming paradigms involving complex operational details, such as constraint solving and lazy evaluation, do not fit well to traditional debugging techniques relying on the inspection of low-level computation traces. As a solution to this problem, declarative diagnosis uses Computation Trees (shortly, CTs) in place of traces. CTs are built a posteriori to represent the structure of a computation whose top level outcome is regarded as an error symptom by the user. Each node in a CT represents the computation of some observable result, depending on the results of its children nodes. Declarative diagnosis explores a CT looking for a so-called buggy node which computes an incorrect result from children whose results are correct; such a node must point to an incorrect program fragment. The search for a buggy node can be implemented with the help of an external oracle (usually the user with some semiautomatic support) who has a reliable declarative knowledge of the expected program semantics, the so-called intended interpretation. The generic description of declarative diagnosis in the previous paragraph follows [8]. Declarative diagnosis was first proposed in the field of logic programming [10], and it has been successfully extended to other declarative programming paradigms, including lazy functional programming [9], constraint logic programming [11,4] and functional logic programming [2,3]. In contrast to recent approaches to error diagnosis using abstract interpretation [5], declarative diagnosis often involves complex queries to the user. This problem has been tackled by means of various techniques, such as user-given partial specifications of the program’s semantics [3], safe inference of information from answers previously given by the user [2], or CTs tailored to the needs of a particular debugging problem over a particular computation domain [4]. Current research in declarative diagnosis has still to face many challenges regarding both the foundations and the development of practical tools. The aim of this work is to present a declarative method for diagnosing wrong computed answers in CFLP (D), a newly proposed generic programming scheme which can be instantiated by any constraint domain D given as parameter, and supports a powerful combination of functional and constraint logic programming over D [6]. Borrowing ideas from CFLP (D) declarative semantics we obtain a	abstract interpretation;computation;constraint logic programming;constraint satisfaction problem;debugging;declarative programming;functional logic programming;functional programming;generic programming;high- and low-level;interpretation (logic);lazy evaluation;observable;programming paradigm;semantics (computer science);tracing (software)	Rafael Caballero;Mario Rodríguez-Artalejo;Rafael del Vado Vírseda	2006		10.1007/11799573_31	constraint logic programming;constraint programming;declarative programming;computer science;artificial intelligence;theoretical computer science;functional logic programming;inductive programming;fifth-generation programming language;programming language;generic programming;algorithm	AI	-18.737869138995748	22.36553695483602	16547
8cdbb56c4cd39e58cf879f70c4f5d5c3e50b0927	access to objects by path expressions and rules	004 informatik;object oriented database;path expressions	Object oriented databases provide rich structuring capabilities to organise the objects being relevant for a given application. Due to the possible complexity of object structures, path expressions have become accepted as a concise syntactical means to reference objects. Even though known approaches to path expressions provide quite elegant access to objects, there seems to be still a need for more generality. To this end, the rule-language PathLog is introduced. A first contribution of PathLog is to add a second dimension to path expressions in order to increase conciseness. In addition, a path expression can also be used to reference virtual objects. Both enhancements give rise to interesting semantic implications. *Work supported by Deutsche Fomchungsgcmci~~, La 698/Q-l. Pewnirrion to copy without fee all or part of thir material ir granted provided that the copier are not made or dirtributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice in given that copying ie by pew&&on of the Very Larye Data Bare Endowment. To copy otherwire, or to nprblid, rqvinr a fee and/or special pemairrion from the Endowment. Proceedings of the 20th VLDB Conference Santiago, Chile, 1994 Georg Lausen Heinz Uphoff Institut fiir Informatik UniversitZt Freiburg 79104 Freiburg, Germany {lausen,uphoff)Qinformatik.uni-freiburg.de	database;event condition action;path expression;photocopier;sql;vldb	Jürgen Frohn;Georg Lausen;Heinz Uphoff	1994			computer science;theoretical computer science;path expression;database;distributed computing;algorithm	DB	-30.34059246465584	7.417296103729018	16567
265a6bd2543f2584068c5770a27bc8b0dc840bca	constructing the least models for positive modal logic programs	satisfiability;modal logic;modal horn	We give algorithms to construct the least L-model for a given positive modal logic program P , where L can be one of the modal logics KD, T , KDB, B, KD4, S4, KD5, KD45, and S5. If L ∈ {KD5,KD45, S5}, or L ∈ {KD,T,KDB,B} and the modal depth of P is finitely bounded, then the least L-model of P can be constructed in PTIME and coded in polynomial space. We also show that if P has no flat models then it has the least models in KB, K5, K45, and KB5. As a consequence, the problem of checking the satisfiability of a set of modal Horn formulae with finitely bounded modal depth in KD, T , KB, KDB, or B is decidable in PTIME. The known result that the problem of checking the satisfiability of a set of Horn formulae in K5, KD5, K45, KD45, KB5, or S5 is decidable in PTIME is also studied in this work via a different method.	amd k5;algorithm;boolean satisfiability problem;browser user interface;deductive database;horn clause;kilobyte;logic programming;modal depth;modal logic;p (complexity);pspace;polynomial;query language	Linh Anh Nguyen	2000	Fundam. Inform.	10.3233/FI-2000-42102	modal logic;normal modal logic;combinatorics;discrete mathematics;axiom s5;mathematics;algorithm;satisfiability	Logic	-8.250333939973919	17.577892000953543	16568
f600ce3d5ec9d9a496dba1413f72a7b8f05b8509	on the equivalence of schemes	general methods	One objective of the study of schemes for computation is that of finding general methods for checking or verifying a given program against its specifications. If one views the program and its specification as presenting two supposedly equivalent schemes for a computation, then the objective reduces to one of finding general methods for determining whether, in fact, the two schemes are equivalent. Unfortunately, for many classes of schemes which are sufficiently powerful, this “equivalence problem” is not decidable, i.e. there are no general methods capable of determining, in all cases, whether two schemes in the class are equivalent. In this report we investigate the question of which classes of schemes have decidable equivalence problems, or, in otherwords the question of for which classes of schemes there exist general methods capable of determining equivalence.	computation;existential quantification;turing completeness;verification and validation	Stephen J. Garland;David C. Luckham	1972		10.1145/800152.804897	combinatorics;discrete mathematics;mathematics;algorithm	Theory	-14.41616072110485	24.836106913851005	16586
65934304e1441abdd43a2e97a4fd5fc22904a45e	toward interactive design of correct programs	interaction design	Publisher Summary This chapter describes an imagined interaction between a computer programmer and his machine, which might be made feasible within the next decade. It focuses on an intelligent assistant approach. The chapter also presents the earliest published description of an intelligent programming assistant. It describes an imagined interaction between a computer programmer and an intelligent program verifier assistant; such systems will be feasible within the following decade. The programmer is at an interactive console, designing a program, first in its overall outline, then by successive developments in detail. The computer is, of course, serving its customary role as syntactic analyzer, code generator, program executor, prompter, and file handler. In addition, the computer is continually checking the program, at each level of specification, for consistency with the programmeru0027s stated intentions.	interactive design;interactivity	Robert W. Floyd	1971			simulation;computer science;computer engineering	Theory	-29.65957524255239	22.15045172986282	16627
abdb2302f42309bdccda2a6639544375c51108bc	new languages from old: the extension of programming languages by embedding, with a case study	embedding;sequential machines;programming language;set partitions;turing machine;automata;extensible languages;data structures;pushdown automata;snobol4;application oriented languages;data structure;sparse matrices;finite state machine	Embedding is the extension of a programming language without altering the processor for that language, and preferably using only the facilities of that language. While a single subroutine is the simplest form of semantic extension by embedding, embedding can produce powerful application-oriented languages (AOL) in a relatively economical fashion.  A first approximation to a design discipline for writing embedded AOLs is presented. SNOBOL4 is discussed as a particularly receptive host language, in that it has flexible subroutine, data-structure, and storage allocation facilities, has a run-time compiler which can be called from a SNOBOL4 program, and is widely available for almost all research computer systems.  A description is given of AUTOMAT, a SNOBOL4-embedded AOL for classroom or research experiments with abstract sequential machines. The generalized transition table is introduced as a data structure in which finite-state machines (FSM), Turing machines, pushdown automata, and the like can be represented as special cases. Data structures for sets, partitions and covers, sparse matrices, and tapes are provided as well.  The AUTOMAT AOL currently includes over 100 operators developed according to the proposed method. Applications thus far include an extensive FSM utility system, universal simulators for FSM and other models, FSM minimization, and Krohn-Rhodes decomposition. Performance statistics are presented.	apl;automata theory;compiler;data structure;embedded system;experiment;finite-state machine;krohn–rhodes theory;order of approximation;programming language;pushdown automaton;snobol;simulation;sparse matrix;stack (abstract data type);state transition table;subroutine;turing machine;utility system	Michael B. Feldman	1976			data structure;sparse matrix;computer science;turing machine;theoretical computer science;operating system;embedding;context-free language;automaton;programming language;pushdown automaton;algorithm	PL	-25.743931362201305	25.802039431006087	16648
ed8bd95ddffb3b8fd4418c8dd0fdf252fef10913	finding symmetry in models of concurrent systems by static channel diagram analysis	distributed system;symmetry;qa75 electronic computers computer science;concurrency;formal verification;model checking;concurrent systems;modelling language;community structure;promela spin;message passing;state explosion;distributed systems;mobile systems	Over the last decade there has been much interest in exploiting symmetry to combat the state explosion problem in model checking. Although symmetry in a model often arises as a result of symmetry in the topology of the system being modelled, most model checkers which exploit structural symmetry are limited to topologies which exhibit total symmetries, such as stars and cliques. We define the static channel diagram of a concurrent, message passing program, and show that under certain restrictions there is a correspondence between symmetries of the static channel diagram of a program and symmetries of the Kripke structure associated with the program. This allows the detection, and potential exploitation, of structural symmetry in systems with arbitrary topologies. Our method of symmetry detection can handle mobile systems where channel references are passed on channels, resulting in a dynamic communication structure. We illustrate our results with an example using the Promela modelling language.	client–server model;cognitive dimensions of notations;concurrency (computer science);diagram;exploit (computer security);kripke structure (model checking);load balancing (computing);modeller;message passing;model checking;modeling language;promela;server (computing)	Alastair F. Donaldson;Alice Miller;Muffy Calder	2005	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2005.04.010	model checking;discrete mathematics;message passing;concurrency;formal verification;computer science;theoretical computer science;mathematics;distributed computing;symmetry;programming language;community structure	Logic	-16.816689906286168	28.454676445181683	16654
4476f9d87829cf9070066b772a822061ece5ef60	algebraic characterizations for universal fragments of logic	universal horn theory;term structure;fully invariant system;generic class;neumann type correspondence;universal theory	In this paper we address our efforts to extend the well-known connection in equational logic between equational theories and fully invariant congruences to other possibly infinitary logics. In the special case of algebras, this problem has been formerly treated by H. J. Hoehnke [lo] and R. W. Quackenbush [14]. Here we show that the connection extends at least up to the universal fragment of logic. Namely, we establish that the concept of (infinitary) universal theory matches the abstract notion of fully invariant system. We also prove that, inside this wide group of theories, the ones which are strict universal Horn correspond to fully invariant closure systems, whereas those which are universal atomic can be characterized as principal fully invariant systems. Mathematics Subject Classification: 03C52, 08C10, 03G99.	mathematics subject classification;theory;whole earth 'lectronic link	Raimon Elgueta	1999	Math. Log. Q.	10.1002/malq.19990450309	mathematical analysis;discrete mathematics;topology;pure mathematics;mathematics;yield curve;algorithm;algebra	Logic	-10.308601899442808	17.15579622251902	16734
c72964808151a43f2da84f764f352167e47eb12c	an agent cooperation model and dependency measurement based on merge & synthesis on shared timed task environments		In this paper, we analyze cooperation phenomena based on agent synthesis on merge of timed two-input discrete transition systems. The model allows handling of possible incomplete task environment descriptions and representation of nondeterministic directive effects and/or deviating (or blocking) disturbances defined over agent capabilities. Also, we propose an agent dependency measure that uses the result of additive Mergeu0026Synthesis operation for quantification purpose.		Hürevren Kiliç	2018		10.1007/978-3-030-03014-8_24	automated planning and scheduling;nondeterministic algorithm;merge (version control);directive;artificial intelligence;computer science	Embedded	-25.070829143717805	16.071687604055356	16742
8ab872a250c1b2e7599cd6f638603e1a0f9dff5b	contextual negations and reasoning with contradictions	classical logic;knowledge representation;paraconsistent logic	This paper introduces the logical basis for mod­ elling the phenomenon of reasoning in the pres­ ence of contradiction, by identifying this prob­ lem with the notion of change of context. We give here the basic definitions of a new seman­ tics, which works by interpreting one logic into a family of logics via translations, which we call semantics of translations. As a particular ap­ plication we show that a simple logic support­ ing contradictions can be constructed translat­ ing classical logic into three-valued logics. This translation semantics offers a new interpreta­ tion to certain paraconsistent logics which al­ lows the application of them to automated rea­ soning and knowledge representation.	interface description language;knowledge representation and reasoning;paraconsistent logic;three-valued logic	Walter Alexandre Carnielli;Luis Fariñas del Cerro;Mamede Lima-Marques	1991			dynamic logic;knowledge representation and reasoning;principle of explosion;discrete mathematics;classical logic;description logic;paraconsistent logic;many-valued logic;computer science;intermediate logic;artificial intelligence;non-monotonic logic;mathematics;deductive reasoning;logic;term logic;multimodal logic;algorithm;philosophy of logic;autoepistemic logic	AI	-15.773164006601323	9.41854169203702	16765
1485ed06748bc2fcedb024e4006adf2fb6a2da40	invited talk: developing efficient smt solvers		Decision procedures for checking satisfiability of logical formulas are crucial for many verification applications. Of particular recent interest are solvers for Satisfiability Modulo Theories (SMT). SMT solvers decide logical satisfiability (or dually, validity) of formulas in classical multi-sorted first-order logic with equality, with respect to a background theory. The success of SMT for verification applications is largely due to the suitability of supported background theories for expressing verification conditions. In this talk I will discuss how modern SMT solvers work, and the main implementation techniques used. I will also describe how SMT solvers are used in industry and Microsoft in particular.	boolean satisfiability problem;first-order logic;first-order predicate;modulo operation;satisfiability modulo theories;theory	Leonardo Mendonça de Moura	2007			beam (structure);algorithm;sampling (statistics);wavefront;computer science;light beam;beam expander;wavelength;optics;diffraction;grating	Logic	-15.151989480263735	15.65045738008483	16831
129cdc2eb005167118355b0e8121d075737e3644	generating concurrency checks automatically	soundness;points to;difference propagation	This article introduces ATAB, a tool that automatically generates pairwise reachability checks for action trees. Action trees can be used to study the behaviour of real-world concurrent programs. ATAB encodes pairwise reachability checks into alternating tree automata (ATA) that determine whether an action tree has a schedule where any pair of given points in the program are simultaneously reachable. Because the pairwise reachability problem is undecidable in general ATAB operates under a restricted form of lock-based concurrency. ATAB produces ATA that are more compact and more efficiently checkable than those that have been previously used. The process is entirely automated, which simplifies the process of encoding checks for more complex action trees. The ATA produced are easier to scale to large numbers of locks than previous constructions.	alternating tree automata;automata theory;concurrency (computer science);denotational semantics;lock (computer science);reachability problem;serial ata;tree automaton	Jonathan Hoyland;Matthew Hague	2016		10.1145/2955811.2955815	soundness;discrete mathematics;computer science;distributed computing;algorithm	Logic	-12.234647511334353	24.56118654845737	16887
1a60d6c911979703b39ed543e74bf2f54cf96798	implementing dynamic code assembly for client-based query processing	query processing	As determined by the applications’ requirements, nonstandard DBMS are usually conceived for client/server architectures [5, 12]. While the server is responsible for general data-management tasks and for precomputing data, application-oriented processing is done in the mainmemory buffer at the client. Since the expressive power of the query language should be available not only for loading/unloading the buffer, but also for more sophisticated processing tasks on the buffer contents, mainmemory based query processing must be supported [6, 14]. A simple navigational buffer interface is no longer satisfactory. Instead, a declarative interface is required to allow querying the buffer contents. Additionally, adequate main-memory indices should be supported to cope with large application buffers.	assembly language;client–server model;code generation (compiler);common lisp;compile time;compiler;computer data storage;data buffer;data structure;database;executable;ibm system r;mathematical optimization;objectstore;path expression;precomputation;query language;query optimization;query plan;requirement;run time (program lifecycle phase);scanline rendering;server (computing)	Joachim Thomas;T. Gerbes;Theo Härder;Bernhard Mitschang	1995			sargable;real-time computing;computer science;data mining;database;distributed computing;programming language;code generation	DB	-30.168100631450695	6.424399158334593	16888
4e9e2e6516442af6d7c25d428b6b5e5a1cdc862d	custom automations in mizar	proof assistant;68t15;03b35;formalization of mathematics;mizar proof verifier;03f99;automations	The central aim of the Mizar project is to produce strictly formalized mathematical statements with mechanically certified proofs. When writing a Mizar formalization, a significant amount of the user’s time typically goes into browsing the Mizar Mathematical Library (MML) for the already-proved results he needs. Here a few techniques to reduce this time are illustrated.	mizar	Marco B. Caminati;Giuseppe Rosolini	2012	Journal of Automated Reasoning	10.1007/s10817-012-9266-1	computer science;mizar system;proof assistant;programming language;algorithm	SE	-20.133333905243607	20.66831977158963	16920
9274c05aa089ca3bcf38d0e26408c1fde28efbf5	utility aware clustering for publishing transactional data		This work aims to maximise the utility of published data for the partition-based anonymisation of transactional data. We make an observation that, by optimising the clustering i.e. horizontal partitioning, the utility of published data can significantly be improved without affecting the privacy guarantees. We present a new clustering method with a specially designed distance function that considers the effect of sensitive terms in the privacy goal as part of the clustering process. In this way, when the clustering minimises the total intra-cluster distances of the partition, the utility loss is also minimised. We present two algorithms DocClust and DetK for clustering transactions and determining the best number of clusters respectively.	dynamic data	Michael Bewong;Jixue Liu;Lin Liu;Jiuyong Li	2017		10.1007/978-3-319-57529-2_38	data mining;database;world wide web	DB	-30.60242591194918	20.378844420982023	16972
075e4f6789292c3baa2fe62380d68a73b04b006c	remark on algorithm 16: crout with pivoting	system analysis;computer simulation of stochastic processes;monte carlo;simulation of human learning	"""These combinations test all loops of the program. This procedure contains the following errors: a. In SOLVE, the expression c[k] := c[k]-INNERPRODUCT (B[k,p],c[p],p 1, k-1) should read : c[k] := c[k]-INNERPRODUCT '3""""he efficiency of the algorithm will be improved :by the following changes : a. In the elimination phase ofCROUT, replace for i := k-4-1 step 1 Until I~ do A[i, k] := quot XA[i, kl ; b. Omit INNERPRODUCT from the formal parameter list in both CROUT and SOLVE, and declare INNERPRODUCT either locally, or globally. This avoids any reference to INNER-PRODUCT in the calling sequence produced by a compiler. It is also to be noted that a minor modification of CROUT allows it to be used to evaluate the determinant of A. All of these suggestions are included in a later algorithm. On attempting to use this algorithm, I discovered the two following errors : (1) The line following the SWITCH statcinent should read: for L := 1 step 1 until n do (2) The line starting with the label loop', should read: loop: dd := 1-4-d ; bi = x0 X dT.2-xl X ddT2-I-x2 X (dd + d) ; With these two modifications incorporated ~t~e algorithm was translated into the language of the Burroughs Algebraic Compiler and has been used successfully on the Burroughs 220 Computer ."""	algorithm;compiler;parameter (computer programming);pivot table	Henry C. Thacher	1961	Commun. ACM	10.1145/366199.366278	mathematical optimization;simulation;computer science;theoretical computer science;system analysis;monte carlo method	Logic	-15.13288296438825	31.907575219891054	16997
3f5de9b6f1b9a3e3ed437326a78d921ab61c9097	fuzzy semantic web ontology learning from fuzzy uml model	fuzzy uml model;fuzzy description logic;ontology learning;fuzzy ontology;semantic web;knowledge base	How to quickly and cheaply construct Web ontologies has become a key technology to enable the Semantic Web. Classical ontologies are not sufficient for handling imprecise and uncertain information that is commonly found in many application domains. In this paper, we propose an approach for constructing fuzzy ontologies from fuzzy UML models, in which the fuzzy ontology consists of fuzzy ontology structure and instances. Firstly, the fuzzy UML model is investigated in detail, and a kind of formal definition of fuzzy UML models is proposed. Then, a kind of fuzzy ontology called fuzzy OWL DL ontology is introduced. Furthermore, we consider the fuzzy UML model and the corresponding fuzzy UML instantiations (i.e., object diagrams) simultaneously, and translate them into the fuzzy ontology structure and the fuzzy ontology instances, respectively. In addition, since a fuzzy OWL DL ontology is equivalent to a fuzzy Description Logic f-SHOIN(D) knowledge base, how the reasoning problems of fuzzy UML models (e.g., consistency, subsumption, equivalence, and redundancy) may be reasoned through reasoning mechanism of f-SHOIN(D) is investigated, which can help to construct fuzzy ontologies more exactly.	application domain;description logic;diagram;knowledge base;ontology (information science);ontology learning;semantic web;subsumption architecture;turing completeness;unified modeling language	Fu Zhang;Zongmin Ma;Jingwei Cheng;Xiangfu Meng	2009		10.1145/1645953.1646082	fuzzy logic;upper ontology;knowledge base;fuzzy cognitive map;adaptive neuro fuzzy inference system;fuzzy classification;computer science;knowledge management;ontology;artificial intelligence;neuro-fuzzy;applications of uml;semantic web;data mining;database;fuzzy associative matrix;ontology-based data integration;process ontology;fuzzy control system	AI	-22.684091256695364	6.548337024603094	17026
e00f6db2b89e3fea9d2917dcd63dd81020310c99	linear λμ is cp (more or less)		In this paper we compare Wadler’s CP calculus for classical linear processes to a linear version of Parigot’s λμ calculus for classical logic. We conclude that linear λμ is “more or less” CP, in that it equationally corresponds to a polarized version of CP. The comparison is made by extending a technique from Melliès and Tabareau’s tensor logic that correlates negation with polarization. The polarized CP, which is written CP± and pronounced “CP more or less,” is an interesting bridge in the landscape of Curry-Howard interpretations of logic.	curry;curry–howard correspondence;lambda-mu calculus;polarization (waves)	Jennifer Paykin;Steve Zdancewic	2016		10.1007/978-3-319-30936-1_15	mathematics	PL	-9.032020259904694	10.927163249589118	17029
4fc3c3a71a65e1ca1338c1e799127d5d6b88d3aa	on the interaction between inverse features and path-functional dependencies in description logics	functional dependency;object oriented;description logic	We investigate how inverse features can be added to a boolean complete description logic with pathfunctional dependencies in ways that avoid undecidability of the associated logical implication problem. In particular, we present two conditions that ensure the problem remains EXPTIMEcomplete. The first is syntactic in nature and limits the form that dependencies may have in argument terminologies. The second is a coherence condition on terminologies that is sufficiently weak to allow the transfer of relational and emerging objectoriented normalization techniques.	data structure;database normalization;decision problem;description logic;exptime;functional dependency;mathematical optimization;path expression;query optimization;type class;unary operation;undecidable problem	David Toman;Grant E. Weddell	2005			discrete mathematics;description logic;dependency theory;computer science;artificial intelligence;theoretical computer science;mathematics;functional dependency;object-oriented programming;algorithm	AI	-25.59570438921564	10.617194682800083	17065
68f6eafc200484cb05d1395545bb4909bf97fd6f	on creating efficient object-relational views of scientific datasets	scientific application;complex objects;scientific datasets;building block;relational databases natural sciences computing object oriented databases;object relational views;indexed join algorithm;derived data sources;data source abstraction;construction cost;indexation;grace hash join algorithm object relational views scientific datasets data source abstraction basic data sources derived data sources indexed join algorithm;basic data sources;relational databases;object oriented databases;grace hash join algorithm;natural sciences computing;costs data mining clustering algorithms performance analysis image sensors subcontracting algorithm design and analysis petroleum hydrocarbon reservoirs biomedical informatics;object relational	Scientific datasets are often large and distributed in flat files across several storage nodes. Scientists frequently want to analyze subsets of these datasets. A data source abstraction that provides an object-relational view of data while hiding the details of storage and transport mechanisms and dataset layouts is useful in this regard. In this abstraction, basic data sources (BDS) interpret flat files as a set of records and are the building blocks of the view mechanism. Derived data sources (DDS) may be built on top of BDSs and provide more complex objects that serve the scientists' needs. The simplest DDS is one that supports a join based view over BDSs. We investigate issues involving building such DDSs for scientific applications and consider distributed versions of the indexed join and the grace hash join algorithms. We construct cost models that capture their performance in a restricted space of dataset and system parameters and compare them analytically and experimentally	algorithm;cache (computing);experiment;flat file database;hash join;ijustine;join (sql);object-relational database	Sivaramakrishnan Narayanan;Tahsin M. Kurç;Ümit V. Çatalyürek;Joel H. Saltz	2006	2006 International Conference on Parallel Processing (ICPP'06)	10.1109/ICPP.2006.56	parallel computing;relational database;computer science;theoretical computer science;data mining;database	DB	-29.37654716360466	5.775137241727901	17077
0ae73d454d51d8f747bc1c148c99221e2089f2f5	abstract interpretation and object-oriented programming: quo vadis?	object oriented programming;static analysis;abstract interpretation	The aim of this position paper is to draw a quick overview of the main contributions in abstract interpretation of object-oriented programs, and to draw possible lines of research in this field.	abstract interpretation;class hierarchy	Francesco Logozzo;Agostino Cortesi	2005	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2005.01.024	computer science;programming language;object-oriented programming;static analysis;algorithm	PL	-28.929887554479244	25.23296061440837	17078
65c2093e93c50015fa99beedced44b41c060444a	the role of deontic logic in the specification of information systems	deontic logic;information system	In this paper we discuss the role that deontic logic plays in the specification of information systems, either because constraints on the systems directly concern norms or, and even more importantly, system constraints are considered ideal but violable (so-called ‘soft’ constraints). To overcome the traditional problems with deontic logic (the so-called paradoxes), we first state the importance of distinguishing between ought-to-be and ought-to-do constraints and next focus on the most severe paradox, the so-called Chisholm paradox, involving contrary-to-duty norms. We present a multi-modal extension of standard deontic logic (SDL) to represent the ought-to-be version of the Chisholm set properly. For the ought-to-do variant we employ a reduction to dynamic logic, and show how the Chisholm set can be treated adequately in this setting. Finally we discuss a way of integrating both ought-to-be and ought-to-do reasoning, enabling one to draw conclusions from ought-to-be constraints to ought-to-do ones, and show by an example the use(fulness) of this.	deontic logic;information system;modal logic	John-Jules Ch. Meyer;Roel Wieringa;Frank Dignum	1998			dynamic logic;normal modal logic;deontic logic;mathematics;accessibility relation;information system	AI	-16.031458590616598	6.6591185313843075	17105
52969f05c4d726782b0d786c34a768cd259f4cfa	efficient storage reuse of aggregates in single assignment languages	polynomial complexity;general methods	The storage reuse of aggregates is a key problem in implementing single assignment languages. In this paper, on the basis of a typical subset of the single assignment language SISAL, we analyze the inherent limits of storage reuse and define what the maximal storage reuse is. We propose an efficient method of achieving storage reuse, which is of polynomial complexity and linear in common cases. It achieves the maximal storage reuse for an extensive program class into which all common benchmark programs fall. We also show that no general method can guarantee the maximal storage reuse for programs outside the class. In this case, our method can choose the most likely operation to reuse the storage of an aggregate or a set of shared aggregates.	assignment (computer science)	Zhonghua Li;Chris C. Kirkham	1996		10.1007/3-540-61053-7_65	discrete mathematics;computer science;theoretical computer science;algorithm	PL	-23.926775296746282	12.302852891834146	17109
3e896711405559e9896b91a77dc4054ca06d156a	a hybrid object clustering strategy for large knowledge-based systems	graph theory;knowledge based system;requirements management;storage management;storage management object oriented databases deductive databases knowledge based systems graph theory;knowledge base management;object oriented;clustering;storage managers hybrid object clustering strategy large knowledge based systems knowledge based applications object oriented technology hybrid clustering strategy semantic clustering iterative graph partitioning techniques object graphs realistic object bases semantic specification mechanism placement trees object oriented databases granularity benchmark segmented storage scheme large object storage mechanisms;knowledge based systems object oriented databases relational databases knowledge management computer science knowledge engineering tree graphs performance evaluation database systems application software;object oriented databases;knowledge based systems;deductive databases;knowledge base	Object bases underlying knowledge-based applications tend to be complex and require management. This research aims at improving the performance of object bases underlying a class of large knowledge-based systems that utilize object-oriented technology to engineer the knowledge base. In this paper, a hybrid clustering strategy that beneecially combines semantic clustering and iterative graph-paritioning techniques has been developed and evaluated for use in knowledge bases storing information in the form of object graphs. It is demonstrated via experimentation that such a technique is useful and feasible in realistic object bases. A semantic speciication mechanism similar to placement trees has been developed for specifying the clustering. The workload and the nature of object graphs in knowledge bases diier sig-niicantly from those present in conventional object-oriented databases. Therefore, the evaluation has been performed by building a new benchmark called the Granularity Benchmark. A segmented storage scheme for the knowledge base using large object storage mechanisms of existing storage managers is also examined.	benchmark (computing);cluster analysis;database;iterative method;knowledge base;knowledge-based systems;object storage;signature block	Arun Ramanujapuram;Jim E. Greer	1996		10.1109/ICDE.1996.492113	knowledge base;requirements management;method;computer science;knowledge management;knowledge-based systems;open knowledge base connectivity;data mining;database;knowledge extraction;cluster analysis;object-oriented programming	DB	-33.4737375528216	13.221671685236458	17185
25382cb836b8e36410ad1e6cb2d51869f08b6f52	encoding context-sensitivity in reo into non-context-sensitive semantic models	coordination language;mutual exclusion;semantic model;context dependent	Reo is a coordination language which can be used to model the interactions among a set of components or services in a compositional manner using connectors. The language concepts of Reo include synchronization, mutual exclusion, data manipulation, memory and contextdependency. Context-dependency facilitates the precise specification of a connector’s possible actions in situations where it would otherwise exhibit nondeterministic behavior. All existing formalizations of contextdependency in Reo are based on extended semantic models that provide constructs for modeling the presence and absence of I/O requests at the ports of a connector. In this paper, we show that context-dependency in Reo can be encoded in basic semantic models, namely connector coloring with two colors and constraint automata, by introducing additional fictitious ports for Reo’s primitives. Both of these models were considered as not expressive enough to handle context-dependency up to now. We demonstrate the usefulness of our approach by incorporating context-dependency into the constraint automata based Vereofy model checker.	automata theory;color;computation;constraint automaton;context-sensitive grammar;context-sensitive language;encode;graph coloring;input/output;interaction;model checking;mutual exclusion;nondeterministic algorithm;semantic data model;synchronization (computer science);vereofy	Sung-Shik T. Q. Jongmans;Christian Krause;Farhad Arbab	2011		10.1007/978-3-642-21464-6_3	semantic data model;mutual exclusion;computer science;context-dependent memory;distributed computing;reo coordination language;programming language;algorithm	Embedded	-20.269104110728712	28.790014877877436	17218
00ca018575cdf9d75ba581f070785dfcd54f433d	enhancing answer set programming with templates	answer set programming;data structure	The work aims at extending Answer Set Programming (ASP) with the possibility of quickly introducing new predefined constructs and to deal with compound data structures: we show how ASP can be extended with ‘template’ predicate’s definitions. We present language syntax and give its operational semantics. We show that the theory supporting our ASP extension is sound, and that program encodings are evaluated as efficiently as ASP programs. Examples show how the extended language increases declarativity, readability, compactness of program encodings and code reusability. .	answer set programming;data structure;operational semantics;stable model semantics	Giovambattista Ianni;Giuseppe Ielpa;Adriana Pietramala;Maria Carmela Santoro;Francesco Calimeri	2004			orthogonality (programming);programming language;inductive programming;computer science;programming paradigm;answer set programming;procedural programming;symbolic programming;operational semantics;programming domain	AI	-21.760502740287485	19.176618463728705	17224
4ff9a38341d96218fd76e31a71667a3da7eba990	on representation and approximation of operations in boolean algebras	logica booleana;operator algebra;multi valued logic;logica multivalente;logique floue;logica difusa;boolean algebra;fuzzy logic;logique multivalente;algebre boole;algebre operateur;logique booleenne;multivalued logic;boolean logic;algebra operador;algebra boole	Several universal approximation and universal representation results are known for non Boolean multi valued logics such as fuzzy logics In this paper we show that similar results can be proven for multi valued Boolean logics as well Introduction The study of Boolean algebras started when G Boole con sidered the simplest Boolean algebra the two valued algebra B f g ffalse trueg In this algebra the operation and negation a a have the direct logical meaning of or and and not It is known that in this Boolean algebra an arbitrary operation i e an arbitrary function B B B can be represented as a superposition of these three basic logical operations e g the implication a b can be represented as b a etc Logic is still one of the area of application of Boolean algebras but starting from the classical Kolmogorov s monograph Boolean algebras namely algebras of events became an important tool in another area foundations of probability In contrast to the Boolean algebra B in more complex Boolean algebras there are functions B B B which cannot be represented in terms of the three basic operations Until recently these functions were rarely used traditionally in probability applications only the standard operations and a were used or operations which can be explicitly described in terms of these three Some complex events can be easily described by using these three operations but some other complex events like a hypothetic conditional event a if b whose probability is equal to the conditional probability P ajb cannot be described via these three operations this result is due to Lewis for its detailed description see e g Chapter of Some researchers even thought that since we cannot get an expression for a conditional event by using the three basic operations of Boolean algebra we therefore cannot describe such events in Boolean algebra at all It was recently shown however see e g that if we use a new operation an operation which cannot be explicitly represented in terms of and a then it is possible to describe conditional events within the Boolean algebra formalism Namely to describe conditional events a if b we can consider instead of an individual event a potentially in nite sequence of similar events and interpret a if b as a is true in the rst moment of time in which b is true Comment Let us describe this representation in more formal terms for those readers who are familiar with the main notions of mathematical probability theory other readers can skip this comment In more formal terms instead of the original Boolean algebra B of all events measurable subsets of a algebra we consider the set IN of all in nite sequences of events i with a product measure this construction is standard in probability theory where it is used to formulate and prove limit theorems and the new Boolean algebra of all events on N Then the event a if b is interpreted as b a	approximation;boolean algebra;formal grammar;fuzzy logic;logical connective;naruto shippuden: clash of ninja revolution 3;quantum superposition	I. R. Goodman;Vladik Kreinovich	2001	Int. J. Intell. Syst.	10.1002/int.1028	monoidal t-norm logic;t-norm fuzzy logics;boolean algebra;discrete mathematics;pure mathematics;absorption law;mathematics	DB	-11.301738011491407	7.9355008326617265	17327
de1cfb16e99be425d043e7ab91e1d642b3f54658	a delegation-based object calculus with subtying	object oriented language;type inference	"""This paper presents an untyped object calculus that reeects the capabilities of so-called delegation-based object-oriented languages. A type inference system allows static detection of errors, such as message not understood, while at the same time allowing the type of an inherited method to be specialized to the type of the inheriting object. The main advance over previous work is the provision for subtyping in the presence of delegation primatives. This is achieved by distinguishing a prototype, whose methods may be extended or replaced, from an object, which only responds to messages for which it already has methods. An advantage of this approach is that we have full subtyping without restricting the \run-time"""" use of inheritance. Type soundness is proved using operational semantics and an analysis of typing derivations."""	inference engine;operational semantics;prototype;type inference;type safety	Kathleen Fisher;John C. Mitchell	1995		10.1007/3-540-60249-6_40	natural language processing;method;object model;object language;computer science;type inference;database;proof calculus;programming language;object-oriented programming;object definition language	PL	-23.408875798733735	26.055369641735677	17338
543602b2d44eb166feabc47285af094b973d919d	improvement operators		We introduce a new class of change operators. They are a generalization of usual iterated belief revision operators. The idea is to relax the success property, so the new information is not necessarily believed after the improvement. But its plausibility has increased in the epistemic state. So, iterating the process sufficiently many times, the new information will be finally believed. We give syntactical and semantical characterizations of these operators.	belief revision;iteration;plausibility structure;regular language description for xml	Sébastien Konieczny;Ramón Pino Pérez	2008			artificial intelligence;algorithm	AI	-15.572896452503443	7.0441572646385735	17360
ca874f1ed6aff971a8d0c85fd621522155f0c591	the application of constraint semantics to the language of subjective uncertainty	humanidades;filosofia etica	This paper develops a compositional, type-driven constraint semantic theory for a fragment of the language of subjective uncertainty. In the particular application explored here, the interpretation function of constraint semantics yields not propositions but constraints on credal states as the semantic values of declarative sentences. Constraints are richer than propositions in that constraints can straightforwardly represent assessments of the probability that the world is one way rather than another. The richness of constraints helps usmodel communicative acts in essentially the same way that we model agents’ credences. Moreover, supplementing familiar truth-conditional theories of epistemic modals with constraint semantics helps capture contrasts between strong necessity and possibility modals, on the one hand, and weak necessity modals, on the other.	assertion (software development);constraint (mathematics);denotational semantics;theory	Eric Swanson	2016	J. Philosophical Logic	10.1007/s10992-015-9367-5	philosophy;epistemology;mathematics;linguistics;algorithm	AI	-13.746871531032182	6.101848336893742	17381
6d185fd3f7448553d63f7f4b6e2c4499bf64ef0f	discrimination of class inheritance hierarchies - a vector approach				B. Ramachandra Reddy;Aparajita Ojha	2014		10.1007/978-3-319-05948-8_12		ML	-23.033987953862994	19.744362865539884	17383
00e1132d255c77d1508898cd98070f14a521976a	a coherence theorem for martin-löf's type theory	main result;inductive equality;smallest reflexive relation;equality relation;decidable identity;type theory;derivable inside type theory;coherence theorem;conventional equality;certain type;arbitrary type;coherence;equivalence relation	In type theory a proposition is represented by a type, the type of its proofs. As a consequence, the equality relation on a certain type is represented by a binary family of types. Equality on a type may be conventional or inductive. Conventional equality means that one particular equivalence relation is singled out as the equality, while inductive equality – which we also call identity – is inductively defined as the ‘smallest reflexive relation’. It is sometimes convenient to know that the type representing a proposition is collapsed, in the sense that all its inhabitants are identical. Although uniqueness of identity proofs for an arbitrary type is not derivable inside type theory, there is a large class of types for which it may be proved. Our main result is a proof that any type with decidable identity has unique identity proofs. This result is convenient for proving that the class of types with decidable identities is closed under indexed sum. Our proof of the main result is completely formalized within a kernel fragment of Martin-Löf’s type theory and mechanized using ALF. Proofs of auxiliary lemmas are explained in terms of the category theoretical properties of identity. These suggest two coherence theorems as the result of rephrasing the main result in a context of conventional equality, where the inductive equality has been replaced by, in the former, an initial category structure and, in the latter, a smallest reflexive relation.	category theory;turing completeness;type theory	Michael Hedberg	1998	J. Funct. Program.		unit type;computer science;equivalence relation;programming language;type theory	PL	-7.27746679438503	13.829787685810112	17412
7037d5c6fd0ffbe45c64f076f3c6108829916b8f	real-time rewriting logic semantics for spatial concurrent constraint programming		Process calculi provide a language in which the structure of terms represents the structure of processes together with an operational semantics to represent computational steps. This paper uses rewriting logic for specifying and analyzing a process calculus for concurrent constraint programming ((textsf {ccp})), combining spatial and real-time behavior. In these systems, agents can run processes in different computational spaces (e.g., containers) while subject to real-time requirements (e.g., upper bounds in the execution time of a given operation), which can be specified with both discrete and dense linear time. The real-time rewriting logic semantics is fully executable in Maude with the help of rewriting modulo SMT: partial information (i.e., constraints) in the specification is represented by quantifier-free formulas on the shared variables of the system that are under the control of SMT decision procedures. The approach is used to symbolically analyze existential real-time reachability properties of process calculi in the presence of spatial hierarchies for sharing information and knowledge.	concurrent constraint logic programming;constraint programming;real-time transcription;rewriting	Sergio Ramírez;Miguel Romero;Camilo Rocha;Frank D. Valencia	2018		10.1007/978-3-319-99840-4_13	constraint programming;discrete mathematics;time complexity;semantics;process calculus;executable;operational semantics;computer science;rewriting;reachability	Logic	-11.624033192673847	23.84397016874452	17416
dca3eb545702cf18f39a158600c9c7509509a6c9	automatic state capture of self-migrating computations in messengers	distributed system;eficacia sistema;compilacion;multiagent system;systeme reparti;informatique mobile;migration;reseau ordinateur;performance systeme;program transformation;ingenieria logiciel;transformation programme;software engineering;system performance;computer network;transformacion programa;sistema repartido;efficient implementation;mobile code;red ordenador;genie logiciel;compilation;sistema multiagente;mobile computing;migracion;systeme multiagent	With self-migrating computations, the main challenge is the extraction and subsequent restoration of the computation's state during migration. This is very diicult when the navigational statement may be placed anywhere in the code and hence many systems place the burden of state capture on the application programmer. We describe an intermediate approach, where the use of navigational statements is restricted to the top level of the self-migrating computation. This permits an ee-cient implementation of a fully transparent state capture and restoration. We demonstrate that this approach is applicable not only to interpreted mobile code but also to compiled self-migrating computations executing entirely in native mode.	circuit restoration;code mobility;compiler;computation;native (computing);programmer;system migration	Christian Wicke;Lubomir F. Bic;Michael B. Dillencourt;Munehiro Fukuda	1998		10.1007/BFb0057649	embedded system;simulation;computer science;human migration;operating system;mobile computing;algorithm	PL	-23.76799426442959	30.032972998590985	17449
2ca73a91bb8277ca73581284f5e021e06b2c075f	oscar: an object-oriented database system with a nested relational kernel			kernel (operating system);oscar	Andreas Heuer;Jürgen Fuchs;U. Wiebking	1990			relational database management system;relational model;nested set model;component-oriented database;database;relational database;object-relational impedance mismatch;database model;database design;computer science	DB	-31.09152492371097	9.339245754089328	17451
20ddf863082aad7816bb606dcb2e1218ce700e69	query processing in e-commerce environment using predictive partitioned relations	predictive partitioned relation;electronic commerce;query processing data mining delay computer science information technology database systems humans distributed databases image databases audio databases;t technology;image databases;query processing;e commerce;information technology;data mining;database systems;electronic commerce data mining query processing;data warehousing;distributed databases;e commerce data warehousing predictive partitioned relation query processing data mining;difference set;humans;audio databases;computer science;database management system	Too many attributes in a relation are not relevant to fulfilling the user's requirement. Different clients may be interested different set of attributes in the relation. Given this situation, how can the Database Management Systems process relevant attributes instead of reading all attributes? In a data-warehousing environment, materialised view techniques are used and the relation can be vertically partitioned into different sets. If there is c number of clients, then c number of relations will be partitioned. In this paper, we discuss data mining techniques that select most relevant attributes in the relation using predictive partitioned relation. The goal of this research is to locate within a relation those areas of attributes containing tuples relevant to fulfilling the user's requirement. These areas can then be given to a human or automated system for extraction of information, thereby saving a user or query processing system from reading or processing the entire attributes.		Ying Wah Teh;Abu Bakar Zaitun;Sai Peck Lee	2001		10.1109/ICSMC.2001.972030	e-commerce;computer science;data mining;database;superkey;information retrieval;difference set	AI	-27.32564324993702	4.840997645757051	17460
6b7ee33a3dc5ba09074cdbcc06065b58654f43c7	modal similarity		Just as Boolean rules define Boolean categories, the Boolean operators define higher-order Boolean categories referred to as modal categories. We examine the similarity order between these categories and the standard category of logical identity (i.e. the modal category defined by the biconditional or equivalence operator). Our goal is 4-fold: first, to introduce a similarity measure for determining this similarity order; second, to show that such a measure is a good predictor of the similarity assessment behaviour observed in our experiment involving key modal categories; third, to argue that as far as the modal categories are concerned, configural similarity assessment may be componential or analytical in nature; and lastly, to draw attention to the intimate interplay that may exist between deductive judgments, similarity assessment and categorisation.	categorization;kerrison predictor;logical connective;modal logic;semantic similarity;similarity measure;turing completeness	Ronaldo Vigo	2009	J. Exp. Theor. Artif. Intell.	10.1080/09528130802113422		AI	-14.662198326707973	4.297292129897596	17463
1ea25464c972de933237a5249661c056686bff61	an easton like theorem in the presence of shelah cardinals		We show that Shelah cardinals are preserved under the canonical GCH forcing notion. We also show that if GCH holds and F : REG → CARD is an Easton function which satisfies some weak properties, then there exists a cofinality preserving generic extension of the universe which preserves Shelah cardinals and satisfies ∀κ ∈ REG, 2 = F (κ). This gives a partial answer to a question asked by Cody [1] and independently by Honzik [5]. We also prove an indestructibility result for Shelah cardinals.		Mohammad Golshani	2017	Arch. Math. Log.	10.1007/s00153-017-0528-9	mathematical analysis;calculus;mathematics;algorithm	Theory	-8.185296624073157	14.032862662333114	17475
15d8eb2e41e2e9cf6a381b8d1ee820763455f630	the three dimensions of proofs	three dimensions;proof theory;three dimensional;propositional logic;4 dimensional;3 dimensional	In this document, we study a 3-polygraphic translation for the proofs of SKS, a formal system for classical propositional logic. We prove t hat the free3-category generated by this 3-polygraph describes the proofs of classical propositiona l logic modulostructural bureaucracy. We give a 3-dimensional generalization of Penrose diagrams and use it to provide several pictures of a proof. We sketch how local tran sformations of proofs yield a non contrived example of 4-dimensional rewriting. Outline In the first section of this paper, we give a 2-dimensional translation of the formulas of system SKS, a formal system for propositional classical logic [Brünnle r 2004] expressed in the style of the calculus of structures [Guglielmi 2004]. The idea consists in the rep lacement of formulas by circuit-like objects organized in a2-polygraph [Burroni 1993]. This construction is formalize d in theorem 1.4.16. We proceed to section 2, whose purpose is to translate the pro ofs of SKS into3-dimensional objects that form a3-polygraph. There we note that every inference rule can be in terpreted as a directed 3-cell between two circuits. We prove theorem 2.4.3 stating that th e3-polygraph we have built can be equipped with a proof theory which is the same as the SKS one. Section 3 i s where the3-dimensional nature of proofs happens to be useful: theorem 3.3.1 states that the st ructural bureaucracy of SKS [Guglielmi 2004] corresponds to topological moves of 3-cells, called exchange relations. In section 4 we draw several 3-dimensional representations of a given proof. Section 5 is an informal discussion about the 4-dimensional nature of local transformations of 3-dimensional proofs. The final section 6 describes how to adapt the work done here to SLLS, th e calculus of structures-style formalism for linear logic [Straßburger 2003]. 1 The two dimensions of formulas This section gives a2-dimensional translation of SKS formulas, heavily inspire d by the one already known for terms, studied in [Burroni 1993], [Lafont 2003] an d [Guiraud 2004]. After having described the SKS formulas (1.1), we give the in tuition behind their translation into circuit-like objects (1.2): this works by replacing variab les with explicit local resources management operators. This construction requires some theoretical ma terial which is recalled at this moment (1.3). Then we formalize the translation and study its properties ( 1.4): the main purpose of this technical part, that can be skipped on a first approach, is to prove that we can c ompute a canonical representative for circuits corresponding to the same SKS formula (theorem 1.4 .16). Finally we translate the structural congruence on SKS formulas into a congruence on the correspo nding circuits (1.5). Institut de mathématiques de Luminy, Marseille, France ht tp://iml.univ-mrs.fr/∼guiraud	bureaucracy;calculus of structures;congruence of squares;diagram;formal system;linear logic;propositional calculus;rewriting;semantics (computer science)	Yves Guiraud	2006	Ann. Pure Appl. Logic	10.1016/j.apal.2005.12.012	zeroth-order logic;absorption;three-dimensional space;mathematical analysis;discrete mathematics;classical logic;resolution;intuitionistic logic;intermediate logic;mathematics;propositional variable;well-formed formula;proof complexity;algorithm;algebra	Logic	-8.43411131285108	10.186348236557624	17497
b6e314737bc991c2a37b94ae1e8ebccdd68add66	apl graphics and the associative machine	special case;resulting system;apl machine;apl graphics;joint environment;relational facility;general graphic support;limited extension;binary relation;enduring solution;associative machine	A limited extension of APL for binary relations is proposed, and it is shown that general graphic support falls out as a special case. The relational facility can be implemented as an associative machine which coexists with, and complements, the APL machine in a joint environment. The resulting system provides the basis for a robust and enduring solution to the issue of graphics in APL.	apl;graphics	Karl Soop	1987		10.1145/55626.55668	parallel computing;computer science;theoretical computer science;binary relation;algorithm	AI	-23.42607950860271	23.655459188044485	17577
873f08cabb981c78d54b6b8e18c62e445ae7dbe7	computational phonology - part i: foundations		Computational phonology approaches the study of sound patt erns in the world’s languages from a computational perspective. This article e xplains this perspective and its relevance to phonology. A restrictive, universal prope rty of phonological patterns— they are regular—is established, and the hypothesis that th ey are subregular is presented. This article is intended primarily for phonologists who are curious about computational phonology, but do not have a rigorous background in mathematics or computation. However, it is also informative for readers with a ba ckground in computation and the basics of phonology, and who are curious about what co mputational analysis offers phonological theory. 1 What is Computational Phonology? Computational phonology is formal phonology, and formal ph onology is theoretical phonology. Computational phonology is not concerned with t he implementation of phonological theories on computers (though that may be a byproduc t of computational analysis). The primary concern of computational phonology is the conte nt of the theory itself. Computational Phonology – Part I: Foundations This article andComputational Phonology Part II: Grammars, Learning and t he Future present three important contributions of computational ph onology. First, computational analysis of phonological formalisms reveals that th e similarities between generative theories of phonology like Optimality Theory (OT) (Prince a nd Smolensky, 2004) and the one found inThe Sound Pattern of English (SPE) (Chomsky and Halle, 1968) outweigh their differences. This is in part because computational an ysis identifies exactly how different generative theories define and combine the indivi dual factors that make up a language’s phonology. It is also in part because of the second co tribution: computational analysis reveals a restrictive, universal property of phon ol gical patterns: they are REGULAR. What are regular patterns? A definition is given in (1). (1) A pattern is regular if and only if (iff) it is possible to p artition the set of logically possible words intofinitelymany blocks such that a. all words in any block either obey the pattern or all do not, and b. for any block, if it contains words w1 andw2 then, for all wordsv, there is a block which contains bothw1v andw2v. For example, consider the pattern given by words composed on ly of CV syllables. This pattern is regular. To see why, partition all logically poss ible words into three blocks: Block 1 contains all words which either start with V or which c ontain consecutive CC or VV sequences; Block 2 contains other words which end in V; a nd Block 3 contains the remaining words. In other words, logically possible words l ike VCV, CVVC, CVCC belong to Block 1, words like CVCV belongs to Block 2 and words like CVbelong to block 3. Clearly all words in Block 2 belong to the language and all wor ds in Blocks 1 and 3 do not, satisfying condition (1-a). It is not hard to see that (1-b) i s satisfied as well. To illustrate,	articulatory phonology;computer;information;internet key exchange;kaplan–meier estimator;logical possibility;prince;regular expression;relevance;theory of computation;verification and validation	Jeffrey Heinz	2011	Language and Linguistics Compass	10.1111/j.1749-818X.2011.00269.x	natural language processing;computer science;linguistics;communication;phonology	NLP	-8.49289748382362	6.424234855156832	17610
0baed68627ba93b446225f95d7fd1a576c5712d0	explaining sldnf resolution with non-normal defaults	prolog;intelligence artificielle;logical programming;logica defecto;programmation logique;artificial intelligence;logique defaut;inteligencia artificial;default logic;programacion logica	This paper defines a defauit iogic interpretation for normai programs thiat hias the foilowing major characteristics. First, it directiy captures the true nature of SLDNF resoiution as an extension of SLD resoiution. Second, it is semanticaliy convincing, but it requires neither an elaborated nonstandard interpretation nor a radical rewriting of the program clauses that would maice it difficult to understand their meaning. Last, it extends known results for stratified normal programs to programs that satisfy a weaker condition.	default logic;requirement;rewriting;sld resolution;theory	Marco A. Casanova;Andrea S. Hemerly;Ramiro A. de T. Guerreiro	1991	IBM Journal of Research and Development	10.1147/rd.363.0347	computer science;artificial intelligence;machine learning;default logic;prolog;algorithm	PL	-13.721313058152285	7.136372614251759	17643
5b554021d3264c4a92678988c696d320b5b555e9	conditionally knowing what.		Classic epistemic logic focuses on propositional knowledge expressed by “knowing that” operators. However, there are various types of knowledge used in natural language, in terms of “knowing how”, “knowing whether”, “knowing what”, and so on. In [10], Plaza proposed an intuitive know-what operator which was generalized in [16] by introducing a condition. The latter know-what operator can express natural conditional knowledge such as “I know what your password is, if it is 4-digits”, which is not simply a material implication. Essentially this know-what operator packages a first-order quantifier and an S5-modality together in a non-trivial way, thus making it hard to axiomatize. In [16] an axiomatization is given for the single-agent epistemic logic with both know-that and know-what operators, while leaving axiomatizing the multi-agent case open due to various technical difficulties. In this paper, we solve this open problem. The completeness proof is highly non-trivial, compared to the singleagent case, which requires different techniques inspired by first-order intensional logic.	axiomatic system;epistemic modal logic;first-order logic;first-order predicate;intensional logic;multi-agent system;natural language;password;quantifier (logic)	Yanjing Wang;Jie Fan	2014			descriptive knowledge;open problem;discrete mathematics;material implication;password;operator (computer programming);algorithm;intensional logic;natural language;computer science;epistemic modal logic	AI	-15.264574848842422	6.952116555140361	17763
584de3c5afca7cdd4ea832c185c9c60fd5ec746b	an efficient context-free parsing algorithm with semantic actions	context free;computational linguistic;context free grammar;production rule	As M. Tomita [5] has demonstrated, the LR parsing algorithm can be extended so that it will handle acyclic context-free grammars in a very efficient way, producing a parse forest, a set of all possible parsing trees for a given input sentence. In this paper we aim to show how Tomita's parsing algorithm can be extended to support semantic actions, that is, procedural segments associated with grammar production rules, that are executed contextually during the parsing procedure.	algorithm	Marco Piastra;Roberto Bolognesi	1991		10.1007/3-540-54712-6_239	natural language processing;computer science;parsing;s-attributed grammar;extended affix grammar;linguistics;programming language;top-down parsing	NLP	-21.424816110780316	16.162657335413083	17811
619613f27e58ebb3ce4b76a7b53b62190781b542	a decidability result about sufficient-completeness of axiomatically specified abstract data types	decidability result;abstract data type	The problem of deciding whether an axiomatic specification of an abstract data type is sufficiently-complete is known to be in general unsolvable. Regarding axioms as directed rewrite rules instead of symmetric equations a specification defines a reduction relation on terms. It is proved that in the subclass of left-linear axiomatic specifications the property of sufficient-completeness is decidable, if the corresponding reduction relation is normalizing and confluent. The presented algorithm can also be used to determine a set of constructors for a specified data type.	abstract data type	Tobias Nipkow;Gerhard Weikum	1983		10.1007/BFb0009650	discrete mathematics;type family;data type;computer science;mathematics;programming language;abstract data type;algorithm	Logic	-11.0007639554776	17.43098660144985	17830
13d3f89c6f3382b225ec037e31580ab3ce8a70a6	a formalisation of semantic schema integration	databases;schema transformation;transformational generative grammar;semantic integration;linguistic theory;semantics;schema integration;data model;mathematical formulas;schema equivalence;models;entity relationship;knowledge base	Several methodologies for the semantic integration of databases have been proposed in the literature. These often use a variant of the Entity-Relationship (ER) model as the common data model. To aid the schema conforming, merging and restructuring phases of the semantic integration process, various transformations have been deened that map between ER representations which are in some sense equivalent. Our work aims to formalise the notion of schema equivalence and to provide a formal underpinning for the schema integration process. We show how transformational, mapping and behavioural schema equivalence may all be regarded as variants of a more general deenition of schema equivalence. We propose a semantically sound set of primitive transformations and show how they can be used to express the transformations commonly used during the schema integration process. We diierentiate between transformations which apply to any instance of a schema and those which require knowledge-based reasoning since they apply only for certain instances. This distinction could serve to enhance the performance of transformation tools since it identiies which transformations must be veriied by inspection of the schema extension. It also serves to identify when intelligent reasoning is required during the schema integration process.	data model;database;entity–relationship model;global variable;knowledge-based systems;semantic integration;turing completeness	Peter McBrien;Alexandra Poulovassilis	1998	Inf. Syst.	10.1016/S0306-4379(98)00014-3	natural language processing;idef1x;schema migration;theoretical linguistics;knowledge base;formula;information schema;semantic integration;schema;semi-structured model;logical schema;entity–relationship model;data model;computer science;knowledge management;three schema approach;conceptual schema;document structure description;star schema;xml schema;database;semantics;document schema definition languages;transformational grammar;superkey;database schema	DB	-31.228525377078952	12.32162690267525	17849
d0840f9204298d1ce1c4208b45f39287f535b44c	dynamic evaluation forms using declarative modeling		This paper reports preliminary experiences using the Dynamic Condition Response (DCR) graphs declarative process notation to specify dynamic evaluation forms, and using an execution engine for that notation to subsequently “run” the form. The DCR notation was able to express all the patterns of behaviour necessary for a real case: a post-hoc evaluation form for a Danish arbitration court, “Voldgiftsnævnet for Anlæg og Byggeri”. However, some patterns were somewhat cumbersome to express.	declarative programming;hoc (programming language);semantics (computer science);visual basic for applications	Rasmus Strømsted;Hugo A. López;Søren Debois;Morten Marquard	2018			computer science	PL	-27.864595981307804	18.859050007745026	17896
44d5a242ef36e2c4ea062b5b25a20019bed8c1f6	on d0l power series	power series;formal series;serie formelle;language theory;regular language;teoria lenguaje;rational series;lenguaje racional;theoreme skolem mahler lech;d0l power series;langage rationnel;rational number;slender language;serie exponentielle d0l;decidibilidad;langage mince;decidabilite;serie exponentielle;serie rationnelle;theorie langage;decidability;serie formal;skolem mahler lech theorem	We study D0L power series. We show how elementary morphisms introduced by Ehrenfeucht and Rozenberg can be used in connection with power series, characterize the sequences of rational numbers and integers which can be appear as coe cients in D0L power series and establish various decidability results. TUCS Research Group Mathematical Structures of Computer Science	computer science	Juha Honkala	2000	Theor. Comput. Sci.	10.1016/S0304-3975(98)00338-7	decidability;regular language;computer science;philosophy of language;calculus;pure mathematics;mathematics;formal power series;power series;algorithm;rational number	Theory	-5.22900915298276	19.045334491543	17957
90b1e9d4539b01eadb34dbe1bcc54910d078f903	a system for conceptual pathway finding and deductive querying		We describe principles and design of a system for knowledge bases applying a natural logic. Natural logics are forms of logic which appear as stylized fragments of natural language sentences. Accordingly, such knowledge base sentences can be read and understood directly by a domain expert. The system applies a graph form computed from the input natural logic sentences. The graph form generalizes the usual partialorder ontological sub-class structures by accommodation of affirmative sentences comprising recursive phrase structures. In this paper we focus on the logical inference rules for extending the concept graph form enabling deductive querying as well as computation of pathways between the concepts mentioned in the sentences.	algorithm;computation;experiment;gene regulatory network;gödel's ontological proof;heuristic;knowledge base;natural language;pathfinding;precomputation;prototype;recursion;subject-matter expert;subsumption architecture;transitive closure;vertex-transitive graph	Troels Andreasen;Henrik Bulskov;Jørgen Fischer Nilsson;Per Anker Jensen	2015		10.1007/978-3-319-26154-6_35	rule of inference;subject-matter expert;stylized fact;data mining;knowledge base;computation;natural language processing;recursion;natural language;computer science;phrase;artificial intelligence	AI	-19.552278076010616	6.4009125796117115	17985
988acc2cae9b8ae8d5061a8993070992fe63d12d	thresholded semantic framework for a fully integrated fuzzy logic language		Abstract This work proposes a declarative semantics based on a fuzzy variant of the classical notion of least Herbrand model for the so-called FASILL programming language (acronym of “Fuzzy Aggregators and Similarity Into a Logic Language”) which has been recently designed and implemented for coping with implicit/explicit truth degree annotations, a great variety of connectives and unification by similarity. Also, we define an immediate consequence operator that allows us to give a fixpoint characterization of the least Herbrand model for a FASILL program. A remarkable aspect of our declarative and fixpoint semantics is the fact that both have been enriched with the use of thresholds thanks to the natural ability of the underlying fuzzy language for managing such constructs. Moreover, we have connected both semantics with an operational one, which also manages thresholds in an efficient way, by proving the correctness of the whole semantic framework.	algorithm;correctness (computer science);declarative programming;fixed point (mathematics);fuzzy logic;herbrand award;logical connective;multi-adjoint logic programming;non-monotonic logic;observable;operational semantics;pl/i;programming language;turing completeness;unification (computer science)	Pascual Julián Iranzo;Ginés Moreno;Jaime Penabad	2017	J. Log. Algebr. Meth. Program.	10.1016/j.jlamp.2017.08.002	fuzzy control language;mathematics;fuzzy logic;natural language processing;operational semantics;correctness;semantics;well-founded semantics;action semantics;logic programming;artificial intelligence	PL	-14.854630094616704	12.188055128585045	18014
19627018f766a947e5e5a8366493c31da97adca9	holcf: higher order logic of computable functions	domain theory;higher order;fixed point;type classes;higher order logic	This paper presents a survey of HOLCF a higher order logic of computable functions The logic HOLCF is based on HOLC a variant of the well known higher order logic HOL which o ers the additional concept of type classes HOLCF extends HOLC with concepts of domain theory such as complete partial orders continuous functions and a xed point operator With the help of type classes the extension can be formulated in a way such that the logic LCF constitutes a proper sublanguage of HOLCF Therefore techniques from higher order logic and LCF can be combined in a fruitful manner avoiding drawbacks of both logics The development of HOLCF was entirely conducted within the Isabelle system	computable function;domain theory;hol (proof assistant);isabelle;logic of computable functions;sublanguage;type class;whole earth 'lectronic link	Franz Regensburger	1995		10.1007/3-540-60275-5_72	higher-order logic;computer science;intermediate logic;programming language;least fixed point	AI	-15.622709866757676	18.00706308021271	18053
c510b89c4045621388edf06df84169ac9d97a46e	two finite specifications of a queue	proof theory;data type;universiteitsbibliotheek	It is known that a queue is not finitely definable in ACP with handshaking communication (Baeten and Berg&a, 1988). In this paper, two finite specifications of a queue in ACP with abstraction and handshaking are proved correct relative to a standard specification of a queue that employs an infinite data type for representing its contents. The proofs are given in the proof theory of &RL, and the only ‘T-laws’ used are XT = x and X(Z(JJ + z) + y) = x(v + z). Therefore the proofs are adequate for both ‘branching bisimilarity’ and ‘observation equivalence’. Additionally, it is shown that standard concurrency follows from RSP for a class of processes guardedly specifiable in ACP with abstraction.	airline control program (acp);bisimulation;branching factor;concurrency (computer science);handshaking;usb on-the-go	Marc Bezem;Alban Ponse	1997	Theor. Comput. Sci.	10.1016/S0304-3975(96)00257-5	combinatorics;discrete mathematics;data type;computer science;theoretical computer science;proof theory;mathematics;programming language;algorithm	Logic	-9.332948934853249	22.97284015135055	18076
14dab4e7263d93cdddec45eef718afb7a42b0a7c	database-driven distributed 3d simulation	geoinformation systems database driven distributed 3d simulation world model standardized interfaces object identification standard decentralized methods simulation model data schema central database object data simulation clients object oriented data management 3d real time simulation techniques database driven approach industrial automation space robotics gis;object oriented modeling data models solid modeling distributed databases synchronization load modeling;real time systems digital simulation distributed databases object oriented databases;distributed databases;object oriented databases;digital simulation;real time systems	Distributed 3D simulations are used in various fields of application like geo information systems (GIS), space robotics or industrial automation. We present a new database-driven approach that combines 3D real-time simulation techniques with object-oriented data management. It consists of simulation clients that replicate from a central database object data as well as the data schema itself. The central database stores static and dynamic parts of a simulation model, distributes changes caused by the simulation, and logs the simulation run. Compared to standard decentralized methods this approach has several advantages like persistence for state and course of time, object identification, standardized interfaces for simulation, modeling and evaluation, as well as a consistent data schema and world model for the overall system, which at the same time serves as a means for communication.	automation;database;geographic information system;persistence (computer science);real-time locating system;robotic spacecraft;robotics;self-replicating machine;simulation	Martin Hoppen;Michael Schluse;Jürgen Roßmann;Björn Weitzig	2012	Proceedings Title: Proceedings of the 2012 Winter Simulation Conference (WSC)	10.1109/WSC.2012.6465091	data modeling;semi-structured model;computer science;theoretical computer science;database model;data mining;database;distributed object;database schema;distributed database;database design	Robotics	-33.27346363420221	12.743583615357188	18092
318a02cd5c33f1aa6e85123abcc3da7fe4a311c3	quantifiers types resolution in nl software requirements	natural languages semantics syntactics pragmatics markov random fields joining processes;quantifier type resolution;quantifier type resolution nl quantifiers markov logic;pragmatics;formal specification;markov logic;markov random fields;semantics;natural languages;nl quantifiers;syntactics;natural language processing formal logic formal specification markov processes;formal logic;joining processes;markov processes;natural language processing;markov logic nl software requirements quantifier type resolution natural language quantifier classification syntactic expression ambiguous quantification natural language quantification natural language sentences first order logic complex sentence classification	"""Natural language quantifiers can be classified according to their semantic type in addition to their syntactic expression. Quantification in Natural language (NL) has two types, ambiguous quantification and Unambiguous quantification. Unambiguous quantification is very simple and also called exact quantification, but ambiguous quantification is complex and also called inexact quantification. Inexact quantifiers include """"many, much, a lot of, several, some, any, a few, little, fewer, fewest, Less, greater, at least, at most, more, exactly"""". To identify the problems of Natural language Quantification, convert these Natural Language sentences into First order logic by attaching weights and classify these complex sentences by using Markov Logic."""	ambiguous grammar;first-order logic;first-order predicate;formal language;markov chain;markov logic network;nl (complexity);natural language;natural language processing;software requirements;unambiguous turing machine	Mehreen Saba;Imran Sarwar Bajwa	2013	2013 12th Mexican International Conference on Artificial Intelligence	10.1109/MICAI.2013.49	natural language processing;computer science;formal specification;mathematics;semantics;markov process;natural language;logic;existential quantification;algorithm;pragmatics	DB	-8.635259475494522	19.51460514043608	18118
cc6bea63807090faab5bc7f32c7d920036b3fd93	formal semantics for perceptual classification		A formal semantics for low-level perceptual aspects of meaning is presented, tying these together with the logical-inferential aspects of meaning traditionally studied in formal semantics. The key idea is to model perceptual meanings as classifiers of perceptual input. Furthermore, we show how perceptual aspects of meaning can be updated as a result of observing language use in interaction, thereby enabling fine-grained semantic plasticity and semantic coordination. This requires a framework where intensions are (1) represented independently of extensions, and (2) structured objects which can be modified as a result of learning. We use Type Theory with Records (TTR), a formal semantics framework which starts from the idea that information and meaning is founded on our ability to perceive and classify the world, i.e., to perceive objects and situations as being of types. As an example of our approach, we show how a simple classifier of spatial information based on the Perceptron can be cast in TTR.	high- and low-level;inferential theory of learning;perceptron;semantics (computer science);type theory	Staffan Larsson	2015	J. Log. Comput.	10.1093/logcom/ext059	formal system;formal semantics;syntax;formal semantics;operational semantics;denotational semantics;computational semantics	PL	-16.728401230076123	5.924351653977787	18159
7f3deda8653959368b739f4b5f969893351775d4	building an object-oriented database system, the story of o2					1992				DB	-31.401065963264774	10.15224497919175	18183
f5115ba88b1d8985cb5297d521abdbd3a596ca20	inductive logic programming: from machine learning to software engineering	inductive logic programming;software engineering;logic programming;machine learning	From the Publisher:#R##N#Although Inductive Logic Programming (ILP) is generally thought of as a research area at the intersection of machine learning and computational logic, Bergadano and Gunetti propose that most of the research in ILP has in fact come from machine learning, particularly in the evolution of inductive reasoning from pattern recognition, through initial approaches to symbolic machine learning, to recent techniques for learning relational concepts. In this book they provide an extended, up-to-date survey of ILP, emphasizing methods and systems suitable for software engineering applications, including inductive program development, testing, and maintenance. #R##N#Inductive Logic Programming includes a definition of the basic ILP problem and its variations (incremental, with queries, for multiple predicates and predicate invention capabilities), a description of bottom-up operators and techniques (such as least general generalization, inverse resolution, and inverse implication), an analysis of top-down methods (mainly MIS and FOIL-like systems), and a survey of methods and languages for specifying inductive bias. #R##N#Logic Programming series	inductive logic programming;machine learning;software engineering	Francesco Bergadano;Daniele Gunetti	1995			multi-task learning;inductive bias;statistical relational learning;horn clause;computer science;theoretical computer science;machine learning;functional logic programming;computational logic;inductive transfer;programming paradigm;inductive programming;prolog;logic programming;algorithm	SE	-20.308456292348026	12.817716815304966	18188
f090895e52e8eae0d7c0300f055562085d9adbf1	logiccrowd: crowd-powered logic programming based mobile applications			logic programming	Jurairat Phuttharak;Seng Wai Loke	2018	Comput. J.	10.1093/comjnl/bxx027	theoretical computer science;inductive programming;functional reactive programming;programming paradigm;declarative programming;functional logic programming;computer science;logic programming;programming domain;reactive programming	EDA	-24.058001127520424	21.51371803198219	18202
d5a7c70d4033055b8fd09553c93620d7dd86cc6d	a method to share word knowledge of dependability case	dependability	Abstract System developers should explain system errors sufficiently, during a system failure situation. This is an essential act in order to verify the dependability of systems. Dependability Case (D-Case) is one method for confirming systems of dependability suffciently, against a system failure situations. However, in D-Case, we could not clearly describe the relationship of words (people, objects, activities) within its (D-Case) nodes. For this reason, we introduce a new way to define Word Relationship Diagram (WRD) that describes the relationship of words within D-Case and propose our conversion rules from Dependability Case to Word Relationship Diagram (D2W rule) and vice versa (from Word Relationship Diagram to Dependability Case (D2W rule)). In addition to this, (1) we created a rule for the relationship of words for D-Case. (2) We applied the rules to the Dependability Case (D-Case) and the Word Relationship Diagram (WRD) of a train operation system.	dependability	Masanori Matsumura;Vaise Patu;Yutaka Matsuno;Shota Takama;Tatsuya Tokuno;Shuichiro Yamamoto	2013		10.1016/j.procs.2013.09.076	real-time computing;computer science;artificial intelligence;data mining;algorithm	NLP	-30.836093018141582	15.84568025662598	18205
5fd8229e2a5f073c1284768c54029975c344b18a	a denotational semantics of inheritance and its correctness	herencia;lenguaje programacion;object oriented language;semantica denotacional;programming language;heritage;operational semantics;recursividad;denotational semantic;recursivite;object oriented;denotational semantics;langage programmation;oriente objet;inheritance;recursivity;orientado objeto;semantique denotationnelle	Abstract   This paper presents a denotational model of inheritance. The model is based on an intuitive motivation of inheritance as a mechanism for deriving modified versions of recursive definitions. The correctness of the model is demonstrated by proving it equivalent to an operational semantics of inheritance based upon the method lookup algorithm of object-oriented languages.	correctness (computer science);denotational semantics	William R. Cook;Jens Palsberg	1994	Inf. Comput.	10.1006/inco.1994.1090	action semantics;computer science;theoretical computer science;programming language;denotational semantics of the actor model;object-oriented programming;operational semantics;denotational semantics;algorithm	PL	-22.065197475029073	22.656562122189495	18231
512cda3bd8646ec748c759edec875af6d28d93ad	"""the """"hilbert method"""" for solving transducer equivalence problems"""		In the past decades, classical results from algebra, including Hilbert’s Basis Theorem, had various applications in formal languages, including a proof of the Ehrenfeucht Conjecture, decidability of HDT0L sequence equivalence, and decidability of the equivalence problem for functional tree-to-string transducers. In this paper, we study the scope of the algebraic methods mentioned above, particularily as applied to the equivalence problem for register automata. We provide two results, one positive, one negative. The positive result is that equivalence is decidable for MSO transformations on unordered forests. The negative result comes from a try to extend this method to decide equivalence on macro tree transducers. We reduce macro tree transducers equivalence to an equivalence problem for some class of register automata naturally relevant to our method. We then prove this latter problem to be undecidable.		Adrien Boiret;Radoslaw Piórkowski;Janusz Schmude	2018	CoRR		conjecture;discrete mathematics;equivalence (measure theory);undecidable problem;algebraic number;decidability;formal language;macro;mathematics	Logic	-6.926599247610917	16.142863253849747	18234
5bf88a104d780df1bb31afd8b55056dd3d91d744	on composing concurrent logic processes	temporal logic;technology and engineering;logical process	This paper discusses two methodological approaches for combining concurrent programs written in a modular way. The rst one is based on the composition of traces. It is intuitive and simple but evidences the need for a more abstract approach. The second one meets this need. It composes speciications written in a UNITY-like temporal logic, and rests on Abadi and Lamport's composition principle ((1]). The two methodologies are explained using the concurrent logic programming language Log but can actually be applied to other concurrent logic	concurrent logic programming;programming language;temporal logic;tracing (software);unity	Jean-Marie Jacquet;Koen De Bosschere	1995			description logic;temporal logic;interval temporal logic;computer science;artificial intelligence;theoretical computer science;multimodal logic;algorithm;temporal logic of actions	Logic	-17.20699353560582	18.40237287236349	18266
f75e442b38495292dbcb2d4d6f2574f2fe7a9c96	on the development of a formal methodology for knowledge representation in defeasible logic programming	argumentative systems;non monotonic reasoning;knowledge representation;defeasible reasoning	Defeasible Logic Programming (DeLP) is a formalism able to represent incomplete and potentially contradictory information that combines logic programming with defeasible argumentation. In the past few years, this formalism has been applied to real world scenarios with encouraging results. Not withstanding, the outcome one may obtain in this or any other argumentative system is directly related to the decisions (or lack thereof) made during the phase of knowledge representation. In addition, this is exacerbated by the usual lack of a formal methodology able to assist the knowledge engineer during this critical phase.#R##N##R##N#In this article, we propose a formal methodology for knowledge representation in DeLP, that defines a set of guidelines to be used during this phase. Our methodology results in an key tool to improve DeLP's applicability to concrete domains.	defeasible logic;defeasible reasoning;knowledge representation and reasoning;logic programming	Alejandro G. Stankevicius;Marcela Capobianco	2012		10.1007/978-3-642-34459-6_1	knowledge representation and reasoning;computer science;knowledge management;artificial intelligence;defeasible reasoning;algorithm	AI	-16.53077650681083	5.848880529412027	18293
17c212f048d3f4f631e3583d49668c339ab1d7cb	on type systems for object-oriented database programming languages	oodb;programming language design;programming language;satisfiability;type checking;typing;object oriented;object oriented programming languages;object oriented database programming language;object oriented database;uniform consistency;oodbpl;type system	"""The concept of an object-oriented database programming language (OODBPL) is appealing because it has the potential of combining the advantages of object orientation and database programming to yield a powerful and universal programming language design. A uniform and consistent combination of object orientation and database programming, however, is not straightforward. Since one of the main components of an object-oriented programming language is its type system, one of the first problems that arises during an OODBPL design is related to the development of a uniform, consistent, and theoretically sound type system that is sufficiently expressive to satisfy the combined needs of object orientation and database programming.The purpose of this article is to answer two questions: """"What are the requirements that a modern type system for an object-oriented database programming language should satisfy?"""" and """"Are there any type systems developed to-date that satisfy these requirements?"""". In order to answer the first question, we compile the set of requirements that an OODBPL type system should satisfy. We then use this set of requirements to evaluate more than 30 existing type systems. The result of this extensive analysis shows that while each of the requirements is satisfied by at least one type system, no type system satisfies all of them. It also enables identification of the mechanisms that lie behind the strengths and weaknesses of the current type systems."""	compiler;database;programming language;requirement;type system	Yuri Leontiev;M. Tamer Özsu;Duane Szafron	2002	ACM Comput. Surv.	10.1145/592642.592643	fourth-generation programming language;first-generation programming language;type conversion;very high-level programming language;unit type;uniqueness type;programming domain;data type;reactive programming;type safety;computer science;object;theoretical computer science;object-relational mapping;functional logic programming;database;object type;programming paradigm;low-level programming language;inductive programming;fifth-generation programming language;programming language;object-oriented programming;programming language specification	PL	-24.205783079336307	26.363101768087542	18312
48c05884733e4fd367646afcddbd328c83707a3a	denotational semantics for polarized (but-non-constrainted) lk by means of the additives	teoria demonstracion;theorie preuve;cut elimination;semantica denotacional;proof theory;logique mathematique;computability;logica matematica;mathematical logic;denotational semantic;informatique theorique;calculabilite;denotational semantics;linear logic;semantique denotationnelle;calculabilidad;computer theory;informatica teorica	"""Following the idea that a classical proof is a superimposition of`constructive' proofs, we use the linear connective \&"""" to deene an embedding P a of polarized but non-constrained LK in linear logic, which induces a denotational semantics. A classical cut-elimination step is now a cut-elimination step in one of thèconstructive slices' superimposed by the classical proof."""	denotational semantics;linear logic;logical connective	Lorenzo Tortora de Falco	1997		10.1007/3-540-63385-5_51	linear logic;mathematical logic;discrete mathematics;computer science;proof theory;mathematics;computability;programming language;denotational semantics;algorithm	PL	-13.120345418193152	15.747302125207938	18327
ceee91549362354530044cefedc8296b2e97574a	views as the security objects in a multilevel secure relational database management system	security of data data structures relational databases;consistency object views security objects multilevel secure relational database management system security policy relational dbms views completeness;data security relational databases information retrieval environmental management certification power system security database systems access control multilevel systems database languages;data structures;relational database management system;relational databases;security policy;security of data;multilevel security	A mandatory security policy for a multilevel secure relational DBMS using views as the security objects is presented. The advantages and disadvantages of this approach are examined. A method of ensuring the completeness and consistency of the set of secure views is described, as well as an approach to implementing views as the security objects. >	multilevel security;relational database management system	Jackson Wilson	1988		10.1109/SECPRI.1988.8099	computer security model;cloud computing security;information schema;relational database management system;relational model;security information and event management;data structure;relational database;computer science;security policy;information security;database model;data mining;database;security service;computer security;object-relational impedance mismatch;database design	Security	-31.752498052044437	10.71384150105347	18344
bb50ff4377f75f99614fb252870d24ba6a371b24	implementation of object-relational dbmss in a relational database course	cs1;course design;distance education;empirical analysis;relational database;teaching methodology;modeling language;large scale courses;streaming media;object relational;on line learning	Object-relational DBMS was gradually added as a new topic to the author's database course in response to the rapid changes in DBMS technology in the real world. Implementation of ORDBMS technology in a traditional relational database course had significant impacts on the database curriculum. As an outcome, students were able to solve problems that could not be solved well in a relational database. ORDBMS was implemented with Universal Modeling Language (UML) and the Oracle 8i server. Course design, teaching methodology, class activities and the outcome of the course are discussed.	object-relational database;relational database management system;server (computing);unified modeling language	Ming Wang	2001		10.1145/364447.364715	distance education;data definition language;sql;database server;entity–relationship model;relational database;computer science;database model;teaching method;data mining;database;modeling language;programming language;view;world wide web;database schema;object-relational impedance mismatch;database design;component-oriented database	DB	-33.106495752058684	11.209030959529214	18404
1a17ac2eb84f3b80630011caf1f0b6f066b1fa3b	bytecode model checking: an experimental analysis	virtual machine;java bytecode;experimental analysis;temps polynomial;time complexity;langage java;program verification;machine virtuelle;verificacion programa;model checking;polynomial time;on the fly;lenguaje java;verification enumerative;verification programme;maquina virtual;java language;tiempo polinomial	Java bytecode verification is traditionally performed by a polynomial time dataflow algorithm. We investigate an alternative based on reducing bytecode verification to model checking. Despite an exponential worst case time complexity, model checking type-correct bytecode is polynomial in practice when carried out using an explicit state, onthe-fly model checker like Spin. We investigate this theoretically and experimentally and explain the practical advantages of this alternative.	acm transactions on programming languages and systems;algorithm;best, worst and average case;blog;correctness (computer science);data-flow analysis;dataflow architecture;domain-driven design;european joint conferences on theory and practice of software;experiment;fixed point (mathematics);fixed-point iteration;formal methods;formal specification;hol (proof assistant);isabelle;java bytecode;java virtual machine;lecture notes in computer science;model checking;polynomial;program analysis;requirement;spin;schmidt decomposition;springer (tank);stata;state space;subroutine;symposium on principles of programming languages;time complexity;transition system;type safety;type system;www;world wide web	David A. Basin;Stefan Friedrich;Marek Gawkowski;Joachim Posegga	2002		10.1007/3-540-46017-9_6	time complexity;real-time computing;computer science;theoretical computer science;operating system;programming language;algorithm	Logic	-22.20585710526302	30.782591076677317	18410
2dc8d45122f938bc895207c637368b653bef8faa	satisfying maintenance goals	course of action;look ahead;formal semantics;satisfiability;rational agent	A rational agent derives its choice of action from its beliefs and goals. Goals can be distinguished into achievement goals and maintenance goals. The aim of this paper is to define a mechanism which ensures the satisfaction of maintenance goals. We argue that such a mechanism requires the agent to look ahead, in order to make sure that the execution of actions does not lead to a violation of a maintenance goal. That is, maintenance goals may constrain the agent in choosing its actions. We propose a formal semantics of maintenance goals based on the notion of lookahead, and analyze the semantics by proving some properties. Additionally, we discuss the issue of achievement goal revision, in case the maintenance goals are so restrictive that all courses of action for satisfying achievement goals will lead to a violation of maintenance	parsing;rational agent;semantics (computer science)	Koen V. Hindriks;M. Birna van Riemsdijk	2007		10.1007/978-3-540-77564-5_6	rational agent;knowledge management;artificial intelligence;formal semantics;satisfiability	AI	-17.45115672264486	5.827501442227361	18453
6d31405a6f8046421f556fe4e94719f8bdfa3465	towards a uniform structured representation for application generation		This paper presents an overview of a representation scheme being developed to define static data structures, the restrictions that establish the data model's integrity constraints, and the computational models that operate on the data. The context for this discussion is the role of a uniform structured representation for application generation in which semantically-rich expressions of the application's behavior are transformed into operational models with efficient performance.		Bruce I. Blum	1991	International Journal of Software Engineering and Knowledge Engineering	10.1142/S0218194091000068	computer science;theoretical computer science;data mining;engineering drawing	SE	-30.572557004449965	12.207081340975561	18462
e36cafeddec261acc2b5e9ef32c5fb2aa5b57b84	data-driven loop invariant inference with automatic feature synthesis		We present LOOPINVGEN, a tool for generating loop invariants that can provably guarantee correctness of a program with respect to a given specification. We extend the data-driven approach to inferring sufficient loop invariants from a collection of program states. In contrast to existing data-driven techniques, LOOPINVGEN is not restricted to a fixed set of features – atomic predicates that are composed together to build complex loop invariants. Instead, we start with no initial features, and use program synthesis techniques to grow the set on demand. We compare with existing static and dynamic tools for loop invariant inference, and show that LOOPINVGEN enables a less onerous and more expressive form of inference.		Saswat Padhi;Todd D. Millstein	2017	CoRR		program synthesis;for loop;correctness;machine learning;inference;invariant (mathematics);data-driven;artificial intelligence;mathematics;loop invariant	PL	-14.642626345641649	26.757760407367286	18484
d02ce0df0c3ee6b365f19bf4075f7cb6de073985	aalta: an ltl satisfiability checker over infinite/finite traces	temporal logic;satisfiability;model checking	Linear Temporal Logic (LTL) is been widely used nowadays in verification and AI. Checking satisfiability of LTL formulas is a fundamental step in removing possible errors in LTL assertions. We present in this paper Aalta, a new LTL satisfiability checker, which supports satisfiability checking for LTL over both infinite and finite traces. Aalta leverages the power of modern SAT solvers. We have conducted a comprehensive comparison between Aalta and other LTL satisfiability checkers, and the experimental results show that Aalta is very competitive. The tool is available at www.lab205.org/aalta.	artificial intelligence;boolean satisfiability problem;formal verification;linear temporal logic;tracing (software)	Jianwen Li;Yinbo Yao;Geguang Pu;Lijun Zhang;Jifeng He	2014		10.1145/2635868.2661669	model checking;linear temporal logic;temporal logic;computer science;programming language;algorithm;satisfiability	Logic	-14.251199160747499	26.097053491470596	18486
34b0c67735ef5d5f2b7673c50c2f9954259a9e1a	fuzzy extensions for relationships in a generalized object model	modelizacion;representacion conocimientos;base donnee;perspectiva;fuzzy set;incertidumbre;uncertainty;interaction;logique floue;database;base dato;semantics;recommandation;conjunto difuso;logica difusa;ensemble flou;semantica;semantique;perspective;fuzzy logic;modelisation;systeme incertain;object oriented;recomendacion;oriente objet;recommendation;interaccion;incertitude;knowledge representation;sistema incierto;representation connaissances;modeling;orientado objeto;uncertain system;object model	Abstract#R##N##R##N#Numerous approaches for introducing and managing uncertainty in object-oriented models have been proposed. This paper examines various semantics of uncertainty and the interaction with three kinds of relationships inherent to object models: for instance-of, a-kind-of, and a category. A generalized object model incorporating the perspective of semantic data modeling, artificial intelligence, and database systems is the basis for the recommendations for fuzzy extensions to these three kinds of relationships. © 2001 John Wiley & Sons, Inc.		Valerie V. Cross	2001	Int. J. Intell. Syst.	10.1002/int.1038	fuzzy logic;interaction;perspective;systems modeling;object model;uncertainty;computer science;artificial intelligence;machine learning;mathematics;semantics;fuzzy set;object-oriented programming;algorithm	DB	-29.66470498916263	12.872054381517303	18532
22e76cd11d915292d6517b9499fef264d4345b75	typing artifacts in megamodeling	model transformation;megamodeling;type system	Model management is essential for coping with the complexity introduced by the increasing number and varied nature of artifacts involved in model-driven engineering-based projects. Global model management (GMM) addresses this issue by enabling the representation of artifacts, particularly transformation composition and execution, within a model called a megamodel. Type information about artifacts can be used for preventing type errors during execution. Built on our previous work, in this paper we present the core elements of a type system for GMM that improves its original typing approach and enables both typechecking and type inference on artifacts within a megamodel. This type system is able to deal with non-trivial situations such as the use of higher order transformations. We also present a prototypical implementation of such a type system.	dependent type;domain-specific language;emulator;executable;first-order predicate;function type;google map maker;graphical user interface;model-driven engineering;prototype;recursion;socket am3;transformation language;type inference;type rule;type safety;type system;typing	Andrés Vignaga;Frédéric Jouault;M. Cecilia Bastarrica;Hugo Brunelière	2011	Software & Systems Modeling	10.1007/s10270-011-0191-2	simulation;type system;computer science;artificial intelligence;programming language;algorithm	PL	-27.82506629961637	29.01003075972807	18588
17d3e77d4c23091f5208a1f95a4713094b11396d	run-time verification of optimistic concurrency	optimistic concurrency;prophecy variable;optimistic concurrency implementation;simple grammar;static verification;run-time verification framework;run-time verification;tressa claim;incorporated prophecy variable;simple syntax	Assertion based specifications are not suitable for optimistic concurrency where concurrent operations are performed assuming no conflict among threads and correctness is cast in terms of the absence or presence of conflicts that happen in the future. What is needed is a formalism that allows expressing constraints about the future. In previous work, we introduced tressa claims and incorporated prophecy variables as one such formalism. We investigated static verification of tressa claims and how tressa claims improve reduction proofs.#R##N##R##N#In this paper, we consider tressa claims in the run-time verification of optimistic concurrency implementations. We formalize, via a simple grammar, the annotation of a program with tressa claims. Our method relieves the user from dealing with explicit manipulation of prophecy variables. We demonstrate the use of tressa claims in expressing complex properties with simple syntax.#R##N##R##N#We develop a run-time verification framework which enables the user to evaluate the correctness of tressa claims. To this end, we first describe the algorithms for monitor synthesis which can be used to evaluate the satisfaction of a tressa claim over a given execution. We then describe our tool implementing these algorithms. We report our initial test results.	concurrent computing;multiversion concurrency control;optimistic concurrency control	Ali Sezgin;Serdar Tasiran;Kivanç Muslu;Shaz Qadeer	2010		10.1007/978-3-642-16612-9_29	computer science;theoretical computer science;database;programming language;algorithm	Logic	-20.445465048863106	29.22361166400908	18612
2b2434bf77c4bb27c6e87958e5d2e4be4f036345	applications of software-defined radio (sdr) technology in hospital environments	software;uhf devices;operating room;frequency 2360 mhz to 2500 mhz software defined radio technology sdr technology hospital environment radio communication system personal computer embedded system radio protocol wired electronic device wireless electronic device operating room intensive care unit wireless medical device seamless interoperability cognitive radio body area network ban wireless sensor network wsn medical environmental surveillance;body sensor networks;intensive care unit;personal computer;surveillance;radio protocol;wired electronic device;hospitals;wsn;hospital environment;wireless sensor networks software interoperability hospitals cognitive radio surveillance;embedded system;software defined radio technology;wireless sensor network;software radio;ban;cognitive radio;wireless medical device;sdr technology;uhf devices biomedical equipment body area networks body sensor networks cognitive radio open systems software radio;seamless interoperability;wireless electronic device;interoperability;body area networks;radio communication system;open systems;medical environmental surveillance;wireless sensor networks;biomedical equipment;frequency 2360 mhz to 2500 mhz;body area network	A software-defined radio (SDR) is a radio communication system where the major part of its functionality is implemented by means of software in a personal computer or embedded system. Such a design paradigm has the major advantage of producing devices that can receive and transmit widely different radio protocols based solely on the software used. This flexibility opens several application opportunities in hospital environments, where a large number of wired and wireless electronic devices must coexist in confined areas like operating rooms and intensive care units. This paper outlines some possible applications in the 2360-2500 MHz frequency band. These applications include the integration of wireless medical devices in a common communication platform for seamless interoperability, and cognitive radio (CR) for body area networks (BANs) and wireless sensor networks (WSNs) for medical environmental surveillance. The description of a proof-of-concept CR prototype is also presented.	analog;coexist (image);cognitive radio;compliance behavior;etsi satellite digital radio;embedded system;embedding;frequency band;hospitals;interoperability;lewy body disease;medical devices;megahertz;operating room;outlines (document);personal computer;plug (physical object);plug and play;programming paradigm;protocols documentation;prototype;requirement;sdpr gene;seamless3d;benefit;intensive care unit	Raúl Chávez-Santiago;Aleksandra Mateska;Konstantin Chomu;Liljana Gavrilovska;Ilangko Balasingham	2013	2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2013.6609738	embedded system;cognitive radio;wireless sensor network;telecommunications;computer science;engineering;computer network	Embedded	-32.633369035205725	22.28947375648823	18639
10f9218d7789f4ac025749697027823e287d5abd	lightweight linear types in system fdegree.	type systems;polymorphism;linear logic	We present System F°, an extension of System F that uses kinds to distinguish between linear and unrestricted types, simplifying the use of linearity for general-purpose programming. We demonstrate through examples how System F° can elegantly express many useful protocols, and we prove that any protocol representable as a DFA can be encoded as an F° type. We supply mechanized proofs of System F°'s soundness and parametricity properties, along with a nonstandard operational semantics that formalizes common intuitions about linearity and aids in reasoning about protocols.  We compare System F° to other linear systems, noting that the simplicity of our kind-based approach leads to a more explicit account of what linearity is meant to capture, allowing otherwise-conflicting interpretations of linearity (in particular, restrictions on aliasing versus restrictions on resource usage) to coexist peacefully. We also discuss extensions to System F^o aimed at making the core language more practical, including the additive fragment of linear logic, algebraic datatypes, and recursion.	acm sigact;abstract machine;algebraic data type;aliasing;amiga walker;coexist (image);cyclone;dave grossman (game developer);deterministic finite automaton;functional programming;general-purpose modeling;haskell;honda e series;information and computation;integrated facility for linux;international federation for information processing;international symposium on memory management;jean;jones calculus;lambda calculus;language primitive;lecture notes in computer science;linear logic;linear system;mason;manual memory management;mitchell corporation;monadic predicate calculus;operational semantics;parametric polymorphism;parametricity;programming language;recursion;rice's theorem;springer (tank);stateful firewall;substructural type system;symposium on logic in computer science;symposium on principles of programming languages;system f;theoretical computer science;theory;type safety;uniqueness type;usage analysis;utility functions on indivisible goods	Karl Mazurak;Jianzhou Zhao;Steve Zdancewic	2010		10.1145/1708016.1708027	polymorphism;linear logic;computer science;programming language;algorithm	PL	-16.20614053889056	19.725094349673796	18651
b38126fe9a84f2d09ce5a4c5fd29cf4bc702544e	thunk recycling for lazy functional languages: operational semantics and correctness	lazy evaluation;semantics;bisimulation;thunk;optimization;haskell;small step operational semantics	Lazy evaluation helps programmers write clear programs. However, it has significant run-time overheads for building many as-yet unevaluated expressions, or thunks. Because thunk allocation is a space-consuming task, it is important to reduce the number of thunks in order to improve the performance of a lazy functional program. To resolve this problem, a mechanism named thunk recycling have been proposed. Thunk recycling suppresses thunk generation by reusing and updating an already allocated thunk at the tail of a list, on the condition that the thunk is singly referred. The preliminary implementation of thunk recycling has been developed in Glasgow Haskell Compiler and has succeeded in reducing total memory allocations for benchmark programs. This paper addresses the formal issue of thunk recycling. We present a small-step operational semantics of the thunk recycling and show the correctness of this mechanism on the basis of bisimulation.	benchmark (computing);binary tree;bisimulation;compiler;computation;correctness (computer science);data structure;functional programming;lazy evaluation;mathematical induction;operational semantics;programmer;recursion;the glorious glasgow haskell compilation system;thunk;μ-recursive function	Yasunao Takano;Hideya Iwasaki	2015		10.1145/2695664.2695693	computer science;bisimulation;theoretical computer science;lazy evaluation;database;semantics;programming language;algorithm	PL	-20.957927273450778	24.362227081127227	18653
98a75c138e7e3abc357d5e37b4924c33222a5ee4	a formal approach to implement java exceptions in cooperative systems		The increasing number of systems that work on the top of cooperating elements have required new techniques to control cooperation on both normal and abnormal behaviors of systems. The controllability of the normal behaviors has received more attention because they are concerned with the users expectations, while for the abnormal behaviors it is left to designers and programmers. However, for cooperative systems, the abnormal behaviors, mostly represented by exceptions at programming level, become an important issue in software development because they can affect the overall system behavior. If an exception is raised and not handled accordingly, the system may collapse. To avoid such situation, certain concepts and models have been proposed to coordinate propagation and recovering of exceptional behaviors, including the Coordinated Atomic Actions (CAA). Regardless of the effort in creating these conceptual models, an actual implementation of them in real systems is not very straightforward. This article provides a reliable framework for the implementation of Java exceptions propagation and recovery using CAA concepts. To do this, a Java framework (based on a formal specification) is presented, together with a set of properties to be preserved and proved with the Java Pathfinder (JPF) model checker. In practice, to develop new systems based on the given coordination concepts, designers/programmers can instantiate the framework to implement the exceptional behavior and then verify the correctness of the resulting code using JPF. Therefore, by using the framework, designers/programmers can reuse the provided CAA implementation and instantiate fault-tolerant Java systems. © 2016 Elsevier Inc. All rights reserved.	consensus dynamics;correctness (computer science);dns certification authority authorization;fault tolerance;formal specification;java pathfinder;linearizability;model checking;programmer;software development;software propagation	Simone Hanazumi;Ana C. V. de Melo	2017	Journal of Systems and Software	10.1016/j.jss.2016.07.033	programming language;real-time computing;conceptual model;software engineering;model checking;computer science;correctness;formal specification;real time java;software development;java;java collections framework	SE	-27.658843111503703	30.793737752845107	18655
67939d03c47457262fa3a7af6085f5da3fa8e6b8	sql-sampler: a tool to visualize and consolidate domain semantics by perfect sql sample data	conference item	SQL database designs can result from methodologies such as UML or Entity-Relationship modeling, Description Logic specifications, or relational normalization. Independently from the methodology, the use of good sample data is promoted by academia and commercial database design tools to visualize, validate and consolidate the database designs produced. Unfortunately, advice on what constitutes good sample data, or support to create good sample data are hard to come by. Armstrong databases provide a right notion of sample data that perfectly represent the domain semantics encoded in the form of SQL constraints. We present a tool that computes Armstrong sample tables for different classes of SQL constraints, and different interpretations of null markers. Armstrong tables illustrate the perceptions of an SQL database design about the semantics of an application domain. The tool exemplifies the impact of various design choices on Armstrong tables. These include the expressiveness of the classes of SQL constraints considered, and the semantics of null markers. Armstrong tables complement existing database design methodologies. In particular, they provide data samples that guide the transfer from relational approximations of an application domain to an actual real-life SQL table design.	application domain;approximation;armstrong's axioms;data integrity;data model;database design;decision table;description logic;entity–relationship model;fagin's theorem;functional dependency;humans;interaction;multivalued dependency;pipelines;real life;resource description framework;sql;sampling (signal processing);table (database);unified modeling language;xml	Van Bao Tran Le;Sebastian Link;Flavio Ferrarotti	2014			data transformation services;data definition language;sql;stored procedure;relational database;computer science;query by example;in-memory processing;theoretical computer science;database model;transaction time;data mining;database;programming language;null;object-relational impedance mismatch	DB	-25.45720984023383	9.454801632405681	18656
5ef25c418cd1fdbcab7d3c5741ca1ce54b2b45f0	semi-automatic generation of web-based computing environments for software libraries	web-based computing environments;wide web browser;xml description file;engineering library software;systems library slicot;software libraries;matlab binary;local machine;semi-automatic generation;necessary web page;web interface;web computing environment;additional software installation;web pages;world wide web	A set of utilities for generating web computing environments related to mathematical and engineering library software is presented. The web interface can be accessed from a standard world wide web browser with no need for additional software installations on the local machine. The environment provides a user-friendly access to computational routines, workspace management, reusable sessions and support of various data formats, including Matlab binaries. The creation of new interfaces is a straightforward process. All necessary web pages are automatically generated from XML description files. The integration of the control and systems library SLICOT demonstrates the efficacy of this approach.	library (computing);semiconductor industry	Pedher Johansson;Daniel Kressner	2002		10.1007/3-540-46043-8_88	web service;web modeling;web mapping;web-based simulation;web design;human–computer interaction;web standards;computer science;web api;operating system;web navigation;web page;database;programming language;user interface;web 2.0;world wide web;web server	HPC	-32.48850032815433	26.015217307634305	18681
3233c38be4e16fcf1e609f8c5218cd117e5a2bb9	a logic programming framework for possibilistic argumentation: formalization and logical properties	engineering;lenguaje programacion;monotone operator;systeme intelligent;fuzzy set;procesamiento informacion;programmation;programming language;vague knowledge;sistema inteligente;logique floue;conjunto difuso;logica difusa;ensemble flou;03b52;raisonnement;ingenierie;programacion;fuzzy logic;programming theory;defeasible logic programming;information processing;possibilistic logic;intelligent systems;intelligent system;razonamiento;intelligent agent;defeasible argumentation;langage programmation;ingenieria;theorie programmation;sistema difuso;systeme flou;qualitative reasoning;logic programs;reasoning;traitement information;programming;fuzzy system	In the last decade defeasible argumentation frameworks have evolved to become a sound setting to formalize commonsense, qualitative reasoning. The logic programming paradigm has shown to be particularly useful for developing different argument-based frameworks on the basis of different variants of logic programming which incorporate defeasible rules. Most of such frameworks, however, are unable to deal with explicit uncertainty, nor with vague knowledge, as defeasibility is directly encoded in the object language. This paper presents Possibilistic Defeasible Logic Programming (P-DeLP), a new logic programming language which combines features from argumentation theory and logic programming, incorporating as well the treatment of possibilistic uncertainty. Such features are formalized on the basis of PGL, a possibilistic logic based on Gödel fuzzy logic. One of the applications of PDeLP is providing an intelligent agent with non-monotonic, argumentative inference capabilities. In this paper we also provide a better understanding of such capabilities by defining two non-monotonic operators which model the expansion of a given program P by adding new weighed facts associated with argument conclusions and warranted literals, respectively. Different logical properties for the proposed operators are studied.	admissible numbering;argumentation framework;commonsense reasoning;defeasible logic;defeasible reasoning;description logic;disjunctive normal form;fuzzy logic;gödel;horn clause;idempotence;intelligent agent;international ergonomics association;ll parser;linear algebra;logic programming;non-monotonic logic;object language;pict;programming language;programming paradigm;rationality;read-write memory;requirement;sensitivity and specificity;t-norm fuzzy logics;tip (unix utility);traffic collision avoidance system;turing completeness;unification (computer science);vagueness;well-formed formula	Teresa Alsinet;Carlos Iván Chesñevar;Lluis Godo;Guillermo Ricardo Simari	2008	Fuzzy Sets and Systems	10.1016/j.fss.2007.12.013	fuzzy logic;dynamic logic;programming;description logic;qualitative reasoning;horn clause;information processing;computer science;artificial intelligence;machine learning;functional logic programming;computational logic;mathematics;fuzzy set;programming paradigm;inductive programming;prolog;logic programming;intelligent agent;multimodal logic;reason;algorithm;fuzzy control system;philosophy of logic;autoepistemic logic	AI	-17.325454282076343	11.106007558380925	18699
cb36c1bffc8ef8fa881f7b8b0612dd641b2566c3	the temporal reasoning tools timegraph i-ii	constraint handling;data structures;graph theory;temporal databases;temporal reasoning;ai-applications;interval algebra;point algebra;timegraph i;timegraph ii;artificial intelligence;domain-independent temporal reasoning systems;graph partitioning;interval disjointness;metagraph data structure;metric information;point-interval exclusion;qualitative information;search;story comprehension;temporal information management;temporal reasoning tools;timegraphs	We describe two domain-independent temporal reasoning systems called TimeGraph I and II which can be used in AI applications as tools for efficiently managing large sets of relations in the Point Algebra, in the Interval Algebra, and metric information such as absolute times and durations. Our representation of time is based on timegraphs, graphs partitioned into a set of chains on which the search is supported by a metagraph data structure. TimeGraph I was originally developed by Taugher, Schubert and Miller in the context of story comprehension. TimeGraph II provides useful extensions, including efficient algorithms for handing inequations, and relations expressing point-interval exclusion and interval disjointness. These extensions make the system much more expressive in the representation of qualitative information and suitable for a large class of applications.		Alfonso Gerevini;Lenhart K. Schubert;Stephanie Schaeffer	1995	International Journal on Artificial Intelligence Tools	10.1142/S0218213095000140	mathematical optimization;computer science;artificial intelligence;machine learning;algorithm	EDA	-18.994660379806376	8.208315798431146	18718
26e8e30966b4b460f21946ceabf2212217bf8380	a system for the semiautomatic generation of e-r models from natural language specifications	lenguaje natural;entity relationship model;query language;architecture systeme;formal specification;rule based system;langage naturel;base connaissance;modelo entidad relacion;intelligence artificielle;modele entite relation;lenguaje interrogacion;specification formelle;especificacion formal;deductive database;natural language understanding;automated logical datase design;base dato deductiva;natural language;estructura datos;artificial intelligence;base conocimiento;arquitectura sistema;structure donnee;langage interrogation;base donnee deductive;inteligencia artificial;systeme gestion base donnee;information system;database design;knowledge representation;system architecture;sistema gestion base datos;database management system;data structure;systeme information;logical form;sistema informacion;knowledge base	Abstract   Techniques that allow a program to generate E-R models for small application domains are described. The inputs to the program are natural language specifications. The program consists of two major components: a natural language understander and an E-R generator. The paper describes briefly the knowledge representation structures constructed by the natural language understander, and, then, explains the E-R generator in detail. The E-R generator consists of two kinds of rules: specific rules linked to the semantics of some words in the sentences, and generic rules that identify ER-entities and ER-relationships on the basis of the logical form of the sentence and on the basis of the ER-entities and ER-relationships under construction. The program has been tested on a set of database problems.	natural language	Fernando Gomez;Carlos Segami;Carl Delaune	1999	Data Knowl. Eng.	10.1016/S0169-023X(98)00032-9	natural language processing;knowledge base;natural language programming;logical form;data structure;entity–relationship model;computer science;artificial intelligence;formal specification;database;natural language;information system;algorithm;database design;query language	DB	-30.10335232493629	14.344116232745762	18837
33712f6c2ea8474f656d3de1e3dd1a02e6a70465	a topological characterization of consistency of logic theories in propositional logic	similarity degree;propositional logic;truth degree;logic metric space;consistency	Abstract#R##N##R##N#The main purpose of this note is to characterize consistency of logic theories in propositional logic by means of topological concept. Based on the concepts of truth degree of formulas and similarity degree between formulas the concept of logic metric space has been proposed by the first author. It is proved in this note that a closed logic theory Γ is consistent if and only if it contains no interior point in the logic metric space. Moreover the relationship between logic closedness and topological closedness of a logic theory Γ is discussed. Finally, the concept of full divergency is also characterized by means of the topological concept of density. (© 2006 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim)	propositional calculus	Guo-Jun Wang;Yan-Hong She	2006	Math. Log. Q.	10.1002/malq.200610007	predicate logic;fuzzy logic;dynamic logic;zeroth-order logic;complete theory;mathematical analysis;discrete mathematics;description logic;higher-order logic;topology;tautology;many-valued logic;intuitionistic logic;intermediate logic;predicate functor logic;pure mathematics;mathematics;propositional variable;well-formed formula;propositional calculus;consistency;logic;multimodal logic;second-order logic;algorithm;autoepistemic logic	Logic	-10.420960284542236	12.008063836018117	18839
49aca7330002f5af3bea27978398b4cfefed7ffc	optimal guard synthesis for memory safety		This paper presents a new synthesis-based approach for writing low-level memory-safe code. Given a partial program with missing guards, our algorithm synthesizes concrete predicates to plug in for the missing guards such that all buffer accesses in the program are memory safe. Furthermore, guards synthesized by our technique are the simplest and weakest among guards that guarantee memory safety, relative to the inferred loop invariants. Our approach is fully automatic and does not require any hints from the user. We have implemented our algorithm in a prototype synthesis tool for C programs, and we show that the proposed approach is able to successfully synthesize guards that closely match hand-written programmer code in a set of real-world C programs.	algorithm;guard (computer science);high- and low-level;memory safety;predicate (mathematical logic);programmer;prototype	Thomas Dillig;Isil Dillig;Swarat Chaudhuri	2014		10.1007/978-3-319-08867-9_32	embedded system;real-time computing;programming language;algorithm	Logic	-20.80957973031286	30.585666955741992	18849
5a1bdfb30bda2d071676f406548509dab133ef32	handling behavioral semantics in persistent meta-modeling systems	information systems;web services data integration information systems relational databases;web services;relational databases;semantics databases unified modeling language object oriented modeling data models metamodeling web services;data integration;information exchange behavioral semantics persistent meta modeling systems data integration data exchange information systems modeling formalisms model based databases exploitation languages web service model transformations relational databases	The increasing number of information systems modeling formalisms raises several problems such as data integration or data exchange. To address these problems, several meta-modeling systems have been proposed. However, few of them use a database as a back-end repository in order to offer a persistent solution for addressing over sized models. Yet, with the growing size of data manipulated in information systems, there is a need to exploit databases properties like scalability and querying capabilities. In this paper, we present persistent meta-modeling systems with their meta-modeling capabilities. We show that these systems support the definition of structural and descriptive semantics of models, but they can not express the behavioral semantics of models. Therefore, the aim of our work is to combine benefits of classic modeling systems, typically their capability to express behavioral semantics of models elements, together with advantages of databases i.e. their scalability and querying capabilities. Our approach focuses on the capability to dynamically introduce new operators that could be exploited by model-based databases exploitation languages. In particular, such operators could be implemented with an external program stored outside the database, or with a web service. As a consequence of this extension, we will be able to perform model transformations in database, trigger web services from relational databases, information exchange and data integration could be also supported in a persistent context.	database trigger;electronic component;expect;information exchange;information system;metamodeling;model transformation;persistence (computer science);programming language;relational database;scalability;systems modeling;web service	Youness Bazhar	2012	2012 Sixth International Conference on Research Challenges in Information Science (RCIS)	10.1109/RCIS.2012.6240459	web service;idef1x;data modeling;database theory;data model;relational database;computer science;theoretical computer science;data integration;database model;data mining;database;world wide web;information system	DB	-31.46009787432349	12.360624333829119	18892
d72bb56ee27d153054e14828615e4f80dcdd25cf	cyberetl: towards visual debugging transformations in data integration	data integrity;sql;integration information;visual design;interrogation base donnee;interrogacion base datos;information integration;integracion informacion;database query	we describe the CyberETL system for visual designing and debugging transformations in data integration. CyberETL has some innovative features, such as visual designing of transformations, bidirection translation between transformations and SQL queries, and lineage-based data comparison and debugging. This enables very robust and fast integration of data in ETL process without the bother and nail-biting of error-prone transformation designing.	debugging	Youlin Fang;Dongqing Yang;Shiwei Tang;Yunhai Tong;Weihua Zhang;Libo Yu;Qiang Fu	2003		10.1007/978-3-540-45160-0_51	sql;computer science;information integration;data integrity;data mining;database;world wide web	ML	-32.55902633541443	10.009011216993823	18925
0b46a5b443f7a609daec511e0fb020b6994d7ccb	a fuzzy language	lenguaje programacion;fuzzy set;procesamiento informacion;programmation;programming language;fuzzy data;lambda calculus;conjunto difuso;ensemble flou;satisfiability;programacion;information processing;langage programmation;lambda calculo;theorie information;traitement information;lambda calcul;programming;information theory;teoria informacion	With the aim of designing and implementing programming languages that take into account the fuzzy paradigm we will modify the classical lambda calculus by adding a degree to each term and by rede0ning the b-reduction. Thus, for the new calculus to verify the Church–Rosser property, the degree computed with can be made through a function that is a t-norm or an s-conorm. With this new tool we design a nondeterminist language that satis0es fuzzy data programming requirements, and an example of its behaviour is shown. c © 2002 Elsevier B.V. All rights reserved.	church–rosser theorem;fuzzy logic;lambda calculus;programming language;programming paradigm;requirement;t-norm	Daniel Sánchez Alvarez;Antonio F. Gómez-Skarmeta	2004	Fuzzy Sets and Systems	10.1016/S0165-0114(02)00547-X	programming;information processing;information theory;computer science;artificial intelligence;fuzzy number;lambda calculus;mathematics;fuzzy set;fuzzy control language;algorithm;satisfiability	PL	-17.080236912634415	11.154133557181723	18961
67ff5cf7f48a61edcdececb8c1d841fd102d6817	procedures and modules in fortran 90	encapsulation;lenguaje programacion;appel procedure;programming language;procedimiento;programmation modulaire;abstract data types;tipo dato;programacion modular;encapsulacion;ingenieria logiciel;abstract data type;data type;fortran 90;software engineering;functional programming;llamada procedimiento;generic relation;signal processing;type abstrait;data abstraction;genie logiciel;langage programmation;programme recursif;modular programming;tipo abstracto;programmation fonctionnelle;fortran;relation generique;programa recursivo;recursive program;software design;type donnee;programacion funcional;procedure call;relacion generica;type safety;procedure	No new feature in Fortran 90 is more important than modules for the support of effective software engineering methods. Fortran 90 modules make it possible to encapsulate portions of software: to make entities accessible only from those parts of a program in which the software designer desires and plans for their accessibility. Programmers can control the visibility of software entities (variables, procedures, data types, etc.) from the site of declaration (with PUBLIC/PRIVATE specifications) or from usage sites (with USE-ONLY specifications), and these entities can be packaged together in units for convenient and safe access throughout an application. In this way, Fortran 90 modules fully support the concept of data abstraction, one of the most important organizing principles of software design. This paper illustrates the use of modules to create a package of signal processing functions. It discusses the use of explicit interfaces defined in modules or interface blocks to improve the type safety of Fortran code, user-defined generic procedures and operators, specification of the intended use of procedure arguments (INTENT IN, OUT, and INOUT), dummy-argument arrays that inherit their shape from actual arguments, local arrays with shapes that depend on procedure inputs, recursion, optional and keyword procedure-arguments, and functions that deliver arrays or structures as values. The paper is not comprehensive. Topics omitted include internal procedures, statement functions, multiple entry, alternate return, and pointers, and little is said about the large collection of intrinsic procedures in Fortran 90. All of the illustrative code follows a programming method based on formulas and equations rather than step-by-step procedures. In this method, known as functional programming, names are defined only once; their associated values are never replaced with new ones. Fortran 90 facilitates the use of this method by permitting the definition of functions whose values are arrays or structures and by allowing functions to invoke themselves.	fortran	Rex L. Page	1996	Computer Standards & Interfaces	10.1016/0920-5489(96)01011-2	computer science;theoretical computer science;operating system;software engineering;signal processing;database;programming language;functional programming;abstract data type;algorithm	HCI	-27.032613276980598	25.745209933019435	18980
7f1e0db3f77b671d94903d748c73d19003a827e3	extended query answering using integrity rules	base donnee;metadata;interrogation base donnee;methode formelle;database;interrogacion base datos;base dato;formal method;metadonnee;query;metadatos;query answering;database query;requete	The conventional use of databases is commonly restricted to the retrieval of factual data in the form of tuples or records. However most databases also contain metadata in the form of integrity rules which can provide a rich source of additional information not normally available to the user. Integrity rules define what data values and relationships may exist within the database and so their interrogation can provide answers as to whether a certain database state is possible. Our paper describes how this may be achieved and specifies a formal approach to implementing such an enquiry system.	database	Barry G. T. Lowden;Jerome Robinson	2000		10.1007/3-540-39963-1_27	query optimization;formal methods;computer science;data mining;database;view;metadata;information retrieval	DB	-27.50558861379719	9.157167080899884	19046
a29ef700629e7196211765fb9f478684bac2c584	some relations between predicate calculus and semantic net representations of discourse	semantic network representation;association list;computational application;predicate calculus formalism;semantic network;binary predicate;ordinary predicate logic convention;conjoined set;predicate logic;semantic relation;natural language	Networks can be used to represent syntactic trees of the semantic relations that hold between words in sentences. They can be alternately symbolized as association l i s t s or conjoined sets of t r i p les . A semantic net represents a sentence as a conjoined set of binary predicates. An algorithm is presented that converts a semantic network into predicate calculus formalism. The simpler syntax of semantic network representations in contrast of ordinary predicate logic conventions is taken as an argument for their use in computational applications. Descriptive Terms: Semantic networks, Predicate logic, Natural language, Computational l inguist i c s , Association l i s t s .	algorithm;first-order logic;formal grammar;large eddy simulation;natural language;semantic network;syntactic predicate	Robert F. Simmons;Bertram C. Bruce	1971			discrete mathematics;semantic similarity;semantic computing;semantic gap;semantic equivalence;semantic role labeling;natural language processing;semantic web stack;artificial intelligence;computer science;semantic network;semantic compression	AI	-15.25145998218126	11.317675889518293	19053
6e6fcf48ca118b783441814da8356eaf1cc0a3d1	reasoning algebraically about probabilistic loops	developpement logiciel;modelizacion;algebraic laws;probabilistic action systems;probability;von wright;metodo formal;methode formelle;programmation stochastique;intelligence artificielle;aprendizaje probabilidades;probabilistic approach;data refinement;reasoning about loops;formal method;modelisation;refinement method;calcul raffinement;algebra;back;desarrollo logicial;enfoque probabilista;approche probabiliste;software development;refinement calculus;probabilistic refinement calculus;apprentissage probabilites;artificial intelligence;inteligencia artificial;probabilistic logic;methode raffinement;stochastic programming;logique probabiliste;modeling;metodo afinamiento;programacion estocastica;probability learning	Back and von Wright have developed algebraic laws for reasoning about loops in the refinement calculus. We extend their work to reasoning about probabilistic loops in the probabilistic refinement calculus. We apply our algebraic reasoning to derive transformation rules for probabilistic action systems. In particular we focus on developing data refinement rules for probabilistic action systems. Our extension is interesting since some well known transformation rules that are applicable to standard programs are not applicable to probabilistic ones: we identify some of these important differences and we develop alternative rules where possible. In particular, our probabilistic action system data refinement rules are new.	correctness (computer science);iteration;kleene algebra;lazy evaluation;linear algebra;linear programming relaxation;omega;peano axioms;probabilistic database;randomized algorithm;refinement (computing);refinement calculus;schedule (computer science);technical standard;transformer	Larissa Meinicke;Ian J. Hayes	2006		10.1007/11901433_21	refinement calculus;stochastic programming;probabilistic analysis of algorithms;formal methods;probabilistic ctl;systems modeling;probabilistic relevance model;computer science;artificial intelligence;software development;probability;probabilistic logic;probabilistic argumentation;algorithm	AI	-16.279000366922894	15.283125183654972	19056
196c5e6e095e5b6b963b2bea8a731f46c6d1b221	an interpretation of classical proofs		Without Abstract	interpretation (logic)	Glen Helman	1983	J. Philosophical Logic	10.1007/BF02329200	discrete mathematics;mathematics;mathematical proof	Logic	-11.113969200045924	11.44125761579371	19057
c9ab6c02ea1f3d28d37eee3f33d9ba2496986d10	simple, decidable type inference with subtyping	programming language;featherweight java;polymorphism;type inference	We demonstrate a method to infer polymorphically principal and subtyping-minimal types for an ML-like core language by assigning ranges within a lattice to type variables. We demonstrate the termination and completeness of this algorithm, and proceed to show that it solves a broad special-case of the generally-undecidable semi-unification problem. Our procedure requires no type annotations, leaves no subtyping constraints in the inferred types, and produces no proof obligations. We demonstrate the practical utility of our technique by showing a type-preserving encoding of Featherweight Java into the expression calculus over which we infer types.	algorithm;java;semiconductor industry;type inference;undecidable problem;unification (computer science)	Eli Gottlieb	2011	CoRR		polymorphism;computer science;type inference;programming language;algorithm	PL	-16.07676544368301	20.741684533358047	19102
766160389010f716095bb3d820804f832c34db88	abstract representation theorems for demonic refinement algebras	satisfiability;representation theorem	representation theorems for demonic refinement algebras< Jean-Lou De Carufel∗, Jules Desharnais∗∗ Département d’informatique et de génie logiciel, Pavillon Adrien-Pouliot, 1065, avenue de la Médecine, Université Laval, Québec, QC, Canada G1V 0A6 A R T I C L E I N F O A B S T R A C T Article history: Available online 15 July 2010 MSC: 03G10 16Y60 68R99	ieee annals of the history of computing;jean;jules;linear algebra;refinement (computing)	Jean-Lou De Carufel;Jules Desharnais	2010	J. Log. Algebr. Program.	10.1016/j.jlap.2010.07.014	filtered algebra;combinatorics;discrete mathematics;computer science;mathematics;algebra representation;cellular algebra;algebra;satisfiability	Logic	-19.658784320270694	18.512591702858412	19148
6c238c1b64c9a4811757ff723e407cc80bb01043	extracting schema from semistructured data with weight tag	target set of label path;oem model with weight;top down;semistructured data;label path supporting degree;schema extraction	This paper put forward the concept of OEM model with weight on its edges, developes a new approach to extracting schema from semistructured data with weight on its edges, and gives two theorems related to computing taget set of label path and suporting degree of label path. Using wideth-first and top-down traversing strategy ,the algorithm computes target set and supporting degree of every label in a label path, and decides whether the label is retained in schema model according to its magnitude of supporting degree and weight of the label .In the last, we test the validity and efficiency of the algorithm. The schema scale of the semistructured data obtained from the same OEM database in this paper is smaller than that in other paper.		Jiu-Zhong Li;Shuo Shi	2009		10.1007/978-3-642-01513-7_126	computer science;top-down and bottom-up design;data mining;database;algorithm	DB	-31.976653325642733	5.849976146197342	19225
ee9c2ca20a7f7839667559742d178105795bb770	occurrence graphs for interval timed coloured nets	coloured petri net;equivalence relation;time petri net;dynamic properties;real time systems	We present an approach to construct the occurrence graph for ITCPN (Interval Timed Coloured Petri Nets). These models, defined by Van Der Aalst in [VAN] can simulate other timed Petri nets and allow to describe large and complex real-time systems. We define classes as sets of states between two occurrences, and we use these classes to define the occurrence graph of an ITCPN. Then an equivalence relation based on time is defined for classes, and we show that occurrence graphs reduced using this equivalence relation are finite if and only if the set of reachable markings is finite. These graphs can be used to verify all the dynamic properties such as reachability, boundedness, home, liveness and fairness properties but also performance properties: minimal and maximal bounds along a occurrence sequence or a cycle. Finally we complete delay based equivalence with a colour based equivalence in order to achieve further reduction.		Gérard Berthelot;Hanifa Boucheneb	1994		10.1007/3-540-58152-9_6	discrete mathematics;stochastic petri net;equivalence relation;petri net	Logic	-9.628066701335445	24.59723796897997	19321
4e96c3f7d5435558853c105968c18f47b88d5082	dataflow visual programming language debugger supported by fisheye view	debugging;program visualization dataflow visual programming language debugger fisheye view information visualization;computer languages;debugger;program visualization;fisheye view;wires;information visualization;computer screen;visual programming language;visualization;dataflow visual programming language debugger;visual information;heuristic algorithms;data flow analysis;programming languages data flow analysis program debugging program visualisation;dataflow visual programming language;computer screen dataflow visual programming language debugger fisheye view visual information;program debugging;program development;algorithm design and analysis;computer languages containers debugging wires instruments visualization signal processing algorithms computer displays lenses educational institutions;program visualisation;programming languages;containers	"""Programs developed by dataflow visual programming languages (DFVPLs) often contain lots of visual information. If more useful information could be displayed in the limited range of screen, the efficiency of debugging would be improved. We took full advantage of the characteristic of fisheye view that can display both """"local detail"""" and """"global context"""" simultaneously, improved the previous implementation models and algorithms, and solved the problems of using fisheye view on DFVPL such as nest hierarchy, the difference in nodes' size is big and wires are complex. We combined the debugger with fisheye view and proved its practicability and feasibility by some contrastive experiments."""	algorithm;dataflow;debugger;debugging;experiment;fisheye;view model;visual programming language	Yangyi Sui;Lili Pang;Jun Lin;Xiaotuo Zhang	2008	2008 The 9th International Conference for Young Computer Scientists	10.1109/ICYCS.2008.77	algorithm design;information visualization;visualization;computer science;theoretical computer science;operating system;data-flow analysis;database;visual programming language;programming language;debugging	Robotics	-31.013360640412408	24.776659642409104	19338
83d00699f870b48bf643ee38bb5fafbdbc735d87	in all but finitely many possible worlds: model-theoretic investigations on 'overwhelming majority' default conditionals		Defeasible conditionals are statements of the form ‘ifAthen normallyB’. One plausible interpretation introduced in nonmonotonic reasoning dictates that (\(A\Rightarrow B\)) is true iff B is true in ‘most’ A-worlds. In this paper, we investigate defeasible conditionals constructed upon a notion of ‘overwhelming majority’, defined as ‘truth in a cofinite subset of\(\omega \)’, the first infinite ordinal. One approach employs the modal logic of the frame \((\omega , <)\), used in the temporal logic of discrete linear time. We introduce and investigate conditionals, defined modally over \((\omega , <)\); several modal definitions of the conditional connective are examined, with an emphasis on the nonmonotonic ones. An alternative interpretation of ‘majority’ as sets cofinal (in \(\omega \)) rather than cofinite (subsets of \(\omega \)) is examined. For these modal approaches over \((\omega , <)\), a decision procedure readily emerges, as the modal logic \({\mathbf {K4DLZ}}\) of this frame is well-known and a translation of the conditional sentences can be mechanically checked for validity; this allows also for a quick proof of \(\mathsf {NP}\)-completeness of the satisfiability problem for these logics. A second approach employs the conditional version of Scott-Montague semantics, in the form of \(\omega \)-many possible worlds, endowed with neighborhoods populated by collections of cofinite subsets of \(\omega \). This approach gives rise to weak conditional logics, as expected. The relative strength of the conditionals introduced is compared to (the conditional logic ‘equivalent’ of) KLM logics and other conditional logics in the literature.	possible world;theory	Costas D. Koutras;Christos Rantsoudis	2017	Journal of Logic, Language and Information	10.1007/s10849-017-9251-5	discrete mathematics;artificial intelligence;mathematics;algorithm	Theory	-13.604080249409156	8.15558640284246	19403
501d1071540f01c24ca9195ab1faeda329a8243f	deterministic timed finite state machines: equivalence checking and expressive power		There has been a growing interest in defining models of automa ta enriched with time. For instance, timed automata were introduced as automata extended with cl ocks. In this paper, we study models of timed finite state machines (TFSMs), i.e., FSMs enriched wit h time, which accept timed input words and generate timed output words. Here we discuss some models f TFSMs with a single clock: TFSMs with timed guards, TFSMs with timeouts, and TFSMs with both timed guards and timeouts. We solve the problem of equivalence checking for all three mo dels, and we compare their expressive power, characterizing subclasses of TFSMs with timed guard s n of TFSMs with timeouts that are equivalent to each other.	automata theory;consistency model;expressive power (computer science);finite-state machine;formal equivalence checking;timed automaton;turing completeness	Davide Bresolin;Khaled El-Fakih;Tiziano Villa;Nina Yevtushenko	2014		10.4204/EPTCS.161.18	discrete mathematics;real-time computing;computer science;timed automaton;algorithm	Logic	-10.763707555212749	24.887549937040756	19407
8afa5f0bf550512e1d8c5fdb48e1ca0f4b4c5829	a note on negation in categorial grammar	categorial grammar	A version of strong negation is introduced into Categorial Grammar. The resulting syntactic calculi turn out to be systems of connexive logic.	axiomatic system;categorial grammar;circuit complexity;embedded system;four-valued logic;intuitionistic logic;logical connective;nl (complexity);quantum information;sequent calculus;stable model semantics;word sense	Heinrich Wansing	2007	Logic Journal of the IGPL	10.1093/jigpal/jzm012	generative grammar;categorial grammar;link grammar;operator-precedence grammar;computer science;affix grammar;emergent grammar;relational grammar;programming language;lexical functional grammar;mildly context-sensitive grammar formalism;combinatory categorial grammar;head-driven phrase structure grammar	NLP	-13.018600290553197	11.937597018329393	19417
39f0d881bae0f3fdecd5c20c3ce2b2587a88f4cf	exploiting indifference for customization of partial order skylines	total order;pareto optimisation;information retrieval databases query processing informatics pareto analysis testing warehousing face humans information systems;information systems;user preference;query processing;partial order skyline customization;user preferences;pareto semantics;pareto aggregation;query processing information systems pareto optimisation;user preference partial order skyline customization pareto semantics query processing pareto aggregation;partial order	Unlike numerical preferences, preferences on attribute values do not show an inherent total order, but skyline computation has to rely on partial orderings explicitly stated by the user. In such orders many object values are incomparable, hence skylines sizes become unpractical. However, the Pareto semantics can be modified to benefit from indifferences: skyline result sizes can be essentially reduced by allowing the user to declare some incomparable values as equally desirable. A major problem of adding such equivalences is that they may result in intransitivity of the aggregated Pareto order and thus efficient query processing is hampered. In this paper we analyze how far the strict Pareto semantics can be relaxed while always retaining transitivity of the induced Pareto aggregation. Extensive practical tests show that skyline sizes can indeed be reduced about two orders of magnitude when using the maximum possible relaxation still guaranteeing the consistency with all user preferences	computation;database;linear programming relaxation;numerical analysis;pareto efficiency;user (computing);vertex-transitive graph	Wolf-Tilo Balke;Ulrich Güntzer;Wolf Siberski	2006	2006 10th International Database Engineering and Applications Symposium (IDEAS'06)	10.1109/IDEAS.2006.22	partially ordered set;pareto analysis;bayesian efficiency;computer science;theoretical computer science;data mining;database;information system;total order	DB	-25.010410055410333	5.5429095594641655	19489
ed435bee7009f21c5c3e537eeac2b3c9bd56550c	revision and updating in dynamic doxastic logic	logic;information change;belief revision;hypertheory semantics;theory change;formal logic;primitive update operator;rational theory change analysis;hypertheory semantics dynamic doxastic logic updating rational theory change analysis belief revision information change theory change primitive update operator;belief maintenance;computational linguistics;dynamic doxastic logic;formal logic belief maintenance computational linguistics;updating	"""The topic of the paper is the introduction of an update-operator in the sense of Katsuno and Mendelzon (1992) into dynamic doxastic logic developed by Segerberg (1995). Dynamic doxastic logic (DDL) was introduced as a tool for analyzing rational theory change, taking its starting point in the AGM-approach to belief revision. AGM deals with change of information about a static world. But there is another kind of theory change, namely, change of information triggered by changes in the (dynamic) world. This kind of theory change was analyzed by several authors, who called it """"updating"""". The author proposes a first step in an analysis of updating in the framework of DDL by introducing a primitive update operator in the language of DDL and interpreting it in a semantics which is a simple extension of hypertheory-semantics. It is shown that this operator is an update-operator in the sense of Katsuno and Mendelzon."""	belief revision;doxastic logic	Uta Wille	1998		10.1109/TIME.1998.674149	doxastic logic;artificial intelligence;theoretical computer science;mathematics;algorithm	AI	-15.201628792796154	9.73141117549993	19506
93173cf1930022938f5fb113c6acca85bb313967	identification of time petri net models	timing fault diagnosis structural rings acceleration petri nets context fault detection;system identification discrete event systems mixed integer linear programming petri nets	This paper deals with the identification of time Petri net systems. An identification algorithm for timed net systems must take into account that the firing of a transition requires not only that the enabling condition is met, as in untimed net systems, but it is also required that the firing interval of a transition is congruent with the observed firing instant times. The key idea behind the approach is to express these conditions by a set of logical propositions that can be directly transformed into linear mixed-integer inequalities. The identification algorithm consists of building the logical propositions from the observed behavior and solving a mixed-integer linear programming problem.	algorithm;integer programming;linear programming;petri net	Francesco Basile;Pasquale Chiacchio;Jolanda Coppola	2017	IEEE Transactions on Systems, Man, and Cybernetics: Systems	10.1109/TSMC.2016.2523929	discrete mathematics;real-time computing;stochastic petri net;computer science;distributed computing;process architecture;petri net	Logic	-6.6819127526748145	28.091489122748865	19527
0a7046c406682875587d2b19f27b7e902f4dd474	modern features for systems programming languages	programming language;multiparadigm programming;expressive power;fortran;lisp	This paper presents a case for the design and implementation of a modern programming language for systems programming. It shows that traditional systems languages like C and Fortran possess features no longer relevant to the modern world. The paper also demonstrates how many of these features have a negative impact on the practice of systems programming. Finally, it proposes alternatives to these features that promote better practice.Additionally, the paper presents a number of features that should be included in a modern systems languages and argues in favor of their inclusion. It shows that these features have a beneficial impact on the expressive power of the language or the practices it promotes. The paper also demonstrates that these features do not compromise the objectives of simplicity, efficiency, and direct control that characterize a systems language.	expressive power (computer science);fortran;programming language;system programming	Eric L. McCorkle	2006		10.1145/1185448.1185599	fourth-generation programming language;first-generation programming language;declarative programming;very high-level programming language;programming domain;reactive programming;computer science;programming language generations;extensible programming;third-generation programming language;functional logic programming;programming paradigm;procedural programming;symbolic programming;inductive programming;fifth-generation programming language;programming language theory;programming language;second-generation programming language;high-level programming language;comparison of multi-paradigm programming languages;algorithm	OS	-24.92854944982375	25.881464739116694	19538
0faab399051467b52623c52f4e154c29e54f205b	relational semantics revisited	fixpoints;egli milner ordering;powerdomains;denotational semantic;hoare ordering;relational semantics;smyth ordering	We examine in this paper to what extent at domains containing a `bottom' element, domain constructions such as Plotkin's, and special orderings such as the Egli-Milner ordering or the Smyth ordering can be eschewed (i.e., replaced, respectively, by sets, powersets and the subset relation) in the denotational deenition of the relational semantics of (possibly innnitely) nondeterministic sequential programs.	kripke semantics;plotkin bound	Eike Best;Kerstin Strecker	2009	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2009.07.072	natural language processing;computer science;programming language;kripke semantics;algorithm	Logic	-11.683886565406368	17.177175992285438	19584
d9ee86256c48529e74b6e12e11494757565fe2b6	modeling and reasoning with paraconsistent rough sets	approximate reasoning;engineering and technology;teknik och teknologier;four valued logics;rough sets;rough set;paraconsistent reasoning	We present a language for defining paraconsistent rough sets and reasoning about them. Our framework relates and brings together two major fields: r ough sets [23] and paraconsistent logic programming [9]. To model inconsistent and incomplete info rmation we use a four-valued logic. The language discussed in this paper is based on ideas of our p revious work [21, 32, 22] developing a four-valued framework for rough sets. In this approach mem bership function, set containment and set operations are four-valued, where logical values ar t (t ue),f (false),i (inconsistent) andu (unknown). We investigate properties of paraconsistent rough sets as w ell as develop a paraconsistent rule language, providing basic computational machinery for our app roach.	approximation;computation;computer programming;fixed point (mathematics);four-valued logic;literal (mathematical logic);logic programming;paraconsistent logic;recursion;rough set;semantic similarity;simulation	Aida Vitória;Jan Maluszynski;Andrzej Szalas	2009	Fundam. Inform.	10.3233/FI-2009-209	discrete mathematics;rough set;paraconsistent logic;computer science;machine learning;mathematics;algorithm;dominance-based rough set approach	AI	-13.971073174518622	11.926288809047099	19610
226b15f24e3557ae403402ee97391455f832da35	on warranted inference in argument trees based framework	argument tree;previous desirable conclusion;desirable conclusion;inference relation;warranted inference;dialectical principle;flat belief base;consistent formula;so-called warranted inference;new inference relation	In this paper, we focus on logical argumentation introduced by Besnard and Hunter. First, we consider the so-called warranted inference which is based on the dialectical principle that is widely used in the literature of argumentatation. More precisely, we compare warranted inference with respect to the most frequently used coherence based approaches from flat belief bases in terms of productivity. It turns out that warranted inference is incomparable, w.r.t. productivity, with almost the coherence based approaches considered in this paper. Also, although too productive in some situations, warranted inference does not entail some very desirable conclusions which correspond to those which can be entailed from each consistent formula. Then, we introduce a new inference relation where the key idea is that the support of a counter-argument must not entail the conclusion of the objected argument which is quite intuitive. We show then that this inference relation ensures the inference of the previous desirable conclusions. Besides, we suggest to distinguish two levels of attacks: strong attacks and weak attacks. We propose then to weight our new inference relation based on the structure of the argument tree and also by taking into account the level strength of attacks.		Safa Yahi	2011		10.1007/978-3-642-23963-2_16	artificial intelligence;disjunction introduction;machine learning;mathematics;algorithm;statistics	NLP	-14.644178379400735	5.477843173632032	19630
07175fb49a41ba3f477e3c08cf5daa4bff773f00	betaeta-complete models for system f	computacion informatica;ciencias basicas y experimentales;matematicas;grupo a	We show that Friedman's proof of the existence of non-trivial βη-complete models of λ→ can be extended to system F. We isolate a set of conditions that are sufficient to ensure βη-completeness for a model of F (and α-completeness at the level of types), and we discuss which class of models we get. In particular, the model introduced in Barbanera and Berardi (1997), having as polymorphic maps exactly all possible Scott continuous maps, is βη-complete, and is hence the first known complete non-syntactic model of F. In order to have a suitable framework in which to express the conditions and develop the proof, we also introduce the very natural notion of ‘polymax models’ of System F.	system f	Stefano Berardi;Chantal Berline	2002	Mathematical Structures in Computer Science	10.1017/S0960129502003778	artificial intelligence;mathematics;algorithm	Logic	-8.557639792212285	15.321962238329473	19641
1af3e874820e7d5a6bfbccf57cedc6b3fab8f477	multi-stage javascript		Multi-stage languages support generative metaprogramming via macros evaluated in a process preceding the actual interpretation or compilation of the program in which they are used. Macros update the source of their hosting program by emitting code that takes their place in the file, while their code may also be produced, fully or partially, by nested macros. All macros at the same nesting belong to the same stage, with the outer stage collecting the macros affecting only the main program. We extended JavaScript with staging annotations and implemented them in Spider Monkey, emitting pure JavaScript code as the final outcome of stage computation. We discuss how the original Spider Monkey system is minimally affected with extensions in the syntax, parser and internal AST structures, and the addition of an unparser, a staging loop, some library functions and a debugger backend component for AST inspection. Since stages have a generative metaprogramming role we do not foresee any interplay with the browser DOM, and thus there is no reason to repeat their evaluation on every page load. Hence, such JavaScript extensions are meant only for development-time, emitting pure JavaScript code that can be run in any browser. Finally, to enable debugging stages in any browser we implemented a pure JavaScript client, communicating with the extended Spider Monkey, and offering the necessary AST display and unparsing that a browser debugger does not provide.	compiler;computation;debugger;debugging;disk staging;entry point;full scale;javascript engine;metaprogramming;multistage interconnection networks;software deployment;unparser;wheels	Anthony Savidis;Yannis Apostolidis;Yannis Lilis	2018	CoRR		debugging;programming language;debugger;syntax;parsing;computer science;metaprogramming;macro;unparser;javascript	PL	-27.84983005858956	26.464849461970132	19657
721c3f205bbb439f206dfef60b1e3c1f5d37e454	theory exploration versus theorem proving	theorem proving		automated theorem proving	Bruno Buchberger	1999	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(05)80610-3	discrete mathematics;computer science;fundamental theorem;mathematics;automated theorem proving;programming language;algorithm	Logic	-19.056363775347922	18.59996545900183	19759
8f5df35d52f83e51223a63aa8ee26f200577c6f4	non-classical operations hidden in classical logic	4 valued logic;necessity;negation;classical logic;kripke semantics	Objects of consideration are various non-classical connectives “hidden” in the classical logic in the form of G˛s with ˛ —a classical connective, and s—a propositional variable. One of them is negation, which is defined as G ⇒ s; another is necessity, which is defined as G ∧ s. The new operations are axiomatized and it is shown that they belong to the 4-valued logic of Lukasiewicz. A 2-point Kripke semantics is built leading directly to the 4-valued logical tables.		V. Sotirov	2008	Journal of Applied Non-Classical Logics	10.3166/jancl.18.309-324	modal logic;predicate logic;dynamic logic;zeroth-order logic;complete theory;discrete mathematics;classical logic;higher-order logic;paraconsistent logic;philosophy;many-valued logic;intuitionistic logic;computer science;negation as failure;intermediate logic;predicate functor logic;negation;mathematics;linguistics;kripke semantics;logic;multimodal logic;algorithm;philosophy of logic;rule of inference;autoepistemic logic	Theory	-14.321055308094865	10.176914641785036	19766
cd750162c01b78f6797cf83631d4aa18f7841ba8	loop checking in partial deduction	partial deduction	D In the framework of Lloyd and Shepherdson [16], partial deduction involves the creation of SLDNF-trees for a given program and some goals up to certain halting points. This paper identifies the relation between halting criteria for partial deduction and loop checking (as formalized in [l]). For simplicity, we consider only positive programs and SLD-resolution here. It appears that loop checks for partial deduction must be complete, whereas traditionally, the soundness of a loop check is more important. However, it is also shown that sound loop checks can contribute to improve partial deduction. Finally, a class of complete loop checks suitable for partial deduction is identified. a	attack tree;computer science;divergence (computer science);echo (computing);for loop;halting problem;natural deduction;sld resolution;standard sea level;turing completeness	Roland N. Bol	1993	J. Log. Program.	10.1016/0743-1066(93)90022-9	discrete mathematics;computer science;mathematics;algorithm	PL	-16.32339538168754	21.200333198887417	19781
d36543c752e3460203dd95fc467b9cfbe2debdce	λ z : zermelo's set theory as a pts with 4 sorts	theorie type;language theory;teoria conjunto;cerradura transitiva;theorie ensemble;teoria lenguaje;program verification;set theory;verificacion programa;fermeture transitive;type theory;transitive closure;verification programme;theorie langage	We introduce a pure type system (PTS) λZ with four sorts and show that this PTS captures the proof-theoretic strength of Zermelo’s set theory. For that, we show that the embedding of the language of set theory into λZ via the ‘sets as pointed graphs’ translation makes λZ a conservative extension of IZ + AFA + TC (intuitionistic Zermelo’s set theory plus Aczel’s antifoundation axiom plus the axiom of transitive closure)—a theory which is equiconsistent to Zermelo’s. The proof of conservativity is achieved by defining a retraction from λZ to a (skolemised version of) Zermelo’s set theory and by showing that both transformations commute via the axioms AFA and TC.	alternating finite automaton;pure type system;set theory;transitive closure	Alexandre Miquel	2004		10.1007/11617990_15	urelement;zermelo–fraenkel set theory;zermelo set theory;computer science;philosophy of language;programming language;transitive closure;s;type theory;algorithm;set theory	Logic	-12.086705814560698	17.105585366544165	19814
2c16df89941129b3ee5f2e8ccab34fa986d61587	a file system based on concept analysis	navegacion informacion;formal specification;information retrieval;navigation information;interrogation base donnee;information browsing;interrogacion base datos;logical programming;information organization;specification formelle;especificacion formal;organizacion informacion;concept analysis;programmation logique;recherche information;file system;organisation information;recuperacion informacion;programacion logica;database query	We present the design of a le system whose organization is based on Concept Analysis à la WilleGanter . The aim is to combine querying and navigation facilities in one formalism. The le system is supposed to o er a standard interface but the interpretation of common notions like directories is new. The contents of a le system is interpreted as a Formal Context, directories as Formal Concepts, and the sub-directory relation as Formal Concepts inclusion. We present an organization that allows for an e cient implementation of such a Conceptual File System. Key-words: le systems, Concept Analysis, querying, navigation, logic.	algorithm;approximation;automated theorem proving;cache (computing);climate forecast system;diagram;directory (computing);electronic organizer;formal concept analysis;linear algebra;prototype;remote file sharing;semantics (computer science)	Sébastien Ferré;Olivier Ridoux	2000		10.1007/3-540-44957-4_69	computer science;formal concept analysis;class implementation file;data mining;formal specification;database;data file;global namespace;world wide web	AI	-30.575793059697162	8.652897550978006	19818
5f0de8bb1c800eb18a9868a0316ded2920ce88d7	dryvr 2.0: a tool for verification and controller synthesis of black-box cyber-physical systems		"""We present a demo of DryVR 2.0, a framework for verification and controller synthesis of cyber-physical systems composed of black-box simulators and white-box automata. For verification, DryVR 2.0 takes as input a black-box simulator, a white-box transition graph, a time bound and a safety specification. As output it generates over-approximations of the reachable states and returns """"Safe"""" if the system meets the given bounded safety specification, or it returns """"Unsafe"""" with a counter-example. For controller synthesis, DryVR 2.0 takes as input black-box simulator(s) and a reach-avoid specification, and uses RRTs to find a transition graph such that the combined system satisfies the given specification."""	approximation;automata theory;black box;cyber-physical system;simulation	Bolun Qi;Chuchu Fan;Minghao Jiang;Sayan Mitra	2018		10.1145/3178126.3187008	model checking;real-time computing;matlab;control theory;black box (phreaking);cyber-physical system;bounded function;computer science;graph	Logic	-10.490529195162637	28.01073747020323	19829
a9fded175d1cffd268fb6829e517e0569f4b20eb	formalization and detection of events using interval-based semantics	active database;event detection;specification language;active objects;event condition action;sliding window	Active databases utilize Event-Condition-Action rules to provide active capability to the underlying system. An event was initially defined to be an instantaneous, atomic occurrence of interest and the time of occurrence of the last event in an event expression was used as the time of occurrence for an entire event expression (detection-based semantics), rather than the interval over which an event expression occurs (interval-based semantics). This introduces semantic discrepancy for some operators when they are composed more than once. Currently, all active databases detect events using the detection-based semantics rather than the interval-based semantics. SnoopIB is an interval-based event specification language developed for expressing primitive and composite events that are part of active rules. Algorithms for event detection using interval-based semantics pose some challenges, as not all events are known (especially their starting points). In this paper, we address the following: 1) briefly explain the need for interval-based semantics, 2) formalization of events accumulated over a semantic window and 3) how diversified events (e.g., sliding window, accumulated) are detected using interval-based semantics in the context of Sentinel – an active object oriented database.	active database;active object;algorithm;discrepancy function;event condition action;specification language	Raman Adaikkalavan;Sharma Chakravarthy	2005			sliding window protocol;real-time computing;specification language;computer science;data mining;database;programming language	DB	-25.11161651070536	13.95755176594636	19842
afe07ae778c225596e3a0be47aaa7900f0c557d9	exploiting fixable, removable, and implied values in constraint satisfaction problems	interchangeability;search space;intelligence artificielle;logical programming;constraint satisfaction;satisfaction contrainte;programmation logique;interchangeabilite;intercambiabilidad;artificial intelligence;constraint solving;constraint satisfaction problem;inteligencia artificial;satisfaccion restriccion;programacion logica	Complete algorithms for constraint solving typically exploit properties like (in)consistency or interchangeability, which they detect by means of incomplete yet effective algorithms and use to reduce the search space. In this paper, we study a wide range of properties which includes most of the ones used by existing CSP algorithms as well as some which have not yet been considered in the literature, and we investigate their use in CSP solving. We clarify the relationships between these notions and characterise the complexity of the problem of checking them. Following the CSP approach, we then determine a number of relaxations (for instance local versions) which provide sufficient conditions whose detection is tractable. This work is a first step towards a comprehensive framework for CSP properties, and it also shows that new notions still remain to be exploited.	constraint satisfaction;removable media	Lucas Bordeaux;Marco Cadoli;Toni Mancini	2004		10.1007/978-3-540-32275-7_19	constraint logic programming;mathematical optimization;constraint satisfaction;constraint learning;computer science;constraint graph;artificial intelligence;machine learning;constraint satisfaction dual problem;mathematics;complexity of constraint satisfaction;constraint satisfaction problem;algorithm;hybrid algorithm;local consistency	Theory	-7.6961620376356725	19.49473473886428	19872
29147459563ad5b1fb305bd16a8c64b5d428786c	type systems for programming languages	type system;programming language	1 These course notes are intended for the personal use of students in Computer Science 15{814 at Carnegie Mellon University. This is an incomplete, working draft not intended for publication. Please do not distribute these notes without the permission of the author.	computer science	John C. Mitchell	1990			system f;typed lambda calculus;theoretical computer science;simply typed lambda calculus;mathematics;programming language;algorithm	Logic	-20.9106246997465	19.902723727767984	19908
6b0578e07ea17a8cd5f8d40fdb4f6bb13baaf26b	towards a formal model for dynamic networks through refinement and evolving graphs		Due to the highlydynamic behavior and the time complexity in Mobile Ad-hoc NEtworks (MANETs), modeling distributed algorithms and looking at their assumptions represent a challenging research task. Also, proving the correctness of these algorithms for dynamic networks is a topic of intensive research. In fact, the solutions which have been proposed to express and prove the correctness of distributed algorithms are usually done manually. In addition, all these solutions lack a consensus about their development and their proof. The main contribution of this paper is to propose a general and formal model for dynamic networks based on evolving graphs and Event-B formal method. In fact, evolving graphs is a powerful tool to express fine-grained properties. This model allows to handle topological events and to characterize the concept of time with some particularities. We implement it with Event-B, based on refinement technique. To illustrate the proposed model, we investigate an example of a distributed algorithm encoded by local computations models.	refinement (computing)	Faten Fakhfakh;Mohamed Tounsi;Ahmed Hadj Kacem;Mohamed Mosbah	2015		10.1007/978-3-319-23509-7_16	combinatorics;discrete mathematics;machine learning	PL	-11.744012111400986	29.8468803457357	19917
063f592e916b9be7bd754353bdea5e71aac6cb3d	thread-sensitive points-to analysis for multithreaded java programs	sensibilidad contexto;context aware;interprocedural analysis;context information;java programming;flot donnee;langage java;flujo datos;program verification;analisis programa;verificacion programa;sensitivity analysis;multithread;lenguaje java;analyse information;multitâche;program analysis;sensibilite contexte;analyse programme;data flow;verification programme;information analysis;multitarea;java language;points to analysis	Every running thread has its own thread context that consists of values of the fields of the target thread object. To consider the thread context in understanding the behaviors of concurrently running threads, we propose a thread-sensitive interprocedural analysis for multithreaded Java applications. Our thread-sensitive analysis exploits thread-context information, instead of the conventional calling-context information, for computing dataflow facts holding at a statement. The thread-sensitive analysis is highly effective in distinguishing dataflow facts for different threads, producing more precise dataflow information than non-thread-sensitive analysis. The analysis is also generally much more efficient than conventional (calling) context-sensitive analysis. It uses the target thread objects at a thread start site to distinguish different thread contexts. We give a thread-sensitive points-to analysis as an instance of thread-sensitive analysis. We have implemented it and give some experimental results. We discuss several possible applications of the analysis.	compiler;context-sensitive grammar;dataflow;emulator;escape analysis;exception handling;interpreter (computing);interprocedural optimization;java;lam/mpi;pointer (computer programming);pointer analysis;sourceforge;static program analysis;thread (computing);type system;virtual machine	Byeong-Mo Chang;Jong-Deok Choi	2004		10.1007/978-3-540-30182-0_95	program analysis;data flow diagram;thread;real-time computing;java concurrency;computer science;operating system;thread safety;data analysis;programming language;sensitivity analysis;statistics	PL	-22.80169623380623	31.67881969455244	19918
c24c6ffcc1b67c919815c2482917c30dc77b2958	expressiveness and nash equilibrium in iterated boolean games	nash equilibrium;expressiveness;iterated boolean games;ltl	We introduce and investigate a novel notion of expressiveness for temporal logics that is based on game theoretic properties of multiagent systems. We focus on iterated Boolean games, where each agent i has a goal γi, represented using (a fragment of) Linear Temporal Logic (LTL). The goal γi captures agent i’s preferences: the models of γi represent system behaviours that would satisfy i, and each player is assumed to act strategically, taking into account the goals of other players, in order to bring about computations satisfying their goal. In this setting, we apply the standard gametheoretic concept of Nash equilibria: the Nash equilibria of an iterated Boolean game can be understood as a (possibly empty) set of computations, each computation representing one way the system could evolve if players chose strategies in Nash equilibrium. Such an equilibrium set of computations can be understood as expressing a temporal property—which may or may not be expressible within a particular LTL fragment. The new notion of expressiveness that we study is then as follows: what LTL properties are characterised by the Nash equilibria of games in which agent goals are expressed in fragments of LTL? We formally define and investigate this notion of expressiveness and some related issues, for a range of LTL fragments.	agent-based model;boolean algebra;boolean satisfiability problem;computation;expressive power (computer science);game theory;inferring horizontal gene transfer;iterated function;iteration;linear temporal logic;model checking;multi-agent system;nash equilibrium;propositional variable;software agent;time complexity	Julian Gutierrez;Paul Harrenstein;Giuseppe Perelli;Michael Wooldridge	2016			epsilon-equilibrium;best response;expressivity;equilibrium selection;algorithm;nash equilibrium	AI	-17.50624532095825	4.935339947731676	19966
d9ef3cf8d641769998e1a456fa67059f3297acfc	integration of time in canonical testers for real-time systems	sequences;formal specification;time integration canonical testers real time systems formal specification models extended time input output state machine test sequences;input output programs;state machine;program verification;input output;system testing real time systems application software robustness control systems computer applications humans protocols explosions delay effects;finite state machines;program testing;program verification real time systems input output programs finite state machines formal specification program testing sequences;real time systems	Testing a real-time system is not an easy thing, because the notion of time is not under control. In this paper, we present different formal models of specification that introduce time. We use one of these, the extended-time input-output state machine, to develop an algorithm to create a canonical tester which is used afterwards for producing test sequences with time.	real-time clock;real-time computing	Patrice Laurençot;Richard Castanet	1997		10.1109/WORDS.1997.609956	control engineering;real-time computing;formal verification;computer science;formal specification;algorithm	Embedded	-11.362768471860436	26.87480985481518	19969
7e8b703d721d105764e3f0add65f61f6573adf57	deciding the satisfiability of mitl specifications		In this paper we present a satisfiability-preserving reduction from MITL interpreted over finitelyvariable continuous behaviors to Constraint LTL over clocks, a variant of CLTL that is decidable, and for which an SMT-based bounded satisfiability checker is available. The result is a new complete and effective decision procedure for MITL. Although decision procedures for MITL already exist, the automata-based techniques they employ appear to be very difficult to realize in practice, and, to the best of our knowledge, no implementation currently exists for them. A prototype tool for MITL based on the encoding presented here has, instead, been implemented and is publicly available.	automata theory;boolean satisfiability problem;common lisp the language;decision problem;prototype	Marcello M. Bersani;Matteo Rossi;Pierluigi San Pietro	2013		10.4204/EPTCS.119.8	discrete mathematics;theoretical computer science;mathematics;algorithm	Logic	-13.99931510951992	25.509976671659942	19991
2f2e7c33ce939b8ea4a9d4fd71826b74e56e54a5	here's the beef: answer set programming !	answer set programming;knowledge representation and reasoning;nonmonotonic reasoning;logic programs	At the occasion of the Third International Conference on Principles of Knowledge Representation and Reasoning [1] in 1992, Ray Reiter delivered an invited talk entitled “Twelve Years of Nonmonotonic Reasoning Research: Where (and What) Is the beef?”,1,2 reflecting the state and future of the research area of Nonmonotonic Reasoning (NMR;[2]). Ray Reiter describes it in [3] as a “flourishing subculture” making many outside researchers “wonder what on earth this stuff is good for.” Although he seemed to be rather optimistic about the future of NMR, he nonetheless saw its major contribution on the theoretical side, providing “important insights about, and solutions to, many outstanding problems, not only in AI but in computer science in general.” Among them, he lists “Logic Programming implementations of nonmonotonic reasoning”. Although the link between Michael Gelfond and Vladimir Lifschitz’ Stable Model Semantics for Logic Programming [4] and NMR formalisms like Ray Reiter’s Default Logic [5] were discovered soon after the proposal of Stable Model Semantics,3 it still took some years until the first such implementation was conceived, namely, the smodels system [8, 9]. The emergence of such a highly efficient and robust system has boosted the combination of Logic Programming and NMR and finally led to a novel declarative programming paradigm, referred to as Answer Set Programming (ASP;[10–14]). Since its inception, ASP has been regarded as the computational embodiment of Nonmonotonic Reasoning and a primary candidate for an effective tool for Knowledge Representation and Reasoning. After all, it seems nowadays hard to dispute that ASP brought new life to Logic Programming and NMR research and has become a major driving force for these two fields, helping dispel gloomy prophecies of their impending demise. Meanwhile, the prospect of ASP has been demonstrated in numerous application scenarios, including bio-informatics [15, 16], configuration [17], database integration [18], diagnosis [19], hardware design [20], insurance industry [21], model checking [22], phylogenesis [23, 24], planing [12], security protocols [25], etc.4 A highlight among these applications is arguably the usage of ASP for the high-level control of the space shuttle [26, 27]. The increasing popularity of ASP is for one thing due to the availability of efficient off-the-shelf ASP systems [28–32] and for another due to its rich modeling language, jointly allowing for an easy yet efficient handling of	answer set programming;bioinformatics;british informatics olympiad;computation;computer science;declarative programming;default logic;emergence;heterogeneous database system;high- and low-level;knowledge representation and reasoning;logic programming;michael gelfond;model checking;modeling language;non-monotonic logic;phylogenesis;planning;programming paradigm;raymond reiter;stable model semantics;vladimir lifschitz	Torsten Schaub	2008		10.1007/978-3-540-89982-2_16	knowledge representation and reasoning;computer science;artificial intelligence;non-monotonic logic;answer set programming;deductive reasoning;algorithm	AI	-20.77757049026486	7.3119166569785445	20019
e51e767c08ad3f3477233b5f9a0d0ab1ab18a264	on the completeness and decidability of duration calculus with iteration	variable etat;systeme temps reel;expresion regular;variable binding;quantifier;state variable;specification;metodo formal;methode formelle;formal methods;automaton;higher order;formal method;automata;calcul duree;duration calculus;especificacion;informatique theorique;automate;quantificateur;variable estado;expression reguliere;completitud;real time system;decidibilidad;timed automata;sistema tiempo real;completeness;decidabilite;cuantificador;completude;regular expression;decidability;duration calcul;computer theory;real time systems;informatica teorica	The extension of the duration calculus (DC) by iteration, which is also known asKleene star , enables the straightforward specification of repetitive behaviour in DC and facilitates the translation of design descriptions between DC, timed regular expressions and timed automata. In this paper we present axioms and a proof rule about iteration in DC.We consider abstract-time DC and its extension by a state-variable binding existential quantifier known as higher-order DC (HDC). We show that the -complete proof systems for DC and HDC known from our earlier work can be extended by our axioms and rule in various ways in order to axiomatise iteration completely. The additions we propose include either the proof rule or an induction axiom. We also present results on the decidability of a subset of the extension DC ∗ of DC by iteration. © 2005 Elsevier B.V. All rights reserved.	automata theory;duration calculus;existential quantification;first-order logic;graphics device interface;iteration;mathematical induction;quantifier (logic);regular expression;timed automaton	Dimitar P. Guelev;Dang Van Hung	2005	Theor. Comput. Sci.	10.1016/j.tcs.2005.01.017	discrete mathematics;formal methods;computer science;mathematics;automaton;programming language;algorithm	Logic	-9.846576693295724	25.28580229782796	20028
ff32984a6dbe9d9009ca1684597691bc7e4857d6	verification of siphons and traps for algebraic petri nets	tool support;red petri;theorem proving;concurrent systems;informatique theorique;systeme concurrent;petri net;correctness proof;reseau petri;computer theory;symbolic representation;informatica teorica	Siphons and traps are structures which allow for some implications on the net's behaviour and can be used in manual correctness proofs for concurrent systems. We introduce symbolic representations of siphons and traps which work quite well even in infinite cases and are still intuitively readable. The verification of symbolic siphons and traps is traced back to unification and structural induction on the terms. This approach is motivated by some additional considerations. For unification and other proposed structural reasoning mechanisms tool support is given by completeness proof tools studied in the theorem-proving community.	algebraic petri net	Karsten Wolf	1997		10.1007/3-540-63139-9_49	discrete mathematics;computer science;artificial intelligence;mathematics;automated theorem proving;programming language;petri net;algorithm	Logic	-14.244985316229965	21.845695627429986	20029
35b33bdde20d9b7001d77a7d3e26961643b885ce	logics for representation of propositions with fuzzy modalities		In the paper we introduce logical calculi for representation of propositions with modal operators indexed by fuzzy values. There calculi are called Heyting-valued modal logics. We introduce the concept of a Heyting-valued Kripke model and consider a semantics of Heyting-valued modal logics at the class of Heyting-valued Kripke models.	algorithm;dynamical system;finite model property;first-order predicate;fuzzy logic;kripke semantics;modal logic;modal operator;model checking;specification language	Andrew M. Mironov	2016			fuzzy logic;t-norm fuzzy logics;combinatorics;modalities;discrete mathematics;mathematics	AI	-13.147818178036987	12.72556806081837	20042
338882e9e3f92d5146d2d244e4cc36f52ddde68b	positive constructed formulas preprocessing for automatic deduction	control theory;electronic mail;syntactics;heuristic algorithms;calculus;planning;algorithm design and analysis	The preprocessing of positively-constructed formulas (PCFs) for automatic deduction search algorithms is considered in this paper. A new efficient algorithm for conversion of the predicate calculus language formulas to the language of PCFs and equivalent rules of reducing PCFs, preserving the original heuristic structure of knowledge represented by the formulas of the predicate calculus language, are presented.	automated theorem proving;conjunctive normal form;database normalization;disjunctive normal form;first-order logic;heuristic;natural deduction;preprocessor;programming computable functions;search algorithm	Evgeny A. Cherkashin;Artem Davydov;Alexander Larionov	2016	2016 39th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)	10.1109/MIPRO.2016.7522341	planning;algorithm design;computer science;theoretical computer science;management;algorithm	AI	-16.4353550490388	16.30347550393705	20084
16077ad3c95a5e8f100f0b2ae7bd8f18329e917f	a decade of software model checking with slam	verificacion modelo;software model checking;verification modele;program verification;analisis programa;verificacion programa;model checking;program analysis;analyse programme;verification programme	SLAM is a program-analysis engine used to check if clients of an API follow the APIu0027s stateful usage rules.	application programming interface;model checking;program analysis;simultaneous localization and mapping;stateful firewall	Thomas Ball;Vladimir Levin;Sriram K. Rajamani	2011	Commun. ACM	10.1145/1965724.1965743	program analysis;model checking;computer science;operating system;database;programming language;algorithm	SE	-22.43234450822579	31.070802841229295	20099
e9b2638c8978e2c8677c3947c2040c8d8d24dab0	updating dl-lite ontologies through first-order queries		In this paper we study instance-level update in DL-LiteA, the description logic underlying the owl 2 ql standard. In particular we focus on formula-based approaches to ABox insertion and deletion. We show that DL-LiteA, which is well-known for enjoying first-order rewritability of query answering, enjoys a first-order rewritability property also for updates. That is, every update can be reformulated into a set of insertion and deletion instructions computable through a nonrecursive datalog program. Such a program is readily translatable into a first-order query over the ABox considered as a database, and hence into sql. By exploiting this result, we implement an update component for DL-LiteA-based systems and perform some experiments showing that the approach works in practice.	abox;adobe flash lite;computable function;conjunctive query;datalog;description logic;experiment;first-order predicate;ontology (information science);query (complexity);rdf schema;recursive language;sparql;sql;tbox	Giuseppe De Giacomo;Xavier Oriol;Riccardo Rosati;Domenico Fabio Savo	2016		10.1007/978-3-319-46523-4_11	data mining;sql;description logic;theoretical computer science;computer science;web ontology language;database;abox;differentiation rules;conjunctive query;ontology (information science);datalog	DB	-24.41881533846365	8.95890888798477	20105
30bf0d0fac25a6e729ec80ccc787605829c6e1e6	recent complexity results in logic programming and nonmonotonic reasoning, and why they matter (invited talk abstract)	nonmonotonic reasoning		logic programming;non-monotonic logic	Georg Gottlob	1993				AI	-13.710679963156943	10.70664841189481	20162
64592a90239b97d1ee0b7fff2713f6b6e198b2f9	why just boogie? translating between intermediate verification languages	formal verification	The verification systems Boogie and Why3 use their respective intermediate languages to generate verification conditions from high-level programs. Since the two systems support different back-end provers (such as Z3 and AltErgo) and are used to encode different high-level languages (such as C# and Java), being able to translate between their intermediate languages would provide a way to reuse one system’s features to verify programs meant for the other. This paper describes a translation of Boogie into WhyML (Why3’s intermediate language) that preserves semantics, verifiability, and program structure to a large degree. We implemented the translation as a tool and applied it to 194 Boogieverified programs of various sources and sizes; Why3 verified 83% of the translated programs with the same outcome as Boogie. These results indicate that the translation is often effective and practically applicable.	encode;formal verification;high- and low-level;intermediate representation;java;structured programming;z3 (computer)	Michael Ameri;Carlo A. Furia	2016	CoRR	10.1007/978-3-319-33693-0%206	formal verification;computer science;theoretical computer science;programming language;algorithm	PL	-21.587777636713398	27.244341974302717	20164
f5a02beeb6ea7d8620aa2484484f798d79e11674	on two ways of querying disjunctive weighted data.				Patrick Bosc;Laurence Duval;Olivier Pivert	1999			database;programming language	AI	-5.684973400956097	8.260607939068509	20182
26f70d1ab8b25dc65a9007b354569835b839f6db	ellipsis in a labelled deduction system	labelled deductive systems	"""Using the LDSNL model of utterance interpretation being developed by Gabbay and Kempson (cf. [17, 29, 30]), this paper demonstrates how the dynamics of the proof process adopted explains configurational restrictions imposed on the interpretation of elliptical fragments. The blurring of traditional semantic and syntactic dichotomies in the LDSNL proof-theoretic reconstruction of interpretation successfully provides a basis for predicting the array of variation displayed by different elliptical forms. The logic adopted is a composite system of a type logic nested within a database logic. Two resource-sensitive sub-types of Conditional Introduction form the basis for explaining the ellipsis data. The result is a demonstration of how the simple device of adding labels to an inference system can provide a useful tool not only at the meta-logic level of comparing alternative logic and grammar formalisms, but also at the level of explaining natural language data. 1 Ellipsis: the puzzle Ellipsis phenomena as in (1) pose a particular challenge under assumptions of current linguistic orthodoxy: (1) John wants to visit Mary in hospital. Sue too (i) Sue does too (ii) Sue wants to too (iii) They provide evidence on the one hand for a purely semantic/pragmatic explanation of how interpretation is assigned to the simple fragmentary string (cf. Dalrymple et al. [12], Priist et al. [41]), and contrarily on the other hand for a configurationally-based syntactic explanation requiring the assignment of richer structure to the string than is visible (cf. Lappin [33], Fiengo and May [14]). Pragmatically, they constitute an extreme case of context-dependency of natural-language interpretation, with various forms, all of which require reconstruction by the hearer of some salient contextually provided one place predicate as part of the process of interpretation. Moreover the interpretation of elliptical fragments may depend exclusively on the way in which the hearer builds up such a one-place predicate as interpretation, for a fragment may be ambiguous despite lack of any ambiguity in its antecedent. In (2) for example a single interpretation of the first conjunct (with his fixed as picking out 'John'), is paired with possible interpretations of the second conjunct either as 'Harry admires John's mother' or as 'Harry admires Harry's mother': Bull, of the 1GPL, Vol. 3 No. 2,3, pp. 489-528 1995 4 8 9 © at U niersity of M antoba on Jne 9, 2015 http://jigpfordjournals.org/ D ow nladed from 490 Ellipsis in a Labelled Deduction System (2) John admires his mother, and so does Harry. However, despite semantic/pragmatic similarity between different elliptical forms, distinct forms nevertheless display different potential for interpretation. First there is a subject restriction, applicable only to VP ellipsis as in (lii) and (liii). Thus the socalled bare argument ellipsis form in (li) (Reinhart [43]) can be used to mean either that Sue wants to visit Mary in hospital, or that John wants to visit Sue in hospital, (lii) and (liii), """"VP ellipsis"""", can only mean the first of these. Secondly, both forms of ellipsis are sensitive to restrictions on their interpretation analogous to restrictions on w/i-gap construal, but in different ways. Parallelling the acceptability of (3) (3) Whoi did John deny that Bill had interviewed eC. bare argument ellipsis allows reconstruction across clausal boundaries, giving rise to an interpretation of the fragment in (4) as 'Mary did not deny that she had taken the icecream': (4) Mary denied that she had taken the cakes. But not the icecream. Again parallelling ty/i-gap construal and the unacceptability of (5): (5) *WhOi did John go out with a woman who is studying e;? the fragment in (6) precludes the construction of an interpretation which would involve reconstructing the fragment across the relative clause boundary of the antecedent sentence as 'John is also going out with a woman who is studying Racine': (6) John is going out with a woman who is studying Palestrina. Racine too. (7), similarly, allows no reconstruction across the relative clause boundary (7) They are interviewing the man who Bill assaulted. But not Harry. The apparent """"subjacency"""" effect does not extend to w/i-islands or sentential subjects. Unlike the unacceptability of (8)-(9) (8) *Who do you know why e< stole from the shop? (9) *Who was that John doted on a clear? (10) and (11) fully license reconstruction of the fragment across the intervening clausal boundary: The data appear to vary according as the inferential effect of the fragment varies. Fragments as additions such as these preclude reconstruction across a relative clause boundary. Corrections and fragment wh questions do not (Oeirdre Wilson and Robin Cooper pc) (i) I've been befriending the man who stole diamonds from Harrods. Sorry rubies. (11) """"I'm giving 200,000 ecus to the institution that obtains the most money from a European foundation."""" """"Which foundation?"""" 1 return to these data later. at U niersity of M antoba on Jne 9, 2015 http://jigpfordjournals.org/ D ow nladed from I. ELLIPSIS: THE PUZZLE 491 (10) I know why Bill stole from the shop. But not Sam. (11) That John doted on Mary was clear. And Sue too. This bifurcation of subjacency effects, some carried over to ellipsis, others not, does not extend to VP ellipsis. To the contrary, though V'P-ellipsis reconstruction might seem not to provide the environment for testing such subjacency effects since it only allows an interpretation in which the subject of the elliptical VP is reconstructed as the subject of the recovered predicate, with no possibility of reconstructing it into some subordinate position, there are subjacency effects none-the-less in antecedentcontained VP ellipsis such as (12)-(15), and these exactly parallel tu/i-gap dependencies: (12) John interviewed everyone that Bill said Sue did. (13) *John interviewed everyone that I know the man who did. (14) *John interviewed everyone that I know why John did. (15) *John interviewed everyone who that Bill had was clear. There thus appear to be complex syntactic limitations on the reconstruction of elliptical expressions which a purely interpretive reconstruction would have no means of explaining. However, explaining these phenomena in terms of complex syntactic structure intrinsic to the elliptical expression involves analysing the elliptical form under an interpretation, and this leads to multiple formal ambiguity not only of the elliptical fragment but also in the unambiguous antecedent from which its interpretation is derived (cf. Fiengo and May [14]). In consequence such a solution does not reconstruct the semantic/pragmatic observation that ellipsis involves a mapping from some weak specification of content onto some more fully specified interpretation relative to the context in which it occurs. In this paper I adopt the LDSNL strategy (Gabbay and Kempson [17]) of defining subject proof-theoretically through a control label and particularised form of -> Elimination and, with a correspondingly distinct subtype of -> Introduction, analyse the different forms of ellipsis in terms of two types bf -»• Introduction. This simple proof-theoretic distinction is, I shall claim, sufficient when set within the general framework of LDSNL both to capture the diverse configurationaJ constraints on the interpretation process and nevertheless to retain the semantic/pragmatic insight that the linguistic input underdetermines the proposition which is its assigned content. In particular, the use of a logic more stringent than a simple typed logic provides just the additional restrictiveness needed. 1.1 LDSti^: a summary The point of departure for all explanations within the LDSNL framework is the following pair of informal observations, and I impose as a minimum criterion of success on any account of elliptical fragments that it reflect these. at U niersity of M antoba on Jne 9, 2015 http://jigpfordjournals.org/ D ow nladed from 492 Ellipsis in a Labelled Deduction System (i) Utterance interpretation is a left-right reasoning process in which, by the information encoded with each word in sequence, the hearer progressively builds a structure which she takes to correspond to the interpretation intended by the speaker. (ii) The input to interpretation provided by words underdetermines the content attributed to them in context, and as part of the interpretation process, the hearer has to make choices for all under-determined aspects. It is this inferential activity of utterance interpretation which the LDSNL system purports to model. The claim of Gabbay and Kempson [17] and Kempson [28, 29, 30] is that this inferential task is a goal-driven task of labelled natural deduction. Summarising, we assume the task for the hearer is to establish the proposition expressed by the words the speaker has used, a formula of logical type t whose content is to be established through steps of inference, in part deduction over the logical type-specifications of the provided words (as in other parsing as deduction frameworks—Pereira [38, 39], Konig [31], Moortgat [36], Hepple [22], Morrill et al. [37]). Some of the words provide a labelled formula (= 'declarative unit'), the word itself being the label, the formula being the logical type specification associated with that word. Some provide annotations on those labelled formulae imposing an order on their application (in particular the concept of subject is expressed as an ordering on the premises that one differentiated premise be used last in a chain of deductions leading to a : t.). Others provide instructions on the requisite proof configuration to be built e.g. wh expressions, which dictate the building of an independent database linked to its host only through unification of a term of the labelling algebra in each database. Yet others impose extra choices, as in anaphora and, as we shall see, ellipsis. The overall task is to build a labelled proof co"""	anaphora (linguistics);bifurcation theory;emoticon;first-order predicate;inference engine;inferential programming;inferential theory of learning;interpretation (logic);john collison;logic level;multidimensional digital pre-distortion;natural deduction;natural language;parsing;regular expression;type theory;unification (computer science)	Ruth Kempson	1995	Logic Journal of the IGPL	10.1093/jigpal/3.2-3.489	computer science	NLP	-10.594427827844122	6.3344447933354395	20185
bb23a5c90f1c8a7e71805458eb57c4796d407099	towards creation of logical framework for event-driven information systems	logical framework;information system	Event-Condition-Action (ECA) rules offer extensible and flexible approach to realizing active Enterprise Information Systems. Such systems are enabled to actively respond on events or state changes. Hence their behavior is programable by means of ECA rules. We propose an implementation of ECA rules in a completely logical framework, usingTransaction Datalog ¬ as an underlying logic. In this way, we extend the current ECA framework by means of powerful and declarative semantics, which also have an appropriate procedural interpretation. We show how a logical calculus of Transaction Datalog ¬ can be exploited for realizing composite events, conditions, and actions; justifying the use of declarative semantics for solving some of the existing issues in reactive systems.	algorithm;complex event processing;database;datalog;declarative programming;enterprise information system;event condition action;event-driven programming;f-logic;formal system;high- and low-level;high-level programming language;information systems;java;logic programming;logical framework;mathematical optimization;procedural programming;process calculus;programming paradigm;prototype;sparql;sql;seamless3d;semantics (computer science);unified framework;xpath;xquery	Darko Anicic;Nenad Stojanovic	2008			boiler (power generation);injector;knowledge management;flow (psychology);electrical conduit;computer science;inlet;reversing;flue gas;logical schema	DB	-29.1007797182357	13.507811128287065	20186
fdd972e44de8a763594c0c14d698565f9948f11d	difficult configurations - on the complexity of ltrl	automaton complexity;linear time temporal logic;machine turing;linear order;logica temporal;complexite automate;temporal logic;satisfiabilite;turing machine;satisfiability;informatique theorique;complejidad automata;logique temporelle;reachability analysis;maquina turing;analyse atteignabilite;computer theory;informatica teorica	The complexity of LTrL, a global linear time temporal logic over traces is investigated. The logic is global because the truth of a formula is evaluated in a global state, also called configuration. The logic is shown to be non-elementary with the main reason for this complexity being the nesting of until operators in formulas. The fragment of the logic without the until operator is shown to be EXPSPACE-hard.	elementary;expspace;linear temporal logic;tracing (software)	Igor Walukiewicz	1998		10.1007/BFb0055048	zeroth-order logic;linear temporal logic;temporal logic;interval temporal logic;computation tree logic;computer science;turing machine;artificial intelligence;bunched logic;mathematics;total order;algorithm;descriptive complexity theory;satisfiability	Logic	-8.909598064551068	19.97286408022675	20191
8f8347bfa29e4082dde2bff75ce5df9c791a2738	a fast and efficient method for processing web documents	web documents;theoretical framework;linear time algorithm;model checking;property a;document processing;description logic	  This paper investigates the possibility of realizing some Web document processing tasks in the context of modal, especially  description logics, providing a precise theoretical framework with well-analyzable computational properties. A fragment of  SHIQ description logic which can primarily be used in document processing is introduced. The paper also presents a linear  time algorithm for model checking Web documents proving that the logical approach can compete even in efficiency with other  industrial solutions.    		Dániel Szegö	2004		10.1007/978-3-540-24685-5_88	model checking;web modeling;description logic;document processing;computer science;artificial intelligence;theoretical computer science;database;information retrieval	ML	-20.987413928782413	8.876354225300947	20225
0a505c9ea876acfd0fed63a1dded63c4dc0444e3	nominals for everyone	data description;semantics;satisfiability;upper bound;conference paper;hybrid logic;artificial intelligence;number restrictions;description logic;modal operators;keywords coalgebraic;probabilistic logics;relational structures	It has been recognised that the expressivity of description logics benefits from the introduction of non-standard modal operators beyond existential and number restrictions. Such operators support notions such as uncertainty, defaults, agency, obligation, or evidence, whose semantics often lies outside the realm of relational structures. Coalgebraic hybrid logic serves as a unified setting for logics that combine non-standard modal operators and nominals, which allow reasoning about individuals. In this framework, we prove a generic EXPTIME upper bound for concept satisfiability over general TBoxes, which instantiates to novel upper bounds for many individual logics including probabilistic logic with nominals.	abox;blocking (computing);description logic;exptime;hybrid logic;kripke semantics;method of analytic tableaux;modal logic;tbox	Lutz Schröder;Dirk Pattinson;Clemens Kupke	2003			t-norm fuzzy logics;normal modal logic;discrete mathematics;description logic;computer science;artificial intelligence;mathematics;semantics;linguistics;upper and lower bounds;accessibility relation;algorithm;modal operator;satisfiability	AI	-15.046177366466688	8.954488298303245	20226
2670622b91ee1bda855f9ab15247cb1189f83cce	system description: tramp: transformation of machine-found proofs into nd-proofs at the assertion level	automatic proving;deduction automatique;program transformation;demostracion automatica;logical programming;transformation programme;theorem proving;demonstration automatique;demonstration theoreme;transformacion programa;programmation logique;demostracion teorema;programacion logica;automatic deduction	The TRAMP system transforms the output of several automated theorem provers for first order logic with equality into natural deduction proofs at the assertion level. Through this interface, other systems such as proof presentation systems or interactive deduction systems can access proofs originally produced by any system interfaced by TRAMP only by adapting the assertion level proofs to their own needs.	assertion (software development);norsk data	Andreas Meier	2000		10.1007/10721959_37	discrete mathematics;computer science;artificial intelligence;mathematics;automated theorem proving;programming language;algorithm	AI	-18.932598392980626	20.433744051003416	20245
4f923b3da01961a9b715c472f36b4b01a9f7db56	environment abstraction with state clustering and parameter truncating	state clustering;control systems;protocols;pattern clustering;parameter truncating;state space methods;realistic system;realistic system environment abstraction state clustering parameter truncating predicate abstraction counter abstraction parameterized system verification syntactic analysis real life cache coherence protocol;radiation detectors;abstract data types;protocols state space methods counting circuits control systems software engineering computer science coherence concrete humans abstracts;data mining;software engineering;arrays;environment abstraction;counting circuits;computational modeling;cache coherence protocol;reachability analysis abstract data types pattern clustering;abstracts;syntactic analysis;counter abstraction;number of clusters;aerospace electronics;process control;coherence;humans;computer science;predicate abstraction;parameterized system verification;localized state;real life cache coherence protocol;reachability analysis;concrete	Environment abstraction enriches predicate abstraction by idea from counter abstraction to develop a framework for verification of parameterized systems. However, despite various effects, the constructed abstractions still go beyond the capability of the usual model checkers for many realistic systems. In this paper, a new technique, called state clustering, is proposed to group local states into a small number of clusters, by purely syntactic analysis. The size of array variables in the resulting abstractions are further reduced using parameter abstraction technique. By combining different abstraction techniques, real-life cache coherence protocols such as FLASH have been successfully verified.	adobe flash;cache coherence;cluster analysis;heuristic (computer science);local variable;model checking;parsing;predicate abstraction;reachability;real life;truncation	Hong Pan;Yi Lv;Huimin Lin	2009	2009 Third IEEE International Symposium on Theoretical Aspects of Software Engineering	10.1109/TASE.2009.16	communications protocol;real-time computing;coherence;concrete;abstraction inversion;computer science;theoretical computer science;parsing;process control;distributed computing;programming language;particle detector;computational model;abstract data type;abstraction model checking	Logic	-20.19044937324107	30.531745663958766	20372
d31f6ce9fde66ea1d7f0e85bdb2ecbf1c884e6ae	ifo: a formal semantic database model	tratamiento datos;modelizacion;base donnee;semantic data base;mise a jour;database;object database;base dato;data processing;conception;traitement donnee;formal semantics;base donnee semantique;ifo;modelisation;semantic data model;diseno;design;model updating;puesta al dia;information system;modeling;base dato semantica;systeme information;updating;sistema informacion	A new, formally defined database model is introduced that combines fundamental principles of “semantic” database modeling in a coherent fashion. Using a graph-based formalism, the IFO model provides mechanisms for representing structured objects, and functional and ISA relationships between them. A number of fundamental results concerning semantic data modeling are obtained in the context of the IFO model. Notably, the types of object structure that can arise as a result of multiple uses of ISA relationships and object construction are described. Also, a natural, formal definition of update propagation is given, and it is shown that (under certain conditions) a correct update always exists.	database model	Serge Abiteboul;Richard Hull	1987	ACM Trans. Database Syst.	10.1145/32204.32205	semantic data model;design;data processing;computer science;formal semantics;data mining;database;information system;algorithm	DB	-30.246865787110064	12.996111949254558	20381
ee0bf2622ee87f645118802e0905c65873c24b64	some aspects and examples of infinity notions	finiteness definitions;axiom of choice;set theory;research paper	Our main contribution is a formal deenition of what could be called a T-notion of innnity, for set theories T extending ZF. Around this deenition we organize some old and new notions of innnity; we also indicate some easy independence proofs.	zermelo–fraenkel set theory	J. W. Degen	1994	Math. Log. Q.	10.1002/malq.19940400116	urelement;mathematical analysis;discrete mathematics;topology;pure mathematics;mathematics;actual infinity;axiom of choice;set theory;algebra	Logic	-7.4442984826538305	12.671202786718323	20384
0dd6f9a9bbc7471741b9b7f10bb01124724198dc	implementing modal and relevance logics in a logical framework	logical framework;natural deduction	We present a framework for machine implementation of both partial and complete fragments of large families of non-classical logics such as modal, relevance, and intuitionistic logics. We decompose a logic into two interacting parts, each a natural deduction system: a base logic of labelled formulae, and a theory of labels characterizing the properties of the Kripke models. Our approach is modular and supports uniform proofs of cor-rectness and proof normalization. We have implemented our work in the Isabelle Logical Framework.	ercim cor baayen award;formal system;interaction;intuitionistic logic;isabelle;kripke semantics;logical framework;modal logic;natural deduction;relevance;theory	David A. Basin;Seán Matthews;Luca Viganò	1996			t-norm fuzzy logics;normal modal logic;non-classical logic;logical framework;accessibility relation	Logic	-13.227751705952329	12.780252655892488	20393
2525217bebd57ad45bd72563474e787eb40d2b5c	computational complexity of a hybridized horn fragment of halpern-shoham logic		We propose hybridization of sub-propositional fragments of Halpern-Shoham logic as a way of obtaining expressive and decidable referential interval temporal logics. In the paper, we hybridize a Horn fragment of Halpern-Shoham logic whose language is restricted in its modal part to necessity modalities, and prove that satisfiability problem in this fragment is $$\\textsc {NP}$$-complete over reflexive or an irreflexive and dense underlying structure of time.	computational complexity theory;fuzzy logic	Przemyslaw Andrzej Walega	2017		10.1007/978-3-662-54069-5_17	artificial intelligence;theoretical computer science;mathematics;algorithm	AI	-13.070810526424951	13.34965443107065	20503
192d4561dac548d73991790be63c249f41210ea1	formal methods: an international perspective	formal specification;program verification;software engineering;specification languages;correctness proofs;formal description languages;formal methods;formal specification;mathematical techniques;mathematical verification;proofs of correctness;rigorous development methods;software development;stepwise refinement	Formal Methods has become a recognized body of knowledge over the past few years. Conference names, company and project titles, special publications, and government research programs all legitimize and define the area. Expressed as simply as possible, the goal of Formal Methods is to base the software development process squarely upon a workable set of mathematical techniques. The common names associated with various sub-classes of Formal Methods express both the purpose and mode of the technique: formal specification, mathematical verification, proofs of correctness, formal description languages, rigorous development methods, stepwise refinement, etc. There are many named methods --VDM, Z, Larch, traces, functional verification, etc. -and many associated with tools -GYPSY, HOL, Boyer-Moore, B, etc.	correctness (computer science);formal methods;formal specification;formal verification;hol (proof assistant);larch family;refinement (computing);software development process;stepwise regression;top-down and bottom-up design;tracing (software)	Susan L. Gerhart	1991			formal system;formal methods;computer science;theoretical computer science;software engineering;formal semantics;formal specification;formal epistemology;refinement;programming language	DB	-21.223321299630992	18.476993689928772	20579
cbc54dae9a5a1ef97340e01956f217bbe15114da	adaptive top-k algorithm in slca-based xml keyword search	top k;two layer based index construction;query processing;slca based xml keyword search;adaptive top k algorithm;two layer based index construction adaptive top k algorithm slca based xml keyword search;xml keyword search computer science education computer science query processing joining processes indexing;computer science education;xml indexing query processing;keyword search;indexing;indexation;xml;joining processes;xml keyword search;xml keyword search top k;computer science	Computing top-k results matching XML queries is gaining importance due to the increasing of large XML repositories. In this paper, we propose a novel two-layer-based index construction and associated algorithms for efficiently computing top-k results for SLCA-based XML keyword search. We have conducted expensive experiments and the results show great advantage on efficiency compared with existing approaches.	data model;experiment;random access;search algorithm;semiconductor industry;vii;xml	Hang Yu;Zhi-Hong Deng;Yong-Qing Xiang;Ning Gao;Ming Zhang;Shiwei Tang	2010	2010 12th International Asia-Pacific Web Conference	10.1109/APWeb.2010.37	binary xml;search engine indexing;simple api for xml;xml;computer science;database;world wide web;information retrieval;efficient xml interchange	DB	-31.63559086041988	4.412636726420522	20584
0127c7d0f9a715977097cf73336e5cd6a3c3ebc6	lolliproc: to concurrency from classical linear logic via curry-howard and control	logique lineaire;langage fonctionnel;lenguaje programacion;distributed system;session management;session types;theorie type;systeme reparti;linearity;race condition;programming language;confluencia;linearite;gestion seson;lenguaje funcional;lambda calculus;simultaneidad informatica;logical programming;confluence;type systems;logica lineal;functional programming;linear functional;linearidad;teoria de tipos;process calculus;concurrency;sistema repartido;algebra proceso;programmation logique;intuitionistic logic;theory;type theory;strong normalization;logique intuitionniste;algebre processus;langage programmation;concurrent programs;deadlock;interbloqueo;lambda calculo;design;programmation fonctionnelle;funcional lineal;interblocage;process algebra;functional language;lambda calcul;programacion logica;programacion funcional;linear logic;simultaneite informatique;gestion session;languages;logica intuicionista;fonctionnelle lineaire;type system	"""While many type systems based on the intuitionistic fragment of linear logic have been proposed, applications in programming languages of the full power of linear logic - including double-negation elimination - have remained elusive. Meanwhile, linearity has been used in many type systems for concurrent programs - e.g., session types - which suggests applicability to the problems of concurrent programming, but the ways in which linearity has interacted with concurrency primitives in lambda calculi have remained somewhat ad-hoc. In this paper we connect classical linear logic and concurrent functional programming in the language Lolliproc, which provides simple primitives for concurrency that have a direct logical interpretation and that combine to provide the functionality of session types. Lolliproc features a simple process calculus """"under the hood"""" but hides the machinery of processes from programmers. We illustrate Lolliproc by example and prove soundness, strong normalization, and confluence results, which, among other things, guarantees freedom from deadlocks and race conditions."""	concurrency (computer science);concurrent computing;confluence;curry;curry–howard correspondence;deadlock;functional programming;hood method;hoc (programming language);interpretation (logic);lambda calculus;linear logic;normalization property (abstract rewriting);process calculus;programmer;programming language;race condition;type system	Karl Mazurak;Steve Zdancewic	2010		10.1145/1863543.1863551	concurrent constraint logic programming;process calculus;concurrency;computer science;programming language;functional programming;logic programming;algorithm;concurrent object-oriented programming	PL	-20.627514649818316	22.683952101875814	20598
8e00d5b2d60025e5704e76531738b314f6beefa1	code generation using a formal model of reference counting		Reference counting is a popular technique for memory management. It tracks the number of active references to a data object during the execution of a program. Reference counting allows the memory used by a data object to be freed when there are no active references to it. We develop the metatheory of reference counting by presenting an abstract model for a functional language with arrays. The model is captured by an intermediate language and its operational semantics, defined both with and without reference counting. These two semantics are shown to correspond by means of a bisimulation. The reference counting implementation allows singly referenced data objects to be updated in place, i.e., without copying. The main motivation for our model of reference counting is in soundly translating programs from a high-level functional language, in our case, an executable fragment of the PVS specification language, to efficient code with a compact footprint in a small subset of a low-level imperative language like C.	reference counting	Gaspard Férey;Natarajan Shankar	2016		10.1007/978-3-319-40648-0_12	reference counting;computer science;theoretical computer science;mathematics;weak reference;programming language;algorithm	NLP	-21.53423141001701	28.7285674937305	20604
5d1a40fa20ebfc6fe76425a73f925c88d323f04f	complexity of metric temporal logics with counting and the pnueli modalities	unit interval;satisfiability problem;temporal logic;last unit;common metric temporal logic;nthe modality;pnueli modalities;pnueli modality;until-since temporal logic;next unit;metric temporal logics;continuous time	unit interval;satisfiability problem;temporal logic;last unit;common metric temporal logic;nthe modality;pnueli modalities;pnueli modality;until-since temporal logic;next unit;metric temporal logics;continuous time		Alexander Moshe Rabinovich	2008		10.1007/978-3-540-85778-5_8	discrete mathematics;mathematics;algorithm	Theory	-12.637991964574832	14.14207646097504	20612
934ddc0f1e8b3c188de3c07f51936acd3ae46878	a first-order interpreter for knowledge-based golog with sensing based on exact progression and limited reasoning	cognitive robotics;knowledge representation;action change and causality	While founded on the situation calculus, current implementations of Golog are mainly based on the closedworld assumption or its dynamic versions or the domain closure assumption. Also, they are almost exclusively based on regression. In this paper, we propose a first-order interpreter for knowledge-based Golog with sensing based on exact progression and limited reasoning. We assume infinitely many unique names and handle first-order disjunctive information in the form of the so-called proper KBs. Our implementation is based on the progression and limited reasoning algorithms for proper KBs proposed by Liu, Lakemeyer and Levesque. To improve efficiency, we implement the two algorithms by grounding via a trick based on the unique name assumption. The interpreter is online but the programmer can use two operators to specify offline execution for parts of programs. The search operator returns a conditional plan, while the planning operator is used when local closed-world information is available and calls a modern planner to generate a sequence of actions.	algorithm;closed-world assumption;cognitive work analysis;color gradient;disjunctive normal form;first-order predicate;hector levesque;online and offline;programmer;situation calculus;software propagation;unique name assumption;unit propagation	Yi Fan;Minghui Cai;Naiqi Li;Yongmei Liu	2012			knowledge representation and reasoning;computer science;artificial intelligence;theoretical computer science;machine learning;algorithm;statistics;cognitive robotics	AI	-18.54148793287477	12.94307158663423	20627
10d5cabbf44b6f70b92d93511d524f826289591f	abstract solvers for dung's argumentation frameworks		solvers for Dung’s argumentation frameworks Remi Brochenin a, Thomas Linsbichler b,∗, Marco Maratea a, Johannes P. Wallner b and Stefan Woltran b a Dipartimento di Informatica, Bioingegneria, Robotica e Ingegneria dei Sistemi, Università di Genova, Italy E-mails: r.brochenin@tue.nl, marco@dibris.unige.it b Institute of Information Systems, TU Wien, Austria E-mails: linsbich@dbai.tuwien.ac.at, wallner@dbai.tuwien.ac.at, woltran@dbai.tuwien.ac.at Abstract. Abstract solvers are a quite recent method to uniformly describe algorithms in a rigorous formal way via graphs. Compared to traditional methods like pseudo-code descriptions, abstract solvers have several advantages. In particular, they provide a uniform formal representation that allows for precise comparisons of different algorithms. Recently, this new methodology has proven successful in declarative paradigms such as Propositional Satisfiability and Answer Set Programming. In this paper, we apply this machinery to Dung’s abstract argumentation frameworks. We first provide descriptions of several advanced algorithms for the preferred semantics in terms of abstract solvers. We also show how it is possible to obtain new abstract solutions by “combining” concepts of existing algorithms by means of combining abstract solvers. Then, we implemented a new solving procedure based on our findings in CEGARTIX, and call it CEGARTIX+. We finally show that CEGARTIX+ is competitive and complementary in its performance to CEGARTIX on benchmarks of the first and second argumentation competition. Abstract solvers are a quite recent method to uniformly describe algorithms in a rigorous formal way via graphs. Compared to traditional methods like pseudo-code descriptions, abstract solvers have several advantages. In particular, they provide a uniform formal representation that allows for precise comparisons of different algorithms. Recently, this new methodology has proven successful in declarative paradigms such as Propositional Satisfiability and Answer Set Programming. In this paper, we apply this machinery to Dung’s abstract argumentation frameworks. We first provide descriptions of several advanced algorithms for the preferred semantics in terms of abstract solvers. We also show how it is possible to obtain new abstract solutions by “combining” concepts of existing algorithms by means of combining abstract solvers. Then, we implemented a new solving procedure based on our findings in CEGARTIX, and call it CEGARTIX+. We finally show that CEGARTIX+ is competitive and complementary in its performance to CEGARTIX on benchmarks of the first and second argumentation competition.	argumentation framework	Rémi Brochenin;Thomas Linsbichler;Marco Maratea;Johannes Peter Wallner;Stefan Woltran	2018	Argument & Computation	10.3233/AAC-170031		AI	-21.73711937299092	15.479011214816994	20636
3fd3394a28d6e75fd77f89ed2423dd29dd2a6c11	systematic construction of natural deduction systems for many-valued logics	sequent calculi;abstract algebra;many valued logics;normal form theorems;soundness;construction principle;inference mechanisms;natural deduction systems;truth tables;natural deduction;normal form;normal form theorems natural deduction systems many valued logics construction principle sequent calculi truth tables cut free completeness soundness;many valued logics inference mechanisms;multivalued logic;cut free completeness;multivalued logic virtual manufacturing abstract algebra;virtual manufacturing;many valued logic	A construction principle for natural deduction systems for arbitrary finitely-many-valued first order logics is exhibited. These systems are systematically obtained from sequent calculi, which in turn can be automatically extracted from the truth tables of the logics under consideration. Soundness and cut-free completeness of these sequent calculi translate into soundness, completeness and normal form theorems for the natural deduction systems.	church–rosser theorem;lecture notes in computer science;method of analytic tableaux;natural deduction;normalization property (abstract rewriting);sequent calculus;soundness (interactive proof);springer (tank)	Matthias Baaz;Christian G. Fermüller;Richard Zach	1993		10.1109/ISMVL.1993.289558	soundness;discrete mathematics;abstract algebra;many-valued logic;computer science;truth table;mathematics;sequent;sequent calculus;natural deduction;algorithm;algebra	Logic	-13.458718642255445	13.56529526758038	20641
4bb9dd5ed2e6b973cba14bb5920cf7f87d75f47b	model checking analysis of observational transition system with smv	cafeobj;model checking;model checking tool;model-based verification;smv	Model checking is an important way in developing high confidence systems.A model checking method based on theorem proving is proposed,which describes the infinite system state by CafeOBJ and transfers a CafeOBJ specification to a finite SMV speci-fication,and its tool is implemented.By observing transition system shows that a counter example of a corresponding SMV specification is also a counter example of a CafeOBJ specification.It find errors lurked in systems at earlier stages of their development processes to avoid spending much money and time to correct errors later.		Tao He;Huazhong Li;Guorong Qin	2011		10.1007/978-3-642-27452-7_73	real-time computing;computer science;theoretical computer science;algorithm	Logic	-15.23220606142377	27.29987450794006	20657
2551f2b05ccda0fff207d273af5e944adccb311e	two-layer modal logics: from fuzzy logics to a general framework		Mathematical Fuzzy Logic (MFL) started as the study of logics based on left-continuous t-norms,1 most prominently Łukasiewicz logic Ł, Gödel–Dummett logic G, Product logic Π, Hájek logic BL, and the system MTL introduced by Esteva and Godo. In the last years, the scope of MFL has been progressively expanded by considering weaker logical systems characterized by their completeness with respect to a semantics of linearly algebras (such as chains endowed with a uninorm, or a non-commutative tnorm to interpret &, or other kinds of linearly ordered residuated lattices). There have been some proposals for a general framework to deal in a uniform way with this growing family of logics. The first one appeared in [12] when Hájek and Cintula introduced that classes of core and 4-core fuzzy logics, understood as axiomatic expansions of MTL and MTL4, resp. A wider framework, encompassing weaker systems and stronger expansions, has been proposed in [3] based on the notion of weakly implicative semilinear logic. In both approaches MFL retains what we can arguably see as its defining feature, namely the study of systems of non-classical logics with a semantics based on (linearly ordered) scales of degrees of truth. This is what makes these logics specially suited for the study of gradual aspects of vagueness and imprecision, found in sentences like ‘it is heavily raining’ or ‘that man is tall’. A conceptually different issue, that of uncertainty, has also been addressed inside MFL. The main idea, introduced in [13] and later developed by Hájek in his monograph [11], is that one could use probability to determine the truth degree of statements such as ‘tomorrow it will probably rain’ or ‘the probability that tomorrow it will rain is high’. Indeed, one takes classical logic and its formulae φ to describe crisp events, introduces a new modal operator P which can be applied on them to create atomic modal formulae Pφ which may be read as ‘probably φ’ (or better ‘the probability of φ is high’), and finally these atomic modal formulae are combined by using the connectives of Łukasiewicz logic. What we obtain is a two-layer modal fuzzy logic built on atomic formulae Pφ whose truth values are given by a probability measure. Several works have followed this idea with variations. In [9] Godo, Esteva and Hájek replaced Łukasiewicz logic on the second layer by ŁΠ, but kept classical logic for non-modal formulae. The logic ŁΠ, with its expanded language, enabled them to deal with conditional probability. Flaminio and Montagna also considered conditional probability in [7], and Godo and Marchioni investigated coherent conditional probabilities in [10]. Marchioni also proposed a class of logics of uncertainty in [14] with different kinds of measures (besides probability) to quantify the uncertainty of events. In all of these works classical logic has been kept as the underlying logic for non-modal formulae. However, if one wants to deal with uncertainty and vagueness at once, i.e. with the probability of vague events, as in ‘tomorrow it will probably rain heavily’, the two-layer paradigm can still be useful provided that the underlying classical logic is substituted by a fuzzy logic. This idea has been also investigated in some works, as [5] where finite Łukasiewicz systems Łn are taken as the logics of vague events. Other recent works along these lines are surveyed in [6]. There has even been a first attempt at an abstract theory of two-layer modal fuzzy logics in [15]; but it is restricted to the family of core fuzzy logics which, as argued above, has become too narrow to contain the current scope of MFL.	atomic formula;bl (logic);coherence (physics);commutation theorem;degree of truth;fuzzy logic;gödel;logical connective;message format language;modal logic;modal operator;programming paradigm;residuated lattice;t-norm;vagueness;łukasiewicz logic	Petr Cintula;Carles Noguera	2013			artificial intelligence;kripke semantics;natural language processing;computer science;fuzzy logic;modal;normal modal logic;accessibility relation	Logic	-11.702402162681357	8.74867808688044	20660
a8e84060829e4f15d203dd8510b6a563e836adcd	autoepistemic logic of knowledge and beliefs	logics of knowledge and beliefs;semantics of logic programs and deductive databases;nonmonotonic reasoning;non monotonic reasoning;logic programs;knowledge representation;disjunctive logic programming;deductive databases	In recent years, various fotmalizations of nonmonotonic reasoning and different semantics for normal and disjunctive logic programs have been proposed, including autoepistemic logic, circumscription, CWA, GCWA, ECWA, epistemic specifications, stable, well-founded, stationary and static semantics of normal and disjunctive logic programs. In this paper we introduce a simple nonmonotonic knowledge representation framework which isomorphically contains all of the above-mentioned nonmonotonic formalisms and semantics as special cases and yet is significantly more expressive than each one of these formalisms considered individually. The new formalism, called the Autoepistemic Logic of Knowledge and Beliefs, AELB, is obtained by augmenting Moore’s autoepistemic logic, AEL, already employing the knowledge operator, C, with an additional belief operator, t?. As a result, we are able to reason not only about formulae F which are known to be true (i.e., those for which CF holds) but also about those which are only believed to be true (i.e., those for which !3F holds). The proposed logic constitutes a powerful new formalism which can serve as a unifyingfrumework for several major nonmonotonic formalisms. It allows us to better understand mutual relationships existing between different formalisms and semantics and enables us to provide them with simpler and more natural definitions. It also naturally leads to new, even more expressive, flexible and modular formalizations and semantics. @ 1997 Published by Elsevier Science B.V.	abductive reasoning;application domain;autoepistemic logic;circumscription (logic);cognitive work analysis;disjunctive normal form;formal system;knowledge representation and reasoning;logic programming;negation as failure;non-monotonic logic;programming language;prototype;semantics (computer science);stationary process;well-founded semantics	Teodor C. Przymusinski	1997	Artif. Intell.	10.1016/S0004-3702(97)00032-5	dynamic logic;knowledge representation and reasoning;discrete mathematics;description logic;higher-order logic;stable model semantics;computer science;artificial intelligence;theoretical computer science;non-monotonic logic;computational logic;mathematics;well-founded semantics;probabilistic logic network;logic;multimodal logic;algorithm;philosophy of logic;autoepistemic logic	AI	-15.719337383928004	7.701452803148301	20669
a95b9508ec8836e4d8170c7fe0e5e65b78c48e35	a proof theory for model checking: an extended abstract		While model checking has often been considered as a practica l alternative to building formal proofs, we argue here that the theory of sequent calculus proofs can b e used to provide an appealing foundation for model checking. Since the emphasis of model check ing is on establishing the truth of a property in a model, we rely on the proof theoretic notion of additive inference rules, since such rules allow provability to directly describe truth conditi ons. Unfortunately, the additive treatment of quantifiers requires inference rules to have infinite sets of premises and the additive treatment of model descriptions provides no natural notion of state expl oration. By employing a focused proof system, it is possible to construct large scale, synthetic r ules that also qualify as additive but contain elements of multiplicative inference. These additive synt hetic rules—essentially rules built from the description of a model—allow a direct treatment of state exp loration. This proof theoretic framework provides a natural treatment of reachability and non-r eachability problems, as well as tabled deduction, bisimulation, and winning strategies.	bisimulation;coinduction;exptime;first-order predicate;fixed point (mathematics);intensional logic;language binding;linear logic;model checking;natural deduction;petri net;predicate abstraction;proof calculus;reachability;rewriting;sequent calculus;synthetic intelligence;theory;uml state machine;unfolding (dsp implementation);utility functions on indivisible goods	Quentin Heath;Dale Miller	2016		10.4204/EPTCS.238.1	model checking;discrete mathematics;automated proof checking;proof theory;structural proof theory;abstraction model checking	Logic	-12.085674785907663	14.977344320056975	20671
1d7c0639faceae1270b787791977b6bff9bf9882	towards an efficient tableau method for boolean circuit satisfiability checking	boolean circuits;fonction booleenne;diagrama binaria decision;diagramme binaire decision;search space;deduction automatique;satisfiabilite;boolean function;logique propositionnelle;logical programming;satisfiability;program optimization;formal verification;programmation logique;propositional logic;funcion booliana;verification formelle;optimisation programme;logica proposicional;symbolic model checking;programacion logica;automatic deduction;satis ability;optimizacion programa;binary decision diagram	Boolean circuits offer a natural, structured, and compact representation of Boolean functions for many application domains. In this paper a tableau method for solving satisfiability problems for Boolean circuits is devised. The method employs a direct cut rule combined with deterministic deduction rules. Simplification rules for circuits and a search heuristic attempting to minimize the search space are developed. Experiments in symbolic model checking domain indicate that the method is competitive against state-of-the-art satisfiability checking techniques and a promising basis for further work.	academy;application domain;backtracking;boolean circuit;boolean satisfiability problem;circuit satisfiability problem;computer engineering;conjunctive normal form;cut rule;experiment;heuristic (computer science);keneth alden simons;level of detail;long division;method of analytic tableaux;model checking;natural deduction;parsing;prototype;software propagation;text simplification	Tommi A. Junttila;Ilkka Niemelä	2000		10.1007/3-540-44957-4_37	boolean circuit;and-inverter graph;combinatorics;circuit minimization for boolean functions;discrete mathematics;boolean domain;boolean expression;formal verification;product term;standard boolean model;computer science;maximum satisfiability problem;program optimization;mathematics;propositional calculus;boolean function;boolean satisfiability problem;programming language;binary decision diagram;algorithm;satisfiability	AI	-16.913038876740643	16.117831626338692	20800
00ffe406c3d7331163ce071fd7e513cb77db32e8	certifying compilation and run-time code generation	program assemblers;compilateur;executable code;memory safety;certifying compilation;generation code;generacion codigo;code generation;run time code generation;typed assembly language;satisfiability;compiler;evaluation partielle;program specialization;code executable;partial evaluation;proof carrying code;source language;assembleur programme;specialisation programme;type safety;compilador;certification compilation	"""A certifying compiler takes a source language program and produces object code, as well as a \certi cate"""" that can be used to verify that the object code satis es desirable properties, such as type safety and memory safety. Certifying compilation helps to increase both compiler robustness and program safety. Compiler robustness is improved since some compiler errors can be caught by checking the object code against the certi cate immediately after compilation. Program safety is improved because the object code and certi cate alone are su cient to establish safety: even if the object code and certi cate are produced on an unknown machine by an unknown compiler and sent over an untrusted network, safe execution is guaranteed as long as the code and certi cate pass the veri er. Existing work in certifying compilation has addressed statically generated code. In this paper, we extend this to code generated at run time. Our goal is to combine certifying compilation with run-time code generation to produce programs that are both veri ably safe and extremely fast. To achieve this goal, we present two new languages with explicit run-time code generation constructs: Cyclone, a type safe dialect of C, and TAL/T, a type safe assembly language. We have designed and implemented a system that translates a safe C program into Cyclone, which is then compiled to TAL/T, and nally assembled into executable object code. This paper focuses on our overall approach and the front end of our system; details about TAL/T will appear in a subsequent paper."""	code generation (compiler);compiler;cyclone;executable;memory safety;object code;run time (program lifecycle phase);self-modifying code;type safety;typed assembly language	Luke Hornof;Trevor Jim	1999	Higher-Order and Symbolic Computation	10.1023/A:1010039502678	code word;dead code;compile time;memory safety;compiler;code bloat;parallel computing;real-time computing;dynamic compilation;loop-invariant code motion;object code;type safety;computer science;dead code elimination;just-in-time compilation;redundant code;compilation error;programming language;executable;partial evaluation;code generation;unreachable code;satisfiability;source code	PL	-22.0746718658152	29.00687351082609	20912
cffb6c2358d976a1b0e8016f4a2d7ebf1da825f5	tractable deduction in knowledge representation systems.	knowledge representation	A widely-used way to deal with the in-tractability of various deduction problems is based on identifying their tractable cases. Current techniques for this are highly problem speciic. We present a new technique that obtains powerful tractability results for many diierent problems. Our technique is based on a notion of partial consistency for logical theories. Any set of theories for which a xed level of partial consistency can guarantee logical consistency provides a tractable class of deduction problems. We apply this technique to obtain tractability results in the areas of constraint satisfaction problems, databases with incomplete information and disjunctive logic programming.	algorithm;automated planning and scheduling;cobham's thesis;constraint satisfaction problem;database;default logic;disjunctive normal form;knowledge representation and reasoning;lamina emergent mechanism;logic programming;naruto shippuden: clash of ninja revolution 3;natural deduction;subsumption architecture;theory	Mukesh Dalal	1992			natural language processing;knowledge representation and reasoning;computer science;artificial intelligence	AI	-19.625956185927524	9.531598734839113	20945
820d54093cf0d89f7333fbc9ec3535bde22ca4da	call-graph caching: transforming programs into networks	analogical reasoning;computer program;call graph;program transformation;automatic generation;partial evaluation;network algorithm;incremental algorithm;consistency maintenance;program specification	There are computer programs that use the same flow of control when run on different inputs. This redundancy in their program execution traces can be exploited by preserving suitably abstracted call-graphs for subsequent reuse. We introduce a new programming transformation Call-Graph Caching (CGC) which partially evaluates the control flow of sets of such programs into a network formed from their call-graphs. CGC can automatically generate efficient state-saving structure-sharing incremental algorithms from simple program specifications. As an example, we show how a straightforward, inefficient LISP program for conjunctive match is automatically transformed into the RETE network algorithm. Simple and understandable changes to elegant functional (and other) programs are automatically translated by CGC into new efficient incremental network algorithms; this abstraction mechanism is shown for a class of conjunctive matching algorithms. We establish criteria for the appropriate application of CGC to other AI methods, such as planning, chart parsing, consistency maintenance, and analogical reasoning.	a new kind of science;automated planning and scheduling;cache (computing);call graph;chart parser;computer program;control flow;dynamic problem (algorithms);parsing;rete algorithm;tracing (software)	Mark W. Perlin	1989			call graph;computer science;artificial intelligence;machine learning;database;programming language;partial evaluation;algorithm	AI	-18.697027115875986	23.66911651290263	20965
ad61ff65786754ebe823c1746dd3ed167ad7639f	mlf: raising ml to the power of system f	type inference;type annotations;ml;system f;second-order polymorphism;first-class polymorphism	We propose a type system MLF that generalizes ML with first-class polymorphism as in System F. Expressions may contain second-order type annotations. Every typable expression admits a principal type, which however depends on type annotations. Principal types capture all other types that can be obtained by implicit type instantiation and they can be inferred.All expressions of ML are well-typed without any annotations. All expressions of System F can be mechanically encoded into MLF by dropping all type abstractions and type applications, and injecting types of lambda-abstractions into MLF types. Moreover, only parameters of lambda-abstractions that are used polymorphically need to remain annotated.	system f	Didier Le Botlan;Didier Rémy	2003	SIGPLAN Notices	10.1145/944746.944709		PL	-15.2334705067232	19.775432258839146	20985
c73eebc699c84f0dde734aea1c0f97d76c018315	research on location-dependent queries in mobile databases.	mobile database		database	Agustinus Borgy Waluyo;Bala Srinivasan;David Taniar	2005	Comput. Syst. Sci. Eng.		mobile search;mobile database;computer science;database	DB	-31.08324604335604	16.831424201428394	21014
ace946951e432f3dec3e43641c2e4e971e5aa15a	programming physics softwares in flash	programming language;physics education;object oriented programming languages;computer simulation	We discuss various aspects of programming physics education software in Adobe Flash. Since the authoring environment for Flash is initially developed for non-programmers, it is easy to learn even for those having no previous knowledge in programming language, although some previous experience in programming may help in mastering advanced features of ActionScript, an object-oriented programming language used for developing Flash programs. The most attractive feature of Flash is its strong graphic capability not available in other standard programming languages. © 2007 Elsevier B.V. All rights reserved.	actionscript;adobe flash;bus mastering;programmer;programming language	Koo-Chul Lee;Julian Lee	2007	Computer Physics Communications	10.1016/j.cpc.2007.02.074	computer simulation;fourth-generation programming language;first-generation programming language;declarative programming;very high-level programming language;programming domain;reactive programming;computer science;theoretical computer science;extensible programming;third-generation programming language;functional logic programming;computer programming;programming paradigm;procedural programming;symbolic programming;inductive programming;fifth-generation programming language;programming language theory;programming language;object-oriented programming;second-generation programming language;high-level programming language;comparison of multi-paradigm programming languages;physics education	AI	-27.713639844443456	24.11624923450804	21028
258a6935d8a0a7ecdc562957faf715b7078510eb	universal computer-oriented language	translation of computer languages;design of computer languages	The basic idea of UNCOL Universal Computer-Oriented Language—is to introduce a language between problem-oriented languages, POLs, and machine languages, MLs. This third level consists of a single language, UNCOL, which has the character of a generalized machine-line language.	programming language;turing machine;uncol	W. B. Dobrusky;Thomas B. Steel	1961	Commun. ACM	10.1145/366199.366220	natural language processing;fourth-generation programming language;universal networking language;language primitive;computer science;domain-specific language;third-generation programming language;context-free language;cone;ontology language;abstract family of languages;low-level programming language;natural language;fifth-generation programming language;programming language;second-generation programming language;high-level programming language	PL	-24.76911863163723	22.357009267653687	21058
6a1ebd289d8a3312870e02dadeb23d76c44529bd	an algorithm for structuring flowgraphs	program transformation;fortran	This paper describes an algorithm which transforms a flowgraph into a program containing control constructs such as if then else statements, repeat (do forever) statements, multilevel break statements (causing jumps out of enclosing repeats), and multilevel next statements (causing jumps to iterations of enclosing repeats). The algorithm can be extended to create other types of control constructs, such as while or until. The program appears natural because the constructs are used according to common programming practices. The algorithm does not copy code, create subroutines, or add new variables. Instead, goto statements are generated when no other available control construct describes the flow of control. The algorithm has been implemented in a program called STRUCT which rewrites Fortran programs using constructs such as while, repeat, and if then else statements. The resulting programs are substantially more readable than their Fortran counterparts.	algorithm;control flow;fortran;goto;human-readable medium;iteration;signal-flow graph;struct (c programming language);subroutine	Brenda S. Baker	1977	J. ACM	10.1145/321992.321999	computer science;theoretical computer science;mathematics;programming language;algorithm	PL	-15.349274304783076	31.83550564958347	21082
c5007d1b93e16ec5e1de0b91d4d632fe931fd759	relational data base design for the intensional aspects of a knowledge base	modelizacion;base relacional dato;entity relationship model;relational data;representacion conocimientos;logic design;modelo eutidad relacion;base connaissance;relational database;modele entite relation;modelisation;conception logique;base donnee relationnelle;base conocimiento;knowledge representation;representation connaissances;modeling;concepcion logica;knowledge base	The paper describes a methodology for designing relational data base schemata for intensional knowledge represented in terms of structured inheritance networks. The intensional data base design is, for two reasons, the first important step in producing a knowledge base design: firstly, the intensional data base itself is expected to be the subject of a large part of user information requests; secondly, it will be very large for non-trivial application domains. The design methodology includes two major steps: (a) identification of the knowledge partitions deriving from the specific knowledge representation formalism; and (b) design of the logical schema by taking into account both application-dependent and appication-independent information.	emoticon;formal system;intensional logic;knowledge base;knowledge representation and reasoning;logical data model;relational database management system	Sonia Bergamaschi;Flavio Bonfatti;Lorenza Cavazza;Claudio Sartori;Paolo Tiberio	1988	Inf. Syst.	10.1016/0306-4379(88)90037-3	knowledge base;relational database;computer science;artificial intelligence;knowledge-based systems;data mining;database;genus–differentia definition;algorithm	DB	-29.787909426329943	12.258159960734874	21098
6cba671dd200d79e89afa81a6550d83bfad89fdf	mutrex: a mutation-based generator of fault detecting strings for regular expressions	electronic mail;automata;syntactics;transforms;writing;context;fault diagnosis	Regular expressions (regexes) permit to describe set of strings using a pattern-based syntax. Writing a correct regex that exactly captures the desired set of strings is difficult, also because a regex is seldom syntactically incorrect, and so it is rare to detect faults at parse time. We propose a fault-based approach for generating tests for regexes. We identify fault classes representing possible mistakes a user can make when writing a regex, and we introduce the notion of distinguishing string, i.e., a string that is able to witness a fault. Then, we provide a tool, based on the automata representation of regexes, for generating distinguishing strings exposing the faults introduced in mutated versions of a regex under test. The basic generation process is improved by two techniques, namely monitoring and collecting. Experiments show that the approach produces compact test suites having a guaranteed fault detection capability, differently from other test generation approaches.	automata theory;automaton;computation;experiment;fault detection and isolation;heuristic (computer science);maximal set;parsing;regular expression;test suite;time complexity	Paolo Arcaini;Angelo Gargantini;Elvinia Riccobene	2017	2017 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)	10.1109/ICSTW.2017.23	computer science;theoretical computer science;distributed computing;automaton;programming language;writing;algorithm	SE	-18.15850349988614	25.63298819768855	21154
e48f19c5783827d7b289101c763ef3fe1deb15d1	symbolic execution and thresholding for efficiently tuning fuzzy logic programs		Fuzzy logic programming is a growing declarative paradigm aiming to integrate fuzzy logic into logic programming. One of the most di cult tasks when specifying a fuzzy logic program is determining the right weights for each rule, as well as the most appropriate fuzzy connectives and operators. In this paper, we introduce a symbolic extension of fuzzy logic programs in which some of these parameters can be left unknown, so that the user can easily see the impact of their possible values. Furthermore, given a number of test cases, the most appropriate values for these parameters can be automatically computed. Finally, we show some benchmarks that illustrate the usefulness of our approach.	benchmark (computing);declarative programming;fuzzy logic;logic programming;logical connective;programming paradigm;symbolic execution;test case;thresholding (image processing)	Ginés Moreno;Jaime Penabad;José A. Riaza;Germán Vidal	2016		10.1007/978-3-319-63139-4_8	fuzzy logic;theoretical computer science;operator (computer programming);fuzzy electronics;computer science;declarative programming;machine learning;artificial intelligence;logic programming;test case;thresholding;symbolic execution	AI	-17.824688641910782	20.48842732152211	21155
3c4072abae6d75cb76ed5a857b448c8af099a100	logische verifikation nebenläufiger programme	developpement logiciel;formal specification;logic;concurrent program;specification programme;program verification;specification formelle;verificacion programa;desarrollo logicial;software development;programa competidor;verification programme;program specification;especificacion programa;logique;logica;programme concurrent			Heiko Krumm	1987	Angewandte Informatik		computer science;software development;formal specification;programming language;logic	HCI	-23.88704180025166	31.0113625959676	21262
967ad3a6606eaecc85facfbbca1e3f91d073ff63	automated generation of diverse npc-controlling fsms using nondeterministic planning techniques		We study the problem of generating a set of Finite State Machines (FSMs) modeling the behavior of multiple, distinct NPCs. We observe that nondeterministic planning techniques can be used to generate FSMs by following conventions typically used when manually creating FSMs modeling NPC behavior. We implement our ideas in DivNDP, the first algorithm for automated diverse FSM generation.	np-completeness	Alexandra Coman;Hector Muñoz-Avila	2013			real-time computing;computer science;theoretical computer science;algorithm	Robotics	-13.604216775848885	28.148849232033914	21266
6a4ca3b6b1dd2a8ff05c8f179e0370b3d5944d7f	a grammar for hierarchical object descriptions in logic programs	grammar;clocks;semantics;gui components hierarchical object descriptions object modeling formal grammars computer vision probabilistic logic programming bilattice based logical reasoning blr object detection object recognition high level vision tasks nonhierarchical object grammars object class compositional descriptions compositional object hierarchies context sensitive specification grammar meta grammar object grammars automatic compiler graphical user interface components;inference mechanisms;context sensitive grammars;graphical user interfaces;probabilistic logic context sensitive grammars inference mechanisms logic programming;logic programming;stochastic processes;syntactics;feature extraction;grammar graphical user interfaces stochastic processes feature extraction syntactics semantics clocks;probabilistic logic	Modeling objects using formal grammars has recently regained much attention in computer vision. Probabilistic logic programming, such as Bilattice based Logical Reasoning (BLR), is shown to produce impressive results in object detection/recognition. Although hierarchical object descriptions are preferred in high-level vision tasks for several reasons, BLR has been applied to non-hierarchical object grammars (compositional descriptions of object class). To better align logic programs (esp. BLR) with compositional object hierarchies, we provide a formal grammar, which can guide domain experts to describe objects. That is, we introduce a context-sensitive specification grammar or a meta-grammar, the language of which is the set of all possible object grammars. We show the practicality of the approach by an automatic compiler that translates example object grammars into a BLR logic program and applied it for detecting Graphical User Interface (GUI) components.	align (company);compiler;computer vision;context-sensitive grammar;formal grammar;graphical user interface;high- and low-level;logic programming;object detection;sensor	Toufiq Parag;Claus Bahlmann;Vinay D. Shet;Maneesh Kumar Singh	2012	2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2012.6239171	grammar systems theory;natural language processing;tree-adjoining grammar;method;l-attributed grammar;object model;feature extraction;computer science;affix grammar;theoretical computer science;extended affix grammar;graphical user interface;grammar;semantics;probabilistic logic;context-free grammar;programming language;logic programming;stochastic context-free grammar	Vision	-26.812195568325613	21.463946048666752	21269
8680d19ccf4cfe2188010120809d9fce3b01df56	defeasible logic is stable	defeasible logic	We define, and give some of the intuition behind the definition of, a nonmonotonic logic called defeasible logic. Results are proved which enable us to see how well the definition captures our intuitions. These results indicate that defeasible logic is well behaved. Section 3 considers, among other things, what happens when a defeasible logic is inconsistent The next two sections are concerned with stability, that is, the property of being undisturbed by the addition or deletion of redundant information In Section 4 we consider redundant rules, and in Section 5 we consider lemma addition. That is the adding of a proved result to the 'axioms' of the logic. KeywordsDefeasible logic, nonmonotonic reasoning, non-classical logic, knowledge representation, expert systems	defeasible logic;expert system;knowledge representation and reasoning;logic programming;non-monotonic logic	David Billington	1993	J. Log. Comput.	10.1093/logcom/3.4.379	computer science	AI	-14.623670282828797	7.608750956515056	21271
88ad955b9f94603a913d6322e3acd2497ae99175	recognizing plans with loops represented in a lexicalized grammar		This paper extends existing plan recognition research to handle plans containing loops. We supply an encoding of plans with loops for recognition, based on techniques used to parse lexicalized grammars, and demonstrate its effectiveness empirically. To do this, the paper first shows how encoding plan libraries as context free grammars permits the application of standard rewriting techniques to remove left recursion and !-productions, thereby enabling polynomial time parsing. However, these techniques alone fail to provide efficient algorithms for plan recognition. We show how the loop-handling methods from formal grammars can be extended to the more general plan recognition problem and provide a method for encoding loops in an existing plan recognition system that scales linearly in the number of loop iterations.	algorithm;attribute grammar;combinatory categorial grammar;context-free grammar;encode;formal grammar;iteration;left recursion;library (computing);parsing expression grammar;polynomial;rewriting;time complexity	Christopher W. Geib;Robert P. Goldman	2011			natural language processing;computer science;artificial intelligence;machine learning;programming language;algorithm	AI	-21.19529266301693	16.29572392449735	21317
44c064d3f293ebb9a4ff5cdbc420df1eb76d4128	a high-level language for modeling algorithms and their properties	state transition system;user needs;set theory;formal verification;model checking;distributed algorithm;high level language	Designers of concurrent and distributed algorithms usually express them using pseudo-code. In contrast, most verification techniques are based on more mathematically-oriented formalisms such as state transition systems. This conceptual gap contributes to hinder the use of formal verification techniques. Leslie Lamport introduced PlusCal, a high-level algorithmic language that has the “look and feel” of pseudocode, but is equipped with a precise semantics and includes a high-level expression language based on set theory. PlusCal models can be compiled to TLA and verified using the model checker tlc. However, in practice the use of PlusCal requires good knowledge of TLA and of the translation from PlusCal to TLA. In particular, the user needs to annotate the generated TLA model in order to define the properties to be verified and to introduce fairness hypotheses. Moreover, the PlusCal language enforces certain restrictions that often make it difficult to express distributed algorithms in a natural way. We propose a new version of PlusCal with the aim of overcoming these limitations, and of providing a language in which algorithms and their properties can be expressed naturally. We have implemented a compiler of our language to TLA, supporting the verification of algorithms by finite-state model checking.	algol;atomicity (database systems);compiler;distributed algorithm;distributed computing;fairness measure;formal methods;formal verification;forward error correction;high- and low-level;high-level programming language;leslie speaker;locality of reference;look and feel;model checking;pluscal;pseudocode;set theory;state space;state transition table;tla+;unified expression language;verification and validation	Sabina Akhtar;Stephan Merz;Martin Quinson	2010		10.1007/978-3-642-19829-8_4	model checking;distributed algorithm;formal verification;computer science;theoretical computer science;programming language;high-level programming language;algorithm;set theory	PL	-20.09121787271308	25.560552018296267	21339
c195c81b517f7ce8ddc258ff2e99e9a034892019	mastering test generation from smart card software formal models	machine abstraite;modelizacion;carte a puce;smart card;model based reasoning;problema valor limite;model specification;tratamiento transaccion;raisonnement base sur modele;condiciones limites;mobile radiocommunication;formal specification;informatique mobile;java card;functional testing;formal model;test cubierta;condition aux limites;interoperabilite;interoperabilidad;validacion;smart card standard;lenguaje uml;cellular radio;boundary value problem;electronic fund transfer;securite informatique;boundary testing;formal specifications;maquina abstracta;langage modelisation unifie;langage java;langage ocl;automated test generation;interpretacion abstracta;program verification;radiocommunication service mobile;abstract machine;generacion automatica prueba;specification formelle;efecto borde;computer security;modelisation;transferencia computarizada de fondos;especificacion formal;control proceso;verificacion programa;edge effect;specification modele;smart cards;boundary condition;especificacion modelo;object oriented;generation test;test coverage;unified modelling language;seguridad informatica;effet bord;process control;generation automatique test;automatic test generation;test generation;oriente objet;cost effectiveness;model based testing;lenguaje java;validation;interoperability;interpretation abstraite;radiotelephonie cellulaire;transaction processing;abstract interpretation;radiocomunicacion servicio movil;mobile computing;verification programme;modeling;probleme valeur limite;orientado objeto;generacion prueba;couverture test;commande processus;monetique;traitement transaction;object constraint language;lenguaje forzado objeto;java language	The growing complexity of new smart card platforms, including multi-subscription or multi-application functionalities, led up to more and more difficulty in testing such systems. In previous work, we have introduced a new method for automated test generation from state-based formal specifications (B abstract machines, UML/OCL models, Z specifications). This method uses cause-effect analysis and boundary computation to produce test cases as sequences of operation invocations. This method is embedded in a model-based test generator which has been exercised on several applications in the domain of smart card software (GSM 11-11 application, electronic purse system and Java Card transaction mechanism). In all these applications, a B abstract machine was built specifically for automatic test generation by an independent validation team. Writing a specific formal model for testing has been shown to be cost-effective, and has the advantages that it can be tailored towards the desired test objectives. This paper focuses on showing the application of this test generation process from formal models in the context of Smart Card applications. We describe how the test generation can be controlled by using several model coverage criteria. These criteria are of three kinds: multiple condition coverage, boundary-value coverage and behavior coverage. This makes it possible to generate a systematic minimal test suite achieving strong coverage results. The test engineer chooses the criteria depending on the application test objectives and then fully controls the test generation process.	bus mastering;smart card	Fabrice Bouquet;Bruno Legeard;Fabien Peureux;Eric Torreborre	2004		10.1007/978-3-540-30569-9_4	embedded system;test data generation;simulation;engineering;automatic test pattern generation;test suite;test case;test management approach;algorithm;test harness	SE	-23.47359160681263	30.351835337936908	21383
843816721686c71686a7688210c99769207c2039	summarizing and propagating uncertain information with triangular norms	triangular norm	Abstract   A wide variety of numerical or symbolic approaches to reasoning with uncertainty have been proposed in the artificial intelligence (AI) literature. This article postulates a list of desiderata that any such formalism should try to satisfy. The author then proposes a new approach to reasoning with uncertainty, which is organized in three layers: representation, inference, and control.  In the representation layer the structure required to capture information used in the inference layer and meta-information used in the control layer are described. In this structure, numerical slots take values on linguistic term sets with fuzzy-valued semantics. These term sets capture the input granularity usually provided by users or experts.  In the inference layer a large number of uncertainty calculi based on triangular norms (T-norms), intersection operators whose truth functionality entails low computational complexity, are described. It is shown that for a common negation operator, the selection of a T-norm uniquely and completely describes an uncertainty calculus. Previous experiments have determined the existence of a small number of equivalence classes among the uncertainty calculi (as a function of the input granularity). This property drastically reduces the number of different combining rules to be considered.  In the control layer the policy selection for the different calculi used in the inference layer, based on their meanings, properties, and contextual information, is specified. Conflicts and ignorance measurements are also defined. The proposed formalism is compared against the requirements of the desiderata and contrasted with existing schemes for reasoning with uncertainty.		Piero P. Bonissone	1987	Int. J. Approx. Reasoning	10.1016/0888-613X(87)90005-3	discrete mathematics;computer science;artificial intelligence;machine learning;mathematics;t-norm;algorithm;statistics	AI	-13.06774013793729	7.83003050283816	21406
5f7d2da12f62b60bd2fc641cd9ee6801156a60d0	history dependent automata for service compatibility	service oriented computing	We use History Dependent Automata (HD-automata) as a syntax-indepentend formalism to check compatibility of services at binding time in Service-Oriented Computing. Informally speaking, service requests are modelled as pairs of HD-automata 〈Co, Cr〉; Cr describes the (abstract) behaviour of the searched service and Co the (abstract) behaviour guaranteed by the invoker. Symmetrically, service publication consists of a pair of HD-automata 〈So, Sr〉 such that So provides an (abstraction of) of the behaviour guaranteed by the service and Sr yields the requirement imposed to invokers. An invocation 〈Co, Cr〉 matches a published interface 〈So, Sr〉 when Co simulates Sr and So simulates Cr.	automata theory;automaton;formal system;name binding;service-oriented device architecture	Vincenzo Ciancia;Gian Luigi Ferrari;Marco Pistore;Emilio Tuosto	2008		10.1007/978-3-540-68679-8_39	real-time computing;computer science;theoretical computer science;distributed computing	Embedded	-33.065915774063996	31.16940370676697	21435
5b2897799d76e6f89d6c69ab697793a16442b500	belief revision, minimal change and relaxation: a general framework based on satisfaction systems, and applications to description logics		Abstract Belief revision of knowledge bases represented by a set of sentences in a given logic has been extensively studied but for specific logics, mainly propositional, and also recently Horn and description logics. Here, we propose to generalize this operation from a model-theoretic point of view, by defining revision in the abstract model theory of satisfaction systems. In this framework, we generalize to any satisfaction system the characterization of the AGM postulates given by Katsuno and Mendelzon for propositional logic in terms of minimal change among interpretations. In this generalization, the constraint on syntax independence is partially relaxed. Moreover, we study how to define revision, satisfying these weakened AGM postulates, from relaxation notions that have been first introduced in description logics to define dissimilarity measures between concepts, and the consequence of which is to relax the set of models of the old belief until it becomes consistent with the new pieces of knowledge. We show how the proposed general framework can be instantiated in different logics such as propositional, first-order, description and Horn logics. In particular for description logics, we introduce several concrete relaxation operators tailored for the description logic ALC and its fragments EL and ELU , discuss their properties and provide some illustrative examples.	belief revision;description logic;linear programming relaxation	Marc Aiguier;Jamal Atif;Isabelle Bloch;Céline Hudelot	2018	Artif. Intell.	10.1016/j.artint.2017.12.002	monoidal t-norm logic;t-norm fuzzy logics;discrete mathematics;non-monotonic logic;mathematics;algorithm	ML	-15.324947215016287	8.139299602991345	21437
14e6ffe3e9040c28f8186388889a9f5663bb4fa4	programmanalyse des xrtl-zwischencodes		We present the xGCC analysis tool for the verification of safety properties of the XRTL intermediate code. These properties include the absence of buffer overflow, division by zero and the use of uninitialized variables and memory. XRTL is our extension of the Register Transfer Language (RTL). This language independent intermediate code is generated by frontends of the GNU Compiler Collection (GCC) for the programming languages C, C++, Java and Fortran 77. These programming languages are supported by xGCC since we have modified only the language independent part of the compiler. We apply abstract interpretation for the analysis of XRTL. Lists of valid intervals are used for the abstraction of sets of registers and memory blocks. Valid intervals are intervals with additional contraints that simplify the implementation of the XRTL operations. The precision of the abstraction is parameterized by the list length. We describe the interpretation of sequential and parallel XRTL instructions. We take branching conditions into account for restricting the search space, and apply the Widening/Narrowing techniques to speed up the fixpoint computation for XRTL loops. We present the implementation of xGCC and explain the tool design. We demonstrate xGCC analysis tool on a collection of examples. We analyse the tool performance on examples from the Numerical Recipes in C, and introduce several optimizations.	abstract interpretation;buffer overflow;c++;computation;division by zero;fixed point (mathematics);fortran;gnu compiler collection;java;numerical recipes;numerical method;processor register;programming language;register transfer language	Werner Backes	2004				PL	-19.340138715462427	29.84828623500558	21459
e6116db44fa0ea77e7eed79a700f5dadc36e07e5	a lisp environment for creating and applying rules for musical performance			lisp	Anders Friberg;Johan Sundberg	1986			musical;programming language;preprocessor;lisp;computer science	HPC	-25.938695148510973	21.244559979165796	21488
8757aa8c7ce795addc2096ce21e7037727e7d12a	dsp implementation of electric drive control system	digital signal processing;control systems;high level languages;inverters;control systems hardware inverters digital signal processing object oriented programming current measurement;object oriented programming;object oriented programming c language digital signal processing chips electric drives high level languages machine control;machine control;object oriented programming dsp electric drive control system programs development high level language c program;c language;current measurement;electric drives;digital signal processing chips;hardware	This paper presents the development of programs for control electric drive. Prepared programs have been implemented on the DSP using a high-level language. The use of C++ program has enabled a logical division. The use of object-oriented programming greatly simplified the structure of the program.	c++;control system;high- and low-level;high-level programming language	Dominik Luczak	2012	2012 8th International Symposium on Communication Systems, Networks & Digital Signal Processing (CSNDSP)	10.1109/CSNDSP.2012.6292783	embedded system;real-time computing;computer science;programming language implementation;control system;digital signal processing;programming language;object-oriented programming;high-level programming language	Arch	-33.572775357018635	29.470015664658288	21556
3de42fcb76c7c0c711438b9942903476749ab404	pep - more than a petri net tool	programming environment;petri net	More than a Petri Net Tool Bernd Grahlmann and Eike Best ABSTRACT The PEP system (Programming Environment based on Petri Nets) supports the most important tasks of a good net tool, including HL and LL net editing and comfortable simulation facilities. In addition, these features are embedded in sophisticated programming and veri cation components. The programming component allows the user to design concurrent algorithms in an easy-to-use imperative language, and the PEP system then generates Petri nets from such programs. The PEP tool's comprehensive veri cation components allow a large range of properties of parallel systems to be checked e ciently on either programs or their corresponding nets. This includes user-de ned properties speci ed by temporal logic formulae as well as speci c properties for which dedicated algorithms are available. PEP has been implemented on Solaris 2.4, SunOS 4.1.3 and Linux. Ftp-able versions are available.	algorithm;embedded system;horseland;imperative programming;ll parser;linux;petri net;simulation;sunos;temporal logic	Bernd Grahlmann;Eike Best	1996		10.1007/3-540-61042-1_58	computer science;petri net	Embedded	-29.775667918076547	29.103836555411185	21557
389448818fca6de12b71b32f69c3a6677980d9fb	good design principles in a compiler university course	compiler construction;lenguaje programacion;design principle;object oriented language;compilateur;langage c;programming language;compiler design;langage java;object oriented programming;compiler;c language;interpreteur;javacc;jjtree;object oriented;design pattern;langage programmation;object orientation;lenguaje java;design patterns;interpreter;programmation orientee objet;interprete;compilador;lenguaje c;java language	This paper presents what aims to be an example of good design principles applied to compiler construction. To be more specific, it presents an interpreter of a very simple object oriented language, called SmallScript, that has been designed to be taught in a Compiler University course. Our aim is not to develop a new, revolutionary language, neither to show a spectacular advance in some research field of compiler construction. Instead, as university teachers, we aim to offer both students and teachers an example of how a modern interpreter can be designed, focusing on object orientation and using modern tools like JavaCC and JJTrree.	class diagram;code refactoring;compiler;javacc;world wide web	César F. Acebal;Raúl Izquierdo;Juan Manuel Cueva Lovelle	2002	SIGPLAN Notices	10.1145/510857.510870	compiler correctness;computer science;theoretical computer science;compiler construction;programming language;object-oriented programming	PL	-27.706565509816794	24.046768850424694	21566
7c22d0c93ec575bcb9afc87f8c43cc4fa991f4b9	truth in constructive metamathematics				John Staples	1978	Notre Dame Journal of Formal Logic	10.1305/ndjfl/1093888413	logical truth;metamathematics	Crypto	-12.427027557977716	10.899123363198855	21605
fd171aeaf7b0340e97a70fa412e3093058df33bd	decidability of invariant validation for paramaterized systems	boolean variable;large class;control part;corresponding verification condition;invariant validation;program invariants;many-sorted first-order logic;systematic procedure;paramaterized system;herbrand theorem	The control part of many concurrent and distributed programs reduces to a set Π = {p1, . . . , pn} of symmetric processes containing mainly assignments and tests on Boolean variables. However, the assignments, the guards and the program invariants can be Π-quantified, so the corresponding verification conditions also involve Π-quantifications. We propose a systematic procedure allowing the elimination of such quantifications for a large class of program invariants. At the core of this procedure is a variant of the Herbrand Theorem for many-sorted first-order logic with equality.	concurrent computing;data validation;first-order logic;first-order predicate;heuristic (computer science);invariant (computer science);lamport's bakery algorithm;quantifier (logic);universal instantiation	Pascal Fontaine;E. Pascal Gribomont	2003		10.1007/3-540-36577-X_8	combinatorics;discrete mathematics;mathematics;algorithm	Logic	-14.964458283549298	22.182901092282652	21610
5c567faa51a3785cd5f8e3829dc8207103c9a78d	from set to hyperset unification	system of equations	In this paper we show how to extend a set unification algorithm— that is, an extended unification algorithm incorporating the axioms of a simple theory of sets—to hyperset unification (roughly speaking, to sets in which membership can form cycles). This result is obtained by enlarging the domain of terms (hence, trees) to that of graphs involving free as well as interpreted function symbols (namely, the set element insertion and the empty set), which can be regarded as a convenient denotation of hypersets. We present a hyperset unification algorithm that (nondeterministically) computes, for each given unification problem, a finite collection of systems of equations in solvable form whose solutions represent a complete set of solutions for the given unification problem. The crucial issue of termination of the algorithm is addressed and solved by the addition of simple nonmembership constraints. Finally, the hyperset unification problem in	algorithm;bisimulation;decision problem;graph (discrete mathematics);han unification;modulo operation;nondeterministic algorithm;set theory;turing completeness;unification (computer science)	Davide Aliffi;Agostino Dovier;Gianfranco Rossi	1999	Journal of Functional and Logic Programming		system of linear equations;computer science	Logic	-9.727190891134057	18.230250754527056	21624
9cbd4eee86fe006723e6f71fa72fd0e3c45cfb61	model transformations with reference models	generic model;reference model;model transformation;model management;model driven architecture	In this paper we introduce and explore an extension to the existing paradigm of model transformation. Specifically, we extend existing model transformation approaches by considering reference models and human input as important sources for and during model transformation. To cater for this new type of model transformation, we develop an approach grounded on a common generic model and a series of transformation operators, which constitute an non-trivial extension to the “classical” model management operators.	model transformation;programming paradigm	Willem-Jan van den Heuvel;Manfred A. Jeusfeld	2007		10.1007/978-1-84628-858-6_6	single-index model;ball-and-stick model;logical data model	DB	-28.431367795698982	15.080929282477848	21732
4613068cecf801d3673e8fc125b3a1b2f0311d56	an algebraic approach to the disjunction property of substructural logics	algebraic approach;cut elimination;well connectedness;disjunction property;residuated lattice;substructural logic			Daisuke Souma	2007	Notre Dame Journal of Formal Logic	10.1305/ndjfl/1193667706	monoidal t-norm logic;combinatorics;discrete mathematics;mathematics;programming language;substructural logic;algebra	Logic	-11.456095987238697	13.096191920780589	21754
15a36557460c263ca53cc3bf64955878f117ca88	a new n-ary existential quantifier in description logics	blow up;quantifier;logica descripcion;genie chimique;intelligence artificielle;chemical engineering;quantificateur;artificial intelligence;ingenieria quimica;inteligencia artificial;description logic;process engineering;cuantificador;logique description	Motivated by a chemical process engineering application, we introduce a new concept constructor in Description Logics (DLs), an n-ary variant of the existential restriction constructor, which generalizes both the usual existential restrictions and so-called qualified number restrictions. We show that the new constructor can be expressed in ALCQ, the extension of the basic DL ALC by qualified number restrictions. However, this representation results in an exponential blow-up. By giving direct algorithms for ALC extended with the new constructor, we can show that the complexity of reasoning in this new DL is actually not harder than the one of reasoning in ALCQ. Moreover, in our chemical process engineering application, a restricted DL that provides only the new constructor together with conjunction, and satisfies an additional restriction on the occurrence of roles names, is sufficient. For this DL, the subsumption problem is polynomial.	algorithm;description logic;existential quantification;polynomial;quantifier (logic);subsumption architecture;time complexity	Franz Baader;Carsten Lutz;Eldar Karabaev;Manfred Theißen	2005	KI	10.1007/11551263_4	description logic;computer science;artificial intelligence;mathematics;algorithm	AI	-16.217321795611674	12.392330518644783	21783
a43b91c59e60319f129bc61ca29d13eff604abfb	operational machine specification in a functional programming language	machine abstraite;langage fonctionnel;lenguaje programacion;formal specification;compilateur;executable specification;programming language;maquina abstracta;lenguaje funcional;tipo dato;semantics;abstraction;abstract data type;data type;compiler;abstraccion;dynamic behaviour;semantica;semantique;functional programming;specification language;abstract machine;machine specification;denotational semantic;article letter to editor;graph rewriting;type abstrait;langage programmation;prototype implementation;tipo abstracto;programmation fonctionnelle;lenguaje especificacion;external research report;type donnee;functional language;programacion funcional;langage specification;functional programming language;lenguaje formal;formal language;compilador;langage formel	This paper advocates the use of a functional programming language for the formal specification of (abstract) machines. The presented description method describes machines at two levels. At the bottom layer machine components and micro instructions to handle them are described by an abstract data type. The top layer describes the machine instructions in terms of these micro instructions. The use of a functional language for this purpose has several advantages. The abstraction mechanisms offered by a functional programming language are that good that one can abstract from irrelevant details as is required for a specification language. Functional languages have a well defined semantics such that the meaning of the specification is clear as well. On the other hand they offer the advantages of a programming language: the compiler can check the specification for partial correctness eliminating e.g. type errors and unbound identifiers (errors which occur in many published descriptions). Furthermore, the specification can be executed such that one obtains a prototype implementation almost for free. Such an executable formal specification can for instance be used to investigate the dynamic behaviour of the described machine. For a simple machine, the proposed description method is compared with several other description methods: a traditional style, a denotational semantics and a formal specification in the language Z. To show that the proposed method is indeed useful to describe large and complicated machines, the method is applied for the specification of an abstract imperative graph rewrite machine (the ABC-machine) which has been used in the construction of the compiler for the functional language Concurrent Clean.	apl;abstract data type;compiler;correctness (computer science);denotational semantics;description logic;executable;formal specification;functional programming;identifier;imperative programming;programming language;prototype;relevance;rewrite (programming);specification language	Pieter W. M. Koopman;Marko C. J. D. van Eekelen;Marinus J. Plasmeijer	1995	Softw., Pract. Exper.	10.1002/spe.4380250502	interface description language;compiler;formal language;formal methods;language primitive;object language;specification language;data type;formal verification;computer science;theoretical computer science;operating system;formal specification;functional specification;abstraction;low-level programming language;programming language;functional programming;abstract data type;programming language specification;high-level programming language;algorithm;language of temporal ordering specification;graph rewriting	PL	-23.991876370373728	27.75772349623349	21791
0892e7f9863522d72a7f4fd613770a2be2ffdfb1	decomposing abstract dialectical frameworks		We introduce a decomposition scheme for abstract dialectical frameworks (ADFs). The decomposition proceeds along the ADF’s strongly connected components. For several semantics, the decomposition-based version coincides with the original semantics. For others, the scheme defines new semantics. These new semantics allow us to deal with pertinent problems such as odd-length negative cycles in a more general setting, that for instance also encompasses logic programs.	relevance;strongly connected component	Sarah Alice Gaggl;Hannes Strass	2014		10.3233/978-1-61499-436-7-281	theoretical computer science;artificial intelligence;natural language processing;dialectic;semantics;strongly connected component;computer science	Logic	-15.279041322732095	10.143940416000316	21813
b942c1d464467f44371ebaa6bd6c275fc48a68f1	an incompleteness result for deductive synthesis of logic programs	program synthesis;logic programs;first order logic	We formalise the derivation of logic programs from their speciications by deductive synthesis, and introduce the notion of uniform equivalence between logical systems. This enables us to present an incompleteness result for deductive synthesis of logic programs from rst-order logic speciications.	deductive database;logic programming;turing completeness	Kung-Kiu Lau;Mario Ornaghi	1993			dynamic logic;zeroth-order logic;description logic;higher-order logic;computer science;artificial intelligence;bunched logic;first-order logic;computational logic;programming language;multimodal logic;algorithm;philosophy of logic;autoepistemic logic	Logic	-14.937984324598292	14.010843127562318	21829
330825fa84a907613d515026e33e76cfb66800d9	on binary trees	binary tree	A simple example Draw by hand a binary search tree with 2 at the root and values 1, 3, and 4 in the subtrees. How many different tree structures are possible? Next, start Eclipse and create a new project for this laboratory (note that you will need to include the BAILEY variable for this lab). Write a short program to construct the tree you just drew and print it out using the toString method. Note that there are three constructors: no data, one data item and one data item with two children. If you need a BinaryTree with only one child (with a data value) you’ll need to specify the other child as an empty tree. Notice how the toString methods prints out the trees and in particular, the empty trees.	data item;eclipse;java;recursion (computer science);search tree;tree structure	Rolf Klein;Derick Wood	1989			combinatorics;binary tree;binary expression tree;k-d tree;interval tree;mathematics	ML	-29.530364562326213	25.641874938953144	21844
40ba04f01bb78c3ab8b8229fbfc438d821f55068	a measure of semantic relatedness for resolving ambiguities in natural language database requests	semantic ambiguity;lenguaje natural;base relacional dato;natural language interfaces;query language;interfase usuario;user interface;modifier attachment;sistema informatico;langage naturel;interrogation base donnee;interrogacion base datos;semantics;computer system;relational database;semantica;semantique;word sense disambiguation;lenguaje interrogacion;semantic relatedness;conceptual schema;natural language understanding;natural language;database schema;base donnee relationnelle;ambiguity;interface utilisateur;systeme informatique;langage interrogation;ambiguedad;database query;ambiguite	A measure of semantic relatedness based on distance between objects in the database schema has previously been used as a basis for solving a variety of natural language understanding problems including word sense disambiguation, resolution of semantic ambiguities, and attachment of post noun modifiers. The use of min/max values which are usually recorded as part of the process of designing the database schema is proposed as a basis for solving the given problems as they arise in natural language database requests. The min/max values provide a new source of knowledge for resolving ambiguities and a semantics for understanding what knowledge has previously been used by distance measures in database schemas.	natural language;semantic similarity	Julia A. Johnson;Richard S. Rosenberg	1991	Data Knowl. Eng.	10.1016/0169-023X(92)90038-D	natural language processing;semantic similarity;relational database;computer science;conceptual schema;data mining;database;semantics;natural language;user interface;database schema;query language	DB	-33.207203650451376	9.264899986871445	21864
97d0c6e974dfe779432748f392b387d44462ab3d	a logical approach to structure sharing in tags.	tree adjoining grammar	Tree adjoining grammars (TAG) represent a derivational formalism to construct trees from a given set of initial and auxiliary trees. We present a logical language that simultaneously describes the generated TAG-tree and the corresponding derivation tree. Based on this language we formulate constraints indicating whether a tree and a derivation tree mean a valid TAGgenerated tree. A method is presented that extracts the underlying TAG from an (underspecified) TAG-tree and its derivation. This leads to an alternative approach of representing shared structures by means of TAGs. The result is a more general representation of movement which requires no indices since it basically makes use of the properties of the adjunction operation.	abstract syntax tree;algorithm;parse tree;semantics (computer science);tag (metadata);tree structure;tree-adjoining grammar	Adi Palm	2000			tree-adjoining grammar;natural language processing;attribute grammar;computer science;artificial intelligence	NLP	-21.53382099265148	16.0699146317247	21931
be272a93f0f811d700c398814ca8190171ec0427	extensible pattern matching in an extensible language	programming language;general techniques;pattern matching;language extension;functional language	Pattern matching is a widely used technique in functional languages, especially those in the ML and Haskell traditions, where it is at the core of the semantics. In languages in the Lisp tradition, in contrast, pattern matching it typically provided by libraries built with macros. We present match, a sophisticated pattern matcher for Racket, implemented as language extension. using macros. The system supports novel and widely-useful pattern-matching forms, and is itself extensible. The extensibility of match is implemented via a general technique for creating extensible language extensions. 1 Extending Pattern Matching The following Racket1 [12] program finds the magnitude of a complex number, represented in either Cartesian or polar form as a 3-element list, using the first element as a type tag: (define (magnitude n) (cond [(eq? (first n) ’cart) (sqrt (+ (sqr (second n)) (sqr (third n))))] [(eq? (first n) ’polar) (second n)])) While this program accomplishes the desired purpose, it’s far from obviously correct, and commits the program to the list-based representation. Additionally, it unnecessarily repeats accesses to the list structure making up the representation. Finally, if the input is ’(cart 7), it produces a hard-to-decipher error from the third function. In contrast, the same program written using pattern matching is far simpler: (define (magnitude n) (match n [(list ’cart x y) (sqrt (+ (sqr x) (sqr y)))] [(list ’polar r theta) r])) The new program is shorter, more perspicuous, does not repeat computation, and produces better error messages. For this reason, pattern matching has become a ubiquitous tool in functional programming, especially for languages in the Haskell and ML families. Unfortunately, pattern matching is less ubiquitous in functional languages in the 1 Racket is the new name of PLT Scheme. Lisp tradition, such as Common Lisp, Scheme, and Racket. This is unfortunate, since as we demonstrate in the remainder of the paper, not only are the same benefits available as in Haskell or ML, but the extensibility provided by languages such as Racket leads naturally to expressive and extensible pattern matchers. 1.1 More Expressive Patterns The function can also be easily converted to arbitrary-dimensional coordinates, using the ... notation for specifying an arbitrary-length list: (define (magnitude n) (match n [(list ’cart xs ...) (sqrt (apply + (map sqr xs)))] [(list ’polar r theta ...) r])) Racket is untyped, so we can add argument checking in the pattern match to catch errors early. Here we use the ? pattern, which tests the value under consideration against the supplied predicate: (define (magnitude n) (match n [(list ’cart (? real? xs) ...) (sqrt (apply + (map sqr xs)))] [(list ’polar (? real? r) (? real? theta) ...) r])) This implementation is more robust than our original function, but it approaches it in complexity, and still commits us to a list-based representation of coordinates.	cartesian closed category;common lisp;compiler;computation;condition number;domain-specific language;emoticon;error message;extensibility;extensible programming;functional programming;haskell;library (computing);list of programming languages by type;metalinguistic abstraction;pattern matching;programmer;racket;rewriting;scheme	Sam Tobin-Hochstadt	2010	CoRR		computer science;theoretical computer science;extensible programming;pattern matching;programming language;functional programming;algorithm	PL	-23.105734032459758	25.074632294283614	22035
2beb71b47c9c4d69250c3a91f962c18249f9c434	when wide scope is not enough: scope and topicality of discourse referents	modal subordination;location;sign space;catalan sign language lsc;specificity;scope	This paper analyses the semantic attributes discourse referents in Catalan Sign Language may have in order to have a corresponding location established in sign space. It is argued that a combination of scope and topicality is required when analysing the correlation between the introduction of entities into the discourse and assigning a spatial location.	entity;item unique identification;programming language;quantifier (logic);refinement (computing);semi-continuity;sensitivity and specificity;theory	Gemma Barberà	2011		10.1007/978-3-642-31482-7_7	mathematics;linguistics;communication;algorithm	NLP	-11.203670173684156	9.480374504378128	22049
fdb292232bfbf50541fdde480b0a9504aa1dd81f	a novel stochastic game via the quantitative mu-calculus	macquarie university institutional repository;draw and stalemate;researchonline;digital repository;temporal logic;macquarie university;intermediate fixed points;probabilistic system;fixed point;probabilistic systems;quantitative logic;mu calculus;stochastic games	The quantitative μ-calculus qMμ extends the applicability of Kozen’s standard μ-calculus [5] to probabilistic systems. Subsequent to its introduction [9,4] it has been developed by us [6,7,8] and by others [2]. Beyond its natural application to define probabilistic temporal logic [10], there are a number of other areas that benefit from its use. One application is stochastic two-player games, and the contribution of this paper is to depart from the usual notion of “absolute winning conditions” and to introduce a novel game in which players can “draw”. The extension is motivated by examples based on economic games: we propose an extension to qMμ so that they can be specified; we show that the extension can be expressed via a reduction to the original logic; and, via that reduction, we prove that the players can play optimally in the extended game using memoryless strategies.	algorithm;fixed point (mathematics);modal μ-calculus;symbolic stream generator;temporal logic	Annabelle McIver;Carroll Morgan	2006	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2005.10.039	digital library;temporal logic;computer science;artificial intelligence;mathematics;fixed point;algorithm	Logic	-6.010932989857294	4.605474537851455	22051
9b09341d63284351e80c95d3a80c4c241a3d8ed5	psi: exact symbolic inference for probabilistic programs		Probabilistic inference is a key mechanism for reasoning about probabilistic programs. Since exact inference is theoretically expensive, most probabilistic inference systems today have adopted approximate inference techniques, which trade precision for better performance (but often without guarantees). As a result, while desirable for its ultimate precision, the practical effectiveness of exact inference for probabilistic programs is mostly unknown. This paper presents PSI, a novel symbolic analysis system for exact inference in probabilistic programs with both continuous and discrete random variables. PSI computes succinct symbolic representations of the joint posterior distribution represented by a given probabilistic program. PSI can compute answers to various posterior distribution, expectation and assertion queries using its own back-end for symbolic reasoning. Our evaluation shows that PSI is more effective than existing exact inference approaches: (i) it successfully computed a precise result for more programs, and (ii) simplified expressions that existing computer algebra systems (e.g., Mathematica, Maple) fail to handle.	approximation algorithm;assertion (software development);beta;bernoulli polynomials;computer algebra system;debugging;end-to-end principle;gamma correction;maple;mathematical optimization;pareto efficiency;portable document format;randomized algorithm;rayleigh–ritz method;relational operator;scalability;source lines of code;symbolic computation;type inference;wolfram mathematica	Timon Gehr;Sasa Misailovic;Martin T. Vechev	2016		10.1007/978-3-319-41528-4_4	theoretical computer science;database;programming language	Logic	-16.870479669190637	25.79412470149261	22106
6b618817f838f3aaffff6b3e8bc86973fa3d477e	rewrite rule grammars with multitape automata		The majority of computational implementations of phonological and morphophonological alternations rely on composing together individual finite state transducers that represent sound changes.  Standard composition algorithms do not maintain the intermediate representations between the ultimate input and output forms.  These intermediate strings, however, can be very helpful for various tasks: enriching information (indispensable for models of historical linguistics), providing new avenues to debugging complex grammars, and offering explicit alignment information between morphemes, sound segments, and tags.  This paper describes a multi-tape automaton approach to creating full models of sequences of sound alternation that implement phonological and morphological grammars. A model and a practical implementation of multitape automata is provided together with a multitape composition algorithm tailored to the representation used in this paper.  Practical use cases of the approach are illustrated through two common examples; a phonological example of a complex rewrite rule grammar where multiple rules interact and a diachronic example of modeling sound change over time.	automaton	Mans Hulden	2017	J. Language Modelling	10.15398/jlm.v5i1.158	debugging;programming language;natural language processing;automaton;phonology;sound change;alternation (linguistics);morpheme;rule-based machine translation;computer science;grammar;artificial intelligence	Theory	-23.07666837121057	17.894663828890504	22171
ae35c14dfc4763823c2cb02d9bd05ee04302d8ec	modularity in denotational semantics	denotational semantic;side effect	Abstract   We consider a modular approach to denotational semantics. We reformulate and extend the idea of monads as notions of computation to algebraic structure together with a construction of an extended semantic category. We show that upon making that reformulation, one can obtain some account of modularity, in particular accounting for the interaction between side-effects and various forms of nondeterminism. That involves extending the notion of distributivity of a monad over another monad to distributivity of a monad over any algebraic structure. We give a general theorem which asserts when algebraic structure extends along a Kleisli category, thus allowing modularity.	denotational semantics	John Power	1997	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(05)80153-7	discrete mathematics;computer science;mathematics;programming language;denotational semantics of the actor model;side effect;algorithm	Logic	-12.165478127653492	17.617126862457916	22176
db2941a872533addc5aa1353a7e8753487c375b5	solving large combinatorial problems in logic programming	combinatorial problems;logic programs	D Many problems in operations research and hardware design are combinatorial problems which can be seen as search problems with constraints. We present an application of CHIP (constraint Handling In Prolog) to large problems in disjunctive scheduling, graph coloring, and firmware design. CHIP is a constraint logic-programming language combining the declarative aspects of PROLOG with the efficiency of constraint-solving techniques. It is shown that it allows a natural expression of problems to be executed as efficiently as special-purpose programs written in procedural languages. a	constraint logic programming;constraint satisfaction problem;disjunctive normal form;firmware;graph coloring;operations research;procedural programming;programming language;prolog;scheduling (computing)	Mehmet Dincbas;Helmut Simonis;Pascal Van Hentenryck	1990	J. Log. Program.	10.1016/0743-1066(90)90052-7	covering problems;computer science;theoretical computer science;programming language;algorithm	Theory	-22.40820359347981	19.626995109308613	22222
c6d4eff279b5f359090709d9de462d26da6be8fd	using deduction techniques for natural language understanding			natural deduction;natural language understanding	Michael Kohlhase	2000			linguistics;natural language understanding;natural language processing;computer science;artificial intelligence	DB	-18.89693321664906	11.3318069800282	22225
363db3b93f07ff3320446ac27cfb1850dd431c77	the breakdown of the information model in multi-database systems	database system;information model	"""A database tries to present a faithful model of some body of information we can loosely call """"reality"""" (even though the information might be historical, obsolete, fictional, speculative, or even erroneous). The model is constructed from whatever sorts of materials a particular data model provides, such as relations and attributes, or entities, attributes, and relationships. In this paper, we will build models using the materials provided by functional object models such as Iris [Fi]: objects, types, and functions. Functions collectively model the notions of attributes, relationships, and operations."""	data model;database;entity;information model;speculative execution	William Kent	1991	SIGMOD Record	10.1145/141356.141358	data modeling;semi-structured model;information model;computer science;database;physical data model;information retrieval;database testing;database design	DB	-29.465609607102863	9.889622223117607	22228
67ce1cb770662b3b977b6fc68a80b00af5e69738	tablog: the deductive-tableau programming language	resolution;programming language;semantics;computer logic;computer programming;first order;logic programs;first order logic;programming languages	TABLOG (Tableau Logic Programming Language) is a language combining functional and logic programming using first-order (quantifier-free) predicate logic with equality. TABLOG incorporates advantages of LISP and PROLOG.  A program in TABLOG is a list of formulas in a first-order logic (including equality, negation, and equivalence) that is more general and more expressive than PROLOG'S Horn clauses. Whereas PROLOG programs must be relational, TABLOG programs may define either relations or functions. While LISP programs yield results of a computation by returning a single output value, TABLOG programs can be relations and can produce several results simultaneously through their arguments.	computation;first-order logic;functional programming;horn clause;logic programming;method of analytic tableaux;parameter (computer programming);programming language;prolog;quantifier (logic);turing completeness	Yonathan Malachi;Zohar Manna;Richard J. Waldinger	1984		10.1145/800055.802049	dynamic logic;first-generation programming language;constraint programming;description logic;declarative programming;horn clause;stable model semantics;programming domain;computer science;theoretical computer science;functional logic programming;first-order logic;computational logic;semantics;programming paradigm;procedural programming;symbolic programming;inductive programming;datalog;fifth-generation programming language;programming language;prolog;logic programming;algorithm;philosophy of logic	PL	-17.33855494162362	20.260908388931018	22230
86d08d15306b3321bd2165b385a8682e289a88c8	dynamic semantics and vp-ellipsis	dynamic semantics	A compositional theory of meaning is one in which the meaning of a complex expression is a function of the meanings of its constituents. As argued in :lanssen (1983), there are philosophicM, practical and technical advantages in having a compositional semantics for natural language. In particular, a compositional semantics is more likely to be implementable than a non-compositional one simply because all the information required to build the meaning of a given constituent must be contained in its sub-constituents. There are a number of obstacles towards gaining a grammar for natural language (NL) which conforms to the compositionality principle. This paper considers one such obstacle, the phenomenon of VP ellipsis (VPE) as illustrated in (1).	nl (complexity);natural language;programming language;vp-info;video processing engine	Claire Gardent	1990		10.1007/BFb0018446	mathematical optimization;combinatorics;discrete mathematics;mathematics	NLP	-12.314710337978287	7.3520625294891	22242
5974d7116fc9f2c50cc2b3ba8a4ef510bf2dbaed	predicting the visualization intensity for interactive spatio-temporal visual analytics: a data-driven view-dependent approach	interactive visualization;compute unified device architecture cuda;spatio temporal data clustering;intensity prediction	ABSTRACTThe continually increasing size of geospatial data sets poses a computational challenge when conducting interactive visual analytics using conventional desktop-based visualization tools. In recent decades, improvements in parallel visualization using state-of-the-art computing techniques have significantly enhanced our capacity to analyse massive geospatial data sets. However, only a few strategies have been developed to maximize the utilization of parallel computing resources to support interactive visualization. In particular, an efficient visualization intensity prediction component is lacking from most existing parallel visualization frameworks. In this study, we propose a data-driven view-dependent visualization intensity prediction method, which can dynamically predict the visualization intensity based on the distribution patterns of spatio-temporal data. The predicted results are used to schedule the allocation of visualization tasks. We integrated this strategy with a parallel visualizatio...	visual analytics	Jing Li;Tong Zhang;Qing Liu;Manzhu Yu	2017	International Journal of Geographical Information Science	10.1080/13658816.2016.1194424	software visualization;visual analytics;information visualization;interactive visualization;interactive visual analysis;computer science;data science;theoretical computer science;parallel rendering;data mining	Visualization	-26.772888429444166	15.935453731852615	22244
e89e2c53dea14cf0a373d0bf48503bf4a48fbd48	propositional dynamic logic with fixed points: algorithmic tools for verification of finite state machines	fixed point;specification and verification;propositional dynamic logic;finite state machine	The Propositioned Dynamic Logic (PDL) [1] and single exponential decision procedeures for PDL are tools for specification and verification of simple nondeterministic systems [2]. But there is not any kind of concurrency, no features to separate abort and infinite loops, etc.	dynamic logic (modal logic);finite-state machine	Nikolay V. Shilov	1992		10.1007/BFb0023897	dynamic logic;zeroth-order logic;modal μ-calculus;discrete mathematics;resolution;interval temporal logic;computer science;intermediate logic;theoretical computer science;mathematics;fixed point;high-level verification;propositional variable;finite-state machine;algorithm;functional verification	Logic	-11.87594624412119	23.76443394679241	22269
c75a68d8dd73f3db042e19b6e54968e4a429e460	eager and lazy enumerations in concurrent prolog			lazy evaluation;logic programming;prolog	Hideki Hirakawa;Takashi Chikayama;Koichi Furukawa	1984			programming language;enumeration;algorithm;prolog;computer science	DB	-21.504618506508514	22.417145537535742	22375
ae0fb4a71293524ab13fa1db8c21c97cf010ea4a	experiences from a tbox reasoning application: deriving a relational model by owl schema analysis		Given an OWL-DL ontology of an application domain, we derive an application-specific relational model as known from classical database design, where the relational model is created from an ER model. The mapping depends as usual on the cardinalities of properties; namely whether a property is functional or not on a certain domain. In particular, since properties may be defined for several classes, where they can be functional on some of them, and multivalued on others, a detailed schema inspection is necessary. We show that this requires the definition of auxiliary classes at processing time followed by containment checks. Thus, the process cannot be implemented declaratively by a simple set of SPARQL queries, but requires the use of a procedural framework that interferes with the ontology.	application domain;cardinality (data modeling);database design;declarative programming;entity–relationship model;erdős–rényi model;functional programming;relational model;sparql;semantic reasoner;simple set;tbox	Thomas Hornung;Wolfgang May	2013			relational model;ontology;application domain;data mining;computer science;database;schema (psychology);simple set;sparql;entity–relationship model;database design	DB	-25.18566494860211	9.659901665814928	22385
4b21e38feb8fdd7be539239d43c8f61608726efd	a tool for the certification of plcs based on a coq semantics for sequential function charts	software engineering;embedded system;sequential function chart;industrial automation;coq proof assistant	In this report we describe a tool framework for certifying properties of PLCs: CERTPLC. CERTPLC can handle PLC descriptions provided in the Sequential Function Chart (SFC) language of the IEC 61131–3 standard. It provides routines to certify properties of systems by delivering an independently checkable formal system description and proof (called certificate) for the desired properties. We focus on properties that can be described as inductive invariants. System descriptions and certificates are generated and handled using the COQ proof assistant. Our tool framework is used to provide supporting evidence for the safety of embedded systems in the industrial automation domain to third-party authorities. In this document we describe the tool framework: usage scenarios, the architecture, semantics of PLCs and their realization in COQ, proof generation and the construction of certificates.	automation;confidentiality;coq (software);embedded system;formal system;inductive reasoning;np-completeness;proof assistant;public key certificate;sequential function chart;space-filling curve;state transition table	Jan Olaf Blech	2011	CoRR		computer science;systems engineering;engineering;automation;software engineering;programming language;algorithm	Logic	-20.708297789058886	27.074884241502698	22397
b41c42dcadbc9df8cb44375d4712393687918888	langages formels : quelques aspects quantitatifs. (on some quantitative aspects of formal languages)			formal language	Aldric Degorre	2009				PL	-24.75704111813028	20.061322196900967	22435
920921fa4f2b80a1a746b2d664a1ee8a319628e8	haskell 98 libraries: character utilities	library char;character utilities	19.1 Library Char 195	haskell	Simon L. Peyton Jones	2003	J. Funct. Program.	10.1017/S0956796803002119	natural language processing;computer science;programming language;algorithm	PL	-27.76843098990062	22.55205734209172	22447
10cbb25e3d98c0e8b0511bccd76b53e5f2615270	keeping calm in the face of change - towards optimisation of frp by reasoning about change	synchronous data flow;change propagation;institutional repository research archive oaister;functional reactive programming;hybrid systems;domain specific languages	Functional Reactive Programming (FRP) is an approach to rea ctiv programming where systems are structured as networks of functions opera ting on signals. FRP is based on the synchronous data-flow paradigm and supports both (an app roximation to) continuoustime and discrete-time signals (hybrid systems). What sets FRP apart from most other languages for similar applications is its support for systems w ith dynamic structure and for higher-order reactive constructs (e.g. signals carrying s ignals or functions on signals). This paper contributes towards advancing the state of the ar of FRP implementation by studying the notion of signal change and change propagation in a setting of structurally dynamic networks of n-ary signal functions operating on mixed continuous-time a nd discretetime signals. We first define an ideal denotational semantics (time is truly continuous) for this kind of FRP, along with temporal properties, expressed in temporal logic, of signals and signal functions pertaining to change and change propag ation. Using this framework, we then show how to reason about change; specifically, we iden tify a d justify a number of possible optimisations, such as avoiding recomputation of unchanging values. Note that due to structural dynamism, and the fact that the output of a sign al function may change because time is passing even if the input is unchanging, the problem i s significantly more complex than standard change propagation in networks with static st ru ture.	agda;combinatory logic;const (computer programming);dvb-s2;dataflow;denotational semantics;emoticon;expanded memory;failure;fixed-point combinator;functional reactive programming;haskell;hybrid system;init;mathematical optimization;megabyte;product type;programming paradigm;resources, events, agents (accounting model);semiconductor fabrication plant;software propagation;splice (system call);synchronous data flow;temporal logic;undefined behavior;version control	Neil Sculthorpe;Henrik Nilsson	2010	Higher-Order and Symbolic Computation	10.1007/s10990-011-9068-x	computer science;artificial intelligence;communication;algorithm	PL	-18.992941065709193	30.791096172014708	22450
71fc12528e337ecf3679ad48be6d09fb4fa2f7bb	knowledge compilation and weighted model counting for inference in probabilistic logic programs		Over the last decade, building on advances in the areas of knowledge compilation and weighted model counting has drastically increased the scalability of inference in probabilistic logic programs. In this paper, we provide an overview of how this has been possible and point out some open challenges.	compiler;knowledge compilation;scalability	Jonas Vlasselaer;Angelika Kimmig;Anton Dries;Wannes Meert;Luc De Raedt	2016			probabilistic ctl;computer science;theoretical computer science;machine learning;data mining;probabilistic argumentation;probabilistic logic network	AI	-19.38677136687729	12.74057698073862	22464
0a9d27546db435d30d191e06f39a62212747d3e4	parameterised notions of computation	programming language;input output;parameters;computational effects;electronic computers computer science	Moggi’s Computational Monads and Power et al ’s equivalent notion of Freyd category have captured a large range of computational effects present in programming languages. Examples include non-termination, non-determinism, exceptions, continuations, side-effects and input/output. We present generalisations of both computational monads and Freyd categories, which we call parameterised monads and parameterised Freyd categories, that also capture computational effects with parameters. Examples of such are composable continuations, side-effects where the type of the state varies and input/output where the range of inputs and outputs varies. By also considering structured parameterisation, we extend the range of effects to cover separated side-effects and multiple independent streams of I/O. We also present two typed λ-calculi that soundly and completely model our categorical definitions — with and without symmetric monoidal parameterisation — and act as prototypical languages with parameterised effects.	computation;continuation;divergence (computer science);exception handling;input/output;nondeterministic algorithm;programming language;typed lambda calculus	Robert Atkey	2006	J. Funct. Program.	10.1017/S095679680900728X	input/output;computer science;theoretical computer science;programming language;parameter;algorithm	PL	-14.862421407225561	20.11998656184487	22505
dd8b52065d3486c3feb258f594718afb6b1519a4	"""denotational semantics for """"natural"""" language question-answering programs"""	denotational semantics;scott-strachey style denotational semantics;computer programmer;possible answer;small data base;language question answerers;suitable mean;simple question answerer;software engineer;semantic interpretation;question answering;natural language;software engineering	"""Scott-Strachey style denotational semantics is proposed as a suitable means of communicating the specification of """"natural"""" language question answerers to computer programmers and software engineers. The method is exemplified by a simple question answerer communicating with a small data base. This example is partly based on treatment of fragments of English by Montague. Emphasis is placed on the semantic interpretation of questions. The """"meaning"""" of a question is taken as a function from the set of universes to a set of possible answers."""	database;denotational semantics;montague grammar;natural language;programmer;question answering;semantic interpretation;software engineer	Michael G. Main;David B. Benson	1983	American Journal of Computational Linguistics			NLP	-25.40338970949506	17.96567552948232	22510
6d59ddc04b76325faefa8420252a4a6cc78650dc	the inquery retrieval system	information retrieval;power efficiency	As larger and more heterogeneous text databases become available informa tion retrieval research will depend on the development of powerful e cient and exible retrieval engines In this pa per we describe a retrieval system IN QUERY that is based on a probabilis tic retrieval model and provides support for sophisticated indexing and complex query formulation INQUERY has been used successfully with databases con taining nearly documents	database;naruto shippuden: clash of ninja revolution 3	James P. Callan;W. Bruce Croft;Stephen M. Harding	1992			document retrieval;query expansion;electrical efficiency;computer science;concept search;data mining;database;vector space model;data retrieval;information retrieval	Web+IR	-31.81767638428144	6.544801251965304	22532
09c7256312c71dfb0c1b3edd8eb3e194c37a2540	un modèle d'exécution paramétrique pour systèmes de bases de données actifs. (a parametric execution model for active database systems)		An active database system is able to execute automatically some predened actions in response to speci c events when some conditions are satis ed. Active rule, of the form Event-Condition-Action, are the core of this approach. This thesis is concerned with active systems execution models. The execution model of an active rule system describes when and how (scheduling, synchronization) rules triggered during the execution of a transaction are executed during an application. First, we propose a taxonomy and a graphic representation of active systems execution models. Then we set out a parametric execution model named Fl'are (Flexible active rule execution). An essential characteristic of the model is to consider rule modules { each module being intended to a particular use of rules. The behaviour of each rule of a module can be speci ed, and then, the execution strategy of each module. In order to do that, one just has to choose a value for each proposed parameter among a set of prede ned values. We also give a denotational (or functionnal) semantics of the model. We show that this formalism provides an implementable speci cation that we use within the framework of the experiment we are making around NAOS { an active rule system for the O2 object-oriented DBMS { in order to replace its execution engine by Fl'are.		Thierry Coupaye	1996				DB	-26.89339622521503	31.040534422794785	22537
034ee2965739e367e8cca392e1d0b73a71b71a24	symmetries in natural language syntax and semantics: the lambek-grishin calculus	kripke model;natural language;structure preservation;point of view;categorial grammar	In this paper, we explore the Lambek-Grishin calculus LG: a symmetric version of categorial grammar based on the generalizations of Lambek calculus studied in Grishin [1]. The vocabulary of LG complements the Lambek product and its left and right residuals with a dual family of type-forming operations: coproduct, left and right difference. The two families interact by means of structure-preserving distributivity principles. We present an axiomatization of LG in the style of Curry’s combinatory logic and establish its decidability. We discuss Kripke models and Curry-Howard interpretation for LG and characterize its notion of type similarity in comparison with the other categorial systems. From the linguistic point of view, we show that LG naturally accommodates non-local semantic construal and displacement — phenomena that are problematic for the original Lambek calculi.	algorithm;axiomatic system;biconnected component;categorial grammar;circuit complexity;combinatory logic;curry;curry–howard correspondence;cut (graph theory);cut rule;displacement mapping;generative grammar;interaction;iteration;kripke semantics;nl (complexity);natural language;relevance;sequent calculus;vocabulary;xfig	Michael Moortgat	2007		10.1007/978-3-540-73445-1_19	discrete mathematics;categorial grammar;mathematics;linguistics;natural language;algorithm	NLP	-10.712905188639397	11.210349136703174	22588
21c28c07525ab0e23bfe0b2eafe0307ea8c5fd91	enforcing concurrent temporal behaviors	temporal logic;false negative;program transformation;testing;nondeterminism;satisfiability;qa76 electronic computers computer science computer software;concurrency;model checking;behavior monitoring;counterexample analysis	The outcome of verifying software is often a 'counterexample', i.e., a listing of the actions and states of a behavior not satisfying the specification. In order to understand the reason for the failure it is often required to test such an execution against the actual code. In this way we also find out whether we have a genuine error or a ''false negative''. Due to nondeterminism in concurrent code, recovering an erroneous behavior on the actual program is not guaranteed even if no abstraction was made and we start the execution with the prescribed initial state. Testers are faced with a similar problem when they have to show that a suspicious scenario can actually be executed. Such a scenario may involve some intricate scheduling and thus be illusive to demonstrate. We describe here a program transformation that translates a program in such a way that it can be verified and then reverse transformed for testing a suspicious behavior. Since the transformation implies changes to the original code, we strive to minimize its effect on the original program.		Doron A. Peled;Hongyang Qu	2005	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2004.01.034	model checking;real-time computing;concurrency;temporal logic;computer science;theoretical computer science;software testing;programming language;algorithm;satisfiability	Logic	-18.265363430898603	28.83857291366991	22590
80981bf52097845f831d7055d9042fe73f870c64	specifying non-markovian rewards in mdps using ldl on finite traces (preliminary version)		In Markov Decision Processes (MDPs), the reward obtained in a state depends on the properties of the last state and action. This state dependency makes it difficult to reward more interesting long-term behaviors, such as always closing a door after it has been opened, or providing coffee only following a request. Extending MDPs to handle such nonMarkovian reward function was the subject of two previous lines of work, both using variants of LTL to specify the reward function and then compiling the new model back into a Markovian model. Building upon recent progress in the theories of temporal logics over finite traces, we adopt LDLf for specifying non-Markovian rewards and provide an elegant automata construction for building a Markovian model, which extends that of previous work and offers strong minimality and compositionality guarantees.	automata construction;closing (morphology);compiler;digital footprint;markov chain;markov decision process;reinforcement learning;tracing (software)	Ronen I. Brafman;Giuseppe De Giacomo;Fabio Patrizi	2017	CoRR		artificial intelligence;computer science;machine learning;principle of compositionality;automata construction;markov process;markov decision process	AI	-10.150280426144127	23.217461987931436	22636
662a5a47d1ece4d73be20e7e78e5a596613afe74	power and limits of structural display rules	structural rules;display theorem;proof theory;display calculus	What can (and cannot) be expressed by structural display rules? Given a display calculus, we present a systematic procedure for transforming axioms into structural rules. The conditions for the procedure are given in terms of (purely syntactic) abstract properties of the base calculus; thus, the method applies to large classes of calculi and logics. If the calculus satisfies certain additional properties, we prove the converse direction, thus characterising the class of axioms that can be captured by structural display rules. Determining if an axiom belongs to this class or not is shown to be decidable. Applied to the display calculus for tense logic, we obtain a new proof of Kracht’s Display Theorem I.	acm transactions on computational logic;ackermann function;algebraic equation;algorithm;atomic formula;calculus of constructions;categorial grammar;directed acyclic graph;expressive power (computer science);formal system;hilbert system;intuitionistic logic;linear algebra;modal logic;peano axioms;primitive recursive function;provability logic;residuated lattice;sahlqvist formula;schematic;semiconductor industry;sequent calculus;temporal logic;theory;universal quantification;word lists by frequency	Agata Ciabattoni;Revantha Ramanayake	2016	ACM Trans. Comput. Log.	10.1145/2874775	discrete mathematics;differentiation rules;computer science;time-scale calculus;proof theory;mathematics;proof calculus;programming language;natural deduction;algorithm	PL	-11.346634661926108	14.921669495098552	22641
c9074e437147597dcda2b5deed9e321a714f8857	automated proof construction in type theory using resolution	automatic proving;deduction automatique;lambda calculus;demostracion automatica;logical programming;higher order;program optimization;theorem proving;demonstration automatique;demonstration theoreme;theorem prover;programmation logique;type theory;interactive proofs;lambda calculo;optimisation programme;demostracion teorema;lambda calcul;programacion logica;automatic deduction;correctness proof;optimizacion programa	We provide techniques to integrate resolution logic with equality in type theory. The results may be rendered as follows. - A clausification procedure in type theory, equipped with a correctness proof, all encoded using higher-order primitive recursion. - A novel representation of clauses in minimal logic such that the A-representation of resolution proofs is linear in the size of the premisses. - A translation of resolution proofs into lambda terms, yielding a verification procedure for those proofs. - The power of resolution theorem provers becomes available in interactive proof construction systems based on type theory.	type theory	Marc Bezem;Dimitri Hendriks;Hans de Nivelle	2000		10.1007/10721959_10	discrete mathematics;computer science;automated proof checking;lambda calculus;proof theory;mathematics;automated theorem proving;programming language;structural proof theory;proof complexity;algorithm	Theory	-18.25696544567494	20.45950479368495	22678
54e76d753ab84c92d3059babcb13607dcc23a055	towards a unified solution: data record region detection and segmentation	rst structure;web pages;tree edit distance;edit distance;web data record extraction;group dynamic;information integration;web information integration;string edit distance	Although the task of data record extraction from Web pages has been studied extensively, yet it fails to handle many pages due to their complexity in format or layout. In this paper, we propose a unified method to tackle this task by addressing several key issues in a uniform manner. A new search structure, named as Record Segmentation Tree (RST), is designed, and several efficient search pruning strategies on the RST structure are proposed to identify the records in a given Web page. Another characteristic of our method which is significantly different from previous works is that it can effectively handle complicated and challenging data record regions. It is achieved by generating subtree groups dynamically from the RST structure during the search process. Furthermore, instead of using string edit distance or tree edit distance, we propose a token-based edit distance which takes each DOM node as a basic unit in the cost calculation. Extensive experiments are conducted on four data sets, including flat, nested, and intertwine records. The experimental results demonstrate that our method achieves higher accuracy compared with three state-of-the-art methods.	experiment;graph edit distance;intel matrix raid;row (database);tree (data structure);web page	Lidong Bing;Wai Lam;Yuan Gu	2011		10.1145/2063576.2063761	edit distance;computer science;artificial intelligence;wagner–fischer algorithm;information integration;machine learning;web page;data mining;database;string-to-string correction problem;world wide web;information retrieval;group dynamics	DB	-30.972524467018744	4.713463734676985	22698
aaf72cd33ef5f2dd9ff3297c8212c44f8bde9718	efficient encodings from csp into sat, and from maxcsp into maxsat	articulo;sat;csp;maxsat;encoding;maxcsp		boolean satisfiability problem	Josep Argelich;Alba Cabiscol;Inês Lynce;Felip Manyà	2012	Multiple-Valued Logic and Soft Computing		computer science;maximum satisfiability problem;communicating sequential processes;encoding	AI	-17.674989633401257	15.343864316279792	22737
81320758cd47d922e455a8966480f3033a3bc0fd	guest editorial	intelligent agents;general	This special issue brings together a set of papers that address some of the areas of intersection between database technology and the World Wide Web. This intersection is fast becoming a key focus of database research, with multiple facets that touch many different aspects of database systems. As semistructured data and XML become prevalent, data models and query languages for them must be proposed, studied, and implemented. As database systems become key components in the construction of large web sites, new sets of requirements must be incorporated. Query optimization in the Web environment poses new challenges and requires new techniques. The five papers in this issue were selected from among the fourteen that were submitted in response to our call for papers. Each paper was reviewed, and for each them two rounds of reviews were made possible by the enthusiastic cooperation provided by our reviewers. The first paper in this issue, “Semantic caching of web queries”, by Boris Chidlovskii and Uwe M. Borghoff tackles performance problems of “web queries” (intended as queries asked for by means of meta-searchers, which return the tuples that satisfy a given Boolean condition). More precisely, the paper considers conjunctive queries and studies how “semantic caching” can be useful. The cache maintains regions, and when a query is issued, part of the result is found in the cache and only the “remainder” is retrieved over the net. The paper “Learning response time for WebSources using query feedback and application in query optimization” by Jean-Robert Gruser, Louiqa Raschid, Vladimir Zadorozhny and Tao Zhan also considers performance issues, but from a different point of view. The goal is to predict, by means of learning techniques, the response time of a Web source. It also shows how the predictions can be used by a scrambling optimizer, that is, an optimizer that changes a query execution plan. Mary Fernandez, Daniela Florescu, Alon Levy and Dan Suciu in the paper “Declarative specifcation of web sites with Strudel”, provide a comprehensive description of Strudel, a system for implementing “data-intensive” web sites. Major features of the system are the distinction of three levels (data, structure, and presentation) and the use of a declarative query language for the specification of the structure of sites. The paper also contains an a posteriori discussion of some experiences and brief indications about three “successors” of Strudel. In the paper “Analysing navigation behaviour in web sites integrating multiple information systems”, Bettina Berendt and Myra Spiliopoulou discuss how the navigation behavior of web users can be analyzed to evaluate the quality of a site (with specific reference to sites that dynamically generate pages in response to queries specified by filling forms). The analysis is based on the WUM environment, proposed by one of the authors in a previous paper, and is demonstrated with an experiment on a large real site. In the final paper of the issue “UnQL: a query language and algebra for semistructured data based on structural recursion”, Peter Buneman, Mary Fernandez and Dan Suciu present the full details of a language for semistructured data and XML. The paper is theoretical in nature, but the language has a strong motivation regarding the current interest in semistructured data and XML. Various formal results are shown, some of which could find application in the implementation of query languages for XML. We thank all the authors who submitted papers to this special issue, as well as the anonymous referees for their disinterested and essential contribution.	cpu cache;cache (computing);conjunctive query;data model;data structure;data-intensive computing;database;david levy (chess player);declarative programming;information system;jean;mathematical optimization;myra wilson;query language;query optimization;query plan;recursion;requirement;response time (technology);structural induction;tao framework;web 2.0;world wide web;xml	Paolo Atzeni;Alberto O. Mendelzon	2000	The VLDB Journal	10.1007/s007780050079		DB	-30.630910796587333	6.567649223642956	22749
010950e846df26d3dea6e6e09b8eb958f99be183	decidable reasoning in a modified situation calculus	semantic web service;context dependent;situation calculus;first order logic;knowledge base	We consider a modified version of the situation calculus built using a two-variable fragment of the first-order logic extended with counting quantifiers. We mention several additional groups of axioms that can be introduced to capture taxonomic reasoning. We show that the regression operator in this framework can be defined similarly to regression in the Reiter’s version of the situation calculus. Using this new regression operator, we show that the projection and executability problems are decidable in the modified version even if an initial knowledge base is incomplete and open. For an incomplete knowledge base and for context-dependent actions, we consider a type of progression that is sound with respect to the classical progression. We show that the new knowledge base resulting after our progression is definable in our modified situation calculus if one allows actions with local effects only. We mention possible applications to formalization of Semantic Web services.	color gradient;context-sensitive language;first-order logic;first-order predicate;knowledge base;semantic web service;situation calculus;two-variable logic	Yilan Gu;Mikhail Soutchanski	2007			fluent calculus;knowledge base;discrete mathematics;computer science;artificial intelligence;context-dependent memory;first-order logic;mathematics;situation calculus;natural deduction;algorithm	AI	-16.105220762250106	9.774918768536896	22817
581f88cc3ceab34dedd85e5dd84deaf77c34fccb	a composition approach to time analysis of first order lazy functional programs	functional programming;first order;timing analysis	We consider operational semantics of contexts (terms with holes) in the setting of lazy functional languages, with the aim of providing a balance between operational and compositional reasoning, and a framework for semantics-based program analysis and manipulation. Introduction In this note we initiate a new direction in the semantics of functional programs. The approach is based on operational semantics; our aims are to provide a operational route to high-level semantic issues, such as program analysis and source-to-source transformation. We investigate the idea of giving a direct operational semantics to program contexts—that is, “incomplete” programs containing a number of holes in the place of some subexpressions. The idea of providing an operational semantics for contexts has been studied by Larsen (et al) for process algebras [Lar86][LX91]. In that setting, a context is viewed as an action transducer, which consumes actions provided by its internal processes (the holes) and produces externally observable actions. The operational semantics of contexts contains transitions of the form C b a C which is interpreted as: by consuming action a, context C can produce action b and change into C. We describe some initial steps towards providing an operational semantics for contexts in a functional setting. Functional Action-Transducers In the process setting a context is viewed as an action transducer. What is the corresponding notion for a context in the functional setting? In a functional language, the role of an “action” is played by the observables of the language: namely a lazy data constructor— cons, true, “λ”. We take a bold step, and demand that the “actions” should themselves be contexts—but not arbitrary contexts. They should be contexts built from the observables of the language. We will call these observable contexts. Observable contexts will be ranged-over by O, O, etc. For some context C containing occurrences of a single hole, if we have a transduction of the form: C O ′ O C Universitetsparken 1, 2100 København Ø, DENMARK. e-mail: dave@diku.dk	algebraic data type;audio feedback;email;functional programming;high- and low-level;lazy evaluation;observable;operational semantics;process calculus;program analysis;source transformation;transducer;transduction (machine learning)	Bror Bjerner;S. Holmström	1989		10.1145/99370.99382	reactive programming;computer science;theoretical computer science;first-order logic;programming language;functional programming;static timing analysis;algorithm	PL	-16.64664914764311	21.046891710770737	22824
771cb22ca1c64a371bedde9492762855bc4cf19c	a per model of secure information flow in sequential programs	lenguaje programacion;security properties;formal specification;programming language;sequential program;logical programming;secure information flow;programme sequentiel;analisis programa;specification formelle;especificacion formal;equivalence relation;programmation logique;langage programmation;formal specication;binding time analysis;program analysis;higher order functions;analyse programme;analisis semantico;analyse semantique;programacion logica;semantic analysis	"""This paper proposes an extensional semantics-based formal specification of secure information-flow properties in sequential programs based on representing degrees of security by partial equivalence relations (pers). The specification clarifies and unifies a number of specific correctness arguments in the literature, and connections to other forms of program analysis. The approach is inspired by (and equivalent to) the use of partial equivalence relations in specifying binding-time analysis, and is thus able to specify security properties of higher-order functions and """"partially confidential data"""". We extend the approach to handle nondeterminism by using powerdomain semantics and show how probabilistic security properties can be formalised by using probabilistic powerdomain semantics."""	information flow	Andrei Sabelfeld;David Sands	1999		10.1007/3-540-49099-X_4	program analysis;computer science;artificial intelligence;theoretical computer science;formal specification;database;mathematics;equivalence relation;programming language;higher-order function;algorithm	PL	-22.83733781185975	29.56960074403821	22852
7fe053387f2e8d90b6495b83e04ef8a0d0a69b47	process representation using transaction logic		Representing and answering the queries about the dynamic behavior of processes in knowledge base systems has become a challenging research area in the field of logic programming and knowledge representation systems. In this report, we are going to show how transaction logic can be used to efficiently represent dynamic behavior embedded in different domains. The ability of properly representing state changes in transaction logic enables us to express dynamic behavior of processes in different domains. The use of transaction logic to represent dynamic behavior decreases the size of knowledge bases and the query response time in comparison with other existing approaches. The efficiency of our method along with other features of transaction logic and its theoretical basis makes it an appropriate approach to represent dynamic behavior of processes in various domains.	automated theorem proving;domain-specific language;encode;embedded system;entity;first-class function;knowledge base;knowledge representation and reasoning;logic programming;modeling language;process calculus;process modeling;process specification;response time (technology);scalability;transaction logic	Reza Basseda	2013			multimodal logic;database;knowledge representation and reasoning;knowledge base;transaction logic;computer science;logic programming;transaction processing system;response time;transaction processing	AI	-29.54656666590255	12.131749672186	22862
70bcba8d45e9d9bcdea98106cb53da3d01bd8ab9	multi-agent justification logic: communication and evidence elimination	evidence elimination;rational agent;dynamic epistemic logic;justification logic	This paper presents a logic combining Dynamic Epistemic Logic, a framework for reasoning about multi-agent communication, with a new multi-agent version of Justification Logic, a framework for reasoning about evidence and justification. This novel combination incorporates a new kind of multi-agent evidence elimination that cleanly meshes with the multi-agent communications from Dynamic Epistemic Logic, resulting in a system for reasoning about multi-agent communication and evidence elimination for groups of interacting rational agents.	assertion (software development);belief revision;colon classification;dynamic epistemic logic;epistemic modal logic;feedback;interaction;multi-agent system;rational agent;well-formed formula	Bryan Renne	2011	Synthese	10.1007/s11229-011-9968-7	rational agent;description logic;paraconsistent logic;epistemic modal logic;epistemology;bunched logic;non-monotonic logic;mathematics;deductive reasoning;multimodal logic;algorithm;philosophy of logic;autoepistemic logic	AI	-17.411291570334605	4.978703752934414	22878
ed2ecf1f4f3382c500d4979444107b49b00b0337	case of (quite) painless dependently typed programming: fully certified merge sort in agda		We present a full certification of merge sort in the language Agda. It features: termination warrant without explicit proof, no proof cost to ensure that the output is sorted, and a succinct proof that the output is a permutation of the input.	agda;dependent type;merge sort	Ernesto Copello;Alvaro Tasistro;Bruno Bianchi	2014		10.1007/978-3-319-11863-5_5	parallel computing;computer science;programming language;algorithm	PL	-17.3543674735218	22.92232384939478	22893
05fb415e45bc4eb470303199ddb357b64e0ec690	passive testing and application to the gsm-map protocol	computacion informatica;grupo de excelencia;active testing;conformance testing;extended finite state machine;ciencias basicas y experimentales;gsm map protocol;global system for mobile communication;passive testing;mobile application;finite state machine	Passive testing is the process of collecting traces of messages exchanged between an operating implementation and its environment, in order to verify that these traces actually belong to the language accepted by the provided finite state machine specification. In this paper, we present an extension of the existing algorithms to consider an extended finite state machine as the specification. An algorithm is also introduced to take into account the number of transitions covered. These techniques are illustrated by the application to a real protocol, the GSM (global system for mobile communication)-MAP (mobile application part). q 1999 Elsevier Science B.V. All rights reserved.	algorithm;automata theory;experiment;extended finite-state machine;mobile app;reachability;simple directmedia layer;simulation;tracing (software)	Marine Tabourier;Ana R. Cavalli	1999	Information & Software Technology	10.1016/S0950-5849(99)00039-7	embedded system;extended finite-state machine;real-time computing;simulation;computer science;conformance testing;database;finite-state machine;virtual finite-state machine	SE	-31.581215752135257	30.99725217404046	22897
3cb7e743b1ae8759999b16cfb7e9ffba8eaa0999	from declarative model to solution: scheduling scenario synthesis	scenario synthesis;scheduling;scheduling domain language;petri nets;trees (mathematics);programming;deductive programming;reachability tree;predicate transition petri net;search space;program transformations;scenario generation;declarative model;reachability analysis;scheduling problem;protocols;automata;reflection;petri net;mathematical model	This paper presents deductive programming for scheduling scenario generation. Modeling for solution is achieved through program transformations. First, declarative model for scheduling problem domain is introduced. After that model is interpreted as scheduling domain language and as predicate transition Petri net. Generated reachability tree presents search space with solutions. At the end results are discussed and analyzed.	declarative programming;petri net;problem domain;program transformation;reachability;scheduling (computing)	Bruno Blaskovic;Mirko Randic	2007	2007 9th International Conference on Telecommunications		job shop scheduling;real-time computing;dynamic priority scheduling;computer science;theoretical computer science;operating system;two-level scheduling;programming language;petri net	Robotics	-17.202409408811462	29.49326130394567	22899
f380f7d3866b6354ad5d1fb5eb6008da65c8189d	ontology-based mappings	view;tuple generating dependency;disjunctive embedded dependency;equality generating dependency;mapping;article;ontology	Data translation consists of the task of moving data from a source database to a target database. This task is usually performed by developing mappings, i.e. executable transformations from the source to the target schema. However, a richer description of the target database semantics may be available in the form of an ontology. This is typically defined as a set of views over the base tables that provides a unified conceptual view of the underlying data. We investigate how the mapping process changes when such a rich conceptualization of the target database is available. We develop a translation algorithm that automatically rewrites a mapping from the source schema to the target ontology into an equivalent mapping from the source to the target databases. Then, we show how to handle this problem when an ontology is available also for the source. Differently from previous approaches, the language we use in view definitions has the full power of non-recursive Datalog with negation. In the paper, we study the implications of adopting such an expressive language. Experiments are conducted to illustrate the trade-off between expressibility of the view language and efficiency of the chase engine used to perform the data exchange.	algorithm;conceptualization (information science);database;datalog;executable;experiment;recursion	Giansalvatore Mecca;Guillem Rull;Donatello Santoro;Ernest Teniente	2015	Data Knowl. Eng.	10.1016/j.datak.2015.07.003	computer science;ontology;data mining;database;ontology-based data integration;programming language;view;database schema;suggested upper merged ontology	DB	-26.013904748899016	9.53120588716584	22955
2d2b97cb34fa44b4d889dae16d9fa5bba2b740af	timed contract compliance under event timing uncertainty		Despite that many real-life contracts include time constraints, for instance explicitly specifying deadlines by when to perform actions, or for how long certain behaviour is prohibited, the literature formalising such notions is surprisingly sparse. Furthermore, one of the major challenges is that compliance is typically computed with respect to timed event traces with event timestamps assumed to be perfect. In this paper we present an approach for evaluating compliance under the effect of imperfect timing information, giving a semantics to analyse contract violation likelihood.	real life;sparse matrix;tracing (software)	María-Emilia Cambronero;Luis Llana;Gordon J. Pace	2017		10.3233/978-1-61499-838-9-33	data mining;computer science	AI	-12.834955389742255	29.74667847053719	22974
d0f39fadfb0811cb9e5d76f01d772a814993d583	towards robustness analysis using pvs	formal method;multiple faults model;classical fault-injection technique;numerous advantage;acl2 logic;associated verification methodology1;automated tool;register transfer;preliminary study;towards robustness analysis;dependability analysis	This work targets the use of formal methods for enhancing dependability analysis of sequential circuits described at the Register Transfer (RT) level. We consider solutions oriented towards theoremproving techniques as an alternative to classical fault-injection techniques, for analyzing the consequences of errors caused by transient faults. A preliminary study was conducted to evaluate the advantages of a highly automated tool like ACL2 in that context. However, this study showed that, in spite of its numerous advantages, the ACL2 logic is not always expressive enough to deal with the properties under consideration here. In this paper, we briefly explain the shortcomings of ACL2 relatively to our problem, and we investigate the application of PVS, thus enabling to improve our simple and multiple faults models and the associated verification methodology1.	acl2;correctness (computer science)	Renaud Clavel;Laurence Pierre;Régis Leveugle	2011		10.1007/978-3-642-22863-6_8	real-time computing;simulation;computer science;algorithm	NLP	-14.099941355617428	29.01024286104672	23034
806b703aa6ec17d16e785a6903348b2df188c274	formal modeling and analysis of timed systems		ion of Probabilistic Systems (Invited Talk) . . . . . . . . . . . . . . . . . . . 1 Joost-Pieter Katoen From Analysis to Design (Invited Talk) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Bruce H. Krogh Efficient On-the-Fly Algorithms for Partially Observable Timed Games (Invited Talk) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Franck Cassez Undecidability of Universality for Timed Automata with Minimal Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 Sara Adams, Joël Ouaknine, and James Worrell On Timed Models of Gene Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 Grégory Batt, Ramzi Ben Salah, and Oded Maler Costs Are Expensive! . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 Patricia Bouyer and Nicolas Markey Hypervolume Approximation in Timed Automata Model Checking . . . . . 69 Vı́ctor Braberman, Jorge Lucángeli Obes, Alfredo Olivero, and Fernando Schapachnik Counter-Free Input-Determined Timed Automata . . . . . . . . . . . . . . . . . . . . 82 Fabrice Chevalier, Deepak D’Souza, and Pavithra Prabhakar Towards Budgeting in Real-Time Calculus: Deferrable Servers . . . . . . . . . 98 Pieter J.L. Cuijpers and Reinder J. Bril Automatic Abstraction Refinement for Timed Automata . . . . . . . . . . . . . 114 Henning Dierks, Sebastian Kupferschmid, and Kim G. Larsen Dynamical Properties of Timed Automata Revisited . . . . . . . . . . . . . . . . . . 130 Cătălin Dima Robust Sampling for MITL Specifications . . . . . . . . . . . . . . . . . . . . . . . . . . . 147 Georgios E. Fainekos and George J. Pappas On the Expressiveness of MTL Variants over Dense Time . . . . . . . . . . . . . 163 Carlo Alberto Furia and Matteo Rossi Quantitative Model Checking Revisited: Neither Decidable Nor Approximable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179 Sergio Giro and Pedro R. D’Argenio	algorithm;approximation;audio feedback;dima;dynamical system;gene regulatory network;model checking;observable;real-time transcription;sara (computer);sergio verdú;timed automaton;universality probability	Takeo Kanade	2013		10.1007/978-3-642-40229-6	formal verification	Logic	-20.43648738153367	18.723512495213953	23077
0ed81dbaa5d60151b8b13b5c6afe65a39d890046	proving ground confluence of equational specifications modulo axioms		Terminating functional programs should be deterministic, i.e., should evaluate to a unique result, regardless of the evaluation order. For equational functional programs such determinism is exactly captured by the ground confluence property. For terminating equations this is equivalent to ground local confluence, which follows from local confluence. Checking local confluence by computing critical pairs is the standard way to check ground confluence. The problem is that some perfectly reasonable equational programs are not locally confluent and it can be very hard or even impossible to make them so by adding more equations. We propose a three-step strategy to prove that an equational program as is is ground confluent: First: apply the strategy proposed in [8] to use non-joinable critical pairs as completion hints to either achieve local confluence or reduce the number of critical pairs. Second: use the inductive inference system proposed in this paper to prove the remaining critical pairs ground joinable. Third: to show ground confluence of the original specification, prove also ground joinable the equations added. These methods apply to order-sorted and possibly conditional equational programs modulo axioms such as, e.g., Maude functional modules.	confluence (abstract rewriting);critical pair (logic);divergence (computer science);inductive reasoning;inference engine;maude system;modulo operation;newman's lemma;peano axioms	Francisco Durán;José Meseguer;Camilo Rocha	2018		10.1007/978-3-319-99840-4_11	determinism;inductive reasoning;discrete mathematics;axiom;modulo;normalization property;confluence;computer science	Logic	-17.121423662527803	22.065033083829686	23147
c00e9067ca15a56138c166e567a4a6ffbcc73467	networks of hybrid systems: connections faults modelling and detection	fault detection;hybrid system	This work aims at presenting a new framework for detection of connection faults in networks of hybrid systems. The considered faults are losses of connections which are represented through automata. The hybrid systems which are the nodes of the network are modelled using a qualitative representation through stochastic automata. A distributed fault detection policy for the considered faults is proposed.	hybrid system	Marta Capiluppi;Manfred Morari	2007		10.1007/978-3-540-71493-4_55	real-time computing;computer science;theoretical computer science;distributed computing	Logic	-5.973449684779762	28.31286907676248	23149
1d44f31b8e41c42e69467e92da5b7f9d17a4d47b	what is an inference rule?	formal logic;inference mechanisms;classical propositional logic;complexity;first-order logic;inference rule;nonstandard propositional logic;propositional modal logics;semantic framework;truth inference;validity inference	What is an inference rule? This question does not have a unique answer. One usually nds two distinct standard answers in the literature: validity inference (` v ' if for every substitution , the validity of ] entails the validity of ']), and truth inference (` t ' if for every substitution , the truth of ] entails the truth of ']). In this paper we introduce a general semantic framework that allows us to investigate the notion of inference more carefully. Validity inference and truth inference are in some sense the extremal points in our framework. We investigate the relationship between various types of inference in our general framework, and consider the complexity of deciding if an inference rule is sound, in the context of a number of logics of interest: classical propositional logic, a nonstandard propositional logic, various propositional modal logics, and rst-order logic.	complexity;first-order logic;modal logic;propositional calculus	Ronald Fagin;Joseph Y. Halpern;Moshe Y. Vardi	1990				AI	-14.303891938092915	8.067250517618191	23181
8a7c200220caa2302570bcca7492fdac15cc77f9	parameterized verification of a cache coherence protocol: safety and liveness	protocole transmission;securite;system modeling;cache memory;program verification;vivacite;antememoria;protocolo transmision;antememoire;verificacion programa;cache coherence protocol;weak second order logic of one successor function;safety;liveness;verification programme;finite state processes;seguridad;logique ws1s;transmission protocol	In a previous paper we presented a method which allows to compute abstractions for parameterized systems modeled in the decidable logic WS1S. These WS1S systems provide an intuitive way to describe parameterized systems of finite state processes. The abstractions can be used to establish properties of the parameterized network. To be able to prove liveness properties, an algorithm is used which enriches the abstract system with fairness constraints. We summarize this verification method and present its application by the verification of both safety and liveness properties of a non-trivial example of a cache coherence protocol, provided by Steve German.	cache coherence;liveness	Kai Baukus;Yassine Lakhnech;Karsten Stahl	2002		10.1007/3-540-47813-2_22	real-time computing;systems modeling;cpu cache;computer science;theoretical computer science;operating system;database;distributed computing;programming language;computer security;algorithm;liveness	Logic	-11.184542648653714	26.30610659303054	23220
3367eec54ec90734a5015efee3d24a87b1310c7c	institutions for ocl-like expression languages		An institution for the “Object Constraint Language” (OCL) is described. First, a general framework for building institutions for OCL-like languages is defined. This framework is instantiated for a substantial subset of OCL.	object constraint language	Alexander Knapp;María Victoria Cengarle	2015		10.1007/978-3-319-15545-6_14	uml tool;computer science;applications of uml;programming language;node;algorithm;object constraint language	PL	-25.989385858599473	20.795465348835524	23229
d924fd26e541ec5af4898fd0de3939a93f45a3ed	strictly primitive recursive realizability, ii. completeness with respect to iterated reflection and a primitive recursive ω-rule	reflection principle		iterated function;primitive recursive function;recursion (computer science)	Zlatan Damnjanovic	1998	Notre Dame Journal of Formal Logic	10.1305/ndjfl/1039182252	combinatorics;discrete mathematics;topology;primitive recursive function;mathematics;reflection principle;μ-recursive function;primitive recursive arithmetic;primitive element;μ operator	Logic	-5.428401930655956	18.608145781409476	23236
a9d2681eb336ae34697ff6f44d7c6e05ac6ff6b6	terminating tableau calculi for hybrid logics extending k	tense logic;loop checks;decision procedure;hybrid logic;decision procedures;tableau systems	This article builds on work by Bolander and Blackburn [7] on terminating tableau systems for the minimal hybrid logic K. We provide (for the basic uni-modal hybrid language) terminating tableau systems for a number of non-transitive hybrid logics extending K, such as the logic of irreflexive frames, antisymmetric frames, and so on; these systems don’t employ loop-checks. We also provide (for hybrid tense logic enriched with the universal modality) a terminating tableau calculus for the logic of transitive frames; this system makes use of loop-checks.	divergence (computer science);finite model property;frame language;hoare logic;hybrid logic;infinite impulse response;method of analytic tableaux;modal logic;modality (human–computer interaction);rewriting;source-to-source compiler;temporal logic;transitive reduction;vertex-transitive graph	Thomas Bolander;Patrick Blackburn	2009	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2009.02.027	discrete mathematics;description logic;many-valued logic;intermediate logic;mathematics;algorithm	Logic	-13.329326314400694	13.246975767049005	23239
83ff41350d592f8b59e9288b1c130490a1ab7175	optimizing a certified proof checker for a large-scale computer-generated proof		In recent work, we formalized the theory of optimal-size sorting networks with the goal of extracting a verified checker for the largescale computer-generated proof that 25 comparisons are optimal when sorting 9 inputs, which required more than a decade of CPU time and produced 27 GB of proof witnesses. The checker uses an untrusted oracle based on these witnesses and is able to verify the smaller case of 8 inputs within a couple of days, but it did not scale to the full proof for 9 inputs. In this paper, we describe several non-trivial optimizations of the algorithm in the checker, obtained by appropriately changing the formalization and capitalizing on the symbiosis with an adequate implementation of the oracle. We provide experimental evidence of orders of magnitude improvements to both runtime and memory footprint for 8 inputs, and actually manage to check the full proof for 9 inputs.	algorithm;automated proof checking;blitzkrieg;central processing unit;comparator;computation;computer-generated holography;haskell;mathematical optimization;memory footprint;norm (social);online and offline;optimizing compiler;oracle database;sorting network	Luís Cruz-Filipe;Peter Schneider-Kamp	2015		10.1007/978-3-319-20615-8_4	computer science;theoretical computer science;distributed computing;programming language;algorithm	PL	-16.19705895293173	25.830723107115038	23284
185fd2499aa2e80eb2cb39d49fa6cddaed3d5d8c	reasoning in inconsistent knowledge bases	reasoning with inconsistency;sistema experto;data integrity;nonmonotonic negation;circonscription;operational semantics;semantics;base connaissance;negation;logical programming;indexing terms;raisonnement;semantica;semantique;circumscription;self consistent;deductive database;logic programming;nonmonotonic reasoning;programmation logique;base dato deductiva;razonamiento;nonmonotonic negation inconsistent knowledge bases reasoning operational semantics declarative semantics logic programming deductive databases;base conocimiento;base donnee deductive;negacion;systeme expert;logic programs;reasoning;nonmonotonic reasoning deductive databases data integrity logic programming;programacion logica;inconsistency;expert systems usa councils logic programming deductive databases educational institutions electronic mail humans computer science military computing banking;circonscripcion;deductive databases;knowledge base;expert system	"""Databases and knowledge bases could be inconsistent in many ways. For example, during the construction of an expert system, we may consult many different experts. Each expert may provide us with a group of rules and facts which are self-consistent. However, when we coalesce the facts and rules provided by these different experts, inconsistency may arise. Alternatively, knowledge bases may be inconsistent due to the presence of some erroneous information. Thus, a framework for reasoning about knowledge bases that contain inconsistent information is necessary. However, existing frameworks for reasoning with inconsistency do not support reasoning by cases and reasoning with the law of excluded middle (""""everything is either true or false""""). In this paper, we show how reasoning with cases, and reasoning with the law of excluded middle may be captured. We develop a declarative and operational semantics for knowledge bases that are possibly inconsistent. We compare and contrast our work with work on explicit and non-monotonic modes of negation in logic programs and suggest under what circumstances one framework may be preferred over another. >"""		John Grant;V. S. Subrahmanian	1995	IEEE Trans. Knowl. Data Eng.	10.1109/69.368510	case-based reasoning;knowledge base;analytic reasoning;index term;computer science;artificial intelligence;non-monotonic logic;negation;data integrity;data mining;database;semantics;deductive reasoning;logic programming;operational semantics;expert system;circumscription;reason;algorithm	DB	-18.908986860615872	10.072410852194176	23300
90149244448c9e696b8b12ae44437e7c44b521e0	towards programming languages for machine learning and data mining	pattern set mining;data mining;towards programming language;constraint-based itemset;data mining component;constraint satisfaction;constraint programming modeling principle;programming language;optimization problem;data mining problem;high-level modeling language	Today there is only little support for developing software that incorporates a machine learning or a data mining component. To alleviate this situation, we propose to develop programming languages for machine learning and data mining. We also argue that such languages should be declarative and should be based on constraint programming modeling principles. In this way, one could declaratively specify the prob- lem of machine learning or data mining problem of interest in a high-level modeling language and then translate it into a constraint satisfaction or optimization problem, which could then be solved using particular solvers. These ideas are illustrated on problems of constraint-based item- set and pattern set mining.		Luc De Raedt;Siegfried Nijssen	2011		10.1007/978-3-642-21916-0_3	constraint programming;constraint satisfaction;computer science;artificial intelligence;third-generation programming language;machine learning;data mining;database;inductive programming;fifth-generation programming language;programming language;second-generation programming language;comparison of multi-paradigm programming languages;algorithm	ML	-23.03735359156017	12.3231592204925	23323
5e80669ff1ae9a2df40b7e4952ed588330365789	generalized page replacement algorithms in a relational data base	relational data;abstraction;data base;simulation experiment;integrity;sub schema;privacy	This paper describes a generalized page replacement problem in which the pages have variable sizes and the cost of a page fault is a function of the particular page referenced. In such an environment the conventional page replacement algorithms are found to perform inadequately.  New algorithms are proposed for reducing the cost incurred because of page faults in response to a series of references. Simulation experiments have been run in order to compare the cost performance of these algorithms with standard techniques, and also with the minimum achievable cost. The latter is determined by means of a novel tree-pruning algorithm.  One practical environment in which the general problem may arise is a relational data base having an implied relations facility, in which some relations are maintained in definition form until queried. The implied relation is analogous to a page, and the processing time for restructuring a relation from its definition varies from one relation to another. An efficient replacement algorithm is needed to manage the process as the implied relations alternate between the state of definitions and the state of explicit representation.	experiment;page fault;page replacement algorithm;relational database management system;simulation	Richard G. Casey;I. M. Osman	1974		10.1145/800296.811507	relational database;computer science;data mining;database;abstraction;programming language;privacy;algorithm	DB	-27.96919724229842	4.384204294412423	23373
859c0ceb95f7642d415ff774d7ed206933d33def	inheritance in object-oriented knowledge representation		This paper contains the consideration of inheritance mechanism in such knowledge representation models as object-oriented programming, frames and object-oriented dynamic networks. In addition, inheritance within representation of vague and imprecise knowledge are also discussed. New types of inheritance, general classification of all known inheritance types and approach, which allows avoiding in many cases problems with exceptions, redundancy and ambiguity within objectoriented dynamic networks and their fuzzy extension, are introduced in the paper. The proposed approach bases on conception of homogeneous and inhomogeneous or heterogeneous class of objects, which allow building of inheritance hierarchy more flexibly and efficiently.	framing (world wide web);knowledge representation and reasoning;vagueness	Dmytro Terletskyi	2015		10.1007/978-3-319-24770-0_26	discrete mathematics;artificial intelligence;mathematics;algorithm	PL	-30.674048041385866	13.847873380290103	23400
aa5db8196097dd97035875a94a1e5edaf4e47699	special issue on structural operational semantics	structural operational semantics;special issue	This special volume of Information and Computation aims at documenting state-of-the-art research, new developments, and directions for future investigation in the field of structural operational semantics (SOS). It includes papers about the SOS of concurrent processes, addressing ordered SOS, simulation up-to, mobility, name-passing, and congruence formats. It also contains papers about the SOS of programming languages, focussing on bialgebraic semantics, co-inductive and bi-inductive forms of big-step SOS, the relationship to rewriting logic semantics, and the use of logical relations. Since its introduction by Robin Milner [1] and Gordon Plotkin [2] around 1980, SOS has become one of the main semantic description frameworks. It has been used to describe the static and dynamic semantics of numerous programming and concurrent process languages, and some non-executable specification languages. Its most characteristic feature is the use of axioms and inference rules to define transition relations, which are usually structural in the sense that transitions for compound phrases are inferred from transitions for their sub-phrases. SOS has considerable intuitive appeal and flexibility, and is based on simple and familiar mathematical foundations. At the same time, it is remarkably powerful. For instance, when a concurrent process language is defined by an SOS whose rules are in a restricted format, various useful properties of the language, such as compositionality of its operators w.r.t. a given semantic equivalence, are ensured. were published in a special issue of Theoretical Computer Science [4].) However, papers that were not presented at these workshops were equally welcome, and all submissions have been refereed and subjected to the same quality criteria, meeting the standards of Information and Computation. Eleven papers have been selected out of a total of seventeen submissions:	axiomatic system;executable;information and computation;logical relations;operational semantics;parallel computing;plotkin bound;programming language;rewriting;simulation;software documentation;specification language;theoretical computer science;turing completeness;zeller's congruence	Rob J. van Glabbeek;Peter D. Mosses	2009	Inf. Comput.	10.1016/j.ic.2008.10.006	computer science;artificial intelligence;algorithm	Logic	-12.281351968493809	19.54470676439732	23412
61e362db877e0d5656d1715bd18955bd25634c0b	intelligent control using integrity constraints	intelligent control;integrity constraints	This paper describes how integrity constraints, whether user supplied or automatically generated during the search, and analysis of failures can be used to improve the execution of function free logic programs. Integrity constraints are used to guide both the forward and backward execution of the Programs. This work applies to arbitrary node and literal selection functions and is thus transparent to the fact whether the logic program is executed SeWentiallY or in parallel.	data integrity;intelligent control;literal (computer programming);logic programming;ptc integrity	Madhur Kohli;Jack Minker	1983			real-time computing;computer science;artificial intelligence;data integrity;database;distributed computing	DB	-22.06588530375706	17.448670078090153	23464
618e6a0bd7d39a893e5f3a4c45fde95f74e37d0c	rule formats for timed processes	operational semantics	Building on previous work [15,8], this paper describes two syntactic ways of defining ‘well-behaved’ operational semantics for timed processes. In both cases, the semantic rules are derived from abstract operational rules for behaviour comonads and thus ensure congruence results. The first of them, a light-weight attempt using schematic rules, is shown to be sound, i.e., to indeed induce abstract rules as introduced in [8]. Then a second format, based on a new and very general kind of abstract rules, comonadic SOS (CSOS), is presented which uses meta rules and is also complete, i.e., it characterises all possible CSOS rules for timed processes.		Marco Kick	2002	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(04)80498-5	discrete mathematics;semantics of business vocabulary and business rules;computer science;theoretical computer science;mathematics;programming language;operational semantics;algorithm	Logic	-25.525383657248938	19.011751099019172	23542
284a5c145f8213a186d786678ed624feebdf6e54	querying and controlling the future behaviour of complex objects	complex objects;query language;optimisation;database management systems;query languages database management systems database theory object oriented programming optimisation;production system;object oriented programming;satisfiability;behavior satisfying user defined goals complex objects complexly structured systems futuristic query language query optimisation optimal control;query languages;complex system;production systems database languages query processing flexible manufacturing systems optimal control relational databases encoding manufacturing industries computer science database systems;database theory;optimal control problem	The complex system formalism is utilized for describing structural and behavioral properties of complexly structured systems. A complex system is a production system that models a database with complex objects and explicitly supports time and nondeterminism. Consequently, complex systems can be used to predict the evolution of databases. To obtain predictions about future behavior, a futuristic query language is defined. A query optimisation algorithm is provided for a subset of this language. In general, complex systems do not yield unique answers to futuristic queries because of the inherent nondetermination. Therefore, an optimal control problem is formulated that finds behavior satisfying user-defined goals. Subsequently, such goals can be converted into additional system constraints, thus reducing nondeterminism and providing for the optimal system's behavior. >		Alexander Tuzhilin;Zvi M. Kedem	1989		10.1109/ICDE.1989.47247	sargable;query optimization;database theory;complex systems;query expansion;boolean conjunctive query;database tuning;data control language;computer science;query by example;theoretical computer science;data mining;database;rdf query language;programming language;view;query language	HCI	-28.753638415906725	10.987710068052928	23603
d73035186ea851d6484f201b8c0f1c600b30d609	stop it, and be stubborn!	ignoring problem;safety progress liveness properties;stubborn set ample set persistent set partial order methods	This publication discusses how automatic verification of concurrent systems can be made more efficient by focusing on always may-terminating systems. First, making a system always may-terminating is a method for meeting a modelling need that exists independently of this publication. It is illustrated that without doing so, non-progress errors may be lost. Second, state explosion is often alleviated with stubborn, ample, and persistent set methods. They use expensive cycle or terminal strong component conditions in many cases. It is proven that for many important classes of properties, if the systems are always may-terminating, then these conditions can be left out.	abstract rewriting system;concurrency (computer science);cycle (graph theory);modeling language;newman's lemma	Antti Valmari	2015	2015 15th International Conference on Application of Concurrency to System Design	10.1145/3012279	data mining;algorithm	SE	-12.890527499130926	29.736044403580877	23652
b021dbb7d7db89183c418f4e64a333e3e5c13c1d	on the data model and access method of summary data management	insertion;multidimensional access method;disjointness constraint;category;database management systems;experience;data management;average income;database management systems data structures;summary data search;data model;summary data management;computational complexity;data structures;summary;statistical function;data models database systems query processing out of order computer science programming profession bridges transaction databases;female engineer;derivability problem;access method;metaknowledge;sd tree;deletion data model access method summary data management trinary tuple statistical function category summary metaknowledge average income female engineer experience computational complexity derivability problem disjointness constraint multidimensional access method sd tree summary data search insertion;trinary tuple;deletion	A data model and an access method for summary data management are presented. Summary data, represented as a trinary tuple (statistical function, category, summary), are metaknowledge summarized by a statistical function of a category of individual information typically stored in a conventional database. For instance, (average-income, female engineer with 10 years' experience and master's degree, $45000) is a summary datum. The computational complexity of the derivability problem has been found intractable in general, and the proposed summary data model, enforcing the disjointness constraint, alleviates the intractable problem without loss of information. In order to store, manage, and access summary data, a multidimensional access method called summary data (SD) tree is proposed. By preserving the category hierarchy, the SD tree provides for efficient operations, including summary data search, derivation, insertion, and deletion. >	data model	Meng Chang Chen;Lawrence McNamee	1989	IEEE Trans. Knowl. Data Eng.	10.1109/69.43426	insertion;data structure;category;data model;data management;computer science;artificial intelligence;theoretical computer science;machine learning;data mining;database;computational complexity theory;access method;algorithm	DB	-28.856307308787358	7.507660260211694	23654
5b2509f68cb7e099166ad91fab27e29e5addeded	uniform selection of feasible paths as a stochastic constraint problem	feasible path;structural criterion;feasible paths problem;statistical structural testing;stochastic constraint program;automatic structural test data;feasible paths;stochastic constraint programming;uniform selection;testing method;stochastic constraint problem;stochastic decision problem;stochastic programming;constraint programming;decision problem;probability distribution;statistical distributions;statistical testing;test methods;software testing;structural testing	Automatic structural test data generation is a real challenge of software testing. Statistical structural testing has been proposed to address this problem. This testing method aims at building an input probability distribution to maximize the coverage of some structural criteria. Under the all paths testing objective, statistical structural testing aims at selecting each feasible path of the program with the same probability. In this paper, we propose to model a uniform selector of feasible paths as a stochastic constraint program. Stochastic constraint programming is an interesting framework which combines stochastic decision problem and constraint solving. This paper reports on the translation of uniform selection of feasible paths problem into a stochastic constraint problem. An implementation which uses the library PCC(FD) of SICStus Prolog designed for this problem is detailed. First experimentations, conducted over a few academic examples, show the interest of our approach.	constraint programming;constraint satisfaction problem;decision problem;sicstus prolog;software testing;test data generation;white-box testing	Matthieu Petit;Arnaud Gotlieb	2007	Seventh International Conference on Quality Software (QSIC 2007)	10.1109/QSIC.2007.67	constraint logic programming;probability distribution;mathematical optimization;constraint satisfaction;computer science;theoretical computer science;machine learning	SE	-26.01191054559968	16.44201240735469	23656
1750f256a36272ef9643b263f11c676fd4e2e4ea	completeness of a bytecode verifier and a certifying java-to-jvm compiler	java bytecode;certifying compilation;abstract state machine;bytecode verification;java	During an attempt to prove that the Java-to-JVM compiler generates code that is accepted by the bytecode verifier, we found examples of legal Java programs that are rejected by the verifier. We propose therefore to restrict the rules of definite assignment for the try-finally statement as well as for the labeled statement so that the example programs are no longer allowed. Then we can prove, using the framework of Abstract State Machines, that each program from the slightly restricted Java language is accepted by the Bytecode Verifier. In the proof we use a new notion of bytecode type assignment without subroutine call stacks.	abstract state machines;class implementation file;compiler;control flow;definite assignment analysis;entry point;formal verification;isabelle;java virtual machine;path (graph theory);programming language;proof assistant;structural induction;subroutine;top-down and bottom-up design	Robert F. Stärk;Joachim Schmid	2003	Journal of Automated Reasoning	10.1023/A:1025003423108	real-time computing;computer science;programming language;java;algorithm;abstract state machines	PL	-21.78148060957158	29.377866920652004	23673
82e1dde49bfa6058e55a63a6687d5438a67d2b40	on some properties of recursively enumerable equivalence relations	equivalence relation	The study of equivalence relations in a recursion theoretic framework was started by Ju. L. ERSOV (see [5] ) and later on applied to logic first by A. VISSER in [13] and then by C. BERNARDI, F. MONTAQNA, A. SoRsr and G. SOHMARU~A (see [3], [S], IlOJ, [l 11). Results of representation and classification for equivalence relations have been proved by the above mentioned authors and more recently by A. LACHLAN and A. KANDA ([6], [7]). An equivalence relation (ex.) Ron the set w natural numbers is said to be recursively enumerable (or positive) if ( R ) = ((5, y) I %By) is an r.e. set. Following CARROLL [4], we will say that R is an r.e.e.r. Here we do not want to deal with (R) in place of R, because, as we shall see below, not all recursive properties of R are reflected in < R ) .	recursion;recursively enumerable language;recursively enumerable set;theory;turing completeness	Stefano Baratella	1989	Math. Log. Q.	10.1002/malq.19890350309	equivalence class;maximal set;logical equivalence;quotient algebra;adequate equivalence relation;topology;borel equivalence relation;recursively enumerable set;congruence relation;mathematics;transitive relation;equivalence relation;recursively enumerable language;algebra	Theory	-6.175472253330247	14.356002987757915	23725
e6b022fd01b5d6213b4242ab126a03e59e8807c0	on the composition of well-structured programs	clumsy code;well-structured program;improved program;acm computing surveys;sample program;unstructured program;goto-less program;structured programming;well-structured programs;programming task;program logic;cm computing surveys	"""i A professional programmer's know-how used to consist of the mastery of a set of techniques applicable to specific problems and to some specific computer. With the increase of computer power, the programmers' tasks grew more complex, and hence the need for a systematic approach became evident. Recently, the subject of programming methods, generally applicable rules and patterns of development, received considerable attention. """"Structured programming"""" is the formulation of programs as hierarchical, nested structures of statements and objects of computation. We give brief examples of structured programs, show the essence of this approach, discuss its relationship with program verification, and comment on the role of structured languages."""	computation;formal verification;power supply unit (computer);programmer;programming language;statement (computer science);structured programming	Niklaus Wirth	1974	ACM Comput. Surv.	10.1145/356635.356639	computer science;artificial intelligence;algorithm	PL	-28.15835017764438	23.189035467074497	23726
a33c7d131e2cf73a9185312d3b27998d0ee8f3c1	refinement of kripke models for dynamics	automatic verification;labelled transition system;kripke model;communication protocol;dynamic properties	We propose a property-preserving refinement/abstraction theory for Kripke Modal Labelled Transition Systems incorporating not only state mapping but also label and proposition lumping, in order to have a compact but informative abstraction. We develop a 3-valued version of Public Announcement Logic (PAL) which has a dynamic operator that changes the model in the spirit of public broadcasting. We prove that the refinement relation on static models assures us to safely reason about any dynamic properties in terms of PAL-formulas on the abstraction of a model. The theory is in particular interesting and applicable for an epistemic setting as the example of the Muddy Children puzzle shows, especially in the view of the growing interest for epistemic modelling and (automatic) verification of communication protocols.	kripke semantics;refinement (computing);value (ethics)	Francien Dechesne;Simona Orzan;Yanjing Wang	2008		10.1007/978-3-540-85762-4_8	communications protocol;discrete mathematics;kripke structure;computer science;theoretical computer science;mathematics;kripke semantics;algorithm	Logic	-16.258041878758462	12.998353874182023	23748
0a984f5cb8ee347a711eeb0295549697322a8914	certified reasoning on real numbers and objects in co-inductive type theory. (raisonnement certifié sur les nombres réels et les objets en théorie des types co-inductifs)		In this thesis we adopt Formal Methods based on Type Theory for reasoning on the semantics of computer programs. The ultimate goal is their certification, that is, to prove that a fragment of software meets its formal specification. Application areas of our research are the representation of and computation on the Real Numbers and the Object-oriented Languages based on Objects. In our investigation, Coinductive specification and proof principles play an essential role. In the first part of the thesis we deal with Constructive Real Numbers. We construct the reals using streams, i.e. infinite sequences, of signed digits. We work with constructive logic, that is, we develop “algorithmic” mathematics. We implement the Real Numbers in Coq using streams, which are managed using coinductive judgments and corecursive algorithms. Then we introduce a constructive axiomatization and we use it for proving the adequacy of our construction. In the second part of the thesis we approach Object-based Calculi with side-effects, in order to address important properties, such as Type Soundness, formally. We state our investigation focusing on Abadi and Cardelli’s impς-calculus, which features objects, cloning, dynamic lookup, imperative method update, object types and subsumption. We reformulate impς using modern encoding techniques, as Higher-Order Abstract Syntax and Coinductive proof systems in Natural Deduction style. Then we formalize impς in Coq and we prove the Type Soundness. To my parents, Elisa, and Alessia	algorithm;axiomatic system;coinduction;computation;computer program;coq (software);corecursion;formal methods;formal specification;higher-order abstract syntax;imperative programming;inductive type;intuitionistic logic;lookup table;natural deduction;object type (object-oriented programming);side effect (computer science);stream (computing);subsumption architecture;type safety;type theory	Alberto Ciaffaglione	2003				PL	-16.104685543804035	17.70253446851925	23847
fec581bc20216467e3ebccfa9d6003c97cf438fe	error traces in model-based debugging of hardware description languages	fault localization;model specification;condition dependence;automated debugging;conditional dependency;potential influence;model based diagnosis;source level debugging;software debugging;error trace;program slicing;hardware description language	In this article we address the fault localization problem in HDLs, particularly in VHDL designs. Our approach relies on the model-based diagnosis paradigm and, unlike to other approaches that rely on the design's gate-level representation, we accurately represent the program's syntax and semantics in a debugging model. This detailed modeling approach, however, may cause scalability problems for larger designs, thus reducing the model's complexity and size is a crucial issue. Creating a debugging model specifically for a given test case in terms of its execution trace is, although tractable in terms of the model's size, uneligible for source level debugging. We illustrate this result by a simple example and relate it to similar findings in the area of program slicing. Moreover, we present a solution to this problem and discuss implications on software debugging by means of our recent empirical results.	cobham's thesis;debugging;hardware description language;program slicing;programming paradigm;scalability;test case;tracing (software);vhdl	Bernhard Peischl;Franz Wotawa	2005		10.1145/1085130.1085136	shotgun debugging;program slicing;real-time computing;computer science;theoretical computer science;algorithmic program debugging;programming language;debugging	SE	-16.39801301499562	29.129386679146194	23883
1e2e07ba4d757355702b1b035511df94767e9b1c	modular construction of complete coalgebraic logics	construccion modular;prueba;68q55;syntax;51e24;observable;consistencia semantica;variety;building block;soundness;03b45;semantics;probabilistic system;syntaxe;semantica;semantique;modal logic;preuve;proof systems;construction modulaire;informatique theorique;logique modale;probabilistic systems;consistance semantique;coalgebra;logica modal;coalgebre;completitud;modular construction;completeness;variedad;sintaxis;variete;completude;proof;computer theory;systeme preuve;informatica teorica	We present a modular approach to defining logics for a wide variety of state-based systems. The systems are modelled as coalgebras, and we use modal logics to specify their observable properties. We show that the syntax, semantics and proof systems associated to such logics can all be derived in a modular fashion. Moreover, we show that the logics thus obtained inherit soundness, completeness and expressiveness properties from their building blocks. We apply these techniques to derive sound, complete and expressive logics for a wide variety of probabilistic systems, for which no complete axiomatisation has been obtained so far.	automata theory;axiomatic system;expressive power (computer science);modal logic;modular design;observable;probabilistic automaton;proof calculus;soundness (interactive proof);t-norm fuzzy logics;viz: the computer game	Corina Cîrstea;Dirk Pattinson	2007	Theor. Comput. Sci.	10.1016/j.tcs.2007.06.002	modal logic;monoidal t-norm logic;t-norm fuzzy logics;soundness;discrete mathematics;syntax;observable;completeness;proof;mathematics;semantics;variety;algorithm	Logic	-10.840168882783669	18.652788074407813	23905
9036cbc34d2af2d2d9528251bd4bc4b9582dfb71	the inverse lambda calculus algorithm for typed first order logic lambda calculus and its application to translating english to fol	present soundness;target language;order logic lambda calculus;present algorithm;fol-lambda formula;inverse lambda calculus algorithm;input g;english sentence;appropriate knowledge representation language;translates sentence;fol-lambda formula f;formal language	In order to answer questions and solve problems that require deeper reasoning with respect to a given text, it is necessary to automatically translate English sentences to formulas in an appropriate knowledge representation language. This paper focuses on a method to translate sentences to First-Order Logic (FOL). Our approach is inspired by Montague’s use of lambda calculus formulas to represent the meanings of words and phrases. Since our target language is FOL, the meanings of words and phrases are represented as FOL-lambda formulas. In this paper we present algorithms that allow one to construct FOL-lambda formulas in an inverse manner. Given a sentence and its meaning and knowing the meaning of several words in the sentence our algorithm can be used to obtain the meaning of the other words in that sentence. In particular the two algorithms take as input two FOL-lambda formulas G and H and compute a FOL-lambda formula F such that F with input G, denoted by F@G, is H; respectively, G@F = H. We then illustrate our algorithm and present soundness, completeness and complexity results, and briefly mention the use of our algorithm in a NL Semantics system that translates sentences from English to formulas in formal languages.	algorithm;answer set programming;compiler;first-order logic;first-order predicate;formal language;ibm notes;knowledge representation and reasoning;lambda calculus;lambda cube;linear temporal logic;montague grammar;nl (complexity);natural language;stable model semantics;vladimir lifschitz	Chitta Baral;Marcos Alvarez Gonzalez;Aaron Gottesman	2012		10.1007/978-3-642-30743-0_4	natural language processing;mathematics;linguistics;algorithm	NLP	-15.9787368960735	11.16791703663566	23931
08af2a01c5d7fbd5d56951efe82774114a026580	model-checking dense-time duration calculus	verification;systeme temps reel;05bxx;logic;embedded real time systems;dense time duration calculus;duration;calcul duree;duration calculus;model checking;dense time;informatique theorique;calculus;real time system;decidibilidad;sistema tiempo real;verificacion;decidabilite;logique;logica;decidability;computer theory;informatica teorica	Since the seminal work of Zhou Chaochen, M. R. Hansen, and P. Sestoft on decidability of dense-time Duration Calculus [ZHS93] it is well known that decidable fragments of Duration Calculus can only be obtained through withdrawal of much of the interesting vocabulary of this logic. While this was formerly taken as an indication that key-press verification of implementations with respect to elaborate Duration Calculus specifications were also impossible, we show that the model property is well decidable for realistic designs which feature natural constraints on their switching dynamics. The key issue is that the classical undecidability results rely on a notion of validity of a formula that refers to a class of models which is considerably richer than the possible behaviours of actual embedded real-time systems: that of finitely variable trajectories. By analysing two suitably sparser model classes we obtain model-checking procedures for rich subsets of Duration Calculus. Together with undecidability results also obtained, this sheds light upon the exact borderline between decidability and undecidability of Duration Calculi and related logics.	duration calculus;embedded system;model checking;real-time clock;real-time computing;undecidable problem;vocabulary	Martin Fränzle	2004	Formal Aspects of Computing	10.1007/s00165-004-0032-y	decidability;model checking;duration calculus;verification;real-time operating system;computer science;calculus;duration;mathematics;logic;algorithm	Logic	-9.988906551100863	25.358423348892053	23935
8d2f461b8b8fb13bba461d02cb5650e6f3248cae	program construction and verification components based on kleene algebra		Variants of Kleene algebra support program construction and verification by algebraic reasoning. This entry provides a verification component for Hoare logic based on Kleene algebra with tests, verification components for weakest preconditions and strongest postconditions based on Kleene algebra with domain and a component for step-wise refinement based on refinement Kleene algebra with tests. In addition to these components for the partial correctness of while programs, a verification component for total correctness based on divergence Kleene algebras and one for (partial correctness) of recursive programs based on domain quantales are provided. Finally we have integrated memory models for programs with pointers and a program trace semantics into the weakest precondition component.	correctness (computer science);hoare logic;kleene algebra;linear algebra;operational semantics;postcondition;precondition;predicate transformer semantics;recursion;refinement (computing);tracing (software)	Victor B. F. Gomes;Georg Struth	2016	Archive of Formal Proofs		algorithm;software construction;kleene algebra;kleene star;mathematics	Logic	-18.946939026250885	26.37276630632917	23947
31f289ac4c5a73c9911b46840a62260811719a88	exceptional c or c with exceptions	tolerancia falta;developpement logiciel;lenguaje programacion;fault tolerant;langage c;programming language;implementation;correction erreur;sistema informatico;complejidad programa;ejecucion programa;computer system;program execution;ejecucion;c language;desarrollo logicial;execution programme;error correction;fault tolerance;software development;error handling;langage programmation;exception handling;systeme informatique;maniement exception;program complexity;correccion error;fiabilite logiciel;fiabilidad logicial;software reliability;tolerance faute;lenguaje c;complexite programme	C does not have exception handling facilities. Errors are handled by examining the value returned by each function and signals (conditions reported to the program) are handled by using library functions. These approaches lead to ad hoc error-handling techniques and can make programs hard to understand. Exceptional C, a superset of C, provides exception handling facilities. Exceptional C integrates the two techniques used by C programmers (i.e., status values and signals) to handle errors into one unified exception handling mechanism. In this paper, I review exception handling models, specify the criteria used for designing the exception handling facilities in Exceptional C, and then describe these facilities. I also illustrate the use of the exception handling facilities with examples.	exception handling;hoc (programming language);programmer	Narain H. Gehani	1992	Softw., Pract. Exper.	10.1002/spe.4380221003	exception handling;fault tolerance;real-time computing;computer science;operating system;programming language	SE	-24.05510063355969	30.821974822476886	23953
9b85104278a1dd51a8c710ed671a76c39ddc4b00	une approche générique de la réécriture	automatic proving;logicalite;demostracion automatica;logical programming;completion de knuth et bendix;demonstration automatique;lemme de newman;knuth bendix;programmation logique;rewriting systems;programacion logica;systeme reecriture;systeme de knuth et bendix;reecriture abstraite;propriete de church rosser	In this paper we study rewriting as an operational proof syst em in a generic (i.e. logic-independent) way. To do this, we propose simple condi ti s which allow to characterise logics were this kind of effective proof can be applied. This conditions are based on a proof tree property, that we call semi-commutation. We then show h o to define the standard notion of termination, confluence and local confluencec in this meta -formalism. It allows us to obtain the fundamental results of Church-Rosser and Newman and to p ropose a generic Knuth and Bendix completion method. MOTS-CLÉS: réécriture abstraite, logicalité, propriété de Church-R osser, lemme de Newman, système de Knuth et Bendix, complétion de Knuth et Bendix	aronszajn tree;church–rosser theorem;confluence;knuth–bendix completion algorithm;linear algebra;rewriting;samuel newman;semiconductor industry;termination analysis	Marc Aiguier;Diane Bahrami	2003	Technique et Science Informatiques	10.3166/tsi.22.401-433	algorithm	Logic	-10.658774549110204	19.21741984310638	23975
42fd38f9b421d7caa0f4dfa6ea5e9570a4b5f4fa	proving group isomorphism theorems (extended abstract)	proving group isomorphism theorems;extended abstract	We report the first computer proof of the three isomorphism theorems in group theory. The first theorem, the easiest of the three, was considered by Larry Wos as one of challenging problems for theorem provers. The technique we used is conditional completion which consists of one simplification rule called contextual rewriting and one inference rule called clausal superposition. Conditional completion works on conditional equations made from clauses and is a powerful method for clause-based theorem proving with equality.		Hantao Zhang	1992		10.1007/3-540-56393-8_22	group isomorphism;isomorphism;isomorphism extension theorem	Theory	-10.88963442806274	16.755164235820285	24011
c114b19f1bc2e9a60c1cc92852a3f402d80913bd	on the comparison of theories: preferring the most specific explanation	default reasoning;computational mechanics;formal model;first order;default logic	"""Default reasoning is reasoning with generalised knowledge which we want to use if there is no more specific knowledge also applicable. This paper presents a formal, model-theoretic characterisation of default reasoning. Defaults are treated as possible hypotheses in a """"scientific"""" theory to explain the results. One of the problems with systems that reason with defaults occurs when two answers can be produced, and one is preferred. In terms of our default logic, we define a semantic characterisation of the notion of the more specific theory. This overcomes many of the problems which motivated non-normal defaults, and provides a semantics for correct inheritance in inheritance systems, where we want choose the result supported by the most specific knowledge. We also show how to produce a general computational mechanism in terms of normal first order predicate calculus deduction systems."""	default logic;first-order logic;model of computation;natural deduction;theory	David Poole	1985			computer science;artificial intelligence;computational mechanics;non-monotonic logic;first-order logic;mathematics;default logic;algorithm	AI	-15.066754190480577	6.173967166152406	24053
73ea3fb6ce9738623f42d96bd46aa6f5b88a994b	on graph-theoretic fibring of logics	substructural logic	A graph-theoretic account of fibring of logics is developed, capitalizing on the interleaving characteristics of fibring at the linguistic, semantic and proof levels. Fibring of two signatures is seen as an m-graph where the nodes and the m-edges include the sorts and the constructors of the signatures at hand. Fibring of two models is an m-graph where the nodes and the m-edges are the values and the operations in the models, respectively. Fibring of two deductive systems is an m-graph whose nodes are language expressions and the m-edges represent the inference rules of the two original systems. The sobriety of the approach is confirmed by proving that all the fibring notions are universal constructions. This graph-theoretic view is general enough to accommodate very different fibrings of propositional based logics encompassing logics with non-deterministic semantics, logics with an algebraic semantics, logics with partial semantics, and substructural logics, among others. Soundness and weak completeness are proved to be preserved under very general conditions. Strong completeness is also shown to be preserved under tighter conditions. In this setting, the collapsing problem appearing in several combinations of logic systems can be avoided.	algebraic semantics (computer science);formal system;forward error correction;graph theory;linear algebra;np-completeness;type signature	Amílcar Sernadas;Cristina Sernadas;João Rasga;Marcelo E. Coniglio	2009	J. Log. Comput.	10.1093/logcom/exp024	monoidal t-norm logic;t-norm fuzzy logics;discrete mathematics;computer science;mathematics;programming language;substructural logic;algorithm	Logic	-10.613434045176481	16.976377941284554	24077
8828e42907430ff7c5917cf719eb72b0aab3fe51	control conditions for transformation units: parallelism, as-long-as-possible, and stepwise control		The concept of graph transformation units is a formal and as well intuitive means to model processes on graphs. Thereby the control condition of a transformation unit plays an important role. It provides so to say the intelligence of the unit by describing its desired behaviour. The thesis regards control conditions focusing on two aspects: expressivity and practicability. Considering expressivity it implements two kinds of control conditions, aslong-as-possible and parallel expressions. As their names imply these control conditions are able to express the as-long-as-possible iteration respectively parallel composition of already described behaviour. Focusing on practical executability the thesis introduces the concept of stepwise control conditions. Whereas conventional control conditions in principle describe desired behaviour their computation may take a long time, since first all possible derivations have to be computed and then are checked against the control condition. Stepwise control conditions allow to directly guide the derivation process and so may save computation time.	computation;control flow;graph rewriting;iteration;stepwise regression;time complexity	Melanie Luderer	2016				SE	-9.987498917971497	24.155631347614666	24122
bdef138f14d6df1bedc46eb8983b1098cba6d815	cyclic steady state space refinement		A method aimed at refinement of the cyclic steady state space reachable in the given multimodal transportation network is proposed. The paper introduces the concept of a System of Concurrent Multimodal Cyclic Processes in which several subnetworks interact each other via distinguished subsets of common shared workstations as to provide a variety of demand- responsive work-piece transportation/handling services. Searching for the cyclic steady state behavior the following question is considered: Is the cyclic steady state space reachable in the given network structure ? The declarative approach employed makes it possible to evaluate the reachability of cyclic behaviors on a scale that reflects real practice.	refinement (computing);state space;steady state	Grzegorz Bocewicz;Zbigniew Antoni Banaszak;Pawel Pawlewski	2014		10.1007/978-3-319-05353-0_2	discrete mathematics;real-time computing;mathematics;algorithm	Robotics	-23.18918835765178	17.380048221450505	24129
81af17dfef6e241d235170c02848bd3456cb0f9b	alternation hierarchies of first order logic with regular predicates		We investigate the decidability of the definability problem for fragments of first order logic over finite words enriched with regular numerical predicates. In this paper, we focus on the quantifier alternation hierarchies of first order logic. We obtain that deciding this problem for each level of the alternation hierarchy of both first order logic and its twovariable fragment when equipped with all regular numerical predicates is not harder than deciding it for the corresponding level equipped with only the linear order. Relying on some recent results, this proves the decidability for each level of the alternation hierarchy of the two-variable first order fragment while in the case of the first order logic the question remains open for levels greater than two. The main ingredients of the proofs are syntactic transformations of firstorder formulas as well as the infinitely testable property, a new algebraic notion on varieties that we define.	first-order logic;linear algebra;numerical analysis;quantifier (logic)	Luc Dartois;Charles Paperman	2015		10.1007/978-3-319-22177-9_13	arithmetic;discrete mathematics;algorithm	Logic	-9.652509184389025	16.395457902991456	24160
a325a1382d80129248ea0667989a1f21c1b45672	relational database organization based on views and fragments	relational database	Traditionally data is organized at the logical level in relational databases on the basis of its semantics. But there are more abstractions of the real world to be represented. A part of this missing semantics is that of views. We include views as an integral part of the data model. After defining views on a relational scheme, we present a methodology to decompose the relations of this scheme into a set of disjoint fragments. One or more fragments represent a view. Then these fragments and not the relations of the relational scheme are materialized. We further develop a scheme for maintaining the consistency of a database made up of fragments (which include attributes of the left or right side of a split functional dependencies) of a non-3NF relation by introducing the concept of update clusters and virtual attributes. The methodology results in a database design where the database operations access less amount of irrelevant data in comparison to the design where the base relations are materialized. We briefly discuss the applicability of this methodology in designing databases based on centralized, distributed, parallel database environments and for secure database systems.	relational database	Günther Pernul;Kamalakar Karlapalem;Shamkant B. Navathe	1991			data definition language;information schema;relational model;entity–relationship model;relational database;computer science;database model;database;database schema;object-relational impedance mismatch;database design	DB	-28.6392522090169	10.162074203913832	24183
0cc9495845911ff7800c5ac1cdced2408c63b6fd	block structured object programming	lenguaje programacion;object oriented language;langage c;programming language;meta classes;implementation;information hiding;ingenieria logiciel;oo programming;object oriented programming;software engineering;nested classes;block structure;ejecucion;c language;object oriented;language development;genie logiciel;langage programmation;oriente objet;langage newton;orientado objeto;lenguaje c	Most popular object oriented languages have a flat class declaration structure. Providing the possibility of nesting classes gives several interesting properties. Objects may hide their internal representation. Objects may be born within other objects whose structure they know. Meta classes may be implemented as singular, one of its kind, objects in which are nested the classes they control. Languages of the scandinavian school of object oriented programming do offer these possibilities. The author has implemented them and experimented their effects in the Newton language developped at the Compiler Design Laboratory of the Swiss Federal Institute of Technology.	algorithm;beta;bruce ellis;c++;david ungar;declaration (computer programming);eiffel;international standard book number;jensen's inequality;multiple inheritance;newton;object-based language;prentice hall international series in computer science;programming language;reference (c++);simula;smalltalk;springer (tank);strong and weak typing;type class;type safety	Charles Rapin	1997	SIGPLAN Notices	10.1145/254459.254472	computer science;theoretical computer science;object-oriented design;programming language;object-oriented programming;algorithm	PL	-26.227604673535893	23.203221279220706	24224
90e110e3597a9b31023b11989dd64d3d62832e9d	trace effects and object orientation	object oriented programming;type and effect;featherweight java;model checking;object oriented;polymorphism;temporal program logic;type constraints;generic programming	Trace effects are statically generated program abstractions, that can be model checked for verification of assertions in a temporal program logic. In this paper we develop a type and effect analysis for obtaining trace effects of Object Oriented programs in Featherweight Java. We observe that the analysis is significantly complicated by the interaction of trace behavior with inheritance and other Object Oriented features, particularly overridden methods, dynamic dispatch, and downcasting. We propose an expressive type and effect inference algorithm, combining polymorphism and subtyping/subeffecting constraints, to obtain a flexible trace effect analysis in an Object Oriented setting.	algorithm;downcasting;dynamic dispatch;java;temporal logic;type inference	Christian Skalka	2005		10.1145/1069774.1069787	method;real-time computing;computer science;programming language;object-oriented programming;algorithm	PL	-21.686890844927863	28.89780125853058	24248
607be32223fa257e59db9c9800af19bcce039d59	dup -- explicit un-sharing in haskell		We propose two operations to prevent sharing in Haskell that do not require modifying the data generating code, demonstrate their use and usefulness, and compare them to other approaches to preventing sharing. Our claims are supported by a formal semantics and a prototype implementation.	compiler;dup;functional programming;haskell;lazy evaluation;programmer;prototype;referential transparency;semantics (computer science);type system	Joachim Breitner	2012	CoRR		computer science;database;programming language;algorithm	PL	-22.502710260814784	26.31610801038561	24259
f424d462715597a1b9e3463df2e47d150c52e2eb	prolog in practical compiler writing	lenguaje programacion;compilacion;compilateur;programming language;edison;implementation;prolog;tratamiento lenguaje;compiler;ejecucion;language processing;traitement langage;langage programmation;compilation;compilador	We discuss the experiences gained with implementing the programming language Edison in Prolog. The evaluation of Prolog in this application area is based on a comparison with two other Edison compilers, one written in Pascal (procedural approach) and the other generated using the compiler writing systems PGS and GAG (declarative approach)	prolog	Jukka Paakki	1991	Comput. J.	10.1093/comjnl/34.1.64	compiler;parallel computing;computer science;programming language;implementation;prolog;algorithm	OS	-25.037796503650874	24.02401650799787	24420
b99f158a0a10bf18a7424a2394b95063c13bbf87	verifying invariants by approximate image computation	computational method;satisfiability;safety properties;formal verification;reachability analysis	Abstract   Automatic formal verification of safety properties typically requires computing reachable states of a system. A more efficient (and less automatic) alternative is to check whether a user suggested superset of reachable states is an invariant, i.e. whether it contains its image specified by the transition relation of the system. Still, this approach may be prohibitively expensive due to the complexity of image computation. To alleviate this problem we suggest to use approximate image computations, and we show that even though the approximation computes a superset of the image, it can, in certain cases, be used to answer categorically the question whether the suggested invariant contains its image. More precisely, we first establish sufficient conditions that the approximate image computation and the suggested invariant need to satisfy in order to always reach a conclusive result of the verification process. Then, we use these results to show that the three approximate image computation methods proposed previously for approximate reachability analysis could be used for exact invariant verification.	approximation algorithm;computation;invariant (computer science)	Felice Balarin	1997	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(05)82559-9	combinatorics;discrete mathematics;formal verification;computer science;theoretical computer science;mathematics;programming language;satisfiability	ECom	-13.461070208407117	27.0828693528624	24455
2bc3c34efe45a1cdb84681b7e08cb532eb96ceef	integration of formal specification into the standard asic design flow	formal specification;design flow;hardware description languages;mathematical analysis;formal method;application specific integrated circuits;circuit cad;mathematical analysis formal specification standard asic design flow industrial design environment behavioural vhdl models tabular form;formal specification hardware description languages application specific integrated circuits circuit cad;formal specifications application specific integrated circuits systems engineering and theory natural languages circuit synthesis computer science computer industry mathematical model mathematical analysis electronics industry;industrial design	This paper presents our approach to leverage formal methods in an industrial design environment by closing the gap between the specification and design phases. We achieve this goal by deriving behavioural VHDL models from a formal system specification in tabular form that is easily accessible to mathematical analysis.	application-specific integrated circuit;design flow (eda);formal specification	Werner Haas;Stefan Gossens;Ulrich Heinkel	2002		10.1109/HASE.2002.1173122	formal methods;industrial design;specification language;formal verification;computer science;systems engineering;design flow;theoretical computer science;formal specification;formal equivalence checking;refinement;application-specific integrated circuit;hardware description language;programming language;computer engineering	EDA	-15.092876128300825	29.72648568054265	24459
bd59bb24c6a843f44088785f3168755c67207e0b	"""the expressive power of """"possible-is-certain"""" semantics (extended abstract)"""	expressive power;extended abstract	The partial stable models of a logic program form a class of models that include the (unique) well-founded model, total stable models and other two interesting subclasses: maximal stable models and least undefined stable models. As stable models different from the wellfounded are not unique, DATALOG¬ queries do not in general correspond to functions. The question is: what is the expressive powers of the various types of stable models when they are restricted to the class of all functional queries? The paper shows that this power does not go in practice beyond the one of stratified queries, except for least undefined stable models which, instead, capture the whole boolean hierarchy  BH\mathcal{B}\mathcal{H} . Finally, it is illustrated how the latter result can be used to design a BH\mathcal{B}\mathcal{H} so that exponential time resolution is eventually enabled only for hard problems.		Sergio Greco;Domenico Saccà	1996		10.1007/BFb0027777	formal semantics;action semantics;formal semantics;operational semantics;denotational semantics;computational semantics	Theory	-11.42545052165541	18.32282837798047	24476
65a2d3abc36b939d1e2a59faca57befce31fda19	enhancing dlv instantiator by backjumping techniques	answer sets;answer set programming;benchmark problem;knowledge management;knowledge representation and reasoning;information integration;logic programming;nonmonotonic reasoning;stable model semantics;artificial intelligence;68n17;computational efficiency;disjunctive logic programming;68t27;deductive databases	Disjunctive logic programming (DLP) is a powerful formalism for knowledge representation and reasoning. The high expressiveness of DLP language, together with the recent availability of some efficient DLP system, has favoured the application of DLP in emerging areas like Knowledge Management and Information Integration. These applications have often to deal with huge input data, and have evidenced the need to improve the efficiency of DLP instantiators. Program instantiation is the first phase of a DLP computation; in this phase, variables are replaced by constants to generate a ground program which is then evaluated by propositional algorithms in the second phase of the computation. The instantiation process may be computationally expensive, and in fact its efficiency has been recognized to be a key issue for solving real-world problems by using disjunctive logic programming. Given a program P, a good instantiation for P is a ground program P′ having precisely the same answer sets as P and such that: (1) P′ can be computed efficiently from P, and (2) P′ does not contain “useless” rules, (P′ is as small as possible) and can thus be evaluated efficiently. In this paper, we present a structure-based backjumping algorithm for the instantiation of disjunctive logic programs, that meets the above requirements. In particular, given a rule r to be grounded, our algorithm exploits both the semantical and the structural information about r for computing efficiently the ground instances of r, avoiding the generation of “useless” rules. That is, from each general rule r, we compute only a relevant subset of its ground instances, avoiding the generation of “useless” instances, while fully preserving the semantic of the program. We have implemented this algorithm in DLV—the state-of-the-art implementation of DLP—and we have carried out an experimentation activity on an ample collection of benchmark problems. The experimental results are very positive: the new technique improves sensibly the efficiency of the DLV system on many program classes.	algorithm;analysis of algorithms;backjumping;benchmark (computing);commonsense reasoning;computation;dlv;digital light processing;disjunctive normal form;h2 database engine;information extraction;knowledge management;knowledge representation and reasoning;linear algebra;logic programming;requirement;semantics (computer science);test and evaluation master plan;timeline;universal instantiation;useless rules	Simona Perri;Francesco Scarcello;Gelsomina Catalano;Nicola Leone	2007	Annals of Mathematics and Artificial Intelligence	10.1007/s10472-008-9090-9	knowledge representation and reasoning;mathematical optimization;stable model semantics;computer science;artificial intelligence;information integration;theoretical computer science;non-monotonic logic;answer set programming;machine learning;mathematics;programming language;logic programming;algorithm	AI	-19.762110380973073	13.720375022485223	24510
01aad9c6b783cfef3d6104bd1d1608fceaabdd28	ingredients for successful system level automation & design methodology	models of computation;abstract state machines;dissertation;behavioral hierarchy;validation;heterogeneity	begins with two typedefs. We use this coding style to avoid repeating complex types, which in this case are abs list<fsm model> and the vector<fsm model>. The abs list class is a utility class explained in the chapter that explains the graph package. However, its basic purpose is to provide a medium through which the abstract semantics functions can be invoked on all the refinements. The hierarchy fsms t type is used to hold pointers to the refinements within a state. The refinement t type is declared at [Listing 4.2, Line 26] for the data member fsm refs that holds the HFSM refinements in this state. Note that the class fsm model is not defined as yet and there is simply a forward class declaration used to allow compilation of the fsm state class. The entry actions() and exit actions() shown in [Listing 4.2, Line 8 & 9] can be overridden by the user if the user desires to employ the capability of entry and exit actions. The set run complete() and is run complete() member functions set the run complete private data member in [Listing 4.2, Line 27] and returns the Boolean value, respectively. When the run complete Boolean data member is set, then the refinements in this state must follow the run-to-complete semantics. The resetting of refinements within a state is done using reset hierarchy() in [Listing 4.2, Line 14]. This member function is usually invoked when a transition is set to the RESET Hiren D. Patel Chapter 4. Behavioral Hierarchy with Hierarchical FSMs (HFSMs) 61 type, which requires resetting the destination state’s refinements. Adding refinements into a state is done through the add fsm refinement() member function. This inserts a pointer to the HFSM into the fsm refs data member. However, before doing that, we ensure certain requirements on the added refinement, such as that the fsm pointer cannot be NULL and we check these using the private member function elaboration() [Listing 4.2, Line 29]. We define a member function that returns a Boolean value of true indicating that the state has at least one refinement embedded within it via the has hierarchy(). To retrieve refinements, we have the get fsm refinement models() member function in [Listing 4.2, Line 18], which returns a list of all the HFSMs in that state. We use an additional private member function get fsm refinement() in [Listing 4.2, Line 28] to get a pointer of type refinement t such that members from the abstract semantics can be invoked on all the refinements in the state. Listing 4.3: fsm transition Class Definition 1 class f sm t r an s i t i o n : public bgl edge , public abs semant i c s { 2 public : 3 typedef a b s l i s t < fsm model > r e f i n ement t ; 4 typedef vector< fsm model ∗> h i e r a r chy f sms t ; 5 virtual bool guard ( ) ; 6 virtual bool c h o i c e a c t i o n s ( ) ; 7 virtual bool commit act ions ( ) ; 8 bool i s e n ab l e d ( ) ; 9 bool i s p r e empt iv e ( ) ; 10 bool i s r e s e t ( ) ; 11 bool i s r un comp l e t e ( ) ; 12 void s e t enab l ed ( ) ; 13 void s e t preempt ive ( ) ; 14 void s e t r e s e t ( ) ; 15 void s e t run comp le t e ( ) ; 16 bool r e s e t h i e r a r c h y ( ) ; 17 void add fsm re f inement ( fsm model ∗ fsm ) ; 18 bool has h i e r a r chy ( ) ; 19 h i e r a r chy f sms t ge t f sm re f inement mode l s ( ) ; 20 /// Overload members o f a b s t r a c t semantics 21 /// . . . more code 22 /// Defaul t , over loaded cons t ruc to r s 23 /// Overloaded opera tors 24 /// . . . more code 25 private : 26 r e f i n ement t f sm r e f s ; 27 bool enabled ; 28 bool preemptive ; 29 bool r e s e t ; 30 bool run complete ; 31 void e l abo r a t i on ( fsm model ∗ fsm ) ; 32 r e f i n ement t ∗ ge t f sm re f i n ement ( ) ; 33 } ; The fsm transition class represents the transitions that connect the states of type fsm state. As depicted in Figure 4.6, the fsm transition class inherits from the bgl edge and abs sHiren D. Patel Chapter 4. Behavioral Hierarchy with Hierarchical FSMs (HFSMs) 62 emantics classes. The bgl edge class is a generic base class for representing edges in a graph and the abs semantics the abstract semantics associated with an executable entity. The virtual member functions in [Listing 4.3, Line 5 7] show the guard(), choice actions() and the commit actions(). The guard() member function requires the designer to test the guard associated with this transition and return a true in the case where the guard is satisfied and false when it is not. The return Boolean value is used in the kernel to trigger the actions associated with this transition. Depending on whether the guard is satisfied, the choice actions() or the commit actions() are executed. Member functions to get the properties of the transition are shown in [Listing 4.3, Line 8 11] and those that set the properties in [Listing 4.3, Line 12 15]. These get and set member functions check the type of the transition such as PREEMPTIVE, RESET or RUN-TO-COMPLETE. Their respective private data members are shown in [Listing 4.3, Line 27 30]. The transitions can also embed other HFSM refinements and thus we have the add fsm refinement() member function that allows inserting refinements in a transition. 4.5.3 HFSM Graph Representing the HFSM Model The generic graph package implements an moc graph templated class, which allows any MoC to easily implement their graph-like structure. The fsm graph class aggregates the moc graph class and renders the class with member functions specific for describing an FSM. Listing 4.4 displays the source code listing for the fsm graph class definition. The fsm graph typedefs a list of fsm state and fsm transition objects with state list and transition list. These are used as return types for retrieving all the states and transitions in the FSM using the get all states() and get all transitions() member function in [Listing 4.4, Line 7 & 8]. The set init state() member function sets the initial state for the HFSM, which assigns the pointer shown in [Listing 4.4, Line 28]. Similarly, the set final state() and the set curr state() set the final st and curr st private members in [Listing 4.4, Line 29 & 30]. Given a transition, the get target() member function returns the destination state of that transition. However, to retrieve the states that are connected to a particular state via some transition, we employ the get adjacent states(). The return type of this is once again a list of fsm state pointers. To retrieve all the outgoing transitions from a state, we use the get outgoing transitions() member function, which returns a transition list. The get num states() and get num transitions() return the number of states and transitions in the HFSM. Until now, the member functions described are used for manipulating the information present in the HFSM. However, to insert states and transitions into the HFSM we provide the insert state() and insert transition() member functions. We have overloaded these members so as to allow passing by reference as well as by pointer. The insertion of transition requires three arguments and they are the fsm state from which the transition is outgoing and the fsm state to which the transition is the destination and lastly the transition to be associated with the from and to fsm state objects as shown in [Listing 4.4, Line 21]. Finally, the private member shown in [Listing Hiren D. Patel Chapter 4. Behavioral Hierarchy with Hierarchical FSMs (HFSMs) 63 Listing 4.4: fsm graph Class Definition 1 class fsm graph 2 { 3 public : 4 typedef vector< f sm s ta t e ∗ > s t a t e l i s t ; 5 typedef vector< f sm t r an s i t i o n ∗ > t r a n s i t i o n l i s t ; 6 /// Functions only s p e c i f i c f o r fsm graph 7 s t a t e l i s t g e t a l l s t a t e s ( ) ; 8 t r a n s i t i o n l i s t g e t a l l t r a n s i t i o n s ( ) ; 9 void s e t i n i t s t a t e ( f sm s ta t e ∗ s t ) ; 10 void s e t f i n a l s t a t e ( f sm s ta t e ∗ s t ) ; 11 void s e t c u r r s t a t e ( f sm s ta t e ∗ s t ) ; 12 f sm s ta t e ∗ g e t i n i t s t a t e ( ) ; 13 f sm s ta t e ∗ g e t f i n a l s t a t e ( ) ; 14 f sm s ta t e ∗ g e t c u r r s t a t e ( ) ; 15 f sm s ta t e ∗ g e t t a r g e t ( const f sm t r an s i t i o n & t r ) ; 16 s t a t e l i s t g e t a d j a c e n t s t a t e s ( const f sm s ta t e & s t ) ; 17 t r a n s i t i o n l i s t g e t o u t g o i n g t r a n s i t i o n s ( const f sm s ta t e & s t ) ; 18 unsigned int get num state s ( ) ; 19 unsigned int ge t num t ran s i t i on s ( ) ; 20 void i n s e r t s t a t e ( f sm s ta t e & s t ) ; 21 void i n s e r t t r a n s i t i o n ( const f sm s ta t e & from , const f sm s ta t e & to , f sm t r an s i t i o n & t r ) ; 22 void i n s e r t s t a t e ( f sm s ta t e ∗ s t ) ; 23 void i n s e r t t r a n s i t i o n ( const f sm s ta t e ∗ from , const f sm s ta t e ∗ to , f sm t r an s i t i o n ∗ t r ) ; 24 fsm graph ( ) ; 25  ̃ fsm graph ( ) ; 26 private : 27 moc graph< f sm state , f sm t r an s i t i o n > fsm g ; 28 f sm s ta t e ∗ i n i t s t ; 29 f sm s ta t e ∗ f i n a l s t ; 30 f sm s ta t e ∗ c u r r s t ; 31 } ; 4.4, Line 27] is an aggregation of the moc graph class. The fsm g private data member is of type moc graph with the vertices being fsm state type and the edges being fsm transition type. Capturing the graph-like structure of the HFSM using fsm graph only allows a way to represent the HFSM, but it does not implement the abstract semantics or any real semantics associated with it. Since we wish to keep the semantic representation separate from the model representation, we implement the semantics or the HFSM kernel in the fsm director class. However, keeping heterogeneous behavioral hierarchy in mind, we need to associate one instance of the simulation kernel to one level of hierarchy of a model. To do this, we implement another class called fsm model that represents one HFSM model that has an association with the fsm director class as shown in [Listing 4.5, Line 18]. This class mainly implement	adaptive binary optimization;automation;boolean algebra;chomsky hierarchy;compiler;const (computer programming);declaration (computer programming);embedded system;executable;field (computer science);fire class;graph (discrete mathematics);information privacy;init;integer (computer science);method (computer programming);object composition;pointer (computer programming);preemption (computing);programming style;refinement (computing);rendering (computer graphics);requirement;return type;simulation;typedef;uml state machine	Hiren D. Patel	2007			simulation;computer science;systems engineering;management science	AI	-29.410872216398758	29.883655133940202	24513
edc8ab7f01aba6e4a3b78fc602f152a171972ac2	diagnosable discrete event system design: a case study of automatic temperature control system	binary integer linear programming problem diagnosable discrete event system design automatic temperature control system petri nets diagnosability analysis;sensor systems;diagnosability petri nets discrete event system;discrete event systems temperature control state space methods petri nets sensor systems delay sufficient conditions information management marine technology paper technology;sensors;probability density function;temperature control;temperature sensors;diagnosability;data mining;discrete event system;integer programming;control system synthesis;temperature control control system synthesis discrete event systems integer programming linear programming petri nets;discrete event systems;linear programming;diagnosability analysis;petri nets;valves;binary integer linear programming problem;petri net;encoding;diagnosable discrete event system design;automatic temperature control system;integer linear program	This paper presents an approach using Petri nets for designing diagnosable discrete event systems such as complex automatic temperature control (ATC) system. The concept is based on diagnosability analysis and enhancement. In this paper, we interpret and formulate the diagnosability problem as a binary integer linear programming problem that may have a feasible solution. If the system is predicted to be non-diagnosable, the approach tries to add sensors to enhance its diagnosablity, i.e., to make the system diagnosable.	algorithm;control system;integer programming;iterative method;linear programming;petri net;sensor;systems design	YuanLin Wen;MuDer Jeng	2008	2008 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2008.4811873	discrete mathematics;real-time computing;integer programming;computer science;linear programming;mathematics;petri net	Robotics	-6.253869466289449	28.62235422158464	24520
49cbad4462ebcd76860cec6409c0a867e7e360c8	qualitative and quantitative temporal constraints and relational databases: theory, architecture, and applications	query processing;relational database;integration;hospital patient management temporal constraints relational databases database theory temporal databases artificial intelligence temporal reasoning query answering simple temporal problem framework partitioned sets;artificial intelligent;temporal constraints;temporal database;simple temporal problem;medical information systems;relational model;temporal relational databases;medical information systems temporal databases relational databases database theory temporal reasoning query processing;artificial intelligence;qualitative and quantitative temporal constraints;temporal databases;relational databases;query answering;modular architecture;database theory;temporal constraint propagation;temporal reasoning;constraint theory relational databases artificial intelligence hospitals deductive databases intelligent structures management information systems event detection;structured data	Many different applications in different areas need to deal with both: 1) databases, in order to take into account large amounts of structured data, and 2) quantitative and qualitative temporal constraints about such data. In this paper, we propose an approach that extends: 1) temporal databases, and 2) artificial intelligence temporal reasoning techniques and integrate them in order to face such a need. Regarding temporal reasoning, we consider some results that we proved recently about efficient query answering in the Simple Temporal Problem framework and we extend them in order to deal with partitioned sets of constraints and to support relational database operations. Regarding databases, we extend the relational model in order to consider also qualitative and quantitative temporal constraints both in the data (data expressiveness) and in the queries (query expressiveness). We then propose a modular architecture integrating a relational database with a temporal reasoner. We also consider classes of applications that fit into our approach and consider patient management in a hospital as an example.	artificial intelligence;modular programming;relational database;relational model;semantic reasoner;temporal database	Vittorio Brusoni;Luca Console;Paolo Terenziani;Barbara Pernici	1999	IEEE Trans. Knowl. Data Eng.	10.1109/69.824613	relational database;computer science;data mining;database;temporal database;information retrieval	DB	-29.315042854440748	9.319734765508322	24526
e37e370fa7bbcd9c0ac6c2a34bed09dedac01c38	implementation concepts for an extensible data model and data language	front end;database system;base donnee;langage manipulation donnee;lazy evaluation;interrogation base donnee;lenguaje manipulacion dato;database;interrogacion base datos;base dato;data type;data model;efficient implementation;functional data;systeme gestion base donnee;information system;extension;sistema gestion base datos;database management system;database query;systeme information;data manipulation language;sistema informacion	Future database systems must feature extensible data models and data languages in order to accommodate the novel data types and special-purpose operations that are required by nontraditional database applications. In this paper, we outline a functional data model and data language that are targeted for the semantic interface of GENESIS, an extensible DBMS. The model and language are generalizations of FQL [11] and DAPLEX [40], and have an implementation that fits ideally with the modularity required by extensible database technologies. We explore different implementations of functional operators and present experimental evidence that they have efficient implementations. We also explain the advantages of a functional front-end to ¬1NF databases, and show how our language and implementation are being used to process queries on both 1NF and ¬1NF relations.	daplex;data model;database;fits;facebook query language;first normal form;genesis	Don S. Batory;T. Y. Leung;T. E. Wise	1988	ACM Trans. Database Syst.	10.1145/44498.45062	data manipulation language;data type;data model;computer science;theoretical computer science;front and back ends;data mining;lazy evaluation;database;programming language;information system;database design	DB	-30.04369181663481	10.568268899569146	24604
33fa18f578253b735a236fb52e71ce5fa143361e	extending the explicit substitution paradigm	logique lineaire;metodo paso a paso;step by step method;erasure;confluencia;confluence;logica lineal;borradura;red prueba;proof net;intuitionistic logic;duplication;reecriture;reseau preuve;logique intuitionniste;methode pas a pas;effacement;duplicacion;preservation;explicit substitution;rewriting;linear logic;preservacion;logica intuicionista;reescritura	We present a simple term language with explicit operators for erasure, duplication and substitution enjoying a sound and complete correspondence with the intuitionistic fragment of Linear Logic’s Proof Nets. We establish the good operational behaviour of the language by means of some fundamental properties such as confluence, preservation of strong normalisation, strong normalisation of well-typed terms and step by step simulation. This formalism is the first term calculus with explicit substitutions having full composition and preserving strong normalisation.	computation;confluence;de bruijn graph;de bruijn index;explicit substitution;first-order predicate;free variables and bound variables;functional programming;garbage collection (computer science);lambda calculus;linear logic;modulo operation;normalization property (abstract rewriting);operational semantics;path ordering (term rewriting);programming paradigm;relevance;scope (computer science);semantics (computer science);sequent calculus;simulation;subject reduction;turing completeness	Delia Kesner;Stéphane Lengrand	2005		10.1007/978-3-540-32033-3_30	linear logic;normalisation by evaluation;rewriting;intuitionistic logic;computer science;artificial intelligence;calculus;mathematics;programming language;preservation;confluence;algorithm;gene duplication	PL	-12.242005493217972	16.378424043035245	24605
a09622ffd86d297dfad0a3c6071f00d3af2d3b9a	delaunay: a database visualization system	query language;visual query system;user interface;object database;garbage collection;object oriented;partitions;object oriented database;visual system;cyclic garbage;visual query language	Visual query systems have traditionally supported a set of pre-defined visual displays. We describe the Delaunay system, which supports visualizations of object-oriented databases specified by the user with a visual constraint-based query language. The highlights of our approach are the expressiveness of the visual query language, the efficiency of the query engine, and the overall flexibility and extensibility of the framework. The user interface is implemented using Java and is available on the WWW.	database;delaunay triangulation;extensibility;java;query language;user interface;www	Isabel F. Cruz;Michael Averbuch;Wendy T. Lucas;Melissa Radzyminski;Kirby Zhang	1997		10.1145/253260.253376	online aggregation;sargable;query optimization;query expansion;web query classification;visual system;data control language;computer science;query by example;theoretical computer science;database;rdf query language;garbage collection;programming language;object-oriented programming;user interface;web search query;view;query language;object query language	DB	-32.30732625011275	8.652114436053838	24612
b523ea00b31ff0c615345ccb06956202c3ab1e65	the functional dendritic cell algorithm: a formal specification with haskell		The Dendritic Cell Algorithm (DCA) has been described in a number of different ways, sometimes resulting in incorrect implementations. We believe this is due to previous, imprecise attempts to describe the algorithm. The main contribution of this paper is to remove this imprecision through a new approach inspired by purely functional programming. We use new specification to implement the deterministic DCA in Haskell - the hDCA. This functional variant will also serve to introduce the DCA to a new audience within computer science. We hope that our functional specification will help improve the quality of future DCA related research and to help others understand further its algorithmic properties.		Julie Greensmith;Michael B. Gale	2017	2017 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2017.7969518	computer science;artificial intelligence;theoretical computer science;algorithm	PL	-22.156357914211167	14.805122527660727	24628
48c9a853ba53b9ae552b0376ee214cd82cedac07	incompleteness of first-order temporal logic with until	logique mathematique;logica matematica;mathematical logic;engineering and technology;teknik och teknologier;first order temporal logic;logique ordre 1;one order logic;logica orden 1	The results presented in this paper concern the axiomatizability problem of first-order temporal logic with linear and discrete time. We show that the logic is incomplete, i.e., it cannot be provided with a finitistic and complete proof system. We show two incompleteness theorems. Although the first one is weaker (it assumes some first-order signature), we decided to present it, for its proof is much simpler and contains an interesting fact that finite sets are characterizable by means of temporal formulas. The second theorem shows that the logic is incomplete independently of any particular signature.	first-order logic;first-order predicate;proof calculus;temporal logic	Andrzej Szalas;Leszek Holenderski	1988	Theor. Comput. Sci.	10.1016/0304-3975(88)90045-X	predicate logic;dynamic logic;zeroth-order logic;complete theory;mathematical logic;discrete mathematics;linear temporal logic;higher-order logic;temporal logic;many-valued logic;interval temporal logic;intermediate logic;bunched logic;predicate functor logic;mathematics;signature;consistency;logic;multimodal logic;second-order logic;algorithm	Theory	-12.010442344310544	13.974636268309242	24633
daf13116c1bc6aac3508652d9e5be3749657913c	views of objects and rules	expressive power;database system	Object-oriented database systems provide expressive power and the potential for reuse, extensibility and modifiability. Relational and deductive database systems provide a simple logical semantics and the ability to represent complex derived information. Many proposals have been made for database systems that offer both sets of features, and some combined systems have been implemented. In this paper, I shall review recent work on one particular approach to the problem of designing deductive object-oriented database systems that offer both sets of features.	deductive database;extensibility	Rodney W. Topor	1994			data mining;database;computer science;database schema;database catalog;view;database design;database theory;database model;spatiotemporal database;intelligent database	DB	-30.46335565207278	11.166704726163681	24662
d2f44f5a184c98217f6cf9bfccd9c84d6e64a20e	typical ambiguity and elementary equivalence	institutional repositories;fedora;russell s simple theory of types;vital;typical ambiguity;vtls;ils;elementary equivalence	A sentence of the usual language of set theory is said to be stratified if it is obtained by “erasing” type indices in a sentence of the language of Russell’s Simple Theory of Types. In this paper we give an alternative presentation of a proof the ambiguity theorem stating that any provable stratified sentence has a stratified proof. To this end, we introduce a new set of ambiguity azioms, inspired by FraissC’s characterization of elementary equivalence; these axioms can be naturally used to give different proofs of the ambiguity theorem (semantic or syntactic, classical or intuitionistic). MSC: 03B15, 03F50, 03F55.	ambiguity function;history of type theory;provable security;set theory;turing completeness	Daniel Dzierzgowski	1993	Math. Log. Q.	10.1002/malq.19930390147	mathematical analysis;discrete mathematics;elementary equivalence;mathematics;algorithm;algebra	Theory	-9.667177204339202	12.947190571222794	24694
52c20d0f8bd5f52054d5fba698a7d4046e04dd3f	application of two different methods for extending lattice-valued restricted equivalence functions used for constructing similarity measures on l-fuzzy sets		This work was partially supported by the Brazilian Funding Agency CNPq under the Process 307781/2016-0, the ResearchrnServices of Universidad Publica de Navarra and by the research project TIN2016-77356-P from MINECO, AEI/FEDER, UE.	fuzzy set;turing completeness	Eduardo Silva Palmeira;Benjamín R. C. Bedregal;Humberto Bustince;Daniel Paternain;Laura De Miguel	2018	Inf. Sci.	10.1016/j.ins.2018.02.022	fuzzy logic;algebra;discrete mathematics;equivalence (measure theory);funding agency;fuzzy set;lattice (order);mathematics	AI	-10.36216571220173	9.787510307489296	24714
3011495d9d8448487ad1385fce621c14e9ce687a	intelligent databases: old challenges and new opportunities	data intensive application;database system;programming environment;intelligent information system;rule based system;rule based;non monotonic reasoning;information system;deductive databases	The evolution of existing information systems and a new wave of data-intensive applications are creating a strong demand for database-centered programming environments much more sophisticated and intelligent than those supported by current database systems. In this paper, we describe the contributions that deductive databases offer to the evolution of databases and information systems to satisfy said demands. In addition to all database essentials, deductive databases support rule-based logic-oriented languages that allow terse formulations of complete applications, along with reasoning and queries. Thus, they support a rule-based interface that eliminates the impedance mismatch problem (between programming language and query sublanguage) and elevates the design and development of database applications to the level of declarative, knowledge-based specifications. In this paper, we review the evolution of the enabling technology and architectures of deductive database prototypes; then we focus on their applications, as seen by the author through his experience with theLDL/LDL++ project. In particular, the paper describes the languages and the (bottom-up) execution technology used by the first generation of deductive database prototypes. Then the paper discusses how the experience with a first-generation system (LDL) guided the design and implementation of a second-generation prototype (LDL++).	bottom-up parsing;characteristic impedance;compiler;data-intensive computing;deductive database;documentation;information system;logic programming;non-monotonic logic;programming language;prototype;recursion;software deployment;software prototyping;sublanguage;top-down and bottom-up design;unavailability	Carlo Zaniolo	1992	Journal of Intelligent Information Systems	10.1007/BF00962921	rule-based system;database theory;computer science;artificial intelligence;theoretical computer science;machine learning;data mining;database;programming language;information system;database testing	DB	-28.995774009296305	11.31738571554208	24717
6168df2c7ca44fd45f72063ad12c219976912c35	generation and evaluation of glyphs representing superclass-subclass relationships	visual programming;shape object oriented programming fractals concrete educational programs tree graphs programming environments prototypes java programming profession;object oriented programming languages;inheritance;object oriented languages inheritance visual programming;object oriented languages;visual programming glyph representation inheritance relationships superclass subclasses object oriented programming language	This paper proposes and evaluates glyph representation of inheritance relationships between a superclass and subclasses in an object-oriented programming language. The inheritance relationships in object-oriented programming languages are usually represented by a diagram of a tree graph or a nested structure in a visual programming environment. Using the proposed representation, the inheritance relationships are represented by inclusion relationships of glyphs. A prototype system of glyph generation is developed. Experiments using the Java 2 Standard Edition, which has more than 1,500 classes, show that one can recognize inheritance relationships in the proposed representation faster than the usual textual representation.	actionscript;diagram;glyph;integrated development environment;java platform, standard edition;java version history;prototype;visual programming language	Noritaka Osawa	2000		10.1109/VL.2000.874362	natural language processing;multiple inheritance;first-generation programming language;protocol;trait;declarative programming;object-based language;delegation;programming domain;reactive programming;computer science;third-generation programming language;functional logic programming;class-based programming;programming paradigm;procedural programming;inductive programming;fifth-generation programming language;composition over inheritance;programming language;object-oriented programming;second-generation programming language;comparison of multi-paradigm programming languages;algorithm	PL	-29.8195320665401	24.040115408399146	24737
aff64df2be4924377cb4c5ef2640dfe9646e3e6f	modal logics in timed process algebras	timed process algebras;modal logics;process algebra;temporal logic;modal logic	Processes of timed process algebras may be modelled as timed transition systems and modal and temporal logics can be used to express properties of transition systems. This paper introduces an extension to propositional modal-calculus with an additional modal operator. This quantitative box operator is used to express quantitative properties of timed transition systems. Deenabil-ity of important timed properties is the main body of the paper. The properties under consideration include time determinism, time additivity, maximal progress and nite variability. Finally, minimal timed logics for the classes of time deterministic and time additive transition systems are chracterised axiomatically.	maximal set;modal logic;modal operator;process calculus;spatial variability;timed automaton;utility functions on indivisible goods	Lubos Brim	1992			normal modal logic;discrete mathematics;theoretical computer science;mathematics;timed automaton;accessibility relation;algorithm	Logic	-10.15486994245505	23.34266635233653	24875
8bd4fc2d30daf38754462967e60d6b68a75838f8	first-order default logic revisited	first order;world view;default logic	Reiter’s original proposal for default logic is unsatisfactory for open default theories because of Skolemization and grounding. In this paper, we reconsider this long-standing problem and propose a new world view semantics for firstorder default logic. Roughly speaking, a world view of a firstorder default theory is a maximal collection of structures satisfying the default theory where the default part is fixed by the world view itself. We show how this semantics generalizes classical first-order logic and first-order answer set programming, and we discuss its connections to Reiter’s semantics and other related semantics. We also argue that first-order default logic under the world view semantics provides a rich framework for integrating classical logic based and rule based formalisms in the first-order case.	answer set programming;default logic;design rationale;first-order logic;first-order predicate;knowledge representation and reasoning;maximal set;skolem normal form;stable model semantics;theory	Yi Zhou	2014			description logic;higher-order logic;stable model semantics;computer science;artificial intelligence;default argument;first-order logic;computational logic;well-founded semantics;default logic;multimodal logic;algorithm;philosophy of logic	AI	-15.363271643070828	8.67896711472333	24975
b72213ccf6e194781efacf06c34a766523c5657e	order-sorted model theory for temporal executable specifications	time dependent;executable specification;selected works;programming paradigm;software systems;temporal constraints;declarative languages;bepress;state transition;model theory	Abstract   An order-sorted, temporal programming paradigm is presented. It consists of a typed, modular, declarative language, its associated order-sorted temporal Horn clause logic basis, and a model theory generalizing order-sorted algebras with predicates to temporal order-sorted structures. The essence of this generalization is in time-dependent interpretation of predicates, so that a temporal order-sorted model amounts to a sequence of order-sorted equational models with predicates, one per each state. The main advantage of the presented paradigm in comparison with paradigms based on Horn-clause logic with equality is that it is more expressive, particularly so in representing properly state transitions, and other event-oriented, temporal behavioral properties of objects. At the same time, the generalized paradigm is proved to have the initial model semantics. The rules for temporal order-sorted deduction are established as an appropriate generalization of the rules for order-sorted Horn-clause logic with equality. The initial model is a quotient temporal order-sorted structure constructed from the initial temporal order-sorted structure and a congruence relation derived from a given set of temporal constraints. Temporal order-sorted model theoretic properties are also naturally established for temporal queries. The temporal constraint language has an execution model, and it is intended to be a basis for a prototyping tool for complex, typed, modular software systems.	executable	Suad Alagic;Mara Alagic	1997	Theor. Comput. Sci.	10.1016/S0304-3975(96)00134-X	discrete mathematics;linear temporal logic;interval temporal logic;computation tree logic;computer science;mathematics;programming paradigm;programming language;algorithm;temporal logic of actions;software system;model theory;algebra	ECom	-12.265040751529362	22.19796401485006	24985
416694803c14d67d79184ff4e591cd1b0c73dad2	coalgebraic tools for randomness-conserving protocols		We propose a coalgebraic model for constructing and reasoning about state-based protocols that implement efficient reductions among random processes. We provide basic tools that allow efficient protocols to be constructed in a compositional way and analyzed in terms of the tradeoff between latency and loss of entropy. We show how to use these tools to construct various entropy-conserving reductions between processes.	automata theory;automaton;bernoulli polynomials;entropy (information theory);entropy rate;existential quantification;information theory;initial algebra;markov chain;mealy machine;randomness;state transition table;stochastic process;stream (computing);tree (data structure)	Dexter Kozen;Matvey Soloviev	2018		10.1007/978-3-030-02149-8_18	latency (engineering);theoretical computer science;mathematics;randomness;discrete mathematics;seven basic tools of quality;stochastic process	Security	-10.546136163618025	26.416582330020415	24996
f967093dedd36189a47c66a8e15e9f43abbf9299	materialization calculus for contexts in the semantic web		Representation of context dependent knowledge in the Semantic Web is becoming a recognized issue and a number of DL-based formalisms have been proposed in this regard: among them, in our previous works we introduced the Contextualized Knowledge Repository (CKR) framework. In CKR, contexts are organized hierarchically according to a broader-narrower relation and knowledge propagation across contexts is limited among contexts hierarchically related. In several applications, however, this structure is too restrictive, as they demand for a more flexible and scalable framework for representing and reasoning about contextual knowledge. In this work we present an evolution of the original CKR (based on OWL RL), where contexts can be organized in any graph based structure (declared as a metaknowledge base) and knowledge propagation is allowed among any pair of contexts via a new ”evaluate-in-context” operator. In particular, we detail a materialization calculus for reasoning over the revised CKR framework and prove its soundness and completeness. Moreover, we outline the current implementation of the calculus on top of SPRINGLES, an extension of standard RDF triple stores for representing and rule-based inferencing over multiple RDF named graphs.	logic programming;named graph;scalability;semantic web;software propagation;triplestore;web ontology language	Loris Bozzato;Luciano Serafini	2013			calculus;rdf;metaknowledge;operator (computer programming);named graph;semantic web;social semantic web;natural language processing;computer science;artificial intelligence;soundness;semantic web stack	AI	-22.95474582324088	9.199819741060848	25002
2313e36a42984d8234335cbd7c581dc8f47b7480	exxon's experience with the michael jackson design method	design method	88 DATABASE Winter-Spring 1980 In 1973, Exxon Corporation formed a team of experience d analysts to find ways of improving the application development process . The team found that program developmen t took more time than any other activity. It was subsequentl y concluded that program structure was fundamental to effective development, enhancement and reliability . After further investigation, the team recommended th e Michael Jackson program design methodology with mino r modifications and additions . The combination of Jackson' s approach (and diagrammatic notation), walk-throughs an d top-down implementation was called Program Structure Technology (PST) . The basic principle of PST is that the program will hav e a clear, logical structure if it is derived from the structure o f the data processed . During implementation, the program i s designed, coded and tested, using top-down techniques . Walk-throughs by the project team serve as checkpoints . An interactive graphics package was written to aid th e applications developer in creating the hierarchic design dia-	diagram;graphics;jackson;program structure tree;programmer;structured programming;top-down and bottom-up design	Jayne B. Menard	1980	DATA BASE	10.1145/1017517.1017530	design methods;computer science;engineering	SE	-32.52970011317238	24.490466195180176	25037
8cfd3b3b89a649e434d6ecfa9a08b17fd6ceff21	the internal structure of the fortran cep translator	internal structure;fortran	Suppose that F is a compiler for a fully nested language L and that P is a program written in L. The compiler F must know the ancestry relations among the predicates due to the passive produefimts, but it need not have the ~ctive productions in any particular order. The compiler can process from-left-to-right through the program P; as each new predicate Q becomes a key symbol about which the compiler must try to make a match, the compiler need look only to the l@ of Q.	compiler;complex event processing;fortran	O. G. Mancino;Maria Morandi Cecchi	1965	Commun. ACM	10.1145/363791.363799	computer architecture;parallel computing;computer science;programming language	PL	-23.99747939663627	25.70475881667787	25068
104b0b8250d29eb09f96a0428bb701a365c54178	finite automata over structures - (extended abstract)	atomic operation;finite automaton;closure property;emptiness problem;state transition;order fragment;atomic relation;arbitrary structure;underlying structure;automaton test;finite automata model	We introduce a finite automata model for performing computations over an arbitrary structure       $\mathcal S$    . The automaton processes sequences of elements in       $\mathcal S$    . While processing the sequence, the automaton tests atomic relations, performs atomic operations of the structure       $\mathcal S$    , and makes state transitions. In this setting, we study several problems such as closure properties, validation problem and emptiness problems. We investigate the dependence of deciding these problems on the underlying structures and the number of registers of our model of automata. Our investigation demonstrates that some of these properties are related to the existential first order fragments of the underlying structures.	automaton;finite-state machine	Aniruddh Gandhi;Bakhadyr Khoussainov;Jiamou Liu	2012		10.1007/978-3-642-29952-0_37	combinatorics;discrete mathematics;quantum finite automata;deterministic automaton;ω-automaton;mathematics;timed automaton;algorithm	Theory	-9.968255123664754	22.735325220734193	25078
100f79f14b761fa96e2aa492410f86b8661ce81f	combining string abstract domains for javascript analysis: an evaluation		Strings play a central role in JavaScript and similar scripting languages. Owing to dynamic features such as the eval function and dynamic property access, precise string analysis is a prerequisite for automated reasoning about practically any kind of runtime property. Although the literature presents a considerable number of abstract domains for capturing and representing specific aspect of strings, we are not aware of tools that allow flexible combination of string abstract domains. Indeed, support for string analysis is often confined to a single, dedicated string domain. In this paper we describe a framework that allows us to combine multiple string abstract domains for the analysis of JavaScript programs. It is implemented as an extension of SAFE, an open-source static analysis tool. We investigate different combinations of abstract domains that capture various aspects of strings. Our evaluation suggests that a combination of few, simple abstract domains suffice to outperform the precision of state-of-the-art static analysis tools for JavaScript.	apl;abstract interpretation;automated reasoning;benchmark (computing);dataflow;eval;javascript;open-source software;programming language;scripting language;static program analysis;string (computer science);the australian	Roberto Amadini;Alexander Jordan;Graeme Gange;François Gauthier;Peter Schachte;Harald Søndergaard;Peter J. Stuckey;Chenyi Zhang	2017		10.1007/978-3-662-54577-5_3	computer science;theoretical computer science;programming language;world wide web	SE	-19.557576998975488	27.691541848041002	25091
79b514ea338e72c1a38e47ba62b66c576c07bbff	tractable epistemic reasoning with functional fluents, static causal laws and postdiction		We present an epistemic action theory for tractable epistemic reasoning as an extension to the h-approximation (HPX) theory. In contrast to existing tractable approaches, the theory supports functional fluents and postdictive reasoning with static causal laws. We argue that this combination is particularly synergistic because it allows one not only to perform direct postdiction about the conditions of actions, but also indirect postdiction about the conditions of static causal laws. We show that despite the richer expressiveness, the temporal projection problem remains tractable (polynomial), and therefore the planning problem remains in NP. We present the operational semantics of our theory as well as its formulation as Answer Set Programming.	action theory (philosophy);answer set programming;approximation;backdrop cms;causal filter;cobham's thesis;fluent (artificial intelligence);home automation;microsoft personal web server;operational semantics;polynomial;ramification problem;scott continuity;stable model semantics;synergy;time complexity	Manfred Eppe	2014	CoRR		simulation;artificial intelligence;machine learning	AI	-16.136219953758278	11.56578690561201	25107
18b8e6e0603e324e476f83d0e9fa7956058293a4	regular model checking for ltl(mso)	temporal logic;model checking;regular model checking;datavetenskap datalogi;monadic logic;computer science;communication protocols;parameterized systems	Regular model checking is a form of symbolic model checking for parameterized and infinite-state systems whose states can be represented as words of arbitrary length over a finite alphabet, in which regular sets of words are used to represent sets of states. We present LTL(MSO), a combination of the logics monadic second-order logic (MSO) and LTL as a natural logic for expressing the temporal properties to be verified in regular model checking. In other words, LTL(MSO) is a natural specification language for both the system and the property under consideration. LTL(MSO) is a two-dimensional modal logic, where MSO is used for specifying properties of system states and transitions, and LTL is used for specifying temporal properties. In addition, the first-order quantification in MSO can be used to express properties parameterized on a position or process. We give a technique for model checking LTL(MSO), which is adapted from the automata-theoretic approach: a formula is translated to a buchi regular transition system with a regular set of accepting states, and regular model checking techniques are used to search for models. We have implemented the technique, and show its application to a number of parameterized algorithms from the literature.	approximation algorithm;automata theory;first-order predicate;fuzzy logic;modal logic;model checking;solid modeling;specification language;transition system	Parosh Aziz Abdulla;Bengt Jonsson;Marcus Nilsson;Julien d'Orso;Mayank Saksena	2011	International Journal on Software Tools for Technology Transfer	10.1007/s10009-011-0212-z	model checking;communications protocol;linear temporal logic;temporal logic;computer science;monadic predicate calculus;algorithm	Logic	-11.17994442522275	24.957824792106077	25161
6cad175d72f8714937d72158abe80ef61d0cdc31	a logical approach to designing procedural programming languages (abstract)	programming language;temporal logic;rule based;search algorithm;program generation;concurrent programs;distributed computing environment	New concepts can sometimes be developed by viewing a well established subject matter from a different perspective. A new perspective which has the potential for developing new concepts for designing and defining procedural programming languages is presented. The guidelines for designing procedural programming languages are educed from an analysis of generalized temporal logic. Generalized temporal logic is briefly discussed to present a logical approach to language definition and to arrive at the necessary syntactic and semantic entities for defining procedural programming languages. Then a new definition is given to the essential constructs of sequential programming languages. Next the Propositional Procedural programming Logic Lpp is presented. It is shown that the logic Lpp can be used to express sequential programs, generalized rule-based (in which rules can be fired concurrently) programs, and a certain type of concurrent programs. In this framework, program execution is equivalent to executing a model-searching algorithm. Model-searching algorithms exist independent of a language definition. This allows one to implement a language in any of the sequential, parallel, or distributed computing environments. The potential, applicability, and some of the advantages of this logical approach are presented.	concurrent computing;distributed computing;entity;logic programming;parallel computing;procedural programming;programming language;search algorithm;subject matter expert turing test;temporal logic	Ashvin Radiya	1990		10.1145/100348.100432	rule-based system;fourth-generation programming language;first-generation programming language;declarative programming;language primitive;temporal logic;programming domain;reactive programming;computer science;artificial intelligence;theoretical computer science;third-generation programming language;functional logic programming;computer programming;database;programming paradigm;procedural programming;inductive programming;fifth-generation programming language;programming language theory;programming language;logic programming;second-generation programming language;comparison of multi-paradigm programming languages;algorithm;distributed computing environment;search algorithm;parallel programming model	PL	-26.214427780569824	20.770985441594814	25250
e3ac22ed9b96add57c48c2d3113c68ba3e4b1f04	construction of integrity preserving triple graph grammars	schema compliance;bidirectional transformation;integrity preservation;triple graph grammars;negative application conditions	Triple Graph Grammars (TGGs) are a rule-based technique of specifying a consistency relation over a source, correspondence, and target domain, which can be used for bidirectional model transformation.#R##N##R##N#A current research challenge is increasing the expressiveness of TGGs by ensuring that global constraints in the involved domains are not violated by the transformation. Negative Application Conditions (NACs) can be used to enforce this property, referred to as schema compliance.#R##N##R##N#In previous work, we have presented a polynomial control algorithm for integrity preserving TGGs, using NACs only to ensure schema compliance, meaning that, for efficiency reasons, the usage of NACs must be restricted appropriately. In this paper, we apply the well-known translation of global constraints to application conditions for a given TGG and set of global constraints. We show that the derived set of NACs is indeed sufficient and necessary to ensure schema compliance, i.e., that the TGG together with the derived NACs is integrity preserving by construction.		Anthony Anjorin;Andy Schürr;Gabriele Taentzer	2012		10.1007/978-3-642-33654-6_24	database;mathematics;algorithm	HCI	-26.021088251325292	12.777095159944091	25290
48ae2d919d7b0fd789aef91d456d6e4748ebba0b	a possibilistic logic view of preference queries to an uncertain database	databases;silicon;total order;databases cities and towns uncertainty encoding silicon possibility theory calculus;query processing;uncertainty;hierarchical queries;uncertain database;hierarchical queries possibilistic logic view preference queries uncertain database;data association;preference queries;calculus;possibilistic logic;formal logic;cities and towns;possibility theory;query processing formal logic possibility theory;encoding;conference proceeding;possibilistic logic view	The paper shows how three different types of hierarchical queries involving qualitative preference and leading to totally ordered results, which have been recently considered in the setting of division operation, can be conveniently encoded in possibilistic logic. This enables such a handling of preference queries to be interfaced with a recently proposed approach for dealing with pieces of data associated with certainty levels represented in the framework of possibilistic logic.	hierarchical and recursive queries in sql;null (sql);ordinal data;possibility theory;relational algebra;uncertain data	Patrick Bosc;Olivier Pivert;Henri Prade	2010	International Conference on Fuzzy Systems	10.1109/FUZZY.2010.5584028	possibility theory;uncertainty;artificial intelligence;machine learning;data mining;database;mathematics;silicon;logic;total order;encoding;statistics	DB	-27.027375390283872	8.606664745145144	25295
70816a3bdc05c9628af8a19704e8f5ff77e68d38	foundational belief change	belief;gardenfors p;revision;foundationalism;satisfiability;maxichoix;alchourron c e;croyance;makinson d;fondationnalisme;belief change;nebel b;fuhrmann a;contraction	"""anonymous referee for the Journal of Philosophical Logic for their insightful comments on earlier versions of this paper. I also thank Prasanta Bandy-opadhyay, Jim Cain, Francis Jeery Pelletier and Ted Sider for their sugges tions and encouragement, and acknowledge the help of my friend Ramkrish-nan Nambimadom in the construction of a crucial proof. Abstract: This paper is concerned with the construction of a base contraction (revision) operation such that the theory contraction (revision) operation generated by it will be fully AGM-rational. It is shown that the theory contraction operation generated by Fuhrmann's minimal base contraction operation , even under quite strong restrictions, fails to satisfy the \supplementary postulates"""" of belief contraction. Finally Fuhrmann's construction is appropriately modiied so as to yield the desired properties. The new construction may be described as involving a modiication of safe (base) contraction so as to make it maxichoice. We often change our beliefs. We learn new things, occasionally things that connict with our current beliefs. On such occasions new beliefs replace the old ones. It is as if this process is completed in two steps: (1) rst we identify and throw out the beliefs that connict with the new information and then (2) we accept the new information. In the literature (1) is referred to as the problem of \belief contraction"""", and (2) as the problem of \belief expansion"""". The combination of (1) followed by (2) is called \belief revision"""". Though this account of belief change is very intuitive, 1 its logic is not understood very well. If it is assumed that a rational epistemic (doxastic) agent wants to minimize unnecessary loss of information, then belief contraction becomes a very diicult problem. 2 This paper is about belief revision seen from a foundationalist perspective. We show that in order to satisfy GG ardenfors postulates on belief revision in a founda-tionalist framework, we need to radically revise Fuhrmann's Fuhrmann 91] construction of a reject-set. This result makes an interesting connection between Nebel's Nebel 89] theory revision based on maxichoice base revision and his Nebel 91] unambiguous partial meet revision. This contraction operation also bridges the gap between Fuhrmann's Fuhrmann 88, Fuhrmann 91] minimal theory revision approach and Nebel's approach. 1 Background We assume that the objects of beliefs are propositions, which may be represented as equivalence classes of sentences, or as sets of worlds. The believer's language L is assumed to contain the usual propositional …"""	belief revision;doxastic logic;francis;gadu-gadu;nan;norm (social);ted;turing completeness	Abhaya C. Nayak	1994	J. Philosophical Logic	10.1007/BF01049408	foundationalism;philosophy;epistemology;artificial intelligence;belief;contraction;mathematics;linguistics;algorithm;satisfiability	AI	-12.520227953108641	5.257321298897882	25309
05301ce5f4fffac1082082beee04bdec500f3f21	performance evaluation of new clustering algorithm in object-oriented database systems	cluster algorithm;performance evaluation;object database;software engineering;object oriented database systems;object oriented database;dynamic adaptation;database management system;hierarchical model	It is widely acknowledged that clustering is a cornerstone for the performance of database management systems. In particular, object-oriented databases have properties which may compromise the effectiveness of a clustered structure, i.e., object updates and multiple relationships. The clustering problem consists in finding a partition of the set of the objects in the database. Our approach is similar to the one adopted in Schkolnick [1] for the hierarchical model. When objects are grouped together, it is desirable that these objects be accessed together in the future. We have an operational approach: we will evaluate the benefits of our clustering strategy and show how to dynamically adapt it.	algorithm;cluster analysis;performance evaluation	Vlad Ingar Wietrzyk	1996		10.1007/BFb0034718	database theory;intelligent database;semi-structured model;database tuning;computer science;data science;database model;data mining;database;view;database schema;physical data model;database testing;database design;hierarchical database model;spatiotemporal database;component-oriented database	DB	-33.415199580693354	12.92538450252635	25376
cb7293138e9d988bb74d3ba6e026dabc178bdb9c	revisiting the cardinality reasoning for binpacking constraint	bin packing;constraint programming;global constraints	In a previous work, we introduced a filtering for the BinPacking constraint based on a cardinality reasoning for each bin combined with a global cardinality constraint. We improve this filtering with an algorithm providing tighter bounds on the cardinality variables. We experiment it on the Balanced Academic Curriculum Problems demonstrating the benefits of the cardinality reasoning for such bin-packing problems.	algorithm;cardinality (data modeling);computation;gnu compiler collection;requirement;set packing	François Pelsser;Pierre Schaus;Jean-Charles Régin	2013		10.1007/978-3-642-40627-0_43	mathematical optimization;constraint programming;combinatorics;discrete mathematics;bin packing problem;computer science;mathematics	AI	-15.570795705087216	24.201904848933246	25384
269ce631ad7f90c8f06de66cc3cd372f2f2dd408	on the complexity of input output conformance testing	computer and information science;data och informationsvetenskap	Input-output conformance (ioco) testing is a well-known approach to model-based testing. In this paper, we study the complexity of checking ioco. We show that the problem of checking ioco is PSPACEcomplete. To provide a more efficient algorithm, we propose a more restricted setting for checking ioco, namely with deterministic models and show that in this restricted setting ioco checking can be performed in polynomial time.	algorithm;coinduction;conformance testing;input/output;model-based testing;pspace-complete;refinement (computing);simulation;time complexity	Neda Noroozi;Mohammad Reza Mousavi;Tim A. C. Willemse	2013		10.1007/978-3-319-07602-7_18	computer science;theoretical computer science;algorithm	Logic	-10.511579049218884	26.579827805241706	25482
4d23990c766a3697fcdfaf28e467f2e0f7b718f4	experience with the universal intermediate language janus	janus;portability;intermediate language;abstract machine model;uncol	Janus k a symbolic language intended for use as an intermediate language in the transportation of software. Since its initial design four years ago, it has been used to implement a portable Pascal compiler, in the design of an Algol 68 compiler, and to realize a portable package of mathematical routines. These experiences, together with a critical re-evaluation of the design criteria, have led to some modification of the specillcations of Janus and an increased confidence in the viability of the approach. They have also indicated some problems yet to be solved. This paper reviews the significant lessons which we have learned, and quotes some results which support our confidence.	algol 68;compiler;janus	Bruce K. Haddon;William M. Waite	1978	Softw., Pract. Exper.	10.1002/spe.4380080509	simulation;computer science;engineering;operating system;programming language;intermediate language	PL	-28.546284167487016	23.485105643327593	25507
9b34adc926897db621c6badfdd634a6b773f6901	fault isolation by test scheduling for embeded systems using probabilistics approach	probability;software fault tolerance;embedded systems;program testing;scheduling;operating systems computers	This paper deals with the isolation of the failed components in the system. Each component can be affected in a random way by failures. The detection of the state of a component or a subsystem is carried out using tests. The objective of this research is to exploit the techniques of built in test and available knowledge to generate the sequence of tests which makes it possible to locate quickly the whole of the components responsible for the failure of the system. One considers an operative system according to a series structure for which one knows the cost of tests and the conditional probability that a component is responsible for the failure. The various strategies of diagnosis are analyzed. The treated algorithms call upon the probabilistic analysis of the systems.	algorithm;exploit (computer security);operating system;probabilistic analysis of algorithms;scheduling (computing)	Daoud Aït-Kadi;Zineb Simeu-Abazi;Ahmed Arous	2013	Proceedings of 2013 International Conference on Industrial Engineering and Systems Management (IESM)	10.1007/s10845-015-1088-7	reliability engineering;real-time computing;computer science;engineering;probability;distributed computing;scheduling;software fault tolerance	Robotics	-5.049562523650679	31.011513548355726	25513
37c32f7c0c8cd5f676ad823e467f25ad12a4e8b0	a parallel execution of functional logic language with lazy evaluation	lazy evaluation		lazy evaluation	Jong H. Nang;D. W. Shin;Seungryul Maeng;Jung Wan Cho	1992			lazy initialization;strict programming language;programming language;graph reduction;eager evaluation;logic programming;computer science;lazy evaluation	PL	-21.250622654466834	23.048346612375276	25520
ca605980a124c77829133f9e9d0ffc9597ace5df	generating types is better than programming them.		Many frameworks have been proposed for deriving speciic runtime properties, but there is still a lack of knowledge about how they all t together. To ll this gap, we discuss various kinds of type information and the relations between them, and sketch a method for obtaining the required information. This method is used as part of an system for automatic translation of Prolog programs into strongly typed languages such as PROTOS-L or G odel.	machine translation;prolog;strong and weak typing	Rayk Fenske;Ulrich Geske;Mario Lenz	1995			inductive programming;programming language;functional reactive programming;reactive programming;computer science	PL	-23.76535294737992	22.948071661506095	25523
304ae85393711506fdbcd5cf0251fb7b06bd946c	probabilistic knowledge representation and reasoning at maximum entropy by spirit	directed acyclic graph;knowledge representation and reasoning;propositional logic;knowledge acquisition;probability distribution;conditional probability;maximum entropy;knowledge base;expert system	Current probabilistic expert systems assume complete knowledge of the joint distribution. To specify this distribution one has to construct a directed acyclic graph attached by a lot of tables filled with conditional probabilities. Often these probabilities are unknown and the quantification is more or less arbitrary. SPIRIT is an expert system shell for probabilistic knowledge bases which uses the principle of maximum entropy to avoid these lacks. Knowledge acquisition is performed by specifying probabilistic facts and rules on discrete variables in an extended propositional logic syntax. The shell generates the unique probability distribution which respects all facts and rules and maximizes entropy. After creating this distribution the shell is ready for answering simple and complex queries. The process of knowledge acquisition, knowledge processing and answering queries is revealed in detail on a nontrivial example.	knowledge representation and reasoning	Carl-Heinz Meyer;Wilhelm Rödder	1996		10.1007/3-540-61708-6_67	knowledge representation and reasoning;machine learning;pattern recognition;bayesian network;data mining;mathematics;reasoning system;autoepistemic logic	AI	-19.08486101262938	7.313052125765079	25551
526026bdadc131331d1aebe2ccfa7de09a654eb6	review - access path selection in a relational database management system	relational database management system		relational database management system	Laura M. Haas	1999	ACM SIGMOD Digital Review		data mining;relational database management system;relational model;database schema;database;sql;object-relational impedance mismatch;relational database;database model;computer science;database design	DB	-31.265926033488064	8.901426009503034	25587
b0cad489983b50aebdecff13dbfe50f027d7127c	weighted asynchronous cellular automata	power series;asynchronous automaton;formal series;theorie trace;monoid;automate asynchrone;comportement systeme;serie formelle;asynchrone;automata asincrono;monoide commutatif;metodo secuencial;sequential method;serie entiere;monoide;weighted automata;expressive power;puissance expressive;weighted automaton;formal power series;trace theory;serie potencias;informatique theorique;automate cellulaire;methode sequentielle;cellular automata;cellular automaton;asincrono;asynchronous;computer theory;automata celular;automate pondere;serie formal;informatica teorica	We study weighted distributed systems whose behavior can be described as a formal power series over a free partially commut ative or trace monoid. We characterize the behavior of such systems both, in the det erministic and in the non-deterministic case. As a consequence, we obtain a parti cul ly simple class of sequential weighted automata that have already the full e xpressive power.	automata theory;cellular automaton;distributed computing;finite-state transducer;trace monoid	Dietrich Kuske	2007	Theor. Comput. Sci.	10.1016/j.tcs.2006.11.031	cellular automaton;combinatorics;discrete mathematics;asynchronous communication;mathematics;formal power series;monoid;power series;trace theory;expressive power;algorithm	Logic	-5.341816527111577	22.16011517190809	25602
127bb49395488f5b921bdfbc76d026376039516f	explanatory relations based on mathematical morphology	mathematical morphology;morfologia matematica;logic;satisfiability;raisonnement;artificial intelligent;erosion;razonamiento;reasoning;cumulant;logique;logica;morphologie mathematique	Using mathematical morphology on formulas introduced recently by Bloch and Lang (Proceedings of IPMU’2000) we define two new explanatory relations. Their logical behavior is analyzed. The results show that these natural ways for defining preferred explanations are robust because these relations satisfy almost all postulates of explanatory reasoning introduced by Pino-Pérez and Uzcátegui (Artificial Intelligence, 111:131–169, 1999). Actually, the first explanatory relation is Explanatory-Rational. The second one is not even ExplanatoryCumulative but it satisfies new weak postulates.	artificial intelligence;bloch sphere;erosion (morphology);mathematical morphology;pino;robustness (computer science);traffic collision avoidance system;whole earth 'lectronic link	Isabelle Bloch;Ramón Pino Pérez;Carlos Uzcátegui	2001		10.1007/3-540-44652-4_65	mathematical morphology;erosion;artificial intelligence;mathematics;logic;algorithm;cumulant;satisfiability	AI	-13.531918673762041	9.51124490383648	25632
e251f46d78eac53b6a8e7089b440213dadef34f2	rewriting systems with a clocking mechanism	rewrite systems	A new type of rewriting system is introduced in which every string in a derivation is precede-d by one (clocking) symbol which must be rewritten at every moment of time, (hence in a parallel mode). The rest of the string is rewritten in a sequential mode, (i.e., one symbol at every moment of time). The commnnication from the clocking symbol to the rest of the string, and vice versa, is achieved by incorporating various types of context conditions in every production.	clock rate;race condition;rewriting	Grzegorz Rozenberg;Sebastiaan H. von Solms	1978	Inf. Sci.	10.1016/0020-0255(78)90044-0	real-time computing;computer science;theoretical computer science;mathematics;algorithm	Theory	-18.916755693354858	30.966814542952573	25639
23bc51bdf67ee843f3f97964e2d72b194fa2ccfb	finite state description of communication protocols	communication protocol	Abstract   A finite state model for the specification and validation of communication protocols is considered. The concept of “direct coupling” between interactiing finite state components is used to describe a hierarchical structure of protocol layers. The paper discusses different aspects of protocol validation, some verification tools based on the finite state formalism, and the basic limitations of the finite state modelling of protocols. An “empty medium abstraction” is proposed for reducing the complexity of the overall system description. The concept of “adjoint states” can be useful for summarizing the relative synchronization between the communicating system components. These concepts are applied to the analysis of a simple alternating bit protocol, and to the X.25 call set-up and clearing procedures. The analysis of X.25 shows that the procedures are stable in respect to intermittant perturbations in the synchronization of the interface introduced for different reasons, including occasional packet loss. However, on very rare occasions, an undesirable cyclic behaviour could be encountered.		Gregor von Bochmann	1978	Computer Networks	10.1016/0376-5075(78)90015-6	communications protocol;universal composability;computer science;theoretical computer science;distributed computing;algorithm;computer network	ML	-32.480764272146764	31.461130184548992	25647
beaaef6879ccb89f27175f76977988f61e9162e7	a digital system design language (ddl)	design automation;system modeling;automata boolean equations computer design declarations design automation design language digital systems syntax and semantics system design system model;boolean equations;index terms 8212;indexing terms;syntax and semantics;level of detail;design language;system design;digital systems;index terms automata boolean equations computer design declarations design automation design language digital systems syntax and semantics system design system model;system model;declarations;computer design;index terms automata	Abstract—Successful design and manufacture of future digital systems will depend upon the availability of a suitable design language. A precise, concise language is presented which facilitates the specification of complex digital systems. The language 1) is independent of any particular technology, design procedure, machine organization, etc., 2) allows specification at different levels of detail from architecture to detailed Boolean equations, and 3) may be com- piled into manufacturing information. Its syntax and semantics per- mit documents with an organization which parallels the block struc- ture of the systems they specify.	digital electronics;parallels desktop for mac	James R. Duley;Donald L. Dietmeyer	1968	IEEE Transactions on Computers	10.1109/TC.1968.229145	boolean algebra;systems modeling;index term;specification language;electronic design automation;system model;computer science;theoretical computer science;level of detail;design language;programming language;algorithm;systems design	EDA	-14.289441236624185	30.760158199690633	25661
c269e18ea2b4bc85e8f7a984df65def7cfcf7777	friends need a bit more: maintaining invariants over shared state	developpement logiciel;theorie type;state dependence;formal verification;object oriented;desarrollo logicial;type theory;software development;invariante;oriente objet;verification formelle;orientado objeto;invariant	A friendship systemis introduced for modular static verification of object invariants. It extends a previous methodology, base d on ownership hierarchy encoded in auxiliary state, to allow for state dependenc e across ownership boundaries. Friendship describes a formal protocol for a g anting classto grant a friend classpermission to express its invariant over fields in the granti ng class. The protocol permits the safe update of the granter’s fields w ithout violating the friend’s invariant. The ensuing proof obligations are mini al and permit many common programming patterns. A soundness proof is sketched . The method is demonstrated on several realistic examples, showing that i t significantly expands the domain of programs amenable to static verification.	fits;formal specification;friend class;prith banerjee;programmer;static program analysis;verification and validation	Michael Barnett;David A. Naumann	2004		10.1007/978-3-540-27764-4_5	discrete mathematics;formal verification;computer science;artificial intelligence;software development;invariant;database;mathematics;distributed computing;programming language;object-oriented programming;computer security;type theory;algorithm	PL	-22.769995469822696	29.58520706535694	25752
f0e9ac4008d4caee2966f428cbac5cd48416f095	using functional programming for development of distributed, cloud and web applications in f#		in English— In this paper, we argue that modern functional programming languages – in particular, F# on the .NET platform – are well suited for the development of distributed, web and cloud applications on the Internet. We emphasize that F# can be successfully used in a range of scenarios – starting from simple ASP.NET web applications, and including cloud data processing tasks and data-driven web applications. In particular, we show how some of the F# features (eg. quotations) can be effectively used to develop a distributed web system using single code-base, and describe the commercial WebSharper project by Intellifactory for building distributed client-server web applications, as well as research library that uses Windows Azure for parametric sweep computational tasks.	.net framework;asp.net;byte;client–server model;cloud computing;debugging;distributed computing;functional approach;functional programming;garbage collection (computer science);imperative programming;internet;library;microsoft azure;problem domain;programming language;programming paradigm;real life;server (computing);web application;web development;websharper	Dmitri Soshnikov	2015	CoRR		web service;ajax;web development;web modeling;simulation;data web;web mapping;web-based simulation;web design;computer science;theoretical computer science;distributed web crawling;cloud testing;programming language;web 2.0;world wide web;mashup	OS	-32.237007415588835	27.060149316454922	25755
e8fa93f98df90cf3595f15f71bf63d0ab2fd45c4	viper: a verification infrastructure for permission-based reasoning		The automation of verification techniques based on firstorder logic specifications has benefited greatly from verification infrastructures such as Boogie and Why. These offer an intermediate language that can express diverse language features and verification techniques, as well as back-end tools such as verification condition generators. However, these infrastructures are not well suited for verification techniques based on separation logic and other permission logics, because they do not provide direct support for permissions and because existing tools for these logics often prefer symbolic execution over verification condition generation. Consequently, tool support for these logics is typically developed independently for each technique, dramatically increasing the burden of developing automatic tools for permission-based verification. In this paper, we present a verification infrastructure whose intermediate language supports an expressive permission model natively. We provide tool support, including two back-end verifiers, one based on symbolic execution, and one on verification condition generation; this facilitates experimenting with the two prevailing techniques in automated verification. Various existing verification techniques can be implemented via this infrastructure, alleviating much of the burden of building permissionbased verifiers, and allowing the developers of higher-level techniques to focus their efforts at the appropriate level of abstraction.	experiment;separation logic;symbolic execution	Peter Müller;Malte Schwerhoff;Alexander J. Summers	2017		10.3233/978-1-61499-810-5-104	software verification;computer science;theoretical computer science;runtime verification;programming language;intelligent verification;algorithm;functional verification	SE	-19.681336645685022	27.143665120046517	25764
332883a900617b87335f3fb45f1aa6fee3622f97	indexing and retrieval of xml-encoded structured documents in dynamic environment	inverted index;information retrieval;xml language;acceso directo;fichier inverse;index structure;indexing and retrieval;dynamic environment;indexing;recherche information;inverted file;acces direct;indexation;estructura datos;indizacion;direct access;archivo invertido;structure donnee;recuperacion informacion;structured documents;data structure;langage xml;lenguaje xml	In order to retrieve structured documents efficiently, many researches have been done to design indexing technique that supports fast and direct access for arbitrary element as well as whole document. On the other hand, fast and efficient indexing technique for supporting dynamic update of structured documents in business domain is required. In this paper, we propose an inverted index structure that supports dynamic update, such as including both structure and content updates, quickly. In the proposed index structure, in addition to a horizontal term-based index as in general inverted file structure, we add a vertical index. The vertical index uses element identifier as key. Using this dual index structure, it is possible to support fast and efficient updates on the parts of a document as well as whole document as reducing reindexed space and time dramatically.	xml	Sung Wan Kim;Jaeho Lee;Hae Chull Lim	2002		10.1007/3-540-45785-2_11	inverted index;index term;data structure;computer science;database;programming language;world wide web;information retrieval	Web+IR	-30.918177204677722	5.441315765279074	25778
bd25786fc6a64531249eb03f91227dc5567a06b4	a deductive extension for odmg compliant object databases	object database		object data management group	Pedro R. Falcone Sampaio;Norman W. Paton	1999	L'OBJET		computer science;database;programming language;object definition language	Robotics	-31.381724724506704	9.878689977310932	25820
98dc9d6e81ac5b4ddd2583e23821830d19a89738	a generalized approach to verification condition generation		In a world where many human lives depend on the correct behavior of software systems, program verification assumes a crucial role. Many verification tools rely on an algorithm that generates verification conditions (VCs) from code annotated with properties to be checked. In this paper, we revisit two major methods that are widely used to produce VCs: predicate transformers (used mostly by deductive verification tools) and the conditional normal form transformation (used in bounded model checking of software). We identify three different aspects in which the methods differ (logical encoding of control flow, use of contexts, and semantics of asserts), and show that, since they are orthogonal, they can be freely combined. This results in six new hybrid verification condition generators (VCGens), which together with the fundamental methods constitute what we call the VCGen cube. We consider two optimizations implemented in major program verification tools and show that each of them can in fact be applied to an entire face of the cube, resulting in optimized versions of the six hybrid VCGens. Finally, we compare all VCGens empirically using a number of benchmarks. Although the results do not indicate absolute superiority of any given method, they do allow us to identify interesting patterns.	algorithm;benchmark (computing);bridging (networking);categorization;control flow;formal verification;lambda cube;model checking;predicate transformer semantics;software system;solver;specification language;traceability;transformers;turing completeness;verification and validation	Cláudio Belo Lourenço;Maria João Frade;Shin Nakajima;Jorge Sousa Pinto	2018	2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC)	10.1109/COMPSAC.2018.00032	model checking;real-time computing;software system;cube;theoretical computer science;software;control flow;encoding (memory);semantics;computer science;bounded function	Logic	-20.49142437982539	27.493703638781955	25848
d857984f158030c9ad17451b7b61a3c47532f5b3	basic hoops: an algebraic study of continuous t -norms	total order;logic;mathematical logic and foundations;philosophy;computational linguistics;complete lattice	A continuous t-norm is a continuous map * from [0, 1]2 into [0, 1] such that \(\langle [0, 1], *, 1 \rangle\) is a commutative totally ordered monoid. Since the natural ordering on [0, 1] is a complete lattice ordering, each continuous t-norm induces naturally a residuation → and \(\langle [0, 1], *,\rightarrow, 1\rangle\) becomes a commutative naturally ordered residuated monoid, also called a hoop. The variety of basic hoops is precisely the variety generated by all algebras \(\langle [0, 1], *,\rightarrow, 1\rangle\) , where * is a continuous t-norm. In this paper we investigate the structure of the variety of basic hoops and some of its subvarieties. In particular we provide a complete description of the finite subdirectly irreducible basic hoops, and we show that the variety of basic hoops is generated as a quasivariety by its finite algebras. We extend these results to Hajek’s BL-algebras, and we give an alternative proof of the fact that the variety of BL-algebras is generated by all algebras arising from continuous t-norms on [0, 1] and their residua. The last part of the paper is devoted to the investigation of the subreducts of BL-algebras, of Godel algebras and of product algebras.		P. Aglianò;Isabel M. A. Ferreirim;Franco Montagna	2007	Studia Logica	10.1007/s11225-007-9078-1	discrete mathematics;topology;complete lattice;philosophy;epistemology;computational linguistics;pure mathematics;mathematics;logic;total order;algebra	DB	-7.875862218621227	11.57085748411447	25879
33924cdf36dce2e971f702b7cb272dad1234c7a4	preface: twenty years of the qed manifesto	qed;article letter to editor;formal verification;formalization of mathematics	QED is the very tentative title of a project to build a computer system that effectively represents all important mathematical knowledge and techniques. The QED system will conform to the highest standards of mathematical rigor, including the use of strict formality in the internal representation of knowledge and the use of mechanical methods to check proofs of the correctness of all entries in the system. . .		John Harrison;Josef Urban;Freek Wiedijk	2016	J. Formalized Reasoning	10.6092/issn.1972-5787/5974	q.e.d.;formal verification;computer science;mathematics;programming language;algorithm	AI	-21.08440008623614	18.422578715135312	26023
1d9cfc95c4bc5cea190f94802623b36e26c4f6d4	typed feature structures as descriptions	computational benefit;satisfiable feature structure;explicit interpretation;effective algorithm;typed feature structure;feature structure;satisfiability	A description is an entity that can be interpreted as true or false of an object, and using feature structures as descriptions accrues several computational benefits. In this paper, I create an explicit interpretation of a typed feature structure used as a description, define the notion of a satisfiable feature structure, and create a simple and effective algorithm to decide if a feature structure is satisfiable.	algorithm	Paul John King	1994			computer science;theoretical computer science;machine learning;linguistics;feature;algorithm;satisfiability	AI	-15.621632978622761	6.649367010593567	26031
8ad88b5b9ce8dd66d1b1c4f5bc6550f7dfb7601c	object specification logic	first order	 A logic for specifying and reasoning about object classes and their instances(aspects) is presented and illustrated. This logic is an extensionof a rather standard linear temporal, many-sorted, first-order predicatelogic with equality. The extensions where designed to be as simple aspossible while supporting the envisaged locality of arguments, object specializationand object aggregation. Objects are specified through theiraspects. Each aspect establishes a local vocabulary (signature).... 		Amílcar Sernadas;Cristina Sernadas;José Félix Costa	1995	J. Log. Comput.	10.1093/logcom/5.5.603	dynamic logic;description logic;logic optimization;horn clause;interval temporal logic;computer science;first-order logic;programming language;multimodal logic;language of temporal ordering specification;object definition language	Logic	-15.828471920008731	17.061738354241665	26065
64eeab699153c9457b5c3e9d5a320f33a3bbc490	relational joins on non-keyed fields (abstract)	first order predicate logic;prolog;logic data base;data base view;query by example;group	Within a relational database system, the join operator is needed to support an association Between two relations. Creating a join, however, can be costly in terms of computing time, especially if the fields used for the join are not keyed fields. In some relational systems, as the RDB system developed at St. Norbert ColleRe, all fields of a relation are stored in a tuple file. In addition to this tuple file, each keyed field is stored in an index for fast access to field values and tuples containing these values. For non-keyed fields, however, the field values are stored only in the tuple file and in no other structure. Searchin~ then, on a non-keyed field is slow and requires a sequential search of the tuple file. Creating a join on two non-keyed fields results in excessive I/O's as shown in the following algorithms, and results in (#Ri * #R2) + #matches logical I/O's.	algorithm;amiga rigid disk block;input/output;join (sql);key (cryptography);linear search;relational database management system	James F. Blahnik	1986		10.1145/324634.325021	dynamic logic;description logic;relational model;higher-order logic;horn clause;computer science;query by example;theoretical computer science;predicate functor logic;first-order logic;database;predicate variable;group;programming language;prolog;logic programming	DB	-28.29378862165261	7.098749194603611	26161
0b6f74caaf1952ff0e30718d8ed2d66d1a457201	the genesis of possible worlds semantics	kripke s a;systeme s5;prior a n;carnap r;semantics;geach p t;quantification;logique deontique;semantique;deontic logic;hintikka j;modal logic;wittgenstein l;logique modale;mondes possibles;system s5;tarski a;possible worlds;von wright g h;montague r	This article traces the development of possible worlds semantics through the work of: Wittgenstein, 1913–1921; Feys, 1924; McKinsey, 1945; Carnap, 1945–1947; McKinsey, Tarski and Jónsson, 1947–1952; von Wright, 1951; Becker, 1952; Prior, 1953– 1954; Montague, 1955; Meredith and Prior, 1956; Geach, 1960; Smiley, 1955–1957; Kanger, 1957; Hintikka, 1957; Guillaume, 1958; Binkley, 1958; Bayart, 1958–1959; Drake, 1959–1961; Kripke, 1958–1965.	donald becker;genesis;kripke semantics;montague grammar;possible world;tracing (software)	B. Jack Copeland	2002	J. Philosophical Logic	10.1023/A:1015273407895	modal logic;philosophy;epistemology;artificial intelligence;deontic logic;mathematics;semantics;possible world;algorithm	AI	-13.061415886265372	10.68161356977518	26183
eb68008cc7595bbe9f4843a599a185b93a987bc4	multi-diagrams of relations between fuzzy sets: weighted limits, colimits and commutativity		Limits and colimits of diagrams, defined by maps between sets, are universal constructions fundamental in different mathematical domains and key concepts in theoretical computer science. Its importance in semantic modeling is described by M. Makkai and R. Paré in [1], where it is formally shown that every axiomatizable theory in classical infinitary logic can be specified using diagrams defined by maps between sets, and its models are structures characterized by the commutativity, limit and colimit of those diagrams. Z. Diskin in [2], taking a more practical perspective, presented an algebraic graphic-based framework for data modeling and database design. The aim of our work is to study the possibility of extending these algebraic frameworks to the specification of fuzzy structures and to the description of fuzzy patterns on data. For that purpose, in this paper we describe a conservative extension for the notions of diagram commutativity, limit and colimit, when diagrams are constructed using relations between fuzzy sets, evaluated in a multi-valued logic. These are used to formalize what we mean by “a relation R is similar to a limit of diagram D,” “a similarity relation S is identical to a colimit of diagram D colimit,” and “a diagram D is almost commutative.”	data modeling;database design;diagram;fuzzy logic;fuzzy set;linear algebra;machine learning;map;theoretical computer science	Carlos Leandro;Luís Monteiro	2016	CoRR		combinatorics;discrete mathematics;direct limit;mathematics;algorithm	AI	-13.379823517962075	8.187175875423012	26187
9f8d486c5bd9d94732a08e8c4731145c6a49f62e	logic between expressivity and complexity	automated deduction;universiteitsbibliotheek;expressive power;dynamic logic;modal logic;computational logic	Automated deduction is not just application or implementation of logical systems. The field of computational logic also poses deep challenges to our understanding of logic itself. I will discuss some key issues. This text is just an appetizer that will be elaborated in the lecture. 1 Logic and the Balance of Expressive Power and Computational Complexity Defining and proving/computing are the main faces of logic. But they require a balance. Historically, first-order logic arose from type theory by giving up expressive power in order to gain axiomatizability (and better semantic transfer properties between models). The same move occurred a bit later in going from first-order logic to modal languages: one gives up yet more expressive power, but now one gains decidability (as well as discovering a new nice structural invariance for the modal language: viz. bisimulation). 2 Upward from Modal to Guarded Fragments What makes the modal move to weaker languages tick? Did we go too fast? Essentially, standard modal operators are local guarded quantifiers of the special first-order form ∃y(G(x, y) & φ(x, y)), where G is an atomic guard predicate, and the x, y are finite tuples of variables. Restricting quantifiers to only these forms defines the Guarded Fragment (GF). Theorem 1. GF is decidable, with an effective finite model property. Up to logical equivalence, GF is also the set of first-order formulas that are invariant for guarded bisimulation, a structural invariance that lies in between bisimulation and (potential) isomorphism. But the border with complex behaviour lies still a bit higher up inside firstorder logic. Decidability continues to hold for the ‘Loosely Guarded Fragment’ that allows conjunctions of guard atoms &G: ∃y(&G(x, y) & φ(x, y)), where any two variables in x, y occur under at least one atom in &G. J. Giesl and R. Hähnle (Eds.): IJCAR 2010, LNAI 6173, pp. 122–126, 2010. c © Springer-Verlag Berlin Heidelberg 2010 Logic between Expressivity and Complexity 123 Beyond this lie the ‘cliffs of complexity’: quantifiers expressing well-known confluence (grid) properties are not loosely guarded, think of ∀yz((R(x, y) & R(x, z)) → ∃u(R(y, u) & R(z, u))). These can encode Tiling Problems, and so their logic becomes undecidable. Infinitary Second-order Fixed-point logics highly complex, non-RE . . . . . . FOL RE, undecidable GF decidable, NEXPtime ML decidable, Pspace Aside (restricting a language versus re-interpretation): modal and related moves in logic have two faces. We either restrict to fragments, or we interpret all of FOL in some suitable generalized semantics, where not all assignments of objects to variables are available, encoding ‘dependencies’ in the model. 3 Aside: Downward to ‘Poor Man’s Logics’ Modal logics tend to be Pspace-complete. But this is not rock bottom yet. Going down even fruther to feasible logics with (N)Ptime satisfiability problems often takes non-Boolean languages. Open Problem. Find a principled logical analysis for this move. 4 Model Theory in the Small: Lindström Theory Our style of analysis multiplies logics. So, how can we understand the landscape of possible logical systems in greater generality? Theorem 2 (Lindström). FOL is inclusion-maximal with respect to the Compactness and Löwenheim-Skolem properties. For the latter one can also choose: the Karp Property (invariance of all sentences for potential isomorphisms), the RE property (axiomatizability of the valid sentences). Traditionally, foundational attention has only been paid to extensions of firstorder logic (second-order, infinitary logics). Many characterizations exist, mostly via model-theoretic properties. But what happens if we look down in the landscape below FOL, on the idea that ‘Small is Beautiful’? Proof methods in Lindström theory require explicit encoding for back-andforth properties of partial isomorphisms that capture first-order expressive power. But these are typically non-guarded grid properties. Still, new methods have been developed that work for small languages: Theorem 3. ML is maximal with respect to the properties of Compactness and Invariance for Bisimulation.	appetizer;automated theorem proving;bisimulation;boolean satisfiability problem;computational complexity theory;computational logic;confluence;encode;finite model property;first-order logic;first-order predicate;grammatical framework;guard digit;guarded command language;international joint conference on automated reasoning;lecture notes in computer science;list of code lyoko episodes;maximal set;modal logic;modal operator;nexptime;natural deduction;pspace-complete;springer (tank);tiling window manager;turing completeness;type theory;undecidable problem;viz: the computer game	Johan van Benthem	2010		10.1007/978-3-642-14203-1_10	modal logic;dynamic logic;natural language processing;linear temporal logic;description logic;logic optimization;higher-order logic;paraconsistent logic;horn clause;computer science;theoretical computer science;bunched logic;computational logic;automated reasoning;substructural logic;multimodal logic;expressive power;algorithm;philosophy of logic;autoepistemic logic	Logic	-9.911696311565775	16.05269670343206	26211
033eac01d20f31b33a062ed1f4de80a9bbdda8f9	explanation-based learning of open textured predicates in logic programming models of law			explanation-based learning;logic programming	S. Bertarello;Stefania Costantini;Gaetano Aurelio Lanzarone	1994			abductive logic programming;programming language;inductive programming;theoretical computer science;functional logic programming;predicate (grammar);explanation-based learning;logic programming;computer science	Vision	-18.992370633194614	11.60079356284086	26266
8f75f4cdac0f6e06c88b27966e44cad37f840d16	disjunctive temporal reasoning in partially ordered models of time	distributed system;total order;time scale;constraint satisfaction;computational complexity;temporal reasoning;partial order	Certain problems in connection with, for example, cooperating agents and distributed systems require reasoning about time which is measured on incomparable or unsynchronized time scales. In such situations, it is sometimes approporiate to use a temporal model that only provides a partial order on time points. We study the computational complexity of partially ordered temporal reasoning in expressive formalisms consisting of point algebras extended with disjunctions. We show that the resulting algebra for partially ordered time contains four maximal tractable subclasses while the equivalent algebra for total-ordered time contains two.	cobham's thesis;computational complexity theory;disjunctive normal form;distributed computing;maximal set	Mathias Broxvall;Peter Jonsson	2000			partially ordered set;constraint satisfaction;computer science;computational complexity theory;total order;algorithm	AI	-15.82196442901827	12.878280452660801	26278
57733531d3037e97a37dd46cf39ef946593a67a1	decidability of contextual reasoning	propositional logic	Contexts were first suggested in McCarthy’s Turing Award Paper, (McCarthy 1987), as a possible solution to the problem of generality in AI. McCarthy’s concern with the existing AI systems has been that they can reason only about some particular, predetermined task. When faced with slightly different circumstances they need to be completely rewritten. In other words, AI systems lack generality. Cyc (Guha & Lenat 1990), a large common-sense knowledge-base currently being developed at MCC, is one example of where contexts have already been put to use in attempt to solve the problem of generality. Because of the complexity of the problem of generality, it has been speculated that any reasoning system which would be able to solve this problem would itself be computationally unacceptable. The purpose of this paper is to show that propositional contextual reasoning is decidable.	acm turing award;cyc;reasoning system	Vanja Buvac	1994			propositional formula;dynamic logic;zeroth-order logic;decidability;classical logic;resolution;interval temporal logic;intuitionistic logic;computer science;intermediate logic;propositional variable;well-formed formula;propositional calculus;deductive reasoning;default logic;algorithm;autoepistemic logic	AI	-16.354105938324356	8.360174544668228	26374
a34b6d5baca65a2532df9e4a36661a83823b39a3	searching while keeping a trace: the evolution from satisfiability to knowledge compilation	compilacion;representacion conocimientos;ingenierie connaissances;satisfiabilite;satisfiability;real world application;knowledge compilation;representation connaissance;component analysis;compilation;short period;knowledge representation;satisfactibilidad;exhaustive search;knowledge base;knowledge engineering	Satisfiability testing has seen significant growth over the last decade, leading to orders of magnitude improvements in performance over a relatively short period of time. State of the art algorithms for satisfiability, which are based on DPLL search, were originally meant to find a single satisfying assignment, but their scope has been extended recently to perform exhaustive searches for the purpose of counting and enumerating models. Moreover, the algorithms have been augmented with sophisticated techniques, such as component analysis and formula caching, which are critical to their performance in real-world applications. In a parallel thread of developments, work has been commencing in the area of knowledge compilation, which aims at converting knowledge bases into tractable representations that allow some hard operations to be performed in polytime on the compiled representations. Work in this area has lead to the identification of a comprehensive taxonomy of tractable languages that explicates their relative succinctness and the polytime operations they support.	knowledge compilation	Adnan Darwiche	2006		10.1007/11814771_2	knowledge representation and reasoning;knowledge base;computer science;artificial intelligence;theoretical computer science;knowledge engineering;brute-force search;mathematics;programming language;algorithm;satisfiability	AI	-19.4793247585503	14.758097838246105	26379
42d6963514f1e9ebd9abe7ac99a5f2ba2993ee6c	xquery at your web service	web service description language;xquery;wsdl;web service;interface;web services;xml;tight coupling;modules	XML messaging is at the heart of Web services, providing the flexibility required for their deployment, composition, and maintenance. Yet, current approaches to Web services development hide the messaging layer behind Java or C# APIs, preventing the application to get direct access to the underlying XML information. To address this problem, we advocate the use of a native XML language, namely XQuery, as an integral part of the Web services development infrastructure. The main contribution of the paper is a binding between WSDL, the Web Services Description Language, and XQuery. The approach enables the use of XQuery for both Web services deployment and composition. We present a simple command-line tool that can be used to automatically deploy a Web service from a given XQuery module, and extend the XQuery language itself with a statement for accessing one or more Web services. The binding provides tight-coupling between WSDL and XQuery, yielding additional benefits, notably: the ability to use WSDL as an interface language for XQuery, and the ability to perform static typing on XQuery programs that include Web service calls. Last but not least, the proposal requires only minimal changes to the existing infrastructure. We report on our experience implementing this approach in the Galax XQuery processor.	command-line interface;floor and ceiling functions;java;name binding;random access;software deployment;type system;web services description language;web service;world wide web;xml;xquery	Nicola Onose;Jérôme Siméon	2004		10.1145/988672.988754	web service;web modeling;computer science;ws-policy;web page;database;programming language;ws-i basic profile;world wide web;devices profile for web services;information retrieval	Web+IR	-27.397642062972505	28.26309328039689	26474
3c57d0d05c94793fa08abac621f864519d85844b	modeling structured open worlds in a database system: the fll-approach	database system			Thomas Ludwig	1991	IWBS Report		data modeling;database theory;database model;database;database schema;database design;spatiotemporal database	DB	-31.025292079370796	9.29291040306314	26484
03bfc140b007a8bb2214a3c1954f7c66184280b6	tools for modular programming: finding out what's needed	generalized inverses;top down;regularization;minimization of functionals;normal equations;levels of abstraction;linear operators;least squares splines;interpolating splines;data structure;generalized splines;programming tool;structural properties;projection methods;structured data	Modularity, in our view,1 deals with how to structure programs so that specified purposes are more easily attained. Our current work deals with three purposes—modifiability, reliability, and efficiency—and seeks to identify what structural properties, tools, and language features facilitate the attainment of these purposes. Our approach is to study the structure of three actual programs to see how their structure makes the programs more or less modifiable, reliable, and efficient. The first program is a 1,000 statement PL/I program written in top-down style with levels of abstraction. The second is a large circuit analysis program (AEDCAP). The third is a general purpose package for processing list structured data in a fashion that permits changing details of the data structure representation without revising high level algorithms operating on the data. The case study results will be used to assess the potential utility of possible programming tools and language features. 2	algorithm;data structure;high-level programming language;modular programming;modularity (networks);network analysis (electrical circuits);pl/i;principle of abstraction;programming tool;top-down and bottom-up design	John B. Goodenough	1973		10.1145/800192.805763	mathematical optimization;theoretical computer science;mathematics;algorithm	SE	-32.13981902371931	25.38888415302234	26502
786a67ee3ffe494481550bacb8a3067fcbd95960	declarative parsing and annotation of electronic dictionaries		We present a declarative annotation toolkit based on X ML and PROLOG technologies, and we apply it for annotating the Campe Dictionary to obtain an electronic version in X ML (TEI). For parsing flat structures, we use a very compact grammar formalism called extended definite clause grammars (ED CG’s), which is an extended version of the DCG’s that are well–known from the logic programming language P ROLOG. For accessing and transforming X ML structures, we use the X ML query and transformation language F NQUERY. It turned out, that the declarative approach in P ROLOG is much more readable, reliable, flexible, and faster than an alternative implementation which we had made in JAVA and XSLT for the TEXTGRID community project.	bus (computing);declarative programming;definite clause grammar;dictionary;formal grammar;human-readable medium;java;natural language;parsing;programming language;prolog;requirement;text encoding initiative;transformation language;usability;xslt	Christian Schneiker;Dietmar Seipel;Werner Wegstein;Klaus Prätor	2009			s-attributed grammar;programming language	PL	-27.566160920073738	19.80663258840803	26511
06f4d421f203e8cc26e3fc0ba2c8e506f6596211	high-level language support for programming distributed systems	distributed system;programming language;operating system;process model;high level language;language design	This paper presents a strategy to simplify the programming of heterogeneous distributed systems. Our approach is based on integrating a high-level distributed programming model, called the process model, directly into programming languages. Distributed applications written in such languages are portable across di erent environments, are shorter, and are simpler to develop than similar applications developed using conventional approaches. In this paper, we discuss the process model, and present overviews of Hermes and Concert/C, two languages that implement this model. Hermes is a secure, representation-independent language designed explicitly around the process model. Concert/C is the C language augmented with a small set of extensions to support the process model while allowing reuse of existing C code. Hermes has been prototyped; an implementation of Concert/C is in development.	distributed computing;hermes (programming language);high- and low-level;high-level programming language;process modeling;programming model	Joshua S. Auerbach;David F. Bacon;Arthur P. Goldberg;Germán S. Goldszmidt;Mark T. Kennedy;Andy Lowry;James R. Russell;William Silverman;Robert E. Strom;Daniel M. Yellin;Shaula Yemini	1991		10.1145/962128	fourth-generation programming language;first-generation programming language;natural language programming;real-time computing;language primitive;specification language;programming domain;data control language;computer science;programming language implementation;theoretical computer science;operating system;software engineering;process modeling;database;distributed computing;programming paradigm;low-level programming language;fifth-generation programming language;programming language;programming language specification;high-level programming language	PL	-28.59042403759778	30.680027092016253	26530
ffd7eebb1134d54f95cbd1e57890a1bd7d6690ab	axiomatic theories of partial ground ii				Johannes Korbmacher	2018	J. Philosophical Logic	10.1007/s10992-017-9444-z		Logic	-12.394960326316973	11.664624059828737	26551
621b9daafb3e80c8aac14bed03ad96e6ed09d022	visual specifications for modular reasoning about asynchronous systems	distributed system;systeme reparti;descomposicion grafo;sistema temporizado;programmation modulaire;timed system;modular reasoning;verification modele;programacion modular;program verification;transmision asincronica;asynchronous system;verificacion programa;formal verification;sistema repartido;formal reasoning;message sequence chart;model checking;causalite;systeme temporise;modular programming;asynchronous transmission;verification formelle;transmission asynchrone;verification programme;graph decomposition;decomposition graphe;causality;causalidad	I am ranked 7811 among the top 10,000 most cited authors in Computer Science (among 790,329 authors) by CiteSeerX Scientific Literature Digital Library, 2008. My recent journal publication in JLAP (journal of logic and algebraic programming) is ranked 3rd among the hottest 25 articles of JLAP by ScienceDirect. I would like to note that CiteSeerX ranking is skewed towards common names and my name is unique in this list. The statistics I provide below were therefore obtained by processing the results of the Google-Scholar search engine. My work has an estimated lower bound of 1200 citations. Most of them appear in papers published in well established journals and conferences. Among the citations, 845 are original citations, i.e., none of the authors of the citing paper is an author of the cited papers, and 355 are self citations, i.e., one of the authors of the citing paper is an author of the cited papers. I enumerate the cited papers, sorted by the number of citations, in decreasing order. Each paper is annotated with T:E(S) where: T is the total number of citations, E is the number of citations excluding self citations, and S is the number of self citations. For most of the papers I also provide a small sample of such citations.	asynchronous circuit;citeseerx;computer science;digital library;enumerated type;google scholar;journal of logical and algebraic methods in programming;scientific literature;web search engine	Nina Amla;E. Allen Emerson;Kedar S. Namjoshi;Richard J. Trefler	2002		10.1007/3-540-36135-9_15	asynchronous system;model checking;real-time computing;causality;formal verification;computer science;artificial intelligence;theoretical computer science;operating system;machine learning;modular programming;asynchronous communication;database;distributed computing;programming language;message sequence chart;algorithm	Theory	-24.97814244843904	32.06264228684885	26629
de8b2a616129672aeef93695612151d8dee560ce	introduction to simulation languages	general algorithmic language;model structure;popular language;special purpose programming language;awkward language;specialized simulation language;language design;digital computer;popular discrete simulation language;special purpose simulation language;general purpose language;simulation study;computer language;modelling process;simulation languages aid;programming language;brief introduction;language best fit;higher level language;simulation language;discrete simulation;data representation;statistical test;generic algorithm	Early effort in a simulation study is concerned with defining the system to be modeled and describing it in terms of logic flow diagrams and functional relationships. But eventually one is faced with the problem of describing the model in a language acceptable to the computer to be used. Most digital computers operate in a binary method of data representation, or in some multiple of binary such as octal or hexadecimal. Since these are awkward languages for users to communicate with, programming languages have evolved to make, easier to converse with the computer. Unfortunately, so many general and special purpose programming languages have been developed over the years, that i t is a nearly impossible task to decide which language best fits or is even a near best fit to any particular application. Over 170 programming languages were in use in the United States in 1972 [1] and today there are even more. Consequently, the usual procedure is to use a language known by the analyst, not because it is best, but because it is known. It should be stated that any general algorithmic language is capable of expressing the desired model; however, one of the specialized simulation languages may have very distinct advantages in terms of ease, efficiency and effectiveness of use.  It is not the purpose of this paper to teach how to program in any of the languages described, nor to discuss implementation techniques. What we do hope to accomplish is to make the reader aware of the characteristics of some of the more popular languages, their strengths and weaknesses. The major differences between special purpose simulation languages in general are: (1) the organization of time and activities, (2) the naming and structuring of entities within the model, (3) the testing of activities and conditions between elements, (4) the types of statistical tests possible on the data and (5) the ease of changing model structure. In the following sections we intend to provide comparisons of several languages after first showing the various philosophies of language design and describing a number of key factors involved in choosing a language.	algol;computer;curve fitting;data (computing);diagram;entity;fits;hexadecimal;octal;programming language	Robert E. Shannon	1977			natural language processing;fourth-generation programming language;simulation;language primitive;specification language;computer science;domain-specific language;technical report;gpss;discrete event simulation;third-generation programming language;syntax;ontology language;low-level programming language;modeling language;fifth-generation programming language;programming language theory;programming language;world wide web;second-generation programming language;high-level programming language;comparison of multi-paradigm programming languages;simulation language	PL	-28.515981939906933	23.888995426850208	26648
61c2c7b9bcf030bd2f07312b51d6ea338ef5001b	a formal correctness proof for code generation from ssa form in isabelle/hol	intermediate representation;code generation;formal semantics;theorem prover	Optimizations in compilers are the most error-prone phases in the compilation process. Since correct compilers are a vital precondition for software correctness, it is necessary to prove their correctness. We develop a formal semantics for static single assignment (SSA) intermediate representations and prove formally within the Isabelle/HOL theorem prover that a relatively simple form of code generation preserves the semantics of the transformed programs in SSA form. This formal correctness proof does not only verify the correctness of a certain class of code generation algorithms but also gives us a sufficient, easily checkable correctness criterion characterizing correct compilation results obtained from implementations (compilers) of these algorithms. To be published inProceedings der 3. Arbeitstagung Programmiersprachen (ATPS) auf der 34. Jahrestagung der Gesellschaft f ür Informatik, Ulm, Germany, 2004, to appear in Lecture Notes in Informatics.	algorithm;assignment (computer science);automated theorem proving;basic block;code generation (compiler);cognitive dimensions of notations;compiler;correctness (computer science);data dependency;dataflow;dead code elimination;eisenstein's criterion;experiment;formal proof;formal verification;hol (proof assistant);informatics;isabelle;lecture notes in computer science;machine code;precondition;semantics (computer science);speculative execution;static single assignment form;topological sorting;very long instruction word	Jan Olaf Blech;Sabine Glesner	2004			compiler;precondition;programming language;formal proof;correctness;code generation;static single assignment form;algorithm;formal verification;hol;mathematics	PL	-18.787138214158155	26.5278702398493	26649
79a04cf7a82d9fdc27ad3cd4354eebfe8ce51286	three-valued semantic pluralism: a defense of a three-valued solution to the sorites paradox		Disagreeing with most authors on vagueness, the author proposes a solution that he calls ‘three-valued semantic pluralism’ to the age-old sorites paradox. In essence, it is a three-valued semantics for a first-order vague language with identity with the additional suggestion that a vague language has more than one correct interpretation. Unlike the traditional three-valued approach to a vague language, three-valued semantic pluralism can accommodate the phenomenon of higher-order vagueness and the phenomenon of penumbral connection when equipped with ‘suitable conditionals’. The author also shows that three-valued semantic pluralism is a natural consequence of a restricted form of the Tolerance principle ( $$\hbox {T}_R$$ T R ) and a few related ideas, and argues that ( $$\hbox {T}_R$$ T R ) is well-motivated by considerations about how we learn, teach, and use vague predicates.	appendix;atomic sentence;battle of midway;emoticon;first-order predicate;forty nine;immune tolerance;interpretation process;k3 rate constant;logical connective;mathematical induction;moravec's paradox;motivation;pluralism (philosophy);sorites sp. 210j;sorites sp. sor_viii;theory;vagueness	Wen-Fang Wang	2017	Synthese	10.1007/s11229-017-1517-6	epistemology;pluralism (political theory);semantics;mathematics;sorites paradox;predicate (grammar);vagueness;phenomenon	NLP	-13.022978321506143	5.387766230687842	26707
fcd81efa4278b25179c78e00b2668f8adb93957b	a formal description of system/360	additional operator;functional characteristic;common variable;auxiliary table;programming significance;formal description;initial study;system description	All SYSTEM/360 functional characteristics having programming significance are completely and concisely described.#R##N##R##N#The description, which is formal rather than verbal, is accomplished by a set of programs, interacting through common variables, used in conjunction with auxiliary tables.#R##N##R##N#The language used in the programs involves operators and notation selected from mathematics and logic, together with additional operators and conventions defined to facilitate system description.#R##N##R##N#Although the formal description is complete and self-contained, text is provided as an aid to initial study.#R##N##R##N#Examples to illustrate the application of the formal description are given in an appendix.	ibm system/360	Adin D. Falkoff;Kenneth E. Iverson;Edward H. Sussenguth	1964	IBM Systems Journal	10.1147/sj.32.0198	formal system;computer science;theoretical computer science;formal specification;programming language;algorithm	Robotics	-24.495670389116107	18.41264642967981	26735
351f8b8e6a2f8b13ab9e94582eadbc154ba9feb2	a schema transformation methodology for distributed heterogeneous information systems	schema transformation;control systems;object oriented data model cosmos;distributed database architectures;programming environments;distributed database;information systems;object oriented data model;general purpose programming environment;programming environment;information systems distributed databases object oriented modeling object oriented databases data models control systems scattering;highly structured abstract constructs;conceptual model;heterogeneous data;scattering;object oriented programming;programming environments distributed databases object oriented programming;two layered approach distributed heterogeneous information systems schema transformation object oriented data model cosmos distributed database architectures highly structured abstract constructs general purpose programming environment;data model;heterogeneous information;distributed databases;two layered approach;object oriented databases;distributed heterogeneous information systems;object oriented modeling;data models	The advantages of the object-oriented data model Cosmos as a conceptual modeling facility for distributed database architectures are highlighted. It is shown that this data model not only offers the possibility of defining highly structured abstract constructs but also provides a general-purpose programming environment for the definition of additional operations upon already existing heterogeneous data. The problem of extracting information from existing heterogeneous data representations is also addressed, and the use of a two-layered approach is suggested. In the two-layered quintessence model architecture the user can utilize highly structured objects at the upper layer, while all interschema and interlanguage transformations are performed in a transparent manner at the lower layer. A two-layered model configuration is introduced for the case of semidecentralized distributed heterogeneous information systems and it is explained how its lower layer is in a position to map heterogeneous host schemas to each other. >	information system	Mike P. Papazoglou;Louis Marinos;Nikolaos G. Bourbakis	1990		10.1109/PARBSE.1990.77164	computer science;theoretical computer science;database;distributed computing	HPC	-33.091148260188305	12.627382219543279	26768
e017526890728ca2ee3f7e94772d366925deddc6	evl: a framework for multi-methods in c++	object oriented programming;multiple dispatch;multi methods;runtime type information;c	Abstract Multi-methods are functions whose calls at runtime are resolved depending on the dynamic types of more than one argument. They are useful for common programming problems. However, while many languages provide different mechanisms to implement them in one way or another, there is still, to the best of our knowledge, no library or language feature that handles them in a general and flexible way. In this paper, we present the EVL (Extended Virtual function Library) framework which provides a set of classes in C++ aiming at solving this problem. The EVL framework provides a generalization of virtual function dispatch through the number of dimensions and the selection of the function to invoke using a so-called Function Comparison Operator . Our library provides both symmetric and asymmetric dispatch algorithms that can be refined by the programmer to include criteria other than class inheritance. For instance, the EVL framework provides multi-methods with predicate dispatch by defining a dedicated FCO based not only on the dynamic types of the arguments but also on their values. This flexibility greatly helps to resolve ambiguities without having to define new functions. Our multi-methods also unify dispatch tables and caching by introducing cache strategies for which the implementation is a balance between memory and speed. To define multi-methods in C++, we implement a non-intrusive reflection library providing fast dynamic casting and supporting dynamic class loading. Our multi-methods are policy-based class templates that support virtual but not repeated inheritance. They check the type compatibility of functions at compile-time, preserve type-safety and resolve function calls at runtime by invoking the cache or updating it by computing the selected function for the requested tuple of types. By default, our multi-methods handle dispatch errors at runtime by throwing exceptions but an error-code strategy can be set up by defining a dedicated policy class. Performance of our multi-methods is comparable with that of standard virtual functions when configured with fast cache.	c++;c++03;c++11;compiler;definition;documentation;dynamic dispatch;electronic visualization laboratory;function pointer;multiple dispatch;programmer;programming paradigm	Yannick Le Goc;Alexandre Donzé	2015	Sci. Comput. Program.	10.1016/j.scico.2014.08.003	real-time computing;run-time type information;computer science;theoretical computer science;programming language;late binding;double dispatch;multiple dispatch;algorithm	Logic	-25.373484938713514	28.481955533551105	26806
99f97582aee64e087104d795df1f3fb419b85bd4	economical discourse representation theory	discourse representation theory;satisfiability;first order;first order logic	First-order logic (FOL) is undecidable — that is, no algorithm exists that can decide whether a formula of FOL is valid or not. However, there are various fragments of FOL that are known to be decidable. FO, the two-variable fragment of FOL, is one of such languages [1,2]. FO is a first-order language where formulas have maximally two variables, no function symbols, but possibly do have equality. FO has the finite model property [1], which means that if a formula of FO is satisfiable, it is satisfiable in a finite model. In this paper we propose a controlled fragment of Discourse Representation Theory (DRT, [3]) with a semantics formalised on the basis of the two-variable fragment with equality. DRT encapsulates the idea of text interpretation that “one and the same structure serves simultaneously as content and context” [4], where content refers to the semantic interpretation of sentences already processed, and context serves in aiding the interpretation of anaphoric expressions (such as pronouns) in subsequent sentences. However, providing a two-variable natural language fragment is, in itself, not a new idea. In fact, the framework presented here is very much inspired by Ian Pratt-Hartmann’s language E2V [5]. But as Pratt-Hartmann himself notes, E2V is “certainly not proposed as a practically useful controlled language”. Our aim is to try to find out how useful a controlled language based on the two-variable fragment actually can be, mostly from a computational linguistic point of view. We will do this by:	anaphora (linguistics);computation;controlled natural language;finite model property;first-order logic;first-order predicate;knuth–morris–pratt algorithm;mind;pipelines;semantic interpretation;two-variable logic	Johan Bos	2009		10.1007/978-3-642-14418-9_8	discrete mathematics;mathematics;algorithm	AI	-12.02796906226969	7.669930010213436	26822
0aff5af82978bbe95d5a4fc85a97506bdb4d4a57	composition of different-type relations via the kleisli category for the continuation monad		We give the way of composing different types of relational notions under certain condition, for example, ordinary binary relations, up-closed multirelations, ordinary (possibly non-up-closed) multirelations, quantale-valued relations, and probabilistic relations. Our key idea is to represent a relational notion as a generalized predicate transformer based on some truth value in some category and to represent it as a Kleisli arrow for some continuation monad. The way of composing those relational notions is given via identity-on-object faithful functors between different Kleisli categories. We give a necessary and sufficient condition to have such identity-on-object faithful functor.	continuation;monad (functional programming)	Koki Nishizawa;Norihiro Tsumagari	2018		10.1007/978-3-030-02149-8_7	discrete mathematics;arrow;truth value;probabilistic logic;functor;continuation;kleisli category;binary relation;monad (functional programming);mathematics	Logic	-7.377657027745063	15.193857296202935	26881
b191175fea3c4fee70231137c989714284a27021	xml-based devs modeling and interpretation	xml schema;reusability;programming language;xlsc;devsjava;devs formalism;xml;devs	There are various implementations for the DEVS formalism in different programming languages. Examples are DEVS-JAVA, DEVS/C++, and SmallDEVS. A model written for a specific simulator implementation cannot be readily reused by other DEVS simulators, especially if they are written in a different programming language. For models to become independent of the programming language, recent approaches such as DEVS-XML or XFD-DEVS use XML to model DEVS, and they translate the model to a simulator specific representation in order to simulate it. This work shares the idea to model DEVS in an XML-based manner. An XML Schema for the language, called XLSC, is presented. The here presented approach differs from the above quoted in that it does not translate the model to the target simulator's programming language. Instead, an XLSC model is directly interpreted. For this, a parser and interpreter are prototypically developed in Java, and it is shown how both can be applied to simulate XLSC models with DEVSJAVA. If an interpreter exists, the use of XLSC enables models to be exchangable among different DEVS simulators regardless of their implementation language. The interpreter thereby acts as the interface between the simulator and the model.	c++;devs;java;object language;programming language;s (programming language);semantics (computer science);simulation;xml schema	Nicolas G. Meseth	2009			reusability;xml;computer science;theoretical computer science;devs;xml schema;database;programming language	PL	-26.70001807150957	22.58840170034926	26922
e46a5ac7c2427652de77a62bb4284b7243c6064a	from denotational to operational and axiomatic semantics for algol-like languages: an overview	operational semantics;denotational semantic;partial order	The advantages of denotational over operational semantics are argued. A denotational semantics is provided for an Algol-like language with finite-mode procedures, blocks with local storage, and sharing (aliasing). Procedure declarations are completely explained in the usual framework of complete partial orders, but cpo's are inadequate for the semantics of blocks, and a new class of store models is developed. Partial correctness theory over store models is developed for commands which may contain calls to global procedures, but do not contain function procedures returning storable values.	algol;axiomatic semantics	Boris A. Trakhtenbrot;Joseph Y. Halpern;Albert R. Meyer	1983		10.1007/3-540-12896-4_382	apply;normalisation by evaluation;action semantics;theoretical computer science;denotational semantics of the actor model;axiomatic semantics;operational semantics;denotational semantics;algorithm	Logic	-16.84655166496327	19.932812415138354	26958
7557ffb697b8042cb7433a3973e2634f27858019	a process for novice programming using goals and plans	plan;process of programming;goal	We propose to improve the teaching of programming to novices by using a clearly-defined and detailed process that makes use of goals and plans, and a visual programming language. We present a simple notation for designing programs in terms of data flow networks of goals and plans, and define a detailed process that uses this notation, and that ultimately results in a program in a visual programming language (BYOB). Results from an evaluation are presented that show the effectiveness of this approach.	dataflow;visual programming language	Minjie Hu;Michael Winikoff;Stephen Cranefield	2013			simulation;reactive programming;computer science;systems engineering;extensible programming;computer programming;programming paradigm;procedural programming;symbolic programming;inductive programming;fifth-generation programming language;programming language;programming language specification;high-level programming language	HCI	-28.718308196849136	23.395073257471015	26999
e8b8e5fdd355872d0eaa12f8d2928525e429f1a5	interpreting presuppositions using active logic: from contexts to utterances	logical framework;active logic;accommodation;discourse;presupposition;context;logical form	Presupposition is a pervasive feature of human language. It involves many interesting interactions between the utterances of a discourse and the context of the discourse. In this paper we focus on issues of logical form connected with the interaction of presupposition and discourse context, and illustrate our theory with some implementational work using the active logic framework. After reviewing some of the major issues in presuppositiontheory we turn to a largely successful uniied approach of Heim. We show how the main principles of this theory can be implemented in active logic. But we also nd two serious diiculties. These consist in (a) a straightforward counterexample and (b) a type of discourse that we call a garden-path discourse. We maintain that both the counterexample and the garden-path type of discourse can be handled by our active-logic version of Heim's theory. This requires us to reformulate and extend Heim's Although this work is largely theoretical, both Heim's theory and ours have important things to say about the incremental processing of the utterances that make up discourse. And we present our theory as a speciication of a processing device that takes logical form of a sentence along with current discourse context as input and delivers an updated discourse context as output. As an experiment, we have implemented portions of this device.	emoticon;interaction;pervasive informatics	John Gurney;Donald Perlis;Khemdut Purang	1997	Computational Intelligence	10.1111/0824-7935.00044	accommodation;logical framework;logical form;presupposition;computer science;artificial intelligence;linguistics;algorithm	NLP	-12.974148845069227	6.755973318701034	27005
ab2bd26041305bfc3ca45f980d23d28229e2e6e2	interval computation as deduction in chip	interval computations;chip;floating point;logic programs	"""Logic programming realizes the ideal of \computation is deduction,"""" but not when oating-point numbers are involved. In that respect logic programming languages are as careless as conventional computation: they ignore the fact that oating-point operations are only approximate and that it is not easy to tell how good the approximation is. It is our aim to extend the beneets of logic programming to computation involving oating-point arithmetic. Our starting points are the ideas of Cleary and the CHIP programming language. Cleary proposed a relational form of interval arithmetic which was incorporated in BNR Prolog in such a way that variables already bound can be bound again. In this way the usual logical interpretation of computation no longer holds. In this paper we develop a technique for narrowing intervals that we relate both to Cleary's work and to the constraint-satisfaction techniques of artiicial intelligence. We then modify CHIP by allowing domains to be intervals of real numbers. To reduce arithmetic primitives with interval domains, we use our interval narrowing technique as an implementation of the Looking-Ahead Inference Rule. We show that the result is a system where answers are logical consequences of a declarative logic program, even when oating-point computations have been used. We believe ours is the rst system with this property."""	approximation algorithm;chip (programming language);computation;constraint satisfaction problem;interpretation (logic);interval arithmetic;logic programming;natural deduction;programming language;prolog;relational database	Jimmy Ho-Man Lee;M. H. van Emden	1993	J. Log. Program.	10.1016/0743-1066(93)90045-I	chip;discrete mathematics;interval temporal logic;computer science;floating point;artificial intelligence;theoretical computer science;mathematics;programming language;algorithm	PL	-15.35942325661817	11.282291306101031	27019
b8dca9a16bc19c00c6fb9e0f4986adce1bbe66ba	semantic log based replication model for optimizing heterogeneous dbms interaction	databases;protocols;replication;heterogeneous systems;availability;probability density function;heterogeneous environment;semantic technologies;availability data engineering knowledge engineering reliability engineering ontologies application software database systems fault tolerant systems spatial databases semantic web;semantic technology;semantic log based replication model;ontologies artificial intelligence;semantic web ontologies artificial intelligence replicated databases;heterogeneous dbms interaction;large scale;heterogeneous dbmss;database systems;log based;semantic;semantic web;ontology dbms replication log based heterogeneous dbmss semantic;physical schema;ontologies;dbms;log based schema;ontology semantic log based replication model heterogeneous dbms interaction database management system physical schema trigger based schema log based schema semantic technology;database management system;ontology;replicated databases;trigger based schema	The growth of database application usage requires Database Management Systems (DBMS) that are accessible, reliable, and dependable. One approach to handle these requirements is replication mechanism.Replication mechanism can be divided into various categories. Some related works consider two categories for replication mechanisms: heterogeneous and homogenous however majority of them classify them in three groups: physical, trigger-based and log- based schema. Log-based replication mechanisms are the most widely used category among DBMS vendors.Adapting such approach for heterogeneous systems is a complex task, because of lack of log understanding in the other end. Semantic technologies provide a suitable framework to address heterogeneity problems in large scale and dynamic resources. In this paper we introduce a new approach to tackle replication problem in a heterogeneous environment by utilizing ontologies.	database schema;management system;ontology (information science);optimizing compiler;replication (computing);requirement	Ali Farahmand Nejad;Sadegh Kharazmi;Shahabedin Bayati;Seyed Koosha Golmohammadi;Hassan Abolhassani	2009	2009 First International Confernce on Advances in Databases, Knowledge, and Data Applications	10.1109/DBKDA.2009.24	computer science;ontology;data mining;database;semantic technology;world wide web	DB	-33.67220041695613	12.8776435862391	27050
6da535fdee8631e171dc8e719af2bd14d3de2035	parameterised multiparty session types	parallel algorithm;dependent type theory;type checking decidable;multiparty session;type theory;guarantee type-safe;parameterised multiparty session type;communication pattern;interaction structure;web services usecases;deadlock-free multiparty interaction;web service	For many application-level distributed protocols and parallel algorithms, the set of participants, the number of messages or the interaction structure are only known at run-time. This paper proposes a dependent type theory for multiparty sessions which can statically guarantee type-safe, deadlock-free multiparty interactions among processes whose specifications are parameterised by indices. We use the primitive recursion operator from Gödel’s System T to express a wide range of communication patterns while keeping type checking decidable. To type individual distributed processes, a parameterised global type is projected onto a generic generator which represents a class of all possible end-point types. We prove the termination of the type-checking algorithm in the full system with both multiparty session types and recursive types. We illustrate our type theory through non-trivial programming and verification examples taken from parallel algorithms and web services usecases.	deadlock;dependent type;gödel;interaction;parallel algorithm;primitive recursive function;recursion;run time (program lifecycle phase);type safety;type system;type theory;web service	Pierre-Malo Deniélou;Nobuko Yoshida;Andi Bejleri;Raymond Hu	2009	Logical Methods in Computer Science	10.2168/LMCS-8(4:6)2012	web service;discrete mathematics;computer science;theoretical computer science;distributed computing;parallel algorithm;programming language;type theory;algorithm	PL	-28.719590185191628	32.25942004618924	27061
814b3581740e91c17fd4882068bdb0b8967f3012	prototyping the semantics of a dsl using asf+sdf: link to formal verification of dsl models	algebraic specification;transformation model;labeled transition system;software engineering;formal verification;domain specific language	A formal definition of the semantics of a domain-specific language (DSL) is a key prerequisite for the verification of the correctness of models specified using such a DSL and of transformations applied to these models. For this reason, we implemented a prototype of the semantics of a DSL for the specification of systems consisting of concurrent, communicating objects. Using this prototype, models specified in the DSL can be transformed to labeled transition systems (LTS). This approach of transforming models to LTSs allows us to apply existing tools for visualization and verification to models with little or no further effort. The prototype is implemented using the ASF+SDF Meta-Environment, an IDE for the algebraic specification language ASF+SDF, which offers efficient execution of the transformation as well as the ability to read models and produce LTSs without any additional pre or post processing.	asf+sdf meta-environment;algebraic specification;command-line interface;correctness (computer science);digital subscriber line;domain-specific language;formal verification;operational semantics;prototype;rewrite (programming);rewriting;semantics (computer science);specification language;state space	Suzana Andova;Mark van den Brand;Luc Engelen	2011		10.4204/EPTCS.56.5	formal verification;computer science;domain-specific language;theoretical computer science;database;programming language	PL	-21.077356693619773	26.49177046578126	27074
6e06b6e460984c90e722bb798afaa96fe4238491	measurement based model to study the affect of increase in data size on database query response time			responsiveness	Rekha Singhal;Manoj K. Nambiar	2013			information retrieval;data mining;database;response time;computer science	DB	-31.132394732783972	7.100167828120241	27103
a462bd702cd8f69c5b7f22980e9df17d2cb18269	generalised multi-pattern-based verification of programs with linear linked structures	automatic verification;05bxx;dynamic linked data structures;implementation;program verification;dynamic linking;experimental result;verificacion programa;formal verification;dynamic data structure;informatique theorique;estructura datos;resultado experimental;structure donnee;program analysis;implementacion;resultat experimental;verification programme;data structure;computer theory;informatica teorica	The paper deals with the problem of automatic verification of programs working with extended linear linked dynamic data structures, in particular, pattern-based verification is considered. In this approach, one can abstract memory configurations by abstracting away the exact number of adjacent occurrences of certain memory patterns. With respect to the previous work on the subject the method presented in the paper has been extended to be able to handle multiple patterns, which allows for verification of programs working with more types of structures and/or with structures with irregular shapes. The experimental results obtained from a prototype implementation of the method show that the method is very competitive and offers a big potential for future extensions.	data structure;dynamic data;dynamization;lambda calculus;linked data;linked list;list of data structures;liveness;programming paradigm;prototype;sensor	Milan Ceska;Pavel Erlebach;Tomás Vojnar	2007	Formal Aspects of Computing	10.1007/s00165-007-0031-x	program analysis;simulation;data structure;formal verification;computer science;database;runtime verification;programming language;implementation;algorithm;functional verification	Logic	-18.60921616173587	24.693777774835855	27190
19669f08ed1408523717ea4fb9c4175f0925e448	sos rule formats for idempotent terms and idempotent unary operators	structural operational semantics;concurrency;bisimilarity;rule formats;process algebra;equational logic	A unary operator f is idempotent if the equation f(x) = f(f(x)) holds. On the other end, an element a of an algebra is said to be an idempotent for a binary operator if a = a a. This paper presents a rule format for Structural Operational Semantics that guarantees that a unary operator be idempotent modulo bisimilarity. The proposed rule format relies on a companion one ensuring that certain terms are idempotent with respect to some binary operator. This study also offers a variety of examples showing the applicability of both formats.	bisimulation;idempotence;modulo operation;operational semantics;unary operation	Luca Aceto;Anna Ingólfsdóttir;Eugen-Ioan Goriac	2014	J. Log. Algebr. Program.	10.1016/j.jlap.2013.07.003	process calculus;discrete mathematics;concurrency;equational logic;computer science;mathematics;programming language;algorithm;algebra	DB	-10.742498006732653	17.292559578587912	27193
632c1af2307b01d327fa1b6a622cfc081b44d762	mptp – motivation, implementation, first experiments	atp;theorem proving;theorem prover;first order;mptp;mizar mathematical library;mpa;mizar	We describe a number of new possibilities for current theorem provers that arise with the existence of large integral bodies of formalized mathematics. Then we describe the implementation of the MPTP system, which makes the largest existing corpus of formalized mathematics available to theorem provers. MPTP (Mizar Problems for Theorem Proving) is a system for translating the Mizar Mathematical Library (MML) into untyped first-order format suitable for automated theorem provers and for generating theorem-proving problems corresponding to MML. The first version generates about 30,000 problems from complete proofs of Mizar theorems and about 630,000 problems from the simple (one-step) justifications done by the Mizar checker. We describe the design and structure of the system, the main problems encountered in this kind of system, their solutions, current limitations, and planned extensions. We present results of first experiments with re-proving the MPTP problems with theorem provers. We also describe first implementation of the Mizar Proof Advisor (MPA) used for selecting suitable axioms from the large library for an arbitrary problem and, again, present first results of this combined MPA/ATP architecture on MPTP.	automated theorem proving;experiment;first-order predicate;mizar	Josef Urban	2004	Journal of Automated Reasoning	10.1007/s10817-004-6245-1	computer science;mathematics;automated theorem proving;mizar system;programming language;algorithm	PL	-17.891841706442392	18.276628619834952	27207
702cd26c0a66f3d772d34be30678bf6f4b50a6e9	temporal logic programming	lenguaje programacion;logica temporal;programming language;temporal logic;prolog;logical programming;programmation logique;langage programmation;programacion logica;logique temporelle	Temporal logic, often used as a specification language for programs, can serve directly as a programming language. We propose a specific programming language TEMPLOG, which extends the classical PROLOG-like languages to include temporal operators. PROLOG progams are collections of classical Horn clauses and they are efficiently interpreted by SLD-resolution. Similarly, TEMPLOG programs are collections of temporal Horn clauses and we interpret them with temporal SLD-resolution, a restricted form of a general temporal resolution method.	logic programming;temporal logic	Martín Abadi;Zohar Manna	1987	J. Symb. Comput.	10.1016/S0747-7171(89)80070-7	linear temporal logic;horn clause;temporal logic;interval temporal logic;functional logic programming;programming paradigm;inductive programming;fifth-generation programming language;prolog;logic programming;algorithm	Theory	-17.99531914950969	20.946914480745686	27226
5e4de81395e65695cb41881f086286f657126533	integration proposal for description logic and attributive logic: towards semantic web rules	rule based system;artificial intelligent;semantic web;description logic	The current challenge of the Semantic Web is the development of an expressive yet effective rule language. This paper presents an integration proposal for Description Logics (DL) and Attributive Logics (ALSV) is presented. These two formalisms stem from fields of Knowledge Representation and Artificial Intelligence. However, they are based on different design goals and therefore provide different description and reasoning capabilities. ALSV is the foundation of XTT2, an expressive language for rule-based systems. DL provide formulation for expressive ontology languages such as OWL2. An important research direction is the development of rule languages that can be integrated with ontologies. The contribution of the paper consists in introducing a possible transition from ALSV to DL. This opens up possibilities of using XTT2, a well-founded rule-based system modelling rule language, to improve the design of Semantic Web rules.		Grzegorz J. Nalepa;Weronika T. Adrian	2010	Trans. Computational Collective Intelligence	10.1007/978-3-642-17155-0_1	natural language processing;knowledge representation and reasoning;description logic;semantic web rule language;computer science;theoretical computer science;data mining	AI	-21.17036542385957	8.906202686648156	27237
36bcbef73fec65d025b04a54b39a72fda6f17bcf	semantics-based compiling: a case study in type-directed partial evaluation	attribute grammar;higher order;denotational semantic;partial evaluation;source language;source code;program analysis;type inference;basic research	We illustrate a simple and effective solution to semantics-based compiling. Our solution is based on type-directed partial evaluation, where • our compiler generator is expressed in a few lines, and is efficient; • its input is a well-typed, purely functional definitional interpreter in the manner of denotational semantics; • the output of the generated compiler is three-address code, in the fashion and efficiency of the Dragon Book; • the generated compiler processes several hundred lines of source code per second. The source language considered in this case study is imperative, blockstructured, higher-order, call-by-value, allows subtyping, and obeys stack discipline. It is bigger than what is usually reported in the literature on semantics-based compiling and partial evaluation. Our compiling technique uses the first Futamura projection, i.e., we compile programs by specializing a definitional interpreter with respect to this program. Our definitional interpreter is completely straightforward, stackbased, and in direct style. In particular, it requires no clever staging technique (currying, continuations, binding-time improvements, etc.), nor does it rely on any other framework (attribute grammars, annotations, etc.) than the typed λ-calculus. In particular, it uses no other program analysis than traditional type inference. The overall simplicity and effectiveness of the approach has encouraged us to write this paper, to illustrate this genuine solution to denotational semantics-directed compilation, in the spirit of Scott and Strachey. ∗Computer Science Department, Building 540, Ny Munkegade, DK-8000 Aarhus C, Denmark. Home pages: http://www.brics.dk/~{danvy,jrvest}. Phone: (+45) 89 42 33 69. Fax: (+45) 89 42 32 55. This work is supported by BRICS (Basic Research in Computer Science, Centre of the Danish National Research Foundation).	attribute grammar;compiler;compiler-compiler;computer science;continuation;currying;definition;denotational semantics;direct style;disk staging;fax;imperative programming;lambda calculus;name binding;partial evaluation;program analysis;three-address code;type inference	Olivier Danvy;René Vestergaard	1996		10.1007/3-540-61756-6_85	natural language processing;computer science;theoretical computer science;programming language	PL	-22.22687811518028	25.666478315203523	27253
510501c7051c03d5e2a70089deeda8dfc3a7304f	object model construction for inheritance in c++ and its applications to program analysis	complex pointer arithmetic operation;translation mechanism;program analysis;model checking;static program analyzer;original c;c program;modern object-oriented programming language;static analyzer;object model construction;popular c intermediate language;static analysis	Modern object-oriented programming languages such as C++ provide convenient abstractions and data encapsulation mechanisms for software developers. However, these features also complicate testing and static analysis of programs that utilize object-oriented programming concepts. In particular, the C++ language exhibits features such as multiple inheritance, static and dynamic typecasting that make static analyzers for C++ quite hard to implement. In this paper, we present an approach where static analysis is performed by lowering the original C++ program into a semantically equivalent C program. However, unlike existing translation mechanisms that utilize complex pointer arithmetic operations, virtual-base offsets, virtual-function pointer tables, and calls to run-time libraries to model C++ features, our translation is targeted towards making static program analyzers for C++ easier to write and provide more precise results. We have implemented our ideas in a framework for C++ called CILpp that is analogous to the popular C Intermediate Language (CIL) framework. We evaluate the effectiveness of our translation in a bug finding tool that uses abstract interpretation and model checking. The bug finding tool uncovered several previously unknown bugs in C++ open source projects.	abstract interpretation;algorithm;c++;common intermediate language;compiler;encapsulation (networking);function pointer;google chrome;library (computing);model checking;multiple inheritance;open-source software;pointer (computer programming);programming language;software bug;software developer;static program analysis;typecasting (blogging);virtual method table	Jing Yang;Gogul Balakrishnan;Naoto Maeda;Franjo Ivancic;Aarti Gupta;Nishant Sinha;Sriram Sankaranarayanan;Naveen Sharma	2012		10.1007/978-3-642-28652-0_8	real-time computing;run-time type information;c++;allocator;computer science;theoretical computer science;const;c++11;new;programming language;algorithm;reference;smart pointer	SE	-21.922126216271153	28.069490236547267	27266
45d69862cf995db33aa5aa6b41ec38a4262f2bb8	the gentzenization and decidability of rw		which corresponds in LR, to the axiom (W) of R, would, in an attempted decision procedure based on LR, , produce unbounded multiplicities of the above structures a. It then seemed likely that the system R without the axiom (W), called now RW, would be decidable. Giambrone, in [8], has shown, by using Dunn’s Gentzenization LR, without (WI t) (thus yielding LRW,), that RW, is decidable. Giambrone’s decidability argument involves a refinement of Gentzen’s original argument for the decidability of sentential calculi in [7], together with original insights appropriate to Dunn’s style of Gentzenization, which involves both intensional and extensional sequences in the antecedents of consecutions. In this paper, we apply Giambrone’s decidability argument to achieve decidability for RW, after establishing a Cut-free Gentzenization for the logic by extending the work of Dunn in [l]. It is the establishment that the rule Cut:	decision problem;ibm system r;intensional logic;lr parser;r language;read-write memory;refinement (computing);simpson's rule	Ross T. Brady	1990	J. Philosophical Logic	10.1007/BF00211185	algorithm;mathematics;tray;separator (oil production);hook;ball (bearing);decidability;mechanical engineering	AI	-11.300950620095055	14.052367993382132	27276
e668ebe5cccfe96f36c5c20034ae3b001c16e172	towards automatic deduction and event reconstruction using forensic lucid and probabi	forensic lucid model;credibility weight;probabilisticmodel-checking tool prism;formal model;event reconstruction;ids data;forensic analysis;forensic case modeling;ids evidence;towards automatic deduction;credibility factor;forensic lucid;evidential ids observation	Introduction. We apply the theoretical framework and formal model of the observation tuple with the credibility weight for forensic analysis of the IDS data and the corresponding event reconstruction. Forensic Lucid – a forensic case modeling and specification language is used for the task. In the ongoing theoretical and practical work, Forensic Lucid is augmented with the Dempster-Shafer theory of mathematical evidence to include the credibility factors of the evidential IDS observations. Forensic Lucid’s toolset is practically being implemented within the General Intensional Programming System (GIPSY) and the probabilistic model-checking tool PRISM as a backend to compile the Forensic Lucid model into the PRISM’s code and model-check it. This work may also help with further generalization of the testing methodology of IDSs [10].	compiler;formal language;lucid;model checking;natural deduction;prism (surveillance program);specification language;statistical model	Serguei A. Mokhov;Joey Paquet;Mourad Debbabi	2010		10.1007/978-3-642-15512-3_36	computer science;theoretical computer science;distributed computing;computer security;algorithm	SE	-19.8754788883804	28.60687138019166	27321
c8e940fea786676ed39745a77176617558bfb82d	jump minimization in linear time	minimisation;ecriture programme;minimization;programming language;generation code;program writing;code generation;ordering;branchement inconditionnel;relation ordre;linear time;langage programmation;instruction goto	"""Unlike other instructions, which compute or test values, unconditional branch instructions do no useful work. Rather, they are artifacts of the translation from a flow graph to the linear form of conventional machine language. Careful ordering of the basic blocks of a program can decrease the number of branches required by allowing a basic block to """"fall through"""" to a successor. It is shown that although the general problem of minimizing the number of branches is NP-complete, an efficient algorithm is possible for """"structured"""" programs--those written without goto statements. More specifically, an algorithm is presented that produces an optimal ordering of the basic blocks of any program that uses only the control structures i f t h e n e l s e , loop, and exit . The running time of the algorithm is proportional to the length of the program, provided the number of loops exited by any exit statement can be bounded by a constant."""	algorithm;basic block;branch (computer science);control flow;goto;machine code;np-completeness;time complexity	M. V. S. Ramanath;Marvin H. Solomon	1984	ACM Trans. Program. Lang. Syst.	10.1145/1780.357262	time complexity;minimisation;order theory;computer science;artificial intelligence;programming language;algorithm;code generation	PL	-15.736680396333186	31.90679471803584	27325
22aa001fc597616cbae92e87943011e79d2f5a22	relational level data structures for programming languages	programming language;data structure	We describe a high level of data structure description for programming languages which we call the relational level. At this level one may describe and manipulate data structures without having to decide beforehand which access paths will be used or how the structure may change. This is done by using rather simple mathematical structures and providing powerful primitive operations in the language for manipulating these structures. This is constrast ed with the access path and machine levels of description and it is suggested that a language be able to function well at all three levels. We present some ideas on how a system might be built to implement such a language efficiently. This involves the use of an implementation facility to allow the programmer to specify the implementation of his relational level structures in terms of access path and machine level structures using a “structured programming” approach.	data structure;high-level programming language;language primitive;mathematical structure;programmer;structured programming	Jay Earley	1973	Acta Informatica	10.1007/BF00289502	fourth-generation programming language;first-generation programming language;language primitive;data structure;programming domain;computer science;programming language implementation;theoretical computer science;third-generation programming language;programming paradigm;low-level programming language;fifth-generation programming language;programming language;second-generation programming language;high-level programming language;algorithm	PL	-26.176954723220668	24.688067971052924	27364
64e920919ed2ec2c8e42b9a5c47fc64a69ac6c2b	verification of asynchronous circuits using timed automata	satisfiability;asynchronous circuit;timed automaton;signal transition graph;timing analysis;timed automata	In this work we apply the timing veri cation tool OpenKronos which is based on timed automata to verify correctness of numerous asynchronous cir cuits The desired behavior of these circuits is speci ed in terms of signal transition graphs STG and we check whether the synthesized circuits behave correctly under the assumption that the inputs satisfy the STG conventions and that the gate delays are bounded between two given numbers Our results demonstrate the viability of the timed automaton approach for timing analysis of certain classes of circuits	correctness (computer science);signal transition;star trek generations;static timing analysis;timed automaton	Marius Bozga;Jianmin Hou;Oded Maler;Sergio Yovine	2002	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(04)80468-7	real-time computing;asynchronous circuit;computer science;theoretical computer science;timed automaton;static timing analysis;algorithm;satisfiability	Logic	-12.02359182077838	26.70654902922789	27365
9a5d45b2fd19d5c877f78c1ca7974bac3e9b2283	about partial order reduction in planning and computer aided verification	heuristic search;partial order reduction;classical planning	Partial order reduction is a state space pruning approach that has been originally introduced in computer aided verification. Recently, various partial order reduction techniques have also been proposed for planning. Despite very similar underlying ideas, the relevant literature from computer aided verification has hardly been analyzed in the planning area so far, and it is unclear how these techniques are formally related. We provide an analysis of existing partial order reduction techniques and their relationships. We show that recently proposed approaches in planning are instances of general partial order reduction approaches from computer aided verification. Our analysis reveals a hierarchy of dominance relationships and shows that there is still room for improvement for partial order reduction techniques in planning. Overall, we provide a first step towards a better understanding and a unifying theory of partial order reduction techniques from different areas.	algorithm;cognitive dimensions of notations;computer aided verification;edmund m. clarke;model checking;partial order reduction;reinventing the wheel;state space;stuttering equivalence;turing completeness	Martin Wehrle;Malte Helmert	2012			partial order reduction;heuristic;computer science;artificial intelligence;machine learning;algorithm	Logic	-14.047437508130765	23.385713194428064	27397
ff7c699a53d9191e13865ff962bb753634ff3b2e	the unsolvability of the equivalence problem for lambda-free nondeterministic generalized machines	a-free nondeterministic generalized machine;c-finite language;equivalence problem;equality problem;equivalence problem	It is shown that the equivalence problem for A-free nondeterministic generalized machines is unsolvable, and it is observed that this result implies the unsolvability of the equality problem for c-finite languages.	turing completeness	Timothy V. Griffiths	1968	J. ACM	10.1145/321466.321473	discrete mathematics;generalized nondeterministic finite automaton	Theory	-7.715978102315691	18.08319934606955	27475
29f5546ee899db06eab8cd10fef3eaa7e1969957	resource-sensitive synchronization inference by abduction	abduction;frame inference;separation logic;satisfiability;deterministic parallelism;parallel programs;points to analysis	We present an analysis which takes as its input a sequential program, augmented with annotations indicating potential parallelization opportunities, and a sequential proof, written in separation logic, and produces a correctly-synchronized parallelized program and proof of that program. Unlike previous work, ours is not an independence analysis; we insert synchronization constructs to preserve relevant dependencies found in the sequential program that may otherwise be violated by a naive translation. Separation logic allows us to parallelize fine-grained patterns of resource-usage, moving beyond straightforward points-to analysis. Our analysis works by using the sequential proof to discover dependencies between different parts of the program. It leverages these discovered dependencies to guide the insertion of synchronization primitives into the parallelized program, and to ensure that the resulting parallelized program satisfies the same specification as the original sequential program, and exhibits the same sequential behaviour. Our analysis is built using frame inference and abduction, two techniques supported by an increasing number of separation logic tools.	abductive reasoning;frame language;parallel computing;parallel programming model;pointer analysis;separation logic	Matko Botincan;Mike Dodds;Suresh Jagannathan	2012		10.1145/2103656.2103694	separation logic;computer science;theoretical computer science;machine learning;programming language;algorithm;satisfiability	PL	-19.728197278971336	29.75879510003834	27490
abae88040b0d00fcd5fa2415fc289c89f45e645d	graph matching based authorization model for efficient secure xml querying	authorization xml access control information security computer science web sites protection testing context modeling;xml documents;index structure graph matching secure xml querying data representation data exchange world wide web xml documents access control query rewriting;query processing;authorisation;index structure;data exchange;graph matching;data representation;xml;world wide web;access control;query rewriting;secure xml querying;xml authorisation query processing	XML is rapidly emerging as a standard for data representation and exchange over the World Wide Web and an increasing amount of sensitive business data is processed in the XML format. Therefore, it is critical to have control mechanisms to restrict a user to access only the parts of XML documents that he/she is authorized to access. In this paper, we propose the first DTD-based access control model that employs graph matching to analyze if an input query is fully acceptable, fully rejectable, or partially acceptable, and to rewrite for partially acceptable queries only if necessary, along with the features of optimization and speed-up for query rewriting by introducing an index structure.	access control;apache axis;authorization;control system;data (computing);matching (graph theory);mathematical optimization;query optimization;recursion;rewrite (programming);rewriting;world wide web;xml	Seunghan Chang;Artem Chebotko;Shiyong Lu;Farshad Fotouhi	2007	21st International Conference on Advanced Information Networking and Applications Workshops (AINAW'07)	10.1109/AINAW.2007.195	xml catalog;xml validation;binary xml;xml encryption;xml base;simple api for xml;xml;relax ng;xml schema;streaming xml;computer science;document structure description;xml framework;soap;xml database;xml schema;database;xml signature;world wide web;xml schema editor;cxml;information retrieval;efficient xml interchange	DB	-31.457787909921848	5.849568162237054	27504
f4a6b209f107dfffe48700c8c9174f90589fdd39	trx: a formally verified parser interpreter	proof assistant;programming language;parsing expression grammar;formal verification;context free grammar;automata theory;logic in computer science;formal language	Parsing is an important problem in computer science and yet surprisingly little attention has been devoted to its formal verification. In this paper, we present TRX: a parser interpreter formally developed in the proof assistant Coq, capable of producing formally correct parsers. We are using parsing expression grammars (PEGs), a formalism essentially representing recursive descent parsing, which we consider an attractive alternative to context-free grammars (CFGs). From this formalization we can extract a parser for an arbitrary PEG grammar with the warranty of total correctness, i.e., the resulting parser is terminating and correct with respect to its grammar and the semantics of PEGs; both properties formally proven in Coq.	formal verification;parser	Adam Koprowski;Henri Binsztok	2010		10.1007/978-3-642-11957-6_19	grammar systems theory;left recursion;natural language processing;parser combinator;formal language;compiler-compiler;lalr parser;ll grammar;parsing expression grammar;formal verification;operator-precedence grammar;top-down parsing language;computer science;syntactic predicate;parsing;glr parser;s-attributed grammar;automata theory;extended affix grammar;formal grammar;context-free grammar;proof assistant;programming language;attribute grammar;recursive descent parser;top-down parsing;algorithm	Logic	-20.610072334639888	21.45679386693427	27534
41c2621fabf4165040192f7908d513c9e1d9c4da	eliminating redundant clauses in sat instances	institutional repositories;fedora;satisfiability;vital;performance improvement;linear time;polynomial time;sat solver;vtls;ils	In this paper, we investigate to which extent the eliminatio n of a class of redundant clauses in SAT instances could improve the effic iency of modern satisfiability provers. Since testing whether a SAT instanc e does not contain any redundant clause is NP-complete, a logically incomplete bu t polynomial-time procedure to remove redundant clauses is proposed as a pre-t reatment of SAT solvers. It relies on the use of the linear-time unit propaga tion technique and often allows for significant performance improvements of the subs equent satisfiability checking procedure for really difficult real-world instanc es.	algorithm;best, worst and average case;binary file;bitwise operation;boolean satisfiability problem;compiler;computation;heuristic (computer science);np-completeness;polynomial;preprocessor;software propagation;time complexity;unit propagation	Olivier Fourdrinoy;Éric Grégoire;Bertrand Mazure;Lakhdar Sais	2007		10.1007/978-3-540-72397-4_6	time complexity;computer science;theoretical computer science;programming language;algorithm	AI	-14.833964645440975	24.216033033225855	27568
e4aa4321e296d88ffa45197bae21ebab1f046d0b	translational expressiveness. comparing process calculi using encodings		ion is not of much use without operational correspondence. Because of the various possibilities two choose these two equivalences, it is often not possible to compare different full abstraction results, which is a major drawback of this criterion. 3.2.3. Operational Correspondence Intuitively, operational correspondence requires preservation and reflection of executions. Again, it consists of a completeness a soundness part. The completeness condition, also called adequacy, requires that for all source term steps S 7−→S S′ or source term executions S Z=⇒S S′ there is one emulating execution in the target language such that J S K Z =⇒T T J S′ K, where T ⊆ PT × PT is some equivalences on the target language. Note that there is no difference in the consideration of single source term steps or source term executions. Intuitively, the completeness condition requires that any source term execution is emulated by the target term modulo some equivalence T . Again, completeness is usually the easiest part. For the soundness condition we basically find two formulations. The stricter formulation requires that for all executions of the target J S K Z =⇒T T there exists some execution of the source S Z=⇒S S′ such that J S′ K T T . Intuitively, soundness requires that whatever J S K can do is a translation of some behaviour of S modulo T [FL10]. The weaker formulation requires that for all executions of the target J S K Z =⇒T T there exists some execution of the source S Z=⇒S S′ and some execution of the target T Z=⇒T T ′ such that J S′ K T T ′. Intuitively, it states that any execution of the target is some part of the emulation of an execution in the source modulo T [Par08, Gor10b]. The main difference is that the later formulation allows for intermediate or partial commitment states, i.e., for states that do not need to be related directly to the states of the respective source term but that have to belong to some emulation of a source term step. In this sense, an intermediate state results from the partial emulation of a source term step. We discuss this issue in Section 6.3.1. Again different variants of operational correspondence may arise from different requirements on the assumed equivalence T on the target language. Note that [Nes96, NP00] present operational correspondence without the equivalence, i.e., require J S K Z=⇒T J S′ K whenever S 7−→S S′ and J S K Z =⇒T T implies S Z=⇒S S′ for some S′ such that T Z=⇒T J S′ K, which again leads to a stricter formulation than above. They also present a stricter variant of the soundness part—J S K 7−→T T implies S 7−→S S′ for some S′ such that T T J S′ K—and state that only prompt encodings can satisfy this stricter variant. Moreover, in [FL10] labelled steps are considered instead of a reduction semantics under the assumption that there exists a mapping ·̂ from the labels of the source term into the labels of the target term. Hence, the resulting requirement—J S K λ̂ =⇒ T J S′ K whenever S λ =⇒ S′ and J S K λ =⇒ T implies S λ ′ =⇒ S′ for some λ′, S′ such that J S′ K T T and λ̂′ = λ—can be considered as stricter than the above variant of operational correspondence, because also observables have to be preserved and reflected in some sense. In fact, without this strengthening to labelled semantics, operational correspondence	compiler;denotational semantics;emulator;expressive power (computer science);modulo operation;observable;operational semantics;process calculus;requirement;turing completeness	Kirstin Peters	2012				PL	-16.952546029970165	16.82237455563863	27609
132fc081a6a5fb0e75438ff2b505e5f1230860ac	proving concurrent data structures linearizable	electronic mail;history;standards;law;data structures;software reliability	Linearizability of concurrent data structure implementations is notoriously hard to prove. Consequently, current verification techniques can only prove linearizability for certain classes of data structures. We introduce a generic, sound, and practical technique to statically check the linearizability of concurrent data structure implementations. Our technique involves specifying the concurrent operations as a list of sub-operations and passing this specification on to an automated checker that verifies linearizability using relationships between individual suboperations. We have proven the soundness of our technique. Our approach is expressive: we have successfully verified the linearizability of 12 popular concurrent data structure implementations including algorithms that are considered to be challenging to prove linearizable such as elimination back-off stack, lazy linked list, and time-stamped stack. Our checker is effective, as it can verify the specifications in less than a second.	algorithm;concurrent data structure;lazy evaluation;linearizability;linked list;queue (abstract data type);soundness (interactive proof);specification language	Vineet Singh;Iulian Neamtiu;Rajiv Gupta	2016	2016 IEEE 27th International Symposium on Software Reliability Engineering (ISSRE)	10.1109/ISSRE.2016.31	real-time computing;data structure;computer science;database;distributed computing;programming language;software quality	Logic	-18.466183007560566	28.930489762242697	27695
d39728f797044ba5f11541331b5d69451d1c662c	subtypes and bounded quantification from a fibred perspective		A general categorical description of subtyping and of bounded quanti cation and is presented in terms of brations In fact we shall generalize these bounded quanti ers to constrained quanti ers and In these cases one quanti es over those type variables for which holds Semantically we distinguish three levels types which are bred over depend on subtypings which in turn are bred over depend on kinds K In this setting we can describe constrained quanti cation and as right and left adjoints to the weakening functor which adds the dummy hypothesis to an appropriate context This shows that like ordinary quanti ers these constrained and hence especially bounded quanti ers are adjoints	dummy variable (statistics);quadratically constrained quadratic program	Bart Jacobs	1995	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(04)80015-X	discrete mathematics;topology;mathematics	AI	-8.067114129900183	10.383282039760678	27698
8a83e25c832e1a8f88222eab68228496401e8072	the semantics of the c++ programming language	data type;c programming language	 data types such asstacks and queues can be represented as container classes. For example, the notion of a list defines the wayin which list items, or nodes, are linked to one another and methods of manipulating the items but leavesundefined the type of information stored in a node. A family of list types can be defined simply by specifyingdifferent values for this type information. A template separates the structure common to all members of thefamily from the type information specifying a... 	approximation algorithm;local variable;loop unrolling;memory address;pointer (computer programming);the c++ programming language	Charles Wallace	1993			natural language processing;type signature;c++;decltype;functional logic programming;const;compatibility of c and c++;c++11;programming paradigm;fifth-generation programming language;programming language	PL	-25.502252661710017	26.269309115638126	27738
3abe0a8fa58544760b2df357a85ae48061bd8402	extended resolution proofs for symbolic sat solving with quantification	diagrama binaria decision;diagramme binaire decision;formule cnf;variable elimination;satisfiabilite;logique propositionnelle;general techniques;satisfiability;constraint satisfaction;satisfaction contrainte;formula cnf;propositional logic;satisfaccion restriccion;conjunctive normal form;logica proposicional;sat solver;satisfactibilidad;binary decision diagram	Symbolic SAT solving is an approach where the clauses of a CNF formula are represented using BDDs. These BDDs are then conjoined, and finally checking satisfiability is reduced to the question of whether the final BDD is identical to false. We present a method combining symbolic SAT solving with BDD quantification (variable elimination) and generation of extended resolution proofs. Proofs are fundamental to many applications, and our results allow the use of BDDs instead of—or in combination with—established proof generation techniques like clause learning. We have implemented a symbolic SAT solver with variable elimination that produces extended resolution proofs. We present details of our implementation, called EBDDRES, which is an extension of the system presented in [1], and also report on experimental results.	binary decision diagram;boolean satisfiability problem;conjunctive normal form;constraint learning;dpll algorithm;existential quantification;experiment;resolution (logic);solver;variable elimination	Toni Jussila;Carsten Sinz;Armin Biere	2006		10.1007/11814948_8	variable elimination;conjunctive normal form;mathematical optimization;discrete mathematics;constraint satisfaction;computer science;mathematics;propositional calculus;boolean satisfiability problem;binary decision diagram;algorithm;satisfiability	Logic	-13.86129022844327	20.586506062403682	27740
9e24caa02904c21026d097b288883406cd293826	efficient reachability graph representation of petri nets with unbounded counters	reachability;linear operator;compact representation;graph representation;petri nets;petri net;traces;unbounded integers	In this paper, we define a class of Petri nets, called Petri nets with counters, that can be seen as place/transition Petri nets enriched with a vector of integer variables on which linear operations may be applied. Their semantics usually leads to huge or infinite reachability graphs. Then, a more compact representation for this semantics is defined as a symbolic state graph whose nodes possibly encode infinitely many values for the variables. Both representations are shown behaviourally equivalent.	aggregate data;causal filter;coloured petri net;data structure;denotational semantics;encode;experiment;fifo (computing and electronics);mathematical optimization;reachability;unbounded nondeterminism	Franck Pommereau;Raymond R. Devillers;Hanna Klaudel	2009	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2009.05.034	combinatorics;discrete mathematics;stochastic petri net;computer science;theoretical computer science;mathematics;process architecture;programming language;petri net	Logic	-10.690083856291404	23.845722892929942	27759
28c7e8c939c46f82abed8d8f2da56d4614c26ec8	towards test case generation for synthesizable vhdl programs using model checker	synthesizable vhdl;formal system specification;reliability engineering;timed automata test case generation synthesizable vhdl programs model checker formal system specification;software testing;model checker;formal specification;timed automata test case generation model checking synthesizable vhdl program transformation;clocks;real time;radiation detectors;automatic testing;hardware description languages;semantics;program transformation;program verification;automata;computational modeling;test case generation;model checking;program testing;conferenceobject;automata system testing automatic testing circuit synthesis field programmable gate arrays software testing computational modeling safety mathematical model reliability engineering;vhdl;safety;integrated circuit modeling;performance model;mathematical model;system testing;program verification formal specification hardware description languages program testing;timed automata;field programmable gate arrays;circuit synthesis;synthesizable vhdl programs	VHDL programs are often tested by means of simulations, relying on test benches written intuitively. In this paper, we propose a formal approach to construct test benches from system specification. To consider the real-time properties of VHDL programs, we first transform them to timed automata and then perform model checking against the properties designated from the specification. Counterexamples returned from the model checker serve as a basis of test cases, i.e. they are used to form a test bench. The approach is demonstrated and complemented by a simple case study.	automata theory;logic synthesis;model checking;petri net;real-time clock;simulation;software engineering;software testing;system under test;temporal logic;test bench;test case;test suite;timed automaton;vhdl	Tolga Ayav;Tugkan Tuglular;Fevzi Belli	2010	2010 Fourth International Conference on Secure Software Integration and Reliability Improvement Companion	10.1109/SSIRI-C.2010.22	model checking;computer architecture;real-time computing;computer science;semantics;programming language	SE	-15.482168049201302	29.11797405743762	27805
29107f402d61549228738f5b92f241fd04235e75	a declarative characterization of disjunctive paraconsistent answer sets	substructural logic;satisfiability	In this work, paraconsistent answer sets for extended disjunctive logic programs are presented in terms of a fully declarative approach. In order to do that, we introduce a frame-based semantics. It is known frames are a powerful and elegant tool which have been used to characterise and study substructural logics. Unlike the original definition, no kind of syntactic operation (like the program reduct) is employed in the process. Indeed, paraconsistent answer sets are defined simply by minimising models satisfying some particular conditions. Furthermore, considering that paraconsistent answer sets embed both answer sets and stable models, these semantics are also captured via frames.	answer set programming;declarative programming;denotational semantics;disjunctive normal form;european conference on artificial intelligence;frame language;framing (world wide web);fundamenta informaticae;game semantics;general frame;john d. wiley;journal of logic and computation;journal of the acm;logic programming;logical connective;paraconsistent logic;recursive set;rough set;well-founded semantics;whole earth 'lectronic link	João Alcântara;Carlos Viegas Damásio;Luís Moniz Pereira	2004				AI	-13.601676980937244	14.942297883001467	27859
54909fca9aea67346311f0851ff5442a78d6a580	model streaming for distributed multi-context systems		Multi-Context Systems (MCS) are instances of a nonmonotonic formalism for interlinking heterogeneous knowledge bases in a way such that the information flow is in equilibrium. Recently, algorithms for evaluating distributed MCS have been proposed which compute global system models, called equilibria, by local computation and model exchange. Unfortunately, they suffer from a bottleneck that stems from the way models are exchanged, which limits the applicability to situations with small information interfaces. To push MCS to more realistic and practical scenarios, we present a novel algorithm that computes at most k ≥ 1 models of an MCS using asynchronous communication. Models are wrapped into packages, and contexts in an MCS continuously stream packages to generate at most k models at the root of the system. We have implemented this algorithm in a new solver for distributed MCS, and show promising experimental results.	backtracking;boolean satisfiability problem;computation;consistency model;constraint satisfaction problem;expspace;hex;information access;international standard serial number;knowledge base;multi categories security;parallel computing;privacy;satisfiability modulo theories;semantics (computer science);solver;stable model semantics;streaming algorithm;time complexity	Minh Dao-Tran;Thomas Eiter;Michael Fink;Thomas Krennwallner	2011			computation;asynchronous communication;information flow (information theory);distributed computing;formalism (philosophy);bottleneck;solver;computer science	AI	-18.81543298586993	12.43868697250774	27916
7e205916fb910bbb8f9a6341bc2454add66d0e90	detection and resolution of interactions between services of telephone networks		With the rapid increase of services for telecommunications systems, the designers of services have to resolve the feature interaction problem. The latter occurs when several services behave in an incorrect way once they are put together. In this paper, we present a methodology for the detection and resolution of such a problem. A rst interest of our approach is that the study is achieved at the formal speciication level. Another interest is that both detection and resolution problems are tackled. An originality of the used approach is that the resolution is achieved without changing the services involved in the interaction, by adding controllers which force the services to behave in a desirable way. For that, we have determined suucient conditions on services which make such resolution possible. Several examples illustrate our method.	feature interaction problem;resolution (logic)	Ahmed Khoumsi	1997			telephone network;speech recognition;computer science	Robotics	-32.99124180354445	30.515555972811814	27930
2f6c81cad9fbfb6ae79745871b9ae546dec08c09	a query language for statistical databases	query language	In this chapter we describe a high level. screen-oriented database query language, Summary-Tab1e-by-Examp1e (STBE) for manipulating summary data in statistical databases. STBE uses aggregate functions, relations with set-valued attributes and summary tables to extract and format summary data in tabular form. STBE is similar to Query-byExample, and is a superset of another language, Aggregates-by-Examp1e (ABE). Summary tables are shown to be information equivalent to a set of relations. This equivalency allows STBE to convert a summary table referenced in a query into a set of relations and to evaluate the query ina uniform manner. The expressive power of STBE is at least that of the relational calculus that allows set-valued attributes and aggregate functions. STBE is compared with ABE and SQL. Access path selection in STBE is discussed briefly.	aggregate data;aggregate function;database;expressive power (computer science);high-level programming language;query language;relational calculus;sql;table (information)	Z. Meral Özsoyoglu;Gultekin Özsoyoglu	1985			sargable;data definition language;query optimization;.ql;query expansion;ranking;universal networking language;boolean conjunctive query;data manipulation language;data control language;sparql;query by example;rdf query language;web search query;view;query language;object query language;spatial query	DB	-30.608910731164777	8.042453671551472	28003
d3bb1741c631108596d62020a25050837e4ee8e1	experiences in object-oriented modeling of a real database application	object oriented model;relational database;design and implementation;object oriented	Object-orientation as the catch phrase of the 90ies in computer science is said to model reality in a better way. To find out if this is true, a company which produces photographic base paper initiated a study. The following article summarises the proceeding during the analysis, design, and implementation, and presents the results and experiences gathered during coupling object-oriented modeling and relational databases.		Jutta Göers;K.-P. Lisson;Hans-Günther Linde-Göers	1996		10.1007/BFb0034691	data modeling;database theory;method;semi-structured model;entity–relationship model;relational database;computer science;database model;data mining;database;programming language;object-oriented programming;database schema;database design;component-oriented database	DB	-31.246590593813995	10.7654583476761	28043
404af95b947fa70a7f6a06fc6c21f6a91a680f8a	an optimized algorithm for diagnosability of component-based systems	algorithms;distributed systems	Diagnosability is a crucial system property that determines at design stage how accurate any diagnosis algorithm can be on a partially observable system. The existence of two indistinguishable behaviors, i.e. holding the same observations, with exactly one of them containing the fault violates the diagnosability property. A classical approach for diagnosability verification consists in constructing a finite state machine called twin plant to search for a path representing such indistinguishable behaviors, called a critical path. To avoid the unrealistic hypothesis about the monolithic model of a complex system, recent work constructs local twin plants and then incrementally synchronizes some of them until diagnosability is decided without computing the impractical global twin plant. In this paper, we optimize the distributed approach by abstracting necessary and sufficient diagnosability information from local twin plants to check the existence of critical paths. Thus diagnosability can be analyzed with as small search space as possible. Furthermore, our approach describes how to improve the diagnosis algorithm by using our diagnosability results in a formal way when the system is verified to be diagnosable. Finally, when the system is not diagnosable, the algorithm returns some useful information about its indistinguishable behaviors, which can help in upgrading system diagnosable level.	complex system;complex systems;component-based software engineering;computation;critical path method;distributed algorithm;finite-state machine;formal verification;medical algorithm;partially observable system	Lina Ye;Philippe Dague	2010		10.3182/20100830-3-DE-4013.00025	discrete mathematics;mathematics;distributed computing;algorithm	AI	-7.387808982478722	27.799176120005136	28062
f5f93cc5d0814a7e160b3e2f581c4c9dd6690086	toward the partial evaluation of clp languages	clp language;partial evaluation	In this paper we present a schema for constructing partial evaluators for Constraint Logic Programming (CLP) languages. Our method is based on applying meaningpreserving program transformations to program-query pairs. The three fundamental transformations are specialization, unfolding, and fact orization, which we rigorously define for CLP languages, and which we show preserve breadth-first equivalence. The factorization transformation generalizes three previously stndied techniques: constraint lifting, reparameterization, and auxiliary call creation. We also show that if some restrictions are placed on these transformations they preserve depth-first equivalence as well. A partial evaluator which employs these transformations is presented. It is guaranteed to terminate and generates a residual program containing none of the clauses of the original program. To illustrate the fundamental ideas, we present a detailed example of transforming a CLP(BOO1) program defining a boolean adder circuit into a bit-shifting program when the query is to add a bit-string to itself.	adder (electronics);bitwise operation;breadth-first search;constraint logic programming;depth-first search;interpreter (computing);lifting scheme;partial evaluation;partial template specialization;program transformation;terminate (software);turing completeness;unfolding (dsp implementation)	Timothy J. Hickey;Donald A. Smith	1991		10.1145/115865.115871	computer science;programming language;partial evaluation;algorithm	PL	-16.694100934792832	22.202439312375677	28095
0363e511559e752072a3696b459a341b8f8763f6	transforming lazy functions using comportment properties	lenguaje programacion;semantica denotacional;programming language;program transformation;automatic programming;transformation programme;functional programming;transformacion programa;denotational semantics;langage programmation;programmation fonctionnelle;static analysis;abstract interpretation;programacion funcional;semantique denotationnelle;generic programming	1 I n t r o d u c t i o n If a function is lazy, its argument must be passed as a suspended computation, or thank. This object is typically created on the heap, or boxed, and the function using it must inspect the thunk to determine whether it has been ewluated. It has long been recognized that these thunks are a major obstacle to the application of conventional compiler technology to lazy functional languages. They have been attacked using strictness analysis [9], which determines that certain sub-expressions must be evaluated in order to produce the final output. Instead of being suspended, such expressions may be safely evaluated early, and their values passed in an unboxed form on the stack. There is also a dual approach, known as termination analysis, which determines that certain sub-expressions will have been evaluated by a certain point. If we know that an argument of a function will have been evaluated by the t ime the function is called, we can pass the argument in unboxed form to a call-by-value variant of the function (even if the function is not strict). A typical functional language incorporates lazy data structures, and strictness anMysis techniques have been developed that produce information about latent demand [14]. Similarly termination analysis may be extended to produce statements about latent termination: though an expression might not terminate, it may be that if it does then its components will also have been evaluated. The information supplied by these analyses has been used to improve compiler performance, but the changes to the code generator are seldom formally	code generation (compiler);compiler;computation;data structure;functional programming;lazy evaluation;object type (object-oriented programming);strictness analysis;terminate (software);termination analysis;thunk	Ross Paterson	1997		10.1007/BFb0033840	computer science;theoretical computer science;mathematics;programming language;functional programming;generic programming;static analysis;denotational semantics;algorithm	PL	-21.90181767709644	25.98609389046916	28154
fd77b7e22ad928cf4d669c5abbbda52d80fd71ac	formal semantics analysis for vhdl rtl synthesis	formal semantics;language level optimization;rtl synthesis;vhdl		vhdl	Haifeng Zhou;Wei Cao;Zhenghui Lin	2003	Comput. Syst. Sci. Eng.		formal methods;semantics of logic;programming language;action semantics;denotational semantics;computer science;formal verification;operational semantics;vhdl	Logic	-16.029804537026948	28.570558348475338	28170
390f1ee73744a4b65bfe5de4f45078a7f148700c	certification for mu-calculus with winning strategies		We define memory-efficient certificates for μ-calculus model checking problems based on the well-known correspondence of the μ-calculus model checking with winning certain parity games. Winning strategies can independently checked, in low polynomial time, by observing that there is no reachable strongly connected component in the graph of the parity game whose largest priority is odd. Winning strategies are computed by fixpoint iteration following the naive semantics of μ-calculus. We instrument the usual fixpoint iteration of μ-calculus model checking so that it produces evidence in the form of a winning strategy; these winning strategies can be computed in polynomial time in |S| and in space O(|S||φ|), where |S| is the size of the state space and |φ| the length of the formula φ. On the technical level our work can be seen as a new, simpler, and immediate constructive proof of the correspondence between μ-calculus and parity games.	connected component (graph theory);fixed point (mathematics);fixed-point iteration;modal μ-calculus;model checking;naive semantics;parity bit;polynomial;state space;strongly connected component;time complexity;whole earth 'lectronic link	Martin Hofmann;Harald Ruess	2016		10.1007/978-3-319-32582-8_8	discrete mathematics;computer science;artificial intelligence;mathematics;algorithm	Logic	-6.910293567340663	23.30228224018671	28179
0a2881a7f4e73c275151c025844286c85954073f	in the meaning of essentially unprovable theorems in the presburger theory of addition	presburger theory;unprovable theorems	Without Abstract		Giovanni Faglia;Paul Young	1993		10.1007/BFb0022567	discrete mathematics;presburger arithmetic;mathematics;algebra	Logic	-9.476036738463726	12.577903644856477	28215
7596341878dd1e9e07e7d0cbd93340d56a926829	jbtools: an experimental platform for the formal b method	incremental development;user interface;code generation;b method;formal method;software requirements;experimental research;open source	The B method, developed by Jean-Raymond Abrial, is a formal method which enables an incremental development process ( also known as the process of refinement) which stems from an abstract specification. A B specification requires a certain knowledge of mathematical notations (set and classical logic) as well as specific terminology (Machines, Generalized substitutions, Machines composition). The whole process is of course supported by high-performance industrial tools (among which is an automatic prover) such as ClearSy’s Atelier B or B-Core’s B Toolkit; however, the distribution of the B Method and the creation of experimental research tools face constraints of the market for existing tools: cost, limited access, and limited number of systems (hardware and software) requiring this kind of support. The purpose of the jBTools project is to resolve this problem by proposing an open-source platform base and experimental tools. In this article, we will present the architecture of the platform, the available base tools (User Interface, Parser, Type Checker) producing an XML format and the optimized code generator for the French project RNTL BOM.	b-method;browser object model;code generation (compiler);experiment;formal methods;iterative and incremental development;jean;jean-raymond abrial;lisp machine;open-source software;parser;refinement (computing);type system;user interface;xml	Jean-Christophe Voisinet	2002		10.1145/638476.638504	b-method;computer science;theoretical computer science;database;programming language	SE	-33.423756330122885	25.198085320687284	28222
e82234f36521ebbc8996b30220b60ceef6aa8614	on using conditional definitions in formal theories	formal specification;teoria conjunto;theorie ensemble;logical programming;program verification;set theory;specification formelle;theorem proving;especificacion formal;demonstration theoreme;verificacion programa;programmation logique;demostracion teorema;verification programme;programacion logica	In this paper, our intention is to explore the notion of definition in formal theories and, in particular, that of conditional definitions. We are also interested in analyzing the consequences of the latter on the structure of corresponding proof systems. Finally, we shall investigate the various ways such proof systems can be simplified. In formal texts, conditional definitions lead to such oddities as division by zero, the minimum of an empty set, or, more generally, the application of a function to an argument lying outside its domain. In the presence of such illdefined expressions, people usually divide in two groups. A first group considers that such pathologies are totally uninteresting, that professional mathematicians never write things of that kind, and that it would correspond, in every day life, to people not mastering their mother tongue (so that the problem really is that of having people first learn how to correctly express themselves). A second group, especially among the formal developers, considers that this is a serious question, that it may lead to crashes, and that it is thus a problem that one must face and somehow “solve” (we shall see below that a quite a number of “solutions” have been proposed). In this paper, we again study the problem and try to give our view and contribution to it. Our “solution” is certainly not entirely novel but it opens, we think, a number of interesting ways to be explored in the domain of mechanized proof strategy . As our work is done in relation with B, we shall make our investigation on a specific formal theory, namely set theory as it has been re-constructed in the B-Book [1]. Before developing our main subject, it is certainly worthwhile recalling what we understand by a proper concept of definition in a formal theory. For this, we shall follow the way this notion was presented in an introductory book of logic written by P. Suppes [9] in the fifties, where an entire chapter is devoted to that question. The present paper is to be read with that very understanding of the concept of definition in mind.	bus mastering;crash (computing);division by zero;set theory;theory (mathematical logic)	Jean-Raymond Abrial;Louis Mussat	2002		10.1007/3-540-45648-1_13	discrete mathematics;computer science;formal specification;mathematics;automated theorem proving;programming language;algorithm;set theory	PL	-9.575980279362371	5.70654488192835	28225
540a5ec500d11d0e6baaeb461d2db6739599d6de	on a characterization of the lattice of subsystems of a transition system	transition systems	It was first proved by Birkhoff and Frink, and the result now belongs to the folklore, that any algebraic lattice is up to isomorphism the lattice of subuniverses of a universal algebra. A study of subsystems of a transition system yields a new algebraic concept, that of a strongly algebraic lattice. We give here a representation theorem to the manner of Birkhoff and Frink of such lattices. A transition system is a pair (S, S), where (i) S is a set of states, (ii) S ⊆ S × S is the transition relation. We write s S s for (s,s) ∈ S. Nondeterministic transition systems, those (S, S) for which the set of successors of any element s ∈ S is an arbitrary set, are easily seen to be coalgebras of the covariant powerset functor ᏼ: Sets → Sets from the category of sets to itself. Observe that any unary algebra (S,Ᏺ) gives rise to a unique transition system (S, S), but the converse in the general case is false. A subsystem of a transition system (S, S) is a subset X of S which has the following stability property: s S s and s ∈ X imply s ∈ X. The empty set and the universe S are subsystems of (S, S), they are said to be trivial. It is straightforward to check that the set Subs(S) of subsystems of (S, S) is stable for arbitrary unions and intersections. Given a subset X of S, we denote by X the subsystem of (S, S) generated by X. It is the intersection of all subsystems of (S, S) containing X. The notation * S will be used to denote the reflexive and transitive closures of the binary relation	algebraic equation;birkhoff interpolation;linear algebra;powerset construction;state (computer science);transition system;transitive closure;unary operation	Jean-Paul Mavoungou;Celestin Nkuimi Jugnia	2006	Int. J. Math. Mathematical Sciences	10.1155/IJMMS/2006/82318	combinatorics;mathematical analysis;discrete mathematics;topology;pure mathematics;mathematics;geometry;map of lattices;algebra	Logic	-4.5971229398813485	17.683401112187607	28253
38116c9e091014d517c16059be61854c354a7c5f	ordered epistemic logic: semantics, complexity and applications	epistemic logics;answer set programming;modal logic;default logics;ordered epistemic logic;reasoning with different knowledge bases;knowledge representation;problem solving in epistemic logic;model generation for epistemic logic	Many examples of epistemic reasoning in the literature exhibit a stratified structure: defaults are formulated on top of an incomplete knowledge base. These defaults derive extra information in case information is missing in the knowledge base. In autoepistemic logic, default logic and ASP this inherent stratification is not preserved as they may refer to their own knowledge or logical consequences. Defining the semantics of such logics requires a complex mathematical construction. As an alternative, this paper further develops ordered epistemic logic. This logic extends first order logic with a modal operator and stratification is maintained. This allows us to define an easy to understand semantics. Moreover, inference tasks have a lower complexity than in autoepistemic logic and the logic integrates seamlessly into classical logic and its extensions. In this paper we also propose a generalization of ordered epistemic logic, which we call distributed ordered epistemic logic. We argue that it can provide a semantic foundation for a number of distributed knowledge representation formalisms found in the literature.	autoepistemic logic;closed-world assumption;complexity;default logic;epistemic modal logic;first-order logic;knowledge base;knowledge representation and reasoning;modal operator;oled	Hanne Vlaeminck;Joost Vennekens;Maurice Bruynooghe;Marc Denecker	2012			modal logic;predicate logic;dynamic logic;zeroth-order logic;normal modal logic;knowledge representation and reasoning;classical logic;description logic;higher-order logic;epistemic modal logic;many-valued logic;computer science;intermediate logic;artificial intelligence;bunched logic;non-monotonic logic;answer set programming;computational logic;epistemic possibility;logic;substructural logic;multimodal logic;algorithm;philosophy of logic;autoepistemic logic	AI	-18.154343337671182	8.439748781080201	28259
5ab31c52e0e2e2ad4b5934a1302a17dac0d33f20	gosat: floating-point satisfiability as global optimization		We introduce goSAT, a fast and publicly available SMT solver for the theory of floating-point arithmetic. We build on the recently proposed XSat solver [1] which casts the satisfiability problem to a corresponding global optimization problem. Compared to XSat, goSAT is an integrated tool combining JIT compilation of SMT formulas and NLopt, a feature-rich mathematical optimization backend. We evaluate our tool using several optimization algorithms and compare it to XSat, Z3, and MathSat. Our evaluation demonstrates promising results.	algorithm;automatic differentiation;boolean satisfiability problem;compiler;emoticon;executable;global optimization;just-in-time compilation;mathematical optimization;optimization problem;satisfiability modulo theories;software feature;solver;z3 (computer)	M. Ammar Ben Khadra;Dominik Stoffel;Wolfgang Kunz	2017	2017 Formal Methods in Computer Aided Design (FMCAD)	10.23919/FMCAD.2017.8102235	global optimization;theoretical computer science;computer science;floating point;just-in-time compilation;satisfiability;satisfiability modulo theories;mathematical optimization;boolean satisfiability problem;solver	EDA	-15.62950166454411	24.69814207955634	28282
583876dd888ef699cc61ca557334da6178bb0bd5	qemu/cpc: static analysis and cps conversion for safe, portable, and efficient coroutines	coroutines;static analysis;cps conversion	Coroutines and events are two common abstractions for writing concurrent programs. Because coroutines are often more convenient, but events more portable and efficient, it is natural to want to translate the former into the latter. CPC is such a source-to-source translator for C programs, based on a partial conversion into continuation-passing style (CPS conversion) of functions annotated as cooperative.  In this article, we study the application of the CPC translator to QEMU, an open-source machine emulator which also uses annotated coroutine functions for concurrency. We first propose a new type of annotations to identify functions which never cooperate, and we introduce CoroCheck, a tool for the static analysis and inference of cooperation annotations. Then, we improve the CPC translator, defining CPS conversion as a calling convention for the C language, with support for indirect calls to CPS-converted function through function pointers. Finally, we apply CoroCheck and CPC to QEMU (750 000 lines of C code), fixing hundreds of missing annotations and comparing performance of the translated code with existing implementations of coroutines in QEMU.  Our work shows the importance of static annotation checking to prevent actual concurrency bugs, and demonstrates that CPS conversion is a flexible, portable, and efficient compilation technique, even for very large programs written in an imperative language.	calling convention;cartesian perceptual compression;concurrency (computer science);continuation;continuation-passing style;coroutine;emulator;function pointer;imperative programming;mod (video gaming);open-source software;software bug;source-to-source compiler;static program analysis	Gabriel Kerneis;Charlie Shepherd;Stefan Hajnoczi	2014		10.1145/2543728.2543733	embedded system;real-time computing;computer science;operating system;coroutine;programming language;static analysis	PL	-21.048005559960703	28.678639428612577	28305
b2ec6a8a9370e9aa15bff950fbe7a735a992bf11	validity of positive xpath queries with wildcard in the presence of dtds		This paper discusses the validity problem for positive XPath queries with wildcard in the presence of DTDs. A given XPath query p is valid under a DTD D if, for every XML document T conforming to D, the answer to p on T is nonempty. The validity problem is one of the basic static analyses of queries, together with the satisfiability and the containment problems. Although the validity problem is the dual of the satisfiability problem, the complexity of validity for positive XPath classes is not obvious because the XPath class does not contain the negation operator. In this paper, first, it is shown that the path union operator in XPath queries easily makes the validity problem intractable. Then, we focus on wildcard, which is a special case of path union and more popular than path union in the real world. Interestingly, wildcard together with child and descendant-orself axes and qualifier causes intractability while the validity problem becomes tractable for XPath classes defined by any combination of three of child axis, descendant-or-self axis, qualifier, and wildcard.	apache axis;boolean satisfiability problem;cobham's thesis;static program analysis;wildcard character;xpath	Kenji Hashimoto;Yohei Kusunoki;Yasunori Ishihara;Toru Fujiwara	2011			database;wildcard;operator (computer programming);xml;programming language;xpath;negation;satisfiability;computer science;boolean satisfiability problem;special case	DB	-7.877142500374247	17.179463588922477	28315
d658d95d0b4663c1e918e30e0602ed65a8075807	ontology database: a new method for semantic modeling and an application to brainwave data	relational database;semantic model;integrity constraints;semantic web;knowledge base	We propose an automatic method for modeling a relational database that uses SQL triggers and foreign-keys to efficiently answer positive semantic queries about ground instances for a Semantic Web ontology. In contrast with existing knowledge-based approaches, we expend additional space in the database to reduce reasoning at query time. This implementation significantly improves query response time by allowing the system to disregard integrity constraints and other kinds of inferences at run-time. The surprising result of our approach is that loadtime appears unaffected, even for medium-sized ontologies. We applied our methodology to the study of brain electroencephalographic (EEG and ERP) data. This case study demonstrates how our methodology can be used to proactively drive the design, storage and exchange of knowledge based on EEG/ERP ontologies.	data integrity;erp;electroencephalography;loader (computing);neural oscillation;ontology (information science);relational database;response time (technology);sql;semantic web	Paea LePendu;Dejing Dou;Gwen A. Frishkoff;Jiawei Rong	2008		10.1007/978-3-540-69497-7_21	semantic data model;knowledge base;semantic similarity;semantic computing;relational database;computer science;semantic web;social semantic web;data integrity;data mining;semantic web stack;database;view;graph database;information retrieval;semantic analytics;database design	DB	-24.904760740224063	7.911483343315781	28362
17ad8e78144239e4a5d110045f4caf7e2eb50956	theorems of peano arithmetic are buridan-volpin recursively satisfable	peano arithmetic	A b s t r a c t. The notion of recursive satisfaction is extended from prenex 89 arithmetic sentences to any rst-order arithmetic sentence by allowing the scope of a negative (existential) quantier to depend on positive (universal) quantiers which may lie within its scope.	peano axioms;recursion	David Isles	1997	Reports on Mathematical Logic		second-order arithmetic;algebra;discrete mathematics;mathematics;gentzen's consistency proof;peano curve;hilbert's second problem;robinson arithmetic;pa degree;non-standard model of arithmetic;true arithmetic	Theory	-9.650627003153923	14.254631263848415	28379
7dde79fca394399b5dc11f5c2c3ceeeedb92cb1f	model checking semi-continuous time models using bdds	timed systems;continuous time models;model checking;symbolic model checking	The veri cation of timed systems is extremely important but also extremely di cult Several methods have been proposed to assist in this task including extensions to symbolic model checking One possible use of model checking to analyze timed systems is by modeling passage of time as the number of taken transitions and ap plying quantitative algorithms to determine the timing parameters of the system The advantage of this method is its simplicity and e ciency In this paper we extend this technique in two ways First we present new quantitative algorithms that are more e cient than their predecessors The new algorithms determine the number of occurrences of events in all paths between a set of starting states and a set of nal states We then use these algorithms to introduce a new model of time in which the passage of time is dissociated from the occurrence of events With this new model it is possible to verify systems that were previously thought to require dense time models We use the new method to verify two such examples previously analyzed by the HyTech tool a steam boiler example and a fuel injection controller	algorithm;model checking	Sérgio Vale Aguiar Campos;Marcio Teixeira;Marius Minea;Andreas Kuehlmann;Edmund M. Clarke	1999	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(04)80670-4	model checking;real-time computing;computer science;theoretical computer science;programming language;abstraction model checking;symbolic trajectory evaluation;algorithm	Logic	-12.818788193122964	26.2243649257494	28408
4a87a3c9e7d0e70efaf863892bc704e5889e0157	model checking propositional projection temporal logic based on spin	verification;propositional projection temporal logic;nondeterministic finite automaton;temporal logic;deterministic finite automaton;automaton;satisfiability;spin;model checking;normal form	This paper investigates a model checking algorithm for Propositional Projection Temporal Logic (PPTL) with finite models. To this end, a PPTL formula is transformed to a Normal Form Graph (NFG), and then a Nondeterministic Finite Automaton (NFA). The NFA precisely characterizes the finite models satisfying the corresponding formula and can be equivalently represented as a Deterministic Finite Automaton (DFA). When the system to be verified can be modeled as a DFA As, and the property of the system can be specified by a PPTL formula P, then ¬P can be transformed to a DFA Ap. Thus, whether the system satisfies the property or not can be checked by computing the product automaton of As and Ap, and then checking whether or not the product automaton accepts the empty word. Further, this method can be implemented by means of the verification system SPIN.	model checking;spin;temporal logic	Cong Tian;Zhenhua Duan	2007		10.1007/978-3-540-76650-6_15	powerset construction;levenshtein automaton;model checking;nondeterministic finite automaton with ε-moves;block cellular automaton;nested stack automaton;büchi automaton;verification;nondeterministic finite automaton;state diagram;temporal logic;computer science;two-way deterministic finite automaton;deterministic finite automaton;probabilistic automaton;continuous automaton;deterministic automaton;automaton;spin;dfa minimization;timed automaton;pushdown automaton;generalized nondeterministic finite automaton;algorithm;satisfiability	Logic	-10.425284521900952	23.91168111677233	28410
5ce429d6a0ce8daad66f6550a8273b21a6ee9fc2	two modellings for theory change				Adam Grove	1988	J. Philosophical Logic	10.1007/BF00247909	belief revision;algebra;ramsey reset test;discrete mathematics;mathematics	Logic	-11.110625540054675	10.993502882624194	28441
fc99579bfd0c4a1dd07eb45b87be7f4f641661b1	a concurrent simulator for petri nets based on the paradigm of actors of hewitt.		In this paper we propose a concurrent simulator for Petri nets based on the model of Actors of Hewitt. The classes of Petri nets that are supported for the simulation are Place-Transition Nets and Elementary Nets. The simulator is written in Scala, a programming language with a library implementing the Actors model.	apl;actor model;algorithm;concurrency (computer science);finite-state machine;graphical user interface;java;petri net;programming language;scala;simulation	Luca Bernardinello;Francesco Adalberto Bianchi	2012			computer science;artificial intelligence;process architecture;operations research;petri net	EDA	-24.625400379361597	22.877880495827306	28540
1a95e5233ea50def1cda5b3552ec7a7cbfa24452	final semantics for a higher order concurrent language	second order;concurrent language;complete metric space;operational semantics;higher order	We show that adequate semantics can be provided for imperative higher order concurrent languages simply using syntactical nal coalgebras. In particular we investigate and compare various behavioural equivalences on higher order processes deened by nality using hyper-sets and c.m.s.'s. Correspondingly, we derive various coinduction and mixed induction-coinduction proof principles for establishing these equivalences .	coinduction;imperative programming;parallel computing	Marina Lenisa	1996		10.1007/3-540-61064-2_32	mathematics;linguistics;programming language;operational semantics;denotational semantics;algorithm	Logic	-13.55686138355381	18.104624935854673	28570
a7f2c5d158c74faee5c8b79152642e7a304ec710	verification of two-variable logic revisited	verification;automata complexity theory markov processes polynomials probabilistic logic nickel;temporal logic automata theory formal verification markov processes;temporal logic;formal verification;probabilistic model checking;two variable logic;automata theory;markov processes;markov decision process two variable logic first order logic decidable verification problem automata theoretic technique linear temporal logic ltl probabilistic verification bootstrapping fo 2 verification technique model checking automata translation nondeterministic system recursive markov chain;probabilistic model checking two variable logic verification	Two-variable logic is a fragment of first-order logic that allows for decidable verification problems. In previous work, we developed an approach to FO2 verification that is particularly useful for probabilistic systems, based on analysis of the translation of FO2 to automata. In this work we show that the techniques introduced there can be applied to give information on other logics, and can be used in conjunction with automata-theoretic techniques for Linear Temporal Logic (LTL) in the context of probabilistic verification. First we revisit the technique of our prior work starting with FO2 without the successor relation. Making use of recent results by Weis we show here that we can get quite small automata for these formula. We then show that we can recapture the automata size bounds for general FO2 formulas by bootstrapping results for FO2 without successor. Next, we look at combining FO2 verification techniques with those for LTL. We present here a language that subsumes both FO2 and LTL, and inherits the model checking properties of both languages. Our automata translation gives new bounds on model-checking for this large language for non-deterministic systems, and is particularly useful for probabilistic systems: e.g Markov Chains, Recursive Markov Chains, and Markov Decision Processes.	automata theory;automaton;fallout 2;first-order logic;first-order predicate;formal verification;linear temporal logic;markov chain;markov decision process;model checking;recursion (computer science);two-variable logic;verification and validation	Michael Benedikt;Rastislav Lenhardt;James Worrell	2012	2012 Ninth International Conference on Quantitative Evaluation of Systems	10.1109/QEST.2012.38	discrete mathematics;linear temporal logic;description logic;verification;probabilistic ctl;temporal logic;formal verification;quantum finite automata;interval temporal logic;computation tree logic;computer science;theoretical computer science;automata theory;mathematics;runtime verification;probabilistic logic;markov process;programming language;probabilistic argumentation;multimodal logic;algorithm;statistics;temporal logic of actions	Logic	-11.75541870993316	23.54715167457998	28578
d239b22bb8ad39995bca6134a4cd70f91f14c3b5	the hr program for theorem generation	automatic proving;demostracion automatica;group theory;constraint satisfaction;theorem proving;demonstration automatique;generation theoreme;demonstration theoreme;satisfaction contrainte;satisfaccion restriccion;demostracion teorema	Automated theory formation involves the production of objects of interest, concepts about those objects, conjectures relating the concepts and proofs of the conjectures. In group theory, for example, the objects of interest are the groups themselves, the concepts include element types, subgroup types, etc., the conjectures include implication and if-and-only-if conjectures and these become theorems if they are proved, non-theorems if disproved. Similar to Zhang’s MCS program [11], the HR system [1] – named after mathematicians Hardy and Ramanujan – performs theory formation in mathematical domains. It works by (i) using the MACE model generator [9] to generate objects of interest from axiom sets (ii) performing the concept formation and conjecture making itself and (iii) using the Otter theorem prover [8] to prove conjectures. In domains where Otter and MACE are effective, HR can produce large numbers of theorems for testing automated theorem provers (ATPs), or smaller numbers of prime implicates, which represent some of the fundamental facts in a domain. We explain how HR operates in §2 and give details of a representative session in §3. As discussed in §4, the applications of HR to automated reasoning include the generation of constraints for constraint satisfaction problems, the generation of lemmas for automated theorem proving, and the production of benchmark theorems for the TPTP library of test problems for ATP systems [10]. HR is a Java program available for download here: www.dai.ed.ac.uk/ ̃simonco/research/hr.	automated reasoning;automated theorem proving;benchmark (computing);concept learning;constraint satisfaction problem;download;java;kronos;test & performance tools platform	Simon Colton	2002		10.1007/3-540-45620-1_24	discrete mathematics;constraint satisfaction;computer science;artificial intelligence;calculus;mathematics;automated theorem proving;programming language;group theory	AI	-5.268288513276299	15.646541979966948	28582
c0152427177fa66bc521ab04a6861975bf32b01d	algebraic file synchronization: adequacy and completeness		With distributed computing and mobile applications, synchronizing diverging replicas of data structures is a more and more common problem. We use algebraic methods to reason about filesystem operations, and introduce a simplified definition of conflicting updates to filesystems. We also define algorithms for update detection and reconciliation and present rigorous proofs that they not only work as intended, but also cannot be improved on. To achieve this, we introduce a novel, symmetric set of filesystem commands with higher information content, which removes edge cases and increases the predictive powers of our algebraic model. We also present a number of generally useful classes and properties of sequences of commands. These results are often intuitive, but providing exact proofs for them is far from trivial. They contribute to our understanding of this special type of algebraic model, and toward building more complete algebras of filesystem trees and extending algebraic approaches to other data storage protocols. They also form a theoretical basis for specifying and guaranteeing the error-free operation of applications that implement an algebraic approach to synchronization.	approximation;blueprint;central processing unit;computer data storage;correctness (computer science);data structure;distributed algorithm;distributed computing;edge case;file synchronization;formal verification;human error;linear algebra;mobile app;network partition;pointer (computer programming);preprocessor;relational database;rename (relational algebra);self-information;sparse matrix;synchronization (computer science);synchronizer (algorithm);usability;xml	Elöd Csirmaz	2016	CoRR		parallel computing;computer science;theoretical computer science;operating system;distributed computing;programming language;algorithm	PL	-28.21201420163211	14.003222369208622	28590
418ab63749282c2ae73569fd4e974876d3bfdd81	explanations, belief revision and defeasible reasoning	default reasoning;explanations;belief revision;knowledge representation;belief change;defeasible reasoning;change theory	We present different constructions for non-prioritized belief revision, that is, belief changes in which the input sentences are not always accepted. First, we present the concept of explanation in a deductive way. Second, we define multiple revision operators with respect to sets of sentences (representing explanations), giving representation theorems. Finally, we relate the formulated operators with argumentative systems and default reasoning frameworks.	belief revision;david makinson;default logic;defeasible reasoning;sven jaschan	Marcelo A. Falappa;Gabriele Kern-Isberner;Guillermo Ricardo Simari	2002	Artif. Intell.	10.1016/S0004-3702(02)00258-8	knowledge representation and reasoning;computer science;artificial intelligence;non-monotonic logic;mathematics;belief revision;theory of change;defeasible reasoning	AI	-15.810980083031707	6.980107943528129	28656
6d449a20fdc8cea1feecfa8685dd55179b4504f0	integrity checking in deductive databases - the ultimate method?	integrity checking;deductive databases		deductive database	Matilde Celma;Hendrik Decker	1994			computer science;database;programming language	DB	-30.416050848897218	9.55666841893099	28688
4d6ac8c609e0397205dd92a25ef33a02c1ed837f	action models for conditionals	frame problem;actions;conditionals;causality	Possible worlds semantics for conditionals leave open the problem of how to construct models for realistic domains. In this paper, we show how to adapt logics of action and change such as John McCarthy's Situation Calculus to conditional logics. We illustrate the idea by presenting models for conditionals whose antecedents combine a declarative condition with a hypothetical action.		Jeremy Lent;Richmond H. Thomason	2015	Journal of Logic, Language and Information	10.1007/s10849-015-9213-8	frame problem;causality;philosophy;epistemology;computer science;artificial intelligence;pure mathematics;mathematics;algorithm	NLP	-15.841340635843402	5.815760623612626	28740
1b8ae29543ed10da453ac225d3d4e36f66797855	recursive strategies for answering recursive queries - the rqa/fqi strategy	fqi strategy;recursive strategies;answering recursive queries	In this paper we will discuss several methods for recursive query processing using a recursive control structure. We will describe the QSQR method, introduced in [Vie861 and show that it fails to produce all answers in certain cases. After analyzing the causes of this failure we propose an improved algorithm the RQA/FQI Strategy which is complete over the domain of function-free Horn clauses. The new method uses a two step approach recursive expansion + an efficient variant of LFP iteration to evaluate recursive queries. A short comparison of these methods shows the efficiency of RQA/FQI.	algorithm;control flow;database;hierarchical and recursive queries in sql;horn clause;iteration;least fixed point;recurrence quantification analysis;recursion (computer science)	Wolfgang Nejdl	1987			data mining;horn clause;recursion;machine learning;artificial intelligence;computer science	DB	-19.361792137124212	22.78494591013255	28783
8ec135ab9e2b0fc6dd63756945a959bf249e5b33	automatic generation of intelligent diagram editors	intelligent diagram;real time;diagram interaction;automatic generation;diagram parsing;error correction;visual language;constraint solving;pen based computing;geometric constraints;state transition diagram;empirical evaluation;constraint multi set grammars	The intelligent diagram is a recent metaphor for diagramming in which the underlying graphic editor parses the diagram as it is being constructed, performing error correction and collecting geometric constraints that capture the relationships between diagram components. During diagram manipulation a constraint solver uses these geometric constraints to maintain the diagram's semantics. We introduce the Penguins system. This automates the development of graphical editors that support the intelligent diagram metaphor. It takes a grammatical specification of a particular diagram language and generates an editor specialized for the creation, manipulation and parsing of diagrams in that visual language. We extend previous research in this area by allowing more expressive grammars, performing automatic error correction, and detailing how efficient incremental parsing has been achieved. We also provide an empirical evaluation of the system. This shows that the system can be used to generate customized editors for a wide variety of diagram languages, ranging from state transition diagrams to mathematical equations, with real-time incremental parsing and error correction.	diagram;error detection and correction;forward error correction;graphical user interface;image editing;parsing;real-time clock;solver;visual language	Sitt Sen Chok;Kim Marriott	2003	ACM Trans. Comput.-Hum. Interact.	10.1145/937549.937553	block diagram;sequence diagram;timing diagram;state diagram;functional block diagram;use case diagram;state diagram;error detection and correction;system context diagram;communication diagram;interaction overview diagram;computer science;theoretical computer science;class diagram;n2 chart;elementary diagram;composite structure diagram;system sequence diagram;algorithm;component diagram	AI	-27.334287707726805	19.59741999699117	28788
91ef185a9ca42e76503aefd4d50f1f44d04d043b	resolution on formula-trees	structure arborescente;estructura arborescente;tree structure;structure preservation;proof of correctness	We introduce a nonclausal resolution calculus on formula-trees which comprises classical resolution as a special case. The resolvents produced in this calculus are more structure preserving than in nonclausal resolution by Murray and Manna and Waldinger and simpler than in nested resolution by Traugott. Proofs of correctness and completeness are given. In some examples, first experiences made when implementing the calculus are discussed.	correctness (computer science)	Ulf R. Schmerl	1988	Acta Informatica	10.1007/BF02737109	combinatorics;computer science;mathematics;tree structure;algorithm	Theory	-12.742499953709128	15.283686995622135	28799
01db27f8cfe4cbf8fee2a0f4ca0da14f1de772cb	hsp-type characterization of strong equational classes of partial algebras	algebraic specification;strong equational class;partial algebra;equation;equational logic	This paper presents the first purely algebraic characterization of classes of partial algebras definable by a set of strong equations. This result was posible due to new tools such as invariant congruences, i.e. a generalization of the notion of a fully invariant congruence, and extension of algebras, specific for strong equations.		Bogdan Staruch	2009	Studia Logica	10.1007/s11225-009-9208-z	mathematical analysis;discrete mathematics;subalgebra;equational logic;computer science;equation;mathematics;programming language;algebra	Theory	-10.049564044116071	17.463088563192006	28829
2077cc18da002721390a23392ce4a25d19c3e2a2	why and where: a characterization of data provenance	base relacional dato;query language;base donnee;interrogation base donnee;database;interrogacion base datos;base dato;relational database;lenguaje interrogacion;characterization;base donnee relationnelle;langage interrogation;caracterisation;database query;caracterizacion	"""With the proliferation of database views and curated databases, the issue of data provenance where a piece of data came from and the process by which it arrived in the database is becoming increasingly important, especially in scientific databases where understanding provenance is crucial to the accuracy and currency of data. In this paper we describe an approach to computing provenance when the data of interest has been created by a database query. We adopt a syntactic approach and present results for a general data model that applies to relational databases as well as to hierarchical data such as XML. A novel aspect of our work is a distinction between """"why"""" provenance (refers to the source data that had some influence on the existence of the data) and """"where"""" provenance (refers to the location(s) in the source databases from which the data was extracted). Comments Postprint version. Published in Lecture Notes in Computer Science, Volume 1973, International Conference on Database Theory (ICDT 2001), pages 316-330. Publisher URL: http://www.springerlink.com/link.asp?id=edf0k68ccw3a22hu This conference paper is available at ScholarlyCommons: http://repository.upenn.edu/cis_papers/210"""	data model;digital curation;hierarchical database model;international conference on database theory;lecture notes in computer science;relational database;source data;view (sql);xml	Peter Buneman;Sanjeev Khanna;Wang Chiew Tan	2001		10.1007/3-540-44503-X_20	relational database;computer science;database model;data mining;database;view;world wide web;database design;query language	DB	-30.395083608650438	8.834704633462158	28888
728b628cd83f63aeb3c0a716b6b663546902e709	concept of checking integrity constraints in cellular network relational databases	parallel algorithm;intelligent networks land mobile radio cellular systems relational databases algebra read write memory tree graphs;vlsi chip associative algorithm checking integrity constraints cellular network relational databases parallel algorithm generalized dependencies cellular arrays tabular notation special structure dbms database management system;relational database;cellular arrays;chip;special purpose computers;special purpose computers cellular arrays parallel algorithms parallel architectures relational databases;parallel architectures;integrity constraints;cellular network;relational databases;database management system;parallel algorithms	A fully parallel algorithm for checking generalized dependencies in relational databases implemented in cellular arrays is presented. The author defines mappings between general dependencies given in a tabular notation and the special structure in cellular arrays, and then he introduces an algorithm operating in an associative way. He then shows how this algorithm operates for some types of dependencies. It is suggested that any complete DBMS (database management system) can be efficiently implemented in the cellular array architecture as a VLSI chip. >	data integrity;relational database management system	Jaroslaw A. Chudziak	1990		10.1109/PARBSE.1990.77132	relational model;relational database;computer science;theoretical computer science;database;distributed computing	DB	-28.97928283564895	12.173352858187057	28923
27049590cef9134ebcc602a99bff58ae0f8b9047	a type-free theory of half-monotone inductive definitions	inductive definition;redundancy free program;universe;constructive programming;type theory;realizability;reflection	This paper studies an extension of inductive definitions in the context of a type-free theory. It is a kind of simultaneous inductive definition of two predicates where the defining formulas are monotone with respect to the first predicate, but not monotone with respect to the second predicate. We call this inductive definition half-monotone in analogy of Allen’s term half-positive. We can regard this definition as a variant of monotone inductive definitions by introducing a refined order between tuples of predicates. We give a general theory for half-monotone inductive definitions in a type-free first-order logic. We then give a realizability interpretation to our theory, and prove its soundness by extending Tatsuta’s technique. The mechanism of half-monotone inductive definitions is shown to be useful in interpreting many theories, including the Logical Theory of Constructions, and Martin-Lof’s Type Theory. We can also formalize the provability relation “a term p is a proof of a proposition P” naturally. As an application of this formalization, several techniques of program/proof-improvement can be formalized in our theory, and we can make use of this fact to develop programs in the paradigm of Constructive Programming. A characteristic point of our approach is that we can extract an optimization program since our theory enjoys the program extraction theorem.	uninterpreted function;monotone	Yukiyoshi Kameyama	1995	Int. J. Found. Comput. Sci.	10.1142/S0129054195000147	combinatorics;discrete mathematics;reflection;computer science;mathematics;universe;programming language;type theory;extension by definitions;algorithm	Logic	-10.135942142727254	12.583283457237501	28940
147f235ab5c4bbccec542ae82896afbcf6bc70e4	modular reversibility analysis in self-loops connections of petri net systems	petri net systems;reversibility preservation;modular analysis;self loops connection	It is well known that the use of a modular approach for modeling has many advantages: it allows the modeler to consider different parts of the model independently of one another. A modular approach to analysis is also attractive: it often dramatically decreases the complexity of the analysis task. To create Petri net models of large systems, four bottom-up techniques, consisting of sharing operation, synchronous operation, self-loops connection as well as inhibitor-arc connection, have been developed. This paper focus on the concurrent behavior relation in self-loops connections of Petri net systems. First, for the property of dynamic invariance we show that it is possible to decide dynamic invariance of the global system from invariance of the individual modules. Second, for reversibility property we show that it is possible to construct reversibility of total modular system from reversibility of the individual modules without unfolding to the entire state space. Finally, we present some examples to illustrate the effectiveness of our approaches. The advantages of our approaches are in the context of concurrent language and can synthesize Petri net systems beyond asymmetric choice nets.	bottom-up parsing;deadlock;item unique identification;language equation;liveness;loop (graph theory);modular programming;np-hardness;parallel computing;petri net;reachability;recursion;recursive language;reversible cellular automaton;state space;unfolding (dsp implementation)	Fei Pu	2013	JNW	10.4304/jnw.8.2.493-500	computer science;artificial intelligence;petri net;algorithm	Robotics	-9.458145831413415	24.221339482315166	28942
1a68a0d915d1f007bb55827e498a712fbbd576b2	automated verification of automata communicating via fifo and bag buffers	labeled transition systems;asynchronous communication;equivalence checking	This article presents new results for the automated verification of automata communicating asynchronously via FIFO or bag buffers. The analysis of such systems is possible by comparing bounded asynchronous compositions using equivalence checking. When the composition exhibits the same behavior for a specific buffer bound, the behavior remains the same for larger bounds. This enables one to check temporal properties on the system for that bound and this ensures that the system will preserve them whatever larger bounds are used for buffers. In this article, we present several decidability results and a semi-algorithm for this problem considering FIFO and bag buffers, respectively, as communication model. We also study various equivalence notions used for comparing the bounded asynchronous systems.	automata theory;automaton;fifo (computing and electronics);formal verification	Lakhdar Akroun;Gwen Salaün	2018	Formal Methods in System Design	10.1007/s10703-017-0285-8	theoretical computer science;equivalence (measure theory);distributed computing;formal equivalence checking;computer science;asynchronous communication;fifo (computing and electronics);bounded function;decidability	Logic	-10.660045225208405	25.125975325706406	28946
5ce5081517de1f9540bdae905488750804f827d0	a real-time interval logic and its decision procedure	real time temporal logic;real time;decision procedure;graphical representation	Real-Time Future Interval Logic is a visual logic in which formull have a natural graphical representation, resembling timing diagrams. It is a dense real-time temporal logic that is based on two simple temporal primitives: interval modalities for the purely qualitative part and duration predicates for the quantitative part. We give a decision procedure for the logic by reduction to the emptiness problem for Timed B uchi Automata. The decision procedure forms the core of a proof checker for the logic that we have recently implemented. The logic does not admit instantaneous states, and is invariant under real-time stuttering. These properties facilitate proof methods based on abstraction and reenement. Two natural extensions of the logic lead to non-elementariness and undecidability.	automated proof checking;automated theorem proving;automaton;decision problem;diagram;heuristic (computer science);interval temporal logic;proof calculus;prototype;rajeev alur;real-time clock;real-time locating system;real-time transcription;requirement;theory;time complexity;undecidable problem;visual logic	Y. S. Ramakrishna;Laura K. Dillon;Louise E. Moser;P. M. Melliar-Smith;George Kutty	1993		10.1007/3-540-57529-4_52	predicate logic;dynamic logic;zeroth-order logic;combinatorics;discrete mathematics;linear temporal logic;description logic;logic optimization;higher-order logic;interval temporal logic;computer science;intermediate logic;bunched logic;predicate functor logic;mathematics;sequential logic;substructural logic;multimodal logic;algorithm;temporal logic of actions	Logic	-12.14891107205184	23.451408920687765	28990
3c8d7be2dbce1b58c5ae3b27151a5ad10831a324	key notions of tarski's methodology of deductive systems	present state;mathematical logic;computational linguistic;historical background;deductive system	"""The aim el the article is to outline the historical background and the present state of the methodology of deductive systems invented by Alfred Tarski in the thirties. Key notions of Tax'ski's methodology are presented and discussed through the recent development of the original concepts ~nd ideas. w 1. Consequence operations Philosophical intuit ions which underl ie the not ion of logical consequence were analysed in Tarski (1936). t~oughly, a is def ined to be a consequence of a set of sentences X if under all possible in terpre ta t ions el non-logical te rms in X and a, a is t rue whenever all sentences i n X are t rue. I t is impor tan t to notice t h a t Tarski (1936) appeared in a sequel to the famous """"O poj~ciu p rawdy w j~zykaeh n a u k d e d u k e y j n y c h """" (""""On the concept of t r u t h in formalized languages"""") published as Tarski (1933). Incidental ly , in the la t te r paper the first analysis of the not ion of consequence in te rms of rules of inference appears: a is defined to be a consequence of X if and only if it is derivable f rom X by means of some accept ed logical rules, i.e. operations t ransforming formulas. Still earlier~ in Tarski (1930), (1930a) and in Lukasiewicz and Tarski (1930), an axiomatiz~tion of consequence operation C was provided. Le t S be the set o f sentences e r a f ixed countable language 5p, le~ X, 17, Z range over 2 s and let a, t , ? e S. Then C is required to satisfy: (T1) X =_ C(X) O(O(X)) = O(X) (T3) C(.X)= (..J {C(:Y): 17 _~ X and ~Y is finite) (Td) C({a}) : S for some a e S. F r o m condition (T3) it also follows t h a t (Ts) X _= 17 implies O(X) _ C (Y). With a view oi reaching more profound results, Tarski (1930) added axioms concerning consequences in languages which involve the classical implicat ion (-~) ~nd negat ion (-7) connect ives: ( ~ ) f l e C ( X , a) if and only if a ~ f l e C(X) ( """"-1)* C(a)c~C(""""-]a) = C(O) and C(a, 7 a ) = S . 322 J. Ozelakowski, (~. Malinowski Note that (-+) combines the Deduction Theorem (Tarski (1930), Herbrand (1931)) with Modus Ponens (Rule of Detachment). According to information in Tarski (1956, p. 32)~ the Deduction Theorem~ in its application to the formalism of _Principia Mathematiea, was first established by Tarski us early as in 1921. If the consequence C obeys the law (->) then, in the presence of (T3), condition ( -1)* is equivalent to ( 7 ) C(X, 7a) ~ -S if and only if a e C(X). Nowadays~ it is customary to call a consequence operation an arbit rary function C: 2s-~2 s satisfying merely (T1), (T~) and (Ts). The consequences in the above sense which satisfy (Ts) as well are referred to as finitary. When for every set of sentences X, G(X) -~ S implies that for some finite Xo ~X , C(Xo) ~-S as well, C is called logically compact -~he latter notion results as a weakening of (T,). Incidentally, the discussion of the (o-rule given in Tarski (1933a) justifies interest in non-finitistic consequence operations. According to Tarski, inference relations between sentences have to be pfirely structural i.e. in their definitions only those concepts should be employed which relate to the form and arrangement of the signs of the language (and not to their con~ent). For languages with a definite algebraic structure this presupposition is understood as invariunce of the inference patterns under (all or some) endomorphisms of the language called also substitutions. Accordingly, a consequence ~ is called structural if and only if for all substitutions e of 5 p (Ts) ea e C(eX) whenever a e C(X). The last definition was formulated for sentential languages in ~og and Suszko (1958) i t is important that in tha~ case the class of sabstitutions coincides with the class of all endomorphisms of 5p. The case of predicate logic is more i n v o l v e d cf. e.g. ~ogorzelski and I)rucnal"""	formal language;linear algebra;naruto shippuden: clash of ninja revolution 3;natural deduction;newton–cotes formulas;os-tan;principia cybernetica;semantics (computer science)	Janusz Czelakowski;Grzegorz Malinowski	1985	Studia Logica	10.1007/BF00370426	pure mathematics;mathematics;algorithm	Theory	-11.45000449038465	7.4738516943720406	29013
9a528aef58b9274e90316f7a2c3d2a268c0ef019	the monadic second-order logic of graphs, ii: infinite graphs of bounded width	satisfiability;monadic second order;monadic second order logic	A countable graph can be considered as the value of a certain infinite expression, represented itself by an infinite tree. We establish that the set of finite or infinite (expression) trees constructed with a finite number of operators, the value of which is a graph satisfying a property expressed in monadic second-order logic, is itself definable in monadic second-order logic. From Rabin's theorem, the emptiness of this set of (expression) trees is decidable. It follows that the monadic second-order theory of an equational graph, or of the set of countable graphs of width less than an integerm, is decidable.	logic of graphs;monadic predicate calculus	Bruno Courcelle	1989	Mathematical systems theory	10.1007/BF02088013	combinatorics;discrete mathematics;clique-width;mathematics;monadic predicate calculus;algebra;satisfiability	Logic	-6.517543928764177	16.944311571676703	29030
72fc519fe2b436270bd35c55a44e03e41efaec90	a theory of vague adjectives grounded in relevant observables	fuzzy logic;noun;first order;natural language	The ambiguity and vagueness of natural language vocabulary is one of the biggest obstacles to formalising reasoning systems that can operate with natural language concepts. The paper presents a formal theory of the logic of vague adjectives, which is based on two key ideas: 1) a sharp distinction between count noun concepts and adjectival concepts is build into both syntax and semantics; 2) valid inferences with vague adjectives are explained by means of a theory of the relevance of particular observables to the applicability of any given	formal language;knowledge representation and reasoning;natural language;observable;relevance;theory;vagueness;vocabulary	Brandon Bennett	2006			sorites paradox;noun;algorithm;fuzzy logic;ambiguity;natural language processing;semantics;formalism (philosophy);computer science;small number;artificial intelligence;vagueness	AI	-14.020422429142208	5.938224347834418	29038
49f752c9e6cb048023c8eb424ac0e10d136f5d0c	büchi store: an open repository of $$\omega $$ -automata		We introduce Büchi Store, an open repository of Büchi automata and other types of $$\omega $$ -automata for model-checking practice, research, and education. The repository contains Büchi automata and their complements for common specification patterns and numerous temporal formulae. These automata are made as small as possible by various construction techniques in view of the fact that smaller automata are easier to understand and often help in speeding up the model-checking process. The repository is open, allowing the user to add automata that define new languages or are smaller than existing equivalent ones. Such a collection of Büchi automata is also useful as a benchmark for evaluating translation or complementation algorithms and as examples for studying Büchi automata and temporal logic. These apply analogously for other types of $$\omega $$ -automata, including deterministic Büchi and deterministic parity automata, which are also collected in the repository. In particular, the use of smaller deterministic parity automata as an intermediary helps reduce the complexity of automatic synthesis of reactive systems from temporal specifications.	algorithm;automata theory;benchmark (computing);büchi automaton;model checking;temporal logic;ω-automaton	Yih-Kuen Tsay;Ming-Hsien Tsai;Jinn-Shu Chang;Yi-Wen Chang;Chi-Shiang Liu	2012	International Journal on Software Tools for Technology Transfer	10.1007/s10009-012-0268-4	quantum finite automata;computer science;nested word;theoretical computer science;automata theory;ω-automaton;mobile automaton;algorithm	Logic	-12.786050321050139	25.20591742322746	29077
12f19ccbdf1320fcb439e37eba6c5c51cc512f01	causality versus true-concurrency	standard model;category theory;event structures	Category theory has been successfully employed to structure the confusing setup of models and equivalences for concurrency: Winskel and Nielsen have related the standard models via adjunctions and (co)reflections while Joyal et al. have defined an abstract notion of equivalence, known as open map bisimilarity. One model has not been integrated into this framework: the causal trees of Darondeau and Degano. Here we fill this gap. In particular, we show that there is an adjunction from causal trees to event structures, which we bring to light via a mediating model, that of event trees. Further, we achieve an open map characterization of history preserving bisimilarity: the latter is captured by the natural instantiation of the abstract bisimilarity for causal trees.	automata theory;bisimulation;category theory;causality;concurrency (computer science);fault tree analysis;partial order reduction;turing completeness;universal instantiation;yet another	Sibylle B. Fröschle;Slawomir Lasota	2006	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2006.05.003	standard model;discrete mathematics;mathematics;algorithm;category theory	Logic	-10.545166208403439	21.48066751649485	29089
7d2404708290844881bddebec59af7e4e28ac58e	a development model for data translation	transfer;interfaces;data processing;data definition language;computer programming;normal form;data structure;memory devices;conversion	A model for generalized data translation is presented. Data translation is defined as “the process whereby data stored in a form that can be processed on one computer (the source file) can be translated into a form (target file) which can be used by the same or different processing systems on a possibly different computer.” Inputs to the Data Translator are the source data and two descriptive languages which drive the translation process. A description of the source and target data is presented to the data translator in a Stored Data Definition Language (SDDL). This description includes both the logical (data structure) aspects of the data as well as the physical (storage structure) aspects. A Translation Definition Language (TDL) is used to define the source to target translation parameters.  The data translation model includes several components - the source and target converters, which deal with the storage structure of the data, and a restructurer component which is concerned with changes in the logical structure of the data. A Normal Form of Data is introduced and used to allow the Restructurer to operate independently of the source and target conversion processes. The Normal Form of data provides a means of representing data which is independent of current data structuring dependencies.	a-normal form;cad data exchange;codasyl;communications of the acm;data base task group;data definition language;data structure;database;management information system;management system;prototype;rsa conference;recursion;relational model;security descriptor definition language;source data;source-to-source compiler	James P. Fry;Randall L. Frank;Ernest A. Hershey	1972		10.1145/800295.811486	data exchange;data modeling;data definition language;transfer-based machine translation;data structure;data processing;data transformation;data model;computer science;theoretical computer science;computer programming;database;rule-based machine translation;programming language;logical data model	DB	-31.362430336184424	10.297869262336253	29113
7bcf641b1306ddcf00ff686962954b1ece6cb041	a mathematical semantics of rendering : ii. approximation	mathematical semantics	mathematical semantics	approximation;denotational semantics;rendering (computer graphics)	Eugene Fiume	1991	CVGIP: Graphical Model and Image Processing	10.1016/1049-9652(91)90015-C	simulation;computer science;theoretical computer science;algorithm;computer graphics (images)	Robotics	-11.325320883823824	9.931040789452368	29121
5a391fd86c008dc8aa312c2ad037bec78373ed96	two cooperative versions of the guessing secrets problem	electronic journal;search space;guessing secrets problem;boolean algebra;fuzzy logic;ulam game;world wide web;game theoretic semantics	We investigate two cooperative variants (with and without lies) of the Guessing Secrets Problem, introduced in [4] in the attempt to model an interactive situation arising in the World Wide Web, in relation to the e¢ cient delivery of Internet content. After placing bounds on the cardinality of the smallest set of questions needed to win the game, we establish that the algebra of all the states of knowledge induced by any designated game is a pseudocomplemented lattice. In particular, its join semilattice reduct is embeddable into the corresponding reduct of the Boolean algebra 2 , where N is the cardinality of the search space.	boolean algebra;guardian service processor;linear algebra;polynomial;world wide web	Giuseppe Sergioli;Antonio Ledda;Francesco Paoli;Roberto Giuntini;Tomasz Kowalski;Franco Montagna;Hector Freytes;Claudio Marini	2009	Inf. Sci.	10.1016/j.ins.2009.06.014	fuzzy logic;boolean algebra;mathematical optimization;combinatorics;discrete mathematics;computer science;artificial intelligence;machine learning;mathematics;algorithm	DB	-6.14172845061501	7.105508614269572	29123
b728c23ea1bb9a37d6449f84ab6f532e375b4915	data management support for database management	data management;database management;operating system;database management system	Database management systems today face a rising demand for higher transaction rates and shorter response time. One of the essential requirements for meeting this demand is appropriate operating system support for database management functions. This paper proposes the introduction of the concept of “consistency-preserving containers” into data management in order to improve data management support for database management. Moreover, it presents the formal definition of a transactionoriented interface between data management and database management based on this concept, and introduces a locking scheme allowing high concurrency of transactions while guaranteeing consistency. Logging and recovery aspects of the concept are discussed.	concurrency (computer science);lock (computer science);operating system;relational database management system;requirement;response time (technology)	Rudolf Bayer;Peter Schlichtiger	1984	Acta Informatica	10.1007/BF00289137	data modeling;systems management;intelligent database;database transaction;database tuning;data management;computer science;knowledge management;data administration;document management system;data mining;database;network management application;structure of management information;database schema;data retrieval;physical data model;database testing;database design	DB	-32.6353570160453	11.202377225448892	29143
7fa33c5891ede1ab9ebf88d3dcc893cd85bce2ee	transfinite cardinals in paraconsistent set theory		This paper develops a (nontrivial) theory of cardinal numbers from a naive set comprehension principle, in a suitable paraconsistent logic. To underwrite cardinal arithmetic, the axiom of choice is proved. A new proof of Cantor’s theorem is provided, as well as a method for demonstrating the existence of large cardinals by way of a reflection theorem.	cantor;paraconsistent logic;set theory;transfinite induction	Zach Weber	2012	Rew. Symb. Logic	10.1017/S1755020312000019	mathematical analysis;discrete mathematics;transfinite number;mathematics	Logic	-8.074897045338808	12.45007751977646	29145
e4bf833d75ecc8d6381e837d6449e951fd9df8c1	comparison of inference relations defined over different sets of ranking functions		Skeptical inference in the context of a conditional knowledge base (mathcal R) can be defined with respect to a set of models of (mathcal R). For the semantics of ranking functions that assign a degree of surprise to each possible world, we develop a method for comparing the inference relations induced by different sets of ranking functions. Using this method, we address the problem of ensuring the correctness of approximating c-inference for (mathcal R) by constraint satisfaction problems (CSPs) over finite domains. While in general, determining a sufficient upper bound for these CSPs is an open problem, for a sequence of simple knowledge bases investigated only experimentally before, we prove that using the number of conditionals in (mathcal R) as an upper bound correctly captures skeptical c-inference.		Christoph Beierle;Steven Kutsch	2017		10.1007/978-3-319-61581-3_21	machine learning;open problem;computer science;artificial intelligence;discrete mathematics;semantics;knowledge base;correctness;constraint satisfaction problem;inference;upper and lower bounds;ranking	NLP	-17.5274525961336	9.582707845413115	29167
19c9d6bb8eb5ea3f99936bd2e7973038ab73b1dc	bounded model checking of recursive programs with pointers in k		We present an adaptation of model-based verification, via model checking pushdown systems, to semantics-based verification. First we introduce the algebraic notion of pushdown system specifications (PSS) and adapt a model checking algorithm for this new notion. We instantiate pushdown system specifications in the (mathbb{K}) framework by means of Shylock, a relevant PSS example. We show why (mathbb{K}) is a suitable environment for the pushdown system specifications and we give a methodology for defining the PSS in (mathbb{K}). Finally, we give a parametric (mathbb{K}) specification for model checking pushdown system specifications based on the adapted model checking algorithm for PSS.	model checking;recursion;recursive language	Irina Mariuca Asavoae;Frank S. de Boer;Marcello M. Bonsangue;Dorel Lucanu;Jurriaan Rot	2012		10.1007/978-3-642-37635-1_4	programming language;algorithm	Logic	-17.610220446370285	26.933616259280416	29221
07ae3b9f7b746b2f7c6742012d105843134abfd3	analysing input/output-capabilities of mobile processes with a generic type system	semantica operacional;securite informatique;operational semantics;communication complexity;simultaneidad informatica;complexite communication;input output;computer security;concurrency;semantique operationnelle;algebra proceso;informatique theorique;algebre processus;process algebra;simultaneite informatique;type system;computer theory;informatica teorica	We introduce a generic type system (based on Milner’s sort sy stem) for the synchronouspolyadic -calculus, allowing us to mechanise the analysis of input/output capabilities of mobile processes. The parame ter of the generic type system is a lattice-ordered monoid, the elements of which ar e used to describe the capabilities of channels with respect to their input/ou tput-capabilities. The type system can be instantiated in order to check process pro perties such as upper and lower bounds on the number of active channels, confluence a d absence of blocked processes.	confluence;input/output;mobile agent;tput;type system	Barbara König	2000		10.1007/3-540-45022-X_34	input/output;process calculus;type system;concurrency;computer science;artificial intelligence;communication complexity;mathematics;programming language;operational semantics;algorithm	PL	-7.180993905295715	21.372559911658698	29245
6f939c782c6dec6cf36d31abeb49c9356d1293f6	microprocessors in industry		Prior to 1976, microprocessor developments had little effect in industry at large. In the previous five years the introduction of primitive devices like the Intel 4004, which had been designed for a particular application, the 8008 and the 8080 had fired the imagination of applications engineers in 'future projects' departments of the larger industrial companies. They grasped the idea that here was a technical development that might well enable them to solve practical problems with which they had been battling for years. It was not surprising that they applied themselves to dealing with these problems and to the use of microprocessors either to replace existing analogue designs or to open the way to new methods of working in industry which had until then been largely ignored. Admittedly, some thought was being given to these possibilities by enlightened managements, but at the time there were few of these. In the UK a few companies had earlier formed a user club which by 1976 had become an active group within the lEE, but again their activities were viewed with grave suspicion by members from the long-established sectors of the engineering industry. Notwithstanding the slow take-up by industry, microprocessor developments moved on apace, spurred by the enthusiasm of the new breed of young engineers and computer scientists involved. The original, limited range of devices available were soon complemented by more capable products from other manufacturers. Memory devices were also being developed rapidly, and 8k x 8 ROMs were readily available by early 1978. During 1977-81, microprocessor developments really began to have an impact on the industrial scene. The earlier work by 'future project' people began to filter through as new products. Embedded microprocessor design with built-in firmware was a feature of nearly every new electronic product being launched. The British Government sponsored microprocessor application schemes that helped significantly in getting managements to consider using consultants to see how these new developments could be used in their companies' products and processes. It was found that there were interesting possibilities in most fields of application and particularly in instrumentation, machine control and process control. By 1979 the programmable controller had become accepted as an integral part of industrial automation systems. Many suppliers were developing general-purpose equipment which could be applied to sequential control at a fraction of the cost that would have been applicable five years earlier. As manufacturing companies changed their production lines to take the new products that were being designed around microprocessors, so they made use of the programmable controller to automate the specialist machinery and transfer equipment. This was particularly evident in Japan where the rapidly expanding electronics industry was becoming highly automated. In the next five years manufacturing industry began to reap the benefits of the microprocessor revolution. In the PCs in industry	automation;canonical account;computer scientist;embedded system;firmware;floor and ceiling functions;general-purpose modeling;instrumentation (computer programming);microprocessor;processor design;read-only memory	Lionel R. Thompson	1987	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/0141-9331(87)90331-0	parallel computing;computer science	Arch	-31.56766704806399	21.79861225522841	29248
949d2a87ff902ced26c7b844f91dfb2122018bce	on sets of terms with a given intersection type		We show (1) For each strongly normalizable lambda term M, with beta-eta normal form N, there exists an intersection type A such that in BCD we have ⊢ M : A and N is the unique beta-eta normal term s.t. ⊢ N : A. A similar result holds for finite sets of strongly normalizable terms (2) For each intersection type A if the set of all closed terms M such that in BCD ⊢ M : A is infinite then when closed under beta-eta conversion this set forms an adequate numeral system for untyped lambda calculus. In particular, all these terms are generated from a single 0 by the application of a successor S, S(. . .(S0) . . .)	beta normal form;binary-coded decimal;lambda calculus	Richard Statman	2018	CoRR		numeral system;discrete mathematics;mathematics;lambda;finite set;existential quantification;lambda calculus	Logic	-6.729723918671828	14.758413756105012	29279
de33396e31766734121a281f6c8b833d6ec30a54	on communication and computation	information sources;turing machine;natural language;model of computation;state transition	Comparing technical notions of communication and computation leads to a surprising result, these notions are often not conceptually distinguishable. This paper will show how the two notions may fail to be clearly distinguished from each other. The most famous models of computation and communication, Turing Machines and (Shannon-style) information sources, are considered. The most significant difference lies in the types of state-transitions allowed in each sort of model. This difference does not correspond to the difference that would be expected after considering the ordinary usage of these terms. However, the natural usage of these terms are surprisingly difficult to distinguish from each other. The two notions may be kept distinct if computation is limited to actions within a system and communications is an interaction between a system and its environment. Unfortunately, this decision requires giving up much of the nuance associated with natural language versions of these important terms.	model of computation;natural language;shannon (unit);turing machine	Paul Bohan Broderick	2004	Minds and Machines	10.1023/B:MIND.0000005133.87521.5c	model of computation;computer science;turing machine;artificial intelligence;theoretical computer science;natural language;communication;algorithm	AI	-9.062646252985521	6.317950038353575	29293
d607ab40dbf449b003090f37c2f5ae36a12a2155	intensionality, intensional recursion, and the gödel-löb axiom		computer virology is the study of formalisms that model computer viruses. There are many ways to formalise viruses. We will use the model of Adleman [2], where files can be interpreted either as data, or as functions. We introduce a data type F , and two constants in : (F → F)→ F and out : F → (F → F) such that out (in M)→ M, making (F → F) a retract of F . This might seem the same as the situation where F → F is a retract of F , which yields models of the (untyped) λ -calculus and is difficult to	computer virus;gödel;intension;intensional logic;lambda calculus;recursion	G. A. Kavvos	2017	CoRR		programming language;constructive;discrete mathematics;calculus;extensional definition;gödel;axiom;computer science;modal;recursion;axiom schema;programmer	Logic	-11.303879387813998	14.300530218227506	29358
ac306cd298ff7836db20757e88c6449c93c1c909	declarative languages of composition database development method				Ivan A. Basarab;Bogdan V. Gubsky;Nikolaj S. Nikitchenko	1996			database;programming language;comparison of multi-paradigm programming languages;computer science;declarative programming;second-generation programming language;fifth-generation programming language;ontology language;composition (visual arts)	SE	-24.454827354398834	21.847491065524164	29366
e60effef0291032522fa084742783bb2a7a77b2f	automatically detecting inconsistencies in program specifications		A verification system relies on a programmer writing mathematically precise descriptions of code. A specification that describes the behavior of an operation and a loop invariant for iterative code are examples of such mathematical formalizations. Due to human errors, logical defects may be introduced into these mathematical constructs. Techniques to detect certain logical errors in program specifications, loop invariants, and loop variants are described. Additionally, to make program specifications more concise and to make it easier to create them, RESOLVE has parameter modes: each formal parameter is annotated with a mode that is related to the intended roles of the incoming and outgoing values of that parameter. Methods to check whether the programmer has chosen a plausibly correct mode for each parameter are also explained. The techniques described are lightweight and are applied at an early stage in the verification process.	formal specification;iterative method;loop invariant;pdf/a;parameter (computer programming);programmer;sensor	Aditi Tagore;Bruce W. Weide	2013		10.1007/978-3-642-38088-4_18	theoretical computer science;loop invariant;programmer;computer science	SE	-19.77277947591834	26.76221828033245	29369
103370a4c1e99d6214e9a00ea9aab98883484d0e	invariants for the construction of a handshake register	tratamiento datos;relation ordre partiel;complexite calcul;distributed computing;data processing;traitement donnee;complejidad computacion;waitfree;computational complexity;partial ordering;relacion orden parcial;handshake;ghost variable;invariant;register;partial order	Tromp’s construction of a waitfree atomic register for one writing process and one reading process is presented and proved by means of ghost variables and invariants. Preservation of the invariants is proved mechanically. This approach can be compared with the original proof based on the partial order on the set of accesses of shared variables.	computation;correctness (computer science);data structure;distributed algorithm;invariant (computer science);rudolf m. tromp;shared variables	Wim H. Hesselink	1998	Inf. Process. Lett.	10.1016/S0020-0190(98)00158-6	partially ordered set;data processing;computer science;artificial intelligence;mathematics;algorithm	Logic	-13.236778394019721	31.6727844803466	29426
ce9713dfb56789fe0d8eeef37bad2380f488558a	the relations between implications and left (right) semi-uninorms on a complete lattice		Uninorms are important generalizations of triangular norms and conorms, with a neutral element lying anywhere in the unit interval, and left (right) semi-uninorms are non-commutative and non-associative extensions of uninorms. In this paper, we study the relations between implications and left (right) semi-uninorms on a complete lattice. We firstly investigate the left (right) semi-uninorms induced by implications, give some conditions such that the operations induced by implications constitute left or right semi-uninorms, and demonstrate that the operations induced by a right infinitely ∧-distributive implication, which satisfies the order property, are left (right) infinitely ∨-distributive left (right) semi-uninorms. Then, we discuss the residual operations of left (right) semi-uninorms and show that left (right) residual operators of strict left (right)-conjunctive left (right) infinitely ∨-distributive left (right) semi-uninorms are right infinitely ∧-distributive implications that satisfy the order ...	semiconductor industry	Xiaoying Hao;Meixia Niu;Zhudeng Wang	2015	International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems	10.1142/S0218488515500105	fuzzy logic;mathematical analysis;discrete mathematics;topology;computer science;artificial intelligence;mathematics	Arch	-7.395909269216423	10.369291912042762	29483
5641e9a83b197afaddfd7d4f2304dc86039ea136	intuitionistic and classical natural deduction systems with the catch and the throw rules	natural deduction		natural deduction	Masahiko Sato	1997	Theor. Comput. Sci.	10.1016/S0304-3975(96)00170-3	judgment;discrete mathematics;computer science;mathematics;programming language;sequent calculus;natural deduction;algorithm	ECom	-12.353911985931967	12.513955350239046	29500
c6b1409a6bc226fb0fd4c1b2ffc8989e9480bdac	linearizing well quasi-orders and bounding the length of bad sequences	lexicographic ordering;majoring ordering;well quasi order;fast growing hierarchy;controlled bad sequence;multiset ordering;product ordering	We study the length functions of controlled bad sequences over some well quasi-orders (wqo’s) and classify them in the Fast Growing Hierarchy. We develop a new and selfcontained study of the length of bad sequences over the disjoint product in N (Dickson’s Lemma), which leads to recently discovered upper bounds but through a simpler argument. We also give a tight upper bound for the length of controlled decreasing sequences of multisets of N with the underlying lexicographic ordering, and use it to give an upper bound for the length of controlled bad sequences in the majoring ordering with the underlying disjoint product ordering. We apply this last result to attain complexity upper bounds for the emptiness problem of itca and atra automata. For the case of the product and majoring wqo’s the idea is to linearize bad sequences, i.e. to transform a bad sequence over a wqo into a decreasing one over a well-order, for which upper bounds can be more easily handled. 1 Well-quasi orders and bad sequences A quasi-order is a binary relation ≤ over a given set A that is reflexive and transitive. A sequence X = x0, x1, x2, . . . of elements of A is called good if there are i < j such that xi ≤ xj . A sequence is bad if it is not good. A well quasi-order (wqo) is a quasi-order where all infinite sequences are good, or, equivalently, all bad sequences are finite. The theory of well quasi-orderings was initially developed by Higman [1] (under the name of “finite basis property”) and by Erdös and Rado in an unpublished manuscript, although some early evidence of the theory had already appeared in a work by Neumann [2]. Further developments were [3, 4, 5], and more recently [6]. Wqo’s have become a key ingredient in a great number of results related with decidability, finiteness, and regularity results that appear in areas such as termination proofs for rewriting systems [7, 8], their extensions [9, 10], complexity upper bounds [11, 12], well-structured transition systems [13, 14, 15, 16], etc. From the analysis of a termination proof of a given algorithm S, whose correctness is grounded in the analysis of a certain wqo, one may extract a computational complexity upper bound for S. Roughly, the idea is that any sequence of successive configurations of S (with a given input) is transformed into a bad sequence in the wqo. Thus, having an upper bound for the length of the bad sequence entails an upper bound for the number of steps that the algorithm needs to terminate. Consider the product ordering over Nn, one of the wqo’s analyzed in this work, defined as 〈z1, . . . , zn〉 ≤pr 〈y1, . . . , yn〉 iff ∀i ∈ {1, . . . , n} zi ≤ yi. A sequence x0, . . . , xk being bad in (N,≤pr) means that for all i < j, there exists m such that the m-th component of xj is strictly smaller than the m-th component of xi. Dickson’s Lemma [17] is the statement that	algorithm;automata theory;binary prefix;computational complexity theory;correctness (computer science);lexicographical order;philippe kruchten;rewriting;terminate (software);termination analysis;z1 (computer)	Sergio Abriola;Santiago Figueira;Gabriel Senno	2015	Theor. Comput. Sci.	10.1016/j.tcs.2015.07.012	shortlex order;combinatorics;discrete mathematics;computer science;lexicographical order;mathematics;fast-growing hierarchy;programming language;algorithm;algebra	Theory	-6.818987763583356	20.703432338147156	29523
80c319c9b8710d2670b70ee296c9fe6a8b40becd	the finite controllability of the maslov case		In this paper we show the finite controllability of the Maslov class of formulas of pure quantification theory (specified immediately below). That is, we show that every formula in the class has a finite model if it has a model at all. A signed atomic formula is an atomic formula or the negation of one; a binary disjunction is a disjunction of the form A 1 ⋁ A 2 , where A 1 and A 2 are signed atomic formulas; and a formula is Krom if it is a conjunction of binary disjunctions. Finally, a prenex formula is Maslov if its prefix is ∃···∃∀···∀∃···∃ and its matrix is Krom. A number of decidability results have been obtained for formulas classified along these lines. It is a consequence of Theorems 1.7 and 2.5 of [4] that the following are reduction classes (for satisfiability): the class of Skolem formulas, that is, prenex formulas with prefixes ∀···∀∃···∃, whose matrices are conjunctions one conjunct of which is a ternary disjunction and the rest of which are binary disjunctions; and the class of Skolem formulas containing identity whose matrices are Krom. Moreover, the following results (for pure quantification theory, that is, without identity) are derived in [1] and [2]: the classes of prenex formulas with Krom matrices and prefixes ∃∀∃∀, or prefixes ∀∃∃∀, or prefixes ∀∃∀∀ are all reduction classes, while formulas with Krom matrices and prefixes ∀∃∀ comprise a decidable class. The latter class, however, is not finitely controllable, for it contains formulas satisfiable only over infinite universes. The Maslov class was shown decidable by Maslov in [11].		Stål Aanderaa;Warren D. Goldfarb	1974	J. Symb. Log.		discrete mathematics;of the form;matrix (mathematics);ternary operation;negation;decidability;mathematics;satisfiability;prefix;atomic formula	Theory	-5.822353569588443	15.270381691946598	29588
da839f6c73cb3044753821e220fd92f60292b948	aop++: a generic aspect-oriented programming framework in c++	developpement logiciel;programacion automatica;componente logicial;orientado aspecto;composant logiciel;programmation generique;object oriented programming;automatic programming;object oriented;aspect oriented programming;desarrollo logicial;software development;software component;metaprogrammation;language extension;oriente objet;aspect oriented;programacion generica;metaprogramming;orientado objeto;metaprogramacion;oriente aspect;programmation automatique;generic programming	This paper presents AOP++, a generic aspect-oriented programming framework in C++. It successfully incorporates AOP with object-oriented programming as well as generic programming naturally in the framework of standard C++. It innovatively makes use of C++ templates to express pointcut expressions and match join points at compile time. It innovatively creates a full-fledged aspect weaver by using template metaprogramming techniques to perform aspect weaving. It is notable that AOP++ itself is written completely in standard C++, and requires no language extensions. With the help of AOP++, C++ programmers can facilitate AOP with only a little effort.	approximation;aspect weaver;aspect-oriented programming;bridging (networking);c++;compile time;compiler;concurrent computing;debugging;gnu cflow;generic programming;integrated development environment;interaction;library (computing);overhead (computing);pointcut;profiling (computer programming);programmer;programming paradigm;template (c++);template metaprogramming;tracing (software)	Zhen Yao;Qilong Zheng;Guoliang Chen	2005		10.1007/11561347_8	template;real-time computing;c++;aspect-oriented programming;decltype;computer science;operating system;c++11;policy-based design;programming language;object-oriented programming;algorithm	PL	-26.553642678774725	29.444809518179728	29644
5ddd8feabad9ca38f7916ddeb5e6bcad2820131a	flexible idl compilation for complex communication patterns	opti- mization;corba;flick;interface definition language;middleware;idl compiler;compilation;communication patterns	Distributed applications are complex by nature, so it is ess ential that there be effective software development tools to aid in the construc tion of these programs. Commonplace “middleware” tools, however, often impose a tr adeoff between programmer productivity and application performance. For ins ta ce, manyCORBA IDL compilers generate code that is too slow for high-performan ce systems. More importantly, these compilers provide inadequate support f or sophisticated patterns of communication. We believe that these problems can be over c me, thus making IDL compilers and similar middleware tools useful for a broader range of systems. To this end we have implemented Flick, a flexible and optimizingIDL compiler, and are using it to produce specialized high-perform ance code for complex distributed applications. Flick can produce specially “de composed” stubs that encapsulate different aspects of communication in separate f unctions, thus providing application programmers with fine-grain control over all me ssages. The design of our decomposed stubs was inspired by the requirements of a pa rticul r distributed application called Khazana, and in this paper we describe ou r experience to date in refitting Khazana with Flick-generated stubs. We believe that the special IDL compilation techniques developed for Khazana will be usefu l in other applications with similar communication requirements.	compiler;distributed computing;documentation;middleware;programmer;programming productivity;programming tool;requirement;software development	Eric Eide;James L. Simister;Tim Stack;Jay Lepreau	1999	Scientific Programming			HPC	-28.451930444574355	28.802646204427898	29653
2c76cc22bec382c1ed54d22261efe3eedbb2d040	supervisory control of timed discrete event systems under partial observation based on activity models and eligible time bounds	observability;modelizacion;timed dess;systeme evenement discret;supervisory control;sistema temporizado;controlabilidad;superviseur;eligible time bounds;observabilidad;controllability;timed system;activity models;observabilite;specification language;sistema acontecimiento discreto;modelisation;controlabilite;discrete event system;supervisor;state space method;methode espace etat;systeme temporise;lenguaje especificacion;partial observation;modeling;state space explosion;langage specification;supervision;time discretization;metodo espacio estado	To avoid the state–space explosion by including tick events in timed discrete event systems (DESs) under partial observation, a notion of eligible time bounds is introduced and based on the notion, controllability and observability conditions of languages are presented. In particular, this paper shows that these controllability and observability conditions are necessary and sufficient for the existence of a supervisor to achieve the given language specification. © 2005 Elsevier B.V. All rights reserved.	programming language specification;state space;systems biology	Seong-Jin Park;Kwang-Hyun Cho	2006	Systems & Control Letters	10.1016/j.sysconle.2005.09.003	control engineering;observability;simulation;systems modeling;controllability;specification language;control theory;mathematics;supervisory control	Logic	-8.78988455625395	26.525890193738924	29659
35a1db0b45e85473d37e4e6557f9242b36820509	verification of generalized inference diagnosability for decentralized diagnosis in discrete event systems	generators;electronic mail;inference mechanisms decision making discrete event systems fault diagnosis;verification decentralized diagnosis discrete event system inference diagnosability inferencing;n inference diagnosability generalized inference diagnosability verification discrete event systems inference based framework decentralized decision making disjunctive decentralized diagnosis scheme dual conjunctive scheme nonfailure decision making;automata;indexes;decision making indexes automata generators delays discrete event systems electronic mail;discrete event systems;delays	Previously we have introduced an inference-based framework for decentralized decision-making, where inferencing over the ambiguities of the self and the others is used to issue decisions. In this setting, we previously introduced the notion of N-inference V-diagnosability to characterize the existence of a disjunctive decentralized diagnosis scheme so that any fault can be detected within bounded delay, using at most N-levels of inferencing, by one of the diagnosers. While the disjunctive scheme relies on one of the diagnosers making the failure decision, the dual conjunctive scheme relies on none of the diagnosers making the nonfailure decision. It is known that the two schemes are incomparable, and in another paper we extend our earlier work to provide a more general framework, introducing the notion of N-inference diagnosability, capturing both disjunctive and conjunctive schemes. The contribution of this paper is developing a method for verifying N-inference diagnosability.	disjunctive normal form;verification and validation	Shigemasa Takai;Ratnesh Kumar	2015	2015 IEEE 20th Conference on Emerging Technologies & Factory Automation (ETFA)	10.1109/ETFA.2015.7301419	database index;computer science;artificial intelligence;theoretical computer science;machine learning;automaton	Robotics	-7.778025260472372	27.61306631635512	29660
b49a3aa915ac2d21262406bbe1d3c0071fe39372	on the declarative semantics of deductive databases and logic programs	deductive databases;declarative semantics;logic program	Abstract We investigate the declarative semantics of deductive databases and logic programs. We introduce the class of perfect models of a deductive database and argue that this class of models—enjoying many of the properties of the class of minimal models—provides a correct intended semantics for such databases, incorporating a natural form of the closed-world assumption. We extend the notion of stratified logic programs onto the class of deductive databases, and we prove that every stratified database has perfect models and that every stratified logic program has exactly one such model. We show that perfect models are independent of the choice of a particular stratification and are, in fact, models of McCarthyu0027s prioritized circumscription. Finally, we introduce the class of locally stratified databases and logic programs (significantly extending the class of stratified databases) and show that all our results can be extended onto this broader class.	declarative programming;logic programming	Teodor C. Przymusinski	1988			database theory;database;mathematics;datalog;programming language;algorithm	DB	-17.518366770052495	9.33302985842916	29668
0237856cad9e9ff199031d2ed1bf985c948aeecf	vérification formelle des systèmes temps-réel avec ordonnancement préemptif	minimisation;verification;systeme temps reel;modelizacion;minimization;preemptive scheduling;red petri;temporal constraint;minimizacion;automaton;program verification;modelisation;automata;verificacion programa;formal verification;parametre critique;scheduling;automate;parametro critico;constrenimiento temporal;time petri nets;verification formelle;real time system;sistema tiempo real;verification programme;petri net;modeling;ordonnancement;reseau petri;critical parameter;reglamento;contrainte temporelle;real time systems	In this paper, we propose a method for the verification of timed properties for real-time systems featuring a preemptive scheduling policy: the system, modeled as scheduling time Petri net, is first translated into a stopwatch automaton to which it is timed bisimilar. Timed properties are then verified using HYTECH. The efficiency of this approach relies on two strong points: first, the translation features a minimization of the number of stopwatches of the resulting automaton, which is a critical parameter for the efficiency of the ensuing verification. Second, the translation is performed by an over-approximating algorithm, which is based on Difference Bound Matrix and therefore efficient, but yields nonetheless a timed bisimilar automaton. We have implemented the method and we give some experimental results illustrating its efficiency.		Didier Lime;Olivier H. Roux	2006	Technique et Science Informatiques	10.3166/tsi.25.343-370	real-time computing;simulation;real-time operating system;computer science;artificial intelligence;operating system;automaton;timed automaton;algorithm	Logic	-9.636781698712776	26.475068627837867	29673
623f1f77b7bb6150e29683b8cea60de71f1ccd6a	pattern driven lazy reduction: a unifying evaluation mechanism for functional and logic programs	data types specifications;left hand side;executive function;right hand side;lazy evaluation;term rewriting systems;functional programming;unification;equational theories;completeness;logic programs;substitutions	A novel lazy evaluation mechanism, pattern-driven lazy reduction, is developed that serves as a unifying evaluation mechanism for both functional and logic programs. The reduction of a function call can be viewed as “semantically” unifying the function call with the left hand side of a defining equation, and applying the unifier to the right hand side. Lazy reduction is achieved by the pattern which the function call matches against. Function reductions are actually “driven” by patterns in this sense. It is shown that this evaluation mechanism works well for both functional programs and logic programs that involve “executable” functions. As a result, logic programs can be enhanced with (1) the availability of a functional computing environment where there is no notion of backtracking, thus alleviating the degree of control difficulties typically encountered in logic programs, and (2) the ability to terminate “infinite computations” without the introduction of complex control issues at the user-level. On the other hand, functional programs can be equipped with the power of logic programming languages, e.g., Prolog.	admissible rule;backtracking;computation;defining equation (physical chemistry);executable;lazy evaluation;logic programming;loss function;programming language;prolog;terminate (software);user space	P. A. Subrahmanyam;Jia-Huai You	1984		10.1145/800017.800534	completeness;computer science;theoretical computer science;unification;lazy evaluation;programming language;functional programming;algorithm	PL	-17.18453165223204	20.479298017025478	29720
eb30f47f9486930f0b1e3fb4a66d613043730f2f	levelwise search and pruning strategies for first-order hypothesis spaces	interesting pattern discovery;search space;search algorithm;inductive logic programming;prior knowledge;relational database;data mining;association rule mining;mutual exclusion;pattern discovery;first order;levelwise search;first order patterns	The discovery of interesting patterns in relational databases is an important data mining task. This paper is concerned with the development of a search algorithm for first-order hypothesis spaces adopting an important pruning technique (termed subset pruning here) from association rule mining in a first-order setting. The basic search algorithm is extended by so-called requires and excludes constraints allowing to declare prior knowledge about the data, such as mutual exclusion or generalization relationships among attributes, so that it can be exploited for further structuring and restricting the search space. Furthermore, it is illustrated how to process taxonomies and numerical attributes in the search algorithm. Several task settings using different interestingness criteria and search modes with corresponding pruning criteria are described. Three settings serve as test beds for evaluation of the proposed approach. The experimental evaluation shows that the impact of subset pruning is significant, since it reduces the number of hypothesis evaluations in many cases by about 50%. The impact of generalization relationships is shown to be less effective in our experimental set-up.	association rule learning;data mining;first-order predicate;mutual exclusion;numerical analysis;relational database;search algorithm;spaces;taxonomy (general)	Irene Weber	2000	Journal of Intelligent Information Systems	10.1023/A:1008740003826	beam search;association rule learning;mutual exclusion;relational database;computer science;machine learning;principal variation search;pattern recognition;first-order logic;data mining;best-first search;programming language;search algorithm	AI	-23.094939782350806	5.665863714985041	29777
996bf35b5f8f60cbd8082f5621366da91cb5a4e6	flexible queries in xml native databases		To date, most of the XML native databases (DB) flexible querying systems are based on exploiting the tree st ructure of their semi structured data (SSD). However, it become s important to test the efficiency of Formal Concept Analysis ( FCA) formalism for this type of data since it has been proved a grea t performance in the field of information retrieval (IR). So, the IR in XML databases based on FCA is mainly based on the use of the lattice structure. Each concept of this lattice ca n be interpreted as a pair (response, query). In this work, we provide a new flexible modeling of XML DB based on fuzzy FCA as a first step towards flexible querying of SSD.	crystal structure;formal concept analysis;formal system;fuzzy concept;information retrieval;preprocessor;semiconductor industry;solid-state drive;tree structure;xml database	Olfa Arfaoui;Minyar Sassi Hidri	2013	CoRR		xml validation;xml encryption;streaming xml;computer science;data mining;xml database;database;information retrieval;efficient xml interchange	DB	-32.133775832093974	5.640751834305755	29788
76e47b4c93cee0030e03897906d23c5ddba506c0	research issues in database specification	programming language;denotational methods;mexico city;logic;abstract data types;formal specifications;data bases;software engineering;data model;semantic data model;grammars;complex data;integrity constraints;high level language;data models	"""This paper summarizes discussions of a panel on """"Type Specifications and Databases"""" at VLDB in Mexico City. Panel members are listed at the end of the paper.Significant advances have been achieved in software engineering and programming language research in the development of specification techniques. There are important consequences for design, redesign, precision, and analysis of software. The importance of this work to database applications, and indeed data models and data languages, is now becoming apparent. However, specific database issues (e.g., constraints, complex data relationships, shared data, data independence) alter the specification problem as encountered in programming languages. The summary emphasizes the importance of (precise) specification in the database context and relates recent results in both programming languages and databases. It also lists outstanding theoretical problems and the relationship of advances in specification research to the development of semantic data models and high level languages for databases."""	database;high-level programming language;programming language theory;semantic data model;software engineering;specification (regression);vldb	Michael L. Brodie	1983	SIGMOD Record	10.1145/984532.984536	semantic data model;fourth-generation programming language;data modeling;data manipulation language;data structure;specification language;data model;computer science;database model;data mining;database;programming paradigm;fifth-generation programming language;programming language;abstract data type;programming language specification;second-generation programming language;logic;database design	DB	-30.920664769414568	10.86473816246546	29802
f7123288592087c085c71aa70ea20e028cc72227	simscript ii.5 and simanimation, a tutorial	programming language;technical report	The SIMSCRIPT 11.5 programming language is described, and a complete simulation example is presented.	programming language;simscript ii.5;simulation	Edward C. Russell	1987		10.1145/318371.318394	natural language processing;first-generation programming language;computer science;technical report;gpss;programming language	PL	-25.23013759434986	22.39536410081518	29830
413f281cd7ea6894c0da5fffec97b6bc0b85ed86	on the computational content of intuitionistic propositional proofs	cut elimination;circuit complexity;first order;intuitionistic logic;propositional logic;polynomial time	The intuitionistic calculus was introduced to capture reasoning in constructive mathematics. As such it has much more constructive character than classical logic. This property of the intuitionistic calculus has been extensively studied, but mostly from the point of view of computability and little has been proved about computational complexity. The aim of this paper is to show that the constructive character of intuitionistic logic manifests itself not only on the level of computability but, in case of the propositional fragment, also on the level of polynomial time computability. Recent progress in proof complexity of propositional logic, which concerns various proof systems, suggest that the study of the complexity of intuitionistic propositional proofs may be a fruitful area. In particular for several classical calculi a so-called feasible interpolation theorem was proved [5, 7, 9]. Such theorems enable one to extract a boolean circuit from a proof; the size of the circuit is polynomial in the size of the proof. Indeed, feasible interpolation theorem was proved for the intuitionistic sequent calculus in [8]. The proof was based on the result of Buss and Mints [3] which shows that the well-known disjunction property can be witnessed by polynomial algorithms in case of the propositional fragment of the intuitionistic calculus. In this paper we further generalize the two results on the intuitionistic propositional calculus. The ultimate aim is to obtain a realizability theorem for intuitionistic propositional proofs based on polynomial time computations. We prove a result in this direction (Theorem 3), but we suspect that it is not the best possible result of this type. On the other hand, we show that boolean circuits cannot be replaced by a more restricted type of computation (section 5). Our proof technique is extracted from [3]. In this paper we make it more explicit (Theorem 1) and use the sequent calculus instead of the natural deduction system used in [3]. Goerdt [4] has also proved some related extensions of	algorithm;boolean circuit;computability;computation;computational complexity theory;formal system;interpolation;intuitionistic logic;natural deduction;polynomial;proof complexity;propositional calculus;sequent calculus;time complexity;whole earth 'lectronic link	Samuel R. Buss;Pavel Pudlák	2001	Ann. Pure Appl. Logic	10.1016/S0168-0072(01)00040-9	propositional formula;zeroth-order logic;time complexity;absorption;circuit complexity;discrete mathematics;classical logic;resolution;horn-satisfiability;many-valued logic;intuitionistic logic;intermediate logic;first-order logic;mathematics;propositional variable;well-formed formula;propositional calculus;truth value;proof complexity;algorithm;autoepistemic logic	Logic	-10.044518995475263	14.455272598094353	29851
5577700be00b7a54967c0c3b634be598b14fc344	towards the design of a multilevel secure object-oriented database management system			management system;multilevel security	Bhavani M. Thuraisingham	1995	JOOP		database;computer science;data mining;object-oriented programming;database design;multilevel security	DB	-32.11410927522007	10.517788288474932	29873
129ac50264227930dd84bf0728fa7418375ceb76	existential rules: a graph-based view - (extended abstract)	existential rules;datalog;ontology based data access	We consider rules that allow to assert the existence of new individuals, an ability called value invention in databases [AHV95]. These rules are of the form body → head, where the body and the head are function-free conjunctions of atoms, and variables that occur only in the head are existentially quantified, hence their name ∀∃-rules in [BLMS09, BLM10] or existential rules in [BMRT11, KR11]. Existential rules have long been studied in databases as high-level constraints called tuple generating dependencies (TGDs) [BV84]. Recently, there has been renewed interest for these rules in the context of ontology-based data access (OBDA), a new paradigm that seeks to exploit the semantics encoded in ontologies while querying data. The deductive database language Datalog could be seen as a natural candidate for expressing ontological knowledge in this context, however its limitation is that it does not allow for value invention, since all variables in a rule head necessarily occur in the rule body. Value invention has been recognized as a necessary prerequisite in an open-world perspective, where all individuals are not known a priori. It is in particular a feature of description logics (DLs), well-known languages dedicated to ontological representation and reasoning. This prerequisite motivated the recent extension of Datalog to existential rules, which gave rise to the Datalog +/- formalism [CGK08, CGL09].		Marie-Laure Mugnier	2012		10.1007/978-3-642-32925-8_3	computer science;data mining;database;algorithm	Theory	-24.145760856426627	8.740234326854853	29885
3e86097fbb3cda8efadb42e1175edc817138d9c3	parallel-correctness and transferability for conjunctive queries		A dominant cost for query evaluation in modern massively distributed systems is the number of communication rounds. For this reason, there is a growing interest in single-round multiway join algorithms where data are first reshuffled over many servers and then evaluated in a parallel but communication-free way. The reshuffling itself is specified as a distribution policy. We introduce a correctness condition, called parallel-correctness, for the evaluation of queries w.r.t. a distribution policy. We study the complexity of parallel-correctness for conjunctive queries as well as transferability of parallel-correctness between queries. We also investigate the complexity of transferability for certain families of distribution policies, including the Hypercube distribution policies.	algorithm;cobham's thesis;conjunctive query;correctness (computer science);distributed computing;join (sql);polynomial hierarchy;static program analysis	Tom J. Ameloot;Gaetano Geck;Bas Ketsman;Frank Neven;Thomas Schwentick	2015	J. ACM	10.1145/3106412	computer science;theoretical computer science;data mining;database;distributed database	DB	-24.043412685293013	10.336767642137744	29944
901814e840c72e52c75defcbe7e3e0a78d48c554	program development as a formal activity	transformational semantics;correctness of transformation rules;programming language;optimization technique;abstract data types;program transformation;abstract data type;transformational semantics abstract data types correctness of transformation rules program transformations;program transformations;network address translation computer languages calculus design optimization application software writing shape software engineering programming profession catalogs;program development	A methodology of program development by transformations is outlined. In particular, ways of representing the transformation rules are discussed, and the relationship between notions of their correctness and the semantic definition of programming languages is studied. How transformation techniques are complemented by the use of abstract data types and assertions is described. In the resulting calculus of transformations, the single rules not only represent design or optimization techniques, but they also incorporate verification principles. To illustrate this approach, the Warshall algorithm is developed by successive applications of transformations.	abstract data type;assertion (software development);correctness (computer science);floyd–warshall algorithm;mathematical optimization;programming language	Manfred Broy;Peter Pepper	1981	IEEE Transactions on Software Engineering	10.1109/TSE.1981.230815	program analysis;computer science;theoretical computer science;programming language;abstract data type;algorithm	SE	-25.920357528217743	23.170586626677927	29969
3fbcdbefb8121c482a81daf61d2a25657e2f28c5	towards a systematic account of different semantics for logic programs	fitting semantics;artificial intelligent;stratificatin;well founded semantics;logic programming;nonmonotonic reasoning;stratification;logic in computer science;logic programs;stable semantics	In [14, 15], a new methodology has been proposed which allows to derive uniform characterizations of different declarative semantics for logic programs with negation. One result from this work is that the well-founded semantics can formally be understood as a stratified version of the Fitting (or Kripke-Kleene) semantics. The constructions leading to this result, however, show a certain asymmetry which is not readily understood. We will study this situation here with the result that we will obtain a coherent picture of relations between different semantics for normal logic programs.	coherence (physics);curve fitting;declarative programming;disjunctive normal form;first-order predicate;formal methods;logic programming;non-monotonic logic;operational semantics;programming paradigm;well-founded semantics;whole earth 'lectronic link	Pascal Hitzler	2005	J. Log. Comput.	10.1093/logcom/exi018	stratification;classical logic;description logic;higher-order logic;formal semantics;stable model semantics;action semantics;failure semantics;computer science;artificial intelligence;game semantics;theoretical computer science;non-monotonic logic;predicate functor logic;proof-theoretic semantics;formal semantics;mathematics;predicate transformer semantics;programming language;axiomatic semantics;well-founded semantics;logic programming;operational semantics;multimodal logic;denotational semantics;algorithm;philosophy of logic;computational semantics;autoepistemic logic	AI	-14.930952805441494	11.214648038681934	29996
feecaf96e4ca385da8d99a34df52459a4efee159	restrukturierung objektorientierter datenbankschemata mittels formaler begriffsanalyse	base donnee;restructuration;hierarchized structure;database;retroingenierie;restructuracion;base dato;structure hierarchisee;concept analysis;object oriented;estructura datos;oriente objet;structure donnee;database design;orientado objeto;data structure;ingeniera inversa;estructura jerarquizada;reverse engineering	Eine Restrukturierung eines gegebenen Datenbankschemas kann aus unterschiedlichen Gründen notwendig werden. So kann dadurch z.B. die Verständlichkeit des Schemas verbessert werden. Im Bereich der relationalen Datenbanken steht die Normalisierungstheorie zur Verfügung, um z.B. Redundanz zu vermeiden. Der Normalisierung liegt dabei ein Begriff der Äquivalenz von Datenbankschemata zugrunde. Im Bereich der objektorientierten Datenbanken gibt es bislang kaum vergleichbare Ansätze. In diesem Beitrag präsentieren wir daher einen Ansatz, mit dem Klassenhierarchien in objektorientierten Datenbankschemata in eine „normalisierte” Form transformiert werden können. Dazu muß zunächst eine extensionale Analyse durchgeführt werden, die die notwendigen Informationen über extensionale Beziehungen zwischen den gegebenen Klassen liefert. Anschließend wenden wir Konzepte der formalen Begriffsanalyse an, um eine „normalisierte” Klassenhierarchie abzuleiten. Different reasons may require to restructure an existing database schema. For instance, restructuring can improve the comprehensibility of a schema. For relational databases there is the relational normalization theory which provides a means of avoiding redundant storage of data. Normalization is based on a notion of equivalence for database schemata. At the time being, there are only very few comparable approaches to normalizing object-oriented databases. In this article we present an approach to transforming class hierarchies in object-oriented database schemata into a “normal form”. For this transformation we first have to perform an extensional analysis which provides the necessary information about extensional relationships among the classes in a given schema. Then, we apply the framework of formal concept analysis in order to derive a “normalized” class hierarchy.	class hierarchy;database schema;eine and zwei;formal concept analysis;normalization property (abstract rewriting);relational database;turing completeness;unified model	Ingo Schmitt;Stefan Conrad	1999	Informatik - Forschung und Entwicklung	10.1007/s004500050139	data structure;computer science;formal concept analysis;engineering;database;programming language;object-oriented programming;database design;reverse engineering	DB	-31.470217571164184	9.352691534907533	30035
6ef920bd88fb6701322e2d71d0a4c8beb0c5a364	many sorted algebraic data models for gis	multiple representation;object oriented language;data model;information system	Although many GIS data models are available, a declarative, operational, well-defined, implementation-independent, and objectoriented language is lacking. Based on the theory of many sorted algebra, this work presents a family of geometric data models. Some geographical data models of urban information systems are illustrated using homomorphism. According to the results, the preferred characteristics of mixing declarative and operational statements, multiple representations, tight interdependency among objects, and integration of vector and raster based systems can be achieved through this mechanism.	data model;declarative programming;geographic information system;interdependence;string operations	Feng-Tyan Lin	1998	International Journal of Geographical Information Science	10.1080/136588198241491	discrete mathematics;data model;computer science;theoretical computer science;machine learning;database;object-oriented programming;information system;algorithm	DB	-30.512356233823358	12.362329958220343	30056
06b7d88d9b4802fae3cb92b591341d00cb22941b	on the design of generic static analyzers for modern imperative languages	operational semantics;run time system;structured operational semantics;design and implementation;non structural;control flow;proof of correctness;abstract interpretation;memory model	The design and implementation of precise static analyzers for signi cant fragments of modern imperative languages like C, C++, Java and Python is a challenging problem. In this paper, we consider a core imperative language that has several features found in mainstream languages such as those including recursive functions, run-time system and user-de ned exceptions, and a realistic data and memory model. For this language we provide a concrete semantics |characterizing both nite and in nite computations| and a generic abstract semantics that we prove sound with respect to the concrete one. We say the abstract semantics is generic since it is designed to be completely parametric on the analysis domains: in particular, it provides support for relational domains (i.e., abstract domains that can capture the relationships between di erent data objects). We also sketch how the proposed methodology can be extended to accommodate a larger language that includes pointers, compound data objects and non-structured control ow mechanisms. The approach, which is based on structured, big-step G1SOS operational semantics and on abstract interpretation, is modular in that the overall static analyzer is naturally partitioned into components with clearly identi ed responsibilities and interfaces, something that greatly simpli es both the proof of correctness and the implementation.	abstract interpretation;c++;correctness (computer science);imperative programming;java;operational semantics;pointer (computer programming);python;recursion (computer science);runtime system;static program analysis	Roberto Bagnara;Patricia M. Hill;Andrea Pescetti;Enea Zaffanella	2007	CoRR		memory model;computer science;theoretical computer science;programming language;control flow;operational semantics;denotational semantics;algorithm	PL	-23.275613017084346	27.24113110039918	30073
b04b07575542513bf4a70488458afe52c2a948b2	dynamic dependency pairs for algebraic functional systems	article letter to editor	We extend the higher-order termination method of dynamic dependency pairs to Algebraic Functional Systems (AFSs). In this setting, simply typed lambda-terms with algebraic reduction and separate β-steps are considered. For left-linear AFSs, the method is shown to be complete. For so-called local AFSs we define a variation of usable rules and an extension of argument filterings. All these techniques have been implemented in the higher-order termination tool WANDA.	central processing unit;evert willem beth;first-order predicate;polynomial;recursion;rewriting	Cynthia Kop;Femke van Raamsdonk	2012	Logical Methods in Computer Science	10.2168/LMCS-8(2:10)2012	computer science;theoretical computer science;mathematics;algorithm	Logic	-14.995386439826424	20.135805483787614	30103
810aa43f2fd0a03663ab1106f52b1bf6061314c5	proof of the law of infinite conjunction using the perfect disjunctive normal form		The law of infinite conjunction (Quineu0027s name, see Methods of logic, rev. ed., Holt u0026 Co., New York, p. 254) says that a set of truthfunctional formulas is satisfiable provided every finite subset is. We give a proof of the law which uses some properties of the perfect disjunctive normal form. Any satisfiable formula can be written as a disjunction of conjunctions of literals (a literal is a statement-letter or the negation of one) in which a statement-letter which occurs in any conjunction occurs exactly once in each; a literal by itself counts as a conjunction and a conjunction by itself as a disjunction. We appropriate the names u0027conjunctionu0027 and u0027disjunctionu0027 for conjunctions and disjunctions of this type. Then if {AJ} is a set of formulas every finite subset of which is satisfiable, there is a set {Do} of disjunctions with Dn equivalent to (Al u0026 ... u0026 An). To show that {Di} (and hence {Ai}) is satisfiable, we show that there is a satisfiable set C (an u0027infinite conjunctionu0027) which selects one conjunction from each disjunction. C implies each disjunction, and a set all of whose members are implied by some satisfiable set is surely itself satisfiable. For each n, Dn + 1 implies Dn, and then from properties of disjunctions every conjunction in Dn + implies some conjunction in Dn. So, LEMMA I. For each n there are conjunctions C1,..., Cn, with C1 from Di, such that Cn implies ... implies C1. From this and properties of conjunctions it is plain that there cannot be a statement-letter S and a pair of disjunctions such that S has only positive occurrences in one disjunction and only negative occurrences in the other. So, LEMMA II. For each statement-letter S, either S occurs positively in every disjunction in which it occurs or S occurs negatively in every disjunction in which it occurs. Now, for each statement-letter S in turn, if S occurs positively in every disjunction in which it occurs, delete from every disjunction any conjunction in which S occurs negatively, and if not, delete from every disjunction any conjunction in which S occurs positively. Evidently no disjunction retains more than one conjunction, and by Lemma II none becomes empty. The set C of nondeleted conjunctions selects one from each disjunction, and every statement-letter that occurs in C occurs in only one way, so C is satisfiable. q.e.d. The law is sometimes proved from Koenigu0027s Infinity-lemma on trees. The required tree-structure is of course implicit in the set of disjunctions. To see this,	disjunctive normal form	James Thomson	1967	J. Symb. Log.		conjunctive normal form;discrete mathematics;mathematics;negation normal form;canonical normal form;disjunctive normal form;algorithm	Logic	-4.909200771362321	14.81538923138394	30136
412e79b701a05fc52b707dcb1851e9d111e8875e	composite events for active databases: semantics, contexts and detection	database system;active database;specification language;object oriented	Making a database system active entails developing an expressive event specification language with well-defined semantics, algorithms for the detection of composite events, and an architecture for an event detector along with its implementation. Thii paper presents the semantics of composite events using the notion of a global event history (or a global event-log). Parameter contexts are introduced and precisely defined to facilitate efficient management and detection of composite events. Finally, an architecture and the implementation of a composite event, detector is analyzed in the context of an object-oriented active DBMS.	active database;algorithm;computation;oracle internet directory;scsi initiator and target;specification language;snoop	Sharma Chakravarthy;V. Krishnaprasad;Eman Anwar;S.-K. Kim	1994			database theory;intelligent database;specification language;computer science;database model;data mining;database;programming language;object-oriented programming;database schema;database design;spatiotemporal database	DB	-29.1692095101207	13.841067001946874	30167
a10f4c9846001f42b53a31354064d8658c90ce27	state-based regression with sensing and knowledge	sensing;plans;knowledge;modal logic;regression	This paper develops a state-based regression method for planning domains with sensing operators and a representation of the knowledge of the planning agent. The language includes primitive actions, sensing actions, and conditional plans. The regression operator is direct in that it does not depend on a progression operator for its formulation. We prove the soundness and completeness of the regression formulation with respect to the definition of progression and the semantics of a propositional modal logic of knowledge. The approach is illustrated with a running example that can not be handled by related methods that utilize an approximation of knowledge instead of the full semantics of knowledge as is used here. It is our expectation that this work will serve as the foundation for the extension of work on state-based regression planning to include sensing and knowledge as well.	approximation;automated planning and scheduling;color gradient;conditional (computer programming);data transfer object;ibm notes;modal logic;soundness (interactive proof)	Richard B. Scherl;Tran Cao Son;Chitta Baral	2008		10.1007/978-3-540-89197-0_33	modal logic;regression;artificial intelligence;machine learning;data mining;mathematics;knowledge;plan;algorithm	AI	-16.946292402488123	8.95811201714419	30210
862576569343cfa7c699784267543b81fad23c3a	attributive concept descriptions with complements	complexite;concept;representacion conocimientos;complejidad;intelligence artificielle;complexity;lenguaje descripcion;artificial intelligence;inteligencia artificial;knowledge representation;representation connaissances;langage description;concepto;description language	Schmidt-SchaulL M. and G. Smolka, Attributive concept descriptions with complements, Artificial Intelligence 48 (1991) 1-26. We investigate the consequences of adding unions and complements to attributive concept descriptions employed in terminological knowledge representation languages. It is shown that deciding coherence and subsumption of such descriptions are PSPACE-complete problems that can be decided with linear space.	artificial intelligence;complement (complexity);knowledge representation and reasoning;pspace-complete;schmidt decomposition;subsumption architecture	Manfred Schmidt-Schauß;Gert Smolka	1991	Artif. Intell.	10.1016/0004-3702(91)90078-X	knowledge representation and reasoning;complexity;computer science;artificial intelligence;mathematics;concept;algorithm	AI	-20.133397972820145	9.44367426895286	30228
6a8f2568ac6d9dcae9b2d1b27dc851698e711a14	proof-functional connectives and realizability	functional connectivity	The meaning of a formula built out of proof-functional connectives depends in an essential way upon the intensional aspect of the proofs of the component subformulas. We study three such connectives, strong equivalence (where the two directions of the equivalence are established by mutually inverse maps), strong conjunction (where the two components of the conjunction are established by the same proof) and relevant implication (where the implication is established by an identity map). For each of these connectives we give a type assignment system, a realizability semantics, and a completeness theorem. This form of completeness implies the semantic completeness of the type assignment system.	intensional logic;logical connective;map;turing completeness	Franco Barbanera;Simone Martini	1994	Arch. Math. Log.	10.1007/BF01203032	discrete mathematics;pure mathematics;mathematics;atomic formula	PL	-9.362767391471248	10.936023507913124	30233
3f6a7ac1020ac76812d32ee22a317bdc2ba4ef5d	towards a logic for pragmatics. assertions and conjectures	dual intuitionistic logic;mckinsey tarski kripke s s4 translation;logic for pragmatics	The logic for pragmatics extends classical logic in order to characterize the logical properties of the operators of illocutionary force such as that of assertion and obligation and of the pragmatic connectives which are given an intuitionistic interpretation. Here we consider the cases of assertions and conjectures: the assertion that a mathematical proposition is true is justified by the capacity to present an actual proof of , while the conjecture that is true is justified by the absence of a refutation of . We give sequent calculi of type G3i and G3im inspired by Girard’s LU, with subsystems characterizing intuitionistic reasoning and some forms of classical reasoning with such operators. Extending Gödel, McKinsey, Tarski and Kripke’s translations of intuitionistic logic into S4, we show that our sequent calculi are sound and complete with respect to Kripke’s semantics for S4. 1 Preface The logic for pragmatics, as introduced by Dalla Pozza and Garola in [7, 8] and developed in [2, 3], aims at a formal characterization of the logical properties of illocutionary operators: it is concerned with the operations by which we perform the act of asserting a proposition as true, either on the basis of a mathematical proof or by empirical evidence or by the recognition of physical necessity, or the act of taking a proposition as an obligation, either on the basis of a moral principle or by inference within a normative system. The discipline of pragmatics, first developed in classical texts of 20th century philosophy and philosophical logic from Austin [1] to Grice and Searle, and then resulting in a large body of linguistic literature (already conspicuous when the classical book by Levinson [14] was published) in a complex relationship with semantics and other areas of linguistics, lies at present beyond the scope of our methods. So far the logic for pragmatics has considered only propositional systems, thus has given no contribution to the crucial issue of the reference of individual terms. Moreover the focus of our current work is on impersonal acts of judgement, leaving the consideration of speech acts to future developments. More precisely, our present task is to characterize the abstract behaviour of a few pragmatic operators, as it is manifested in highly regimented forms of reasoniong such as mathematical discourse or the foundations of laws. Within this range, the consideration of the impersonal operator of assertion in Dalla Pozza and Garola’s pragmatic interpretation of intuitionistic logic [7] has given a stimulating insight in the interpretation of intuitionistic and classical connectives, briefly summarized below. In the light of their approach, this paper begins an analysis of dualities in intuitionistic logic, in so far as they can be interpreted as resulting from the relations between an impersonal operator of assertion and one of conjecture. The technical tools used here are as ancient as the S4 translation, revisited in the light of Girard’s sequent calculus LU, but they are perhaps enough to guess some features of the theory yet to be developed. The viewpoint of [7] can be sketched roughly as follows. There is a logic of propositions and J. Logic Computat., Vol. 14 No. 4, c Oxford University Press 2004; all rights reserved 474 Towards a Logic for Pragmatics. Assertions and Conjectures a logic of judgements. Propositions are entities which can be true or false, judgements are acts which can be justified or unjustified. The logic of propositions is about truth according to classical semantics. The logic of judgements gives conditions for the justification of acts of judgements. An instance of an elementary act of judgement is the assertion of a proposition , which is justified by the capacity to exhibit a proof of it, if is a mathematical proposition, or some kind of empirical evidence if is about states of affairs.1 It is then claimed that the justification of complex acts of judgement must be in terms of Heyting’s interpretation of intuitionistic connectives: for instance, a conditional judgement where the assertion of depends on the assertibility of is justified by a method that transforms any justification for the assertion of into a justification for the assertion of . In modern logic the distinction between propositions and judgements was established by Frege: a proposition expresses the thought which is the content of a judgement and a judgement is the act of recognizing the truth of its content. In Frege’s formalism, ‘ ’ expresses the judgement asserting the proposition ; only truth-functional connectives and quantifiers are considered and judgements appear only at the level of the deductive system. It follows that there cannot be nested occurrences of the symbol ‘ ’ and that truth-functional connectives cannot be applied to expressions of judgement. The distinction between propositions and judgements has recently been taken up by Martin-Löf: in his formalism ‘ prop’ expresses the assertion that is a well-formed proposition, and ‘ true’ expresses the judgement that it is known how to verify . However, it seems that propositions are given a verificationist semantics, according to which in order to give meaning to a proposition we must know what counts as a verification of it. Indeed, by replacing Frege’s ‘ ’ with ‘ true’ Martin-Löf seems to adopt the view that it is impossible to separate the truth of a proposition from the conditions of its verification; certainly Martin-Löf theory of types is developed in an epoch é of classical truth, without any reference to it. Unlike Martin-Löf and in agreement with Frege, Dalla Pozza and Garola distinguish between the truth of a proposition and the justification of a judgement, but extend Frege’s framework by introducing pragmatic connectives and giving them Heyting’s interpretation while retaining Tarski’s semantics for the logic of propositions. In their compatibilist approach, classical logic is extended rather than challenged by intuitionistic pragmatics, the latter having a different subject matter from the former. Thus the task and the challenge for Dalla Pozza and Garola’s approach is to characterize the relations between the two levels: their main tool is the S4 interpretation, due to Gödel [10], McKinsey and Tarski [20], and Kripke [12], which they regard as a reflection of the pragmatic level on the semantic one. In this paper we try to show that this elementary tool can be exploited to trace interesting interactions between classical and pragmatics connectives. On one hand, Dalla Pozza and Garola’s framework does appear quite close to the well-established epistemic approach to philosophy of mathematics, advocated by Stewart Shapiro [26]. Justification of judgements depends on knowledge; Kripke’s possible worlds may be regarded as possible states of knowledge and their preordering may correspond to ways our knowledge could evolve in the future. Having a proof of now rules out the possibility of being false at any future state of knowledge, and the possibility that may be false at a future state of knowledge propagates the impossibility of having a proof of backwards to all previous states of knowledge. Similarly, having now a proof that implies rules out the possibility that at some future state of knowledge may be true and false. Notice that this reading explains Gödel’s, McKinsey and Tarski’s and Kripke’s interpretation of intuitionistic logic, which yields and , and not the other well-known interpretation which yields , 1In this introduction the symbol ‘ ’ stands for an arbitrary proposition, not necessarily atomic; the symbol ‘ ’ is specifically used to range among atomic propositions. Towards a Logic for Pragmatics. Assertions and Conjectures 475 and . The following apparently innocent remark plays an important role here: Kripke’s monotonicity condition (i.e. persistence in the future of the valuations of atomic formulas) is related to the fact that in the modal translation an intuitionistic atom is translated as . The epistemic interpretation of Kripke’s semantics can be given an ontological significance: some philosophers have suggested that the right standpoint of the logic for pragmatics may be a reading of Kripke’s possible world semantics that would reduce intuitionistic mathematics to classical epistemic mathematics; presumably, the intensional notion of a proof would be explained away in an ontology of possible states of knowledge. On the other hand, Dalla Pozza insists that the logic for pragmatics is an intensional logic, while Kripke’s semantics for modal logics suggests an extensional interpretation of intensional notions. In the field of deontic logic Dalla Pozza has successfully applied the intensional status of the pragmatic operator of obligation, in opposition to the extensional reading of the KD necessity operator, by introducing a distinction between expressive and descriptive interpretations of norms, which appears to have resolved conceptual confusions [8]. Similarly, Frege’s symbol ‘ ’ may be regarded here as expressing the intentionality of an act of judgement, while the S4 modality ‘ ’ would perhaps describe conditions on the states of knowledge which justify the appropriateness of such an act. Contemporary mathematical intuitionism is based on game semantics, the typed -calculus and categorical logic as much as on Kripke’s semantics; all of these tools belong to a mathematical treatment of the logic for pragmatics. The fruitfulness of a non-reductionist philosophical view may be tested by its capacity to promote a better understanding of their relations. 2 1.1 Conjectures and assertions If the logic of assertions is formalized and interpreted in the proof theory and model theory of mathematical intuitionism, what are the essential features of an illocutionary operator of conjecture ‘ ’ and what shall a logic of conjectures be like, if there has to be a duality between assertions and conjectures? The following th	assertion (software development);atomic formula;atomic sentence;categorical logic;deontic logic;entity;formal system;frege;frege's propositional calculus;frege–church ontology;game semantics;gödel;intensional logic;intentionality;interaction;intuitionistic logic;levinson recursion;logic programming;logical connective;modal logic;modality (human–computer interaction);norm (social);operational semantics;persistence (computer science);possible world;reductionism;semantics (computer science);sequent calculus;subject matter expert turing test;type theory;well-formed formula;whole earth 'lectronic link	Gianluigi Bellin;Corrado Biasi	2004	J. Log. Comput.	10.1093/logcom/14.4.473	modal logic;linear logic;discrete mathematics;law of thought;many-valued logic;intuitionistic logic;intermediate logic;negation;mathematics;minimal logic;kripke semantics;truth value;logic;algorithm	AI	-12.56720350309586	4.3827777423104735	30257
12f98abc0a6d0b5263466ba22d30e964b9d48833	progress in computer-assisted inductive theorem proving by human-orientedness and descente infinie?	theorem proving;artificial intelligent;logic in computer science	In this position paper we briefly review the development history of automated inductive theorem proving and computer-assisted mathematical induction. We think that the current low expectations on progress in this field result from a faulty narrow-scope historical projection. Our main motivation is to explain—on an abstract but hopefully sufficiently descriptive level—why we believe that future progress in the field is to result from human-orientedness and descente infinie.	automated theorem proving;inductive reasoning;mathematical induction	Claus-Peter Wirth	2012	Logic Journal of the IGPL	10.1093/jigpal/jzr048	computer science;artificial intelligence;pure mathematics;mathematics;automated theorem proving;algorithm	Logic	-10.006997987210568	4.968424736006353	30309
5b2060fbbf981938c049e1aba92494d553f3e538	from probabilistic counterexamples via causality to fault trees	fault tree;workingpaper;probabilistic model checking;inproceedings;counterexample;causality	In recent years, several approaches to generate probabilistic counterexamples have been proposed. The interpretation of stochastic counterexamples, however, continues to be problematic since they have to be represented as sets of paths, and the number of paths in this set may be very large. Fault trees (FTs) are a well-established industrial technique to represent causalities for possible system hazards resulting from system or system component failures. In this paper we suggest a method to automatically derive FTs from counterexamples, including a mapping of the probability information onto the FT. We extend the structural equation approach by Pearl and Halpern, which is based on Lewis counterfactuals, so that it serves as a justification for the causality that our proposed FT derivation rules imply. We demonstrate the usefu lness of our approach by applying it to an industrial case study.	algorithm;byzantine fault tolerance;causality;counterfactual conditional;fault tree analysis;model checking;structural equation modeling	Matthias Kuntz;Florian Leitner-Fischer;Stefan Leue	2011		10.1007/978-3-642-24270-0_6	reliability engineering;fault tree analysis;causality;engineering;counterexample;algorithm;statistics	SE	-6.517130224389063	5.561473710277702	30311
c0bf9d8b5ef8d1df01bd06e74c27a4a5374e1996	the logic of query languages for data streams	primary key;query language;conjunctive queries;data stream;continuous query;relational database;probabilistic databases;data model;expressive power;systems and applications;closed world assumption;tight coupling;data stream management system;database query;consistent query answering	Data Stream Management Systems (DSMS) represent a vibrant research area that is rich in technical challenges, which many projects have approached by extending database query languages and models for continuous queries on data streams [1, 3, 4, 9, 5]. These database-inspired approaches have delivered remarkable systems and applications, but have yet to produce solid conceptual foundations for DSMS data models and query languages---particularly if we compare with the extraordinary ones of relational databases. A cornerstone of the success of relational databases was the tight coupling between their data model and their logic-based query languages. In this paper, we show that a similar approach can succeed for data streams and propose a tight-coupled design for DSMS data models and query languages. To express more naturally the behavior of a data stream and attain more powerful on-line queries, we abandon the set-of-tuples model of relational databases, and instead use sequences of tuples ordered by their time-stamps as our data stream model. This approach allows us to overcome the blocking problem that severely impairs the expressive power of data stream query languages. As elucidated in [1]: A blocking query operator is one that cannot produce the first tuple of the output until it has seen the entire input. Previous work had characterized blocking query operators by their non-monotonic behavior [7, 6, 8]. In this paper, we instead use the closed-world assumption [11, 10] to characterize blocking/nonblocking behaviors with respect to the incompleteness/completeness of the streaming database. From this, we infer simple syntactic conditions that make Datalog rules immune from blocking. A significant and surprising new result is that the use of negated goals in the bodies of rules does not imply a blocking behavior: in fact, many very useful nonblocking queries can be expressed using negation. The flip side of this exciting result is that additional conditions must then be imposed on the rules to ensure that (i) the results produced by Datalog programs are ordered according to their time-stamps, and (ii) possible time-skews between streams are also managed explicitly by the rules [2]. These problems, and their possible remedies, are captured and expressed quite naturally using Datalog, which thus emerges as a powerful framework for analyzing and expressing continuous queries. Related problems, including the treatment of data streams without time-stamps, the characterization of monotonic query operators [7, 6, 8], and the use of more general closed-world assumptions were also studied and answered in the course of this research [12].	blocking (computing);closed-world assumption;data model;datalog;event stream processing;monotonic query;online and offline;query language;relational database;streaming algorithm	Carlo Zaniolo	2011		10.1145/1966357.1966363	online aggregation;sargable;query optimization;closed-world assumption;query expansion;web query classification;unique key;boolean conjunctive query;coupling;data model;relational database;computer science;query by example;theoretical computer science;data mining;database;rdf query language;conjunctive query;programming language;view;range query;expressive power;query language;spatial query	DB	-24.85426753046142	9.712163747973104	30352
1da443694367e3ef9822feb0a9471a7294831150	efficient model checking of psl safety properties	nusmv 2 model checker;input variables;semantics;psl safety properties;transducers;program verification;observers;safety properties;infinite word;psl;nusmv psl safety properties model checking;competition model;finite counterexamples;model checking;syntactics;specification languages;data structures;nusmv 2 model checker efficient model checking psl safety properties property specification language bug hunting tool finite counterexamples;specification languages program verification;safety;nusmv;bug hunting tool;transducers semantics safety input variables syntactics observers data structures;property specification language;efficient model checking;open source	Safety properties are an important class of properties as in the industrial use of model checking a large majority of the properties to be checked are safety properties. This work presents an efficient approach to model check safety properties expressed in PSL (IEEE Std 1850 Property Specification Language), an industrial property specification language. The approach can also be used as a sound but incomplete bug hunting tool for general(non-safety) PSL properties, and it will detect exactly the finite counterexamples that are the informative bad prefixes for the PSL formulas in question. The presented technique is inspired by the temporal testers approach of Pnueli and co-authors but is aimed at finite words instead of infinite words. The new approach presented in this paper handles a larger syntactic subset of PSL safety properties than earlier translations for PSL safety subsets and has been implemented on top of the open source NuSMV 2model checker. The experimental results show the approach to be a quite competitive model checking approach when compared to a state-of-the-art implementation of PSL model checking.	academy;algorithm;information;model checking;nusmv;open-source software;property specification language;regular expression;rewriting;semantics (computer science);temporal logic;transducer	Tuomas Kuismin;Keijo Heljanko;Tommi A. Junttila	2010	2010 10th International Conference on Application of Concurrency to System Design	10.1109/ACSD.2010.27	model checking;transducer;computer science;theoretical computer science;semantics;psl;programming language;algorithm	SE	-14.331190468477173	25.658012388416182	30425
db05f8163cf4249656804f5b63cfb741aaa696d5	propositional superposition logic				Athanassios Tzouvaras	2018	Logic Journal of the IGPL	10.1093/jigpal/jzx054	algorithm;dynamic logic (modal logic);resolution (logic);mathematics;zeroth-order logic;discrete mathematics;propositional variable;horn-satisfiability;well-formed formula;intermediate logic;autoepistemic logic	Logic	-12.819049963431974	12.840859063925743	30426
26b4c7b167ffb24a51d5609c3a276030bcaa6d8d	join minimization in xml-to-sql translation: an algebraic approach	algebraic approach;rewrite rule;relation algebra;relational database	Consider an XML view defined over a relational database, and a user query specified over this view. This user XML query is typically processed using the following steps: (a) our translator maps the XML query to one or more SQL queries, (b) the relational engine translates an SQL query to a relational algebra plan, (c) the relational engine executes the algebra plan and returns SQL results, and (d) our translator translates the SQL results back to XML. However, a straightforward approach produces a relational algebra plan after step (b) that is inefficient and has redundant joins. In this paper, we report on our preliminary observations with respect to how joins in such a relational algebra plan can be minimized. Our approach works on the relational algebra plan and optimizes it using novel rewrite rules that consider pairs of joins in the plan and determine whether one of them is redundant and hence can be removed. Our study shows that algebraic techniques achieve effective join minimization, and such techniques are useful and can be integrated into mainstream SQL engines.	emoticon;linear algebra;map;relational algebra;relational database;rewrite (programming);rewriting;sql;select (sql);xml	Murali Mani;Song Wang;Daniel J. Dougherty;Elke A. Rundensteiner	2006	SIGMOD Record	10.1145/1121995.1121999	recursive join;domain relational calculus;sargable;data definition language;query optimization;sql;nested set model;relational model;codd's theorem;relational calculus;relational algebra;relational database;computer science;query by example;theoretical computer science;data mining;relation algebra;database;conjunctive query;language integrated query;programming language;null;tuple relational calculus	DB	-29.264188495277036	6.8050457981866685	30478
c3ca1f14a75330b5d076f77bdf03c18b9093fefb	xpath, transitive closure logic, and nested tree walking automata	universiteitsbibliotheek;xpath;xml;tree walking automata;transitive closure	We consider the navigational core of XPath, extended with two operators: the Kleene star for taking the transitive closure of path expressions, and a subtree relativisation operator, allowing one to restrict attention to a specific subtree while evaluating a subexpression. We show that the expressive power of this XPath dialect equals that of FO(MTC), first order logic extended with monadic transitive closure. We also give a characterization in terms of nested tree-walking automata. Using the latter we then proceed to show that the language is strictly less expressive than MSO. This solves an open question about the relative expressive power of FO(MTC) and MSO on trees. We also investigate the complexity for our XPath dialect. We show that query evaluation be done in polynomial time (combined complexity), but that satisfiability and query containment (as well as emptiness for our automaton model) are 2ExpTime-complete (it is ExpTime-complete for Core XPath).	2-exptime;automata theory;first-order logic;kleene star;path expression;time complexity;transitive closure;tree (data structure);tree walking automaton;xpath	Balder ten Cate;Luc Segoufin	2008		10.1145/1376916.1376952	discrete mathematics;xml;computer science;database;mathematics;programming language;transitive closure;algorithm	DB	-7.687739200544	17.930743541606898	30485
44114b997163b07f342545102cc849b413f9a55c	representation theorems for probability functions satisfying spectrum exchangeability in inductive logic	inductive logic;ddc 120;computacion informatica;de finetti s theorem;equality;uncertain reasoning;apprentissage inductif;theoreme de finetti;logic;inductive logic programming;intelligence artificielle;spectrum;probabilistic approach;satisfiability;munich center for mathematical philosophy mcmp;probability logic;aprendizaje por induccion;razonamiento incierto;ciencias basicas y experimentales;enfoque probabilista;approche probabiliste;spectrum exchangeability;inductive learning;epistemology;artificial intelligence;representation theorem;inteligencia artificial;ddc 160;grupo a;programacion logica inductiva;raisonnement incertain;teorema de finetti;programmation logique inductive;de finetti theorem	We prove de Finetti style representation theorems covering the class of all probability functions satisfying spectrum exchangeability in polyadic inductive logic and give an application by characterizing those probability functions satisfying spectrum exchangeability which can be extended to a language with equality whilst still satisfying that property.	inductive reasoning	Jürgen Landes;Jeff B. Paris;Alena Vencovská	2009	Int. J. Approx. Reasoning	10.1016/j.ijar.2009.07.001	spectrum;artificial intelligence;calculus;mathematics;logic;algorithm;satisfiability	AI	-12.955572435010112	10.086002029862376	30498
9cc5a518a6648cbe7e5d3b8f38ada4c242e9f4d1	existential positive types and preservation under homomorphisms	theorem proving formal logic;theorem proving;first order;formal logic;logic databases computer science;theorem proving finite homomorphism preservation theorem finite structures formal logic	We prove the finite homomorphism preservation theorem: a first-order formula is preserved under homomorphisms on finite structures iff it is equivalent in the finite to an existential positive formula. We also strengthen the classical homomorphism preservation theorem by showing that a formula is preserved under homomorphisms on all structures iff it is equivalent to an existential positive formula of the same quantifier rank. Our method involves analysis of existential positive types and a new notion of existential positive saturation.	first-order predicate;quantifier (logic);quantifier rank	Benjamin Rossman	2005	20th Annual IEEE Symposium on Logic in Computer Science (LICS' 05)	10.1109/LICS.2005.16	discrete mathematics;computer science;first-order logic;mathematics;automated theorem proving;programming language;skolem normal form;logic;algorithm;model theory;algebra	Logic	-9.492875224233728	14.469941582968803	30551
74698e6b306339e6ecbe71f74aa8204feff7bbc7	active databases as a paradigm for enhanced computing environments	active database	Active databases emphasize the notion that a body of information is dynamic and should respond intelligently and in non-trivial ways to the user. It provides a paradigm for research and development which combines aspects of both database and artificial intelligence technologies. A prototype system has shown the viability of this approach. We focus on the following database issues: (1) Descriptions are used as semantic templates for associatively accessing and manipulating data objects. (2) Dynamic views minimize the typical distinctions between queries and retrievals, and between views and real data, and thereby increase the perceived immediacy of the user interface. (3) Constraint Equations are developed as a declarative representation for semantic constraints. The uniform approach they provide for expressing database integrity, consistency, and more general semantics derives its power from the rule-based framework of recent A.I. expert systems. The efficiency of constraint maintenance also is considered. Lastly, (4) The notion of binding time of data associations and reference is discussed relative to both the choice of data model and to the method of data access.	active database;artificial intelligence;data access;data integrity;data model;expert system;logic programming;name binding;programming paradigm;prototype;user interface	Matthew Morgenstern	1983			data science;data mining;database	DB	-29.25348204075132	10.261537656440021	30554
e9da0c115e2ec820862ac33ddf1fdb647fd4fbbb	on supervisory policies that enforce liveness in partially controlled free-choice petri nets	supervisory control;petri nets;petri net	We present a string of observations that lead to the conclusion that the existence of a supervisory policy that enforces liveness for an arbitrary partially-controlled Free-Choice Petri net (FCPN) (cf. section 7.4.3, [3]) is decidable. The paper concludes with some directions for future research. Index Terms Petri Nets, Supervisory Control, Discrete Event Systems.	liveness;petri net	Ramavarapu S. Sreenivas	2006	Mathematics and Computers in Simulation	10.1016/j.matcom.2005.11.002	stochastic petri net;computer science;artificial intelligence;mathematics;process architecture;petri net	Logic	-6.353357116023824	26.864171170566433	30590
810b5bb3454890dc78585785c44413e3d29a50f2	the tractability frontier of graph-like first-order query sets	computational logic;treewidth;query languages;first-order queries;query evaluation;theory;parameterized complexity	The focus of this work is first-order model checking, by which we refer to the problem of deciding whether or not a given first-order sentence is satisfied by a given finite structure. In particular, we aim to understand on which sets of sentences this problem is tractable, in the sense of parameterized complexity theory. To this end, we define the notion of a graph-like sentence set; the definition is inspired by previous work on first-order model checking wherein the permitted connectives and quantifiers were restricted. Our main theorem is the complete tractability classification of such graph-like sentence sets, which is (to our knowledge) the first complexity classification theorem concerning a class of sentences that has no restriction on the connectives and quantifiers. To present and prove our classification, we introduce and develop a novel complexity-theoretic framework that is built on parameterized complexity and includes new notions of reduction.	cobham's thesis;computational complexity theory;first-order logic;first-order predicate;first-order reduction;logical connective;model checking;parameterized complexity;query (complexity)	Hubie Chen	2014	J. ACM	10.1145/3073409	parameterized complexity;combinatorics;discrete mathematics;mathematics;treewidth;algorithm	Logic	-15.839626579963886	12.490098629922977	30661
17fb67791c42a29f6a8612a85297418bd4f86997	t2: a customizable parallel database for multi-dimensional data	application development;satellite data;database system;aggregation function;memory management;magnetic field;atmospheric correction;thematic mapper;light microscopy;three dimensional;multi dimensional;parallel databases;data storage;medical image;storage capacity;remote sensing;indexation;unstructured mesh;parallel machines;technical report;magnetohydrodynamics;scientific research;data retrieval	As computational power and storage capacity increase, processingand analyzing large volumes of data play an increasingly importantpart in many domains of scientific research. Typical examples oflarge scientific datasets include long running simulations oftime-dependent phenomena that periodically generate snapshots oftheir state (e.g. hydrodynamics and chemical transport simulationfor estimating pollution impact on water bodies [4, 6, 20],magnetohydrodynamics simulation of planetary magnetospheres [32],simulation of a flame sweeping through a volume [28], airplane wakesimulations [21]), archives of raw and processed remote sensingdata (e.g. AVHRR [25], Thematic Mapper [17], MODIS [22]), andarchives of medical images (e.g. confocal light microscopy, CTimaging, MRI, sonography). These datasets are usually multi-dimensional. The datadimensions can be spatial coordinates, time, or experimentalconditions such as temperature, velocity or magnetic field. Theimportance of such datasets has been recognized by several databaseresearch groups and vendors, and several systems have beendeveloped for managing and/or visualizing them [2, 7, 14, 19, 26,27, 29, 31]. These systems, however, focus on lineage management, retrievaland visualization of multi-dimensional datasets. They providelittle or no support for analyzing or processing these datasets --the assumption is that this is too application-specific to warrantcommon support. As a result, applications that process thesedatasets are usually decoupled from data storage and management,resulting in inefficiency due to copying and loss of locality.Furthermore, every application developer has to implement complexsupport for managing and scheduling the processing. Over the past three years, we have been working with severalscientific research groups to understand the processingrequirements for such applications [1, 5, 6, 10, 18, 23, 24, 28].Our study of a large set of applications indicates that theprocessing for such datasets is often highly stylized and sharesseveral important characteristics. Usually, both the input datasetas well as the result being computed have underlyingmulti-dimensional grids, and queries into the dataset are in theform of ranges within each dimension of the grid. The basicprocessing step usually consists of transforming individual inputitems, mapping the transformed items to the output grid andcomputing output items by aggregating, in some way, all thetransformed input items mapped to the corresponding grid point. Forexample, remote-sensing earth images are often generated byperforming atmospheric correction on several days worth of rawtelemetry data, mapping all the data to a latitude-longitude gridand selecting those measurements that provide the clearestview. In this paper, we present T2, a customizable paralleldatabase that integrates storage, retrieval and processing ofmulti-dimensional datasets. T2 provides support for many operationsincluding index generation, data retrieval, memory management,scheduling of processing across a parallel machine and userinteraction. It achieves its primary advantage from the ability toseamlessly integrate data retrieval and processing for a widevariety of applications and from the ability to maintain andprocess multiple datasets with different underlying grids. Mostother systems for multi-dimensional data have focused on uniformlydistributed datasets, such as images, maps, and densemulti-dimensional arrays. Many real datasets, however, arenon-uniform or unstructured. For example, satellite data is a twodimensional strip that is embedded in a three dimensional space;water contamination studies use unstructured meshes to selectivelysimulate regions and so on. T2 can handle both uniform andnon-uniform datasets. T2 has been developed as a set of modular services. Since itsstructure mirrors that of a wide variety of applications, T2 iseasy to customize for different types of processing. To build aversion of T2 customized for a particular application, a user hasto provide functions to pre-process the input data, map input datato elements in the output data, and aggregate multiple input dataitems that map to the same output element. T2 presents a uniform interface to the end users (the clients ofthe database system). Users specify the dataset(s) of interest, aregion of interest within the dataset(s), and the desired formatand resolution of the output. In addition, they select the mappingand aggregation functions to be used. T2 analyzes the user request,builds a suitable plan to retrieve and process the datasets,executes the plan and presents the results in the desiredformat. In Section 2 we first present several motivating applicationsand illustrate their common structure. Section 3 then presents anoverview of T2, including its distinguishing features and a runningexample. Section 4 describes each database service in some detail.An example of how to customize several of the database services fora particular application is given in Section 5. T2 is a system inevolution. We conclude in Section 6 with a description of thecurrent status of both the T2 design and the implementation ofvarious applications with T2.	aggregate data;aggregate function;archive;chemical transport reaction;computer data storage;data retrieval;embedded system;lineage (evolution);locality of reference;map;medical ultrasound;memory management;parallel computing;parallel database;planetary scanner;preprocessor;risk aversion;scheduling (computing);simulation;velocity (software development)	Chialin Chang;Anurag Acharya;Alan Sussman;Joel H. Saltz	1998	SIGMOD Record	10.1145/273244.273264	magnetohydrodynamics;three-dimensional space;simulation;scientific method;magnetic field;computer science;microscopy;technical report;data science;computer data storage;data mining;database;rapid application development;data retrieval;memory management	HPC	-32.02856746766006	16.377489122733458	30663
09c4b659d8291fc1ca0f8fbad65752c5efc38e05	realistic program specialization in a multi-paradigm language	partial evaluation	This paper introduces a novel approach for the specialization of functional logic languages. We consider a maximally simpliied abstract representation of programs (which still contains all the necessary information) and deene a non-standard semantics for these programs. Both things mixed together allow us to design a simple and concise partial evaluation method for modern functional logic languages, avoiding several limitations of previous approaches. Moreover, since these languages can be automatically translated into the abstract representation, our technique is widely applicable. In order to assess the practicality of our approach, we have developed a partial evaluation tool for the multi-paradigm language Curry. The partial evaluator is written in Curry itself and has been tested on an extensive benchmark suite (even a meta-interpreter). To the best of our knowledge, this is the rst purely declarative partial evaluator for a functional logic language.	benchmark (computing);curry;interpreter (computing);partial evaluation;partial template specialization;programming paradigm	Elvira Albert;Michael Hanus;Germán Vidal	2000			theoretical computer science;computer science;partial evaluation	PL	-20.590083680769478	24.22355698837963	30679
61716dfde78d2c2ccfe55491a3395dbfa19b1629	data optimizations for constraint automata	article letter to editor;computer science programming languages	Constraint automata (CA) constitute a coordination model based on finite automata on infinite words. Originally introduced for modeling of coordinators, an interesting new application of CAs is implementing coordinators (i.e., compiling CAs into executable code). Such an approach guarantees correctness-by-construction and can even yield code that outperforms hand-crafted code. The extent to which these two potential advantages materialize depends on the smartness of CA-compilers and the existence of proofs of their correctness. Every transition in a CA is labeled by a “data constraint” that specifies an atomic data-flow between coordinated processes as a first-order formula. At run-time, compiler-generated code must handle data constraints as efficiently as possible. In this paper, we present, and prove the correctness of two optimization techniques for CA-compilers related to handling of data constraints: a reduction to eliminate redundant variables and a translation from (declarative) data constraints to (imperative) data commands expressed in a small sequential language. Through experiments, we show that these optimization techniques can have a positive impact on performance of generated executable code.	automata theory;compiler;constraint automaton;correctness (computer science);dataflow;executable;experiment;finite-state machine;first-order logic;first-order predicate;imperative programming;mathematical optimization	Sung-Shik T. Q. Jongmans;Farhad Arbab	2016	Logical Methods in Computer Science		computer science;theoretical computer science;mathematics;programming language;algorithm	PL	-20.42081032336607	28.296795448811086	30695
074d248f603fd0727166638229e59a762436ae40	gentzenizing schroeder-heister's natural extension of natural deduction	systeme schroeder heister;logica formal;simplification;eliminacion;implementation;deduction naturelle;natural extension;higher order;ejecucion;calcul gentzen;natural deduction;simplificacion;formal logic;elimination;logique formelle	In this paper we provide a Gentzen-type formulation of Schroeder-Heister's system of 1]. This system is important from both philosophical and practical points of view: Its philisophical importance is due to the characterization which it provides for the intuitionistic connectives, while the practical one is due to the fact that its notion of higher-order rules and its method of treating the elimination rules were incorporated into the Edinburgh LF (A general logical framework for implementing logical formalisms on a computer, which was developed in the computer science department of the university of Ed-inburgh. See 4],,5]). We shall show that the notions of S.H. that are the most diicult to handle (discharge functions and subrules) become redundant in the Gentzen-type version. The complex normalization proof of S.H. in 2] can be replaced therefore by a standard cut-elimination proof. Moreover , the unusual form of some of the elimination rules of S.H. corresponds to natural, standard form of antecedent rules in sequential calculi. We believe also that the sequential presentation sheds new light on the connection between S.H.'s higher-order rules and the intuitionistic implication and on S.H.'s characterization of the intuitionistic connectives. We assume in what follows an acquaintance with at least the introduction and the rst two sections of 1]. 1 The system GSH 1.1 The language As customary while trying to get rid of discharge functions, we start by introducing a new formal symboì into the language (in 1] this symbol is used only in the metalanguage): Formulas: A 1	computer science;discharger;formal system;gaussian elimination;intuitionistic logic;logical connective;logical framework;natural deduction;needham–schroeder protocol	Arnon Avron	1990	Notre Dame Journal of Formal Logic	10.1305/ndjfl/1093635337	higher-order logic;pure mathematics;mathematics;programming language;sequent calculus;implementation;natural deduction;logic;simplification;algorithm;elimination	Theory	-10.314871967201267	9.561309159305809	30706
aa95a2803ad6e20f57a3ee5186c28b5006a2b3c7	the documentary structure of source code	computacion informatica;grupo de excelencia;linguistic analysis;ciencias basicas y experimentales;source code	Many tools designed to help programmers view and manipulate source code exploit the formal structure of the programming language. Language-based tools use information derived via linguistic analysis to offer services that are impractical for purely text-based tools. In order to be effective, however, language-based tools must be designed to account properly for the documentary structure of source code: a structure that is largely orthogonal to the linguistic but no less important. Documentary structure includes, in addition to the language text, all extralingual information added by programmers for the sole purpose of aiding the human reader: comments, white space, and choice of names. Largely ignored in the research literature, documentary structure occupies a central role in the practice of programming. An examination of the documentary structure of programs leads to a better understanding of requirements for tool architectures. q 2002 Sun Microsystems Inc. Published by Elsevier Science B.V. All rights reserved.	programmer;programming language;requirement;scientific literature;text-based (computing);the practice of programming	Michael L. Van de Vanter	2002	Information & Software Technology	10.1016/S0950-5849(02)00103-9	computer science;engineering;artificial intelligence;software engineering;database;programming language;management;algorithm;source code	SE	-29.70816291943253	22.48078234865368	30743
d766044f77a9702b4930e65de313eeb6e52aaa21	an approach to efficient database design incorporating usage information	base relacional dato;base donnee;concepcion sistema;normalisation;database;base dato;dependance multivaluee;relational database;functional dependency;dependencia multivaluada;criterio;multivalued dependency;criterion;dependance fonctionnelle;system design;critere;normalizacion;base donnee relationnelle;dependencia funcional;database design;standardization;conception systeme	In this paper, a logical database design approach which tries to benefit from both data dependencies and usage information is presented. The approach incorporates a model which extends the traditional data dependency model of relational database theory by combining the natural (functional) and usage views of data. A comprehensive algorithm designing an efficient relational schema on the basis of the model is also presented.	algorithm;computer data storage;data dependency;data integrity;database design;database schema;database theory;heuristic;process (computing);relational database	H. R. Cho;S. J. Park;E. Hevia	1989	Inf. Syst.	10.1016/0306-4379(89)90039-2	data modeling;database theory;information schema;relational model;semi-structured model;dependency theory;entity–relationship model;data model;relational database;computer science;three schema approach;database model;data mining;database;functional dependency;view;database schema;logical data model;standardization;algorithm;database design;multivalued dependency;systems design	DB	-31.153670083921945	11.747025977061316	30749
4d21cf29843c6578a0c85e863454a077d95efa94	queries to temporal databases supporting schema versioning.	temporal query language;satisfiability;temporal database;conceptual schema;schema versioning;schema evolution	The conceptual schema (intention) and raw data (extension) are evolving entities which require adequate support for past, present and even future versions. Temporal Databases supporting schema evolution were developed with the aim of satisfying this need. The support for schema versioning raises two complex subjects: the storage of the several schema versions and their associate data, and the processing of queries that involve more than one schema version. The main objective of this work is to analyse the second aspect in order to propose a strategy for answering those queries. In an environment supporting schema versioning the complete history of schema evolution is kept. In many occasions it can be necessary to query the database’s structure, so this work proposes an extension to the temporal query language TSQL2 in order to support queries to intentional data.	conceptual schema;database schema;entity;intentionality;query language;schema evolution;software versioning;temporal database	Viviane Pereira Moreira;Nina Edelweiss	1999			schema migration;information schema;semi-structured model;logical schema;computer science;conceptual schema;document structure description;star schema;database;document schema definition languages;temporal database;programming language;database schema;xml schema editor;information retrieval;satisfiability	DB	-31.632792122079614	10.786281384623798	30769
2dd1a6535bb8f925108f7d0cfa14dc3a489cad5e	extensions for the graphical query language candid	graphical query language		graphical user interface;query language	Michel Schneider;Claude Trépied	1991			sargable;query optimization;query expansion;data control language;query by example;rdf query language;web search query;query language;object query language	PL	-32.00938277605897	8.385984353864478	30824
fd5981c94ffa78b671d153438d3201d8194efe4d	manipulating accumulative functions by swapping call-time and return-time computations	interesting transformation;accumulative parameter;particular tail-recursive function;functional language;higher order removal;program transformation;accumulative function;circular program;program inversion;return-time computation	interesting transformation;accumulative parameter;particular tail-recursive function;functional language;higher order removal;program transformation;accumulative function;circular program;program inversion;return-time computation	computation;paging	Akimasa Morihata;Kazuhiko Kakehi;Zhenjiang Hu;Masato Takeichi	2012	J. Funct. Program.	10.1017/S0956796812000111	simulation;computer science;theoretical computer science;algorithm	PL	-19.630534878837583	24.572023154640032	30851
eedb64e2f2f888989d939a78406985a7a5d79942	symbolic minimisation of stochastic process algebra models	computational complexity;state space;data structure;limiting factor	Stochastic process algebras have been introduced in order t enable compositional performance analysis. The size of the state space is a limiting factor, es pecially if the system consistsof many cooperating components. To fight state space explosion, compositional a ggregation based on congruence relations can be applied. This paper addresses the computational complex ity of minimisation algorithms and explains how efficient, BDD-based data structures can be employed for this purpose.	algorithm;bisimulation;congruence of squares;data structure;encode;heuristic (computer science);linear function;markov chain;numerical analysis;process calculus;sparse matrix;state space;stochastic process;time complexity	Holger Hermanns;Markus Siegle	1999			discrete mathematics;limiting factor;state space;computational complexity theory;discrete-time stochastic process;stochastic process;data structure;constraint algebra;mathematics;continuous-time stochastic process	SE	-6.345552401189386	24.94239854246848	30853
05a93ad05ed0e641703b68a3582b8943b4989e86	time and games	game semantics	We add the notion of time to denotational models of the λcalculus. The denotation is no longer constant through reduction, but rather decreases with respect to an appropriate order. Categorically, we use a monad over a cartesian category, an order over the morphisms of the Kleisli category, and a Galois connection to model β-reduction. We define a generic monad (time as a resource), and an instance of this construction in game semantics, where our timings are precise enough to simulate parallelism through interleaving.	arrow (symbol);denotational semantics;forward error correction;game semantics;lambda calculus;monad (functional programming);parallel computing;real-time computing;real-time transcription;recursion;reduction (complexity);simulation	Benjamin Leperchey	2005			cartesian coordinate system;morphism;denotation;discrete mathematics;kleisli category;galois connection;monad (functional programming);pure mathematics;game semantics;mathematics;combinatorial game theory	Logic	-11.265138146219016	19.53605205696793	30953
ff721c8d3c5887ebba12da75fcb9edbe423fc1c0	ltl model checking of llvm bitcode with symbolic data		The correctness of parallel and reactive programs is often easier specified using formulae of temporal logics. Yet verifying that a system satisfies such specifications is more difficult than verifying safety properties: the recurrence of a specific program state has to be detected. This paper reports on the development of a generic framework for automatic verification of linear temporal logic specifications for programs in LLVM bitcode. Our method searches explicitly through all possible interleavings of parallel threads (control non-determinism) but represents symbolically the variable evaluations (data non-determinism), guided by the specification in order to prove the correctness. To evaluate the framework we compare our method with state-of-the-art tools on a set of unmodified C programs.	correctness (computer science);llvm;linear temporal logic;model checking;state (computer science);verification and validation	Petr Bauch;Vojtech Havel;Jiri Barnat	2014		10.1007/978-3-319-14896-0_5	computer science;theoretical computer science;programming language;algorithm	SE	-18.266790525538156	27.48006946376213	30972
59516f575665cbc66151892f4808bbd03dd9f489	the graphical specification of similarity queries	directed acyclic graph;image database;satisfiability;fuzzy logic;similarity measure	Image databases will require a completely new organization due to the unstructured and “perceptual” structure of the data they contain. We argue that similarity measures, rather than matching, will be the organizing principle of image databases. Similarity is a very elusive and complex judgment, and typical databases will have to rely on a number of different metrics to satisfy the different needs of their users. This poses the problem of how to combine different similarity measures in a coherent and intuitive way. In this paper we propose our solution, which is loosely based on ideas derived from fuzzy logic in that it uses the equivalent in the similarity domain of the and, or, and not operations. The approach is much more general that that, however, and can be adapted to work with any operation that combines together similarity judgment. With this approach, a query can be described as a Directional Acyclic graph with certain properties. We analyze briefly the properties of this graph, and we present the interface we are developing to specify these queries.	coherence (physics);database;directed acyclic graph;fuzzy logic;graphical user interface;logical connective;organizing (structure);programming paradigm	Simone Santini;Ramesh C. Jain	1996	J. Vis. Lang. Comput.	10.1006/jvlc.1996.0021	fuzzy logic;semantic similarity;computer science;theoretical computer science;data mining;database;directed acyclic graph;similarity heuristic;satisfiability	DB	-21.50991599287718	4.295976434665822	30974
abaab747fad524bf436f8d50004cceaf833dde79	karel universe drag & drop editor	java programming;editor;drag and drop;programming;java	Karel Universe is a drag and drop editor integrated with the Karel J Robot [1] simulator system. It is intended for those students who wish to learn Java with the absolute minimum of syntax. The editor permits the student to create classes, objects, and programs by dragging syntactically correct program fragments from one pane to another. The resulting programs may be then executed in the Karel J Robot simulator.	drag and drop;java;karel the robot;simulation	Joseph Bergin	2006		10.1145/1140124.1140212	programming;simulation;computer science;theoretical computer science;operating system;software engineering;programming language;java;algorithm	AI	-30.59452517973099	25.595280930119984	31020
418cc40458226383e41323e79d2ba5f1075290c1	faster algorithms for algebraic path properties in recursive state machines with constant treewidth	reachability and shortest path;interprocedural analysis;dataflow analysis;constant treewidth graphs	Interprocedural analysis is at the heart of numerous applications in programming languages, such as alias analysis, constant propagation, etc. Recursive state machines (RSMs) are standard models for interprocedural analysis. We consider a general framework with RSMs where the transitions are labeled from a semiring, and path properties are algebraic with semiring operations. RSMs with algebraic path properties can model interprocedural dataflow analysis problems, the shortest path problem, the most probable path problem, etc. The traditional algorithms for interprocedural analysis focus on path properties where the starting point is fixed as the entry point of a specific method. In this work, we consider possible multiple queries as required in many applications such as in alias analysis. The study of multiple queries allows us to bring in a very important algorithmic distinction between the resource usage of the one-time preprocessing vs for each individual query. The second aspect that we consider is that the control flow graphs for most programs have constant treewidth.  Our main contributions are simple and implementable algorithms that support multiple queries for algebraic path properties for RSMs that have constant treewidth. Our theoretical results show that our algorithms have small additional one-time preprocessing, but can answer subsequent queries significantly faster as compared to the current best-known solutions for several important problems, such as interprocedural reachability and shortest path. We provide a prototype implementation for interprocedural reachability and intraprocedural shortest path that gives a significant speed-up on several benchmarks.	algorithm;alias analysis;benchmark (computing);constant folding;control flow;data-flow analysis;dataflow;entry point;interprocedural optimization;linear algebra;preprocessor;programming language;prototype;reachability;recursion (computer science);shortest path problem;software propagation;treewidth	Krishnendu Chatterjee;Rasmus Ibsen-Jensen;Andreas Pavlogiannis;Prateesh Goyal	2015		10.1145/2676726.2676979	longest path problem;theoretical computer science;shortest path problem	PL	-18.18172212783528	30.39030243023392	31033
12756198146e58e830be90b6ccd40b19ed80169e	a note on array grammars	language theory;grammaire formelle;formal grammar;grammaire a structure phase;theorie langage;grammaire matricielle	Recently, there has been considerable interest in studying array grammars that generate or parse sets of two-dimensional arrays. This is probably due to the fact that such grammars can be regarded as general mathematical models for digital picture processing. Although an array is a two-dimensional generalization of a string, there are some problems involved in generalizing conventional string grammars to arrays. The major problem is how to define array rewriting rules or productions. The replacement of one subarray in the host array by another subarray of a different size or shape may cause the rows and columns of the host array to be stretched or shrunk by varying amounts in order to accommodate the array rewriting rule. This is called a shearing effect. The first attempt to introduce array grammars was made by Kirsh [2] in 1964. He gave an example of an array grammar that generated a set of right triangles. However, he gave no general model of array grammars. In 1968, Yodogawa and Honda first proposed general models of array grammars, called two-dimensional phrase-structure grammars [7], which were two-dimensional generalized versions of one-dimensional phrase-structure grammars. Later, similar general array grammars were introduced by Dacey [l] and Rosenfeld [3]. At present, isometric (or isotonic) array grammars introduced by Rosenfeld seem to be considered as better formal models of array grammars, since many studies have concerned those grammars [4,5,6]. Isometric array grammars avoid the shearing problem by requiring both sides of each rewriting rule to have the same shape. On the other hand, two-dimensional phrase-structure grammars, introduced by Yodogawa and Honda, avoid the problem by imposing certain restrictions on each rewriting rule and its application to the host array. This note expounds upon relationships between isometric array grammars and two-dimensional phrase-structure grammars. The notations are adapted as employed in [5,6].	array data structure;column (database);digital camera;isometric projection;isotonic regression;louis rosenfeld;mathematical model;parsing;rewriting;string (computer science)	Eiji Yodogawa	1984	Inf. Process. Lett.	10.1016/0020-0190(84)90075-9	natural language processing;computer science;philosophy of language;mathematics;formal grammar;algorithm	PL	-25.915219884790435	14.727075535990805	31039
d8ec66cf12674ad7e18b3900a0579a4b4444da9a	a new elimination rule for the calculus of inductive constructions	calculus of inductive constructions;dependent pattern matching;pattern matching;type theory;dependent types;coq proof assistant	In Type Theory, definition by dependently-typed case analysis can be expressed by means of a set of equations — the semantic approach — or by an explicit pattern-matching construction — the syntactic approach. We aim at putting together the best of both approaches by extending the pattern-matching construction found in the Coq proof assistant in order to obtain the expressivity and flexibility of equationbased case analysis while remaining in a syntax-based setting, thus making dependently-typed programming more tractable in the Coq system. We provide a new rule that permits the omission of impossible cases, handles the propagation of inversion constraints, and allows to derive Streicher’s K axiom. We show that subject reduction holds, and sketch a proof of relative consistency.	calculus of constructions;cobham's thesis;coq (software);dependent type;natural deduction;pattern matching;proof assistant;software propagation;subject reduction;type theory	Bruno Barras;Pierre Corbineau;Benjamin Grégoire;Hugo Herbelin;Jorge Luis Sacchini	2008		10.1007/978-3-642-02444-3_3	calculus of constructions;dependent type;computer science;pattern matching;programming language;type theory;algorithm	PL	-13.551845533954712	16.93606555619574	31053
94b725598811916b504a3cb2d72f48d2a663fce9	stochastic analogues of invariants martingales in stochastic event-b		In conventional formal model based development frameworks, invariants play a key role in controlling the behaviour of the model (when they contribute to the definition of the model) or in verifying the model's properties (when the model, independently defined, is required to preserve the invariants). However, when variables take values distributed according to some probability distribution, the possibility of verifying that system behaviour is, in the long term, confined to some acceptable set of states can be severely diminished because the system might, in fact, with low probability fail to be thus confined. This short paper proposes martingales as suitable analogues of invariants for capturing suitable properties of non-terminating systems whose behaviour is with high probability good, yet where a small chance of poor behaviour remains. The idea is explored in the context of the well-known Event-B framework.	b-method;divergence (computer science);formal language;invariant (computer science);newman's lemma;verification and validation;whole earth 'lectronic link;with high probability	Richard Banach	2015	2015 International Conference on Evaluation of Novel Approaches to Software Engineering (ENASE)		martingale;invariant	SE	-9.8087053597973	22.810638629588187	31068
25f60b9690c622e09bde393d25a56857e2922fe6	calculus on strong partition cardinals	infinite exponent partition cardinals	The assumption of [HM] and our assumption here, the existence of a strong partition cardinal, is moderately special. On the one hand, it violates the Axiom of Choice and is not relatively consistent with ZF (unlike AC and its negation). On the other hand, under the Axiom of Determinacy (AD), such cardinals are abundant and consistent with countable choice and DC, the principle of Dependent Choices. א1, for example, is a strong partition cardinal and there are strong partition cardinals that are the limits of strong partition cardinals. AD itself, while once considered unimaginably powerful, seems fairly tame now by the yardstick of the large cardinal axiom hierarchy (well below supercompact cardinals in consistency strength). See [Ka] for details.	ka band;tame;zermelo–fraenkel set theory	James M. Henle	2006	Math. Log. Q.	10.1002/malq.200610016	combinatorics;mathematical analysis;discrete mathematics;mathematics	NLP	-7.514853614684891	12.943674177001911	31086
a0bb9d28a64450a88ebb3826ae7a82947992d2fd	a measure of arbitrariness in abductive explanations		We study the framework of abductive logic programming exten ded with integrity constraints. For this framework, we introduce a new measure of the simplicity of an explanation based on its degree of arbitrariness: the more arbitrary the explanation, the less appealing it i s, w th explanations having no arbitrariness — they are calledconstrained— being the preferred ones. In the paper, we study basic prope rties of constrained explanations. For the case when programs in abduct ive theories are stratified we establish results providing a detailed picture of the complexity of the proble m to decide whether constrained explanations exist. To appear in Theory and Practice of Logic Programming (TPLP) .	abductive logic programming;abductive reasoning;association for logic programming;data integrity;theory	Luciano Caroprese;Irina Trubitsyna;Miroslaw Truszczynski;Ester Zumpano	2014	TPLP	10.1017/S1471068414000271	artificial intelligence;algorithm;abductive logic programming	AI	-17.07168181767092	8.130476446983442	31088
62e4ca5fa2c619af7612e011ccad40b85d60332f	on deciding subsumption problems	automated deduction;redundancy elimination;model building	Subsumption is an important redundancy elimination method in automated deduction. A clause D is subsumed by a set $$\mathcal{C}$$ of clauses if there is a clause C ∈ $$\mathcal{C}$$ and a substitution σ such that the literals of Cσ are included in D. In the field of automated model building, subsumption has been modified to an even stronger redundancy elimination method, namely the so-called clausal H-subsumption. Atomic H-subsumption emerges from clausal H-subsumption by restricting D to an atom and $$\mathcal{C}$$ to a set of atoms. Both clausal and atomic H-subsumption play an indispensable key role in automated model building. Moreover, problems equivalent to atomic H-subsumption have been studied with different terminologies in many areas of computer science. Both clausal and atomic H-subsumption are known to be intractable, i.e., Π p 2 -complete and NP-complete, respectively. In this paper, we present a new approach to deciding (clausal and atomic) H-subsumption that is based on a reduction to QSAT2 and SAT, respectively.	atom;automated theorem proving;computer science;np-completeness;natural deduction;servo, subsumption, and symbolic architecture;subsumption architecture	Uwe Egly;Reinhard Pichler;Stefan Woltran	2005	Annals of Mathematics and Artificial Intelligence	10.1007/s10472-005-0434-4	computer vision;model building;computer science	AI	-15.142603405527272	13.263615690223197	31209
bd32c059f71102f3f09bf3b3c96d0780b4ab1710	reduction of the supervisory control problem for petri nets	supervisory control;des supervisory control problem reduction sub petri net legal sets control law design maximally permissive control laws discrete event systems;supervisory control petri nets law automatic control legal factors discrete event systems algorithm design and analysis safety automata state space methods;technology and engineering;control system synthesis;discrete event systems;petri nets;petri net;control system synthesis discrete event systems petri nets	The authors prove a reduction theorem for the supervisory control problem for general Petri nets with general legal sets. To design control laws guaranteeing that the marking stays within the legal set, it suffices to consider a sub-Petri net of the full model. This extends existing design algorithms, allows to prove an important property of maximally permissive control laws and limits the number of events which need to be observed.	algorithm;item unique identification;petri net	Geert Stremersch;René K. Boel	2000	IEEE Trans. Automat. Contr.	10.1109/9.895574	control engineering;real-time computing;stochastic petri net;computer science;control theory;mathematics;process architecture;petri net	EDA	-6.330309128737233	26.89555455792619	31210
c436ab68a4bf172cecafd37c429aeb7f2a63dc86	grace's inheritance		This article is an apologia for the design of inheritance in the Grace educational programming language: it explains how the design of Grace’s inheritance draws from inheritance mechanisms in predecessor languages, and defends that design as the best of the available alternatives. For simplicity, Grace objects are generated from object constructors, like those of Emerald, Lua, and Javascript; for familiarity, the language also provides classes and inheritance, like Simula, Smalltalk and Java. The design question we address is whether or not object constructors can provide an inheritance semantics similar to classes.		James W Noble;Andrew P. Black;Kim B. Bruce;Michael Homer;Timothy Jones	2017	Journal of Object Technology	10.5381/jot.2017.16.2.a2	software peer review;programming language;systems engineering;resource-oriented architecture;package development process;social software engineering;computer science	PL	-27.404531270695376	25.22089777025392	31233
70432ea01471038f2724ed15b1ad2ceff5d7e796	solving infinite games with bounds	verification;infinite games;theoretical computer science;lineare temporale logik;synthesis;linear temporal logic;informatik;unendliche spiele	We investigate the existence and the complexity of computing and implementing optimal winning strategies for graph games of infinite duration. Parameterized linear temporal logics are extensions of Linear Temporal Logic (LTL) by temporal operators equipped with variables for time bounds. In model-checking, such specifications were introduced as “PLTL” by Alur et al. and as “PROMPT-LTL” by Kupferman et al. We show how to determine in doubly-exponential time, whether a player wins a game with PLTL winning condition with respect to some, infinitely many, or all variable valuations. Hence, these problems are not harder than solving LTL games. Furthermore, we present an algorithm with triply-exponential running time to determine optimal variable valuations that allow a player to win a game. Finally, we give doubly-exponential upper and lower bounds on the values of optimal variable valuations. In Muller games, we measure the quality of a winning strategy using McNaughton’s scoring functions. We construct winning strategies that bound the losing player’s scores by two and show this to be optimal. This improves the previous best upper bound of n! in a game with n vertices, obtained by McNaughton. Using these strategies, we show how to transform a Muller game into a safety game whose solution allows to determine the winning regions of the Muller game and to compute a finite-state winning strategy for one player. This yields a novel antichain-based memory structure and the first definition of permissive strategies for Muller games. Moreover, we generalize our construction by presenting a new type of game reduction from infinite games to safety games and show its applicability to several other winning conditions.	algorithm;exptime;linear temporal logic;model checking;scoring functions for docking;time complexity;vertex (graph theory)	Martin Zimmermann	2012			combinatorics;theoretical computer science;mathematics;algorithm	Logic	-6.866700139154835	23.345278772272138	31315
a61c0adb79d61ad0ca7750a53f333f7774d227af	symmetry-driven decision diagrams for knowledge compilation		In this paper, symmetries are exploited for achieving significant space savings in a knowledge compilation perspective. More precisely, the languages FBDD and DDG of decision diagrams are extended to the languages Sym-FBDDX,Y and Sym-DDGX,Y of symmetry-driven decision diagrams, where X is a set of ”symmetry-free” variables and Y is a set of ”top” variables. Both the time efficiency and the space efficiency of Sym-FBDDX,Y and Sym-DDGX,Y are analyzed, in order to put those languages in the knowledge compilation map for propositional representations. It turns out that each of Sym-FBDDX,Y and Sym-DDGX,Y satisfies CT (the model counting query). We prove that no propositional language over a set X ∪ Y of variables, satisfying both CO (the consistency query) and CD (the conditioning transformation), is at least as succinct as any of Sym-FBDDX,Y and Sym-DDGX,Y unless the polynomial hierarchy collapses. The price to be paid is that only a restricted form of conditioning and a restricted form of forgetting are offered by Sym-FBDDX,Y and Sym-DDGX,Y . Nevertheless, this proves sufficient for a number of applications, including configuration and planning. We describe a compiler targeting Sym-FBDDX,Y and Sym-DDGX,Y and give some experimental results on planning domains, highlighting the practical significance of these languages.	automated planning and scheduling;compiler;diagram;duckduckgo;first-order predicate;formal language;high-level programming language;knowledge compilation;negation normal form;online and offline;polynomial hierarchy;sensor	Anicet Bart;Frédéric Koriche;Jean-Marie Lagniez;Pierre Marquis	2014		10.3233/978-1-61499-419-0-51	computer science;artificial intelligence;machine learning;algorithm	AI	-18.84047085554082	14.517814584044498	31330
b8462fdc8a4b5e92ec2406f37fb401d257d398d3	a discipline for constructing multiphase communication protocols	ring network;protocol design;communication protocol;data link control;communicating finite state machine;finite state machine	Many communication protocols can be observed to go through different phases performing a distinct function in each phase. A multiphase model for such protocols is presented. A phase is formally defined to be a network of communicating finite-state machines with certain desirable correctness properties; these include proper termination and freedom from deadlocks and unspecified receptions. A multifunction protocol is constructed by first constructing separate phases to perform its different functions. It is shown how to connect these phases together to realize the multifunction protocol so that the resulting network of communicating finite state machines is also a phase (i.e., it possesses the desirable properties defined for phases). The modularity inherent in multiphase protocols facilitates not only their construction but also their understanding and modification. An abundance of protocols have been found in the literature that can be constructed as multiphase protocols. Three examples are presented here: two versions of IBM's BSC protocol for data link control and a token ring network protocol.	binary symmetric channel;communicating finite-state machine;communications protocol;correctness (computer science);deadlock;multi-function printer;ring network;synchronous data link control;token ring	C. Edward Chow;Mohamed G. Gouda;Simon S. Lam	1985	ACM Trans. Comput. Syst.	10.1145/6110.214400	data link control;communications protocol;ring network;universal composability;real-time computing;computer science;theoretical computer science;distributed computing;finite-state machine;computer security;computer network	Networks	-31.81748908315265	31.725867741541023	31406
47fdca00fb9f2a383be96fb219e2ed9b57f35d71	performance evaluation of a controlled flow-shop system with a timed petri net model	industrial case study;coloured petri net;performance evaluation;simulation;assembly lines;time petri net;performance analysis;control;petri nets;petri net;flow shop;model simulation	This paper presents an original performance analysis applied to a flow-shop system driven by a set of local command units and a central controller. The performance evaluation is done with a timed coloured Petri net model. Simulation results show needs for bounding the controller response time in order to meet production targets.	central processing unit;coloured petri net;control system;fifo (computing and electronics);performance evaluation;production system (computer science);response time (technology);scheduling (computing);simulation	Loïc Plassart;Philippe Le Parc;Frank Singhoff;Lionel Marcé	2006			real-time computing;simulation;stochastic petri net;computer science;petri net	Robotics	-7.943107258307058	30.748294747764657	31431
59989546cf7c260a5bc48a987d189d49520a7eae	quantum logics and bivariable functions	mesure simultanee;logica algebraica;finite atomistic quantum logic;simultaneous measurements;03g12;algebraic logic;03g10;logique mathematique;fonction speciale;03h05;mathematical logic;bivariable functions;supremum measure;d map;funcion especial;modelisation;enrejado;orthomodular lattice;treillis;simultaneous measurement;logique algebrique;modeling infimum measure;special function;quantum logics;s map;conditional state;medicion simultanea;03g25;calcul quantique;quantum computing;logique quantique;lattice	To model noncompatible events, a quantum logic was chosen among various algebraic structures as the suitable one. This paper deals with a characterization of a center in various types of quantum logics by means of special bivariable functions defined on them. Any quantum logic can be described as a union of blocks (a block in a given quantum logic L is the maximal Boolean subalgebra of L) [16]. Center C(L) of a quantum logic L is its Boolean subalgebra of elements compatible with all other elements of L. Each quantum logic L has a center that can be taken as a common part of its blocks. In this paper three types of quantum logics are studied:	boolean algebra;linear algebra;maximal set;quantum logic	Eva Drobná;Olga Nánásiová;Lubica Valásková	2010	Kybernetika		algebraic logic;mathematical logic;mathematical analysis;discrete mathematics;lattice;mathematics;quantum computer;algebra	Theory	-6.674276477422527	9.851815101326554	31436
a655534a2f8d7edae344bfd6fe9fbeb73d2c9472	applying visible strong equivalence in answer-set program transformations	auxiliary atom;answer-set program transformation;correct program;visible strong equivalence;basic notion;visible equivalence;new generalization;visible atom;answer-set semantics;strong equivalence;logic program	auxiliary atom;answer-set program transformation;correct program;visible strong equivalence;basic notion;visible equivalence;new generalization;visible atom;answer-set semantics;strong equivalence;logic program	program transformation;turing completeness	Tomi Janhunen;Ilkka Niemelä	2012		10.1007/978-3-642-30743-0_24	logical equivalence;discrete mathematics;boundary-value analysis;adequate equivalence relation;mathematics;equivalence relation;algorithm	Logic	-11.94209184415958	16.467923682583244	31459
898fdc32d93bd89196ca8aa9fef78f254e315d6c	a generic framework for description logics with uncertainty	description logic	We propose an extension to Description Logics (DLs) with uncertainty which unifies and/or generalizes a number of existing frameworks for DLs with uncertainty. To this end, we first give a classification of these frameworks and identify the essential features as well as properties of the various combination functions allowed in the underlying uncertainty formalisms they model. This also allows us express the semantics of the DL elements in a flexible manner. We illustrate how various DLs with uncertainty can be expressed in our generic framework.	.net framework;axiomatic system;database;description logic;encs;essence;fuzzy logic;mathematical optimization	Volker Haarslev;Hsueh-Ieng Pai;Nematollaah Shiri	2005			discrete mathematics;data mining;mathematics;algorithm	AI	-21.791978529748622	6.655550043704119	31462
dd111b08e5749eab586f387a366bca08c237c213	around dot depth two	interval temporal logic;satisfiability;deterministic finite automata;partial order	It is known that the languages definable by formulae of the logics FO2[<,S], ∆2[<,S], LTL[F,P,X,Y] are exactly the variety DA∗D. Automata for this class are not known, nor is its precise placement within the dot-depth hie rarchy of starfree languages. It is easy to argue that ∆2[<,S] is included in∆3[<]; in this paper we show that it is incomparable with B(Σ2)[<], the boolean combination ofΣ2[<] formulae. Using ideas from Straubing’s “delay theorem”, we extend our earlier work [LPS08] to propose partially-ordered two-way deterministic finite automata with look-around ( po2dla) and a new interval temporal logic called LITL and show that they also characterize the variety DA∗D. We give effective reductions from LITL to equivalent po2dlaand frompo2dlato equivalent FO2[<,S]. Thepo2dlaautomata admit efficient operations of boolean closure and the language non-emptiness of po2dlais NP-complete. Using this, we show that satisfiability of LITL remains NP-complete assuming a fixed look-around le gth. (Recall that forLTL[F,X], it is PSPACE-hard.) A rich set of correspondences has been worked out between div rse mechanisms for defining the first-order definable word languages and their su bclasses (a recent survey is [DGK08]). In the following, CFA refers to counter-free au tomata, SFRE to star-free regular expressions and Ap refers to the variety of aperiodic monoids [Pin86]. CFA≡ SFRE≡ Ap≡ FO[<] ≡ LTL[U,S] ≡ ITL Further, Thomas showed [Tho82] that by restricting the quan tifier-alternation depth in theFO[<] formulae a strictdot-depth hierarchy of star-free languages is obtained, see the paper by Pin and Weil [PW97] for details. For example, B(Σ2)[<] is the class of languages defined by the boolean combination of Σ2[<] formulae, which are the ones which have one block of existential quantifiers followed by o ne block of universal quantifiers followed by a quantifierless formula. For theFO formulations below, given an alphabet A anda∈ A, the unary predicate Qa(x) holds iff the letter at positionx is a. The binary predicateS(x,y) denotes the successor relation on positions, and < is, as usual, its transitive closure. Example 1.Let A = {a,b} be the alphabet described by φA def = ∀x. Qa(x)∨Qb(x), which will be an additional conjunct below, not explicitly m entioned. – φ1 def = ∃x∃y. S(x,y)∧Qa(x)∧Qa(y) is aB(Σ1)[S] formula definingL1 = A∗aaA∗. – φ2 def = ∃x∃y. Qa(x)∧Qa(y)∧∀z. (x < z⊃ y≤ z) is aΣ2[<] formula definingL1. – Let φ3 def = (∀x. f irst(x) ⊃ Qa(x))∧ (∀x. last(x) ⊃ Qb(x))∧ (∀x,y. ((x < y)∧Qa(x)∧Qa(y) ⊃ ∃z. x < z∧z< y∧Qb(z)))∧ (∀x,y. ((x < y)∧Qb(x)∧Qb(y) ⊃ ∃z. x < z∧z< y∧Qa(z))) . Then,φ3 is aΠ2[<] formula defining the language L2 = (ab)∗. ⊓⊔ More recently, Th́erien and Wilke [TW98] showed that the 2-variable fragment FO2[<] [Mor75] (where only two variables occur, quantified any numb er of times), is expressively equivalent to the unambiguous languages and v arietyDAof Scḧutzenberger [Sch76,TT02] and the subset ∆2[<] in the dot-depth hierarchy. Etessami, Vardi and Wilke [EVW02] identified the unary temporal logic LTL[F,P] and Schwentick, Th́ erien and Vollmer [STV02] identifiedpartially-ordered 2-way deterministic finite automata (these are also called linear [LT00]) as equivalent formalisms. In [LPS08], we added to these correspondences a “deterministic” interval tempo ral l gic calledUITL. The papers [TW98,EVW02] also characterized FO2[<,S], which can define languages not definable in the logicFO2[<] such as those in Example 1. For a detailed study of these logics, see the recent papers of Weis and Immerman [WI07], and of Kufleitner and Weil [KW09]. PO2DFA≡UL ≡ DA≡ FO2[<] ≡ ∆2[<] ≡ LTL[F,P] ≡ UITL DA∗D ≡ FO2[<,S] ≡ ∆2[<,S] ≡ LTL[F,P,X,Y] It is clear that∆2[<,S] ⊆ ∆3[<] since successor can be defined using < and one quantifier. In this paper we provide an automaton characteri zation and an interval logic characterization for this class of languages, and we separa te it fromB(Σ2)[<], the languages defined by the boolean combination of Σ2[<] formulae. This also shows that FO2[<,S] is apropersubset of∆3[<], as diagrammatically depicted below.	automata theory;deterministic finite automaton;finite-state machine;first-order predicate;interval temporal logic;karp's 21 np-complete problems;p (complexity);pspace;quantifier (logic);regular expression;span and div;symmetric multiprocessing;transitive closure;tridiagonal matrix algorithm;unary operation;universal quantification	Kamal Lodaya;Paritosh K. Pandya;Simoni S. Shah	2010		10.1007/978-3-642-14455-4_28	partially ordered set;combinatorics;discrete mathematics;interval temporal logic;computer science;deterministic finite automaton;mathematics;programming language;algorithm;algebra;satisfiability	Logic	-6.873012367423466	19.046049343615394	31466
5830f77eebd6c6758e4faaab364eb0994570fd86	an executable specification of asynchronous pi-calculus semantics and may testing in maude 2.0	rewrite rule;executable specification;operational semantics	We describe an executable specification of the operational semantics of an asynchronous version of the π-calculus in Maude by means of conditional rewrite rules with rewrites in the conditions. We also present an executable specification of the may testing equivalence on non-recursive asynchronous π-calculus processes, using the Maude metalevel. Specifically, we describe our use of the metaSearch operation to both calculate the set of all finite traces of a non-recursive process, and to compare the trace sets of two processes according to a preorder relation that characterizes may testing in asynchronous π-calculus. Thus, in both the specification of the operational semantics and the may testing, we make heavy use of new features introduced in version 2.0 of the Maude language and system.	asynchronous circuit;executable;maude system;operational semantics;recursion;rewrite (programming);rewriting;tracing (software);turing completeness;π-calculus	Prasanna Thati;Koushik Sen;Narciso Martí-Oliet	2002	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(05)82539-3	computer science;database;programming language;operational semantics;algorithm	Logic	-26.29636684055314	31.622963412871247	31519
b0778d86cb8c9fba56445e85f6138012685bf93a	monitoring of timed discrete events systems with interrupts	modelizacion;systeme evenement discret;sistema temporizado;neural networks;automaton delimit location;stopwatch automata;surveillance;localization;sputtering;timed system;timed discrete events systems interrupting faults monitoring reachability analysis stopwatch automata;automaton;localizacion;process design;reachability analysis automata theory discrete event systems;sistema acontecimiento discreto;modelisation;automata;artificial neural networks;discrete event system;vigilancia;localisation;intermitencia;monitoring;timed discrete events systems;transistors;automate;discrete event systems;systeme temporise;intermittency;automata theory;genetic algorithms;monitoring discrete event systems sputtering neural networks transistors process design artificial neural networks genetic algorithms optical films optimization methods;monitorage;intermittence;monitoreo;interrupting faults;modeling;time discretization;reachability analysis;optical films;timed discrete events systems monitoring;analyse atteignabilite;automaton delimit location timed discrete events systems monitoring stopwatch automata reachability analysis timed subspace;optimization methods;timed subspace	A framework is introduced for monitoring the interrupting faults in the timed discrete events systems. We introduce the notion of acceptable behavior of the system subjected to these faults: permanent or intermittent. The acceptable behavior of a system is modeled by a stopwatch automaton. The timed sub-spaces in the locations of the automaton delimit exactly the range of the acceptable behavior. They are synthesized using the techniques of reachability analysis of stopwatch automata in a way to detect the system faults as early as possible.	automaton;interrupt;reachability	Adib Allahham;Hassane Alla	2010	IEEE Transactions on Automation Science and Engineering	10.1109/TASE.2009.2015957	real-time computing;computer science;artificial intelligence;machine learning;distributed computing;automaton;timed automaton;artificial neural network;algorithm	Embedded	-6.4233963893300405	28.08097633931851	31556
b4c96b49fb77c76f35a13889674f0d0be965debe	reference management in a loosely coupled, distributed information system	distributed hash table;distributed information system;query answering	References between objects in loosely coupled distributed information systems pose a problem. On the one hand, one tries to avoid referential inconsistencies like, e.g., dangling links in the WWW. On the other hand, using strict constraints as in databases may restrict the data providers severely. We present the solution to this problem that we developed for the Nexus system. The approach tolerates referential inconsistencies in the data while providing consistent query answers to users. For traversing references, we present a concept based on return references. This concept is especially suitable for infrequent object migrations and provides a good query performance. For scenarios where object migrations are frequent, we developed an alternative concept based on a distributed hash table.	information system;loose coupling	Matthias Großmann;Nicola Hönle;Daniela Nicklas;Bernhard Mitschang	2008		10.1007/978-3-540-85713-6_7	computer science;chord;theoretical computer science;database;distributed computing;programming language	DB	-26.263134928236664	7.525635615242696	31557
54d21ecb08dbc0200045f6bb9b854be633d9f43c	on the deductive strength of various distributivity axioms for boolean algebras in set theory	axiom of choice;boolean algebra;distributivity;prime ideal theorem;set theory		set theory	Yasuo Kanai	2002	Math. Log. Q.	10.1002/1521-3870(200204)48:3%3C413::AID-MALQ413%3E3.0.CO;2-C	urelement;zermelo–fraenkel set theory;axiom schema;boolean algebra;kripke–platek set theory;axiom of extensionality;mathematical analysis;discrete mathematics;distributivity;ideal;topology;ultrafilter;morse–kelley set theory;axiom independence;action axiom;general set theory;non-well-founded set theory;scott's trick;reverse mathematics;stone's representation theorem for boolean algebras;mathematics;axiom of choice;boolean prime ideal theorem;theory;s;constructive set theory;set theory;algebra	Logic	-8.481787456801483	12.102655183152368	31588
72f4315c32e99bce08a4851c37ac62837cd32cf6	strategic construction of fitch-style proofs	theorem prover;natural deduction	Symlog is a system for learning symbolic logic by computer that allows students to interactively construct proofs in Fitch-style natural deduction. On request, Symlog can provide guidance and advice to help a student narrow the gap between goal theorem and premises. To effectively implement this capability, the program was equipped with a theorem prover that constructs proofs using the same methods and techniques the students are being taught. This paper discusses some of the aspects of the theorem prover's design, including its set of proof-construction strategies, its unification algorithm as well as some of the tradeoffs between efficiency and pedagogy.	fitch notation	Frederic D. Portoraro	1998	Studia Logica	10.1023/A:1005087316935	discrete mathematics;philosophy;computer science;mathematics;automated theorem proving;programming language;natural deduction;algorithm;algebra	Crypto	-18.739572830513104	17.551603440213963	31621
0f5114571bcbf7c9edef655123dbb3192e52b84f	linear logic and noncommutativity in the calculus of structures	calculus of structures;linear logic	Abstract: macro \clap,whichisused on almostevery page, came out of such a discussion.This thesis would not exist without the support of my wife Jana. During all the timeshe has been a continuous source of love and inspiration.This PhD thesis has been written with the financial support of the DFG-Graduiertenkolleg334 &quot;Spezifikation diskreter Prozesse und Prozesysteme durch operationelle Modelleund Logiken&quot;.iiiivTab l e o f ContentsAcknowledgements iiiTab l e of Contents vList of Figures vii...	calculus of structures;linear logic	Lutz Straßburger	2003			zeroth-order logic;linear logic;higher-order logic;geometry of interaction;intermediate logic;time-scale calculus;proof calculus;minimal logic;noncommutative logic;situation calculus;curry–howard correspondence;monadic predicate calculus;lambda cube;natural deduction;substructural logic;multivariable calculus	Logic	-13.118960917754418	12.297664120491039	31623
290a22f3a22fd34f9756ebcf99b3e007641e9bee	a representation theorem for models of *-free pdl	boolean algebra;kripke model;representation theorem;propositional dynamic logic	We introduce dynamic algebras and show how they can be used to give an algebraic interpretation to propositional dynamic logic (PDL). Dynamic algebras include all Kripke models, the standard interpretation of PDL. We give a simple algebraic condition on *-free dynamic algebras that is necessary and sufficient for representation by *-free Kripke models. In the presence of*, the condition is sufficient for representation by a nonstandard Kripke model. This result leads to a duality between certain topological Kripke models and dynamic algebras analogous to the duality between Boolean algebras and their Stone spaces.	perl data language (pdl);three-valued logic	Dexter Kozen	1980		10.1007/3-540-10003-2_83	modal logic;trivial representation;boolean algebra;combinatorics;discrete mathematics;intuitionistic logic;intermediate logic;consensus theorem;stone's representation theorem for boolean algebras;schaefer's dichotomy theorem;first-order logic;mathematics;propositional variable;compactness theorem;boolean function;complete boolean algebra;kripke semantics;second-order logic;two-element boolean algebra;free boolean algebra;gödel's completeness theorem;model theory;algebra	Logic	-11.190747096349767	12.94885495976793	31626
924f34aa3ff5c6d1d8e93d14d376ff180de5ea93	three level petri nets rule based reduction	petri net;rule based		petri net	Toader Jucan;Oana Captarencu	2002	Sci. Ann. Cuza Univ.		theoretical computer science;rule-based system;distributed computing;petri net;computer science;stochastic petri net	EDA	-22.68206789529906	17.058904340070384	31640
f0a2ac8dbb56afdd9cafdcaa0a483c85678048dd	symbolic model checking of extended finite state machines with linear constraints over integer variables	ctl;extended finite state machine	We propose a symbolic model checking algorithm for a class of extended finite state machines equipped with integer variables (FSM&sol;int). An FSM&sol;int has several constraints on variable assignments, such as a variable of FSM&sol;int keeps its value until the control visits at the definition transition and new value is fed via external input. Our model checking algorithm verifies whether an FSM&sol;int satisfies a property described in CTL-like expressions over integer variables. We have implemented a model checker, and verified that blackjack dealer circuits and a packet multiplex protocol satisfy some designated properties. We have found that the verification of systems with 100 states and 10 integer variables can be carried out in a few seconds in most cases (in the worst case, a few minutes). © 2006 Wiley Periodicals, Inc. Syst Comp Jpn, 37(6): 64–72, 2006; Published online in Wiley InterScience (). DOI 10.1002&sol;scj.20264	finite-state machine;model checking	Takashi Takenaka;Kozo Okano;Teruo Higashino;Kenichi Taniguchi	2006	Systems and Computers in Japan	10.1002/scj.20264	model checking;extended finite-state machine;combinatorics;discrete mathematics;computer science;artificial intelligence;machine learning;mathematics;geometry;ctl*;programming language;symbolic trajectory evaluation;algorithm;statistics	EDA	-11.544266708221103	25.26943968286003	31653
a047e896bc9893a19c4342ed0df655fee6827822	first-order model checking problems parameterized by the model	research outputs;research publications;first order;model checking;constraint satisfaction problem;first order logic	We study the complexity of the model checking problem, for fixed models  A , over certain fragments $\mathcal{L}$ of first-order logic, obtained by restricting which of the quantifiers and boolean connectives we permit. These are sometimes known as the expression complexities of $\mathcal{L}$. We obtain various full and partial complexity classification theorems for these logics $\mathcal{L}$ as each ranges over models  A , in the spirit of the dichotomy conjecture for the Constraint Satisfaction Problem --- which itself may be seen as the model checking problem for existential conjunctive positive first-order logic.	first-order predicate;model checking	Barnaby Martin	2008		10.1007/978-3-540-69407-6_45	mathematical optimization;combinatorics;discrete mathematics;computer science;first-order logic;mathematics;programming language;algorithm	Logic	-9.139069596657464	17.299286625063683	31701
c6208454e50c60a3b0c4045853279189cf478910	necessary and possible set reconciliation and unification in semantic tableau systems (extended abstract)			han unification;method of analytic tableaux	Robert Johnson;Steve Reeves	1992			algorithm;computer science;discrete mathematics;method of analytic tableaux;unification	Theory	-12.516716942643955	12.838222240005567	31704
ae2bf2eaa52d0e32555fdf7998b1fbc51e7acc57	model counting for recursively-defined strings		We present a new algorithm for model counting of a class of string constraints. In addition to the classic operation of concatenation, our class includes some recursively defined operations such as Kleene closure, and replacement of substrings. Additionally, our class also includes length constraints on the string expressions, which means, by requiring reasoning about numbers, that we face a multi-sorted logic. In the end, our string constraints are motivated by their use in programming for web applications. Our algorithm comprises two novel features: the ability to use a technique of (1) partial derivatives for constraints that are already in a solved form, i.e. a form where its (string) satisfiability is clearly displayed, and (2) non-progression, where cyclic reasoning in the reduction process may be terminated (thus allowing for the algorithm to look elsewhere). Finally, we experimentally compare our model counter with two recent works on model counting of similar constraints, SMC [18] and ABC [5], to demonstrate its superior performance.	algorithm;color gradient;concatenation;experiment;kleene star;recursion;recursive definition;substring;web application	Minh-Thai Trinh;Duc-Hiep Chu;Joxan Jaffar	2017		10.1007/978-3-319-63390-9_21	concatenation;theoretical computer science;web application;recursion;computer science;kleene star;expression (mathematics);substring	Logic	-11.14714346053044	23.286867993364915	31710
28bb94919c8a1e8518a0f8fb599960a5fcce30d8	profile trees for büchi word automata, with application to determinization		The determinization of Büchi automata is a celebrated problem, with applications in synthesis, probabilistic verification, and multi-agent systems. Since the 1960s, there has been a steady progress of constructions: by McNaughton, Safra, Piterman, Schewe, and others. Despite the proliferation of constructions, they are all essentially ad-hoc constructions, with little theory behind them other than proofs of correctness. Since Safra, all optimal constructions employ trees as states of the deterministic automaton, and transitions between states are defined operationally over these trees. The operational nature of these constructions complicates understanding, implementing, and reasoning about them, and should be contrasted with complementation, where a solid theory in terms of automata run DAGs underlies modern constructions. In 2010, we described a profile-based approach to Büchi complementation, where a profile is simply the history of visits to accepting states. We developed a structural theory of profiles and used it to describe a complementation construction that is deterministic in the limit. Here we extend the theory of profiles to prove that every run DAG contains a profile tree with at most a finite number of infinite branches. We then show that this property provides a theoretical grounding for a new determinization construction where macrostates are doubly preordered sets of states. In contrast to extant determinization constructions, transitions in the new construction are described declaratively rather than operationally.	büchi automaton;correctness (computer science);declarative programming;deterministic automaton;directed acyclic graph;hoc (programming language);multi-agent system;powerset construction;probabilistic turing machine;tree (data structure)	Seth Fogarty;Orna Kupferman;Moshe Y. Vardi;Thomas Wilke	2013		10.4204/EPTCS.119.11	natural language processing;büchi automaton;programming language	Logic	-12.755397502214876	25.269813553293194	31717
d5c49b45c08276e6dac0fae622bfaa1ff9e11bdb	temporal reasoning without transitive tables	temporal information;artificial intelligent;inference rule;formal language;temporal reasoning	Representing and reasoning about qualitative temporal information is an essential part of many artificial intelligence tasks. Lots of models have been proposed in the litterature for representing such temporal information. All derive from a point-based or an interval-based framework. One fundamental reasoning task that arises in applications of these frameworks is given by the following scheme: given possibly indefinite and incomplete knowledge of the binary relationships between some temporal objects, find the consistent scenarii between all these objects. All these models require transitive tables — or similarly inference rules — for solving such tasks. In [30], we have defined an alternative model, renamed in [31] S-languages – for Set-languages – to represent qualitative temporal information, based on the only two relations of precedence and simultaneity. In this paper, we show how this model enables to avoid transitive tables or inference rules to handle this kind of problem.	artificial intelligence	Sylviane R. Schwer	2007	CoRR		formal language;qualitative reasoning;computer science;artificial intelligence;machine learning;data mining;mathematics;algorithm;rule of inference	AI	-18.716523501797234	7.825466891014815	31787
2525726cbb80539694b6878546a608ad46abfecf	a framework for analyzing programs written in proprietary languages	database system;trace analysis;error recovery;java programming;program analysis;static analysis tools;off the shelf;source to source translation	There are several commercial products that use proprietary languages, which typically look like a wrapper around (some proprietary extension of) the standard SQL language. Examples of these languages include ABAP, Informix, XBase++, SQR and so on. These application are difficult to analyze not only because it is hard to model the semantics of the underlying database systems but also because of the lack of standard tools for analysis. One naive way to analyse such programs is to collect dynamic trace using proprietary debuggers and run the analyses on the trace. However, this form of dynamic trace collection can be a severe performance bottleneck. In this paper, we present our experience with building a framework to help in efficient program analysis in the context of ticket resolution for ABAP programs.  In our framework, we first translate the given ABAP programs to semantically equivalent annotated Java programs. These Java programs are then executed to generate the required dynamic trace. Our framework allows the plugging of off-the-shelf static analysis tools (applied on the Java programs) and dynamic trace analysis tools (on the generated trace) and maps the results from these analysis tools back to the original ABAP programs. One novel aspect of our framework is that it admits incomplete ABAP grammar, which is an important aspect when dealing with proprietary languages where the grammar may not be publicly available. We have used our framework on several benchmarks to validate the translation, and establish the efficiency and the utility of our instrumented Java code along with the collected trace.	abap/4;benchmark (computing);binary search algorithm;computer programming;dbg;database;debugger;debugging;embrace, extend and extinguish;exception handling;ibm informix;internationalization and localization;java;limited availability;map;overhead (computing);pattern matching;sqr;static program analysis;switch statement;tracing (software);xbase;xbase++	V. Krishna Nandivada;Mangala Gowri Nanda;Pankaj Dhoolia;Diptikalyan Saha;Anjan Nandy;Anup Ghosh	2011		10.1145/2048147.2048223	program analysis;computer science;data mining;database;programming language;static program analysis	PL	-21.1897108272325	31.996983776810183	31790
74ef1a2581aba43c0f912dda3844f73e3fecf26f	the correspondence between the concepts in description logics for contexts and formal concept analysis	concept;attribute role;description logic formal concept analysis concept attribute role;description logic;formal concept analysis	Formal concept analysis (FCA) and description logic (DL) are meant to be formalizations of concepts. A formal concept in the former consists of its intent and extent, where the intent is the set of all the attributes shared by each object in the extent of the concept, and the extent is the set of all the objects sharing each property in the intent of the concept. A concept in the latter formalization is simply a concept name, the interpretation of which is a subset of a universe. To consider the correspondence between concepts in both formalizations, a multi-valued formal context must be represented both as a knowledge base and as a model of the DL for contexts, where concepts are decomposed into tuple concepts C, interpreted as a set of tuples and value concepts V, interpreted as a set of attribute-value pairs. We show that there is a difference between the interpretation of concepts ∀R.V/∀R −.C and the Galois connection between the extent/intent of formal concepts in FCA. According to the Galois connection, there should be concepts of the form +∀R.V and +∀R −.C interpreted in FCA, and hence the logical language L for DL is extended to be L + together with +∀ as a constructor so that +∀R.V and +∀R −.C are well-defined concepts. Conversely, according to the interpretation in DL there should be pseudo concepts in FCA so that the interpretation of concepts ∀R.V/∀R −.C is the extent/intent of pseudo concepts. The correspondence between formal concepts and concepts in L +, and between pseudo concepts and concepts in L are presented in this paper.	attribute–value pair;description logic;formal concept analysis;interpretation (logic);knowledge base	Yue Ma;Yuefei Sui;Cungen Cao	2011	Science China Information Sciences	10.1007/s11432-011-4376-7	discrete mathematics;concepts;description logic;computer science;formal concept analysis;machine learning;mathematics;lattice miner;concept;algorithm	AI	-20.269023582516706	5.786476082145151	31798
8e7f401f8e02e0b4eb91321d40d12aeb45382bce	adapting calculational logic to the undefined	three valued logic;partial function;logica booleana;logique trivalente;preuve programme;program proof;formal specification;programmation;semantica formal;logic;predicate calculus;formal semantics;calcul predicat;fixed point;specification formelle;programacion;semantique formelle;especificacion formal;calculo predicado;prueba programa;logique booleenne;logica trivalente;boolean logic;programming logic;programming;logique;logica	Programming abounds with partial functions. Some functions are inherently partial (e.g. division and referring to the first element of a list). Other functions are partial according to their recursive definitions. If partial functions are admitted in formulae, a programming logic is needed that handles partial functions and undefined terms. Dijkstra and Scholten [1], Gries and Schneider [2], Feijen, van Gasteren, and other researchers in programming methodology have discovered that programming logics are useful only if they are suited to ‘proof engineering’, that is, to the design and presentation of proofs. A programming logic is acceptable only if it is a handy tool without overwhelming complexity. So we search for a logic that allows us to handle partial functions and undefined terms in a convenient and simple way. Plenty of logical calculi for partial functions have been proposed [3–22]. We will investigate in Section 1 to what extent these calculi are convenient tools.	boolean expression;complexity;computational logic;david gries;description logic;emoticon;fixed point (mathematics);handy board;logic programming;operand;recursion;recursive definition;software development process;the computer journal;three laws of robotics;three-valued logic;undefined behavior	Birgit Schieder;Manfred Broy	1999	Comput. J.	10.1093/comjnl/42.2.73	programming;discrete mathematics;partial function;computer science;formal semantics;formal specification;mathematics;fixed point;programming language;logic;algorithm	PL	-15.00328563821524	17.575003376294084	31812
70dbf0c2a1c8dc812c1b7b99ec52f13e9940a903	dynamic imperative languages for runtime extensible semantics and polymorphic meta-programming	developpement logiciel;herencia;lenguaje programacion;dynamic typing;langage imperatif;naming;sobrecarga;object oriented language;programming language;langage type;heritage;imperative language;semantics;program transformation;contrato;transformation programme;semantica;semantique;lengua blanco;software pattern;dynamic binding;transformacion programa;contract;object oriented;desarrollo logicial;surcharge;target language;polymorphism;langue cible;typed language;software development;denomination;metaprogrammation;langage programmation;denominacion;oriente objet;generating function;polymorphisme;polimorfismo;contrat;higher order functions;lenguaje imperativo;inheritance;metaprogramming;overload;meta programming;orientado objeto;metaprogramacion;lenguaje tipado	Dynamically typed languages imply runtime resolution for type matching, setting-up an effectible ground for type-polymorphic functions. In statically typed object-oriented languages, operator overloading signifies the capability to statically extend the language semantics in the target program context. We show how the same can be accomplished dynamically in the Delta dynamic language, through simple member-function naming contracts. Additionally, we provide a software-pattern for dynamically extensible function semantics, something that cannot be accommodated with static function overloading. We demonstrate how meta-programming, i.e. crafting of parametric program capsules solving generic problems known as meta-algorithms or meta-components, become truly polymorphic, i.e. can accept an open set of parameter values, as far as those dynamically bind to eligible elements compliant to the meta-program design contract. In Delta, inheritance is dynamically supported as a runtime function, without any compile-time semantics, while all member function calls are resolved through late binding. We employ those features to show how Delta supports the imperative programming of polymorphic higher-order functions, such as generic function composers or the map function.	imperative programming;metaprogramming	Anthony Savidis	2005		10.1007/11751113_9	computer science;programming language;communication;algorithm	PL	-26.157944283676564	28.870609579073726	31824
3e7627b8e37387a3b62a81cd420ac045e3b7204a	symbolic range analysis of pointers	precision alias analysis range analysis speed;computer languages;alias analysis;range analysis;algorithm design and analysis program processors syntactics lips benchmark testing computer languages;c idioms symbolic range analysis alias analysis compilers optimize languages c llvm compiler;speed;precision;syntactics;lips;program processors;algorithm design and analysis;program compilers c language;benchmark testing	Alias analysis is one of the most fundamental techniques that compilers use to optimize languages with pointers. However, in spite of all the attention that this topic has received, the current state-of-the-art approaches inside compilers still face challenges regarding precision and speed. In particular, pointer arithmetic, a key feature in C and C++, is yet to be handled satisfactorily. This paper presents a new alias analysis algorithm to solve this problem. The key insight of our approach is to combine alias analysis with symbolic range analysis. This combination lets us disambiguate fields within arrays and structs, effectively achieving more precision than traditional algorithms. To validate our technique, we have implemented it on top of the LLVM compiler. Tests on a vast suite of benchmarks show that we can disambiguate several kinds of C idioms that current state-of-the-art analyses cannot deal with. In particular, we can disambiguate 1.35x more queries than the alias analysis currently available in LLVM. Furthermore, our analysis is very fast: we can go over one million assembly instructions in 10 seconds.	abstract interpretation;algorithm;alias analysis;array data structure;c++;compiler;llvm;pointer (computer programming);programming idiom	Vitor Paisante;Maroua Maalej;Leonardo B. Oliveira;Laure Gonnord;Fernando Magno Quintão Pereira	2016	2016 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)	10.1145/2854038.2854050	algorithm design;benchmark;parallel computing;alias analysis;computer science;theoretical computer science;operating system;accuracy and precision;speed;programming language	PL	-18.60565527347212	29.582734331748345	31839
68816db0a94fdbb44ea7e95d14cea6b23db77ed7	video query processing in the vdbms testbed for video database research	video streaming;management system;query processing;data stream;continuous query;abstract data type;data type;window join algorithm;stream processing;rank join algorithm;video database	The increased use of video data sets for multimedia-based applications has created a demand for strong video database support, including efficient methods for handling the content-based query and retrieval of video data. Video query processing presents significant research challenges, mainly associated with the size, complexity and unstructured nature of video data. A video query processor must support video operations for search by content and streaming, new query types, and the incorporation of video methods and operators in generating, optimizing and executing query plans. In this paper, we address these query processing issues in two contexts, first as applied to the video data type and then as applied to the stream data type. We first present the query processing functionality of the VDBMS video database management system as a framework designed to support the full range of functionality for video as an abstract data type. We describe two query operators for the video data type which implement the rank-join and stop-after algorithms. As videos may be considered streams of consecutive image frames, video query processing can be expressed as continuous queries over video data streams. The stream data type was therefore introduced into the VDBMS system, and system functionality was extended to support general data streams. From this viewpoint, we present an approach for defining and processing streams, including video, through the query execution engine. We describe the implementation of several algorithms for video query processing expressed as continuous queries over video streams, such as fast forward, region-based blurring and left outer join. We include a description of the window-join algorithm as a core operator for continuous query systems, and discuss shared execution as an optimization approach for stream query processing.	abstract data type;algorithm;complexity;database;fast forward;image processing;join (sql);mathematical optimization;streaming media;testbed	Walid G. Aref;Moustafa A. Hammad;Ann Christine Catlin;Ihab F. Ilyas;Thanaa M. Ghanem;Ahmed K. Elmagarmid;Mirette S. Marzouk	2003		10.1145/951676.951682	online aggregation;sargable;query optimization;query expansion;web query classification;computer science;query by example;theoretical computer science;video tracking;database;rdf query language;smacker video;web search query;view;video post-processing;information retrieval;query language	DB	-31.37042741131124	5.403647211188017	31842
4894d09faeb5ca77b7ab92486276e9eec9486b95	equivalences in multi-valued asynchronous models of regulatory networks		Multi-valued network models can be described by their topology and a set of parameters capturing the effects of the regulators for each component. Dynamics can then be derived and represented as state transition systems. Different network models may lead to the same transition system, meaning dynamics analysis of a representative model covers a larger class of models. While rather clear in the Boolean case, the properties contributing to this effect become more involved for multi-valued models. We analyze these properties and present a mathematical description of the resulting model equivalence classes.		Adam Streck;Heike Siebert	2014		10.1007/978-3-319-11520-7_60	transition system;computer science;theoretical computer science;network model;discrete mathematics;equivalence class;asynchronous communication	Logic	-8.35904213199138	21.158247595501532	31885
3238c74c07d9c4e95cd11ee31fe91fd44993a98a	fault-tolerant bisimulation and process tranformations	labelled transition system;fault tolerant;algebraic logic;qa76 electronic computers computer science computer software;hennessy milner logic	We provide three methods of verifying concurrent systems which are tolerant of faults in their operating environment-algebraic, logical and transformational. The rst is an extension of the bisimulation equivalence, the second is rooted in the Hennessy-Milner logic, and the third involves transformations of CCS processes. Based on the common semantic model of labelled transition systems, which is also used to model faults, all three methods are proved equivalent for certain classes of faults.	bisimulation;concurrency (computer science);hennessy–milner logic;operating environment;transformational grammar;turing completeness;verification and validation	Tomasz Janowski	1994		10.1007/3-540-58468-4_174	discrete mathematics;computer science;bisimulation;theoretical computer science;hennessy–milner logic;algorithm	Logic	-11.530345094710743	22.40266739883321	31969
3e30e2fe9324e1867419539fc5c6faf84d10cdd0	moving objects: logical relationships and queries	moving object;query language;localizacion objeto;temporal logic;moving object database;object location;corps mobile;differential geometry;interrogation base donnee;interrogacion base datos;query optimization;linear constraint;lenguaje interrogacion;tracking movable target;data model;multi dimensional;cuerpo movil;time use;geometrie differentielle;langage interrogation;poursuite;moving body;position velocity;geometria diferencial;localisation objet;database query;persecucion y continuacion	In moving object databases, object locations in some multidimensional space depend on time. Previous work focuses mainly on moving object modeling (e.g., using ADTs, temporal logics) and ad hoc query optimization. In this paper we investigate logical properties of moving objects in connection with queries over such objects using tools from differential geometry. In an abstract model, object locations can be described as vectors of continuous functions of time. Using this conceptual model, we examine the logical relationships between moving objects, and between moving objects and (stationary) spatial objects in the database. We characterize these relationships in terms of position, velocity, and acceleration. We show that these fundamental relationships can be used to describe natural queries involving time instants and intervals. Based on this foundation, we develop a concrete data model for moving objects which is an extension of linear constraint databases. We also present a preliminary version of a logical query language for moving object databases.	aggregate data;aggregate function;algorithm;data model;database;hoc (programming language);mathematical optimization;query language;query optimization;relevance;stationary process;temporal logic;torsion (gastropod);velocity (software development);yet another	Jianwen Su;Haiyan Xu;Oscar H. Ibarra	2001		10.1007/3-540-47724-1_1	differential geometry;query optimization;object-based spatial database;temporal logic;data model;computer science;artificial intelligence;object-oriented design;database;mathematics;algorithm;query language	DB	-29.236992341775792	8.647872089231647	32001
25852c0f561492c3fecfdc14d259102c7a8217be	minimal models vs. logic programming: the case of counterfactual conditionals	minimal models;conditional sentences;logic programming;non monotonic reasoning	Disclaimer/Complaints regulations If you believe that digital publication of certain material infringes any of your rights or (privacy) interests, please let the Library know, stating your reasons. In case of a legitimate complaint, the Library will make the material inaccessible and/or remove it from the website. Please Ask the Library: http://uba.uva.nl/en/contact, or a letter to: Library of the University of Amsterdam, Secretariat, Singel 425, 1012 WP Amsterdam, The Netherlands. You will be contacted as soon as possible.	counterfactual conditional;logic programming;privacy	Katrin Schulz	2014	Journal of Applied Non-Classical Logics	10.1080/11663081.2014.911537	discrete mathematics;horn clause;computer science;artificial intelligence;non-monotonic logic;mathematics;minimal logic;psychology of reasoning;inductive programming;programming language;deductive reasoning;prolog;logic programming;algorithm	Web+IR	-11.034824358428944	8.787835280853	32003
c7330aa9c00d6a632ce48047a6eeeca2dd35aa1c	a constraint solver based on abstract domains		In this article, we apply techniques from Abstract Interpretation (a general theory of semantic abstractions) to Constraint Programming (which aims at solving hard combinatorial problems with a generic framework based on first-order logics). We highlight some links and differences between these fields: both compute fixpoints by iteration but employ different extrapolation and refinement strategies; moreover, consistencies in Constraint Programming can be mapped to non-relational abstract domains. We then use these correspondences to build an abstract constraint solver that leverages abstract interpretation techniques (such as relational domains) to go beyond classic solvers. We present encouraging experimental results obtained with our prototype implementation.	abstract interpretation;artificial intelligence;constraint programming;curve fitting;disjunctive normal form;extrapolation;first-order predicate;fixed point (mathematics);iteration;iterative method;propagator;prototype;refinement (computing);software propagation;solver;static program analysis	Marie Pelleau;Antoine Miné;Charlotte Truchet;Frédéric Benhamou	2013		10.1007/978-3-642-35873-9_26	constraint logic programming;mathematical optimization;constraint programming;theoretical computer science;algorithm	AI	-15.664145644827364	23.57624168055745	32007
1e9faffe60e21931683703f7d3c70fc65d5f8828	implementing multilevel queries in a database environment for vision research	databases;relational data model;networks;interfaces;data integrity;image processing;data processing;scientific database;data definition language;data modeling;dynamic data;design and implementation;safety;hyper rayleigh scattering;data access;modeling;vision;type safety	The database environment for vision research (DEVR) is an entity-oriented scientific database system based on a hierarchical relational data model (HRS). This paper describes the design and implementation of the data definition language, the application programmer's interface, and the query mechanism of the DEVR system. DEVR provides a dynamic data definition language for modeling image and vision data, which can be integrated with existing image processing and vision applications. Schema definitions can be fully interleaved with data manipulation, without requiring recompilation. In addition, DEVR provides a powerful application programmer's interface that regulates data access and schema definition, maintains indexes, and enforces type safety and data integrity. The system supports multi-level queries based on recursive constraint trees. A set of HRS entities of a given type is filtered through a network of constraints corresponding to the parts, properties, and relations of that type. Queries can be constructed interactively with a menu-drive interface, or they can be dynamically generated within a vision application using the programmer's interface. Query objects are persistent and reusable. Users may keep libraries of query templates, which can be built incrementally, tested separately, cloned, and linked together to form more complex queries.© (1995) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.		Rex M. Jakobovits;Linda G. Shapiro;Steven L. Tanimoto	1995		10.1117/12.205321	data access;data modeling;vision;data definition language;relational model;dynamic data;systems modeling;data processing;image processing;type safety;computer science;interface;data integrity;data mining;database;world wide web	DB	-31.57090765492954	9.926341636567756	32023
52f69c3d2a2362f00c2608170aefa0b2b6ac9a86	encoding probabilistic causal model in probabilistic action language	action language;reasoning about action;causal models	Pearl’s probabilistic causal model has been used in many domains to reason about causality. Pearl’s treatment of actions is very different from the way actions are represented explicitly in action languages. In this paper we show how to encode Pearl’s probabilistic causal model in the action language PAL thus relating this two distinct approaches to reasoning about actions. Introduction and motivation Normally an action when executed in a world changes the state of the world. Reasoning about actions is important in several ‘intelligent’ tasks such as planning, hypothetical reasoning, control generation and verification (for dynamical systems), and diagnosis. Often the effect of an action on the world is not deterministic but rather has an uncertainty associated with it. In recent years there have been several approaches to represent and reason with such actions. The first type of approaches include probabilistic generalization of formalisms for reasoning about actions; for example, planning (Kushmerick, Hanks, & Weld 1995; Littman 1997), situation calculus (Bacchus, Halpern, & Levesque 1999; Poole 1998; Reiter 2001; Mateus, Pacheco, & Pinto 2002), and action languages (Baral, Tran, & Le 2002; Eiter & Lukasiewicz 2003). The second type of approaches are based on frameworks of reasoning under uncertainty such as independence choice logic (Poole 1997; 1998) and probabilistic causal model (Pearl 1995; 1999; 2000). In all these proposals, except in Pearl (1999; 2000), actions are explicitly defined (with names) and their effects on the world are described by various means. In Pearl (1999; 2000) the dynamics of the world is described through relationships between variables (which denote properties of objects in the world) that are expressed through functional relationships between them. Furthermore, probabilities are associated with a subset of variables called background (or exogenous) variables. Together they are referred to as probabilistic causal models(PCMs). The effect of actions are then formulated as “local surgery”on these models (Pearl 1995). Copyright c © 2004, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. In this paper our goal is to study the relationship between reasoning about actions in PCMs and similar reasoning in the action description language PAL (Baral, Tran, & Le 2002) as a representative of the approaches where actions are named and have effects associated with it. The motivation behind studying this relationship is to objectively compare the expressiveness of these two formalisms vis-a-vis each other. We select PCM to study as it is the most successful representative of causal reasoning formalisms (Pearl 2000). We pick PAL as a representative of the high level action description languages (Gelfond & Lifschitz 1993; McCain & Turner 1995; Gelfond & Lifschitz 1998), for this is the most similar to PCM in the way it handles uncertainty among the action languages that have actions named distinctly. Moreover, PAL is inspired by PCM1, so it is natural to question how they relate to each other. In fact, the formal analysis of this relation has provided us a better understanding of the two frameworks in term of their advantages and limitations. The rest of this paper is organized as follows. First, we briefly recall the language PAL and PCM. We present an encoding of PCM in PAL together with a correctness result. We also provide the intuition of the encoding as well as of the result and its proof through a detailed example. Finally, we conclude with discussion on related and future works. The language PAL: a brief overview The alphabet of the language PAL (Baral, Tran, & Le 2002) consists of four non-empty disjoint sets F, UI , UN , A. The sets respectively contain fluents, inertial unknown variables, non-inertialunknown variables and actions. Intuitively, both fluents and unknown variables encode properties of the world. An action can have effects on the former but not on the latter. Moreover, inertial variables are unchanged through courses of actions. Unknown variables are assumed to be independent of each other. A fluent literal is a fluent or a fluent preceded by ¬. An unknown variable literal is an unknown variable or an unknown variable preceded by ¬. A literal is either a fluent literal or an unknown variable literal. A formula is a propositional formula constructed from Since the proposal of PAL in Baral, Tran, & Le (2002), other action languages have also been extended to allow actions with probabilistic effects; for example see Eiter & Lukasiewicz (2003).	action language;artificial intelligence;automated planning and scheduling;causal model;causality;correctness (computer science);dynamical system;encode;expressive power (computer science);fluent (artificial intelligence);hector levesque;high-level programming language;literal (mathematical logic);pal;patrick hanks;situation calculus;the dan le batard show with stugotz;toby turner;user interface;vladimir lifschitz;łukasiewicz logic	Tran Hoai Nam;Chitta Baral	2004			natural language processing;action language;computer science;artificial intelligence;machine learning;causal model	AI	-15.569269827304455	5.974456939365605	32041
161c6c92eb885db3ef0019bb4d45cc06b18b0f50	the saga of synchronous bus arbiter: on model checking quantitative timing properties of synchronous programs	synchronous programming;discrete time;interval temporal logic;synchronous system;duration calculus;model checking;decision procedure;finite state automaton	Quantified Discrete-time Duration Calculus, (QDDC), is a form of interval temporal logic [14]. It is well suited to specify quantitative timing properties of synchronous systems. An automata theoretic decision procedure for QDDC allows converting a QDDC formula into a finite state automaton recognising precisely the models of the formula. The automaton can be used as a synchronous observer for model checking the property of a synchronous program. This theory has been implemented into a tool called DCVALID which permits model checking QDDC properties of synchronous programs written in Esterel, Verilog and SMV notations. In this paper, we consider two well-known synchronous bus arbiter circuits (programs) from the literature. We specify some complex quantitative properties of these arbiters, including their response time and loss time, using QDDC. We show how the tool DCVALID can be used to effectively model check these properties (with some surprising results).	arbiter (electronics);automaton;bus (computing);bus mastering;decision problem;duration calculus;esterel;finite-state machine;interval temporal logic;model checking;response time (technology);verilog	Paritosh K. Pandya	2002	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(05)80445-1	model checking;duration calculus;discrete time and continuous time;real-time computing;interval temporal logic;computer science;theoretical computer science;finite-state machine;programming language;algorithm	Logic	-11.215340952320942	26.014710548628788	32075
309cab0dd1b12a648914e7662eb2e37ae4ad7b50	foundations of equational deduction: a categorical treatment of equational proofs and unification algorithms	unification algorithms;categorical treatment;equational deduction;equational proofs	We provide a framework for equational deduction based on category theory. Firstly, drawing upon categorical logic, we show how the compositional structure of equational deduction is captured by a 2-category. Using this formulation, algorithms for solving equations are derived from general constructions in category theory. The basic unification algorithm arises from constructions of colimits. We also consider solving equations in the presence of term rewriting systems and the combination of unification algorithms.	algorithm;han unification;natural deduction	David E. Rydeheard;John G. Stell	1987		10.1007/3-540-18508-9_23	discrete mathematics;mathematics;algorithm	Logic	-14.756113303276692	16.37315291587253	32139
815c67f6e5609d403e1d019fbe71866bb11d1080	the gnat compilation model	hybrid system;configuration management	One of the novel features of GNAT is its unusual approach to the compilation process and the handling of the Ada library. The words novel and unusual only apply from a traditional Ada compilation perspective. By contrast, a typical C or C++ programmer would find many aspects of the model quite familiar. In GNAT, sources are independently compiled to produce a set of objects, and the set of object files thus produced is submitted to the binder/linker to generate the resulting executable. This approach removes all order-of-compilation considerations, and eliminates the traditional monolithic library structure. Not only is the model very simple to understand, but it makes it easier to build hybrid systems in multiple languages, and is much more compatible with conventional configuration management tools (ranging from the simple Unix MAKE program to sophisticated compilation management environments) than the conventional library structure. Needless to say, the approach we present is fully compatible with the Ada rules of compilation.	ada;c++;centralized computing;compiler;configuration management;executable;file binder;gnat;hybrid system;make;object file;programmer;retransmission (data networks);tanenbaum–torvalds debate;unix	Robert Dewar	1994		10.1145/197694.197708	single compilation unit;real-time computing;computer science;database;programming language	PL	-25.153835235369776	27.75533950655403	32169
0416681fc4dbba3ae490d5369f2de9c522240b09	a generalization of a contraction principle in probabilistic metric spaces. part ii	contraction principle;probabilistic metric space	1.1. t-norms. A triangular norm (shortly t-norm) is a binary operationT : [0,1]×[0,1]→ [0,1] := I which is commutative, associative, monotone in each place, and has 1 as the unit element. Basic examples are TL : I × I → I , TL(a,b) =Max(a+ b− 1,0) (Łukasiewicz t-norm), TP(a,b) = ab, and TM(a,b) = Min{a,b}. We also mention the following families of tnorms: (i) Sugeno-Weber family (T λ )λ∈(−1,∞), defined by T SW λ =max(0,(x + y− 1 + λxy)/ (1 + λ)), (ii) Domby family (T λ )λ∈(0,∞), defined by T D λ = (1 + (((1 − x)/x)λ + ((1 − y)/ y)λ)1/λ)−1, (iii) Aczel-Alsina family (T λ )λ∈(0,∞), defined by T AA λ = e−(|logx|+|log y|λ)1/λ .	emoticon;shattered world;t-norm;transform, clipping, and lighting;monotone	Dorel Mihet	2005	Int. J. Math. Mathematical Sciences	10.1155/IJMMS.2005.729	contraction mapping;mathematical analysis;discrete mathematics;topology;mathematics	Theory	-7.317840294671418	11.926679295827915	32233
08ae1689c2b4ab38cef74b92db83b8f9f182f791	an improved deadlock prevention strategy for fmss using theory of regions	fms;petri nets;deadlock prevention;the theory of regions;control systems;automatic control;mathematical model;optimal control;law;petri net	The theory of regions has been recognized as the optimal deadlock prevention policy based on the marking/transition-separation instance (MTSI) or the event-state-separation-problem (ESSP) method for obtaining a maximally permissive controller in existing literature. All legal and live maximal behavior of the Petri nets model can be held by using above methods. However, its major drawback is all MTSIs (ESSPs) are required to indentify for solving the deadlock problems. For the reason, the crucial marking/transition-separation instance (CMTSI) is proposed in this paper to allow designers to deal with system deadlocks using few MTSIs. The advantage of the proposed methodology is that the computational cost can be reduced due to few MTSIs involved in. Importantly, the paper proves and demonstrates that our methodology is utilities.	algorithmic efficiency;computation;deadlock;earth system science partnership;item unique identification;maximal set;petri net;reachability	Yi-Sheng Huang;Yen-Liang Pan	2010	The 3rd International Conference on Information Sciences and Interaction Sciences		control engineering;computer science;operations management;distributed computing	Robotics	-6.704930611959573	29.2669202334612	32252
745335b3b28b7de8cbfeb962fc744f19a8af4a4d	managing the data in electronic learning systems	electronic learning;system modeling data management web based electronic learning systems data intensive systems data modeling e learning systems global database level;system modeling;model system;database management systems;computer aided instruction;e learning system;data model;database management systems internet computer aided instruction data models;internet;electronic learning databases information systems guidelines artificial intelligence management information systems navigation content management xml resource description framework;database design;data models	Web-based electronic learning systems provide one of the most exciting areas in research with a huge practical impact. However, as these systems are also examples of data-intensive systems, it is astounding how little emphasis is put onto the management of data in these systems. In this article, we present a two-level approach to data modeling in e-learning systems. We suggest modeling a global database level, in which the collection of data of the whole system is modeled in a way that follows the guidelines of good database design. On top of this level we model learning units as extended views. The use of views allows a system modeler to separate course content from underlying database structures. The extension of the learning units by operations models system functionality. Further extensions permit adaptivity to various learner types and end-devices.	data mining;data modeling;data-intensive computing;database design;database storage structures;worm's-eye view	Oleg Rostanin;Klaus-Dieter Schewe;Bernhard Thalheim;Alexei Tretiakov	2004	IEEE International Conference on Advanced Learning Technologies, 2004. Proceedings.	10.1109/ICALT.2004.1357444	data modeling;database theory;the internet;systems modeling;intelligent database;system of systems;data model;computer science;knowledge management;three schema approach;data administration;database model;machine learning;data mining;database;world wide web;database schema;information system;database testing;database design;systems design	DB	-32.86465153944506	12.299295999890656	32258
32a3063b4359c88a94dc3c7f3420305caad39106	a synthesis method for decentralized supervisors for timed discrete event systems	supervisory control;decentralized control			Masashi Nomura;Shigemasa Takai	2013	IEICE Transactions		real-time computing;decentralised system;computer science;control theory;supervisory control	Robotics	-5.9754466043206	29.038035470652037	32315
90c95ae3099f227f74d5f847b1fcef29bc42200b	expressive power of entity-linking frameworks		We develop a unifying approach to declarative entity linking by introducing the notion of an entity linking framework and an accompanying notion of the certain links in such a framework. In an entity linking framework, logic-based constraints are used to express properties of the desired link relations in terms of source relations and, possibly, in terms of other link relations. The definition of the certain links in such a framework makes use of weighted repairs and consistent answers in inconsistent databases. We demonstrate the modeling capabilities of this approach by showing that numerous concrete entity linking scenarios can be cast as such entity linking frameworks for suitable choices of constraints and weights. By using the certain links as a measure of expressive power, we investigate the relative expressive power of several entity linking frameworks and obtain sharp comparisons. In particular, we show that we gain expressive power if we allow constraints that capture non-recursive collective entity resolution, where link relations may depend on other link relations (and not just on source relations). Moreover, we show that an increase in expressive power also takes place when we allow constraints that incorporate preferences as an additional mechanism for expressing goodness of links.		Douglas Burdick;Ronald Fagin;Phokion G. Kolaitis;Lucian Popa;Wang Chiew Tan	2019	J. Comput. Syst. Sci.	10.1016/j.jcss.2018.09.001	expressive power;discrete mathematics;entity linking;machine learning;mathematics;artificial intelligence	Theory	-17.509651252166236	7.448892253178498	32317
b15de5b7d5109021e4a6256785b5aa6fa2fabc19	maximality and refutability	paraconsistency;syntactic refutability;maximal propositional logics;inference systems;propositional logic;disjunction property;paraconsistent logic			Tom Skura	2004	Notre Dame Journal of Formal Logic	10.1305/ndjfl/1095386644	zeroth-order logic;t-norm fuzzy logics;discrete mathematics;classical logic;paraconsistent logic;epistemology;intermediate logic;disjunction introduction;mathematics;linguistics;propositional variable;well-formed formula;algorithm;autoepistemic logic	EDA	-12.300771433396687	12.38705961938731	32324
c3a89c3cba0ec516362850f3e1f6db7be3c7a9f3	query optimisation in distributed object-oriented database systems	dynamic programming;distributed system;complexite;database system;optimisation;programacion dinamica;systeme reparti;optimizacion;information transmission;sistema informatico;complejidad;query formulation;formulacion pregunta;computer system;complexity;formulation question;minimizacion costo;distributed objects;sistema repartido;minimisation cout;cost minimization;object oriented;programmation dynamique;oriente objet;optimization;systeme informatique;transmision informacion;systeme gestion base donnee;information system;transmission information;sistema gestion base datos;orientado objeto;database management system;systeme information;sistema informacion	In this paper, query processing and optimisation in distributed object-oriented database systems are discussed. The processing and optimisation of typical queries, called chain queries, in distributed object-oriented database systems are investigated in detail. An algorithm with complexity of O(n 3 * h,) to minimise the total cost is provided using dynamic programming, where n is the number of classes referenced in the query, h, ^ min (n + 2,h) and h is the number of sites in the network. A wide range of diversified issues are addressed and uniformly integrated into our basic solution to the problem. These issues include sorted states of classes; local processing of selections and projections, allowing multiple intermediate results; arbitrary target class at an arbitrary answer site; replicated data; class hierarchies (which captures the IS-A relationship among objects) ; different sites with different processing speeds, and communication lines between different sites with different transfer speeds. The uniformity of this algorithm under so many diversified situations strongly demonstrates the usefulness and the flexibility of the algorithm.	distributed object;mathematical optimization;query optimization	Wei Sun;Weiyi Meng;Clement T. Yu	1992	Comput. J.	10.1093/comjnl/35.2.98	complexity;computer science;artificial intelligence;theoretical computer science;dynamic programming;database;distributed object;object-oriented programming;information system;algorithm	DB	-26.73034517121763	4.96355980778856	32335
e94665aceef342dd615bba5252c2d3b7784e323c	software correctness in object oriented languages: key concepts and comparison	object oriented language			Lady Baquero;Jose Garrido	2010			method;object definition language;theoretical computer science;correctness;object model;software;programming language;object-oriented programming;object-based language;computer science;object code	DB	-25.81643702682222	27.97135318351507	32339
1508d703d3d456eb712276afbd5c752a49df56a5	graded computation tree logic	specification;branching temporal logics;satisfiability;graded modalities	In modal logics, graded (world) modalities have been deeply investigated as a useful framework for generalizing standard existential and universal modalities in such a way that they can express statements about a given number of immediately accessible worlds. These modalities have been recently investigated with respect to the μCalculus, which have provided succinctness, without affecting the satisfiability of the extended logic, that is, it remains solvable in ExpTime. A natural question that arises is how logics that allow reasoning about paths could be affected by considering graded path modalities. In this article, we investigate this question in the case of the branching-time temporal logic CTL (GCTL, for short). We prove that, although GCTL is more expressive than CTL, the satisfiability problem for GCTL remains solvable in ExpTime, even in the case that the graded numbers are coded in binary. This result is obtained by exploiting an automata-theoretic approach, which involves a model of alternating automata with satellites. The satisfiability result turns out to be even more interesting as we show that GCTL is at least exponentially more succinct than graded μCalculus.	2-exptime;algorithm;alternating finite automaton;automata theory;automated planning and scheduling;binary tree;boolean satisfiability problem;ctl*;computation tree logic;decision problem;exptime;emoticon;empty domain;empty string;formal system;internet information services;kripke structure (model checking);like button;maximal set;method of analytic tableaux;modal logic;modality (human–computer interaction);node (computer science);ordinal data;temporal logic;time complexity;turing completeness;x window system	Alessandro Bianco;Fabio Mogavero;Aniello Murano	2009	2009 24th Annual IEEE Symposium on Logic In Computer Science	10.1145/2287718.2287725	combinatorics;discrete mathematics;artificial intelligence;mathematics;specification;algorithm;satisfiability	Logic	-8.382885020890873	17.608993254561263	32448
a4e3dd11368104f32697d19480a04713c37f659f	thesis: function variables for constraint programming	high level modelling;abstraction;function variables;constraint satisfaction;data och systemvetenskap;constraint programming;datavetenskap datalogi;reformulation;computer science	We introduce function variables to constraint programs (CP), variables whose values are one of (exponentially many) possible functions between two sets. Such variables are useful for modelling problems from domains such as configuration, planning, scheduling, etc. We show that a function variable can be mapped into different representations in terms of integer and set variables, and illustrate how to map constraints stated on a function variable into constraints on integer and set variables. As a result, a constraint model expressed using function variables allows for the generation of alternate CP models. Furthermore, we present an extensive theoretical comparison of models of problems involving injective functions supported by asymptotic and empirical studies. Finally, we present and evaluate a practical modelling tool that is based on a high-level language that supports function variables. The tool helps users explore different alternate CP models starting from a function model that is easy to develop, understand, and maintain.	constraint programming	Brahim Hnich	2003	AI Commun.		constraint logic programming;mathematical optimization;constraint programming;binary constraint;constraint satisfaction;computer science;artificial intelligence;local variable;machine learning;abstraction;constraint;algorithm	AI	-20.058605817524626	16.021739542726092	32475
5b08b17c15c60caf9ff3e5ece2a309dfe62d4122	specifying operations for nested relations by rules and partial orders	minimal model;rule based;expressive power;first order;stratification;partitioned normal form;partial order	In this paper rule-based languages over partially ordered nested relations are considered. Starting with ideas of Bancilhons and Koshafians Object Calculus we define a class of purely lattice-based languages, each of them depending on a fixed partial order. We show that for each ordering the semantics of a program can be equivalently defined by minimal model and least fixpoint semantics. Thus our approach is semantically first order. Two well-known orderings — inclusion order and object order — and the corresponding languages are compared in detail. In order to combine the advantages of these two orderings we present a formalism to define more general orderings over so-called Generalized Partitioned Normal Form instances. By examples we demonstrate the expressive power of this approach. Finally, we define the meta-concept modularization that enables us to express operations being non-monotone with respect to each possible partial order. This concept is comparable to the concept of stratification in other approaches, but the basic idea is different.		Peter Sander	1991		10.1007/3-540-54009-1_4	combinatorics;discrete mathematics;mathematics;algorithm	DB	-16.815826611139393	13.411649554441187	32501
9c6011a355870cbb22c47b830f2fb9f512b98693	a formal definition of hierarchical predicate transition nets	hierarchical structure;real world application;formal reasoning;reactive system	Hierarchical predicate transition nets have recently been introduced as a visual formalism for specifying complex reactive systems. They extend predicate transition nets with hierarchical structures so that large systems can be specified and understood stepwisely, and thus are more suitable for real-world applications. In this paper, we provide a formal syntax and an algebraic semantics for hierarchical predicate transition nets, which establish the theory of hierarchical predicate transition nets for precise specification and formal reasoning.	algebraic semantics (computer science);formal grammar;linear algebra;petri net;state transition table	Xudong He	1996		10.1007/3-540-61363-3_12	discrete mathematics;reactive system;computer science;theoretical computer science;mathematics;algorithm	AI	-12.017901338682115	22.814560343829694	32503
179c9beae3973f6d33353c322fc91aff5b7b548a	refinement of intentions		The aim of this paper is to provide a logical analysis of intention refinement process which plays a fundamental role in the belief-desire-intention (BDI) theory. We briefly show the existing results: a logical framework for intention refinement and the extension of hierarchical task network (HTN) planning to capture high-level intentions. We also present two ongoing directions: extending our logical framework with hierarchical decomposition and revision of intentions based on instrumentality. 1 Background and Motivations The mental attitudes of belief, desire, and intention play a central role in the design and implementation of autonomous agents. In 1987 [Bratman] proposed their integration into a belief-desire-intention (BDI) theory that was seminal in AI. Since then numerous approaches were built on the BDI paradigm, both practical (BDI architectures and BDI agents) and formal (BDI logics). The logical approaches that were the most influential are due to [Cohen and Levesque, 1990] and to [Rao and Georgeff, 1991]. However, the practical and logical approaches evolved separately and neither was fruitful for the other. Bratman highlighted the fundamental role of an agent’s future-directed intentions: they are high-level plans to which the agent is committed. Such high-level plans cannot be executed directly: they have to be refined, resulting in more and more elaborate plans. Taking the example of going to Melbourne, Figure 1 shows the process of refinement. The lower-level intentions that are inserted are instrumental for the high-level intention they refine [Bratman, 2009]. At the end of the refinement process plans only contain basic actions: actions the agent can perform intentionally. The operations of refinement of intentions are fundamental in the BDI model. As pointed out in [Rao and Georgeff, 1991], “the potential of non-primitive events for decomposition into primitive events can be used to model hierarchical plan development”. However, the instrumentality relation between higher-level and lower-level intentions is not accounted for in the existing BDI logics, including Cohen&Leveques’s and Rao&Georgeff’s BDI theories and those inspired by their theories, where intentions are viewed as a basic attitude and in Figure 1: (a) The initial intention of the agent is to go to Melbourne; (b) the initial intention is refined by a lower-level intention to go to Melbourne by plane; (c) the intention to fly to Melbourne is further refined into two intentions: to go to the airport and to take the plane. consequence there is no appropriate account of intention refinement. The aim of this research project is to provide a logical analysis of this hierarchical refinement process.	autonomous agent;autonomous robot;belief–desire–intention software model;emoticon;goto;hector levesque;hierarchical task network;high- and low-level;linear belief function;logical framework;programming paradigm;refinement (computing);theory	Zhanhao Xiao	2017		10.24963/ijcai.2017/771	machine learning;artificial intelligence;computer science	AI	-18.52685523573275	4.424572986268968	32506
469f7fbf1fe274c90b7e0e875788882c2d9476cc	designing a user-oriented query modification facility in object-oriented database systems	database system;object oriented data model;query processing;data model;object oriented systems;object oriented database systems	The introduction of user-assisting features into database systems is discussed along two stages. The first stage involves a basic facility that can be used with standard database systems, whereas in the second stage such features are expanded in order to cope with object-oriented systems, adopting semantically richer data models. Examples involving categorization and role-specialization semantic hierarchies illustrate the discussion. A class/metaclass architecture, such as that of the VODAK database system, an algebraic view of query processing and an extension of the object-oriented data model by rule systems are shown to be particularly suitable to design and implement user assistance on the database schema level.	categorization;data model;database schema;metaclass;partial template specialization;relational database management system	Karl Aberer;Wolfgang Klas;Antonio L. Furtado	1994		10.1007/3-540-58113-8_184	materialized view;online aggregation;query optimization;database theory;database server;intelligent database;entity–relationship model;database tuning;data model;computer science;data administration;database model;machine learning;data mining;database;distributed computing;programming language;view;database schema;physical data model;information retrieval;alias;database testing;database design	DB	-30.88206439930487	10.49805698948487	32518
18e9ed47e46d0086c53afa0410040535204d61a3	solving constrained horn clauses using syntax and data		A Constrained Horn Clause (CHC) is a logical implication involving unknown predicates. Systems of CHCs are widely used to verify programs with arbitrary loop structures: interpretations of unknown predicates, which make every CHC in the system true, represent the program’s inductive invariants. In order to find such solutions, we propose an algorithm based on Syntax-Guided Synthesis. For each unknown predicate, it generates a formal grammar from all relevant parts of the CHC system (i.e., using syntax). Grammars are further enriched by predicates and constants guessed from models of various unrollings of the CHC system (i.e., using data). We propose an iterative approach to guess and check candidates for multiple unknown predicates. At each iteration, only a candidate for one unknown predicate is sampled from its grammar, but then it gets propagated to candidates of the remaining unknowns through implications in the CHC system. Finally, an SMT solver is used to decide if the system of candidates contributes towards a solution or not. We present an evaluation of the algorithm on a range of benchmarks originating from program verification tasks and show that it is competitive with state-of-the-art in CHC solving.		Grigory Fedyukovich;Sumanth Prabhu;Kumar Madhukar;Aarti Gupta	2018	2018 Formal Methods in Computer Aided Design (FMCAD)	10.23919/FMCAD.2018.8603011	predicate (grammar);theoretical computer science;horn clause;computer science;satisfiability modulo theories;syntax;logical consequence;data modeling;rule-based machine translation;formal grammar	Logic	-15.389398906124299	23.357988940638478	32519
ae004abfb7661f3ae7e8a46414f104f251460ae4	an intuitionistic predicate logic theorem prover	theorem prover	"""A complete theorem prover for intuitionistic predicate logic based on the cut-free calculus is presented. It includes a treatment of """"quasi-free"""" identity based on a delay mechanism and a special form of unification. Several important optimizations of the basic algorithm are introduced. The resulting system is available in source form from SICS; an Appendix gives some idea of its performance."""	automated theorem proving	Dan Sahlin;Torkel Franzén;Seif Haridi	1992	J. Log. Comput.	10.1093/logcom/2.5.619	discrete mathematics;computer science;artificial intelligence;fundamental theorem;mathematics;automated theorem proving;programming language;algorithm	Logic	-12.130541623132228	14.108995348660509	32561
7d7eadbfabacdf544f4393194ad2b91ba1e84477	solving the ttc 2011 reengineering case with gretl		The from-with-reportSet expression calculates the set of classes which are named “State”. The function theElement() extracts the single element of a collection consisting of only one element and throws an exception if the collection’s size is not one. This expresses the assumption that there is exactly one state class. Finally, this class is assigned to the variable abstractStateClass. The first transformation operation invoked is CreateVertexClass. It creates a new vertex class State in the target schema. The query following the arrow symbol is evaluated on the source graph and has to result in a set. For each member of this set (archetype), a new vertex (image) of the just created type is instantiated in the target graph. The mappings from archetypes to target graph images are automatically saved in a function corresponding to the target metamodel vertex class (imgState). 5 C r e a t e V e r t e x C l a s s Sta t e 6 <== from c : { C l a s s } & (<>−−{ex t end s } <>−−{ c l a s s i f i e r R e f e r e n c e s } −−>{ t a r g e t })+ 7 a b s t r a c t S t a t e C l a s s 8 with i sEmpty ( c <>−−{anno t a t i o n sAndMod i f i e r s } & {Abs t r a c t }) 9 reportSet c end ; The query specifies a set of Class vertices. The variable c iterates over Class vertices, for which a path to the vertex abstractStateClass exists. The structure of this path is specified using a regular path expression [1]. First, a containment edge with role name extends at the far end has to be traversed, followed by another containment edge with role name classifierReferences, followed by a forward edge with role name target. This is exactly how subclasses relate to their superclass. The + specifies a one-ormany iteration. Thus, c is bound not only to direct subclasses of abstractStateClass, but also to indirect ones. The with part ensures that c is not abstract, i.e., it must not reference an Abstract vertex using an edge with containment semantics and far end role name annotationsAndModifiers. For any non-abstract class that extends the abstract state class either directly or indirectly, a new target graph State vertex is created. The mappings from classes to states are stored in a function img State, which can be used in following operation calls for navigating between archetypes and images. The next operation creates the name attribute of type String for the State vertex class, and it sets the attribute values for the vertices created by the last operation call. 10 C r e a t e A t t r i b u t e Sta t e . name : S t r i n g 11 <== from c : keySet ( img Sta t e ) reportMap c −> c . name end ; The query of the CreateAttribute operation has to result in a map assigning values of the attribute’s type to archetyps. Here, the map assigns class names to State archetypes, so the state names are set to the names of the classes they were created for. The CreateEdgeClass operation is used to create a new edge type Transition in the target metamodel defined between State vertices with the given role names and default multiplicities (0,*). The query has to result in a set of triples. In each triple, the first component specifies the archetype of the new edge to be created. The second and third component specify the archetypes of the start and end vertices. For each archetype, a new edge of the just created type is created in the target graph, starting at the vertex that is the image of the second component and ending at the vertex that is image of the third component. 12 CreateEdgeClass T r a n s i t i o n from Sta t e r o l e s r c to Sta t e r o l e ds t	abstract type;arrow (symbol);chorded keyboard;code refactoring;common criteria;correctness (computer science);depth-first search;graph state;iteration;metamodeling;path expression;program comprehension;query language;regular expression;relevance;traceability;vertex (graph theory);gretl	Tassilo Horn	2011		10.4204/EPTCS.74.12	algorithm	AI	-29.430118372184168	29.722961551163145	32565
1b556ecc5bac3faafb4ec208bdadbc39dbe0d2a3	first-order logic and first-order functions		This paper begins the study of first-order functions, which are a generalization of truth-functions. The concepts of truth-table and systems (and clones) of truth-functions, both introduced in propositional logic by Post, are also generalized and studied in the quantificational setting. The general facts about these concepts are given in the first five sections, and constitute a “general theory” of first-order functions. The central theme of this paper is the relation of definition among notions expressed by formulas of first-order logic. We emphasize that logic is not concerned only with the consequence relation among notions expressed by formulas. It also attends to the relation of definition among notions, where a notion is defined from other notions. Sections 5 and 6 deal exclusively with the relation of definition among notions expressed by formulas of first-order logic. In these sections, we study the systems of first-order functions, which are the sets of first-order functions closed under definitions. Sections 7 and 8 are concerned with the relativization of first-order functions to a class of structures. The relativization to a class of structures is a fundamental operation which is used in order to relate the theory of firstorder functions with set theory and first-order model theory, a subject which we have barely scratched the surface. The apparatus developed in this paper enables us to define what is a vehicle for the foundation of classical mathematics in set theory, and, in Sect. 8, we prove that first-order logic with one binary predicate variable is not a minimal vehicle for the foundation of classical mathematics in set theory. Sections 9 and 10 introduce further operations and ideals of first-order functions. Besides some results on the influence of the arguments of a first-order function, a result about definability is proved in Sect. 10.1. It is this theorem that provides necessary and sufficient conditions for a first-order function to be in a finitely generated ideal. In Sect. 11, this result is applied to the problem of predicate definability in classes of structures, the problem with which Beth’s theorem dealt in the case of elementary classes. This paper was awarded the first Newton da Costa prize. This is a prize created in honour of the great Brazilian logician for promoting logic in Brazil see the details at http://www. uni-log.org/newton-da-costa-premio.html. R. A. Freire Log. Univers. Mathematics Subject Classification Primary 03B10; Secondary 03A99.	evert willem beth;first-order logic;first-order predicate;higher-order function;mathematics subject classification;newton;oracle machine;predicate variable;propositional calculus;set theory	Rodrigo de Alvarenga Freire	2015	Logica Universalis	10.1007/s11787-015-0126-8	discrete mathematics;topology;artificial intelligence;mathematics;predicate variable;predicate;algorithm;algebra	Logic	-8.993327718812909	11.769121493172925	32568
8e87689fc9d52f929aeab335f4b2ed070d73f593	learning conjunctions of horn clauses (extended abstract)	protocols;computer science polynomials inference algorithms protocols marine vehicles;equivalence queries;learning algorithm;polynomial time complexity;boolean functions;learning systems boolean functions;polynomials;learning systems;unknown formula;marine vehicles;time use;negated variable;horn clause conjunctions;inference algorithms;clauses learning algorithm horn clause conjunctions horn sentence polynomial time complexity boolean formulas disjunction of literals negated variable equivalence queries membership queries unknown formula;computer science;membership queries;boolean formulas;clauses;disjunction of literals;horn sentence	An algorithm for learning the class of Boolean formulas that are expressible as conjunctions of Horn clauses is presented. (A Horn clause is a disjunction of literals, all but at most one of which is a negated variable). The algorithm uses equivalence queries and membership queries to produce a formula that is logically equivalent to the unknown formula to be learned. The amount of time used by the algorithm is polynomial in the number of variables and the number of clauses in the unknown formula. >	horn clause	Dana Angluin;Michael Frazier;Leonard Pitt	1990		10.1109/FSCS.1990.89537	arithmetic;communications protocol;combinatorics;discrete mathematics;horn-satisfiability;unit propagation;computer science;mathematics;boolean function;algorithm;polynomial	Theory	-7.042136797526479	17.388865676710555	32572
b15752e84489e512f7a5d134f6f64f406e3e5629	incremental semantics for propositional texts	satisfiability	In this paper we are concerned with the special requirements that a semantics of texts should meet It is argued that a semantics of texts should be incremental and should satisfy the break in principle We develop a semantics for propositional texts that satis es these constraints We will see that our requirements do not only apply to the semantics but also have consequences for the syntax The interaction between text structure and text meaning will turn out to be of crucial importance to the semantics of texts We develop two versions of the semantics one representational one in update style	requirement	C. F. M. Vermeulen	1994	Notre Dame Journal of Formal Logic	10.1305/ndjfl/1094061863	propositional formula;mathematics;linguistics;propositional variable;well-formed formula;satisfiability	AI	-13.169385252475452	7.221192549429546	32580
e1ff7ab44117d7ac325b818fb326677f37e7fb79	on the context dependence of many		We augment the applicability of Lappin’s intensional parametrization of the determiners many and few by combinatorial means, and show how to arrive at graded interpretations of corresponding natural language statements.	game semantics;intensional logic;logical connective;natural language;predicate (mathematical logic);quantifier (logic);vagueness	Matthias F. J. Hofer	2015		10.1007/978-3-319-25591-0_25	natural language processing;artificial intelligence;augment;fuzzy logic;natural language;parametrization;vagueness;computer science	AI	-13.544703305645077	8.96289429954288	32581
0b560bfe89051bbd26e567d7b367b085230bb1da	higher category models of the pi-calculus		ABSTRACT We present an approach to modeling computational calculi using higher category theory. Specifically we present a fully abstract semantics for the π-calculus. The interpretation is consistent with Curry-Howard, interpreting terms as typed morphisms, while simultaneously providing an explicit interpretation of the rewrite rules of standard operational presentations as 2-morphisms. One of the key contributions, inspired by catalysis in chemical reactions, is a method of restricting the application of 2-morphisms interpreting rewrites to specific contexts.	category theory;curry;curry–howard correspondence;denotational semantics;rewrite (programming);rewriting;π-calculus	Mike Stay;Lucius Gregory Meredith	2015	CoRR		discrete mathematics;mathematics;algorithm	Logic	-11.96517315982609	19.508304792328474	32587
3b23dec8029fc1300f2734d40bd8f9f7a2138f76	compositional verification of termination-preserving refinement of concurrent programs	rely guarantee reasoning;simulation;termination preservation;refinement;concurrency	Many verification problems can be reduced to refinement verification. However, existing work on verifying refinement of concurrent programs either fails to prove the preservation of termination, allowing a diverging program to trivially refine any programs, or is difficult to apply in compositional thread-local reasoning. In this paper, we first propose a new simulation technique, which establishes termination-preserving refinement and is a congruence with respect to parallel composition. We then give a proof theory for the simulation, which is the first Hoare-style concurrent program logic supporting termination-preserving refinement proofs. We show two key applications of our logic, i.e., verifying linearizability and lock-freedom together for fine-grained concurrent objects, and verifying full correctness of optimizations of concurrent algorithms.	concurrent algorithm;concurrent computing;congruence of squares;correctness (computer science);formal verification;hoare logic;linearizability;refinement (computing);simulation;terminate (software);thread pool;thread-local storage;verification and validation	Hongjin Liang;Xinyu Feng;Zhong Shao	2014		10.1145/2603088.2603123	refinement calculus;concurrency;computer science;theoretical computer science;refinement;programming language;algorithm	Logic	-17.528025244760887	28.64303690625053	32610
ad204abbab93adc2f852c3f161f8299399b8c961	non-traditional squares of predication and quantification	logical squares.;quantification;denying;predication;. internal negation	Three logical squares of predication or quantification, which one can even extend to logical hexagons, will be presented and analyzed. All three squares are based on ideas of the non-traditional theory of predication developed by Sinowjew and Wessel. The authors also designed a non-traditional theory of quantification. It will be shown that this theory is superfluous, since it is based on an obscure difference between two kinds of quantification and one pays a high price for differentiating in this way: losing the definability between the existenceand all-quantifier. Therefore, a combination of nontraditional predication and classical quantification is preferred here. Mathematics Subject Classification (2000). Primary 03C80; Secondary 62A01.		Mireille Staschok	2008	Logica Universalis	10.1007/s11787-007-0029-4	artificial intelligence;mathematics;algorithm	PL	-10.111783376351879	6.551507860571532	32655
3067b55897d5c4af8b43fb553db8c36c709e441a	a hitchhiker's guide to reinventing a prolog machine		We take a fresh, “clean-room” look at implementing Prolog by deriving its translation to an executable representation and its execution algorithm from a simple Horn Clause meta-interpreter. The resulting design has some interesting properties. The heap representation of terms and the abstract machine instruction encodings are the same. No dedicated code area is used as the code is placed directly on the heap. Unification and indexing operations are orthogonal. Filtering of matching clauses happens without building new structures on the heap. Variables in function and predicate symbol positions are handled with no performance penalty. A simple Englishlike syntax is used as an intermediate representation for clauses and goals and the same simple syntax can be used by programmers directly as an alternative to classic Prolog syntax. Solutions of (multiple) logic engines are exposed as answer streams that can be combined through typical functional programming patterns, with flexibility to stop, resume, encapsulate and interleave executions. Performance of a basic interpreter implementing our design is within a factor of 2 of a highly optimized compiled WAM-based system using the same host language. To help placing our design on the fairly rich map of Prolog systems, we discuss similarities to existing Prolog abstract machines, with emphasis on separating necessary commonalities from arbitrary implementation choices. 1998 ACM Subject Classification D.3 Programming Languages, D.3.4 Processors	algorithm;cleanroom;compiler;executable;functional programming;han unification;horn clause;intermediate representation;machine code;predicate variable;programmer;prolog;warren abstract machine	Paul Tarau	2017		10.4230/OASIcs.ICLP.2017.10	programming language;computer science;prolog	PL	-22.74155940246754	24.406108854248433	32656
91554bae9912ee99dfe068f59093fd0c5eb655b8	synthesis of correct-by-construction behavior trees		In this paper we study the problem of synthesizing correct-by-construction Behavior Trees (BTs) controlling agents in adversarial environments. The proposed approach combines the modularity and reactivity of BTs with the formal guarantees of Linear Temporal Logic (LTL) methods. Given a set of admissible environment specifications, an agent model in form of a Finite Transition System and the desired task in form of an LTL formula, we synthesize a BT in polynomial time, that is guaranteed to correctly execute the desired task. To illustrate the approach, we present three examples of increasing complexity.	behavior tree;linear temporal logic;time complexity;transition system	Michele Colledanchise;Richard M. Murray;Petter Ögren	2017	2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2017.8206502	behavior trees;time complexity;linear temporal logic;control engineering;transition system;machine learning;computer science;modularity;artificial intelligence	Robotics	-12.409055342934261	24.88061543599957	32690
18e8c23497d13c27c1b4c44edd756d5892b32789	combining range and inequality information for pointer disambiguation	compiler construction;analyses statiques;plus petit que;compilateurs;static analysis;abstract interpretation;pentagons;less than check;interpretation abtraite;points to analysis	Pentagons is an abstract domain invented by Logozzo and Fahndrich to validate arrayrnaccesses in low-level programming languages. This algebraic structure provides a cheap “less-thanrncheck”, which builds a partial order between the integer variables used in a program. In this paper,rnwe show how we have used the ideas available in Pentagons to design and implement a novel aliasrnanalysis. This new algorithm lets us disambiguate pointers with offsets, so common in C-stylernpointer arithmetics, in a precise and efficient way. Together with this new abstract domain werndescribe several implementation decisions that lets us produce a practical pointer disambiguationrnalgorithm on top of the LLVM compiler. Our alias analysis is able to handle programs as large asrnSPEC’s gcc in a few minutes. Furthermore, we have been able to improve the percentage of pairsrnof pointers disambiguated, when compared to LLVM’s built-in analyses, by a four-fold factor inrnsome benchmarks.	algorithm;alias analysis;assembly language;gnu compiler collection;high- and low-level;llvm;linear algebra;low-level programming language;memory address;offset (computer science);pointer (computer programming);program optimization;social inequality;spec#;word-sense disambiguation	Maroua Maalej;Vitor Paisante;Fernando Magno Quintão Pereira;Laure Gonnord	2018	Sci. Comput. Program.	10.1016/j.scico.2017.10.014	theoretical computer science;pointer (computer programming);compiler;programming language;alias analysis;pointer analysis;abstract interpretation;spec#;compiler construction;escape analysis;computer science	PL	-17.806985029639687	31.827321066386478	32747
b52e5251c8881427f89afc49e7dea2c9e4af3d85	query processing in distributed pios	file servers;server;distributed query optimization query processing object oriented distributed database systems distributed pios server object oriented data model physical data independence fragmentation transparency;object oriented data model;query processing;fragmentation transparency;computer network;distributed query optimization;distributed database system;network servers;transaction databases;object oriented;data structures;spatial databases;database systems;distributed databases;query processing object oriented modeling network servers data models distributed databases object oriented databases transaction databases relational databases spatial databases database systems;physical data independence;relational databases;object oriented databases;distributed pios;object oriented distributed database systems;object oriented modeling;data models	An approach to query processing in objectoriented distributed database systems is proposed in this paper. Distributed PIOS is a server that supports an object-oriented data model, physical data independence (i.e. different strategies for storing class hierarchies, grouping, horizontal and vertical partitioning of objects), and fragmentation transparency (i.e. transactions are not aware of the distribution of database fragments on several nodes of a computer network). The problem of the optimization of distributed queries (i.e. determining which data must be accessed at which site and which data must be transmitted among sites) is the focus of the paper.	class hierarchy;data model;distributed database;fragmentation (computing);mathematical optimization;server (computing)	Fausto Rabitti;Leonardo Benedetti;Federico Demi	1996		10.1109/DEXA.1996.558596	data independence;data modeling;file server;data structure;relational database;computer science;data mining;database;distributed computing;object-oriented programming;distributed database;server	DB	-28.851479448664158	5.994358936595853	32757
1ea441d2386f9ab631645a4e1c5b7334d086499c	expressive and safe static reflection with morphj	animacion por computador;lenguaje programacion;image morphing;controle acces;language extensions;theorie type;data race;programming language;securite;acces concurrent;orientado aspecto;processus leger logiciel;abstraction;langage java;object oriented programming;program verification;abstraccion;morphage;proceso ligero logicial;verificacion programa;acceso simultaneo;real world application;type checking;object oriented;software transactional memory;aspect oriented programming;pattern matching;type theory;safety;thread software;metaprogrammation;langage programmation;language extension;oriente objet;lenguaje java;aspect oriented;access control;concordance forme;morphing de imagenes;computer animation;metaprogramming;verification programme;meta programming;seguridad;orientado objeto;metaprogramacion;languages;oriente aspect;structural abstraction;type system;class morphing;java language;animation par ordinateur	Recently, language extensions have been proposed for Java and C# to support pattern-based reflective declaration. These extensions introduce a disciplined form of meta-programming and aspect-oriented programming to mainstream languages: They allow members of a class (i.e., fields and methods) to be declared by statically iterating over and pattern-matching on members of other classes. Such techniques, however, have been unable to safely express simple, but common, idioms such as declaring getter and setter methods for fields.  In this paper, we present a mechanism that addresses the lack of expressiveness in past work without sacrificing safety. Our technique is based on the idea of nested patterns that elaborate the outer-most pattern with blocking or enabling conditions. We implemented this mechanism in a language, MorphJ. We demonstrate the expressiveness of MorphJ with real-world applications. In particular, the MorphJ reimplementation of DSTM2, a software transactional memory library, reduces 1,107 lines of Java reflection and bytecode engineering library calls to just 374 lines of MorphJ code. At the same time, the MorphJ solution is both high level and safer, as MorphJ can separately type check generic classes and catch errors early. We present and formalize the MorphJ type system, and offer a type-checking algorithm.	algorithm;algorithmic logic;aspect-oriented programming;blocking (computing);declaration (computer programming);expressive power (computer science);high- and low-level;high-level programming language;history of programming languages;java;metaprogramming;mutator method;pattern matching;software design pattern;software transactional memory;structured programming;type system;usability	Shan Shan Huang;Yannis Smaragdakis	2008		10.1145/1375581.1375592	metaprogramming;real-time computing;aspect-oriented programming;computer science;programming language;object-oriented programming;algorithm	PL	-23.917555266835333	29.250310012071647	32776
03e8336bc0cd3d5c75ca784e31ac4c1cde48c66d	a static analysis framework for livelock freedom in csp		In a process algebra with hiding and recursion it is possible to create processes which compute internally without ever communicating with their environment. Such processes are said to diverge or livelock. In this paper we show how it is possible to conservatively classify processes as livelock-free through a static analysis of their syntax. In particular, we present a collection of rules, based on the inductive structure of terms, which guarantee livelock-freedom of the denoted process. This gives rise to an algorithm which conservatively flags processes that can potentially livelock. We illustrate our approach by applying both BDD-based and SAT-based implementations of our algorithm to a range of benchmarks, and show that our technique in general substantially outperforms the model checker FDR whilst exhibiting a low rate of inconclusive results.	algorithm;deadlock;model checking;process calculus;recursion;static program analysis;while	Joël Ouaknine;Hristina Palikareva;A. W. Roscoe;James Worrell	2013	Logical Methods in Computer Science	10.2168/LMCS-9(3:24)2013	discrete mathematics;computer science;theoretical computer science;mathematics;programming language;algorithm	Logic	-15.194879159178868	21.97721832335409	32783
ca774bc43383cf3266024fb31c988c8b74711d30	proof theory for first order lukasiewicz logic	cut elimination;proof theory;first order	An approximate Herbrand theorem is proved and used to establish Skolemization for first-order Łukasiewicz logic. Proof systems are then defined in the framework of hypersequents. In particular, extending a hypersequent calculus for propositional Łukasiewicz logic with usual Gentzen quantifier rules gives a calculus that is complete with respect to interpretations in safe MV-algebras, but lacks cut-elimination. Adding an infinitary rule to the cut-free version of this calculus gives a system that is complete for the full logic. Finally, a cut-free calculus with finitary rules is obtained for the one-variable fragment by relaxing the eigenvariable condition for quantifier rules.	approximation algorithm;first-order predicate;mv-algebra;quantifier (logic);skolem normal form;łukasiewicz logic	Matthias Baaz;George Metcalfe	2007		10.1007/978-3-540-73099-6_5	zeroth-order logic;discrete mathematics;cut-elimination theorem;many-valued logic;computer science;proof theory;first-order logic;mathematics;proof calculus;situation calculus;programming language;natural deduction;structural proof theory;algorithm	Logic	-11.813550083000974	14.435964608695599	32806
9edf86fb3f8ca195eb6491ac8e99d1de21ffd874	refinement strategies for inductive leaming of simple prolog programs		T h i s paper ex tends Shap i ro ' s M o d e l Inference System for syn thes iz ing logic p rog rams f r o m examples of i n p u t / o u t p u t behav io r . A new ref inement ope ra to r for clause genera t i on , based u p o n the d e c o m p o s i t i o n o f P ro l og p rog rams i n t o skeletons, basic P ro l og p rog rams w i t h a we l l unde rs tood c o n t r o l f l ow, a n d techniques, s t a n d a r d P r o l o g p r o g r a m m i n g pract ices is descr ibed. Shap i ro ' s o r i g i na l sys tem is i n t r o duced , skeletons and techniques are discussed, a n d s imp le examples are p r o v i d e d , to f am i l i a r ize the reader w i t h t he necessary t e rm ino logy . T h e M o d e l Inference Sys tem equ ipped w i t h th is new re f inement ope ra to r is compa red a n d cont ras ted w i t h the o r i g i na l vers ion presented by Shap i ro . T h e s t rengths a n d weaknesses o f app l y i n g skeletons a n d techniques to syn thes iz ing P r o l o g p rog rams is discussed. 1 I n t r o d u c t i o n I n d u c t i v e l ea rn ing of concepts, g iven a set of examples a n d coun te rexamples , has been g iven a l o t of a t t e n t i o n i n the A r t i f i c i a l In te l l igence c o m m u n i t y . T h i s paper concerns a special case of i n d u c t i v e l e a r n i n g , syn thes iz ing P ro log p rog rams f r o m examples o f t he i r i n p u t / o u t p u t behav io r . A n i n c r e m e n t a l i n d u c t i v e inference a l g o r i t h m was developed in [Shap i ro , 1983] fo r syn thes iz ing logic p rog rams . Shap i ro n a m e d his i m p l e m e n t a t i o n the M o d e l Inference Sys tem ( M I S ) . M I S has several componen ts i n c l u d i n g : de tec t ion and remova l o f a false clause, de tec t i on of the i n a b i l i t y to prove a goa l k n o w n to be t r u e , and the a b i l i t y to f i nd a new clause to j u s t i f y t he k n o w n t r u t h o f a goa l . Anyone e x p e r i m e n t i n g w i t h M I S q u i c k l y discovers t h a t some p ro g rams are easy to l ea rn , o thers can be synthesized w i t h d i f f i cu l t y , and o thers are beyond the scope of the syst e m . T h e reason fo r t he v a r i a t i o n in pe r fo rmance can be t raced to the re f inement opera to r used to p roduce new clauses, a n d the search s t ra tegy emp loyed to de te rm ine i f t he new clause co r rec t l y imp l ies the examples . Hence the re f inement ope ra to r , due to i t s in f luence u p o n the scope o f t he sys tem, is t he foca l p o i n t fo r th i s paper . T h i s paper w i l l descr ibe M I S w i t h one o f i t s re f inement L e o n S. S te r l i ng Computer Engineering & Science Dept. Case Western Reserve University Cleveland, Ohio 44106 U.S.A. opera tors and w i t h a new re f inement opera to r based on w o r k on decompos ing P ro log p rog rams i n t o skeletons , basic P ro l og p rog rams w i t h a we l l unde rs tood cont r o l f l ow , and techniques, s t a n d a r d P ro log p r o g r a m m i n g pract ices. In cont ras t to Shap i ro 's re f inement opera to r , wh i ch checks and adds new clauses one at a t i m e , the new opera to r produces a l l re f inements and then checks the clauses generated. In fac t , every t i m e M I S t r ies to learn a new clause the re f inement opera to r goes t h r o u g h the same order o f clause genera t ion . By check ing p rev ious ly re fu ted clauses, the p r o g r a m ref ra ins f r o m repea t ing i ts mis takes. We w i l l denote M I S equ ipped w i t h our new opera to r as the M o d e l Inference System w i t h Skeletons and Techniques ( M I S S T ) . I n M I S S T the re f inement opera to r consists o f two phases. T h e f i rs t phase matches the necessary d a t a s t ruc tu res w i t h a skeleton an a p p r o p r i a t e con t ro l f low for the p r o g r a m . T h i s genera t ion of a skeleton is accomp l i shed by c rea t i ng a t e m p l a t e us ing on l y the i n p u t a rgumen ts f r o m the p r o g r a m to be synthes ized. Once the skeleton is c reated, the second phase enhances the skeleton by a p p l y i n g a techn ique to i t . Each technique w i l l generate a p r o g r a m to be checked for correctness. Examp les o f the types o f p r o g r a m s w h i c h are ha rd o r imposs ib le to l ea rn and those easy to learn w i l l be given for each sys tem. We w i l l examine the i m p l i c a t i o n s o f the resul ts and c i te areas for f u t u r e research. 2 T h e M o d e l In fe rence Sys tem T h e M o d e l Inference Sys tem is an i m p l e m e n t a t i o n o f an i n c r e m e n t a l i n d u c t i v e inference a l g o r i t h m . G i v e n a set o f examples and counte rexamples o f a new concept , M I S produces a set of H o r n clauses to represent the concept . Whenever the cu r ren t set of clauses prove a counterexa m p l e t rue , the p r o o f tree is used to de te rm ine the f au l t y clause in the set w h i c h is then removed . I f there exists an examp le no t exp la ined by the cur ren t set of clauses, a new clause is generated us ing a re f inement opera to r . A m a j o r assump t i on o f the system is t h a t an oracle exists w h i c h knows the t r u t h o r fa ls i t y o f any pa r t i cu l a r g r o u n d instance of the concept to be learned. T h e re f inement opera to r de termines the t ype o f P ro log p rog rams w h i c h can e a s i l y / n o t easi ly be learned t h r o u g h M I S . One re f inement opera to r g iven in [Shap i ro , 1983] was special ized fo r genera t ing clauses for de f in i te clause Kirschenbaum and Sterling 757 Ref inement Strategies for Induc t i ve Learn ing Of Simple Prolog Programs g r a m m a r s whereas a second opera to r was presented as a more general opera to r . Each opera to r was designed for syn thes iz ing a d i f ferent t ype o f p r o g r a m . In th i s paper, we use the general ised re f inement ope ra to r fo r a l l compar isons w i t h M I S S T . M I S needs to have access to ce r t a i n knowledge to successful ly synthesize a logic p r o g r a m . T h e f o l l o w i n g t w o types o f knowledge are specif ic to the i n tended ta rge t p r o g r a m a n d are supp l ied by the user. Dec la ra t ions a b o u t the t ype a n d m o d e o f each var iab le are used to de te rm ine how the var iab les w i l l be i n s t a n t i a t e d . I n f o r m a t i o n a b o u t ' a l l owab le ' predicates guides clause c rea t ion . An added goa l i s re la ted to the o ther goals in the clause t h r o u g h the i n s t a n t i a t i o n o f var iab les as specif ied by the t ype a n d m o d e dec la ra t ions . O the r knowledge is i nc l uded as p a r t o f the M I S database. For examp le , there is a l i s t of i n s t a n t i a t i o n s for each t y p e . A l i s t va r iab le can be i n s t a n t i a t e d to [ ] or [X|X.]. T h e general re f inement opera to r pe r fo rms i n the f o l -	arduino;artificial intelligence;computer engineering;correctness (computer science);exptime;execution unit;fo (complexity);field electron emission;fly-by-wire;genera;inference engine;linear algebra;morphological skeleton;naruto shippuden: clash of ninja revolution 3;numerical aperture;oracle database;oxford spelling;prolog;rams;rev;resources, events, agents (accounting model);rog-o-matic;syn flood	Marc Kirschenbaum;Leon Sterling	1991				AI	-21.18714796978164	17.08613829102584	32815
68bf77dde6e784491d00e2e6d198d2c0d5f4e05c	some intuitionistic equivalents of classical principles for degree 2 formulas	reverse mathematics;classical logic	Abstract   We consider the restriction of classical principles like Excluded Middle, Markov’s Principle, Konig’s Lemma to arithmetical formulas of degree 2. For any such principle, we find simple mathematical statements which are intuitionistically equivalent to it, provided we restrict universal quantifications over maps to computable maps.		Stefano Berardi	2006	Ann. Pure Appl. Logic	10.1016/j.apal.2005.04.006	mathematical analysis;discrete mathematics;classical logic;topology;reverse mathematics;mathematics;programming language;algorithm;algebra	Logic	-8.769291947126128	12.762890913963682	32824
2655ed2c2652f72f0ee299c14b7ad98122bc3975	model checking well-behaved fragments of hs: the (almost) final picture		Model checking is one of the most powerful and widespread tools for system verification with applications in many areas of computer science and artificial intelligence. The large majority of model checkers deal with properties expressed in point-based temporal logics, such as LTL and CTL. However, there exist relevant properties of systems which are inherently interval-based. Model checking algorithms for interval temporal logics (ITLs) have recently been proposed to check interval properties of computations. As the model checking problem for full Halpern and Shohamu0027s ITL (HS for short) turns out to be decidable, but computationally heavy, research has focused on its well-behaved fragments. In this paper, we provide an almost final picture of the computational complexity of model checking for HS fragments with modalities for (a subset of) Allenu0027s relations meets, met by, starts, and ends.	model checking	Alberto Molinari;Angelo Montanari;Adriano Peron;Pietro Sala	2016			abstraction model checking;algorithm;theoretical computer science;model checking;computation;ctl*;computational complexity theory;computer science;decidability	Logic	-11.963253454554309	24.71851770454012	32844
8d86a5b63353c81153eb608c3754193a6819e751	efficient multimethods in a single dispatch language	developpement logiciel;oriente message;object oriented language;message oriented;orientado mensaje;object oriented;desarrollo logicial;software development;relative efficiency;smalltalk;metaobjet;oriente objet;metaobjeto;orientado objeto;metaobject	Smalltalk-80 is a pure object-oriented language in which messages are dispatched according to the class of the receiver, or first argument, of a message. Object-oriented languages that support multimethods dispatch messages using all their arguments. While Smalltalk does not support multimethods, Smalltalk's reflective facilities allow programmers to efficiently add them to the language. This paper explores several ways in which this can be done, and the relative efficiency of each. Moreover, this paper can be seen as a lens through which the design issues raised by multimethods, as well as by using metaobjects to build them, can be more closely examined.	automatic programming;compiler;direct manipulation interface;dynamic dispatch;interoperability;metaobject;multiple dispatch;overhead (computing);programmer;programming language;run time (program lifecycle phase);smalltalk;virtual machine	Brian Foote;Ralph E. Johnson;James W Noble	2005		10.1007/11531142_15	real-time computing;computer science;database;programming language;object-oriented programming	PL	-27.174344110995694	30.12026002221422	32860
ed13c3ce31ceb737f5ccd3c27d8e7b9ceaeac66f	a generic formal specification of fusion of modalities in a multimodal hci	formal specification;formal method;transition systems;multi modal interaction	This paper is an overview of a generic formal description allowing to encode multi-modal interactive systems, their behaviors and properties.	encode;formal specification;modal logic;multimodal interaction;transition system	Yamine Aït Ameur;Nadjet Kamel	2004		10.1007/978-1-4020-8157-6_34	grammar systems theory;natural language processing;formal methods;specification language;formal verification;computer science;formal specification;refinement;programming language;communication	HCI	-26.58646464636503	19.55746177390652	32874
b3bdc7a4a144f0fc12c004538dc661597de293c8	efficient inference method for computing an optimal solution in predicate-logic hypothetical reasoning	optimal solution;knowledge based system;predicate logic;search strategy;optimal solution search;hypothetical reasoning;abductive reasoning;branch and bound;fault diagnosis	Hypothetical reasoning, which is one type of abductive reasoning, is an important framework in the development of advanced knowledge-based systems. One problem with hypothetical reasoning is its slow inference speed, which is due to its nonmonotonic inference nature. A fast hypothetical reasoning system with predicate Horn clause expressions has been developed to overcome this problem. However, when the constraints for hypotheses are not strong, the number of hypotheses to be synthetized becomes too large to calculate. The paper presents an efficient hypothetical reasoning method combining best-first search, beam search and branch-andbound search strategies for computing the optimal solution, which is the most desirable solution in many cases. The effectiveness of this method is shown experimentally using fault-diagnosis problems in logic circuits.	abductive reasoning;beam search;best-first search;experiment;horn clause;knowledge-based systems;logic gate;reasoning system;type inference	Akiko Kondo;Mitsuru Ishizuka	1996	Knowl.-Based Syst.	10.1016/0950-7051(95)01015-7	predicate logic;opportunistic reasoning;abductive reasoning;qualitative reasoning;computer science;artificial intelligence;adaptive reasoning;non-monotonic logic;knowledge-based systems;machine learning;reasoning system;deductive reasoning;branch and bound;algorithm;abductive logic programming	AI	-19.510709456550043	10.276202214216989	32921
e79c601b11e8872cb5a39bb777bc926e5eeaa749	integration with spatiotemporal relationship operators in sql	spatiotemporal databases;multidimensional index structures;spatial data;bulk loading;bulk insertion;spatial databases	Spatial objects in the world and location data for the vehicle change their spatial or non-spatial data over time. We therefore propose a spatiotemporal database schema and its operations to support all their history management and integration of spatial and temporal operations in a uniform manner. They will be implemented by extending time dimensions and time comparison operators in a geographic ~ormation system(GIS). Finally, we believe that our proposed spatktemporal database provides a usefhl tia.mework for history management of tirnevarying location data of vehicles and spatial objects in the world.	database schema;geographic information system;relational operator;sql;spatiotemporal database	Jong Yun Lee;Kwang Jin Oh;Keun Ho Ryu	1998		10.1145/288692.288724	computer science;data science;data mining;database;mathematics;spatial analysis;statistics;spatiotemporal database	DB	-29.70983915096941	7.704326568420309	32974
ba61225d5d8b5ada4ed24aa567f2396b879f31ef	a bottom-up approach to understand functional programs	program understanding;bottom up;program comprehension;formal methods;refactorings;object oriented programming;functional programming;formal method;source code;program specification	One affective way to carry out a program comprehension process is by refactoring the source code. In this paper we explore this approach in the functional programming paradigm, on Haskell programs specifically. As result, we have identified many correlations between the traditional (procedural and object-oriented) program comprehension process and the so called understanding-oriented refactorings, in the functional programming context. Besides, we have identified a catalog of refactorings used to improve program efficiency which, applied in reverse order, are valuable for program understanding purposes. Coupled to these refactorings, there is a calculational process by (means of) which we obtain a full formal description of program functionality. All together, a bottom-up program comprehension strategy in the functional setting is described.	bottom-up parsing;bottom-up proteomics;code refactoring;functional programming;haskell;list comprehension;procedural programming;program comprehension;programming paradigm;top-down and bottom-up design	Gustavo Villavicencio	2011		10.1145/1992896.1992910	program analysis;formal methods;computer science;top-down and bottom-up design;programming language;object-oriented programming;algorithm;source code	PL	-27.586097947246802	24.87334116889353	32992
16ac4a18a8d38e999981b351f4fe4fe3804a9351	explicit storage and analysis of billions of states using commodity computers	state space exploration;supervisory control;resource allocation systems;petri nets	The objective of this paper is to develop a framework and associated algorithms for explicit state space exploration of discrete event systems that can scale to very large state spaces. We consider classes of resource allocation systems (RAS), where a set of resources are shared by concurrent processes. In particular, we focus on Gadara RAS, whose Petri net representations have recently been used for liveness enforcement in multithreaded software. We present a framework where each reachable state of the RAS is represented by a single bit. We show how single-bit representations can lead to efficient implementations of supervisory control algorithms. In order to support single-bit state representations, we develop two indexing functions that map each state to a unique integer that serves as the corresponding index of the state in the large bit array. These functions exploit the invariants of the given RAS. Experimental results show that our techniques scale up to exploration and analysis of billions of states on commodity computers.	algorithm;bit array;commodity computing;computer;liveness;open-source software;petri net;state space;sublanguage;thread (computing)	Yin Wang;Jason Stanley;Stéphane Lafortune	2012		10.3182/20121003-3-MX-4033.00058	real-time computing;computer science;theoretical computer science;distributed computing	ML	-10.672626013342787	31.733792967638273	33015
384f80beea6b63d9bf72b18564dea801a905935e	using reflection to build efficient and certified decision procedures	procedimiento;calcul formel;computational reflection;calculo formal;theorem proving;demonstration theoreme;decision procedure;informatique theorique;demostracion teorema;computer algebra;procedure;computer theory;informatica teorica	In this paper we explain how computational reflection can help build efficient certified decision procedure in reduction systems. We have developed a decision procedure on abelian rings in the Coq system but the approach we describe applies to all reduction systems that allow the definition of concrete types (or datatypes). We show that computational reflection is more efficient than an LCF-like approach to implement decision procedures iii a reduction system. We discuss the concept of total reflections, which we have investigated in Coq using two facts: the extraction process available in Coq and the fact that the implementation language of the Coq system can be considered as a sublanguage of Coq. Total reflection is not yet implemented in Coq but we can test its performance as the extraction process is effective. Both reflection and total reflection are conservative extensions of the reduction system in which they are used. We also discuss performance and related approaches. In the paper, we assume basic knowledges of ML and proof-checkers.		Samuel Boutin	1997		10.1007/BFb0014565	procedure;symbolic computation;computer science;artificial intelligence;mathematics;automated theorem proving;programming language;algorithm	EDA	-14.903106415001925	19.617628676587646	33061
633286f02ccaee45f4e3d4040c9b22f2e4456b08	computation of transition adjacency relations based on complete prefix unfolding (technical report)		An increasing number of works have devoted to the application of Transition Adjacency Relation (TAR) as a means to capture behavioral features of business process models. In this paper, we systematically study the efficient TAR derivation from process models using unfolding technique which previously has been used to address the state space explosion when dealing with concurrent behaviors of a Petri net. We reveal and formally describe the equivalence between TAR and Event Adjacency Relation (EAR), the manifestation of TAR in the Complete Prefix Unfolding (CPU) of a Petri net. By computing TARs from CPU using this equivalence, we can alleviate the concurrency caused stateexplosion issues. Furthermore, structural boosting rules are categorized, proved and added to the TAR computing algorithm. Formal proofs of correctness and generality of CPU-based TAR computation are provided for the first time by this work, and they significantly expand the range of Petri nets from which TARs can be efficiently derived. Experiments on both industrial and synthesized process models show the effectiveness of proposed CPU-based algorithms as well as the observation that they scale well with the increase in size and concurrency of business process models.	algorithm;business process;categorization;central processing unit;computation;concurrency (computer science);correctness (computer science);experiment;net (polyhedron);overhead (computing);petri net;state space;turing completeness;unfolding (dsp implementation)	Jisheng Pei;Lijie Wen;Xiaojun Ye	2015	CoRR		discrete mathematics;computer science;engineering;artificial intelligence;theoretical computer science;operating system;machine learning;mathematics;algorithm	SE	-15.113585366781013	29.46119912592788	33068
bc69954b1bcb4cfa83d4da9d9bd7f6eb7db69c4b	automatic correctness proofs for logic program transformations	program transformation;satisfiability;automatic generation;qa75 electronic computers computer science;settore ing inf 05 sistemi di elaborazione delle informazioni;logic programs;linear equations;correctness proof	The many approaches which have been proposed in the literature for proving the correctness of unfold/fold program transformations, consist in associating suitable well-founded orderings with the proof trees of the atoms belonging to the least Herbrand models of the programs. In practice, these orderings are given by ‘clause measures’, that is, measures associated with the clauses of the programs to be transformed. In the unfold/fold transformation systems proposed so far, clause measures are fixed in advance, independently of the transformations to be proved correct. In this paper we propose a method for the automatic generation of the clause measures which, instead, takes into account the particular program transformation at hand. During the transformation process we construct a system of linear equations and inequations whose unknowns are the clause measures to be found, and the correctness of the transformation is guaranteed by the satisfiability of that system. Through some examples we show that our method is able to establish in a fully automatic way the correctness of program transformations which, by using other methods, are proved correct at the expense of fixing sophisticated	correctness (computer science);linear equation;program transformation;system of linear equations	Alberto Pettorossi;Maurizio Proietti;Valerio Senni	2007		10.1007/978-3-540-74610-2_25	computer science;linguistics;linear equation;programming language;algorithm;satisfiability	PL	-14.887281397902091	19.579816065612594	33089
bfb8df151900a3b0a7075ce1c31675c02e2db796	synthesizing bounded-time 2-phase fault recovery	real time;program transformation;program synthesis;fault tolerance;phased recovery;bounded time recovery	We focus on synthesis techniques for transforming existing fault-intolerant real-time programs into fault-tolerant programs that provide phased recovery. A fault-tolerant program is one that satisfies its safety and liveness specifications as well as timing constraints in the presence of faults. We argue that in many commonly considered programs (especially in safety/mission-critical systems), when faults occur, simple recovery to the program’s normal behavior is necessary, but not sufficient. For such programs, it is necessary that recovery is accomplished in a sequence of phases, each ensuring that the program satisfies certain properties. In the simplest case, in the first phase the program recovers to an acceptable behavior within some time θ, and, in the second phase, it recovers to the ideal behavior within time δ. In this article, we introduce four different types of bounded-time 2-phase recovery, namely ordered-strict, strict, relaxed, and graceful, based on how a real-time fault-tolerant program reaches the acceptable and ideal behaviors in the presence of faults. We rigorously analyze the complexity of automated synthesis of each type: we either show that the problem is hard in some class of complexity or we present a sound and complete synthesis algorithm. We argue that such complexity analysis is essential to deal with the highly complex decision procedures of program synthesis.	algorithm;analysis of algorithms;disaster recovery;fault tolerance;graphical user interface;heuristic (computer science);karp's 21 np-complete problems;liveness;mission critical;np-completeness;polynomial;program synthesis;real-time clock;real-time computing;real-time transcription;state space;time complexity;world-system	Borzoo Bonakdarpour;Sandeep S. Kulkarni	2014	Formal Aspects of Computing	10.1007/s00165-014-0325-8	fault tolerance;real-time computing;computer science;distributed computing;algorithm	PL	-13.793988519985877	28.772200498310323	33136
9c8226b00594b95aa1f149d504f2bd0238839d48	region analysis and a π-calculus with groups	memory management;regional analysis;garbage collection;type checking;ambient calculus	We show that the typed region calculus of Tofte and Talpin can be encoded in a typed π-calculus equipped with name groups and a novel effect analysis. In the region calculus, each boxed value has a statically determined region in which it is stored. Regions are allocated and de-allocated according to a stack discipline, thus improving memory management. The idea of name groups arose in the typed ambient calculus of Cardelli, Ghelli, and Gordon. There, and in our π-calculus, each name has a statically determined group to which it belongs. Groups allow for type-checking of certain mobility properties, as well as effect analyses. Our encoding makes precise the intuitive correspondence between regions and groups. We propose a new formulation of the type preservation property of the region calculus, which avoids Tofte and Talpin’s rather elaborate co-inductive formulation. We prove the encoding preserves the static and dynamic semantics of the region calculus. Our proof of the correctness of region de-allocation shows it to be a specific instance of a general garbage collection principle for the π-calculus with effects. We propose new equational laws for letregion , analogous to scope mobility laws in the π-calculus, and show them sound in our semantics.	ambient calculus;biologic preservation;calculi;checking (action);correctness (computer science);ectomesenchymal chondromyxoid tumor;garbage collection (computer science);memory management;object type (object-oriented programming);programming language;sequent calculus;subject reduction;type system	Silvano Dal Zilio;Andrew D. Gordon	2002	J. Funct. Program.	10.1017/S0956796801004270	ambient calculus;typed lambda calculus;computer science;simply typed lambda calculus;garbage collection;programming language;algorithm;memory management	PL	-12.425429528292177	19.835211361540964	33152
571f8d2054b34ae5322f450ca6b44570f80bbff5	an experience with a prolog-based object-oriented language	programming language;domain knowledge;object oriented language;data type;object oriented programming;knowledge base;expressive power	This paper presents an experience with a programming language SPOOL which is based on the combination of object-oriented programming and logic programming. This language inherits the capability of knowledge base organization from object-oriented programming and its expressive power from logic programming. The experience of the application of SPOOL to the program annotation system showed that this combination was quite useful to formalize domain knowledge into declarative data types and make them reusable in different contexts. It also showed the need for further study such as better linguistic support to exploit the full power of this combination.	apl;knowledge base;logic programming;n-gram;programming language;prolog;spooling;synergetics (fuller)	Koichi Fukunaga;Shin'ichi Hirose	1986		10.1145/28697.28719	natural language processing;first-generation programming language;knowledge base;constraint programming;declarative programming;very high-level programming language;language primitive;programming domain;reactive programming;computer science;programming language implementation;extensible programming;functional logic programming;programming paradigm;procedural programming;symbolic programming;low-level programming language;inductive programming;fifth-generation programming language;programming language;object-oriented programming;prolog;logic programming;programming language specification;high-level programming language;algorithm	PL	-25.393008308640344	21.895778098168257	33164
3ef6a3965a7ed54bc6e463bc6c0feb81d57558cb	dialectics of counting and the mathematics of vagueness	granulation;rough inclusion functions;rough y systems;rough natural number systems;algebraic semantics;cover based rough set theories;measures of knowledge;contamination problem;granular rough semantics;mathematics of vagueness;axiomatic theory of granules	New concepts of rough natural number systems are introduced in this research paper from both formal and less formal perspectives. These are used to improve most rough set-theoretical measures in general Rough Set theory (RST) and to represent rough semantics. The foundations of the theory also rely upon the axiomatic approach to granularity for all types of general RST recently developed by the present author. The latter theory is expanded upon in this paper. It is also shown that algebraic semantics of classical RST can be obtained from the developed dialectical counting procedures. Fuzzy set theory is also shown to be representable in purely granule-theoretic terms in the general perspective of solving the contamination problem that pervades this research paper. All this constitutes a radically different approach to the mathematics of vague phenomena and suggests new directions for a more realistic extension of the foundations of mathematics of vagueness from both foundational and application points of view. Algebras corresponding to a concept of rough naturals are also studied and variants are characterised in the penultimate section. keywords: Mathematics of Vagueness, Rough Natural Number Systems, Axiomatic Theory of Granules, Granulation, Granular Rough Semantics, Algebraic Semantics, Rough Y-Systems, Cover Based Rough Set Theories, Rough Inclusion Functions, Measures of Knowledge, Contamination Problem.	algebraic semantics (computer science);axiomatic system;fuzzy set;granule (oracle dbms);intel matrix raid;rough set;set theory;vagueness	Adel Mani	2012	Trans. Rough Sets	10.1007/978-3-642-31903-7_4	discrete mathematics;topology;granulation;mathematics;algorithm;dominance-based rough set approach	AI	-9.963219175512686	9.903883183188489	33216
7f03c9d9f31525fbac524a06af3100f19681552b	info-metrics for modeling and inference		Info-metrics is a framework for rational inference based on insufficient information. The complete info-metric framework, accompanied with many interdisciplinary examples and case studies, as well as graphical representations of the theory appear in the new book “Foundations of Info-Metrics: Modeling, Inference and Imperfect Information,” Oxford University Press, 2018. In this commentary, I describe that framework in general terms, demonstrate some of the ideas via simple examples, and provide arguments for using it to transform information into useful knowledge.		Amos Golan	2018	Minds and Machines	10.1007/s11023-018-9477-2	artificial intelligence;perfect information;computer science;machine learning;inference	ML	-18.058694260411933	6.358098971216642	33239
a7bb1187001aedf35162a75fe35086bd69c9c2f0	a reasoning system for a first-order logic of limited belief		Logics of limited belief aim at enabling computationally feasible reasoning in highly expressive representation languages. These languages are often dialects of first-order logic with a weaker form of logical entailment that keeps reasoning decidable or even tractable. While a number of such logics have been proposed in the past, they tend to remain for theoretical analysis only and their practical relevance is very limited. In this paper, we aim to go beyond the theory. Building on earlier work by Liu, Lakemeyer, and Levesque, we develop a logic of limited belief that is highly expressive while remaining decidable in the first-order and tractable in the propositional case and exhibits some characteristics that make it attractive for an implementation. We introduce a reasoning system that employs this logic as representation language and present experimental results that showcase the benefit of limited belief.	automated theorem proving;cobham's thesis;first-order logic;first-order predicate;hector levesque;reasoning system;relevance	Christoph Schwering	2017		10.24963/ijcai.2017/173	discrete mathematics;machine learning;artificial intelligence;description logic;first-order logic;computer science;non-monotonic logic;deductive reasoning;reasoning system;decidability;logical consequence	AI	-17.544309250584373	12.15065526343441	33245
fc2802ce3c869b11bcef9fbb44b1815800e92063	strategy representation and reasoning for incomplete information concurrent games in the situation calculus		Strategy representation and reasoning for incomplete information concurrent games has recently received much attention in multi-agent system and AI communities. However, most of the logical frameworks are based on concrete game models, lack the abilities to reason about strategies explicitly or specify strategies procedurally, and ignore the issue of coordination within a coalition. In this paper, by a simple extension of a variant of multiagent epistemic situation calculus with a strategy sort, we develop a general framework for strategy representation and reasoning for incomplete information concurrent games. Based on Golog, we propose a strategy programming language which can be conveniently used to specify collective strategies of coalitions at different granularities. We present a formalization of joint abilities of coalitions under commitments to strategy programs. Different kinds of individual strategic abilities can be distinguished in our framework. Both strategic abilities in ATL and joint abilities of Ghaderi et al. can be considered as joint abilities under special programs in our framework. We illustrate our work with a variant of Levesque’s Squirrels World.	agent-based model;collective intelligence;game theory;hector levesque;multi-agent system;programming language;situation calculus	Liping Xiong;Yongmei Liu	2016			complete information;artificial intelligence;machine learning;sort;computer science;situation calculus	AI	-17.43685663667542	4.823191140323734	33358
118dba0a486a05cd86732c1b6392164b77641c5c	local lexing		We introduce a novel parsing concept called local lexing. It integrates the classically separated stages of lexing and parsing by allowing lexing to be dependent upon the parsing progress and by providing a simple mechanism for constraining lexical ambiguity. This makes it possible for language design to be composable not only at the level of context-free grammars, but also at the lexical level. It also makes it possible to include lightweight error-handling directly as part of the language specification instead of leaving it up to the implementation. We present a high-level algorithm for local lexing, which is an extension of Earley’s algorithm. We have formally verified the correctness of our algorithm with respect to its local lexing semantics in Isabelle/HOL.	algorithm;context-free grammar;context-free language;correctness (computer science);formal verification;hol (proof assistant);high- and low-level;isabelle;lexical analysis;parsing;programming language specification	Steven Obua;Phil Scott;Jacques D. Fleuriot	2017	Archive of Formal Proofs			NLP	-21.582009331008894	26.69855159834777	33373
b7d017f3cc0f0571fa77af01a8adddc29844c270	the design of terra: harnessing the best features of high-level and low-level languages	004;language interoperability meta programming high performance lua	Applications are often written using a combination of high-level and low-level languages since it allows performance critical parts to be carefully optimized, while other parts can be written more productively. This approach is used in web development, game programming, and in build systems for applications themselves. However, most languages were not designed with interoperability in mind, resulting in glue code and duplicated features that add complexity. We propose a two-language system where both languages were designed to interoperate. Lua is used for our high-level language since it was originally designed with interoperability in mind. We create a new low-level language, Terra, that we designed to interoperate with Lua. It is embedded in Lua, and meta-programmed from it, but has a low level of abstraction suited for writing high-performance code. We discuss important design decisions — compartmentalized runtimes, glue-free interoperation, and meta-programming features — that enable Lua and Terra to be more powerful than the sum of their parts. 1998 ACM Subject Classification D.3.3 Language Constructs and Features	embedded system;game programming;glue code;high- and low-level;high-level programming language;interoperability;interoperation;low-level programming language;lua;metaprogramming;runtime system;web development	Zach DeVito;Pat Hanrahan	2015		10.4230/LIPIcs.SNAPL.2015.79	computer science;theoretical computer science;scripting language;programming language	PL	-28.412353960756043	28.82651504128761	33375
703ce43f94721300f90b32660244cdf325975d8b	is there a prototypical rule of abduction? (yes, e.g. in proximity based explanations)	abduction;knowledge represention;gestalt;natural language understanding;proximity;knowledge representation	Abstract We show that certain very different types of tasks can be modelled by one rule of abduction which we call coh(2), informally described as being able to go from the conclusions to the premises if the premises cohere, i.e. there is some semantic link between them. These tasks might come from the areas of natural language understanding, planning, perception, etc. Furthermore, there is a family of very closely related rules of coherence coh(n) that can do in one step what otherwise could require iteration of coh(2). More specifically, we formalize proximity-based reasoning as a kind of abduction; we show how it can be captured by such rules; and although we first present these rules informally, we later prove that their meaning can be made precise, and that it is possible to establish some formal properties of them. Of course, this requires a formalism for abduction, which is also sketched in the paper. All this leads to some mathematical results to the effect that certain natural strategies for usin...	abductive reasoning	Wlodek Zadrozny	1994	J. Exp. Theor. Artif. Intell.	10.1080/09528139408953786	knowledge representation and reasoning;computer science;artificial intelligence;distance;algorithm;gestalt psychology	AI	-15.481720621055041	5.9564284612531715	33393
80c1adaa7752db7bc3fff0fb1869c2c2df0cd2bf	alternative data structures for lists in associative devices.	data structure			Jerry L. Potter	1983			computer science;theoretical computer science;data mining;database	Theory	-30.945509046356957	7.56181433322429	33413
89bb40948d07db26d3b2e1764c74ed222227f57d	probabilistic spatial reasoning in constraint logic programming		In this paper we present a novel framework and full implementation of probabilistic spatial reasoning within a Logic Programming context. The crux of our approach is extending Probabilistic Logic Programming (based on distribution semantics) to support reasoning over spatial variables via Constraint Logic Programming. Spatial reasoning is formulated as a numerical optimisation problem, and we implement our approach within ProbLog 1. We demonstrate a range of powerful features beyond what is currently provided by existing probabilistic and spatial reasoning tools.	constraint logic programming;spatial–temporal reasoning	Carl P. L. Schultz;Mehul Bhatt;Jakob Suchan	2016		10.1007/978-3-319-45856-4_20	constraint logic programming;concurrent constraint logic programming;constraint programming;probabilistic ctl;horn clause;constraint satisfaction;non-monotonic logic;functional logic programming;probabilistic logic;reasoning system;inductive programming;automated reasoning;fifth-generation programming language;probabilistic argumentation;prolog;logic programming;probabilistic logic network;abductive logic programming;autoepistemic logic	PL	-18.577253727316634	12.705144739110706	33417
2ef3cee292759afc054599d12109f07f32fa4cba	a prototype inference engine for rule-based geometric reasoning	robot movil;regle inference;search engine;optimisation;buscador;sistema experto;optimizacion;mobile robot;user interface;rule based;logique floue;base connaissance;logica difusa;fuzzy logic;inference rule;prototipo;visual languages;robot control;robot mobile;langage visuel;geometric reasoning;fuzzy inference;visual language;base conocimiento;optimization;moteur recherche;systeme expert;prototype;moving robot;regla inferencia;knowledge base;expert system	Isaac is a rule-based visual language for mobile robots using evidential reasoning and a fuzzy inference engine. A prototype inference engine for Isaac has been implemented, permitting experiments with the Isaac language. This paper discusses this inference engine, describes some preliminary experiences with programming Isaac rulesets, and proposes future optimizations and enhancements to the inference engine.	experiment;inference engine;logic programming;mobile robot;prototype;visual language	Joseph J. Pfeiffer	2004		10.1007/978-3-540-25931-2_20	fuzzy logic;mobile robot;knowledge base;computer science;semantic reasoner;artificial intelligence;machine learning;database;prototype;robot control;user interface;expert system;inference engine;algorithm;search engine;rule of inference	AI	-21.017791502474655	11.67161418081474	33460
19c34ba038512f00f366ebaf7af9194e842a65eb	hypothetical reasoning from situation calculus to event calculus	situation calculus;event calculus	Pinto and Reiter have argued that the Sit uation Calculus improved with time han dling axioms subsumes the features of lin ear time temporal formalisms such as Event Calculus and Interval Logic In this note we nd answers to some of their re marks by showing a modi ed version of Event Calculus that seems to match Sit uation Calculus handling of hypothetical reasoning and projection Further consid eration on semantics and expressive power of Event Calculus put forward by Pinto and Reiter are discussed in the light of re cent proposal for an unifying semantics for languages for time and actions	event calculus;han unification;interval temporal logic;local interconnect network;situation calculus	Alessandro Provetti	1994			algorithm;calculus of communicating systems;interval temporal logic;event calculus;natural deduction;semantics;process calculus;proof calculus;mathematics;situation calculus	AI	-16.103033795735616	9.498429037481138	33514
03fa79070a3c5fdfd059041ae5085372b0ac5035	using tableau to decide description logics with full role negation and identity	identity role;tableau based reasoning;complexity;blocking;role negation;boolean modal logic;completeness;description logic;decidability	This article presents a tableau approach for deciding expressive description logics with full role negation and role identity. We consider the description logic ALBOid, which is ALC extended with the Boolean role operators, inverse of roles, the identity role, and includes full support for individuals and singleton concepts. ALBOid is expressively equivalent to the two-variable fragment of first-order logic with equality and subsumes Boolean modal logic. In this article, we define a sound, complete, and terminating tableau calculus for ALBOid that provides the basis for decision procedures for this logic and all its sublogics. An important novelty of our approach is the use of a generic unrestricted blocking mechanism. Unrestricted blocking is based on equality reasoning and a conceptually simple rule, which performs case distinctions over the identity of individuals. The blocking mechanism ties the proof of termination of tableau derivations to the finite model property of ALBOid.	blocking (computing);boolean algebra;description logic;finite model property;first-order logic;first-order predicate;logic programming;method of analytic tableaux;modal logic;newman's lemma;two-variable logic	Renate A. Schmidt;Dmitry Tishkovsky	2014	ACM Trans. Comput. Log.	10.1145/2559947	decidability;discrete mathematics;complexity;description logic;completeness;computer science;mathematics;blocking;algorithm	AI	-11.141319566097993	15.300288201401901	33525
55a665228af4996cfd2c5deb481082b8836da9f5	automatic programming by composing program components and its realization method	automatic programming	There are several methods of automatic programming, but composing program components is the only method that can automatically create large programs in a procedural language. In practical application domains, many people believe that developing a tool for developing and reusing program components is better than developing an automatic programming system. However, experiments in developing automatic programming systems actually by composing program components, suggest that this method can more efficiently support the development of practical systems. This paper presents studies derived from several experiments, and a method for developing an automatic programming system by composing program components.	admissible numbering;application domain;automatic programming;experiment;procedural programming	Seiichi Komiya	1989	Future Generation Comp. Syst.	10.1016/0167-739X(89)90034-4	real-time computing;programming domain;reactive programming;computer science;theoretical computer science;programming paradigm;procedural programming;symbolic programming;inductive programming;programming language	PL	-28.447953959888927	24.54959334170291	33546
527014362ca0fe8e1bfdbe2e2a48f6ea8d01318c	functional dependencies, supervenience, and consequence relations	semantic interpretation;functional dependency	An analogy between functional dependencies and implicational formulas of sentential logic has been discussed in the literature. We feel that a somewhat different connexion between dependency theory and sentential logic is suggested by the similarity between 'Armstrong's axioms' for functional dependencies and Tarski's defining conditions for consequence relations, and we pursue aspects of this other analogy here for their theoretical interest. The analogy suggests, for example, a different semantic interpretation of consequence relations: instead of thinking of B as a consequence of a set of formulas {A1, ..., A~} when B is true on every assignment of truth-values on which each A~ is true, we can think of this relation as obtaining when every pair of truth-value assignments which give the same truth-values to A1, the same truth-values to A2, .... and the same truth-values to A,~, also make the same assignment in respect of B. We describe the former as the consequence relation 'inferencedetermined' by the class of truth-value assignments (valuations) under consideration, and the latter as the consequence relation 'supervenience-determined' by that class of assignments. Some comparisons will be made between these two notions.	armstrong's axioms;dependency theory (database theory);functional dependency;propositional calculus;semantic interpretation;type class	Lloyd Humberstone	1993	Journal of Logic, Language and Information	10.1007/BF01181684	semantic interpretation;discrete mathematics;computer science;mathematics;functional dependency;algorithm	Logic	-12.81907599717311	5.933657816512454	33576
a1a9d717e4751695c7627aa44ffda5ba0c48cd0e	the complexity of first-order and monadic second-order logic revisited	fixed parameter tractable;first order;model checking;monadic second order logic;elementary functions;first order logic;lower bound	The model-checking problem for a logic L on a class C of structures asks whether a given L-sentence holds in a given structure in C. In this paper, we give super-exponential lower bounds for fixed-parameter tractable model-checking problems for first-order and monadic second-order logic.#R##N##R##N#We show that unless PTIME=NP, the model-checking problem for monadic second-order logic on finite words is not solvable in time f(k)·p(n), for any elementary function f and any polynomial p. Here k denotes the size of the input sentence and n the size of the input word. We establish a number of similar lower bounds for the model-checking problem for first-order logic, for example, on the class of all trees.	first-order predicate;monadic predicate calculus	Markus Frick;Martin Grohe	2004	Ann. Pure Appl. Logic	10.1016/j.apal.2004.01.007	predicate logic;zeroth-order logic;combinatorics;discrete mathematics;linear temporal logic;higher-order logic;algebraic sentence;intermediate logic;first-order logic;mathematics;monadic predicate calculus;programming language;second-order logic;algorithm	Logic	-7.1894883699112455	17.764157411995196	33579
82e5f5ac9ad6d5458e8164ad6195af714c5cf9f9	some structural properties of polynomial reducibilities and sets in np	sublinear time;natural computing;polylog space;first order definability;files;point of view;random access;structural properties	In this abstract and discussion of forthcoming papers, we will be concerned with variations on a common theme: without assuming a solution to <italic>P vs NP</italic>, what can one say of a general nature that relates <italic>structural</italic> properties of general classes of sets in <italic>NP</italic> to reducibilities among these sets? By “structural” we mean, in ways that will become clearer as we proceed, properties which arise from general definitions rather than properties which may arise from a perhaps more “natural” computational point of view. Although quite a bit is known about such questions relative to oracles or relative to the assumption that <italic>P @@@@ NP</italic>, in so far as possible we wish to obtain <italic>absolute</italic> results; that is results which are about sets in <italic>NP</italic> (not relativized) and which can be obtained without assuming a solution to <italic>P vs NP</italic>. In a final section summarizing our results we will make some general comments about the historical antecedents and possible future significance of this approach.	computation;oracle machine;polynomial	Paul Young	1983		10.1145/800061.808770	natural computing;mathematical optimization;combinatorics;discrete mathematics;computer file;computer science;mathematics;programming language;algorithm;random access	Theory	-4.544825325372652	12.015022805330743	33612
d4024d0b91db81e27bb49115f198d99ad3a7d501	multiparty session c: safe parallel programming with message optimisation	endpoint c program;multiparty session;complex communication topology;communication safety;endpoint protocol;asynchronous communication;safe parallel programming;optimised protocol;protocol description language;session c;global progress;message optimisation;global protocol	This paper presents a new efficient programming toolchain for message-passing parallel algorithms which can fully ensure, for any typable programs and for any execution path, deadlock-freedom, communication safety and global progress through a static checking. The methodology is embodied as a multiparty session-based programming environment for C and its runtime libraries, which we call Session C. Programming starts from specifying a global protocol for a target parallel algorithm, using a protocol description language. From this global protocol, the projection algorithm generates endpoint protocols, based on which each endpoint C program is designed and implemented with a small number of concise session primitives. The endpoint protocol can further be refined to a more optimised protocol through subtyping for asynchronous communication, preserving original safety guarantees. The underlying theory can ensure that the complexity of the toolchain stays in polynomial time against the size of programs. We apply this framework to representative parallel algorithms with complex communication topologies. The benchmark results show that Session C performs competitively against MPI.	benchmark (computing);communication endpoint;deadlock;integrated development environment;library (computing);mathematical optimization;message passing interface;parallel algorithm;parallel computing;runtime library;time complexity;toolchain	Nicholas Ng;Nobuko Yoshida;Kohei Honda	2012		10.1007/978-3-642-30561-0_15	real-time computing;computer science;theoretical computer science;distributed computing	PL	-12.521344915744198	31.442909332066794	33629
ca210002e26decb1399c80c851f40597c9d40616	timed refinement for verification of real-time object code programs		We introduce a refinement-based notion of correctness for verification of interrupt driven real-time object code programs, called timed refinement. The notion of timed refinement is targeted at verifica- tion of low-level object code against high-level specification models. For timed refinement, both the object code (implementation) and the specifi- cation are encoded as timed transition systems. Hence, timed refinement can be construed as a notion of equivalence between two timed transi- tion systems that allows for stuttering between the implementation and specification, and also allows for the use of refinement maps. Stuttering is the phenomenon where multiple but finite transitions of the imple- mentation can match a single transition of the specification. Refinement maps allow low-level implementations to be verified against high-level specification models. We also present a procedure for checking timed refinement. The proposed techniques are demonstrated with the verifi- cation of object code programs of six case studies from electric motor control applications.	object code;real-time transcription	Mohana Asha Latha Dubasi;Sudarshan K. Srinivasan;Vidura Wijayasekara	2014		10.1007/978-3-319-12154-3_16	refinement calculus;real-time computing;computer science;theoretical computer science;programming language	Logic	-18.324149173477064	27.981215121374543	33648
d290437c27f5aaa49670b370713ace657f30f94f	the temporal logic of causal structures	temporal logic;multiple hypotheses testing;computational analysis;model checking;causal relationship;causal formul;multiple hypothesis testing;minimal description length;feasible computational complexity;computational complexity;underlying causal structure	Computational analysis of time-course data with an underlying causal structure is needed in a variety of domains, including neural spike trains, stock price movements, and gene expression levels. However, it can be challenging to determine from just the numerical time course data alone what is coordinating the visible processes, to separate the underlying prima facie causes into genuine and spurious causes and to do so with a feasible computational complexity. For this purpose, we have been developing a novel algorithm based on a framework that combines notions of causality in philosophy with algorithmic approaches built on model checking and statistical techniques for multiple hypotheses testing. The causal relationships are described in terms of temporal logic formulæ, reframing the inference problem in terms of model checking. The logic used, PCTL, allows description of both the time between cause and effect and the probability of this relationship being observed. We show that equipped with these causal formulæ with their associated probabilities we may compute the average impact a cause makes to its effect and then discover statistically significant causes through the concepts of multiple hypothesis testing (treating each causal relationship as a hypothesis), and false discovery control. By exploring a well-chosen family of potentially all significant hypotheses with reasonably minimal description length, it is possible to tame the algorithm’s computational complexity while exploring the nearly complete search-space of all prima facie causes. We have tested these ideas in a number of domains and illustrate them here with two examples.	algorithm;causal filter;causal inference;causality;computation;computational complexity theory;experiment;interaction;model checking;neural oscillation;numerical analysis;probabilistic ctl;tame;temporal logic	Samantha Kleinberg;Bud Mishra	2009			discrete mathematics;computer science;artificial intelligence;machine learning;mathematics;algorithm;statistics	AI	-6.86480458441699	6.467223492951451	33689
fc3506dc1feecc397dfecd259eafe21521630ab3	notions of attack and justified arguments for extended logic programs	formal semantics;logic programs	The concept of argumentation may be used to give a formal semantics to a variety of assumption based reasoning formalisms. In particular, various argumentation semantics have been proposed for logic programming with default negation. For extended logic programming, i.e. logic programming with two kinds of negation, there arise a variety of notions of attack on an argument, and therefore a variety of different argumentation semantics. The purpose of this paper is to shed some light on these various semantics, and examine the relationship between different semantics. We identify a number of different notions of attack for extended logic programs, and compare the resulting least fixpoint semantics, defined via acceptability of arguments. We investigate the validity of the coherence principle, and notions of consistency for these semantics.	denotational semantics;fixed point (mathematics);jump point search;least fixed point;logic programming;semantics (computer science);stable model semantics;top-down and bottom-up design;turing completeness;well-founded semantics;whole earth 'lectronic link	Ralf Schweimeier;Michael Schroeder	2002			classical logic;description logic;higher-order logic;formal semantics;principle of bivalence;stable model semantics;action semantics;computer science;game semantics;theoretical computer science;proof-theoretic semantics;formal semantics;predicate transformer semantics;axiomatic semantics;well-founded semantics;operational semantics;logic;multimodal logic;denotational semantics;algorithm;philosophy of logic;computational semantics;autoepistemic logic	AI	-15.13820368447601	6.708701975621764	33705
0e7aa4f5b0c7a62ee03359fa701b8f1ca398bda9	collective argumentation and disjunctive logic programming	stable set;logic programs;disjunctive logic programming	An extension of an abstract argumentation framework is introduced that provides a direct representation of global conflicts between sets of arguments. The extension, called collective argumentation, turns out to be suitable for representing semantics of disjunctive logic programs. Collective argumentation theories are shown to possess a four-valued semantics, and are closely related to multiple-conclusion (Scott) consequence relations. Two special kinds of collective argumentation, positive and negative argumentation, are considered in which the opponents can share their arguments. Negative argumentation turns out to be especially appropriate for analyzing stable sets of arguments. Positive argumentation generalizes certain alternative semantics for logic programs.	admissible heuristic;argumentation framework;denotational semantics;disjunctive normal form;logic programming;relevance;theory;universal instantiation	Alexander Bochman	2003	J. Log. Comput.	10.1093/logcom/13.3.405	dynamic logic;discrete mathematics;description logic;higher-order logic;independent set;horn clause;stable model semantics;many-valued logic;computer science;intermediate logic;predicate functor logic;functional logic programming;computational logic;mathematics;inductive programming;programming language;prolog;logic programming;substructural logic;multimodal logic;algorithm;philosophy of logic;autoepistemic logic	AI	-14.93135187979374	8.625314397229817	33722
4cb8621e2d47b82c89cb25275822ced41f242377	combinatorial games in database theory	query language;combinatorial games;rule based;expressive power;finite model theory	During the past fifteen years, the study of rule-based query languages has occupied a prominent place in database theory. Much of the work in this area has been devoted to a comprehensive investigation of the expressive power of such query languages. In carrying out this investigation, researchers have relied heavily on the method of combinatorial games, a versatile method that originated in classical mathematical logic and became quite indispensable in finite model theory. The aim of this tutorial is to present the main combinatorial games used in database theory, give a representative sample of their numerous applications, butalso indicate someoftheir limitations. Inparticular, the tutorial will cover Ehrenfeucht-Fraiss4 games, games for monadic NP, and pebble games. Moreover, it will illustrate the use of these games in relational calculus, Datalog, and fixpoint logic. Below is a partial list of references for the main results presented in this tutorial. References [1] F. Afrati, S. S. Cosmadakis, and M. Yannakakis. On Datalog vs. polynomial time. In Proc. IW ACM Syrnp. on Principles of Database Systems, 1991. [2] A. V. Aho and J. D. Unman. Universality of data retrieval languages. In Proc. 6th ACM Symp. on Principles of Programming Languages, pages 110117, 1979. [3] M. Ajtai and R. Fagin. Reachability is harder for directed than for undirected finite graphs. Journal of Symbolic Logic, 55(l): 113–150, 1990. Permission to copy without fe~ all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association of Computing Machinerv.To coPv otherwise, or to republish, requires , a fee and/or speiific permission. PODS ’95 San Jose CA USA	data retrieval;database theory;datalog;expressive power (computer science);first-order logic;fixed point (mathematics);graph (discrete mathematics);iw engine;logic programming;pebble game;query language;reachability;relational calculus;symposium on principles of database systems;symposium on principles of programming languages;time complexity;universality probability	Phokion G. Kolaitis	1995		10.1145/212433.212461	rule-based system;combinatorial game theory;natural language processing;finite model theory;computer science;theoretical computer science;combinatorial explosion;database;programming language;expressive power;query language	DB	-19.740507016136057	13.456561181245648	33778
b04fa4675e2a7ce39536786374e4677f87bac797	complexity of algebraic implementations for abstract data types	abstract data type	Abstract   A notion of complexity for algebraic implementations of abstract data types is introduced and studied. The main results concern the expressive power of algebraic specifications and implementations as well as upper and lower bounds on the complexity of implementations in terms of time on Turing Machines.	abstract data type	Hartmut Ehrig;Bernd Mahr	1981	J. Comput. Syst. Sci.	10.1016/0022-0000(81)90014-3	discrete mathematics;computer science;theoretical computer science;abstract data type;generalized algebraic data type;algorithm	Theory	-11.48515565156224	20.445714949233615	33798
043de913c8ae82ca7794dea10a8ac5fdc38e469e	concurrency theory will set standards for description and analysis of software	concurrency theory		concurrency (computer science)	Jan Friso Groote	1996	ACM Comput. Surv.	10.1145/242224.242280	computer science;theoretical computer science;software design description;database;programming language	Logic	-29.898988355169372	26.939615502492902	33800
5e54bce5781ffc2d4498fde7128ceafb3f051e50	life without the terminal type	theorie type;lambda calculus;type theory;lambda calculo;lambda calcul	We introduce a method of extending arbitrary categories by a terminal object and apply this method in various type theoretic settings. In particular, we show that categories that are cartesian closed except for the lack of a terminal object have a universal full extension to a cartesian closed category, and we characterize categories for which the latter category is a topos. Both the basic construction and its correctness proof are extremely simple. This is quite surprising in view of the fact that the corresponding results for the simply typed λ-calculus with surjective pairing, in particular concerning the decision problem for equality of terms in the presence of a terminal type, are comparatively involved.	cartesian closed category;correctness (computer science);decision problem;dependent type;simply typed lambda calculus;type theory	Lutz Schröder	2001		10.1007/3-540-44802-0_30	system f;typed lambda calculus;discrete mathematics;dependent type;pure type system;computer science;cartesian closed category;lambda calculus;simply typed lambda calculus;mathematics;programming language;kind;type theory;algorithm	Logic	-6.461483223946728	14.381748756636927	33820
691285144e4198a9280f97718f46964f3c9e58b1	patterns as objects in grace	object oriented languages computer program languages;selected works;bepress selected works;object oriented languages computer program languages object oriented methods computer science software programming languages electronic computers software;object oriented methods computer science software;pattern matching;minigrace;grace;object orientation;bepress;programming languages electronic computers software	Object orientation and pattern matching are often seen as conflicting approaches to program design. Object-oriented programs place type-dependent behavior inside objects and invoke it via dynamic dispatch, while pattern-matching programs place type-dependent behavior outside data structures and invoke it via multiway conditionals (case statements).  Grace is a new, dynamic, object-oriented language designed to support teaching: to this end, Grace needs to support both styles. We explain how this conflict can be resolved gracefully: by modelling patterns and cases as partial functions, reifying those functions as objects, and then building up complex patterns from simpler ones using pattern combinators. We describe the implementation of this design as an object-oriented framework, and a case study of its effectiveness.	combinatory logic;composite pattern;computer science;data structure;dynamic dispatch;functional programming;graceful exit;pattern matching;regular expression;scripting language	Michael Homer;James W Noble;Kim B. Bruce;Andrew P. Black;David J. Pearce	2012		10.1145/2384577.2384581	software design pattern;method;state pattern;computer science;null object pattern;theoretical computer science;object-oriented design;pattern matching;divine grace;programming language;object-orientation;second-generation programming language	PL	-26.059906982156583	26.500272231693465	33826
c9ea981d8de739876a9a0b353d258dbaea0ab528	set constraints for destructive array update optimization	correct program transformation;scientific code;small-step operational semantics;functional language;destructive array update optimization;interprocedural flow analysis;polynomial time;multi-pass optimization;set constraints;sound liveness analysis;interprocedural update optimization	correct program transformation;scientific code;small-step operational semantics;functional language;destructive array update optimization;interprocedural flow analysis;polynomial time;multi-pass optimization;set constraints;sound liveness analysis;interprocedural update optimization	mathematical optimization	Mitchell Wand;William D. Clinger	2001	J. Funct. Program.	10.1017/S0956796801003938	real-time computing;programming language;algorithm	PL	-20.025477298902622	29.693178981269558	33852
d4bc64a7263b7f2778480ddd738718b26f2801f0	distilling reliable information from unreliable theories		Suppose a domain expert gives us a domain theory which is meant to classify examples as positive or negative examples of some concept. Now suppose, as is often the case, that the expert speciies parts of the theory which might be in need of repair, as opposed to those parts of the theory which are certainly not in need of repair. We say that such a theory is partially mutable. There might be some non-empty set of examples each of which has a classiication in the partially mutable theory which is invariant under all possible sets of repairs to unreliable components of the theory. We call such examples stable. We present an eecient algorithm for identifying stable examples for a large class of rst-order clausal theories with negation and recursion. We further show how to use stability to arbitrate between the theory and a noisy oracle to improve classiication accuracy. We present experimental results on some awed theories which illustrate the approach.	algorithm;domain theory;immutable object;oracle database;recursion;subject-matter expert	Shlomo Argamon;Moshe Koppel	1995			machine learning;subject-matter expert;oracle;discrete mathematics;recursion;artificial intelligence;invariant (mathematics);as is;negation;domain theory;mathematics	ML	-9.665683097773078	6.747647121263813	33890
b0f6086dd8cc0e848de8ee8729f2aaa2d8d1e541	a generic framework for heap and value analyses of object-oriented programming languages	generic analyzers;static analysis;abstract interpretation	Abstract interpretation has been widely applied to approximate data structures and (usually numerical) value information, but their combination is needed to effectively apply static analysis to real software. In this context, we introduce a generic framework that, given a heap and a value analysis, combines them, proving formally its soundness. We plug inside this framework a standard allocation site-based pointer analysis, a TVLA-based shape analysis, and standard existing numerical domains. As far as we know, this is the first sound generic automatic framework for statically typed object-oriented programming languages combining heap and value analyses that allows to summarize and materialize heap identifiers.		Pietro Ferrara	2016	Theor. Comput. Sci.	10.1016/j.tcs.2016.04.001	computer science;theoretical computer science;programming language;static analysis;algorithm	PL	-22.04463778597637	27.26304839934593	33949
163e03a3d39c0138a958f6a05b99424d654b5a6c	strategic ability update: a modal logic account	game theory;logic;modal logic;logic intelligent agent game theory computer science conferences ieee news;intelligent agent;computer science;dynamic epistemic logic;ieee news;conferences;modal logic game theory	We study an update operator for Coalition Logic to talk about the way players' strategic ability changes because of the moves of their opponents. We show its connection with Dynamic Epistemic Logic and we apply it to reason about game theoretical notions like undominated choice.	dynamic epistemic logic;epistemic modal logic	Jan M. Broersen;Rosja Mastop;John-Jules Ch. Meyer;Paolo Turrini	2009	2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2009.328	modal logic;dynamic logic;game theory;description logic;epistemic modal logic;computer science;artificial intelligence;theoretical computer science;computational logic;accessibility relation;logic;intelligent agent;multimodal logic;algorithm	AI	-17.291408154271704	4.552537140840361	33967
498f9ad0e04db2739846fdabbc214187c14caa77	certified higher-order recursive path ordering	multiensemble;relacion orden;lambda calculus;multiplicite;ordering;program verification;higher order;multiset;relation ordre;verificacion programa;reecriture;multiplicidad;lambda calculo;multiconjunto;recursive path ordering;rewriting;typed lambda calculus;verification programme;lambda calcul;multiplicity;reescritura	Recursive path ordering (RPO) is a well-known reduction ordering introduced by Dershowitz [6], that is useful for proving termination of term rewriting systems (TRSs). Jouannaud and Rubio generalized this ordering to the higher-order case thus creating the higher-order recursive path ordering (HORPO) [8]. They proved that this ordering can be used for proving termination of higher-order TRSs which essentially comes down to proving well-foundedness of the union of HORPO and βreduction relation of simply typed lambda calculus (λ→), [1]. This result entails well-foundedness of RPO and termination of λ→. This paper describes author’s undertaking of providing a complete, axiomfree, fully constructive formalization of those results in the theorem prover Coq. Formalization is complete and hence it contains all the dependant results for λ→, multisets and multiset extension of the relation. Also decidability of HORPO has been proven and due to constructive nature of this proof a certified algorithm to verify whether two terms can be oriented with HORPO can be extracted from this proof.	algorithm;automated theorem proving;autonomous robot;color;complete (complexity);coq (software);dershowitz–manna ordering;floor and ceiling functions;higher-order function;path ordering (term rewriting);recursion (computer science);rewrite order;rewriting;simply typed lambda calculus;termination analysis	Adam Koprowski	2006		10.1007/11805618_17	combinatorics;typed lambda calculus;discrete mathematics;higher-order logic;rewriting;order theory;computer science;lambda calculus;mathematics;multiplicity;programming language;algorithm	Logic	-10.982084262442665	19.185249157981282	33987
6dad34fac859e5b090d55c9771a4181184cd116e	learning semantic grammars with constructive inductive logic programming	constructive induction;first order;logic programs	Automating the construction of semantic grammars is a di cult and interesting problem for machine learning. This paper shows how the semantic-grammar acquisition problem can be viewed as the learning of search-control heuristics in a logic program. Appropriate control rules are learned using a new rst-order induction algorithm that automatically invents useful syntactic and semantic categories. Empirical results show that the learned parsers generalize well to novel sentences and out-perform previous approaches based on connectionist techniques.	algorithm;connectionism;heuristic (computer science);inductive logic programming;machine learning;parsing	John M. Zelle;Raymond J. Mooney	1993			natural language processing;computer science;artificial intelligence;machine learning;first-order logic;algorithm	AI	-19.69206802248491	11.804842774792219	34046
e102e72a5852e83d4a55dc1312bd023e113f0d44	formal specification of debuggers	lenguaje programacion;debugging;constructive theory;puesta a punto programa;preuve programme;program proof;formal specification;compilateur;semantica denotacional;programming language;compiler;theorie constructive;debogage;specification formelle;theorem proving;denotational semantic;especificacion formal;demonstration theoreme;teoria constructiva;denotational semantics;type theory;prueba programa;langage programmation;demostracion teorema;semantique denotationnelle;compilador	Programming in constructive type theory corresponds to theorem proving in mathematics: the specification plays the role of the proposition to be proved and the program is obtained from the proof. In this paper, we present an approach of using constructive type theory to derive a debugger of a given programming language from its denotational semantic definition. The development is supported by a proof development system called PowerEpsilon.	automated theorem proving;debugger;formal specification;intuitionistic type theory;programming language	Ming-Yuan Zhu	2001	SIGPLAN Notices	10.1145/609769.609778	compiler;computer science;formal specification;constructive proof;automated theorem proving;programming language;debugging;type theory;denotational semantics;algorithm	PL	-19.217755590660193	21.72667837242308	34050
334a2d69babe5ce0ce18412ed2fb78d500a30539	additive consolidation for dialogue game	argumentation;belief consolidation;legal reasoning;satisfiability;belief revision;dialogue games;coherence	Our purpose is to propose postulates about operators which capture dynamic aspects on a legal argumentation, and to construct operators which satisfy the postulates and expand plaintiff and defendant's theories coherently. These ideas are the generalizations of Olsson's additive consolidation. Consolidation is studied in the field of belief revision, and makes an agent's epistemic state coherent. Therefore, our presentation will build a bridge between legal reasoning and belief revision.	additive model;belief revision;coherence (physics);semiconductor consolidation;theory;utility functions on indivisible goods	Yoshitaka Suzuki;Satoshi Tojo	2005		10.1145/1165485.1165502	coherence;computer science;artificial intelligence;belief revision;algorithm;satisfiability	AI	-16.383586902174684	4.319532352840079	34096
d6414baa3f944f7115b85200c34cd798fa256f00	suszko's thesis, inferential many-valuedness, and the notion of a logical system	logic;mathematical logic and foundations;philosophy;computational linguistics;many valued logic	According to Suszko’s Thesis, there are but two logical values, true and false. In this paper, R. Suszko’s, G. Malinowski’s, and M. Tsuji’s analyses of logical twovaluedness are critically discussed. Another analysis is presented, which favors a notion of a logical system as encompassing possibly more than one consequence relation.	formal system;inferential theory of learning	Heinrich Wansing;Yaroslav Shramko	2008	Studia Logica	10.1007/s11225-008-9111-z	logical possibility;logical nor;non-classical logic;logical consequence;philosophy;epistemology;computer science;artificial intelligence;computational linguistics;truth table;mathematics;logic;algorithm	DB	-13.701034477745125	4.465608441168362	34126
4933bbb5e9da8598226cec775a0db16f9a4b856d	the remit system for paraphrasing relational query expressions into natural language	natural language	1. ARsTRAm. REMIT Relational Model Interpreter and Translator is a formal query language to natural language interpreter designed to aid query verification in a relational database environment. The system has been developed to work in conjunction with the ICL natural language query interface, NEL, which translates English query expressions into the formal query language QUERYMASTER. Funding for this research project has been provided by International Computers Limited.	coherence (physics);formal grammar;icl;nl (complexity);natural language user interface;prolog;prototype;query language;relational calculus;relational database;relational model;semantics (computer science);software portability;verification and validation	Barry G. T. Lowden;Anne N. De Roeck	1986			language identification;natural language programming;data control language;computer science;rdf query language;natural language	DB	-25.22588125765043	18.64608809786984	34168
bc461ab4130f75df087eab57dd6b624b6b760ab9	adding inferencing to a relational database management system	relational database management system	The LogiQuel System is a deductive DBMS which supports the management of clauses expressed in LogiQuel, a language with the expressive power of extended Datalog with negation. The inference mechanism used to evaluate LogiQuel queries is both sound and complete under the tight-tree semantics. The system is implemented on top of a commercial relational DBMS. After a brief overview of LogiQuel we show how a LogiQuel statement is translated into SQL for subsequent evaluation by the DBMS.	relational database management system	R. Marti;Christian A. Wieland;Beat Wüthrich	1989		10.1007/978-3-642-74571-3_20	data definition language;database theory;sql;relational database management system;relational model;entity–relationship model;relational database;database model;view;denormalization;database schema;object-relational impedance mismatch;database design	DB	-30.116002230049627	9.983814743513786	34177
b036a96d14001acdad9e6448878aa2eefe66a5cc	verifying communicating agents by model checking in a temporal action logic	logique lineaire;communication process;logica dynamica;verificacion modelo;multiagent system;logica temporal;temporal logic;temps lineaire;temporal constraint;verification modele;automaton;intelligence artificielle;logical programming;tiempo lineal;program verification;logica lineal;proceso comunicacion;dynamic logic;automata;processus communication;verificacion programa;temporal constraints;model checking;programmation logique;agent intelligent;automate;linear time;intelligent agent;communicative action;constrenimiento temporal;artificial intelligence;interaction protocol;logique dynamique;agente inteligente;inteligencia artificial;sistema multiagente;verification programme;programacion logica;linear logic;logique temporelle;agent programming;contrainte temporelle;systeme multiagent	In this paper we address the problem of specifying and verifying systems of communicating agents in a Dynamic Linear Time Temporal Logic (DLTL). This logic provides a simple formalization of the communicative actions in terms of their effects and preconditions. Furthermore it allows to specify interaction protocols by means of temporal constraints representing permissions and commitments. Agent programs, when known, can be formulated in DLTL as complex actions (regular programs). The paper addresses several kinds of verification problems including the problem of compliance of agents to the protocol, and describes how they can be solved by model checking in DLTL using au-	action algebra;agent-based model;agentspeak;algorithm;automata theory;causal filter;computational model;data integrity;event calculus;formal specification;formal verification;imperative programming;interaction protocol;linear temporal logic;mental state;model checking;precondition;programming language;spin model checker;semantics (computer science);time complexity;unified framework;verification and validation	Laura Giordano;Alberto Martelli;Camilla Schwind	2004		10.1007/978-3-540-30227-8_8	simulation;computer science;artificial intelligence;automaton;intelligent agent;algorithm	AI	-9.68277385765159	25.971685709241207	34201
48d7d57f44cb81f030faa1bf4a401cb505ef59cf	visibly linear temporal logic	temporal logic;visibly pushdown languages;automata based decision procedures;automata theory	We introduce Visibly Linear Temporal Logic (VLTL), a linear-time temporal logic that captures the full class of Visibly Pushdown Languages over infinite words. The novel logic avoids fix points and instead provides natural temporal operators with simple and intuitive semantics. We prove that the complexities of the satisfiability and visibly pushdown model checking problems are the same as for other well known logics, like CaRet and the nested word temporal logic NWTL, which in contrast are strictly more limited in expressive power than VLTL. Moreover, formulas of CaRet and NWTL can be translated inductively and in linear-time into VLTL.	caret;linear temporal logic;model checking;nested word;stack (abstract data type);time complexity	Laura Bozzelli;César Sánchez	2017	Journal of Automated Reasoning	10.1007/s10817-017-9410-z	discrete mathematics;linear temporal logic;description logic;interval temporal logic;computation tree logic;theoretical computer science;mathematics;algorithm	Logic	-11.098239796575106	23.42453054245989	34298
37dfdee69443e34aa3ed782c7b1a0512abd5ba8f	automated theorem proving proof and model generation with disconnection tableaux	teoria demonstracion;variabilidad;regle inference;theorie preuve;proof theory;disconnection tableau calculus;inference rule;completitud;completeness;variability;completude;variabilite;regla inferencia	We present the disconnection tableau calculus, which is a free-variable clausal tableau calculus where variables are treated in a non-rigid manner. The calculus essentially consists of a single inference rule, the so-called linking rule, which strongly restricts the possible clauses in a tableau. The method can also be viewed as an integration of the linking rule as used in Plaisted's linking approach into a tableau format. The calculus has the proof-theoretic advantage that, in the case of a satisfiable formula, one can characterise a model of the formula, a property which most of the free-variable tableau calculi lack. In the paper, we present a rigorous completeness proof and give a procedure for extracting a model from a finitely failed branch.	automated theorem proving	Reinhold Letz;Gernot Stenz	2001		10.1007/3-540-45653-8_10	completeness;computer science;calculus;proof theory;mathematics;proof calculus;algorithm;statistics;rule of inference	Logic	-14.907987478222854	16.057874321335063	34304
a50c6fa9204c50a59ef8af973553ce2558970db7	deriving and paraphrasing information grammars using object-oriented analysis models	lenguaje programacion;information architecture;query language;syntax;analisis sistema;programming language;integration information;pregunta documental;object oriented programming;syntaxe;lenguaje interrogacion;question documentaire;information integration;object oriented;integracion informacion;query;langage programmation;system analysis;oriente objet;analyse systeme;langage interrogation;information system;object oriented analysis;sintaxis;programmation orientee objet;orientado objeto;systeme information;sistema informacion	In this paper the focus is on object-oriented analysis of information systems. We assume that the communication within an application domain can be described by a logbook of events. In our view, the purpose of the analysis phase is to model the structure of this logbook. The resulting conceptual model is referred to as the information architecture, and is an integration of three formal object-oriented analysis models with each a specific view on the application domain. Furthermore, the information architecture forms an abstraction of an underlying grammar, called the information grammar, for the communication within the application domain. This grammar can be used to validate the information architecture in a textual format by informed users. In addition, the information grammar can be used to obtain the relevant data and processes of the application domain, and serves as a basis for the query language of users with the information system.	application domain;domain of discourse;embedded system;information architecture;information retrieval;information system;query language	Paul J. M. Frederiks;Theo P. van der Weide	2002	Acta Informatica	10.1007/s002360200083	grammar systems theory;natural language processing;computer science;programming language;attribute grammar;object-oriented programming;algorithm	NLP	-30.526862262040723	13.913149785081579	34326
7d136b701ae246d88ff061d36b606de638c0fa09	using connection method in modal logics: some advantages	modal logic;first order;polynomial time	For mechanizing modal logics, it is possible to distinguish the approach by translation and the direct approach. In our previous works, we advocate the use of translations to find a proof with a prover dedicated to the target logics but we introduced the notion of backward translation in order to present the proofs in the source logics. In this paper, we show that the connection method is well suited for the backward translation of proofs when first-order serial modal logics are involved. We use Wallen's matrix method for modal logics (extending Bibel's connection method) and Petermann's connection method for order-sorted logics with equational theories. We state that it is possible to build from a connection proof in the target logic a connection proof in the source logic in polynomial time with respect to the size of the proof in the target logic. Such a translation provides interesting insights to compare the approach by translation and the direct approach.	modal logic	Stéphane Demri	1995		10.1007/3-540-59338-1_28	modal logic;dynamic logic;t-norm fuzzy logics;normal modal logic;time complexity;modal μ-calculus;discrete mathematics;computer science;axiom s5;first-order logic;mathematics;accessibility relation;multimodal logic;algorithm	Logic	-13.959054142454523	15.716339781991818	34379
58a6345b5bb10e6ff668b9f6c36b3d64e72a730c	supervisory controller synthesis for decomposable deterministic context free specification languages	deterministic context free languages;supervisory control theory;hierarchical decentralized control	In this paper we consider a supervisory control problem (SCP) consisting of a regular plant language and a deterministic context free specification language, where the latter can be decomposed into a regular and a deterministic context free part. For this setup we give an implementable algorithm calculating two separate controllers for each part of the specification. We show under which conditions the parallel composition of these controllers solves the original SCP. The latter is done by reducing the problem to a special case of hierarchical decentralized control and using existing results from the literature.	abstraction layer;algorithm;deterministic context-free language;distributed control system;specification language	Kaushik Mallik;Anne-Kathrin Schmuck	2016	2016 13th International Workshop on Discrete Event Systems (WODES)	10.1109/WODES.2016.7497821	control engineering;real-time computing;computer science;distributed computing	Embedded	-6.324384912454965	28.68412969975538	34381
c07b7adf640234d8fe46b3b7daf2ad033bbff6ba	redundancy of variables in clp (r)		Constraint logic programming (CLP) languages allow natural and concise programming with constraints. In executing CLP languages the cost of constraint solving dominates the time and space considerations, so optimization of constraint solving is of paramount importance. CLP(R) is a constraint logic programming language over real numbers. Typically, in the execution of a CLP(R) program, many variables remain in a state where the constraint solver continually manipulates them, even if they will never be referenced in the remaining computation. This phenomenon is critical since continually manipulating these variables which are no longer of interest causes slower execution. Detecting such variables is a non-trivial problem. In this paper we describe a data ow analysis to detect redundant variables, and explain how the solver can be modiied to take advantage of this information. Experiments indicate that removing such variables yields substantial space and time saving in the execution of CLP(R).	backtracking;clp(r);computation;constraint logic programming;constraint satisfaction problem;execution unit;experiment;mathematical optimization;overhead (computing);programming language;r language;solver	Andrew D. Macdonald;Peter J. Stuckey;Roland H. C. Yap	1993				PL	-18.504991055281614	32.283416267210065	34440
d4bc209d7a79196c2f17a3f13d476d844f5d1027	three-membered domains for aristotle's syllogistic	mathematical logic;computational linguistic	"""The paper shows that for any invalid polysyllogism there is a procedure for constructing a model with a domain with exactly three members and an interpretation that assigns non-empty, non-universal subsets of the domain to terms such that the model invalidates the polysyllogism. Given the traditional definition of the semantic logical consequence relation for Aristotle's syllogistic, where terms designate non-empty subsets of a domain, we can exhibit z and y, where z is a set of sentences and y is a sentence, such that a domain with at least three members is required to show that y is not a logical consequence of z. (For example, it takes a threemembered domain to show that """"Some a are c"""" is not a logical consequence of """"No bare c"""" and """"No a are b."""") This paper shows that if y is not a logical consequence of z then, no matter how many terms are involved, we can show this using a domain with no more than three memberes. Moreover, thanks to correspondence with T.J. Smiley, it is shown that only proper subsets of three-membered domains are needed to interpret terms. The syntax of the syllogistic: Terms: aI, a2, .... Primitive operators: A, E,I, ° Defined operators: A',E',I',O' (A'zy = Ayz, E'zy = Eyz, 1'zy = Iyz, and 0' zy = Oyz. A, E, I, and ° are basic operators. A, I, A', and l' are positive operators, and the others are negative operators. Sentences: IT z and y are terms and Q is an operator then Qzy is a sentence, and no other expressions are sentences. (z is the subject term and y the predicate term of Qzy.) Next, we give the semantics for the syllogistic. (D, 3) is a model iff D is a non-empty set and 3 is a function whose domain is the set of terms and sentences, where 3 meets these conditions, conditions for a model: i) IT z is a term then 3(z) is a non-empty, non-universal subset of D, and ii) IT z is and y are terms then J(Azy) = tiff J(z) ~ J(y), J(Ezy) = tiff J(z )nJ(y) = 0, 1 I am grateful to T. J. Smiley and an anonymous referee for comments that improved this paper."""		Fred Johnson	1991	Studia Logica	10.1007/BF00370181	epistemology;mathematics;algorithm	Web+IR	-11.75040493518372	7.120933387987345	34453
dc5aa5650652d46c734511358b0614ec8b70223d	reducing disjunctive to non-disjunctive semantics by shift-operations	disjunctive programming;shift operator;semantics;complexity;abstract properties;disjunctive logic programming	It is wellknown that Minker’s semantics GCWA for positive disjunctive programs P is ΠP2 -complete , i.e. to decide if a literal is true in all minimal models of P . This is in contrast to the same entailment problem for semantics of non-disjunctive programs such as STABLE and SUPPORTED (both are co-NP-complete) as well as Msupp P and WFS (that are even polynomial). Recently, the idea of reducing disjunctive to non-disjunctive programs by using so called shift-operations was introduced independently by the authors and Marco Schaerf. In fact, Schaerf associated to each semantics SEM for normal programs a corresponding semantics Weak-SEM for disjunctive programs and asked for the properties of these weak semantics, in particular for the complexity of their entailment relations. While Schaerf concentrated on Weak-STABLE and Weak-SUPPORTED, we investigate the weak versions of Apt, Blair, and Walker’s stratified semantics Msupp P and of Van Gelder, Ross, and Schlipf’s wellfounded semantics WFS. We show that credulous entailment for both semantics is NP-complete (consequently, sceptical entailment is co-NP–complete). Thus, unlike GCWA, the complexity of these semantics belongs to the first level of the polynomial hierarchy. Note that, unlike Weak-WFS, the semantics Weak-Msupp P is not always defined: testing consistency of Weak-Msupp P is also NP–complete. We also show that Weak-WFS and Weak-Msupp P are cumulative (but not rational) and that, in addition, Weak-WFS satisfies some of the well-behaved principles introduced by Dix. This paper is a revised and extended version of [DGM94] which has been presented at ICLP ’94.	alan dix;amiga walker;boolean satisfiability problem;causal filter;causal model;co-np;co-np-complete;computation;disjunctive normal form;ibm notes;international conference on logic programming;literal (mathematical logic);mathematical model;np-completeness;polynomial hierarchy;preferential entailment;relevance;stable model semantics;time complexity;whole earth 'lectronic link	Jürgen Dix;Georg Gottlob;Victor W. Marek	1996	Fundam. Inform.	10.3233/FI-1996-281205	discrete mathematics;complexity;stable model semantics;formal semantics;mathematics;semantics;linguistics;shift operator;programming language;well-founded semantics;operational semantics;algorithm	AI	-12.67582687408789	14.321616790254646	34495
0b5a5ab2f6cb5a92159ae00e296016b8dd4ac8fa	arithmetic strengthening for shape analysis	memory safety;shape analysis;separation logic;software development	Shape analyses are often imprecise in their numerical reasoning, whereas numerical static analyses are often largely unaware of the shape of a program’s heap. In this paper we propose a lazy method of combining a shape analysis based on separation logic with an arbitrary arithmetic analysis. When potentially spurious counterexamples are reported by our shape analysis, the method constructs a purely arithmetic program whose traces over-approximate the set of counterexample traces. It then uses this arithmetic program together with the arithmetic analysis to construct a refinement for the shape analysis. Our method is aimed at proving properties that require comprehensive reasoning about heaps together with more targeted arithmetic reasoning. Given a sufficient precondition, our technique can automatically prove memory safety of programs whose error-free operation depends on a combination of shape, size, and integer invariants. We have implemented our algorithm and tested it on a number of common list routines using a variety of arithmetic analysis tools for refinement.	approximation algorithm;heap (data structure);inference engine;lazy evaluation;memory safety;numerical analysis;numerical integration;precondition;refinement (computing);separation logic;shape analysis (digital geometry);shape context;spatial–temporal reasoning;static program analysis;tracing (software)	Stephen Magill;Josh Berdine;Edmund M. Clarke;Byron Cook	2007		10.1007/978-3-540-74061-2_26	memory safety;separation logic;arbitrary-precision arithmetic;computer science;theoretical computer science;software development;shape analysis;shape analysis;programming language;algorithm	Logic	-17.796390471285164	25.374483760631964	34506
082778ecbb4688eed7af4e75b0d704d78bcedb6f	solar: an automated deduction system for consequence finding	automated deduction;connection tableaux;sol calculus;theorem proving;consequence finding;first order logic	SOLAR (SOL for Advanced Reasoning) is a first-order clausal consequence finding system based on the SOL (Skip Ordered Linear) tableau calculus. The ability to find non-trivial consequences of an axiom set is useful in many applications of Artificial Intelligence such as theorem proving, query answering and nonmonotonic reasoning. SOL is a connection tableau calculus which is complete for finding the non-subsumed consequences of a clausal theory. SOLAR is an efficient implementation of SOL that employs several methods to prune away redundant branches of the search space. This paper introduces some of the key pruning and control strategies implemented in SOLAR and demonstrates their effectiveness on a collection of benchmark problems.	applications of artificial intelligence;automated theorem proving;automatic control;benchmark (computing);clique problem;common intermediate language;control theory;first-order predicate;formal system;heuristic;iterative method;long division;method of analytic tableaux;multi categories security;natural deduction;non-monotonic logic	Hidetomo Nabeshima;Koji Iwanuma;Katsumi Inoue;Oliver Ray	2010	AI Commun.	10.3233/AIC-2010-0465	computer science;artificial intelligence;automated theorem proving;programming language;algorithm	AI	-16.679531595296346	13.738480626038621	34537
63ee55053508cd51090a35245536de63643eb4cf	defeasible reasoning and partial order planning	defeasible logic programming;defeasible reasoning;partial order	Argumentation-based formalisms provide a way of considering the defeasible nature of reasoning with partial and often erroneous knowledge in a given environment. This problem affects every aspect of a planning process. We will present an argumentation-based formalism that an agent could use for constructing plans starting from a previously introduced formalism. In such a formalism, agents represent their knowledge about their environment in Defeasible Logic Programming, and have a set of actions they can execute to affect their environment. These actions are defined in combination with a defeasible argumentation formalism. We will analyze the interplay of arguments and actions when constructing plans using Partial Order Planning techniques.	algorithm;argumentation framework;artificial intelligence;automated planning and scheduling;defeasible logic;defeasible reasoning;international standard book number;linear algebra;logic programming;non-monotonic logic;planning domain definition language;prolog;prototype;semantics (computer science);sensitivity and specificity;threat (computer);vii	Diego R. García;Alejandro Javier García;Guillermo Ricardo Simari	2008		10.1007/978-3-540-77684-0_21	partially ordered set;natural language processing;computer science;artificial intelligence;defeasible reasoning;algorithm	AI	-18.01876661414391	4.9630334465217985	34538
31ad57f47e420d969a6eae4668cc1bdd567d007e	fuzzy logic in narrow sense with hedges		Classical logic has a serious limitation in that it cannot cope with the issues of vagueness and uncertainty into which fall most modes of human reasoning. In order to provide a foundation for human knowledge representation and reasoning in the presence of vagueness, imprecision, and uncertainty, fuzzy logic should have the ability to deal with linguistic hedges, which play a very important role in the modification of fuzzy predicates. In this paper, we extend fuzzy logic in narrow sense with graded syntax, introduced by Novák et al., with many hedge connectives. In one case, each hedge does not have any dual one. In the other case, each hedge can have its own dual one. The resulting logics are shown to also have the Pavelka-style completeness.	fuzzy logic;knowledge representation and reasoning;logical connective;vagueness	Van Huynh Le	2016	CoRR	10.5121/ijcsit.2016.8310	fuzzy logic;description logic;defuzzification;type-2 fuzzy sets and systems;artificial intelligence;fuzzy number;fuzzy control language;algorithm	AI	-14.906636091092887	7.193705979493077	34543
71f239689c9677cde6dcce22d700e69b5a03444c	cooperative answering through controlled query relaxation	cooperative control;airports deductive databases automatic control laboratories buildings collaboration control systems prototypes data analysis information analysis;query relaxation;storage system;query processing;user preferences;user interfaces cooperative systems deductive databases query processing;logic and deduction;cooperative systems;nonstandard query answering;deductive database cooperative answering controlled query relaxation knowledge storage systems syntactic utilities information retrieval cooperative information systems;user interfaces;deductive databases	AS KNOWLEDGE STORAGE SYStems grow in size and complexity, it becomes unreasonable to expect that users always know enough about the data and how the data are organized to construct accurate and useful queries. Even when a system has syntactic utilities for building queries, users continue to compose queries that contain misconceptions, are too general, or are bound to fail. Cooperative-answering strategies seek to detect such situations and collaborate automatically with users to find the information that the users are seeking. They provide users with additional information, 1,2 intermediate answers, 3 qualified answers, 4 or alternative queries. 5	linear programming relaxation	Terry Gaasterland	1997	IEEE Expert	10.1109/64.621228	query optimization;query expansion;web query classification;computer science;data mining;database;user interface;web search query;information retrieval;query language;spatial query	DB	-26.318480764730488	7.076710706913103	34553
3a6d96368132b547d47ab8e71c2ccc90c4477a4e	jml support for primitive arbitrary precision numeric types: definition and semantics	smart card;java modeling language;formal semantics;specification language;design and implementation;semantic gap;language design	The Java Modeling Language (JML) is a notation for specifying and describing the detailed design and implementation of Java modules. An important language design goal of JML has been to preserve the semantics of Java to the extent possible. Thus, in particular, Java numeric expressions have the same meaning in JML. We illustrate how such a semantics fails to match the expectations of specification authors and readers who generally think in terms of arbitrary precision arithmetic (rather than the fixed precision provided by Java). As a result, an unusually high number of published JML specifications are invalid or inconsistent, including cases from the security critical area of smart card applications. We briefly examine JML’s ancestry and language design principles; this helps to explain the origin of the semantic gap between user expectations and the current meaning given to JML numeric expressions. With the objective of better matching user expectations we introduce JMLb, a variant of JML supporting primitive arbitrary precision numeric types as well as “math modes” to control the semantics of arithmetic expressions. This is done in a manner that is consistent with JML’s language design goals. A semantics of JMLb expressions is given by means of an embedding into PVS. The problem presented here will arise in the design of most interface specification languages that must deal with, e.g., mathematical integers in specifications and their fix precision approximations in code. We examine how the problem may manifest itself in other languages (such as Eiffel, Spark and the UML/OCL-Java notation of the KeY project) and comment on the applicability of our solution.	approximation;arbitrary-precision arithmetic;eiffel;fixed-point arithmetic;java modeling language;key;object constraint language;spark;smart card;specification language;unified modeling language	Patrice Chalin	2004	Journal of Object Technology	10.5381/jot.2004.3.6.a3	smart card;universal networking language;language primitive;formal semantics;action semantics;object language;specification language;computer science;theoretical computer science;java modeling language;formal semantics;database;semantics;programming language;operational semantics;semantic gap;computational semantics	PL	-24.023831291600708	23.459683797488893	34570
0f7918f4b40e29724e026d607bdca788702e659f	integrating multiple sources to answer questions in algebraic topology	formal description of arithmetic and set theoretical data types;front end;symbolic computation;bijective base 2 arithmetic;human computer interaction;user interface;software refinement with haskell type classes;computational mathematics;computer algebra system;artificial intelligent;peano arithmetic and hereditarily finite sets	We present in this paper an evolution of a tool from a user interface for a concrete Computer Algebra system for Algebraic Topology (the Kenzo system), to a front-end allowing the interoperability among different sources for computation and deduction. The architecture allows the system not only to interface several systems, but also to make them cooperate in shared calculations.	computation;computer algebra system;interoperability;natural deduction;user interface	Jónathan Heras;Vico Pascual;Ana Romero;Julio Rubio	2010		10.1007/978-3-642-14128-7_28	discrete mathematics;symbolic computation;numerical analysis;computer science;theoretical computer science;front and back ends;mathematics;user interface;algorithm;algebra	Logic	-19.103383570289676	17.145091960758528	34574
a194617f6021e2c954a65f18c998406c3b6819a9	analysis of binary relationships within ternary relationships in er modeling	ternary relationships;er modeling;binary relationships	In this paper, we discuss which binary relationship cardinalities are permitted within ternary relationships, during ER modeling. We develop an implicit Binary Cardinality (IBC) rule, which states that, in a W ternary relationship, the cardinality of any binary relationship embedded in the ternary, is many-to-many when there are no explicit constraints on the data instances. We then present an Explicit Binary Permission (EBP) rule, which explains and enumerates all permitted binary relationships for various cardinalities of ternary relationships. Finally we present an Implicit Binary Override (IBO) rule, which states that the implicit binary cardinalities can be constrained in a ternary relationship by explicit constraints imposed by a binary relationship. We also detail how the cardinalities within the ternary are altered as a single binary, and then multiple binary relationships are imposed. 1. I n t r o d u c t i o n One of the most difficult problems in using entity relationship (ER) models (Chen, 1976) for database design is the decision of whether to use a ternary or multiple binary relationships in complex entity relationship modeling. In this paper we present a comprehensive investigation and set of roles, establishing the relationships between variously configured ternary relationships and binary relationships embedded within those ternary relationships. One of our goals is to establish a set of easy to use rules by establishing the connections between relational theory and semantic modeling Research on the relationships between ternary and binary configurations in ER modeling is not very abundant, according to our literature survey. Chung, Nakamura and Chen (1982) discuss the decomposition of an entity relationship diagram (ERD) into 4NF. Ling (1985a) discusses the idea of normalization of ERD's, and further investigates the relationship between ERD's, multivalued dependencies and join dependencies (Ling, 1985b). Jajodia et. al. (1983) discuss the equivalence of ERD's. Teorey et. al, (1986) states that a ternary relationship type should be used only when the association among three entily types cannot be represented by several binary relationships. Batini, et. al. (1992) discuss schema decomposition primitives. However, none of these works clearly shows the relationship between binary and ternary representations. Some authors, e.g. Hawryszkie~Tcz (1991), offer solutions to translating a ternary relationship into a set of binary relationships, it is	bitwise operation;cardinality (data modeling);database design;diagram;embedded system;entity–relationship model;erdős–rényi model;fourth normal form;join dependency;many-to-many;multivalued dependency;relational theory;ternary numeral system;turing completeness;x86	Il-Yeol Song;Trevor H. Jones	1993		10.1007/BFb0024373	discrete mathematics;cardinality;binary constraint;entity–relationship model;ternary operation;binary number;binary relation;mathematics	DB	-31.323388768835386	14.401870790321164	34618
3e8f205831a423b13486cc53165a5a2e3c20b23e	platform-independent robust query processing	query processing;robustness;estimation;engines;three-dimensional displays;benchmark testing	"""To address the classical selectivity estimation problem for OLAP queries in relational databases, a radically different approach called <monospace>PlanBouquet</monospace> was recently proposed in <xref ref-type=""""bibr"""" rid=""""ref1""""> [1]</xref> , wherein the estimation process is completely abandoned and replaced with a calibrated discovery mechanism. The beneficial outcome of this new construction is that provable guarantees on worst-case performance, measured as Maximum Sub-Optimality (<italic>MSO</italic>), are obtained thereby facilitating robust query processing. The <monospace> PlanBouquet</monospace> formulation suffers, however, from a systemic drawback—the MSO bound is a function of not only the query, but also the optimizer's behavioral profile over the underlying database platform. As a result, there are adverse consequences: (i) the bound value becomes highly variable, depending on the specifics of the current operating environment, and (ii) it becomes infeasible to compute the value without substantial investments in preprocessing overheads. In this paper, we first present <monospace>SpillBound</monospace>, a new query processing algorithm that retains the core strength of the <monospace>PlanBouquet</monospace> discovery process, but reduces the bound dependency to only the query. It does so by incorporating plan termination and selectivity monitoring mechanisms in the database engine. Specifically, <monospace>SpillBound</monospace> delivers a worst-case multiplicative bound, of <inline-formula><tex-math notation=""""LaTeX"""">$D^2+3D$</tex-math><alternatives> <inline-graphic xlink:href=""""venkatesh-ieq1-2664827.gif""""/></alternatives></inline-formula>, where <inline-formula> <tex-math notation=""""LaTeX"""">$D$</tex-math><alternatives><inline-graphic xlink:href=""""venkatesh-ieq2-2664827.gif""""/> </alternatives></inline-formula> is simply the number of error-prone predicates in the user query. Consequently, the bound value becomes independent of the optimizer and the database platform, and the guarantee can be issued simply by query inspection. We go on to prove that <monospace>SpillBound</monospace> is within an <inline-formula> <tex-math notation=""""LaTeX"""">$O(D)$</tex-math><alternatives><inline-graphic xlink:href=""""venkatesh-ieq3-2664827.gif""""/> </alternatives></inline-formula> factor of the <italic>best possible</italic> deterministic selectivity discovery algorithm in its class. We next devise techniques to bridge this quadratic-to-linear MSO gap by introducing the notion of <italic>contour alignment</italic>, a characterization of the nature of plan structures along the <italic> boundaries</italic> of the selectivity space. Specifically, we propose a variant of <monospace>SpillBound</monospace>, called <monospace>AlignedBound</monospace>, which exploits the alignment property and provides a guarantee in the range <inline-formula><tex-math notation=""""LaTeX"""">$\mathbf {[2D+2,D^2+3D]}$</tex-math><alternatives> <inline-graphic xlink:href=""""venkatesh-ieq4-2664827.gif""""/></alternatives></inline-formula>. Finally, a detailed empirical evaluation over the standard decision-support benchmarks indicates that: (i) <monospace>SpillBound </monospace> provides markedly superior performance w.r.t. MSO as compared to <monospace>PlanBouquet</monospace>, and (ii) <monospace>AlignedBound</monospace> provides additional benefits for query instances that are challenging for <monospace>SpillBound</monospace>, often coming close to the ideal of MSO linearity in <inline-formula> <tex-math notation=""""LaTeX"""">$D$</tex-math><alternatives><inline-graphic xlink:href=""""venkatesh-ieq5-2664827.gif""""/> </alternatives></inline-formula>. From an absolute perspective, <monospace>AlignedBound</monospace> evaluates virtually all the benchmark queries considered in our study with MSO of around <bold>10</bold> or lesser. Therefore, in an overall sense, <monospace>SpillBound</monospace> and <monospace>AlignedBound</monospace> offer a substantive step forward in the long-standing quest for robust query processing."""		Srinivas Karthik;Jayant R. Haritsa;Sreyash Kenkre;Vinayaka Pandit;Lohit Krishnan	2019	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2017.2664827	online aggregation;robustness (computer science);machine learning;computer science;online analytical processing;artificial intelligence;query expansion;data mining;relational database;theoretical computer science;database engine;sargable;algorithm;query optimization	DB	-24.765533731305	5.853255482310413	34639
543db3f3b9b72c425a69fb681c01162b7ff38160	general purpose work flow languages	work flow language;heterogeneous transaction processing;transaction processing;coordination system;coordinate system	Work flow management requires language support for work flow specification and task specification. Many approaches and systems for work flow management therefore offer at least one new language for work flow specification; task specification is usually done in a traditional language. This is motivated in particular by the fact that many components already exist and the task of the work flow tool is the specification of the interaction between these components. The intention of this article is to demonstrate that a general purpose programming language can serve both aspects. We do not really see the need to develop yet another language that a user or application programmer must learn. If an existing programming language like C or Prolog is extended towards work flow capabilities, it is easy to reuse autonomous existing software components and to build interfaces among them.	autonomous robot;component-based software engineering;concurrency (computer science);general-purpose programming language;high- and low-level;interoperability;programmer;programming style;prolog;yet another	Alexander Forst;eva Kühn;Omran A. Bukhres	1995	Distributed and Parallel Databases	10.1007/BF01277645	interface description language;real-time computing;language primitive;object language;transaction processing;specification language;computer science;operating system;coordinate system;database;programming language;programming language specification	PL	-28.661473905234523	30.45344343869894	34660
46abacb246289394201f40a7cad63f08b9e79a43	a formal verification method of scheduling in high-level synthesis	automatic control;formal specification;processor scheduling;gate oxide leakage;formal verification method;flow graphs;formal method;high level behavioural specification;formal verification high level synthesis processor scheduling scheduling algorithm automata data visualization merging arithmetic automatic control flow graphs;automata;high level synthesis;arithmetic transformations formal verification method high level synthesis finite state machine high level behavioural specification fsmd model finite path segments;finite state machines;scheduling algorithm;formal verification;low voltage swing;data visualization;merging;high level synthesis finite state machines formal specification formal verification;arithmetic;subthreshold leakage;computer science and informatics;software requirements and specifications;arithmetic transformations;finite state machine;dual threshold voltage;finite path segments;fsmd model;domino logic	This paper describes a formal method for checking the equivalence between the finite state machine with data path (FSMD) model of the high-level behavioural specification and the FSMD model of the behaviour transformed by the scheduler. The method consists in introducing cutpoints in one FSMD, visualizing its computations as concatenation of paths from cutpoints to cutpoints and finally, identifying equivalent finite path segments in the other FSMD; the process is then repeated with the FSMDs interchanged. The method is strong enough to accommodate merging of the segments in the original behaviour by the typical scheduler such as DLS, a feature very common in scheduling but not captured by many works reported in the literature. It also handles arithmetic transformations.	computation;concatenation;dls format;finite state machine with datapath;finite-state machine;formal methods;formal verification;high- and low-level;high-level synthesis;scheduling (computing);turing completeness	Chandan Karfa;Chittaranjan A. Mandal;Dipankar Sarkar;Sri Ram Pentakota;Chris Reade	2006	7th International Symposium on Quality Electronic Design (ISQED'06)	10.1109/ISQED.2006.10	embedded system;electronic engineering;real-time computing;formal verification;computer science;theoretical computer science;formal specification;automaton;finite-state machine;high-level synthesis;scheduling;algorithm	Arch	-15.585245818262598	29.92849365263013	34692
cb83f546abd2da61dd80594b304af4559b1493fd	weak parametric failure equivalences and their congruence formats	structural operational semantics;weak failure equivalence;operational semantics;satisfiability;structured operational semantics;reactive system;rule formats;process algebra	Weak equivalences are important behavioral equivalences in the course of specifying and analyzing the reactive systems using process algebraic languages. In this paper, we propose a series of weak equivalences named weak parametric failure equivalences, which take two previously-known behavioral equivalences, i.e., the weak failure equivalence and the weak impossible future equivalence, as their special cases. More importantly, based on the idea of the structural operational semantics, a series of rule formats are further presented to congruence format for their corresponding weak parametric failure equivalences, i.e., a specific equivalence is further congruent in any languages satisfying its corresponding congruence format. This series of rule formats reflect the gradual changes in the weak parametric failure equivalences. We conclude that, when the weak parametric failure equivalences become coarser, their corresponding rule formats turn tighter.	airline control program (acp);bisimulation;calculus of communicating systems;congruence of squares;existential quantification;formal proof;hoare logic;linear algebra;negation as failure;operational semantics;process calculus;tag (game);turing completeness	Xiaowei Huang;Li Jiao;Weiming Lu	2008			combinatorics;process calculus;discrete mathematics;reactive system;computer science;mathematics;programming language;operational semantics;algorithm;satisfiability	Logic	-10.83926262415735	20.673691249534205	34701
9a879267fed9e686f6dd76d90fb6e57e9309b39f	a feature model of actor, agent, and object programming languages	programming language;feature modeling;object oriented programming;object oriented programming languages;agent oriented programming;comparative study;feature modelling;actor model	This paper presents first steps towards a feature model, which can be used to compare actor-oriented, agent-oriented, and object-oriented programming languages. The feature model is derived from the existing literature on general concepts of programming, and validated against Erlang, Jason, and Java. The model acts as a tool to assist practitioners in selecting the most appropriate programming language for a given task, and is expected to form the basis of further high-level comparative studies in this area.	actor model;erlang (programming language);feature model;high- and low-level;intelligent agent;jason;java;programming language	Howell R. Jordan;Goetz Botterweck;Marc-Philippe Huget;Rem W. Collier	2011		10.1145/2095050.2095077	natural language processing;fourth-generation programming language;first-generation programming language;protocol;declarative programming;very high-level programming language;programming domain;reactive programming;functional reactive programming;computer science;extensible programming;third-generation programming language;machine learning;functional logic programming;programming paradigm;procedural programming;symbolic programming;inductive programming;fifth-generation programming language;programming language theory;programming language;object-oriented programming;second-generation programming language;comparison of multi-paradigm programming languages;concurrent object-oriented programming	PL	-25.8201978901697	21.915221909537543	34703
2aa345f4bf9935f9e6e531df763ca1c8ffd60149	on consistency in temporal object bases	inproceedings;object oriented database	In a temporal object oriented database TOODB not only the actual state but also some parts of the history are stored We identify some problems for the consistency of the database and the update operations in a bitemporal environment We propose a strategy how a database which is only partially temporal can be interpreted intensionally as a temporal database Additionally we analyze some related work which was done for relational and OO frameworks	relational model;temporal database	Holger Riedel	1996			object-based spatial database;deep-sky object;object model;database;database design;object definition language	DB	-29.59868836486594	9.615031919754584	34777
06134c56582493634f973cd730bdce52028992a4	some remarks on cyclic linear logic	non commutative;multiplicative linear logic;decision problem;linear logic	We study three fragments of multiplicative linear logic with circular exchange re spectively LLNC containing all propositional variables LLNC a builded on a single variable and the constant only fragment LLNC By using non commutative proofnets we show that the decision problems of these fragments are polynomially equivalent	decision problem;linear logic	François Métayer	1996	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(05)80411-6	zeroth-order logic;linear logic;combinatorics;discrete mathematics;linear temporal logic;higher-order logic;intuitionistic logic;computer science;intermediate logic;predicate functor logic;decision problem;mathematics;predicate variable;propositional variable;programming language;substructural logic;second-order logic;algebra	Logic	-11.544288672205449	13.525503910796681	34782
80a66b07b0147f22dc08d53d6adcfa7800839ebd	internet of things (iot) applied to an urban garden		The constant increase of migration to the urban area has caused that the rural sector not have the manpower for agricultural production and is not able to supply food to the world's population, which has generated the need to expand the crops towards the cities. To encourage agricultural production in urban sectors, the use of technologies capable of automating and increasing production performance is proposed. This paper presents the develop of a system for the measurement, monitoring and automatic irrigation of an urban garden using Internet of Things (IoT) technology, it has sensors of CO2, humidity, luminosity, temperature, detection of plants and a hybrid application for its remote monitoring connected to a local area network. Also, the communication process used here is tested in order to provide a great connection service between the proposed system and the network.	internet of things;sensor	Gabriela Carrion;Monica Huerta;Boris Barzallo	2018	2018 IEEE 6th International Conference on Future Internet of Things and Cloud (FiCloud)	10.1109/FiCloud.2018.00030	local area network;agricultural engineering;cloud computing;irrigation;agricultural productivity;internet of things;population;urban area;business	Robotics	-30.200511485164945	18.923473428891388	34792
81f56fc6a174f02f527d9de617781ce83001d7ae	set theory and c*-algebras	set theory;irreducible representation;continuum hypothesis;functional analysis;c algebra;operator algebra	"""We survey the use of extra-set-theoretic hypotheses, mainly the continuum hypothesis, in the C*-algebra literature. The Calkin algebra emerges as a basic object of interest. Recently Charles Akemann and the author [5] used Jensen's diamond principle to solve an old open problem about the existence of nontrivial C*-algebras with only one irreducible representation. This raises the question: in general, to what extent is extra-set-theoretic reasoning relevant to C*-algebras? The one extra axiom that has been repeatedly used by C*-algebraists is the continuum hypothesis, although there are a few instances where it was noted that Martin's axiom or some more special weakening of CH would suffice to settle the problem at hand. Interestingly, it appears that C*-algebraists generally tend to regard a problem as solved when it has been answered using CH. This may have to do with the fact that in most cases the other direction of the presumed independence result would involve set theory at a substantially more sophisticated level. There could be an opportunity for set theorists here; it seems likely that most of the theorems we survey below are the easy halves of independence results, the hard — or more set-theoretically sophisticated — directions of which have not yet been proven. CH has been particularly valuable in the study of the Calkin algebra C(l 2) = B(l 2)/K(l 2) (see Example 1.5 below). This algebra can be viewed as a noncommu-tative or """" quantum """" analog of the Stone-ˇ Cech remainder βω − ω [55]. It should be of basic set-theoretic interest and a rich source of future work. In the first section below I give a quick introduction to C*-algebras. In the second section I survey a variety of consistency results taken from the C*-algebra literature, and in the third I focus on the Calkin algebra. My concern is consistency results and I do not discuss other relations between C*-algebras and set theory or logic, such as the Glimm-Effros dichotomy [30] which arose out of C*-algebra representation theory, connections with nonstandard analysis [6, 31], Mundici's work on decidability of isomorphism of AF-algebras [43], or simply the use of basic set-theoretic techniques that do not go beyond ZFC (e.g., my solution of Dixmier's problem using a transfinite recursion of length 2 ℵ0 [60]). I wish to thank Ilijas Farah and the referee for providing several suggestions for improving the exposition of this paper."""	irreducibility;jensen's inequality;linear algebra;recursion;transfinite induction;transfinite interpolation;triune continuum paradigm;zermelo–fraenkel set theory	Nikolai Weaver	2007	Bulletin of Symbolic Logic		zermelo–fraenkel set theory;differential graded algebra;functional analysis;representation theory of hopf algebras;symmetric algebra;filtered algebra;c*-algebra;mathematical analysis;discrete mathematics;irreducible representation;topology;incidence algebra;term algebra;gelfand–naimark theorem;pure mathematics;jordan algebra;continuum hypothesis;quaternion algebra;operator product expansion;mathematics;algebra representation;cellular algebra;operator algebra;subdirectly irreducible algebra;current algebra;set theory;algebra	Theory	-6.270983974004285	12.379150633201219	34793
71f05ec0b40a39aec2f137db9463687244f45e70	towards an approximate conformance relation for hybrid i/o automata	datavetenskap datalogi;computer science	Several notions of conformance have been proposed for check ing the behavior of cyber-physical systems against their hybrid systems models. In this paper, we explore the initial idea of a notion of approximate conformance that allows for comparison of bo th observable discrete actions and (sampled) continuous trajectories. As such, this notion wi ll consolidate two earlier notions, namely the notion of Hybrid Input-Output Conformance (HIOCO) by M. van Osch and the notion of Hybrid Conformance by H. Abbas and G.E. Fainekos. We prove that our p roposed notion of conformance satisfies a semi-transitivity property, which makes it suit able for a step-wise proof of conformance or refinement.	approximation algorithm;automaton;call of duty: black ops;conformance testing;cyber-physical system;hybrid system;input/output;modal logic;observable;refinement (computing);regular expression;sampling (signal processing);semi-continuity;semiconductor industry;vertex-transitive graph	Morteza Mohaqeqi;Mohammad Reza Mousavi	2016		10.4204/EPTCS.232.8	discrete mathematics;computer science;theoretical computer science;algorithm	Logic	-7.098397896352328	24.566192238554088	34841
1d10904e3c4c4688c7f946c5436b42918dca69f4	verifying osek/vdx automotive applications: a spin-based model checking approach		SummaryrnOSEK/VDX, a development standard for automobiles, has now been widely adopted by automotive manufacturers for developing a vehicle-mounted system. The ever increasing complexity of the system has created a challenge for ensuring the reliability of the developed OSEK/VDX applications in exhaustive way. Model checking as an exhaustive verification technique has attracted much attention in the automotive industry. To check OSEK/VDX applications by using model checking verification techniques, we have proposed a method based on SMT-based bounded model checking. However, the method performs a poor efficiency in checking the OSEK/VDX applications that hold many loops, especially it is unable to deal with interruptions. In this paper, to apply model checking verification techniques to check a practical OSEK/VDX application, we develop and investigate an alterative approach based on the well-known model checker Spin. In our Spin-based approach, interruptions are taken into account, and moreover, 2 optimization strategies are used to boost the scalability and efficiency of the approach by reducing state space and accelerating bug detection. We have investigated the Spin-based approach based on a series of experiments. The experimental results show that the approach is an impactful technique to verify the developed OSEK/VDX applications that hold a number of loops and interruptions.	model checking;osek;vdx (library software)	Haitao Zhang;Guoqiang Li;Zhuo Cheng;Jinyun Xue	2018	Softw. Test., Verif. Reliab.	10.1002/stvr.1662	model checking;theoretical computer science;state space;osek;scalability;bounded function;spin-½;computer science;automotive industry	SE	-14.201866628623456	26.380518978800197	34860
7918a6e96384658476150cf815adf7c707e84ea0	advances in rete pattern matching	production system;pattern matching	A central algorithm in production systems is the pattern match among rule predicates and current data. Systems like OPS5 and its various derivatives use the RETE algorithm for this function. This paper describes and analyses several augmentations of the basic RETE algorithm that are incorporated into an experimental production system, YES/OPS, which achieve significant improvement in efficiency and rule clarity. Introduction Rule based systems often spend a large fraction of their execution time matching rule patterns with data. The production system OPS5 [FOR11 and many other systems (e.g. [ART11 [YAP11 [FOR3]), each use the OPS5 pattern match algorithm known as RETE. This paper describes four augmentations of the basic RETE algorithm that achieve much improved performance and rule clarity. As we describe each augmentation, we give an analysis of its effects, and some examples of its use. These ideas are implemented in an experimental production system language, YES/OPS, running on LISP/VM in IBM Yorktown Research. We presume some familiarity with production systems, and the RETE algorithm. The reader is referred to the book, Programming Expert Systems in OPSS [BROl], and the AI Journal article on the RETE algorithm [FOR21 for background information. The first augmentation involves handling changes to existing data. In OPS5, three operations affect the data being matched with the rule patterns: make, which adds new data, remove, which removes data previously added, and modify, which modifies data previously added. However, modlf y is implemented in OPS5 as a remove of the previously existent data, followed by the creation of new data that is a copy of the previous data, except for the attributes that were changed. This new data is then added, which causes a new match cycle to occur. We change this to support modify as an update-inplace operation, and change how the rules are (re-)triggered, for greater clarity. The second augmentation allows the user to group rule patterns (called condition elements) together, in an arbitrary fashion. This enables specifying negated joins of patterns, not just individual condition elements, and plays an important role in specifying when to do maximize and minimize operations (which follows). The grouping can also be used to increase pattern match result sharing among the rules, for efficiency. The third augmentation supports the specification of sorted orderings among sets of data, in a much more efficient and syntactically clear manner. The final augmentation is the ability to do the pattern matching on demand, incrementally. This supports both the incremental addition of new rules, such that the new rule does match the existing data (not possible in OPS5), and the matching of particular patterns as part of an action done w-hen a rule fires, not when the data changes. This aspect eliminates the (OPS5) requirement that data to be manipulated in the action part of a rule must be matched by a condition element pattern in the rule’s tests (its Left Hand Side). This allows many practical rule sets to achieve orders of magnitude performance improvement, by reducing the pattern matching part that part which needs to be data-change sensitive. of the rules to just All examples of rules are written using the YES/OPS syntax. This is similar to OPS5 syntax, except: 1) attributes are not preceded by an “f” character, but are followed instead by a colon “:‘I; 2) the rule form is: (P rule-name WHEN pattern matching specifications THEN actions to be done) MODIFY as update-in-place, new triggering conditions OPS5’s implementation of modify as a remove of the old value, and a re-make of it with the modified attributes causes excessive retriggering of rules. Two commonly occurring instances of unwanted re-triggering are modification of attributes not tested in a rule and modification of an attribute to a value that still passes the same rule patterns as before. Example: Don’t-care slots re-triggering Suppose the user structures his working memory elements for a problem involving genealogy research, as follows: Classname: PERSON Attributes: Name : Father: Mother: Gender: Native-language: Native-country: Language: Marital-status: Spouse : Now suppose some rules infer about ancestry, and other rules infer about languages spoken. If the ancestry rules have fired, and now, some new information about language causes the person’s language : attribute to be changed, in OPS5, the ancestry rules would fire again, even though they had taken all the actions appropriate for their matches to the existing data, and that data had not changed in the attributes of interest. The solution to this behavior in OPS5 is to separate attributes whose change should not re-trigger other rules, into different working memory elements. This is often not the natural partition of the knowledge, and is less efficient, because the RETE must now do run-time joins of the split-apart attributes. Example: Tests true once, true again after modifying, re-triggering In OPS5, whenever a rule’s action part modifies a working memory element such that it still satisfies the rule’s tests, that rule loops. Users are told to “get around” this problem by coding extra control information in the working memory element and set flags that prevent looping. An example from the book Programming Expert Systems in OPS5 [BROl] is the problem of adding one to a set of items. The natural formulation (the one inexperienced users tend to write) looks like : (p add-l-to-items when (goal name: add-l-to-items) ;the goal to do it <i> (item value: <v>) ;an item, whose value is <v> then (modify <i> ;modify the item value: (<v> + 1) >) ;setting the value ; to <v’ + 1 226 / SCIENCE From: AAAI-86 Proceedings. Copyright ©1986, AAAI (www.aaai.org). All rights reserved. This works in YES/OPS, when modify is update-in-place, but loops in OPS5. The suggested rule formulation to get around this problem in OPS5 is to add an extra attribute to Item, called status, and set it from nil to marked when doing the adding. After adding to all the items, the goal is advanced to unmark, and another rule fires repeatedly, once per item, to change the status attribute back to nil. This clearly is more rule firings, and also, more testing (the value of the status attribute must be tested). The example also is now cluttered up with control information, unrelated to the task of adding 1 to a set of items, which makes these OPSS-style rules less readable: (p add-l-to-items when (goal name: add-l-to-items) ;the goal to do it <i> (item value: <v> status: nil) then (modify <i> value: (compute <v> + 1) status: MARKED)) ----------________--------------------------(p change-task when ;this rule fires after ;prev. rule because it <g> (goal name: ;tests fewer things add-l-to-items) then (modify <g> name: UNMARK)) -----______---------------------------------(p unmark when (goal name: UNMARK) <i> (item status: MARKED) then (modify <i> status: NIL)) Having to code this kind of status information makes the rules less clear. Without implementing the new modify definition, the natural rule an expert often writes would need to be “fixed” to eliminate the unwanted triggering. The efficiency also suffers, in that the fixes require more pattern matching tests. New modify definition remom re-tri&gering problems We define modify as an atomic update-in-place operation, rather than as a remove followed by a make of the modified working memory element. The triggering rules are changed so that an existing instantiation that continues to exist after the modify, does NOT cause re-triggering. In addition to improving performance by eliminating extra control flags and their testing and maintenance, modify done as update-inplace reuses existing working memory data structure and RETE memory nodes. This improves the performance by reducing the activity involved with maintaining these structures. Triggering on any change The new modify semantics normally trigger a rule when a rule instantiation that was not previously present gets created. This means that a modify operation does not re-trigger a rule, if it does not result in a new instantiation. Sometimes, however, triggering on any change is desirable. An example might be a rule that counted how many times a person’s marital status changed. Here, we want the rule to re-trigger, no matter what the status changed to. To provide for this case, we extend the syntax to allow specifying re-triggering on any change of one or more selected attributes, by preceding the attribute name by an exclamation point (!). In addition, to specify re-triggering on the change of any attribute in the class, an exclamation point may be placed in front of the class name. This gives behavior like OPS5. For example: (p count-marital-status-changes when (person ! marital-stat:) cc> (counter type: *I retriggers on change Marital-stat-chg value: <v>> then (modify cc> value: (<v> + 1))) New algorithm for Modify in RETE Beta Join nod&s Tokens passed down the RETE have the operation ADD, REMOVE, or MODIFY associated with them (ADD corresponds to make). For modify operations, if at some point in the processing, the test result of the previous value of the modified working memory element differs from that of the current value, the modify operation is converted to a remove or add operation: CASE 1 CASE 2 Previous value: tests fail tests OK Current value: tests OK tests fail New operation: ADD REMOVE When a token arrives at the bottom of the RETE, if the operation is add or remove, then the rule instantiation in the production node is either inserted to or removed from the conflict set, according to the operation; if the operation is modify, then nothing is done. This prevents re-triggering. For modify operations, specification of re-triggering attributes causes an exception. If one or more of the attributes was preceded by an “!” to indicate that re-triggering is wanted on any change of that attribute	atomicity (database systems);colon classification;data structure;exception handling;experience;expert system;fail-safe;graphical data display manager;graphical user interface;in-place algorithm;lisp;ops5;orthogonal polarization spectral imaging;pattern matching;procedural programming;production system (computer science);rete algorithm;rule 184;rule 90;run time (program lifecycle phase);stat (system call);system programming language;thomas j. watson research center;universal instantiation;windows update;xedit	Marshall I. Schor;Timothy Daly;Ho Soo Lee;Beth Tibbitts	1986			computer science;artificial intelligence;pattern matching;data mining;production system;algorithm	AI	-28.367403166850476	19.928870465734356	34934
bcb41bf74a444463b5a2f1a8eb5ee0339ef78a46	language engineering in the context of a popular, inexpensive robot platform	compiler construction;educational robotics;language translation;language engineering;teaching practice	Language engineering - the theory and practice of building language processors and compilers, has long been recognized as important subject in Computer Science curricula. However, due to lack of suitable target systems, educators face significant challenges to teach language engineering classes effectively.  Leveraging the emerging inexpensive robot devices, this paper presents a new approach of using robots as system context to teach language engineering topics. We designed the Chirp-Scribbler Language, which targets the popular Scribbler robot; combined together, they provide an engaging and feature-rich platform to teach a wide range of topics in language engineering.  This paper describes the Chirp-Scribbler Language, its integration with the target robot, and the teaching practice of using them to teach language translation basics in an undergraduate programming course.	central processing unit;chirp;compiler;computer science;robot;scribbler (robot);software feature	Li Xu	2008		10.1145/1352135.1352154	natural language processing;first-generation programming language;natural language programming;very high-level programming language;universal networking language;computer science;programming language implementation;language industry;software engineering;compiler construction;multimedia;modeling language;programming language;language technology;educational robotics	AI	-29.15339013023629	22.923319386058225	34951
092eab775a734bfe2ab111c3313ccb0f9d432ffc	synthesis of composition and discrimination operators for divide-and-conquer logic programs	logic programs;divide and conquer	This chapter gives a brief overview of our framework for stepwise synthesis of logic programs from examples and properties. Directives are extracted for the development of a particular synthesis mechanism whose steps are guided by a divide-and-conquer schema. It features deductive and inductive reasoning. Examples and properties are presented to it in a non-incremental fashion. The objectives and methods of its last steps (synthesis of composition and discrimination operators) are formalized, and illustrated on some sample problems. This chapter is organized as follows. After the introduction, three sample problems are presented in section 1.2. Sections 1.3 to 1.5 present the objectives and methods of some synthesis steps, and illustrate them on the sample problems. Some conclusions on the results are drawn in section 1.6, related work is stated, and future research directions are outlined.	algorithm;database schema;inductive reasoning;john d. wiley;linear algebra;logic programming;non-repudiation;program synthesis;prolog;quantum information;stepwise regression;synapse	Pierre Flener;Yves Deville	1991			theoretical computer science;mathematics;algorithm	AI	-16.878340218570454	17.737498122696014	34960
bf36982be8a3144806ecf74537e1342577cfe3e7	irmis: the care and feeding of a generalized relatively relational database for accelerator components with a connection to the real time epics input output controllers	software;control systems;high energy physics instrumentation computing;epics relational database acclerator light source synchrotron;epics;synchrotrons control engineering computing high energy physics instrumentation computing meta data real time systems relational databases;relational databases crawlers software control systems user interfaces real time systems;relational database;synchrotrons;light source;acclerator;machine metadata irmis generalized relatively relational database real time epics input output controllers automated process synchrotron component data generation accelerator component data generation;synchrotron;crawlers;meta data;control engineering computing;relational databases;user interfaces;real time systems	IRMIS: The care and feeding of a generalized relatively relational database for accelerator components with a connection to the real time EPICS Input output controllers. This paper describes a relational database approach to documenting and maintaining; the feeding. It describes the automated process used to generate accelerator or synchrotron component data for the relational tables and the role of devices and components. The data this obtained turn may be used or presented in a variety of ways to the end use in order to either optimize the maintenance or to provide machine metadata for experimental performance purposes.	input/output;relational database;software documentation	Richard L. Farnsworth;Scott Benes	2012	2012 IEEE 8th International Conference on E-Science	10.1109/eScience.2012.6404469	real-time computing;computer science;operating system;database;database design	DB	-33.52995133216869	17.196480476303815	35072
67225b55a4fbd58eab4a013c808b7d0007adc01c	equivalence of keyed relational schemas by conjunctive queries	schema transformation;conjunctive queries;schema integration;data model;database design	The concept of two schemas being equivalent is fundamental to database design, schema integration, and data model translation. An important notion of schema equivalence, query equivalence, was introduced by Atzeni et al., and used to evaluate the correctness of schema transformations. The logically equivalent notion of calculous equivalence, as well as three progressively more general notions of schema equivalence were introduced in 1984 by Hull, who showed that two schemas with no dependencies are equivalent (under all four notions of equivalence) if and only if they are identical (up to renaming and re-ordering of attributes and relations). Hull also conjectured that the same result holds for schemas with primary keys. In this work, we resolve the conjecture in the affirmative for the case of query equivalence based on mappings using conjunctive relational queries with equality selections.	conjunctive query;correctness (computer science);data integrity;data model;database design;database schema;referential integrity;relational algebra;relational database;turing completeness;unique key	Joseph Albert;Yannis E. Ioannidis;Raghu Ramakrishnan	1999	J. Comput. Syst. Sci.	10.1006/jcss.1999.1628	logical equivalence;discrete mathematics;semi-structured model;logical schema;data model;computer science;theoretical computer science;star schema;mathematics;conjunctive query;database schema;algorithm;database design	DB	-25.972306510989355	10.528601768743531	35085
3482212af098bf1b7d768c64e6d34b66c214b5ac	the combination of specifications and the induced relations in object oriented programs	object oriented programming			G. Steve Hirst;T. B. Dinesh	1991			method;computer science;programming language;object-oriented programming	Logic	-29.601738832926554	26.778346342869813	35099
7a046a24480801f763a8d305e26b2b4ec76e637c	abstractions for biomolecular computations	deoxyribonucleic acid;theoretical analysis;information processing;molecular computing	Deoxyribonucleic acid is increasingly been understood to be an informational macromolecule, capable of information processing. It has found application in the determination of non-deterministic algorithms and in the design of molecular computing devices. This is a theoretical analysis of the mathematical properties and relations of the molec ules c ons tituting DN A, which e xplains in pa rt w hy DN A is a suc ce ssful comp uting molecule. Intro duction. DNA computing has become an established field within computer science; and several molecular computations and devices have been made 1,4,8,9 , from operations on DNA molecules. A close study of the coding systems of DNA alerts us to the caveats of purely computational methods 5 , the most obvious of which might be the property of degeneracy 2. Our accumulated knowledge about biomolecular computing has not been matched by an und ersta nding, in compute r science terms, of the pr inciples, which go vern them 7. By build ing o n what we know about biomolecular systems based on the principles of molecular biology, we can create mathematical abstractions to explain why DNA molecules are accurate in information processing and computation. An abstraction – a mapping from a real-world domain int o a mathematic al domain – highlights e ssential pro pert ies w hile igno ring o ther, complicating o ne s 7. Computer science can provide the much needed abstraction for biomolecular systems 7. A good scientific abstraction has four pro pert ies: it is r elevant, c ap turing the es se ntial pro pe rty o f the p heno menon; computable, bringing to bear computational knowledge about the mathematical representation; understandable, offering a conceptual framework for thinking about the scientific domain; and extensible, allowing the capture of additional real properties in the same mathematical framework 7. The bulk of DNA computations and devices show the capacity of DNA molecules to crunc h a lgo rit hms and perfo rm lo gic al opera tio ns : a kind of minia ture arit hme tic and logic al unit pro cess or. Us ing t he principles o f molecula r b iology a nd mathematic s, an abstraction or framework showing detailed algorithmic and logic operations encoded in DNA is proposed.	algorithm;computable function;computation;computer science;dna computing;degeneracy (graph theory);information processing;jaishankar menon;program evaluation and review technique	Babatunde O. Okunoye	2008	CoRR		information processing;computer science;bioinformatics;theoretical computer science;dna;algorithm	Theory	-20.771555499765242	13.312599019255416	35121
9cb689205894224a6a12b95ddfc2431f9a02a260	page-query compaction of secondary memory auxiliary databases	databases;performance query optimization;relational data;caching;secondary storage;query optimization;system performance;point of view;page query;compact scheme	Prestoring redundant data in secondary memory auxiliary databases is an idea that can often yield improved retrieval performance through better clustering of related data. The clusters can be based on either whole query results or, as this paper indicates, on more specialized units called page-queries. The deliberate redundancy introduced by the designer is typically accompanied by much unnecessary redundancy among the elements of the auxiliary database. This paper presents algorithms for efficiently removing unwanted redundancy in auxiliary databases organized into page-query units. The algorithms presented here extend prior work done for secondary memory compaction in two respects: First, since it is generally not possible to remove all unwanted redundancies, the paper shows how can the compaction be done to remove the most undesirable redundancy from a system performance point-of-view. For example, among the factors considered in determining the worst redundancies are the update behavior and the effects of a particular compaction scheme on memory utilization. Second, unlike traditional approaches for database compaction which aim merely at reducing the storage space, this paper considers the paging characteristics in deciding on an optimal compaction scheme. This is done through the use of page-queries. Simulation results are presented and indicate that page-query compaction results in less storage requirements and more time savings than could be obtained by standard non-page-query compaction.	algorithm;auxiliary memory;cluster analysis;data compaction;database;paging;redundancy (engineering);requirement;simulation	Nabil Kamel	1994	Distributed and Parallel Databases	10.1007/BF01265320	auxiliary memory;query optimization;data compaction;relational database;computer science;theoretical computer science;operating system;data mining;database;computer performance	DB	-28.476836345334934	4.529770709365454	35126
6adf50edde497b7dcfcdf6882f90695d2557f3e8	supporting complex objects in a relational system for engineering databases		Relational databases are of increasing interest for applications outside the traditional business data-processing environment. We have introduced the notion of complex objects as one extension to a relational database system (System R) to better support these non-business applications, in particular, engineering design. In this chapter, we discuss five aspects of complex objects: concepts and semantics, implementation, query optimization, efficient use within application programs, and user interface.	relational database management system	Raymond A. Lorie;Won Young Kim;Dan McNabb;Wil Plouffe;Andreas Meier	1985			object-orientation	DB	-30.541440544049866	10.918697651318952	35145
73793f7225d679f3f0be9611944136f145cfff6a	specification-guided safety verification for feedforward neural networks		This paper presents a specification-guided safety verification method for feedforward neural networks with general activation functions. As such feedforward networks are memoryless, they can be abstractly represented as mathematical functions, and the reachability analysis of the neural network amounts to interval analysis problems. In the framework of interval analysis, a computationally efficient formula which can quickly compute the output interval sets of a neural network is developed. Then, a specification-guided reachability algorithm is developed. Specifically, the bisection process in the verification algorithm is completely guided by a given safety specification. Due to the employment of the safety specification, unnecessary computations are avoided and thus computational cost can be reduced significantly. Experiments show that the proposed method enjoys much more efficiency in safety verification with significantly less computational cost.		Weiming Xiang;Hoang-Dung Tran;Taylor T. Johnson	2018	CoRR			Logic	-9.260467761366462	29.16106142904018	35231
837390e5bb662629c196c26cfe790f805b2cce0d	user-centered computer science - high-ceiling and low-floor approaches to programming languages and algorithms		Understanding computer science algorithms is one of the steepest obstacles when learning computational science. In this paper I will describe a novel approach to learning standard programming languages and basic computer science algorithms that is based on BYOB, one of the more relevant extension of Scratch, a programming tool developed by MIT. In the proposed approach, students can build several algorithms by themselves without having to cope with all the knowledge about programming usually needed when using a standard programming language.	accessibility;algorithm;c++;compiler;computational science;computer programming;computer science;concurrency (computer science);drag and drop;high-level programming language;programming language;programming tool;sorting algorithm	Stefano Federici	2011			programming language;comparison of multi-paradigm programming languages;computer science;theoretical computer science;programming language theory;fourth-generation programming language;second-generation programming language;extensible programming;third-generation programming language;fifth-generation programming language;computer programming	PL	-28.875377181494954	24.812717307765766	35290
512051609d8deea68bbaa16fe3aab0abf42e0707	improving functional programming environments		Recom Technologies, NASA Ames Research Center, Moffet Field, m/s 269-2, CA 94040 USA EMail: jonathw@ptolemy.arc.nasa.gov URL: http://ase.arc.nasa.gov/whittle This paper presents a novel program editor, CYNTHIA, for the functional programming language ML. Motivated by the current lack of good programming environments for functional programming, CYNTHIA incorporates programming by analogy, whereby users write programs by applying abstract transformations toexisting programs, and sophisticated correctness-checking techniques such as checking for semantic errors, e.g., non-termination. CYNTHIA has been used in two introductory courses in ML and the results of these evaluations are presented here. It was found that students usingCYNTHIA committed fewer errors than students using a compiler / text editor approach. In addition, errors that were made could be corrected mor e easily.	compiler;correctness (computer science);divergence (computer science);email;functional programming;http 404;programming language;text editor	Jon Whittle	1999			functional reactive programming;human–computer interaction;computer science;functional programming	PL	-29.34399713090688	24.17127227781947	35314
239e03d1cb17090fb84a75cca27b9853cbea8164	a semantic framework for proof evidence	proof certificates;proof checking;focused proof systems;classical and intuitionistic logics;foundational proof certificates	Theorem provers produce evidence of proof in many different formats, such as proof scripts, natural deductions, resolution refutations, Herbrand expansions, and equational rewritings. In implemented provers, numerous variants of such formats are actually used: consider, for example, such variants of or restrictions to resolution refutations as binary resolution, hyper-resolution, ordered-resolution, paramodulation, etc. We propose the foundational proof certificates (FPC) framework for defining the semantics of a broad range of proof evidence. This framework allows both producers of proof certificates and the checkers of those certificates to have a clear formal definition of the semantics of a wide variety of proof evidence. Employing the FPC framework will allow one to separate a proof from its provenance and to allow anyone to construct their own proof checker for a given style of proof evidence. The foundation on which FPC relies is that of proof theory, particularly recent work into focused proof systems: such proof systems provide protocols by which a checker extracts information from the certificate (mediated by the so called clerks and experts) as well as performs various deterministic and non-deterministic computations. While we shall limit ourselves to first-order logic in this paper, we shall not limit ourselves in many other ways. The FPC framework is described for both classical and intuitionistic logics and for proof structures as diverse as resolution refutations, natural deduction, Frege proofs, and equality proofs.	automated proof checking;computation;first-order logic;first-order predicate;frege system;natural deduction;resolution (logic)	Zakaria Chihani;Dale Miller;Fabien Renaud	2016	Journal of Automated Reasoning	10.1007/s10817-016-9380-6	formal proof;direct proof;proof by contradiction;discrete mathematics;computer-assisted proof;probabilistically checkable proof;computer science;analytic proof;proof theory;mathematics;constructive proof;mathematical proof;proof assistant;structural proof theory;proof complexity;algorithm	Logic	-15.749146118384884	16.723361306920747	35378
a67344f0d217565f2b79339157039ccfb618747b	a flexible cost model for abstract object-oriented database schemas	machine abstraite;database system;proceso concepcion;design process;cost function;maquina abstracta;pregunta documental;funcion coste;abstract machine;question documentaire;base donnee orientee objet;query;fonction cout;object oriented databases;object oriented database;cost model;processus conception	Typically, the design of an object-oriented database schema starts with an analysis of the application and ends with the implementation of the application. We advocate a design process that employs an intermediate phase where the designer can choose between different abstract object-oriented database schemas. This choice influences the space and time costs that arise when the schema is implemented. We present a cost model for abstract object-oriented database schemas that allows the designer to estimate these costs. At the core of the cost model is an abstract object-oriented database machine. Access structures that are used by this abstract database machine are given by an internal schema. With this we can estimate the space costs. Queries and updates are expressed as programs of the abstract database machine. By providing cost functions that characterise cost relevant aspects of the operations of the abstract database machine we can estimate the time costs of the machine programs. Our cost model is parameterised. So, for example, it can be adopted to reflect different implementation database systems. We provide an example to show how the preferred choice of an abstract database schema changes when the parameters of the cost model vary.		Joachim Biskup;Ralf Menzel	2002		10.1007/3-540-45816-6_40	cost database;schema migration;database theory;simulation;design process;semi-structured model;database tuning;computer science;conceptual schema;network model;database model;data mining;database;abstract machine;programming language;view;database schema;database testing;database design	DB	-28.86405267301494	11.710930293965713	35428
57c4d271724ad89c2d08a4997a87d0523319b337	a comparison of two modelling paradigms in the semantic web	owl;logic;conceptual modelling;semantic web;classical logic;rdf	Classical logics and Datalog-related logics have both been proposed as underlying formalisms for conceptual modelling in the context of the Semantic Web. Although these two different formalism groups have some commonalities, and look similar in the context of expressively impoverished languages like RDF, their differences become apparent at more expressive language levels. After considering some of these differences, we argue that, although some of the characteristics of Datalog have their utility, the open environment of the Semantic Web is better served by standard logics.	semantic web	Peter F. Patel-Schneider;Ian Horrocks	2007	J. Web Sem.	10.1016/j.websem.2007.09.004	natural language processing;classical logic;description logic;semantic web rule language;computer science;artificial intelligence;semantic web;rdf;social semantic web;database;programming language;logic	Web+IR	-21.40566199270376	9.11805159464645	35437
3ebe65ef5f4aaad300f54fcc0db0c659ed714969	ecoop’ 99 — object-oriented programming		Object-oriented programs [Dahl, Goldberg, Meyer] are notoriously prone to the following kinds of error, which could lead to increasingly severe problems in the presence of tasking 1. Following a null pointer 2. Deletion of an accessible object 3. Failure to delete an inaccessible object 4. Interference due to equality of pointers 5. Inhibition of optimisation due to fear of (4) Type disciplines and object classes are a great help in avoiding these errors. Stronger protection may be obtainable with the help of assertions, particularly invariants, which are intended to be true before and after each call of a method that updates the structure of the heap. This note introduces a mathematical model and language for the formulation of assertions about objects and pointers, and suggests that a graphical calculus [Curtis, Lowe] may help in reasoning about program correctness. It deals with both garbage-collected heaps and the other kind. The theory is based on a trace model of graphs, using ideas from process algebra; and our development seeks to exploit this analogy as a unifying principle.	algorithm;automatic programming;c++;canonical account;clos network;coherence (physics);compiler;computer programming;correctness (computer science);debugger;debugging;domain-specific language;feasible region;feature model;graphical user interface;heap (data structure);heart rate variability;intentional programming;interference (communication);invariant (computer science);java;knowledge management;library (computing);mathematical model;mathematical optimization;microsoft outlook for mac;name binding;pointer (computer programming);process calculus;profiling (computer programming);programmer;programming language;programming paradigm;recursion;run time (program lifecycle phase);sequent calculus;smalltalk;software engineering;software portability;template metaprogramming	Jan van Leeuwen;Rachid Guerraoui	1999		10.1007/3-540-48743-3	programming language;object-oriented programming;computer science	PL	-25.36082719138109	29.39364963857762	35451
8b0f9bf525395de5d78342b2ea4e0fb1d73d4496	a collaborative framework for non-linear integer arithmetic reasoning in alt-ergo	non linear arithmetic;interval analysis smt satisfiability modulo theories non linear arithmetic ground completion modulo;ground completion modulo;smt;calculus equations collaboration standards inference algorithms cognition context;satisfiability modulo theories;modulo simple properties collaborative framework nonlinear integer arithmetic reasoning ac x combination method linear integer arithmetic associativity properties commutativity properties nonlinear multiplication interval calculus component linear operations sat solver judicious case splits bounded intervals alt ergo theorem prover deductive program verification;theorem proving arithmetic computability groupware inference mechanisms integration mathematics computing program verification;interval analysis	In this paper, we describe a collaborative framework for reasoning modulo simple properties of non-linear integer arithmetic. This framework relies on the AC(X) combination method and on interval calculus. The first component is used to handle equalities of linear integer arithmetic and associativity and commutativity properties of non-linear multiplication. The interval calculus component is used - in addition to standard linear operations over inequalities - to refine bounds of non-linear terms and to inform the SAT solver about judicious case-splits on bounded intervals. The framework has been implemented in the Alt-Ergo theorem prover. We show its effectiveness on a set of formulas generated from deductive program verification.	algorithm;alt-ergo;automated theorem proving;boolean satisfiability problem;ergo proxy;experiment;formal verification;interval arithmetic;modulo operation;neural impulse actuator;nonlinear system;solver	Sylvain Conchon;Mohamed Iguernelala;Alain Mebsout	2013	2013 15th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing	10.1109/SYNASC.2013.29	discrete mathematics;mathematics;programming language;satisfiability modulo theories;algorithm;algebra	Logic	-14.758777553053324	14.892348799589984	35455
e7c3a2f2fbfb765295af3fd78b44d150cd9e19bf	composable trees for configurable behavior	configurable behavior;composable tree	AuguST 2012 | vOL. 55 | NO. 8 | CoMMuniCAtionS oF thE ACM 7 I cOncur whOleheartedly with the composability benefits Brian Beckman outlined in his article “Why LINQ Matters: Cloud Composability Guaranteed” (Apr. 2012) due to my experience using composability principles to design and implement the message-dissemination mechanism for a mobile ad hoc router in a proprietary network. In it, the message-dissemination functionality of the router emerges from the aggregation of approximately 1,500 nodes in a composable tree that resembles a large version of the lambda-tree diagrams in the article. However, instead of being LINQ-based, each node represents a control element (such as if/else, for-loop, and Boolean operations nodes), as well as nodes that directly access message attributes. Each incoming message traverses the composable tree, with control nodes directing it through pertinent branches based on message attributes (such as message type, timestamp, and sender’s location) until the message reaches processing nodes that complete the dissemination. Since assembling and maintaining a 1,500-node tree within the code base would be daunting, a parser assembles the tree from a 1,300-line routing-rule specification based on a domainspecific language (DSL). Defining the routing rules through this DSL-assembled composable tree also provides these additional benefits: Nodes verified independently. Verifying the if/else, message-timestamp, and other nodes can be done in isolation; Routing rules modified for unit testing. As the routing rules mature, their execution requires a full labor fieldconfiguration environment, making it difficult to test new features; a quick simplification of a local copy of the DSL specification defines routing rules that bypass irrelevant lab/field constraints while focusing on the feature being tested on the developer’s desktop;	brian;communications of the acm;composability;diagram;digital subscriber line;for loop;hoc (programming language);language integrated query;level of detail;parse tree;relevance;router (computing);routing;unit testing	CACM Staff	2012	Commun. ACM	10.1145/2240236.2240238	theoretical computer science;computer science	Security	-28.058640031317108	26.393914849572397	35484
a639097b26fe7d66621c221c1f03d045ea00cb41	modelling prolog control	operational semantics;depth first search;fixed point	The goal of this paper is to construct a semantic basis for the abstract interpretation of Prolog programs. Prolog is a well-known logic programming language which applies a depth-rst search strategy in order to provide a practical approximation of Horn clause logic. While pure logic programming has clean x-point, model-theoretic and operational semantics the situation for Prolog is diierent. Diiculties in capturing the declarative meaning of Prolog programs have led to various semantic deenitions which attempt to encode the search strategy in diierent mathematical frameworks. However semantic based analyses of Pro-log are typically achieved by abstracting the more simple but less precise declarative semantics of pure logic Programs. We propose instead to model Prolog control in a simple constraint logic language which is presented together with its declarative and operational semantics. This enables us to maintain the usual approach to declarative semantics of logic programs while capturing control aspects such as search strategy and selection rule.	abstract interpretation;approximation;constraint algorithm;declarative programming;encode;horn clause;logic programming;operational semantics;programming language;prolog;selection rule;theory	Roberto Barbuti;Michael Codish;Roberto Giacobazzi;Giorgio Levi	1991		10.1093/logcom/3.6.579	constraint programming;horn clause;stable model semantics;breadth-first search;computer science;theoretical computer science;semantics;fixed point;definite clause grammar;datalog;programming language;axiomatic semantics;well-founded semantics;prolog;logic programming;operational semantics;algorithm	PL	-17.990104055314365	20.97010455676807	35527
3e3106841525d0b7ff96cc2588af65d1d189c19f	soa based dynamic image processing	image segmentation;image processing;programming language;soa;web service;dynamic linking;image enhancement;feature extraction;service oriented architecture;noise removal	This paper presents the design of a Service Oriented Architecture (SOA) -based platform for image processing on web. In service oriented architectures, the most important element is the service, a resource provided to remote clients via a service contract. SOA-based systems may prove to be a good solution to address problems like large storage and expensive computational requirements faced in image processing applications. We propose a component-based platform which is not tied to a specific programming language or a specific technology. The Web service provides various functions of pre-processing, noise removal, segmentation and feature extraction. Each phase in image processing system is viewed as a Web service operated on the Internet. Each algorithm is implemented as a Web service. Set of algorithms are implemented for each phase and the phases of image processing are linked dynamically. The dynamically linking of web service is based on the quality defining attributes of the input image.	algorithm;component-based software engineering;dynamic linker;feature extraction;image processing;internet;preprocessor;programming language;requirement;service-oriented architecture;web service;world wide web	M. Karajgi;R. Sawant	2010		10.1145/1741906.1742075	web service;computer science;digital image processing;database;multimedia;world wide web	Web+IR	-33.68343928844436	23.87576898190023	35545
abf3ca8b921790591f667797ecd309f2a7665a38	formal semantics for the automated derivation of micro-code	transformation rule;requirement specification;primary goal;machine-independent microprogram synthesis;automated derivation;micro-instruction set processor model;synthesis system;formal semantics;integral part;microprogramming;mathematics;cad;design automation;computer science;satisfiability;application software;computer languages;user interface	A semantics based scheme for use in machine-independent microprogram synthesis is described. The input to the synthesis system consists of a micro-instruction set processor model and requirement specifications. Validity and transformation rules which are an integral part of the system are used in an attempt to prove the existence of a microprogram satisfying the requirements and if feasible such a microprogram can be extracted directly from the proof. The primary goal of this paper is to describe a system of validity and transformation rules for use in such a synthesis system.	floor and ceiling functions;microcode;requirement	Robert A. Mueller;Joseph Varghese	1982	19th Design Automation Conference	10.1145/800263.809295	embedded system;application software;electronic design automation;computer science;theoretical computer science;operating system;formal semantics;cad;microcode;programming language;user interface;algorithm;satisfiability	EDA	-15.16724956696947	29.49127084087967	35550
da198bf7e42e3af284ba8b238deb3ca6c68a0a37	asynchronous links in the pbc and m-nets	high level petri net;red petri;programacion paralela;semantics;parallel programming;specification programme;program verification;timed specification;verificacion programa;informatique theorique;m net;analisis semantico;analyse semantique;verification programme;petri net;program specification;especificacion programa;reseau petri;semantic analysis;programmation parallele;computer theory;petri box calculus;informatica teorica;time constraint	This paper aims at introducing an extension of M-nets, a fully compositional class of high-level Petri nets, and of its low-level counter part, Petri Boxes Calculus (PBC). We introduce a new operator with nice algebraic properties which allows to express asynchronous communications in a simple and flexible way. With this extension, asynchronous communications become at least as simple to express as (existing) synchronous ones. Finally, we show how this extension can be used in order to specify systems with timing constraints.	high- and low-level;linear algebra;periodic boundary conditions;petri net	Hanna Klaudel;Franck Pommereau	1999		10.1007/3-540-46674-6_17	real-time computing;computer science;theoretical computer science;semantics;petri net;algorithm	AI	-24.6953625814577	31.382898374932182	35585
32f16885641ace1550765d814d1be34513d7230c	complexity of bidirectional data flow analysis.	partial redundancy elimination;information flow;round robin;directional data;problem complexity;data flow analysis;data flow;iteration method;flow analysis;interval analysis	The concept of an information flow path arising from the generalized theory of data flow analysis [21] is used to analyze the complexity of data flow analysis. The width (w) of a program flow graph with respect to a class of data flow problems is introduced as a measure of the complexity of round-robin iterative analysis. This provides the first known complexity result for round robin iterative analysis of bidirectional data flows commonly used in algorithms based on the suppression of partial redundancies [6, 7, 8, 9, 17, 18, 25]. We also show that width provides a better bound on the complexity of unidirectional data flows than the classical notion of depth. The paper presents ways to reduce the width, and thereby the complexity of flow analysis, for several interesting problems. Complexity analysis using the notion of width is also shown to motivate efficient solution methods for various bidirectional problems, viz. The alternating iterations method, and an interval analysis based elimination method for the partial redundancy elimination problems. The paper also presents a condition for the decomposability of a bidirectional problem into a sequence of unidirectional problems.	algorithm;control flow;data-flow analysis;dataflow architecture;interval arithmetic;iteration;partial redundancy elimination;round-robin scheduling;viz: the computer game;zero suppression	Dhananjay M. Dhamdhere;Uday P. Khedker	1993		10.1145/158511.158696	mathematical optimization;minimum-cost flow problem;computer science;data-flow analysis;worst-case complexity;programming language;algorithm	PL	-13.939601757559043	23.265588130827034	35597
8a5f4255840cc02b847f8e882f72bc437ad83554	polynomial-time inference of all valid implications for horn and related formulae	resolution;complexity;propositional logic;polynomial time;conjunctive normal form;horn formulae;inference	This paper investigates the complexity of a general inference problem: given a propositional formula in conjunctive normal form, find all prime implications of the formula. Here, a prime implication means a minimal clause whose validity is implied by the validity of the formula. We show that, under some reasonable assumptions, this problem can be solved in time polynomially bounded in the size of the input and in the number of prime implications. In the case of Horn formulae, the result specializes to yield an algorithm whose complexity grows only linearly with the number of prime implications. The result also applies to a class of formulae generalizing both Horn and quadratic formulae.	algorithm;conjunctive normal form;horn clause;time complexity	Endre Boros;Yves Crama;Peter L. Hammer	1990	Annals of Mathematics and Artificial Intelligence	10.1007/BF01531068	time complexity;conjunctive normal form;discrete mathematics;complexity;horn-satisfiability;resolution;computer science;mathematics;propositional calculus;algorithm	AI	-7.78204760539767	17.77919852522556	35604
aff925a08dd72dfc9bfa76218950394e7d522426	timed presence extensions to the presence information data format (pidf) to indicate status information for past and future time intervals	presence information data format	The Presence Information Data Format (PIDF) defines a basic XML format for presenting presence information for a presentity. This document extends PIDF, adding a timed status extension (  element) that allows a presentity to declare its status for a time interval fully in the future or the past.	presence information	Henning Schulzrinne	2006	RFC	10.17487/RFC4481	computer science;data mining;database;information retrieval	HCI	-25.96522091567562	6.454284526802975	35607
bd1398a696db75c9318faed374fb47aa449852be	coloured petri nets and the invariant-method	coloured petri net	Abstract   In many systems a number of different processes have a similar structure and behaviour. To shorten system description and system analysis it is desirable to be able to treat such similar processes in a uniform and succinct way. In this paper it is shown how Petri nets can be generalized to allow processes to be described by a common subnet, without losing the ability to distinguish between them. Our generalization, called coloured Petri nets, is heavily influenced by predicate transition-nets introduced by H.J. Genrich and K. Lautenbach. Moreover our paper shows how the invariant-method, introduced for Petri nets by K. Lautenbach, can be generalized to coloured Petri nets.	coloured petri net	Kurt Jensen	1981	Theor. Comput. Sci.	10.1016/0304-3975(81)90049-9	discrete mathematics;stochastic petri net;computer science;artificial intelligence;mathematics;process architecture;petri net;algorithm	ECom	-9.34999640032051	21.34812235726818	35640
87efbf5c9725b67352b5f5733c049f7249b2be97	sometimes an fexpr is better than a macro	embedded language;efficient implementation	Common Lisp, which is becoming THE Lisp standard, does not support call by text (FEXPR mechanism in Mac/Franz Lisp). This effect can be obtained using macros. Based on the experience of converting an OPS5 implementation from Franz Lisp to Common Lisp, it is argued that sometimes call by text is needed for efficiency, despite its negative aspects.In the case of languages embedded in a Lisp system, using the macro alternative for call by text can cause marco expansion at execution time. This leads to a some-what less efficient implementation of the embedded language.	common lisp;embedded system;fexpr;franz lisp;inline expansion;ops5;run time (program lifecycle phase)	Zavdi L. Lichtman	1986	SIGART Newsletter	10.1145/15719.15720	exception handling;parallel computing;interpreter;computer science;fexpr;lisp;database;programming language;preprocessor	PL	-23.804235904292337	24.38430322493151	35645
dee94dfc59e8c4cefeb1c6c34a3ee631d4803bc3	translating relational & object-relational database models into owl models	databases;heuristic database modeling technique owl model relational database model object relational database model ontology model information extraction dbms database management system integrity constraint;owl;modeling technique;data integrity;information extraction;relational databases data integrity information retrieval knowledge representation languages meta data object oriented databases ontologies artificial intelligence;information retrieval;relational database;relational databases owl ontologies object oriented modeling data models data mining costs computer errors computer science data analysis;data mining;ontologies artificial intelligence;data model;heuristic database modeling technique;knowledge representation languages;relational database model;object relational database model;computational modeling;owl model;object relational databases;reverse engineering data models ontology relational database object relational database owl;integrity constraint;object relational database;integrity constraints;meta data;ontologies;relational databases;dbms;object oriented databases;database management system;ontology model;ontology;object oriented modeling;reverse engineering;data models	We describe an extensible framework for translating data models into Ontology models. Initially, the framework addresses two types of source data models: the Relational Database (RDB) and Object-Relational Database (ORDB) models. The derived Ontology model is based on OWL. The framework extracts information about the source data models from the metadata maintained by the DBMS. The extracted metadata includes most of the integrity constraints that are typically maintained by a DBMS. The extracted metadata is then analyzed to identify Ontology concepts, properties, and explicit relationships, discover redundant Ontology concepts and implicit relationships, and identify restrictions on properties and relationships. The analysis is based on heuristic database modeling techniques. The analyzed data is automatically translated into a rudimentary OWL Ontology model that can be enhanced by Ontology modelers. The paper provides examples to demonstrate how the translation is conducted.	amiga rigid disk block;data integrity;data model;database model;heuristic;object-relational database;relational database;source data;web ontology language	Khalid M. Albarrak;Edgar H. Sibley	2009	2009 IEEE International Conference on Information Reuse & Integration	10.1109/IRI.2009.5211575	upper ontology;bibliographic ontology;computer science;ontology;ontology;data integrity;data mining;database;ontology-based data integration;owl-s;information extraction;information retrieval;process ontology;metadata repository;suggested upper merged ontology	DB	-31.82320194782925	10.55616400321948	35658
ee7ccae848b8c66877ff20a29a34ab15762ec2bc	the existential fragment of s1s over element and successor is the co-buchi languages		Büchi’s theorem, in establishing the equivalence between languages definable in S1S over pP,ăq and the ω-regular languages also demonstrated that S1S over pP,ăq is no more expressive than its existential fragment. It is also easy to see that S1S over pP,ăq is equi-expressive with S1S over pP, sq. However, it is not immediately obvious whether it is possible to adapt Büchi’s argument to establish equivalence between expressivity in S1S over pP, sq and its existential fragment. In this paper we show that it is not: the existential fragment of S1S over pP, sq is strictly less expressive, and is in fact equivalent to the co-Büchi languages. 1 Preliminaries 1.1 Second order theory of one successor Definition 1 (S1S syntax). We introduce the following components: • A set of first order variables, denoted by lower case letters, possibly with subscripts. • A set of second order variables, denoted by upper case letters, possibly with subscripts. S1SpP,ăq has the following set of well formed formulae: φ ::“ t ă t | t P X | φ^ φ | φ | Dxiφ | DXiφ. t is understood to range over terms, which in this case are just the first order variables, and X to range over second order variables.	turing completeness;well-formed formula	Egor Ianovski	2014	CoRR			Logic	-6.4612966876245395	14.304817070647662	35662
cd3f433a62b5376f1bf24783a3454f35c4102268	a rewriting logic for declarative programming	declarative programming;rewriting logic;model theory	We propose a new approach to declarative programming which integrates the functional and relational paradigms by taking possibly non-deterministic lazy functions as the fundamental notion. Programs in our paradigm are theories in a constructor based conditional rewriting logic. We present proof calculi and a model theory for this logic, and we prove the existence of free models which provide an adequate intendeded semantics for programs. We observe that some narrowing strategies are unsound w.r.t. our semantics, and we develop a sound and complete lazy narrowing calculus.	constructor (object-oriented programming);declarative programming;lazy evaluation;programming paradigm;proof calculus;rewriting	Juan Carlos González Moreno;Maria Teresa Hortalá-González;Francisco Javier López-Fraguas;Mario Rodríguez-Artalejo	1996		10.1007/3-540-61055-3_35	constraint programming;declarative programming;horn clause;rewriting;computer science;functional logic programming;programming paradigm;procedural programming;inductive programming;datalog;fifth-generation programming language;programming language;prolog;logic programming;model theory	PL	-16.531086787161048	19.262266461314944	35670
067914cafc1a016035e6a99a03eda1a3ae7d26d0	validated templates for specification of complex ltl formulas	pattern;formal specifications;model checking;composite propositions;scope;ltl	Formal verification approaches that check software correctness against formal specifications have been shown to improve program dependability. Tools such as Specification Pattern System (SPS) and Property Specification (Prospec) support the generation of formal specifications. SPS has defined a set of patterns (common recurring properties) and scopes (system states over which a pattern must hold) that allows a user to generate formal specifications by using direct substitution of propositions into parameters of selected patterns and scopes. Prospec extended SPS to support the definition of patterns and scopes that include the ability to specify parameters with multiple propositions (referred to as composite propositions or CPs), allowing the specification of sequential and concurrent behavior. Prospec generates formal specifications in Future Interval Logic (FIL) using direct substitution of CPs into pattern and scope parameters. While substitution works trivially for FIL, it does not work for Linear Temporal Logic (LTL), a highly expressive language that supports specification of software properties such as safety and liveness. LTL is important because of its use in the model checker Spin, the ACM 2001 system Software Award winning tool, and NuSMV. This paper introduces abstract LTL templates to support automated generation of LTL formulas for complex properties in Prospec. In addition, it presents formal proofs and testing to demonstrate that the templates indeed generate the intended LTL formulas.	correctness (computer science);dependability;formal proof;formal verification;ibm 1401 symbolic programming system;interval temporal logic;linear temporal logic;liveness;model checking;nusmv;spin;specification pattern	Salamah Salamah;Ann Q. Gates;Vladik Kreinovich	2012	Journal of Systems and Software	10.1016/j.jss.2012.02.041	model checking;real-time computing;computer science;formal specification;pattern;programming language;algorithm	SE	-18.5023803279027	27.855299950864925	35681
db416a904942ac6744f078bec62de3c36db058c5	automatic structure extraction from mpi applications tracefiles	internal structure;performance analysis;message passing	The process of obtaining useful message passing applications tracefiles for performance analysis in supercomputers is a large and tedious task. When using hundreds or thousands of processors, the tracefile size can grow up to 10 or 20 GB. It is clear that analyzing or even storing these large traces is a problem. The methodology we have developed and implemented performs an automatic analysis that can be applied to huge tracefiles, which obtains its internal structure and selects meaningful parts of the tracefile. The paper presents the methodology and results we have obtained from real applications. 1 Motivation and Goal In the recent years, parallel platforms have amazingly increased in performance and in number of nodes and processors. Thus, the study of the execution of applications in these platforms has become a hard and tedious work. A complete timestamped sequence of events of an application, that is, a tracefile of the whole application, results in a huge file (10-20 GB). It is impossible to handle this amount of data with tools like Paraver [1]. Also, often, some parts of the trace are perturbed, and the analysis of these parts can be misleading. A third problem is the identification of the most representative regions of the tracefile. To reduce tracefiles sizes, the process of application tracing must be carefully controlled, enabling the tracing in the interesting parts of the application and disabling otherwise. The number of events of the tracefile (hardware counters, instrumented routines, etc...) must be limited. This process is tedious and large and requires knowledge on the source code of the application. For these reasons, several authors [8, 9] believe that the development and utilization of trace based techniques is not useful. However, techniques based on tracefiles allow a very detailed study of the variations on space (set of processes) and time that could affect notably the performance of the application. Therefore, there is a need for developing techniques that allow to handle large event traces. The goal of our approach is to start from very large tracefiles of the whole application, allowing simple tracing methodologies, and then analyzing them automatically. The underlying philosophy is to use resources that are generally Funded by project TIN2004-07739-CO2-01 and by a FPI grant from spanish gov. available (Disk, CPU, ...) in order to avoid spending an expensive resource: analyst time. The tool we have implemented will, first, warn the analyst about those parts of the trace perturbed by an external factor not related to the application or to the machine itself. Second, the tool will give a description of the internal structure of the application and will identify and extract the most relevant parts of the trace. There are other approaches to either avoid or handle large event traces. KOJAK [2] is a tool for automatic detection of performance bottlenecks and inefficient behavior. Our methodology could be applied before KOJAK to reduce the size of the tracefile. VAMPIR Next Generation tool (VNG) [3, 4] consists of two major components: A parallel analysis server and a visualization client, each of them executed on a different platform. An important VNG feature is the utilization of the data structure Complete Call Graph (CCG). It holds the full event stream including time information in a tree. It is also possible to compress the CCG into a compressed Call Graph (cCCG) in order to achieve a compressed representation of trace data. Compression errors can be maintained in a given range. Finally, the main goal of VNG is to make huge event traces accessible for interactive program analysis. Therefore, the VNG approach is different from ours since it does not perform an automatic analysis of the internal structure of the event traces. Related to VNG, there is another tool called DeWiz [5]. It is based, as VNG, on the event graph model. Two important characteristic of DeWiz are modularity, which enables it to be executed in distributed computing infrastructures, and automatic analysis, which enables it to detect significative information on the event graph. However, DeWiz is unable to find event based structure since it works using graph based methods. In that context, our work satisfies the need for an automatic performance analysis based on structural properties of the application. Furthermore, these structural properties of the application are event based and, for that reason, they have clear physical meaning. Other previous work in [6] presents a proposal for dynamic periodicity detection of iterations in parallel applications. The paper is organized as follows: First, an explanation of the methodology we have developed and implemented is presented in Section 2. Next, a presentation of the results we have obtained using this methodology is described in section 3. Finally, conclusions and future work are shown in Section 4.	bottleneck (software);cpu cache;call graph;central processing unit;data structure;digital footprint;distributed computing;expert system;fixed-point iteration;idris;interactive computing;location-based service;lunpack;megabyte;message passing in computer clusters;next-generation network;overhead (computing);parallel computing;profiling (computer programming);program analysis;quasiperiodicity;sensor;server (computing);supercomputer;tracing (software);tree (data structure)	Marc Casas;Rosa M. Badia;Jesús Labarta	2007		10.1007/978-3-540-74466-5_2	parallel computing;message passing;computer science;theoretical computer science;operating system;database;distributed computing;programming language	HPC	-19.890798348284648	31.9270633452892	35703
209aaa48ed7d4f4be26d733d552c053337e7ad7e	"""a """"generalized"""" lexical functional grammar-based processing of an indian language - bangla"""	systeme traitement langage naturel;lexical functional grammar;grammaire fonctionnelle lexicale;analizador sintaxico;natural language processing systems;parser;analyseur syntaxique;india;inde	"""An efficient LFG parser implementation of Indian languages in general and Bangla in particular has been discussed. It has been shown that the classical technique of nonconfigurational syntactic encoding principles lead to too many disjunctive constraints to be satisfied by the parser. Noting that most of the disjunctions do not exist if an a priori knowledge of the verb is available, a """"delayed"""" syntactic encoding formalism has been proposed. The points of syntactic encoding of noun phrases have been treated a """"forward references"""" that are to be temporarily maintained in a """"symbol table"""" for later precipitation. The proposed solution has two parts. The first part deals with identification of forward reference points, which is done by introducing a new metavariable and augmenting the scope of the Locate operator. The second part deals with precipitation of forward references through special schemata called m-structure schemata projected by the verb. An extension of the solution to one type of Bangla complex sentences has also been proposed (in the Appendix). Implementation notes based on object-oriented programming principles have been provided."""	lexical functional grammar	Probal Sengupta;Bidyut Baran Chaudhuri	1998	IJPRAI	10.1142/S0218001498000403	natural language processing;speech recognition;computer science;artificial intelligence;lexical functional grammar	NLP	-23.871349341091843	18.915244833316336	35708
29572b09b9ad165f0feb4f8b2e7a15a47a358a8f	stability constraints and stability normal forms for temporal relational databases	stability normal form;relational database;temporal database;stability constraints;relational model;normal form	This paper presents a new dynamic dependency and a new normal form. We give the concept of the stability constraint and establish complete axioms for stability constraints. In addition, we propose a stability normal form. A temporal third normal form can be losslessly decomposed into a sequence of stability constraints. Using the decomposition, the storage space can be reduced. The stability constraints allow to fill a suitable value into a null value. Thus, this null value therefore turns into a known value.	database normalization;relational database	Stephen Shaoyi Liao;Huaiqing Wang;Weiyi Liu	1999	Information & Software Technology	10.1016/S0950-5849(98)00120-7	mathematical optimization;relational model;third normal form;relational database;computer science;database;temporal database	SE	-27.16409654647377	9.793178712012573	35785
9f988862f5625b6fb2f29194e8770bc4764290ba	design and maintenance of data-intensive web sites	base relacional dato;metodologia;red www;concepcion sistema;teleinformatica;ingenieria logiciel;relational database;software engineering;methodologie;hypermedia;teleinformatique;internet;conceptual modelling;modelisation conceptuelle;system design;base donnee relationnelle;genie logiciel;world wide web;reseau www;systeme gestion base donnee;information system;methodology;sistema gestion base datos;database management system;remote data processing;hipermedia;conception systeme;systeme information;sistema informacion	Many Web sites include signi cant and substantial pieces of information, in a way that is often di cult to share, correlate and maintain. In many cases the management of a Web site can greatly bene t from the adoption of methods and techniques borrowed from the database eld. This paper introduces a methodology for designing and maintaining large Web sites based on the assumption that data to be published in the site are managed using a DBMS. We see the process of designing the site as the result of two intertwined activities: the database design and the hypertext design. Each of these is further divided in a conceptual design phase and a logical design phase, based on speci c data models. A new logical data model, called adm, is used to describe the structure of a Web hypertext. It is page-oriented, in the sense that the main construct is the one of page-scheme, providing an intensional description of a class of pages in the site. Based on the adm scheme of the site, we introduce a language, called Penelope, that allows to automatically generate HTML pages starting from the database content. Penelope is also able to correlate di erent pages in a complex hypertext using a suitable URL invention mechanism to guarantee reference integrity. adm and Penelope strongly support site maintenance: the rst provides a concise description of the site structure; it allows to reason about the overall organization of pages in the site, in order to evaluate the e ectiveness and e ciency of the chosen structure, and possibly to restructure it; at the same time, Penelope alleviates the burden of managing HTML les by hand, and guarantees link consistency in presence of updates and reorganizations.	database design;html;hypertext;intensional logic;logical data model;windows nt;world wide web	Paolo Atzeni;Giansalvatore Mecca;Paolo Merialdo	1998		10.1007/BFb0101001	the internet;relational database;computer science;methodology;database;world wide web;information system;systems design	DB	-28.32071757205513	12.353332643928443	35789
d00466734bc6ae2307ab357f21f3f6dc2a37489a	contextual semantics in quantum mechanics from a categorical point of view		The category-theoretic representation of quantum event structures provides a canonical setting for confronting the fundamental problem of truth valuation in quantum mechanics as exemplified, in particular, by Kochen–Specker’s theorem. In the present study, this is realized on the basis of the existence of a categorical adjunction between the category of sheaves of variable local Boolean frames, constituting a topos, and the category of quantum event algebras. We show explicitly that the latter category is equipped with an object of truth values, or classifying object, which constitutes the appropriate tool for assigning truth values to propositions describing the behavior of quantum systems. Effectively, this category-theoretic representation scheme circumvents consistently the semantic ambiguity with respect to truth valuation that is inherent in conventional quantum mechanics by inducing an objective contextual account of truth in the quantum domain of discourse. The philosophical implications of the resulting account are analyzed. We argue that it subscribes neither to a pragmatic instrumental nor to a relative notion of truth. Such an account essentially denies that there can be a universal context of reference or an Archimedean standpoint from which to evaluate logically the totality of facts of nature.	category theory;domain of discourse;kochen–specker theorem;quantum mechanics;value (ethics)	Vassilios Karakostas;Elias Zafiris	2015	Synthese	10.1007/s11229-015-0970-3	discrete mathematics;epistemology;coherence theory of truth;pure mathematics;truth function;mathematics;truth value;categorical quantum mechanics	NLP	-13.313294774226208	5.373322966011864	35791
033567a905e9e04cac43ab606e4463acf21777a9	near-equational and equational systems of logic for partial functions i			formal system	William Craig	1989	J. Symb. Log.			Logic	-12.654318676286465	12.608789362779675	35812
51591e4c24837200ac9c5d967689e9f643dfec2f	keys and armstrong databases in trees with restructuring	functional dependency;numerical necessary condition;closed set;minimal key system;armstrong databases;strong key;necessary condition;minimal key;various constructor;union constructor	The definition of keys, antikeys, Armstrong-instances are extended to complex values in the presence of several constructors. These include tuple, list, set and a union constructor. Nested data structures are built using the various constructors in a tree-like fashion. The union constructor complicates all results and proofs significantly. The reason for this is that it comes along with non-trivial restructuring rules. Also, so-called counter attributes need to be introduced. It is shown that keys can be identified with closed sets of subattributes under a certain closure operator. Minimal keys correspond to closed sets minimal under set-wise containment. The existence of Armstrong databases for given minimal key systems is investigated. A sufficient condition is given and some necessary conditions are also exhibited. Weak keys can be obtained if functional dependency is replaced by weak functional dependency in the definition. It is shown, that this leads to the same concept. Strong keys are defined as principal ideals in the subattribute lattice. Characterization of antikeys for strong keys is given. Some numerical necessary conditions for the existence of Armstrong databases in case of degenerate keys are shown. This leads to the theory of bounded domain attributes. The complexity of the problem is shown through several examples.	armstrong's axioms;candidate key;data structure;functional dependency;graph database;numerical analysis;ring counter;strong key	Attila Sali;Klaus-Dieter Schewe	2008	Acta Cybern.		discrete mathematics;database;mathematics;algorithm	DB	-5.574123137814616	12.262262005930692	35873
643a205459d6618e748d9b8f67ebfdeaa55cec6b	transformational programming: the derivation of a prolog interpretation algorithm	programming language;development process;efficient implementation;software development;logic programs	In this paper we will present a case study for the transformational programming technique. The problem we intend to study is the construction of an interpreter for the logic programming language Prolog. We will start from a formal algebraic semantic definition of Prolog, through a series of transformations, step by step, reach a compact and efficient implementation written in programming language Ada. The whole development process are carried out with the support of a software development system called AUTO STAR (AUTOmatic Specification To Ada Realization).	abstract machine;ada;algorithm;automated theorem proving;interpreter (computing);linear algebra;programming language;prolog;software development	Ming-Yuan Zhu	1989	SIGPLAN Notices	10.1145/68127.68133	first-generation programming language;constraint programming;declarative programming;very high-level programming language;horn clause;constraint satisfaction;programming domain;reactive programming;computer science;programming language implementation;theoretical computer science;software development;functional logic programming;computer programming;programming paradigm;procedural programming;symbolic programming;inductive programming;fifth-generation programming language;programming language;prolog;logic programming;programming language specification;software development process;algorithm	PL	-23.563215730995395	22.703573793651863	35878
0c16ce33246b85727af90b06f94bbac6dd5d460d	query optimization in an oodbms	query optimization	It is clearly crucial for the success of object-oriented databases to find effiĆ cient implementations that improve on the performance of relational sysĆ tems, rather than being powerful in terms of modeling and features, but just too slow to be used. This paper describes the mapping of COCOON to DASDBS, a nested relational database kernel system, as an example OODBMS mapping to a complex storage system. We describe 1) choices for physical designs that make use of the complex storage model and 2) the generation of efficient, set-oriented execution plans for object-oriented database queries, using rule-based query optimization techniques. We use hierarchical clustering and embedded (sets of) object references, and show how to explore them for efficient path traversals expressed in queries inĆ volving complex objects. Prototypes of both, a physical design tool and a query optimizer have been implemented. Preliminary results show feasibilĆ ity, and execution time improvements of an order of magnitude.	cluster analysis;computer data storage;design tool;embedded system;hierarchical clustering;kernel (operating system);logic programming;mathematical optimization;physical design (electronics);program optimization;query optimization;relational database;run time (program lifecycle phase);storage model	Christian Rich;Marc H. Scholl	1993			query optimization;computer science;theoretical computer science;data mining;database	DB	-29.41185635080314	5.943078652217061	35880
94aeaa5ea0707b3ed4e79658cea635c1d71cdf2d	a series of revisions of david poole’s specificity	artificial intelligence;non-monotonic reasoning;defeasible reasoning;specificity;positive-conditional specification;06a06;68t27;68t30;68t37	In the middle of the 1980s, David Poole introduced a semantic, model-theoretic notion of specificity to the artificial-intelligence community. Since then it has found further applications in non-monotonic reasoning, in particular in defeasible reasoning. Poole tried to approximate the intuitive human concept of specificity, which seems to be essential for reasoning in everyday life with its partial and inconsistent information. His notion, however, turns out to be intricate and problematic, which — as we show — can be overcome to some extent by a closer approximation of the intuitive human concept of specificity. Besides the intuitive advantages of our novel specificity orderings over Poole’s specificity relation in the classical examples of the literature, we also report some hard mathematical facts: Contrary to what was claimed before, we show that Poole’s relation is not transitive in general. The first of our specificity orderings (CP1) captures Poole’s original intuition as close as we could get after the correction of its technical flaws. The second one (CP2) is a variation of CP1 and presents a step toward similar notions that may eventually solve the intractability problem of Poole-style specificity relations. The present means toward deciding our novel specificity relations, however, show only slight improvements over the known ones for Poole’s relation; therefore, we suggest a more efficient workaround for applications in practice.	approximation algorithm;artificial intelligence;computation;defeasible reasoning;discrete sine transform;non-monotonic logic;sld resolution;sensitivity and specificity;theory;turing completeness;vertex-transitive graph;workaround;yet another	Claus-Peter Wirth;Frieder Stolzenburg	2015	Annals of Mathematics and Artificial Intelligence	10.1007/s10472-015-9471-9	computer science;artificial intelligence;mathematics;algorithm	AI	-14.941667212061791	6.9638458433090085	35940
a7349c45047f9164560d192157291ef88ab747ae	generalised rely-guarantee concurrency: an algebraic foundation	concurrent kleene algebra;concurrent programming;program verification;rely guarantee concurrency;program algebra	The rely-guarantee technique allows one to reason compositionally about concurrent programs. To handle interference the technique makes use of rely and guarantee conditions, both of which are binary relations on states. A rely condition is an assumption that the environment performs only atomic steps satisfying the rely relation and a guarantee is a commitment that every atomic step the program makes satisfies the guarantee relation. In order to investigate rely-guarantee reasoning more generally, in this paper we allow interference to be represented by a process rather than a relation and hence derive more general rely-guarantee laws. The paper makes use of a weak conjunction operator between processes, which generalises a guarantee relation to a guarantee process, and introduces a rely quotient operator, which generalises a rely relation to a process. The paper focuses on the algebraic properties of the general rely-guarantee theory. The Jones-style rely-guarantee theory can be interpreted as a model of the general algebraic theory and hence the general laws presented here hold for that theory.	carroll morgan (computer scientist);concurrency (computer science);concurrent computing;correctness (computer science);divergence (computer science);goto;hol (proof assistant);interference (communication);isabelle;iteration;jones calculus;kleene algebra;linear algebra;newman's lemma;process (computing);process calculus;refinement (computing);relation (database);relational model	Ian J. Hayes	2016	Formal Aspects of Computing	10.1007/s00165-016-0384-0	discrete mathematics;concurrent computing;computer science;theoretical computer science;mathematics;programming language;algorithm	Logic	-12.29181899859517	21.0769882571099	35954
b56d8db7caade2db9e089e58b91ead7d1ab9a1d1	database theory column: report on pods 2009	database theory column;database theory column report	The 28th edition of the ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Databases (PODS) took place from June 29 to July 1, 2009, in Providence, Rhode Island. The symposium was organized jointly with the ACM SIGMOD International Conference on Management of Data (SIGMOD). PODS focuses on theoretical aspects of data management systems and techniques, and the co-location with SIGMOD stimulates interaction between theory-oriented and system-oriented research. The proceedings of PODS 2009 is published by ACM Press, and can also be found both on the SIGMOD website (http://www.sigmod.org), as well as in the ACM Digital Library (http: //www.acm.org/dl). The conference program included a keynote talk by Raghu Ramakrishnan, two invited tutorials by Leonid Libkin and by Lars Arge, and 26 contributed papers that were selected by the Program Committee from 97 submissions. Similar to past PODS conferences, most of the contributed papers are preliminary reports on work in progress, and many of them will probably appear in more polished and detailed form in scientific journals. In his keynote talk, Raghu Ramakrishnan presented a new paradigm that models the web as a collection of concepts as opposed to (syntactic) documents. While this clearly is a step towards a semantic web, the key technical challenge outline in his talk concerns techniques to automatically construct such a web of concepts. He presented possible approaches to extract information from web documents to form concept instances and links in the concept web. He also discussed several related issues, including search techniques, advertising, and user-centric modeling for the new web. The tutorial by Leonid Libkin gave a nice survey on logic tools and techniques for studying expressiveness of database languages. Expressive power has been an interesting issue in database theory since the 70’s when it was first known that the (graph) transitive closure query cannot be expressed in the relational query languages (the relational algebra and calculus). In the late 80’s to mid 90’s, this was a very fashionable topic for a variety of query languages most of which extend the core relational ones. In these studies, a range of known logic tools were put in use, which include game-based techniques, locality, 0-1 laws, etc. As database languages find their use in many new applications (web, security, and more), these logic tools would certainly help to gain a fundamental understanding of the languages in these specific contexts. They remain quite relevant. The nice summary of these tools as well as illustrations of their use provided in this tutorial are a significant and excellent continuation of another nice tutorial by Phokion Kolaitis in PODS 1995.	acm sigact;continuation;database theory;digital library;expressive power (computer science);hypertext transfer protocol;lars arge;locality of reference;programming paradigm;query language;relational algebra;relational database;search algorithm;semantic web;transitive closure;web page	Jianwen Su	2009	SIGACT News	10.1145/1711475.1711488	database	DB	-30.645219932841837	6.569936098639814	35957
5fc5f5156b705fbb2759752149ef16251e0e3657	managing uncertainties in image databases: a fuzzy approach	fuzzy relation;image database;relational model;semantic description;normal form;point of view;content based image retrieval;fuzzy database;nf 2 data model	In this paper we present a fuzzy approach for image databases. We exploit the concept of NF 2 relational model as a foundation for building image catalogues containing the semantic description of a given image database. New algebraic operators are defined in order to capture the fuzziness related to the semantic descriptors of an image. We compare our model to the First Normal Form annotated relation model, and show that in a number of interesting cases they can be considered equivalent, from the operational point of view, but in general NF 2 relational model is more powerful, and provides a more suitable framework for dealing with uncertainties in image databases.	database;first normal form;fuzzy logic;point of view (computer hardware company);relational model	Angelo Chianese;Antonio Picariello;Lucio Sansone;Maria Luisa Sapino	2004	Multimedia Tools and Applications	10.1023/B:MTAP.0000031759.22145.5d	relational model/tasmania;relational model;relational database;computer science;database model;data mining;database;automatic image annotation;information retrieval;database design	DB	-27.421969075310194	8.763803423175402	36006
0a516f8874e65fbfd22ef642ff6b507df87674d9	hybridization of institutions	formal specification;institution theory;hybrid logic	Modal logics are successfully used as specification logics for reactive systems. However, they are not expressive enough to refer to individual states and reason about the local behaviour of such systems. This limitation is overcome in hybrid logics which introduce special symbols for naming states in models. Actually, hybrid logics have recently regained interest, resulting in a number of new results and techniques as well as applications to software specification. In this context, the first contribution of this paper is an attempt to ‘universalize’ the hybridization idea. Following the lines of [16], where a method to modalize arbitrary institutions is presented, the paper introduces a method to hybridize logics at the same institution-independent level. The method extends arbitrary institutions with Kripke semantics (for multi-modalities with arbitrary arities) and hybrid features. This paves the ground for a general result: any encoding (expressed as comorphism) from an arbitrary institution to first order logic (FOL) determines a comorphism from its hybridization to FOL. This second contribution opens the possibility of effective tool support to specification languages based upon logics with hybrid features.	first-order logic;formal specification;kripke semantics;modal logic;specification language	Manuel A. Martins;Alexandre Madeira;Razvan Diaconescu;Luís Soares Barbosa	2011		10.1007/978-3-642-22944-2_20	t-norm fuzzy logics;computer science;artificial intelligence;theoretical computer science;formal specification;mathematics;programming language;kripke semantics;algorithm	Logic	-16.354283093957005	17.61624119071917	36013
69f91cd97ab8920cc703a3b6e1cb2391bd9de0f7	twig2stack: bottom-up processing of generalized-tree-pattern queries over xml documents	gtp query;bottom-up processing;final twig result;encoding scheme;twig result;complex gtp query;generalized-tree-pattern query;twig query;processing twig;xml query processing;twig query processing performance;xml document;holistic twig query processing	Tree pattern matching is one of the most fundamental tasks for XML query processing. Holistic twig query processing techniques [4, 16] have been developed to minimize the intermediate results, namely, those root-to-leaf path matches that are not in the final twig results. However, useless path matches cannot be completely avoided, especially when there is a parent-child relationship in the twig query. Furthermore, existing approaches do not consider the fact that in practice, in order to process XPath or XQuery statements, a more powerful form of twig queries, namely, Generalized-Tree-Pattern (GTP) [8] queries, is required. Most existing works on processing GTP queries generally calls for costly post-processing for eliminating redundant data and/or grouping of the matching results.In this paper, we first propose a novel hierarchical stack encoding scheme to compactly represent the twig results. We introduce Twig2Stack, a bottom-up algorithm for processing twig queries based on this encoding scheme. Then we show how to efficiently enumerate the query results from the encodings for a given GTP query. To our knowledge, this is the first GTP matching solution that avoids any post path-join, sort, duplicate elimination and grouping operations. Extensive performance studies on various data sets and queries show that the proposed Twig2Stack algorithm not only has better twig query processing performance than state-of-the-art algorithms, but is also capable of efficiently processing the more complex GTP queries.	algorithm;best, worst and average case;bottom-up parsing;database;enumerated type;holism;line code;pattern matching;top-down and bottom-up design;twig;video post-processing;xml;xpath;xquery;xslt/muenchian grouping	Songting Chen;Hua-Gang Li;Jun'ichi Tatemura;Wang-Pin Hsiung;Divyakant Agrawal;K. Selçuk Candan	2006				DB	-29.926127662157405	4.557593745416664	36024
0d7d98158e31ab7466bdf5c8b9dbe7a2dfb79985	a declarative specification language for temporal database applications			declarative programming;specification language;temporal database	Charalampos I. Theodoulidis	1990				DB	-29.541772484842806	11.59759867139621	36056
f791bc7b4cc165dfe90cf82e64979549ac2fd14b	air and noise pollution monitoring in the city of zagreb by using mobile crowdsensing		The rapid progress of urbanization is leading to serious air and noise pollution. Therefore, significant research effort is focused on creating a fine-grained pollution and noise maps to identify urban areas with critical negative impact on human health. The traditional measuring methods typically use expensive and static equipment which is not suitable for dynamic urban environments because of the low spatio-temporal density of measurements. On the other hand, the growing popularity of mobile phones, and their technological capabilities, opens a new perspective on citizen-assisted environmental monitoring. In this paper we present a mobile crowdsensing (MCS) solution for air quality and noise pollution monitoring. More specifically, we show a practical experience of a real-world system deployment, from sensor calibration to data acquisition and analysis. Our initial results indicate a correlation between air and noise pollution with higher values during peak hours due to an increased number of vehicles on the streets.	crowdsensing;data acquisition;faculty of electrical engineering and computing, university of zagreb;map;mobile app;mobile phone;noise (electronics);sensor;software deployment;system deployment;wearable computer;world-system	Martina Marjanovic;Sanja Grubesa;Ivana Podnar Zarko	2017	2017 25th International Conference on Software, Telecommunications and Computer Networks (SoftCOM)	10.23919/SOFTCOM.2017.8115502	environmental monitoring;computer science;remote sensing;computer network;system deployment;air quality index;pollution;mobile telephony;data acquisition;noise pollution;urbanization	HCI	-30.043637025505316	18.91317651207203	36075
0aacc1c8d61ec8cc17770e4e338db001b0b41b40	basic laws of rool: an object-oriented language.	object oriented language;object oriented design	In this article we introduce some basic algebraic laws of rool, an object-oriented language similar to Java, but with a copy rather than a reference semantics. One immediate application of the basic laws is the derivation of more elaborate laws which formalize object-oriented design practices. We discuss further applications of the basic laws and the importance of proving their soundness with respect to an independent semantics.	a-normal form;algebraic semantics (computer science);compiler;executable;imperative programming;interaction;java virtual machine;linear algebra;mathematical model;precondition;program transformation	Paulo Borba;Augusto Sampaio	2000	RITA		natural language processing;object language;computer science;object-oriented design;programming language;object-oriented programming;object constraint language;object definition language	PL	-24.41936020786432	23.388010878881673	36084
673046f53b0882997bcd45c06a22449d56624b41	implementing set-oriented production rules as an extension to starburst	databases;manufacturing systems;database system;expert systems;data structures;starburst;algorithms;technical report;computer science;transaction processing;database query;production rule	This paper describes the implementation of a set-oriented database production rule language proposed in earlier papers. Cur implementation uses the extensibility features of the Starburst database system, and rule exccution is fully integrated into database query and transaction processing.	database;extensibility;formal grammar;transaction processing	Jennifer Widom;Roberta Cochrane;Bruce G. Lindsay	1991			data definition language;database theory;data structure;database transaction;transaction processing;database tuning;computer science;technical report;data science;transaction log;data mining;database;view;database schema;expert system;database testing;database design	DB	-30.84487669323339	10.076328070821166	36090
f18b4bccffc5627f70ce7534551ee5b557eb9cca	a four-valued logic for rough set-like approximate reasoning	query language;rule based;operational semantics;approximate reasoning;background knowledge;datavetenskap datalogi;computer science;knowledge representation;rough set;knowledge base	This paper extends the basic rough set formalism introduced by Pawlak [1] to a rule-based knowledge representation language, called Rough Datalog, where rough sets are represented by predicates and described by finite sets of rules. The rules allow us to express background knowledge involving rough concepts and to reason in such a knowledge base. The semantics of the new language is based on a four-valued logic, where in addition to the usual values TRUE and FALSE, we also have the values BOUNDARY, representing uncertainty, and UNKNOWN corresponding to the lack of information. The semantics of our language is based on a truth ordering different from the one used in the well-known Belnap logic [2, 3] and we show why Belnap logic does not properly reflect natural intuitions related to our approach. The declarative semantics and operational semantics of the language are described. Finally, the paper outlines a query language for reasoning about rough concepts.	approximation algorithm;database;datalog;four-valued logic;knowledge base;knowledge representation and reasoning;lifting scheme;logic programming;operational semantics;query language;rough set;semantics (computer science);three-valued logic;whole earth 'lectronic link	Jan Maluszynski;Andrzej Szalas;Aida Vitória	2007	Trans. Rough Sets	10.1007/978-3-540-71200-8_11	natural language processing;knowledge representation and reasoning;knowledge base;rough set;computer science;data mining;well-founded semantics;operational semantics;algorithm;query language;computational semantics;dominance-based rough set approach	AI	-17.891330475345583	9.390424757199373	36098
2f0113bae5314d733a8c8990712622d566bad47a	procedure-modular verification of control flow safety properties	java programming;temporal logic;computer and information science;compositional verification;safety properties;development tool;maximal models;control flow;modular verification;data och informationsvetenskap;modeling tool	This paper describes a novel technique for fully automated procedure-modular verification of Java programs equipped with method-local and global assertions that specify safety properties of sequences of method invocations. Modularity of verification is achieved by relativizing the correctness of global properties on the local properties rather than on the implementations of methods, and is based on the construction of maximal models. Tool support is provided by means of ProMoVer, a tool that is essentially a wrapper around a previously developed tool set for compositional verification of control flow safety properties, where program data is abstracted away completely. We evaluate the technique on a small but realistic case study.	control flow;correctness (computer science);java;maximal set	Siavash Soleimanifard;Dilian Gurov;Marieke Huisman	2010		10.1145/1924520.1924525	real-time computing;verification;software verification;computer science;theoretical computer science;high-level verification;runtime verification;programming language;intelligent verification;functional verification	SE	-18.982853607481623	28.007432488886817	36165
4aeb6e5781c17ace147d1e550484de70f671f494	a new data base for syntax-directed pattern analysis and recognition	data base;syntax directed system;data base n dimensional grammar pattern analysis pattern recognition relationship matrix syntax directed system;pattern analysis and recognition;pattern recognition;n dimensional grammar;pattern analysis;relationship matrix;formal language	The problem of developing an appropriate data base for syntax-directed pattern analysis and recognition is considered. A new data base is introduced by generalizing the notion of concatenation in representing patterns with a relationship matrix. The characteristics of relationship matrix are demonstrated in teh context of formal language theory. It is shown that this data base will allow us to remove many of the present restrictions placed on the types of patterns that can be handled by syntax-directed systems. Problems in pattern analysis (description and generation) as well as in pattern recognition are discussed and examples are given to illustrate the potential application of this data base in both of these areas.	concatenation;database;formal language;pattern recognition;teh	Yi-Tzuu Chien;R. Ribak	1972	IEEE Transactions on Computers	10.1109/T-C.1972.223583	natural language processing;formal language;interpreter pattern;state pattern;speech recognition;feature;computer science;pattern recognition;algorithm	DB	-26.48710609724694	16.30292174038393	36231
0ab655f9f258b277c9571f4c2ff49fc9547dbb61	sql-based discovery of exact and approximate functional dependencies	functional dependencies;sql;informing science;database management;functional dependency;computer and information science education;degree of approximation;languages	Students in a typical database course are introduced to theoretical design from a functional dependency standpoint. Functional dependencies are rules of the form X→Y, where X and Y are attributes of a relation r(R). Those rules express the potential one-to-one, and many-to-one relationships among the atributes of R. Unfortunately finding the non-trivial rules X→Y from an existing arbitrary relation is a hard problem. We present an extension of the SQL-based algorithm of Bell and Brockhausen [1] to explore a relation and find its exact and approximate functional dependencies. We use the  g3  measure of Kivinen and Mannila to express the degree of approximation of a dependency. This application could be used either as an example or a project in an advanced database course.	approximation algorithm;functional dependency;sql	Victor Matos;Becky Grasser	2004	SIGCSE Bulletin	10.1145/1041624.1041658	dependency theory;computer science;theoretical computer science;data mining;database;functional dependency;multivalued dependency	DB	-25.85559224011587	10.586096820935538	36280
88796b2a248f1348b5e78644aef2028d05b454ec	proof by consistency	plenitud;specification;intelligence artificielle;abstract data type;consistencia;theorem proving;congruencia;demonstration theoreme;induccion;induction;especificacion;consistance;type abstrait;artificial intelligence;tipo abstracto;inteligencia artificial;completeness;demostracion teorema;completude;consistency;congruence;systeme preuve;proof system	Advances of the past decade in methods and computer programs for showing consistency of proof systems based on first-order equations have made it feasible, in some settings, to use proof by consistency as an alternative to conventional rules of inference. Musser described the method applied to proof of properties of inductively defined objects. Refinements of this inductionless induction method were discuHed by Kapur, Goguen, Huet and Hul/ot, Huet and Oppen, Lankford, Dershowitz, Paul, and more recently by Jouannaud and Kounalis as wel/ as by Kapur, Narendran and Zhang. This paper gives a very general account of proof by consistency and inductionless induction, and shows how previous results can be derived simply from the general theory. New results include a theorem giving characterizations of an unambiguity property that is key to applicability of proof by consistency, and a theorem similar to the Birkhoff's Completeness Theorem for equational proof systems, but concerning inductive proof.	abstract data type;axiomatic system;birkhoff interpolation;clean;complete (complexity);computer program;david musser;first-order predicate;inductive reasoning;init;kilobyte;linear algebra;mathematical induction;normal (geometry);rewriting;rich representation language;star catalogue;terminate (software);time complexity	Deepak Kapur;David R. Musser	1987	Artif. Intell.	10.1016/0004-3702(87)90017-8	direct proof;proof by contradiction;discrete mathematics;computer-assisted proof;completeness;computer science;artificial intelligence;analytic proof;congruence;mathematics;mathematical proof;automated theorem proving;consistency;abstract data type;structural proof theory;specification;proof complexity;algorithm;statistical proof	PL	-15.295632387352544	15.655653284624812	36283
c8e1e3cb5e4bfc5672ff6817903562af9698def6	equivalence checking on system level using stepwise induction		We present an algorithm for equivalence checking between two C++ objects that uses stepwise induction. To prevent the effort of checking each state for reachability, we utilize a hypothesis that approximately describes the reachable states.	algorithm;c++;formal equivalence checking;reachability;stepwise regression;turing completeness	Niels Thole;Görschwin Fey	2014			formal equivalence checking;discrete mathematics;computer science	Logic	-15.128204187198428	27.402906437942875	36296
7dc44cde746cdd4173360b72233a0cdc09e6c8b7	type-shifting and scrambled definites	syntax;objet grammatical;defini indefini;syntactic position;semantics;scrambling;syntaxe;semantique;word order;hollandais;variation syntaxique;position syntaxique;interpretation semantique;allemand;optimality theory;ordre des mots;theorie de l optimalite	We show that the diierence between indeenites and deenites with respect to their syntactic behaviour, in particular scrambling, follows from a diierence in their semantics. In general, deenites can be viewed as a special type of indeenites: they are restricted indeenites in all semantic types. This inherent restriction of deenites makes them insensitive to processes of semantic incorporation. That is, merging an incorporating verb and a predicative deenite is equivalent to merging an ordinary type of transitive verb and an ordinary type of deenite. This will explain the phenomenon of optional scrambling for deenites. Predicative indeenites are dependent on the verb for their interpretation, of which the adjacency requirement between the incorporating verb and the predicative indeenite is only a syntactic reeex.	bibliothèque de l'école des chartes;impredicativity	Jaap van der Does;Helen de Hoop	1998	J. Semantics	10.1093/jos/15.4.393	word order;syntax;philosophy;scrambling;semantics;linguistics	NLP	-12.525302378687654	6.753929146593841	36316
7dc46ba0bc4022c1b2eae746d4b50c0d4cc4f4e3	termination and reduction checking for higher-order logic programs	high order logic;compilateur;relacion orden;programa control;ordering;logical programming;program verification;compiler;higher order;relation ordre;verificacion programa;induccion;induction;programmation logique;inferencia;checking program;programme controle;completitud;decidibilidad;completeness;logique ordre superieur;decidabilite;verification programme;completude;programacion logica;higher order logic;inference;compilador;decidability	In this paper, we present a syntax-directed termination and reduction checker for higher-order logic programs. The reduction checker verifies parametric higher-order subterm orderings describing relations between input and output of well-moded predicates. These reduction constraints are exploited during termination checking to infer that a specified termination order holds. To reason about parametric higher-order subterm orderings, we introduce a deductive system as a logical foundation for proving termination. This allows the study of proof-theoretical properties, such as consistency, local soundness and completeness and decidability. We concentrate here on proving consistency of the presented inference system. The termination and reduction checker are implemented as part of the Twelf system and enable us to verify proofs by complete induction.	formal system;inference engine;input/output;mathematical induction;termination analysis;twelf	Brigitte Pientka	2001		10.1007/3-540-45744-5_32	discrete mathematics;higher-order logic;computer science;mathematics;programming language;algorithm	PL	-14.468462356101128	21.86500504819269	36356
ae7428e4c672332810a2fd0069cd4e903748da43	the recursive disk metaphor - a glyph-based approach for software visualization			glyph;recursion (computer science);software visualization	Richard Müller;Dirk Zeckzer	2015		10.5220/0005342701710176	glyph;computer science;theoretical computer science;programming language	Visualization	-29.393459275180344	26.33689504511956	36358
421b8f165e0385f224a8db0466a3697f98cbd561	incorporating software visualization in the design of intelligent diagnosis systems for user programming	grain size;software development environment;visual features;program debugging;intelligent program diagnosis tutoring;knowledge representation;programming by discovery;empirical evaluation;software development environments;problem solving;software visualization;dynamic behavior	Program diagnosis systems were developed to help users solve programming problems. By providing guidence on errors and misconceptions, these systems can help the users in writing programs and understanding their dynamic behavior. Features of software visualization which aim at providing visual and concrete depictions to the abstractions and operations of programs have also shown to be making programs more understandable. The main theme of this paper is to asses the usefulness of incorporating features of software visualization into the design of program diagnosis systems intended for novices. We report an empirical evaluation to assess the effectiveness of supporting visualization features during problem solving. The system used in the evaluation integrates visualzation and immediacy features and supports a model-tracing based approach to program diagnosis. Unlike other similar systems, our prototype system supports a more flexible style of interaction by increasing the grain size of diagnosis to a complete programming statement. The evaluation reported here seems to suggest that when supported with visualization features, systems for program diagnosis tend to be more effective in helping the users during problem solving.	abstract data type;array data structure;computation;dataflow;embedded system;integrated development environment;lisp;mental model;pop-11;pascal;poplog;problem solving;programming paradigm;prototype;pseudocode;requirement;software visualization;stack (abstract data type);string (computer science);tree (data structure);visual basic;visual basic[.net];visual modeling	Haider Ali Ramadhan;Fadi F. Deek;Khalil Shihab	2001	Artificial Intelligence Review	10.1023/A:1011078011415	knowledge representation and reasoning;software visualization;simulation;human–computer interaction;computer science;artificial intelligence;theoretical computer science;software development;machine learning;development environment;software system	SE	-29.186729591921527	23.69436100920339	36399
dabb91f407b718a76f2093423530fe2b905e4fd9	partial order reduction for the full class of state/event linear temporal logic			linear temporal logic;partial order reduction	Shuanglong Kan;Zhiqiu Huang	2018	Comput. J.	10.1093/comjnl/bxx064	linear temporal logic;discrete mathematics;theoretical computer science;computer science;partial order reduction	Logic	-10.984606471904678	23.699604466188706	36409
7b4f2b2a7bd33ba6c0c3934d97d51cb245d60f22	adaptive query optimization in a deductive database system	performance evaluation;query optimization;deductive databases	Query Optimization in a Deductive Database	deductive database;program optimization;query optimization	Marcia A. Derr	1993		10.1145/170088.170134	sargable;query optimization;database theory;query expansion;computer science;query by example;database;rdf query language;programming language;web search query;view;information retrieval;query language	DB	-30.780126612015902	8.478185363529597	36451
5350fa3ca29c679d3607af713a48098cebb47ab4	profound: program-analysis-based form understanding	web form;integrity constraints;web search;program analysis;static analysis;deep web;javascript;requirement specification;generic programming	"""An important feature of web search interfaces are the restrictions enforced on input values - those reflecting either the semantics of the data or requirements specific to the interface. Both integrity constraints and """"access restrictions"""" can be of great use to web exploration tools. We demonstrate here a novel technique for discovering constraints that requires no form submissions whatsoever. We work via statically analyzing the JavaScript client-side code used to enforce the constraints, when such code is available. We combine custom recognizers for JavaScript functions relevant to constraint checking with a generic program analysis layer. Integrated with a web browser, our system shows the constraints detected on accessed web forms, and allows a user to see the corresponding JavaScript code fragment."""	client-side;data integrity;finite-state machine;form (html);javascript;program analysis;requirement;web search engine	Michael Benedikt;Tim Furche;Andreas Savvides;Pierre Senellart	2012		10.1145/2187980.2188037	program analysis;web service;ajax;web development;web application;web modeling;content security policy;web-based simulation;html;web design;computer science;unobtrusive javascript;web api;server-side scripting;data integrity;database;client-side scripting;javascript;programming language;generic programming;world wide web;static analysis;deep web	SE	-27.490246060199027	27.947554627744353	36489
275f497963f975285548dfe9442d8c840d506c07	some locally tabular logics with contraction and mingle	zlozonośc obliczeniowa;programowanie logiczne;semiconic;algorytmy;locally tabular;logika w informatyce;logika matematyczna;reports on mathematical logic;teoria mnogości;teoria dowodu;quasivariety;mingle;teoria modeli;residuation;rml;algebra uniwersalna	A b s t r a c t. Anderson and Belnapu0027s implicational system RMO! can be extended conservatively by the usual axioms for fusion and for the Ackermann truth constant t. The resulting system RMOis algebraized by the quasivariety IP of all idem- potent commutative residuated po-monoids. Thus, the axiomatic extensions of RMOare in one-to-one correspondence with the relative subvarieties of IP. An algebra in IP is called semiconic if it decomposes subdirectly (in IP) into algebras where the iden- tity element t is order-comparable with all other elements. The semiconic algebras in IP are locally finite. It is proved here that a relative subvariety of IP consists of semiconic algebras if and only if it satisfies x ≈ (x → t) → x. It follows that if an axiomatic extension of RMOhas ((p → t) → p) → p among its theo- rems then it is locally tabular. In particular, such an extension is strongly decidable, provided that it is finitely axiomatized.	table (information)	Ai-ni Hsieh	2010	Reports on Mathematical Logic		discrete mathematics;topology;mathematics;algorithm;algebra	Logic	-9.236451810656702	12.921907702208308	36555
08582cb4bcab918c045ad4addd6b06dc993878f9	success and failure for hereditary harrop formulae	first order;closed world assumption	We introduce the foundational issues involved in incorporating the Negation as Failure (NAF) rule into the framework of rst-order hereditary Harrop formulae of Miller et al. This is a larger class of formulae than Horn clauses, and so the technicalities are more intricate than in the Horn clause case. As programs may grow during execution in this framework, the role of NAF and the Closed World Assumption (CWA) need some modiication, and for this reason we introduce the notion of a completely deened predicate, which may be thought of as a localisation of the CWA. We also show how this notion may be used to deene a notion of NAF for a more general class of goals than literals alone. We also show how an extensional notion of universal quantiication may be incorporated. This makes our framework somewhat diierent from that of Miller et al., but not essentially so. We also show how to construct a Kripke-like model for the extended class of programs. This is essentially a denotational semantics for logic programs, in that it provides a mapping from the program to a pair of sets of atoms which denote the success and ((nite) failure sets. This is inspired by the work of Miller on the semantics of rst-order hereditary Harrop formulae. Note that no restriction on the class of programs is needed in this approach, and that our construction needs no more than ! iterations. This necessitates a slight departure from the standard methods, but the important properties of the construction still hold.	closed-world assumption;cognitive work analysis;denotational semantics;formal methods;horn clause;independence of premise;intensional logic;intuitionistic logic;iteration;kinetic data structure;markov chain;nato architecture framework;negation as failure;theory;while	James Harland	1993	J. Log. Program.	10.1016/0743-1066(93)90007-4	closed-world assumption;discrete mathematics;computer science;artificial intelligence;first-order logic;mathematics;algorithm	Logic	-15.92681597392996	8.483019731202454	36615
17f159cdb984a48b0580a865fdb1f6468b3f06aa	selectscript: a query language for robotic world models and simulations	databases;robot sensing systems database languages data models databases dictionaries;robot sensing systems;valuable information extraction declarative language selectscript scripting language sql relational algebra complex queries foreign programming languages python language oriented programming domain specific language robotic world model robotic simulator select statements;dictionaries;database languages;sql authoring languages relational algebra robot programming;data models	We introduce a new declarative language called SELECTSCRIPT. As its name suggests, it is a scripting language inspired primarily by SQL and its relational algebra. It is intended to be used for complex queries on different kinds of world models. Scripts can be dynamically generated and executed, or embedded into the code of foreign programming languages. A first interpreter was therefore developed for Python. Adapting the ideas of language-oriented programming, which enables developers to create their own domain-specific language, we developed a language stub that can be easily adapted and extended to comply with any (discrete) robotic world model or robotic simulator. We will further show how simple SELECT-statements can be used to extract any kind of valuable information in various return formats, thereby going beyond traditional SQL capabilities.	algorithm;computational complexity theory;computer scientist;control flow;database;declarative programming;domain-specific language;embedded system;interconnection;language-oriented programming;matlab;microsoft outlook for mac;parallel computing;programming language;programming paradigm;prolog;python;query language;relational algebra;requirement;robot;ruby;sql;scripting language;simulation;simulink;table (database)	André Dietrich;Sebastian Zug;Jörg Kaiser	2015	2015 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2015.7140077	natural language processing;fourth-generation programming language;data modeling;first-generation programming language;data definition language;sql;declarative programming;very high-level programming language;language primitive;data manipulation language;data control language;computer science;domain-specific language;artificial intelligence;query by example;third-generation programming language;database;scripting language;programming paradigm;low-level programming language;fifth-generation programming language;programming language;second-generation programming language;high-level programming language;query language	Robotics	-26.78938068337956	23.38233111115557	36682
8715ef9bdd9fb9c20ffb18d0b3106531996d986d	an analysis of structural validity in entity-relationship modeling	entity relationship model;design process;structural validity;data model;binary relationships;ternary relationships;cardinality constraints;case tool;entity relationship modeling;recursive relationships;decision rule;entity relationship	We explore the criteria that contribute to the structural validity of modeling structures within the entityrelationship (ER) diagram. Our approach examines cardinality constraints in conjunction with the degree of the relationship to address constraint consistency, state compliance, and role uniqueness issues to derive a complete and comprehensive set of decision rules. Unlike typical other analyses that use only maximum cardinality constraints, we have used both maximum and minimum cardinality constraints in defining the properties and their structural validity criteria yielding a complete analysis of the structural validity of recursive, binary, and ternary relationship types. Our study evaluates these relationships as part of the overall diagram and our rules address these relationships as they coexist in a path structure within the model. The contribution of this paper is to provide a comprehensive set of decision rules to determine the structural validity of any ERD containing recursive, binary, and ternary relationships. These decision rules can be readily applied to real world data models regardless of their complexity. The rules can easily be incorporated into the database modeling and designing process, or extended into case tool implementations. 2003 Elsevier B.V. All rights reserved.	coexist (image);data model;database model;diagram;entity–relationship model;maxima and minima;recursion	James Dullea;Il-Yeol Song;Ioanna Lamprou	2003	Data Knowl. Eng.	10.1016/S0169-023X(03)00049-1	entity–relationship model;computer science;data mining;database;algorithm	DB	-31.484849007195514	14.24345128035122	36689
fe1548d05225a7268069ea229287ceee0311f4de	workflow representation and runtime based on lazy functional streams	p2g;programming language;distributed computing;p2p;soa;grid;workflows;information exchange;paramedical emergency operations;web services;triana workflows;mobile computing;type system;intermediate representation	Workflows are a successful model for building both distributed and tightly-coupled programs based on a dataflow-oriented coordination of computations. Multiple programming languages have been proposed to represent workflow-based programs in the past. In this paper, we discuss a representation of workflows based on lazy functional streams implemented in the strongly typed language Haskell. Our intent is to demonstrate that streams are an expressive intermediate representation for higher-level workflow languages. By embedding our stream-based workflow representation in a language such as Haskell, we also gain with minimal effort the strong type system provided by the base language, the rich library of built-in functional primitives, and most recently, rich support for managing concurrency at the language level.	computation;concurrency (computer science);dataflow;haskell;intermediate representation;lazy evaluation;programming language;strong and weak typing;type system	Matthew J. Sottile;Geoffrey C. Hulette;Allen D. Malony	2009		10.1145/1645164.1645174	web service;workflow;type system;information exchange;computer science;theoretical computer science;operating system;service-oriented architecture;peer-to-peer;data mining;database;distributed computing;windows workflow foundation;programming language;intermediate language;grid;mobile computing;workflow management system;workflow technology	PL	-28.91212512171103	31.52632076862025	36702
2fbc03af92775dc3f4d8277b2f5c26dc730825ee	what's in an attribute? consequences for the least common subsumer	artificial intelligent;least common subsumer;polynomial time;description logic;knowledge representation	Functional relationships between objects, called “attributes”, are of considerable importance in knowledge representation languages, including Description Logics (DLs). A study of the literature indicates that papers have made, often implicitly, different assumptions about the nature of attributes: whether they are always required to have a value, or whether they can be partial functions. The work presented here is the first explicit study of this difference for (sub-)classes of the Classic DL, involving the same-as concept constructor. It is shown that although determining subsumption between concept descriptions has the same complexity (though requiring different algorithms), the story is different in the case of determining the least common subsumer (lcs). For attributes interpreted as partial functions, the lcs exists and can be computed relatively easily; even in this case our results correct and extend three previous papers about the lcs of DLs. In the case where attributes must have a value, the lcs may not exist, and even if it exists it may be of exponential size. Interestingly, it is possible to decide in polynomial time if the lcs exists. ∗This research was supported in part by grant NSF IRI 9619979.	algorithm;binocular disparity;cobham's thesis;computation;congruence of squares;correctness (computer science);description logic;formal proof;ibm notes;knowledge representation and reasoning;lowest common ancestor;modeling language;polynomial;star filler;subsumption architecture;time complexity	Alexander Borgida;Ralf Küsters	2001	J. Artif. Intell. Res.	10.1613/jair.702	time complexity;knowledge representation and reasoning;discrete mathematics;description logic;computer science;artificial intelligence;machine learning;mathematics;algorithm	AI	-19.00597312616197	9.753845860878856	36772
6e0fec7ef22bf974d1f348daaefc6ed19fc7f021	monotonic abstraction for programs with multiply-linked structures	shape analysis;program verification;monotonic abstraction;datavetenskap datalogi;computer science;dynamic multiply linked data structures	We investigate the use of monotonic abstraction and backward reachability analysis as means of performing shape analysis on programs with multiply pointed structures. By encoding the heap as a vertexand edge-labeled graph, we can model the low level behaviour exhibited by programs written in the C programming language. Using the notion of signatures, which are predicates that define sets of heaps, we can check properties such as absence of null pointer dereference and shape invariants. We report on the results from running a prototype based on the method on several programs such as insertion into and merging of doubly-linked lists.	algorithm;antivirus software;computation;concurrency (computer science);data structure;dereference operator;dynamic data;dynamization;electronic signature;graph labeling;linked list;markov switching multifractal;non-monotonic logic;parallel computing;pointer (computer programming);prototype;reachability;recursion;shape analysis (digital geometry);skip list;the c programming language;tree (data structure);type signature	Parosh Aziz Abdulla;Jonathan Cederberg;Tomás Vojnar	2013	Int. J. Found. Comput. Sci.	10.1142/S0129054113400078	combinatorics;computer science;theoretical computer science;shape analysis;programming language;algorithm	Logic	-14.379385907795998	22.180281291285855	36844
58d10b2641cb4c87b2c0dc0308c4a8b2001697d7	modeling in miningzinc		MiningZinc offers a framework for modeling and solving constraintbased mining problems. The language used is MiniZinc, a high-level declarative language for modeling combinatorial (optimisation) problems. This language is augmented with a library of functions and predicates that help modeling data mining problems and facilities for interfacing with databases. We show how MiningZinc can be used to model constraint-based itemset mining problems, for which it was originally designed, as well as sequence mining, Bayesian pattern mining, linear regression, clustering data factorization and ranked tiling. The underlying framework can use any existing MiniZinc solver. We also showcase how the framework and modeling capabilities can be integrated into an imperative language, for example as part of a greedy algorithm.	cluster analysis;combinatorial optimization;constraint programming;data mining;database;declarative programming;greedy algorithm;high- and low-level;imperative programming;mathematical optimization;predicate (mathematical logic);sequential pattern mining;solver;tiling window manager	Anton Dries;Tias Guns;Siegfried Nijssen;Behrouz Babaki;Thanh Le Van;Benjamin Négrevergne;Sergey Paramonov;Luc De Raedt	2016		10.1007/978-3-319-50137-6_10	constraint programming;machine learning;imperative programming;declarative programming;cluster analysis;artificial intelligence;greedy algorithm;data modeling;constraint satisfaction problem;computer science;solver	ML	-23.010977495773307	12.437708888530507	36856
81aba7ce8a9752d0d7aa4257b12df08c6b422bf3	semantic link prediction through probabilistic description logics		Abstract. Predicting potential links between nodes in a network is a problem of great practical interest. Link prediction is mostly based on graph-based features and, recently, on approaches that consider the semantics of the domain. However, there is uncertainty in these predictions; by modeling it, one can improve prediction results. In this paper, we propose an algorithm for link prediction that uses a probabilistic ontology described through the probabilistic description logic crALC. We use an academic domain in order to evaluate this proposal.	algorithm;description logic;graph (discrete mathematics);link relation	Kate Revoredo;José Eduardo Ochoa Luna;Fábio Gagliardi Cozman	2011			probabilistic ctl;theoretical computer science;machine learning;data mining;mathematics;probabilistic logic	AI	-21.638615902102725	6.963091615524336	36867
6de95b874eaec637a0e5294a2541e0801e7d6202	procedure-modular specification and verification of temporal safety properties	temporal logic;model checking;maximal models;datavetenskap datalogi;computer science	This paper describes ProMoVer, a tool for fully automated procedure-modular verification of Java programs equipped with method-local and global assertions that specify safety properties of sequences of method invocations. Modularity at the procedure-level is a natural instantiation of the modular verification paradigm, where correctness of global properties is relativized on the local properties of the methods rather than on their implementations. Here, it is based on the construction of maximal models for a program model that abstracts away from program data. This approach allows global properties to be verified in the presence of code evolution, multiple method implementations (as arising from software product lines), or even unknown method implementations (as in mobile code for open platforms). ProMoVer automates a typical verification scenario for a previously developed tool set for compositional verification of control flow safety properties, and provides appropriate pre- and post-processing. Both linear-time temporal logic and finite automata are supported as formalisms for expressing local and global safety properties, allowing the user to choose a suitable format for the property at hand. Modularity is exploited by a mechanism for proof reuse that detects and minimizes the verification tasks resulting from changes in the code and the specifications. The verification task is relatively light-weight due to support for abstraction from private methods and automatic extraction of candidate specifications from method implementations. We evaluate the tool on a number of applications from the domains of Java Card and web-based application.	automata theory;code mobility;consistency model;control flow;correctness (computer science);finite-state machine;formal verification;heart rate variability;hoc (programming language);inline expansion;java card;linear temporal logic;link rot;maximal set;maximum flow problem;modal μ-calculus;model checking;oracle machine;programmer;programming paradigm;software product line;specification language;universal instantiation;video post-processing;web application	Siavash Soleimanifard;Dilian Gurov;Marieke Huisman	2013	Software & Systems Modeling	10.1007/s10270-013-0321-0	model checking;verification;temporal logic;computer science;theoretical computer science;high-level verification;runtime verification;programming language;intelligent verification;algorithm;functional verification	SE	-19.156240550671757	27.914509129112055	36878
1be0a7ad21a56685e17e7b0b7bc1add6dbc80c62	software for hybrid computation&#8212;past,present,and future	signal generators;computerized monitoring;potentiometers;servomechanisms;logic testing;analog computers;analog computers potentiometers logic testing servomechanisms computer interfaces software design signal generators computerized monitoring programming servomotors;servomotors;software design;computer interfaces;programming	To understand and appreciate the various software requirements for a hybrid computer system one must first understand the environment in which the system is used. It may perhaps also be useful to trace the evolution of the analog computer to the hybrid computer–specifically as hardware developments directed software developments.	analog computer;hybrid computer;requirement;software requirements	Harriett B. Rigas	1976	Computer	10.1109/C-M.1976.218642	potentiometer;programming;analog computer;computing;computer science;software design;theoretical computer science;software construction;management;servomotor;signal generator;software system	Arch	-31.286750547138567	23.163034366667908	36897
31f8f30c1969db7921d13b9198c826435fcfd830	a note on the complexity of model-checking bounded multi-pushdown systems		In this note, we provide complexity characterizations of model checking multi-pushdown systems. Multi-pushdown systems model recursive concurrent programs in which any sequential process has a finite control. We consider three standard notions for boundedness: context boundedness, phase boundedness and stack ordering. The logical formalism is a linear-time temporal logic extending well-known logic CaRet but dedicated to multi-pushdown systems in which abstract operators (related to calls and returns) such as those for next-time and until are parameterized by stacks. We show that the problem is EXPTIME-complete for context-bounded runs and unary encoding of the number of context switches; we also prove that the problem is 2EXPTIME-complete for phasebounded runs and unary encoding of the number of phase switches. In both cases, the value k is given as an input (whence it is not a constant of the model-checking problem), which makes a substantial difference in the complexity. In certain cases, our results improve previous complexity results.	2-exptime;analysis of algorithms;best, worst and average case;binary file;caret;exptime;formal system;linear temporal logic;model checking;network switch;reachability;recursion;specification language;stack (abstract data type);time complexity;unary coding;unary operation;worst-case complexity	Kshitij Bansal;Stéphane Demri	2012	CoRR		combinatorics;discrete mathematics;mathematics;algorithm	Logic	-11.283003348598495	24.045202202797604	36898
7c9f90a075f5662f3d3a05f97d6056a06f45a149	the first-order theory of the c-degrees with the #-operation	first order		first-order reduction	Patrick Farrington	1982	Math. Log. Q.	10.1002/malq.19820283303	first-order logic;mathematics;programming language	Theory	-13.393443260095257	14.386662329394188	36930
20319f201955023b53fe529998ceefb0fccdae35	interpolation-based software verification with wolverine	bit-vector operation;built-in interpolating decision procedure;smt solvers;craig interpolation;software verification tool;interpolation-based software verification;continuous performance improvement;verification condition;interpolating decision procedure;linux device driver;equality logic	Wolverine is a software verification tool using Craig interpolation to compute invariants of ANSI-C and C++ programs. The tool is an implementation of the lazy abstraction approach, generating a reachability tree by unwinding the transition relation of the input program and annotating its nodes with interpolants representing safe states. Wolverine features a built-in interpolating decision procedure for equality logic with uninterpreted functions which provides limited support for bit-vector operations. In addition, it provides an API enabling the integration of other interpolating decision procedures, making it a valuable source of benchmarks and allowing it to take advantage of the continuous performance improvements of SMT solvers. We evaluate the performance of Wolverine by comparing it to the predicate abstraction-based verifier SatAbs on a number of verification conditions of Linux device drivers.	ansi c;algorithm;application programming interface;benchmark (computing);c++;canonical account;decision problem;device driver;interpolation;lazy evaluation;linux;loop unrolling;modular design;predicate abstraction;reachability;software verification	Daniel Kroening;Georg Weissenbacher	2011		10.1007/978-3-642-22110-1_45	computer science;theoretical computer science;database;programming language;algorithm	Logic	-19.106771607391824	26.647963001621275	36942
2b51828a426b35c183149e68b54c2a1674479402	invariants, modularity, and rights	program assertion;certain updates;certain invariants;modular concurrency reasoning;recent proposal	The quest for modular concurrency reasoning has led to recent proposals that extend program assertions to include not just knowledge about the state, but rights to access the state. We argue that these rights are really just sugar for knowledge that certain updates preserve	admissible heuristic;assertion (software development);concurrency (computer science);description logic;invariant (computer science);modularity (networks);programme level;sugar;trusted computing base;virtual collective consciousness	Ernie Cohen;Eyad Alkassar;Vladimir Boyarinov;Markus Dahlweid;Ulan Degenbaev;Mark A. Hillebrand;Bruno Langenstein;Dirk Leinenbach;Michal Moskal;Steven Obua;Wolfgang J. Paul;Hristo Pentchev;Elena Petrova;Thomas Santen;Norbert Schirmer;Sabine Schmaltz;Wolfram Schulte;Andrey Shadrin;Stephan Tobies	2009		10.1007/978-3-642-11486-1_4	computer science;knowledge management;artificial intelligence;algorithm	AI	-20.884899941073844	22.31654937537411	36950
0e42233627f33e19c72bd0dfe013b1597566d914	formal efficiency analysis for tree transducer composition	optimizing compiler;program transformation;normal form;efficiency analysis;functional language	"""We study the question of efficiency improvement or deterioration for a semantics-preserving program transformation technique for (lazy) functional languages, based on composition of restricted macro tree transducers. By annotating programs to reflect the intensional property """"computation time"""" explicitly in the computed output and by manipulating such annotations, we formally prove syntactic conditions under which the composed program is guaranteed to be no less efficient than the original program with respect to the number of call-by-need reduction steps required to reach normal form. The criteria developed can be checked automatically and efficiently, and thus they are suitable for integration into an optimizing compiler."""	computation;functional programming;intensional logic;lazy evaluation;optimizing compiler;program transformation;time complexity;transducer	Janis Voigtländer	2006	Theory of Computing Systems	10.1007/s00224-006-1235-9	computer science;theoretical computer science;optimizing compiler;programming language;functional programming;functional compiler;algorithm	PL	-17.28254013033733	23.07263921185667	36951
32ea85f0cc10c93c3fabb16381b337b0a6abf98a	exploiting semantics for xml keyword search	semantics;object;independence;lca;keyword search;xml	XML keyword search has attracted a lot of interests with typical search based on lowest common ancestor (LCA). However, in this paper, we show several problems of the LCA-based approaches, including meaningless answers, incomplete answers, duplicated answers, missing answers, and schema-dependent answers. To handle these problems, we exploit the semantics of object, object identifier, relationship, and attribute (referred to as the ORA-semantics). Based on the ORA-semantics, we introduce new ways of labeling and matching. More importantly, we propose a new semantics, called CR (Common Relative) for XML keyword search, which can return answers independent from schema designs. To find answers based on the CR semantics, we discover properties of common relative and propose an efficient algorithms. Experimental results show the seriousness of the problems of the LCA-based approaches. They also show that the CR semantics possesses the properties of completeness, soundness and independence while the response time of our approach is faster than the LCA-based approaches thanks to our techniques.	identifier;lowest common ancestor;openraster;response time (technology);search algorithm;xml	Thuy Ngoc Le;Zhifeng Bao;Tok Wang Ling	2015	Data Knowl. Eng.	10.1016/j.datak.2015.06.003	independence;xml;computer science;object;data mining;database;semantics;programming language;information retrieval	DB	-26.32630304955838	7.681000200318989	36964
b4d91273a5e162eb67f92a0ee5834b397eeb0036	lineal: a linear-algebraic lambda-calculus	f 4 1;f 1 1;quantum physics;computer science programming languages;f 4 2;computer science logic in computer science;03b40 68n18 81p68	We provide a computational definition of the notions of vector space and bilinear functions. We use this result to introduce a minimal language combining higherorder computation and linear algebra. This language extends the λ-calculus with the possibility to make arbitrary linear combinations of terms α.t + β.u. We describe how to “execute” this language in terms of a few rewrite rules, and justify them through the two fundamental requirements that the language be a language of linear operators, and that it be higher-order. We mention the perspectives of this work in the field of quantum computation, whose circuits we show can be easily encoded in the calculus. Finally, we prove the confluence of the entire calculus.	bilinear filtering;boyce–codd normal form;computable function;computation;confluence;curry;curry–howard correspondence;database normalization;fixed point (mathematics);lambda calculus;linear algebra;linear logic;nonlinear system;programming language;quantum computing;quantum entanglement;quantum mechanics;quantum programming;requirement;rewrite (programming);rewriting;self-replicating machine;type system	Pablo Arrighi;Gilles Dowek	2017	Logical Methods in Computer Science	10.23638/LMCS-13(1:8)2017	object language;specification language;computer science;theoretical computer science;mathematics;low-level programming language;programming language;high-level programming language;algorithm	Logic	-12.986800708330941	18.49873343508084	36967
9dc1cd02119479acc05d9e6060bbe16b260c260c	on the logic of information flow	information flow	This paper is an investigation into the logic of information ow. The basic perspective is that logic ows in virtue of constraints (as in 7]), and that constraints classify channels connecting particulars (as in 8]). In this paper we explore some logics intended to model reasoning in the case of idealized information ow, that is, where the constraints involved are exceptionless. We look at this as a step toward the far more challenging task of understanding the logic of imperfect information ow, that is where the constraints admit of exceptional connections. This paper continues and ampliies work presented by the same authors in 10]. Over the past decade, information has emerged as an important topic in the study of logic, language, and computation. This paper contributes to this line of work. We formulate a notion of information network that covers many important examples from the literature of information-theoretic structures. Information networks have two kinds of items. Following Barwise 5] we call these items: sites of information, and channels between sites. We present various logical calculi intended to model perfect reasoning about the ow of information through an information network. To motivate the basic picture, let's brieey consider some real world examples of information ow, and then think about how we can model them in a uniform way. First, imagine a person going into a room and seeing some scene, which we construe as a site. Based on general information about how the world works, the person knows something about how things came to be as they are. Certain connections exist between what they see and other situations. The person's knowledge of these connections and how they work lead him 1	computation;information flow;information theory;logic of information;logic programming	K. Jon Barwise;Dov M. Gabbay;Chrysafis Hartonas	1995	Logic Journal of the IGPL	10.1093/jigpal/3.1.7	information flow;computer science	AI	-9.362679185453462	5.878096165545923	37018
3ebecb7a7cdfee499b9d2001eab8547e9308e6c0	towards a formal foundation to orchestration languages	parallel composition;programming language;orchestration languages;web service;feature construction;web services;process algebra;business process	We introduce a formal framework for studying the semantics of orchestration languages for Web Services. Taking BPEL4WS language as reference case study, we define syntax and semantics of a core language to derive the interactive behaviour of a business process out from a BPEL4WS specification. This is realised by developing a process algebra which, other than usual operators for choice, sequential and parallel composition, features constructs of imperative programming languages, such as iterative cycles and variable assignment. These are meant to focus on the very notion of correlation, which is exploited by BPEL4WS to define a business process as the concurrent behaviour of several process instances.		Mirko Viroli	2004	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2004.05.008	web service;fourth-generation programming language;business process execution language;computer science;theoretical computer science;third-generation programming language;database;fifth-generation programming language;programming language;second-generation programming language	PL	-25.577104065166758	19.624244505603716	37051
3ff550a49a789a46c1df9481dabc77fc92a4c7bf	an xml storage system for object-oriented/object-relational dbmss	xml schema;storage system;design and implementation;object oriented;xml document;world wide web;object relational;power modeling	As XML has become popular as a document standard in the World Wide Web, a lot of research has been done on the XML storage systems for storing and managing XML documents using existing DBMSs. Most of the research activities, however, assume a relational DBMS instead of an object-oriented/object-relational (OO/OR) DBMS, which offers more powerful modeling capabilities. In this paper, we present the design and implementation of an XML storage system designed for an OO/OR DBMS. Specifically, we first analyze the mapping from an XML document structure to OO/OR database schema. Second, we propose a method for describing the mapping using a standard language called the XML Schema Language. Third, we propose system catalog classes for storing the mapping information specified by users in the database. Fourth, we propose a detailed algorithm for storing XML documents in an OO/OR database, based on the mapping information. We believe the proposed system is practically usable for object-oriented programmers and DBMS implementors.	algorithm;computer data storage;database schema;object-relational database;programmer;relational database management system;world wide web;xml schema	Wook-Shin Han;Ki-Hoon Lee;Byung Suk Lee	2003	Journal of Object Technology	10.5381/jot.2003.2.3.a2	well-formed document;xml catalog;xml validation;xml encryption;xml base;simple api for xml;xml;relax ng;xml schema;streaming xml;computer science;document type definition;document structure description;xml framework;soap;data mining;xml database;xml schema;database;xml signature;programming language;object-oriented programming;world wide web;xml schema editor;efficient xml interchange	DB	-33.33271891066104	10.261043842757546	37066
d38b3da092293db966f1e5f666ece59799d854c6	dna computing and molecular programming	dna computing;computational biology;data structure;artificial intelligent	Synthetic biology is a rapidly emerging research area, with expected wide-ranging impact in biology, nanofabrication, and medicine. A key technical challenge lies in embedding computation in molecular contexts where electronic micro-controllers cannot be inserted. This necessitates effective representation of computation using molecular components. While previous work established the Turing-completeness of chemical reactions, defining representations that are faithful, efficient, and practical remains challenging. This paper introduces CRN++, a new language for programming deterministic (mass-action) chemical kinetics to perform computation. We present its syntax and semantics, and build a compiler translating CRN++ programs into chemical reactions, thereby laying the foundation of a comprehensive framework for molecular programming. Our language addresses the key challenge of embedding familiar imperative constructs into a set of chemical reactions happening simultaneously and manipulating real-valued concentrations. Although some deviation from ideal output value cannot be avoided, we develop methods to minimize the error, and implement error analysis tools. We demonstrate the feasibility of using CRN++ on a suite of well-known algorithms for discrete and real-valued computation. CRN++ can be easily extended to support new commands or chemical reaction implementations, and thus provides a foundation for developing more robust and practical molecular programs.	algorithm;compiler;computation;dna computing;error analysis (mathematics);imperative programming;kinetics internet protocol;parameter (computer programming);synthetic biology;turing completeness;turing machine	David Doty;Hendrik Dietz	2018		10.1007/978-3-030-00030-1	computational science;natural computing;computer science;theoretical computer science;machine learning	PL	-20.695491520917372	13.289197869951872	37091
b48210b64fbee2642bfce8419af33865ad16c66e	describing data flow analysis techniques with kleene algebra	gen kill analyses;labelled transition system;data flow graphs;control flow graph;data flow graph;kleene algebra with tests;data flow analysis;kleene algebra;data flow;static program analysis;matrices over a kleene algebra;static intraprocedural data flow analysis;labelled transition systems	Static program analysis consists of compile-time techniques for determining properties of programs without actually running them. Using Kleene algebra, we formalize four instances of a general class of static intraprocedural data flow analyses known as ‘gen/kill’ analyses. This formalization exhibits the dualities between the four analyses in a clear and concise manner. We provide two equivalent sets of equations characterizing the four analyses for two different representations of programs, one in which the statements label the nodes of a control flow graph and one in which the statements label the transitions. We formally describe how the data flow equations for the two representations are related. We also prove the soundness of the KA based approach with respect to the standard approach. c © 2006 Elsevier B.V. All rights reserved.	compile time;compiler;control flow graph;data-flow analysis;dataflow;kleene algebra;static program analysis	Therrezinha Fernandes;Jules Desharnais	2007	Sci. Comput. Program.	10.1016/j.scico.2006.01.009	kleene star;kleene's recursion theorem;computer science;data-flow analysis;kleene algebra;programming language;algorithm	PL	-15.992614821595541	22.377714148871405	37120
99f29abc32d73c6bda80ed8ff3ddca3e775c4dc1	strong representability of fork algebras, a set theoretic foundation	theoretical foundation		theory	István Németi	1997	Logic Journal of the IGPL	10.1093/jigpal/5.1.3	combinatorics;computer science;mathematics;algorithm	Logic	-6.755785415009972	10.151761000534712	37138
8bebe9d5aff6d28476ea03e4bcfcea420540b3fa	true lies: lazy contracts for lazy languages		Contracts are a proven tool in software development. They provide specifications for operations that may be statically verified or dynamically validated by contract monitoring. Contract monitoring for lazy programming languages does not have a generally accepted basis. This paper discusses three approaches, eager, semi-eager, and lazy monitoring, all of which are flawed. The first two may change program behavior, while the last two may lead to silent contract violations.	design by contract;idempotence;lazy evaluation;no silver bullet;programming language;requirement;semiconductor industry;software development;subject reduction;taxonomy (general)	Markus Degen;Peter Thiemann;Stefan Wehr	2009			software development;programming language;computer science	PL	-21.79239570947764	29.78431636197996	37215
b3bf3a7c206ff1f7afa766ca40bdb3dee60d6a02	a comparative exploration of concurrent logic languages		The execution model of Prolog, the first popular language based on Horn Clauses, was designed for efficient evaluation on von Neumann architectures. An alternative process model of execution, better suited for parallel evaluation and reactive programming, has given rise to a new class of languages based on Horn Clause logic, concurrent logic languages. There appears to be a profusion of languages which claim to fall into this class and it is difficult for an initiate to appreciate why each is the way it is. One notable member of this class, FGHC, forms the cornerstone of the Japanese 5th Generation Initiative. Fortunately, the seemingly exponential growth in these languages is only an illusion. A finite number of synchronization mechanisms arise from attempting (or sometimes not attempting) to control two principle synchronization difficulties: the premature binding problem and the binding conflict problem. Suitable combinations of these synchronization mechanisms reproduce the languages of this family. A background knowledge of Prolog is assumed and some familiarity with the difficulties encountered in concurrency would be advantageous.	binding problem;concurrency (computer science);fifth generation computer;horn clause;logic programming;process modeling;prolog;reactive programming;time complexity;von neumann architecture	Graem A. Ringwood	1989	Knowledge Eng. Review	10.1017/S0269888900005130	natural language processing;description logic;theoretical computer science;ontology language;logic programming;algorithm	PL	-17.916582246650098	12.86243698247292	37233
eddd839f63dd36e1d5998e02baecd920fbb499f2	model-checking continous-time markov chains	continuous time;real time;natural extension;continuous time markov chain;number theory;formal verification;discrete time markov chain;model checking;transcendental number theory	We present a logical formalism for expressing properties of continuous-time Markov chains. The semantics for such properties arise as a natural extension of previous work on discrete-time Markov chains to continuous time. The major result is that the verification problem is decidable; this is shown using results in algebraic and transcendental number theory.	formal system;linear algebra;markov chain;model checking	Adnan Aziz;Kumud Sanwal;Vigyan Singhal;Robert K. Brayton	2000	ACM Trans. Comput. Log.	10.1145/343369.343402	uniformization;matrix analytic method;markov decision process;model checking;time reversibility;markov chain;markov kernel;combinatorics;discrete phase-type distribution;number theory;discrete mathematics;markov property;formal verification;computer science;continuous-time markov chain;examples of markov chains;balance equation;mathematics;markov renewal process;additive markov chain;markov algorithm;markov process;markov chain mixing time;markov model;transcendence theory;programming language;absorbing markov chain;variable-order markov model	Logic	-6.401604141117302	24.8311376512142	37237
af1dd4f79aad62436025eb7a1643c1411e57ef91	default logic and specification of nonmonotonic reasoning	nonmonotonic reasoning;default logic	In this paper constructions leading to the formation of belief sets by agents are studied. The focus is on the situation when possible belief sets are built incrementally in stages. An in® nite sequence of theories that represents such a process is called a reasoning trace. A set of reasoning traces describing all possible reasoning scenarios for the agent is called a reasoning frame. Default logic by Reiter is not powerful enough to represent reasoning frames. In the paper a generalization of default logic of Reiter is introduced by allowing in® nite sets of justi® cations. This formalism is called in® nitary default logic. In the main result of the paper it is shown that every reasoning frame can be represented by an in® nitary default theory. A similar representability result for antichains of theories (belief frames) is also presented.	default logic;formal system;frame language;knowledge representation and reasoning;nl (complexity);non-monotonic logic;principle of abstraction;temporal logic;tracing (software);uninterpreted function	Joeri Engelfriet;Victor W. Marek;Jan Treur;Miroslaw Truszczynski	2001	J. Exp. Theor. Artif. Intell.	10.1080/09528130117278	computer science;artificial intelligence;non-monotonic logic;automated reasoning;deductive reasoning;default logic;algorithm	AI	-16.707324306781626	8.588952049169269	37245
7021d65d51276063189d6e435a132d108cd53643	logical programming for the telegram analysis problem	entrada salida;formal specification;gestion fichier;ingenieria logiciel;logical programming;file management;software engineering;attribute grammar;specification formelle;input output;definite clause grammar;programmation logique;forme backus naur;gramatica por atributo;estructura datos;manejo archivos;grammaire par attribut;genie logiciel;structure donnee;grammaire clause definie;logic programs;programacion logica;data structure;backus naur form;entree sortie	Abstract   The telegram analysis problem posed by Henderson and Snowdon has been repeatedly taken into account. This paper adds yet another contribution to the literature on this problem. We propose rigorous specification methods, and describe how programs can be derived from BNF to Definite Clause Grammar in Prolog by two different methods. One method is especially useful for large scale problems and has been applied to file manipulation, and the other is applicable to simple problems.		Koji Torii;Yuji Sugiyama;Mamoru Fujii;Tadao Kasami;Yoshitomi Morisawa	1987	Comput. Lang.	10.1016/0096-0551(87)90008-7	backus–naur form;input/output;data structure;computer science;artificial intelligence;formal specification;definite clause grammar;programming language;attribute grammar;algorithm	Logic	-20.33064725282721	23.932465322855915	37256
731b30e866bf605066b402813c89a8daa2478162	resolution for branching time temporal logics: applying the temporal resolution rule	automated deduction;resolution;temporal logic;normal form branching time temporal logic branching time temporal resolution theorem prover computation tree logic resolution based approach;loop detection algorithm;temporal logics;trees mathematics;logic mathematics concurrent computing power system modeling automata detection algorithms dh hemts;theorem proving;theorem prover;temporal resolution;branching time;normal form;trees mathematics temporal logic theorem proving	In this paper we propose algorithms to implement a branching time temporal resolution theorem prover. The branching time temporal logic considered is Computation Tree Logic (CTL), often regarded as the simplest useful logic of this class. Unlike the majority of the research into temporal logic, we adopt a resolution-based approach. The method appliestepandtemporalresolution rules to the set of formulae in a normal form. Whilst step resolution is similar to the classical resolution rule, the temporal resoluti on rule resolves a formula, ', that must eventually occur with a set of formulaethat together imply that ' can never occur. Thus the method is dependent on the efficient detection of such sets of formulae. We present algorithms to search for these sets of formulae, give a correctness argument, and examples of their operation.	a-normal form;algorithm;automata theory;automated theorem proving;breadth-first search;computation tree logic;correctness (computer science);display resolution;method of analytic tableaux;parallels desktop for mac;resolution (logic);subsumption architecture;temporal logic;unification (computer science);while	Alexander Bolotov;Clare Dixon	2000		10.1109/TIME.2000.856598	discrete mathematics;resolution;linear temporal logic;interval temporal logic;computation tree logic;mathematics;skolem normal form;second-order logic;algorithm	AI	-13.584528820907854	22.053697056551368	37267
258a7d0a84f8200068d749a0ff4f3638951843a9	a logic-algebraic approach to decision taking in a railway interlocking system	graph theory;railway interlocking systems;00a69;logic;13l05;decision theory	The safety of a railway network is a very important issue considered very labour-intensive. Authors have developed different approaches in order to detect automatically the safety for mid-small railway networks. Although these approaches are very simple to implement, they have the drawback of being unsuitable to large networks since the algorithm takes large time to be run. In this paper, we show a new algebraic model which, besides being also simple to implement, has the advantage of being very fast and consequently can be used for checking the safety in a large railway network.	algorithm;linear algebra	Antonio Hernando;Eugenio Roanes-Lozano;Roberto Maestre-Martínez;Jorge Tejedor	2012	Annals of Mathematics and Artificial Intelligence	10.1007/s10472-012-9321-y	simulation;decision theory;graph theory;mathematics;operations research;logic;statistics	AI	-24.22778625975637	15.470201369332523	37276
1325b5440397af5ae35ba992f58fd74667a2d8de	on end extensions of models of subsystems of peano arithmetic	end extension;sgr;induction schema;schemation sigma n collection;arithmetique peano;arithmetized completeness theorem;language theory;schema induction;n collection schema;teoria lenguaje;arithmetique;foundation mathematics;induccion;aritmetica;induction;arithmetics;peano arithmetic;sigma n collection schema;n induction schema;ultrafilter;extension;aritmetica peano;theorie langage;mathematique fondation	We survey results and problems concerning subsystems of Peano Arithmetic. In particular, we deal with end extensions of models of such theories. First, we discuss the results of Paris and Kirby (Logic Colloquium ’77, North-Holland, Amsterdam, 1978, pp. 199–209) and of Clote (Fund. Math. 127 (1986) 163; Fund. Math. 158 (1998) 301), which generalize the MacDowell and Specker theorem (Proc. Symp. on Foundation of Mathematics, Warsaw, 1959, Pergamon Press, Oxford, 1961, pp. 257–263) we also discuss a related problem of Kaufmann (On existence of n end extensions, Lecture Notes in Mathematics, Vol. 859, Springer, Berlin, 1980, p. 92). Then we sketch an alternative proof of Clote’s theorem, using the arithmetized completeness theorem in the spirit of McAloon (Trans. Amer. Math. Soc. 239 (1978) 253) and Paris (Some conservation results for fragments of arithmetic, Lecture Notes in Mathematics, Vol. 890, Springer, Berlin, 1981, p. 251). c © 2001 Elsevier Science B.V. All rights reserved.	kochen–specker theorem;peano axioms;springer (tank)	Costas Dimitracopoulos	2001	Theor. Comput. Sci.	10.1016/S0304-3975(00)00110-9	combinatorics;ultrafilter;computer science;philosophy of language;pure mathematics;mathematics;foundations of mathematics;peano axioms;algorithm;algebra	Theory	-9.765052414570892	11.480373300488882	37297
ad968bca2a94e0b327204ef04a2e34b43e0389bd	elastiq: answering similarity-threshold instance queries in el		Recently an approach has been devised how to employ concept similarity measures (CSMs) for relaxing instance queries over EL ontologies in a controlled way. The approach relies on similarity measures between pointed interpretations to yield CSMs with certain properties. We report in this paper on Elastiq, which is a first implementation of this approach and propose initial optimizations for this novel inference. We also provide a first evaluation of Elastiq on the GeneOntology.	ontology (information science)	Andreas Ecke;Maximilian Pensel;Anni-Yasmin Turhan	2015			machine learning;ontology (information science);artificial intelligence;inference;computer science	Web+IR	-22.82717072519841	8.009057353594065	37412
56f1ab64c2b9d0abcdb1300aaf7955a36cb22832	the use of planning critics in mechanizing inductive proofs	planning critics;mechanizing inductive proofs	Proof plans provide a technique for guiding the search for a proof in the context of tactical style reasoning. We propose an extension to this technique in which failure may be exploited in the search for a proof. This extension is based upon the concept of planning critics. In particular we illustrate how proof critics may be used to patch proof plans in the domain of inductive proofs.	automated planning and scheduling;inductive reasoning;mathematical induction	Andrew Ireland	1992		10.1007/BFb0013060	theoretical computer science;algorithm;computer science;mathematical proof	AI	-18.38575028038816	16.829345808446895	37473
272b5abbcc252924e3fe3a7146ca93643186cd71	conditional termination of loops over heap-allocated data	informatica;software;java bytecode;program transformation;heap sensitive analysis;sistemas operativos ordenadores;termination;static analysis	Static analysis which takes into account the values of data stored in the heap is considered complex and computationally intractable in practice. Thus, most static analyzers do not keep track of object fields nor of array contents, i.e., they are heap-insensitive. In this article, we propose locality conditions for soundly tracking heap-allocated data in Java (bytecode) programs, by means of ghost non-heap allocated variables. This way, heap-insensitive analysis over the transformed program can infer information on the original heap-allocated data without sacrificing efficiency. If the locality conditions cannot be proven unconditionally, we seek to generate aliasing preconditions which, when they hold in the initial state, guarantee the termination of the program. Experimental results show that we greatly improve the accuracy w.r.t. a heap-insensitive analysis while the overhead introduced is reasonable.	aliasing;computational complexity theory;java bytecode;locality of reference;overhead (computing);precondition;static program analysis	Elvira Albert;Puri Arenas;Samir Genaim;Germán Puebla;Guillermo Román-Díez	2014	Sci. Comput. Program.	10.1016/j.scico.2013.04.006	real-time computing;computer science;programming language;static analysis;algorithm	SE	-20.66411463405064	30.48249466062285	37483
9d79d9e905df4c5548c043db86f4290b249053c5	logical approximation and compilation for resource-bounded reasoning			approximation;compiler	David Rajaratnam	2008				AI	-22.07121860779865	20.31239275943261	37505
97ca9bc9634ae0597a08ee7251c00a4de77d545b	attribute coupled grammars	attribute grammars;storage management;attribute grammar;recursive evaluation;target language;source language;evaluation by need;lisp;circularity tests;composition operator	In this paper, attribute grammars are viewed as specifying translations from source language terms into target language terms. The terms are constructed over a hierarchical signature consisting of a semantic and a syntactic part. Attribute grammars are redefined to become morphisms in the category of such signatures, called attribute coupled grammars, such that they come with an associative composition operation. The composition allows for a new kind of modularity in compiler specifications. The paper also discusses properties of the concept with respect to attribute evaluation and application as a tree transformation device.	attribute grammar;compiler;definition;digital signature	Harald Ganzinger;Robert Giegerich	1984		10.1145/502874.502890	natural language processing;context-sensitive grammar;tree-adjoining grammar;indexed grammar;l-attributed grammar;attribute domain;phrase structure grammar;computer science;composition operator;s-attributed grammar;lisp;definite clause grammar;context-free grammar;programming language;attribute grammar;algorithm	PL	-23.859746375153424	19.992137743577437	37519
13708fe9f94fda18c02111568a2484cfc587cb0a	monotonicity rules in calculus				Glen Douglas Anderson;Mavina Vamanamurthy;Matti Vuorinen	2006	The American Mathematical Monthly		mathematical analysis;discrete mathematics;differentiation rules;proof calculus;natural deduction	Theory	-12.125568681121733	12.74684802799037	37530
dad68e0c089c70e2f860952dd34c7721c237bfeb	axiomatizing gsos with termination	complete axiomatization;automatic generation;structured operational semantics;transition systems	We discuss a combination of GSOS-type structural operational semantics with explicit termination, that we call the tagh-format (tagh being short for termination and GSOS hybrid). The tagh-format distinguishes between transition and termination rules, but allows besides active and negative premises as in GSOS,. also for, what is called terminating and passive arguments. We extend the result of Aceto, Bloom and Vaandrager on the automatic generation of sound and complete axiomatizations for GSOS to the setting of tagh-transition systems. The construction of the equational theory is based upon the notion of a smooth and distinctive operation, which have been generalized from GSOS to tagh. We prove the soundness of the synthesized laws and show their completeness modulo bisimulation. The examples provided indicate a significant, though yet not ideal, improvement over the axiomatization techniques known so far.	axiomatic system;bisimulation;gs/os;geosynchronous orbit;modulo operation;newman's lemma;operational semantics	Jos C. M. Baeten;Erik P. de Vink	2001	J. Log. Algebr. Program.	10.1016/j.jlap.2004.03.001	discrete mathematics;mathematics;algorithm	Logic	-11.821981515361355	15.184320071042338	37546
2f5333b1b18dbccdaa2791201f56593501194ffa	temporal capabilities in support of conceptual process modeling using object-role modeling	settore ing inf 05 sistemi di elaborazione delle informazioni	Conceptual data modeling languages must be provided with temporal capabilities to support the data evolution throughout the execution of a conceptual process model. Asides from supporting the storage of historical data, temporal capabilities must also provide the means for verifying the consistency between the data temporal properties and the data modification resulting from the process execution. The Object-Role Modeling (ORM) language is a conceptual data modeling language that is based on the concepts of Fact (i.e. true statements on the represented world), Fact Type, and Fact Base (i.e. the set of all the Facts). Currently, the ORM language does not address the specification of Facts temporal properties, and therefore does not also support the verification of Facts variations during a process execution. The paper introduces an initial ORM overlay methodology that aims to laying the foundation of the conceptual modeling structures that can support the verification of temporal evolution of conceptual data models (i.e., whether a Fact can be asserted or retracted, depending on its temporal properties). Moreover, the overlay methodology also defines a temporal visual notation and an initial semi-formal temporal verbalization that eases the use of the methodology to the ORM modelers. A simple example illustrates the potential application of the overlay methodology.	object-role modeling;process modeling	Daniele Gianni;Paolo Bocciarelli;Andrea D'Ambrogio	2014			computer science;data mining;database;algorithm	Robotics	-30.9946804397039	15.377076230288846	37566
64a9f3b408bbaf856af2053f14aec85511080d2b	an overview of a system for automatic generation of file conversion programs	automatic file translation;data description language;automatic generation;file conversion;data manipulation language;automatic generation of file conversion programs	Abstract#R##N##R##N#This paper describes a processor which automatically produces file conversion programs based on non-procedural user specification. The processor accepts, as input, descriptions of a source file and a desired target file with some auxiliary descriptions of associations between the two. This is specified by a user in a Data Description Language (DDL). To specify validation criteria, complex conversions not built in to the system, security criteria or summary processes, the system also accepts specifications in a Data Manipulation Language (DML). It produces, as an output, a conversion program in PL/1 capable of converting the described source file into the desired target file. The paper describes the structure, system design, capabilities and applications of the DDL/DML language and processor, including an illustrative example.		N. Adam Rin;Maxine D. Brown	1975	Softw., Pract. Exper.	10.1002/spe.4380050208	self-certifying file system;data conversion;data manipulation language;memory-mapped file;device file;computer file;flat file database;zap file;computer science;class implementation file;versioning file system;operating system;unix file types;database;open;data file;programming language;design rule for camera file system	NLP	-32.78729999763574	25.749307024135472	37585
0abffd498918488668c86afcee53b6b35268fe58	hiding propositional constants in bdds	satisfiability;propositional logic;wijsbegeerte;binary decision diagram	In it is described how correctness of safety control systems of railways interlockings can be expressed using large propositional formulas Interlockings are correct if these formulas are tautologies Given the impressive capabilities to show propositional formulas tautologies ascribed to Binary Decision Diagrams BDDs it seems natural to apply them to the formulas that we obtained Somewhat to our dissatisfaction it appeared that the formulas had to be reduced with for the plain BDD technique to yield results The propositional formulas consist of lines and BDDs could only be constructed of formulas covering up to lines Even if dynamic variable ordering is applied formulas up till lines can be handled In this note we describe a very straightforward and easy to implement extension to the BDD technique using hiding functions It turns out that these functions make it possible to check whether the sizable formulas describing the correctness of interlockings are tautologies which shows their strength This strength is con rmed in where it has been described how using partitioned transition relations the complexity of BDD based veri cations can be signi cantly reduced In essence these partitioned transition relations can be viewed as an application of the technique presented in this paper At the end of the article we describe an experiment with randomised SAT formulas It is shown that if the number of occurrences of variables in SAT formulas is small hiding functions apply very well All experiments mentioned above indicate that hiding functions are useful if variables occur rather localised in formulas and or have a relatively small number of occurrences	binary decision diagram;control system;correctness (computer science);experiment;internationalization and localization;naruto shippuden: clash of ninja revolution 3;newton–cotes formulas;propositional calculus	Jan Friso Groote	1996	Formal Methods in System Design	10.1007/BF00121264	propositional formula;zeroth-order logic;computer science;propositional variable;well-formed formula;propositional calculus;binary decision diagram;algorithm;satisfiability	Logic	-13.892376409508381	20.64750544500543	37662
1267610671d3271f148dedcd90d2b6f72724b2fc	possible model semantics for disjunctive databases.	minimal model	This paper presents a novel approach to the semantics of deductive databases. The possible model semantics is introduced as an alternative approach to the classical minimal model semantics. The possible model semantics can distinguish both inclusive and exclusive disjunctions, and provide a exible mechanism for inferring negation in disjunctive databases. The possible model semantics is characterized by a new xpoint semantics of disjunctive databases. A proof procedure called the SLD P -resolution is presented and shown to be sound and complete with respect to the possible model semantics.	3d xpoint;chan's algorithm;deductive database;disjunctive normal form;logical equality;nato architecture framework;negation as failure;p (complexity);possible world;relational database;sld resolution;strati (automobile)	Chiaki Sakama	1989			natural language processing;stable model semantics;data mining;algorithm	DB	-16.49564311833174	9.416123579255533	37725
f4470cc4bd4c47c0faaf415b621015caef620344	semantics of non-terminating rewrite systems using minimal coverings	rewrite rule;haslab haslab uminho;term rewrite system;rewrite systems;normal form	We propose a new semantics for rewrite systems ba.~ed on interpreting rewrite rules as in­ equatioIlB between terms in an ordered algebra. In part.icular, we show thai the algebra. of normal forms in a terminating system is a uniqnely minimal covering of the term algebra. In the non-terminating ca..~e, the existence of this minimal covering is established in the comple­ tion of an ordered algebra formed by rewrit.ing sequences. We thus generalize the properties of normal forms far: non-terminating systelil~ to this minimal covering. ThesE' include the exi~tence of normal forms for arbitrary rewrite ~ystems, and their uniqueness for conBue-nt ~ystems, in which Ca<le the algebra of normal forms i~ isomorphic to the canonical quotient. algebra associated with the rule~ when seen as eqnations. This extend!> the benefits of alge­ braic semantics to systems with non-determinist.ic and non-t.erminating computations. V·le first study properties of abstract. order~, and then instantiat.e the~e to term rewriting sy~tems.	computation;database normalization;divergence (computer science);efficient xml interchange;newman's lemma;rewrite (programming);rewriting;term algebra	José Barros;Joseph A. Goguen	1995		10.1007/3-540-61377-3_29	discrete mathematics;normalization property;mathematics;normal-form game;algorithm;rippling;algebra	DB	-11.00988894334096	17.59328709958851	37728
7f9505054e9573044ce5419e6ba02507c0e2265e	epistemic planning for single and multi-agent systems	automated planning;multi agent system;partial observability;kripke model;complexity class;plan time vs run time knowledge;event models;epistemic logic;partial observation;dynamic epistemic logic	"""Epistemic planning for singleand multi-agent systems In this paper, we investigate the use of event models for automated planning. Event models are the action defining structures used to define a semantics for dynamic epistemic logic. Using event models, two issues in planning can be addressed: Partial observability of the environment and knowledge. In planning, partial observability gives rise to an uncertainty about the world. For single-agent domains, this uncertainty can come from incomplete knowledge of the starting situation and from the nondeterminism of actions. In multi-agent domains, an additional uncertainty arises from the fact that other agents can act in the world, causing changes that are not instigated by the agent itself. For an agent to successfully construct and execute plans in an uncertain environment, the most widely used formalism in the literature on automated planning is """"belief states"""": sets of different alternatives for the current state of the world. Epistemic logic is a significantly more expressive and theoretically better founded method for representing knowledge and ignorance about the world. Further, epistemic logic allows for planning according to the knowledge (and iterated knowledge) of other agents, allowing the specification of a more complex class of planning domains, than those simply concerned with simple facts about the world. We show how to model multi-agent planning problems using Kripke-models for representing world states, and event models for representing actions. Our mechanism makes use of slight modifications to these concepts, in order to model the internal view of agents, rather than that of an external observer. We define a type of planning domain called epistemic planning domains, a generalisation of classical planning domains, and show how epistemic planning can successfully deal with partial observability, nondeterminism, knowledge and multiple agents. Finally, we show epistemic planning to be decidable in the single-agent case, but only semi-decidable in the multi-agent case."""	automated planning and scheduling;dynamic epistemic logic;epistemic modal logic;iteration;kripke semantics;multi-agent system;nondeterministic algorithm;semantics (computer science);semiconductor industry;undecidable problem	Thomas Bolander;Mikkel Birkegaard Andersen	2011	Journal of Applied Non-Classical Logics	10.3166/jancl.21.9-34	complexity class;epistemic modal logic;computer science;knowledge management;artificial intelligence;multi-agent system;mathematics;epistemic possibility;algorithm	AI	-17.52313094730369	5.246325784832726	37751
2225c70e44ea96ae598900821db4734ee7470b72	on quasivariety semantics of fragments of intuitionistic propositional logic without exchange and contraction rules	propositional logic	A b s t r a c t. Let H be the Hilbert-style intuitionistic propositional calculus without exchange and contraction rules (as given by Ono and Komori). An axiomatization of H with the separation property is provided. Of the superimplicational fragments of H, it is proved that just two fail to be finitely axiomatized, and that all are algebraizable. The paper is a study of these fragments, their equivalent algebraic (quasivariety) semantics and their axiomatic extensions.	axiomatic system;intuitionistic logic;linear algebra;propositional calculus	Clint J. van Alten;James G. Raftery	1997	Reports on Mathematical Logic		modal logic;propositional formula;dynamic logic;zeroth-order logic;t-norm fuzzy logics;modal μ-calculus;discrete mathematics;classical logic;resolution;description logic;higher-order logic;horn-satisfiability;tautology;many-valued logic;intuitionistic logic;intermediate logic;mathematics;propositional variable;well-formed formula;propositional calculus;atomic formula;algorithm;rule of inference;autoepistemic logic	Logic	-12.14955801439912	13.034726379965415	37783
9f374e055fdac365cc32cd7fa366aa7c9928a98f	foundations of the conceptual schema approach to information systems		This article is a contribution to the present dicussions on the conceptual schema approach to data bases and information systems.		Eckhard D. Falkenberg	1981		10.1007/978-94-009-7029-8_1	logical schema;three schema approach;conceptual schema;database;database schema	DB	-31.752517517926105	10.311168218266143	37792
2c1796eb88bff830baf2cb37b0d6fd55f938f066	inherent logical structure of computational data: its role in storage and retrieval strategies to support user queries	logical relation;scientific data;data management;fluid flow;scientific database;object oriented;object oriented database;data structure;domain specificity;coordinate system	Abs t r ac t . This paper describes a data management strategy which implements the thesis that the design of object-oriented, scientific databases (e.g., the data structures, hashing procedures) should incorporate the domain-specific relations of objects in the database to facilitate extraction and extrapolation services required by user queries. The strategy has been used to create object-oriented databases from data sets computed by iterating over meshes, incorporating a variety of geometries and coordinate systems. One set of data structures defining the data object and the hashing maps is sufficient to manage this type of data (multiple scalar and vector quantities at nodes in an n dimensional mesh) for interactive examination on both serial and massively parallel facilities.. The question raised is: can this approach which exploits the inherent logical relations of meshed data generated by simulations of fluid flows be applied to scient i f ic data from other domains with different inherent relations7	computation;data structure;database;extrapolation;logical relations;map;simulation	Sandra Walther	1993		10.1007/BFb0021147	data modeling;data structure;data management;computer science;theoretical computer science;coordinate system;database model;data mining;database;programming language;object-oriented programming;data retrieval;logical data model;database design;data	DB	-31.26598387825247	11.69559457418198	37823
ddc8a08e5fa805be7134fa293827fa15383a08f7	knowledge and strategic ability for model checking: a refined approach	imperfect information;model checking	Wepresent a translation that reduces epistemic operators to strategic operators in the context ofmodel checking. The translation is a refinement of the one from [7, 9], and it improves on the previous scheme in two ways. First, it does not suffer any blowup in the length of formulae (the one from [7, 9] did). Second, the new translation is defined in a more general setting: additional constraints can be imposed on strategy profiles that agents can execute; we show that the translation is still valid in such a general case.	alternating-time temporal logic;model checking;norm (social);refinement (computing);time complexity	Wojciech Jamroga	2008		10.1007/978-3-540-87805-6_10	model checking;computer science;artificial intelligence;perfect information;abstraction model checking;algorithm	Logic	-13.566488985968114	23.946385913372616	37856
a4df168f7b0d648b03614135a136d790328d8ac7	natural language specification of performance trees	accident and emergency;performance trees;natural language;performance analysis;performance requirements specification;petri net;requirement specification	The accessible specification of performance queries is a key challenge in performance analysis. To this end, we seek to combine the intuitive aspects of natural language query specification with the expressive power and flexibility of the Performance Tree formalism. Specifically, we present a structured English grammar for Performance Trees, and use it to implement a Natural Language Query Builder (NLQB) for the Platform Independent Petri net Editor (PIPE). The NLQB guides users in the construction of performance queries in an iterative fashion, presenting at each step a range of natural language alternatives that are appropriate in the query context. We demonstrate our technique in the specification of performance queries on a model of a hospital’s Accident and Emergency department.	discharger;entity;iterative method;natural language user interface;petri net;profiling (computer programming);programming language specification;semantics (computer science);service-level agreement;structured english	Lei Wang;Nicholas J. Dingle;William J. Knottenbelt	2008		10.1007/978-3-540-87412-6_11	natural language processing;specification language;computer science;database;programming language;programming language specification;language of temporal ordering specification	DB	-27.932469378811998	18.9395157021013	37873
46c5a48a972a5fcdc183072940039a712efb3bb9	gras, a graph-oriented (software) engineering database system	distribution;incremental computation;base integrada dato;complex objects;database system;error recovery;medio ambiente;hipertexto;graph method;software systems;ingenieria logiciel;metodo grafo;software engineering;data model;methode graphe;grafo;graph oriented database;multiple reader single writer;software engineering environment;graph rewriting;object oriented;almacenamiento;reecriture;environment;graph;graphe;concurrency control;stockage;genie logiciel;derived data;oriente objet;analyse transactionnelle;environnement;integrated database;attributed graphs;controle concurrence;control concurrencia;modula 2;information system;rewriting;software engineering environments;c;version control;orientado objeto;storage;hypertexte;systeme information;hypertext;office automation;transactional analysis;reescritura;graph rewriting systems;analisis transaccional;sistema informacion;base donnee integree;ey words	Modern software systems for application areas such as software engineering, CAD, or office automation are usually highly interactive and deal with rather complex object structures. For the realization of these systems a nonstandard database system is needed which is able to efficiently handle different types of coarseand fine-grained objects (like documents and paragraphs), hierarchical and non-hierarchical relations between objects (like composition-links and cross-references), and finally attributes of rather different size (like chapter numbers and bitmaps). Furthermore, this database system should support incremental computation of derived data, undo/redo of data modifications, error recovery from system crashes, and version control mechanisms. In this paper, we describe the underlying data model and the functionality of GRAS, a database system which has been designed according to the requirements mentioned above. Furthermore, we motivate our central design decisions concerning its realization, and report on experiences and applications. f(ey words: attributed graphs, graph rewriting systems, distribution, version control, derived data, software engineering environments	bitmap;computation;computer-aided design;control system;cross-reference;data model;database;dynamic problem (algorithms);graph rewriting;incremental computing;requirement;software engineering;software system;undo;version control	Norbert Kiesel;Andy Schürr;Bernhard Westfechtel	1995	Inf. Syst.	10.1016/0306-4379(95)00002-L	distribution;modula-2;hypertext;rewriting;data model;computer science;revision control;artificial intelligence;theoretical computer science;operating system;concurrency control;data mining;database;transactional analysis;graph;natural environment;programming language;object-oriented programming;computer security;information system;algorithm;database design;software system;graph rewriting	DB	-30.483234800077998	13.23900834138376	37876
851aca4aeb8b0e5d5e83f25e531fdfada5f440fc	a logic based approach for dynamic access control	modelizacion;controle acces;logica dynamica;mise a jour;securite informatique;interrogation base donnee;interrogacion base datos;semantics;intelligence artificielle;control constraint;semantica;semantique;actualizacion;computer security;dynamic logic;modelisation;control constrenido;access control policy;policy evaluation;query evaluation;seguridad informatica;stable model semantics;artificial intelligence;logique dynamique;access control;inteligencia artificial;logic programs;modeling;database query;updating;contrainte espace commande	ThePolicyUpdater system is a fully-implemented access control system that provides policy evaluations as well as dynamic policy updates. These functions are achieved by the use of a logic-based language L to represent the underlying access control policies, constraints and update propositions. The system performs authorisation query evaluations and conditional policy updates by translating the language L to a normal logic program in a form suitable for evaluation using theStable Model semantics.	access control;authorization;control system;e-commerce;first-order logic;first-order predicate;l (complexity);logic programming;temporal logic	Vino Fernando Crescini;Yan Zhang	2004		10.1007/978-3-540-30549-1_54	dynamic logic;systems modeling;stable model semantics;computer science;artificial intelligence;access control;database;semantics;computer security;algorithm	DB	-21.73400861772018	11.62782973628392	37885
3a7d2cc393c887406e62b8524868b18aba50f33e	functional approach to texture generation	langage fonctionnel;combinators;lenguaje programacion;texture;image processing;programming language;lenguaje funcional;procesamiento imagen;tiling;functional programming;traitement image;procedural texture;sintesis imagen;image synthesis;generation fonction;tesselations;textura;pavage;clean;langage programmation;synthese image;generacion funcion;programmation fonctionnelle;function generation;functional language;programacion funcional;images;noise	We show the applicability of pure functional programming for the construction of modules which create procedural textures for image synthesis. We focus our attention to the construction of generic combinators and transformers of textures, which permit to write texture generators of substantial complexity in a very compact and intuitive manner. We present a concrete package implemented in Clean.	clean;combinatory logic;functional approach;functional programming;parser combinator;procedural texture;rendering (computer graphics);transformers	Jerzy Karczmarczuk	2002		10.1007/3-540-45587-6_15	combinatory logic;computer science;noise;artificial intelligence;procedural texture;mathematics;texture;programming language;functional programming;algorithm	Graphics	-25.356149001503233	23.469557573697173	37931
b58cfb31da01717c1b6c9456767c7ca43ee89cc2	improving the applicability of adaptive consistency: preliminary results	metodo adaptativo;filtering;filtrage;variable elimination;filtrado;methode adaptative;constraint satisfaction;satisfaction contrainte;adaptive method;satisfaccion restriccion	ADC • ADC is the reference algorithm to solve CSP by complete inference • ADC is a specialization of bucket elimination, a more general algorithm used in optimization • The time and space complexity of ADC are exponential on the problem induced width w∗. • ADC follows a Dynamic Programming schema and computes all solutions • Two basic operations: • Join. c 1 r is a new constraint with scope var(c) ∪ var(r) that includes all the tuples permitted by both constraints when values of common variables coincide.	algorithm;dspace;dynamic programming;local consistency;mathematical optimization;partial template specialization;time complexity	Martí Sánchez-Fibla;Pedro Meseguer;Javier Larrosa	2004		10.1007/978-3-540-30201-8_61	filter;variable elimination;mathematical optimization;discrete mathematics;constraint satisfaction;computer science;mathematics;algorithm	DB	-7.412615940851437	18.91478721427153	37947
b8d5d27f140c583807906174ccffcd340dfdf308	querying owl 2 ql ontologies under the sparql metamodeling semantics entailment regime		OWL2QL is the profile of OWL2 targeted to Ontology-Based Data Access (OBDA) scenarios, where large amount of data are to be accessed, and thus answering conjunctive queries over data is the main task. However, this task is quite restrained wrt the classical KR Ask-and-Tell framework based on querying the whole theory, not only facts (data). If we use SPARQL as query language, we get much closer to this ideal. Indeed, SPARQL queries over OWL2QL, under the so-called Direct Semantics Entailment Regime, may comprise any assertion expressible in the language, i.e., both ABox atoms and TBox atoms, including inequalities expressed by means of DifferentIndividuals. Nevertheless this regime is hampered by the assumption that variables in queries need to be typed, meaning that the same variable cannot occur in positions of different types, e.g., both in class and individual position (punning). In this paper we dismiss this limiting assumption by resorting to a recent meta modeling semantics and show that query answering in the resulting entailment regime is polynomially compilable into Datalog (and hence PTIME wrt both TBox and ABox).	abox;assertion (software development);conjunctive query;data access;datalog;metamodeling;ontology (information science);p (complexity);query language;sparql;tbox;web ontology language	Gianluca Cima;Giuseppe De Giacomo;Maurizio Lenzerini;Antonella Poggi	2017			web ontology language;natural language processing;sparql;ontology (information science);semantics;metamodeling;logical consequence;artificial intelligence;computer science	AI	-24.226696234090802	8.69010688528064	37954
bbe90679afe8ee4c2170cd9da1e2ea9c84fe8fe0	discourse context and indexicality	lenguaje natural;pragmatics;pragmatica linguistca;langage naturel;semantics;natural language semantics;intelligence artificielle;semantica;semantique;natural language understanding;pragmatique;natural language;indexation;artificial intelligence;inteligencia artificial;natural language pragmatics	Problems related to indexicality, such as Sidelle’s Answering Machine Paradox [10], have been problematic for direct reference theorists. The solutions suggested to date are not wholly satisfactory. I suggest that the correct solution requires an account of context shifting in natural language. However, existing context shifting accounts assume that context shifting is a purely semantic, operator-governed mechanism. This view has trouble dealing with so-called ‘free shifts’, such as the answering machine problem. I discuss these approaches and sketch a new account in terms of discourse context, which should be viewed as a prag-	el torito (cd-rom standard);kaplan–meier estimator;mind;natural language;property (philosophy);vision	Mark Whitsey	2003		10.1007/3-540-44958-2_28	natural language processing;epistemology;computer science;artificial intelligence;semantics;linguistics;natural language;pragmatics	AI	-13.35440640185711	4.488536983190386	37978
7920990781e0a4077e3a2254dbbf40ee6e62c27b	geometric reasoning and artificial intelligence: introduction to the special volume	artificial intelligent;geometric reasoning	Geometry plays an important role in our understanding of the world. It is therefore important for Artificial Intelligence (AI) systems to be able to represent and reason about geometry. This special volume presents a formal approach to the representation of geometric concepts in selected AI applications and describes some recent developments in algebraic methods for reasoning about geometry. Geometry is also the oldest subject in mathematics to be subjected to considerable rigor and formalism. The axiomatic treatment of geometry has been actively investigated for thousands of years, starting with Euclid. In the early 1950s and 1960s, when there was considerable excitement about theorem proving in AI, Gelernter and his associates [14, 15] initiated an effort in mechanical theorem proving in geometry. After some initial success, Gelernter's approach faced the stumbling block of combinatorial search space, even for relatively simple plane geometry theorems. Gelernter's approach towards mechanical geometry theorem proving was synthetic, i.e., it used properties of points, lines, triangles, etc. He employed the properties and definitions of these key geometry concepts in plane geometry to prove theorems using diagram and geometric construction. Earlier in the	artificial intelligence;automated theorem proving;combinatorial search;diagram;euclid;formal system;linear algebra;stumbleupon;synthetic intelligence	Deepak Kapur;Joseph L. Mundy	1988	Artif. Intell.	10.1016/0004-3702(88)90047-1	opportunistic reasoning;artificial architecture;computer science;artificial intelligence;model-based reasoning;machine learning;reasoning system;automated reasoning;artificial intelligence, situated approach	AI	-10.050109869059058	4.914205582485487	37980
6a008f693b55cd6d1779db2fa6b9b5b5c2d32fa0	what is a disposition?	congresses;combination;recueil d articles;reductionism;category;colloques et congres;conditional;disposition;modality;subjonctif;property;conditionnel;predicate;categorie;predicat;circularity;set of articles;combinaison;propriete;reductionnisme;combinacione;modalite;subjunctive;circularite	Attempts to capture the distinction between categorical and dispositional states in terms of more primitive modal notions – subjunctive conditionals, causal roles, or combinatorial principles – are bound to fail. Such failure is ensured by a deep symmetry in the ways dispositional and categorical states alike carry modal import. But the categorical/dispositional distinction should not be abandoned; it underpins important metaphysical disputes. Rather, it should be taken as a primitive, after which the doomed attempts at reductive explanation can be transformed into circular but interesting accounts.	abandonware;causal filter;counterfactual conditional;modal logic;reductionism	Troy Cross	2005	Synthese	10.1007/s11229-005-5857-2	reductionism;category;philosophy;predicate;epistemology;combination;mathematics;disposition;property;algorithm	ML	-13.141006720071811	4.456859523662542	37994
bf13c11d5d83cf6b8b9e7ad830e5361bb9a2c250	a version of o-minimality for the p-adics		 IntroductionIn this paper we formulate a notion similar to o-minimality but appropriate forthe p-adics. The paper is in a sense a sequel to [11] and [5]. In [11] a notion ofminimality was formulated, as follows. Suppose that L, L+are first-order languagesand M+is an L+-structure whose reduct to L is M. Then M+is said to be M-minimal if, for every N+elementarily equivalent to M+, every parameter-definablesubset of its domain N+is definable with parameters by a... 		Deirdre Haskell;Dugald Macpherson	1997	J. Symb. Log.			Theory	-6.560597113209695	13.180331276395853	37995
eca27977046fc6f4d209cfdcb2d2218836ccac06	toward hybrid attack dependency graphs	dependence graph	This extended abstract presents a set of continuous-domain extensions to the attack graph, a formalism used to model the interactions of multiple exploits and assets in a network. These extensions result in a new modeling framework called the hybrid attack dependency graph, which provides the novel capability of modeling continuous state variables and their evolution over the execution of attacks with duration.	interaction;semantics (computer science)	George Louthan;Phoebe Hardwicke;Peter J. Hawrylak;John Hale	2011		10.1145/2179298.2179368	1-planar graph;outerplanar graph;block graph;pathwidth;dependency graph;split graph;cograph;graph product;clique-width;comparability graph;voltage graph;modular decomposition;partial k-tree;chordal graph;line graph	Web+IR	-7.836768882537307	24.622970766769225	38091
874b5c9671abda2f4def50bf118f22d17b1a913b	beyond rational monotony: some strong non-horn rules for nonmonotonic inference relations	satisfiability;nonmonotonic reasoning	Lehmann, Magidor and others have investigated the eeects of adding the non-Horn rule of rational monotony to the rules for preferential inference in nonmonotonic reasoning. In particular, they have shown that every inference relation satisfying those rules is generated by some ranked preferential model. We explore the eeects of adding a number of other non-Horn rules that are stronger than or incomparable with rational monotony, but which are still weaker than plain monotony. Distinguished among these is a rule of determinacy preservation, equivalent to one of rational transitivity, for which we establish a representation theorem in terms of quasi-linear preferential models. An important tool in the proof of the representation theorem is the following purely semantic result, implicit in work of Freund, but here established by a more direct argument: every ranked preferential model generates the same inference relation as some ranked preferential model that is collapsed, in the sense of being both injective and such that each of its states is minimal for some formula. We also consider certain other non-Horn rules which are incomparable with monotony but are implied by conditional excluded middle, and establish a representation result for a central one among them, which we call fragmented disjunction, equivalent to fragmented conjunction, in terms of almost linear preferential models. Finally, we consider brieey some curious Horn rules beyond the preferential ones but weaker than monotony, notably those which we call conjunctive insistence and n-monotony.	horn clause;non-monotonic logic;vertex-transitive graph	Hassan Bezzazi;David Makinson;Ramón Pino Pérez	1997	J. Log. Comput.	10.1093/logcom/7.5.605	discrete mathematics;computer science;non-monotonic logic;satisfiability	Logic	-8.698034334573796	10.353833117411169	38146
02f4995d0825cf2788109cc3a158e2089e5d2cc2	hyper text caching protocol (htcp/0.0)	experimental	"""Status of this Memo This memo defines an Experimental Protocol for the Internet community. It does not specify an Internet standard of any kind. Discussion and suggestions for improvement are requested. Distribution of this memo is unlimited. Abstract This document describes HTCP, a protocol for discovering HTTP caches and cached data, managing sets of HTTP caches, and monitoring cache activity. This is an experimental protocol, one among several proposals to perform these functions. 1. Definitions, Rationale and Scope 1.1. HTTP/1.1 (see [RFC2616]) permits the transfer of web objects from """"origin servers,"""" possibly via """"proxies"""" (which are allowed under some circumstances to """"cache"""" such objects for subsequent reuse) to """"clients"""" which consume the object in some way, usually by displaying it as part of a """"web page."""" HTTP/1.0 and later permit """"headers"""" to be included in a request and/or a response, thus expanding upon the HTTP/0.9 (and earlier) behaviour of specifying only a URI in the request and offering only a body in the response. 1.2. ICP (see [RFC2186]) permits caches to be queried as to their content, usually by other caches who are hoping to avoid an expensive fetch from a distant origin server. ICP was designed with HTTP/0.9 in mind, such that only the URI (without any headers) is used when describing cached content, and the possibility of multiple compatible bodies for the same URI had not yet been imagined."""	cpu cache;cache (computing);design rationale;hypertext transfer protocol;hypertext caching protocol;internet;proxy server;server (computing);uniform resource identifier;web page	Paul Vixie;Duane Wessels	2000	RFC	10.17487/RFC2756	parallel computing;computer science;database;world wide web	Networks	-31.34563649501288	26.15373553615083	38156
2485a871487fa2a9868ae0f037519da8e6411509	pragmatic hypermedia: creating a generic, self-inflating api client for production use	generic api client;hypermedia api;service oriented architecture	Hypermedia API design is a method of creating APIs using hyperlinks to represent and publish an API's functionality. Hypermedia-based APIs bring theoretical advantages over many other designs, including the possibility of self-updating, generic API client software. Such hypermedia API clients only lately have come to exist, and the existing hypermedia client space did not compare favorably to custom API client libraries, requiring somewhat tedious manual access to HTTP resources. Nonetheless, the limitations in creating a compelling hypermedia client were few.  This paper describes the design and implementation of HyperResource, a fully generic, production-ready Ruby client library for hypermedia APIs. The project leverages the inherent practicality of hypermedia design, demonstrates its immediate usefulness in creating self-generating API clients, enumerates several abstractions and strategies that help in creating hypermedia APIs and clients, and promotes hypermedia API design as the easiest option available to an API programmer.	application programming interface;client (computing);hyperlink;hypermedia;hypertext transfer protocol;library (computing);programmer;ruby;self-organization;serialization	Pete Gamache	2014		10.1145/2567948.2579220	computer science;operating system;service-oriented architecture;database;world wide web	PL	-32.26039468263978	26.96833150500063	38167
03b26c7446e382bf8dcd0c14abee71a655e924c9	representing first-order causal theories by logic programs	action language;answer sets;answer set programming;reasoning about actions nonmonotonic causal logic answer set programming;bepress selected works;artificial intelligent;first order;nonmonotonic causal logic;logic in computer science;reasoning about actions;logic programs	Nonmonotonic causal logic, introduced by Norman McCain and Hudson Turner, became a basis for the semantics of several expressive action languages. McCain’s embedding of definite propositional causal theories into logic programming paved the way to the use of answer set solvers for answering queries about actions described in such languages. In this paper we extend this embedding to nondefinite theories and to first-order causal logic.	action language;causal filter;first-order predicate;hudson;logic programming;stable model semantics;theory	Paolo Ferraris;Joohyung Lee;Yuliya Lierler;Vladimir Lifschitz;Fangkai Yang	2012	TPLP	10.1017/S1471068411000081	dynamic logic;zeroth-order logic;description logic;higher-order logic;horn clause;stable model semantics;many-valued logic;action language;computer science;artificial intelligence;answer set programming;first-order logic;computational logic;logic programming;logic;term logic;multimodal logic;algorithm;philosophy of logic;temporal logic of actions;autoepistemic logic	AI	-15.946846470364527	11.55172018098768	38226
5ae2d5ed02fa3cb65d9a8df344ce4341cdb739a3	on prosleptic syllogisms				Czeslaw Lejewski	1961	Notre Dame Journal of Formal Logic	10.1305/ndjfl/1093956876	algorithm;syllogism;mathematics	Crypto	-12.575962738536417	10.85245448292474	38281
88e2594564966337cb1a95801cceb8b55d698706	some sad remarks about string handling in c	c programming language;machine model	Many people have criticized the C programming language for its failure to enforce discipline in programming. They cite the lack of typechecking and the locutions that expose and indeed emphasize the underlying machine model. C is less often criticized for lack of efficiency. Yet in a critical part of the language -- string handling -- the efficiency of C falls short of the mark.String handling is a major application of C. In the original C book by Kernighan and Ritchie, nearly every example has something to do with strings. In systems programming, another major application, string handling is a large part of the task. If a skillful C programmer cannot easily write programs that manipulate strings efficiently, then the language has failed to meet one of its central goals.There are two problems with strings in C. The first is the use of a representation for strings in which a terminating null character indicates the length of the string. The second is the lack of built-in operations on strings analogous to the ones provided for arithmetic data.	assembly language;brian;canonical account;compiler;newman's lemma;null character;programmer;regular expression;string (computer science);string operations;system programming;the c programming language;type system	Paul W. Abrahams	1988	SIGPLAN Notices	10.1145/51607.51610	string literal;string;computer science;theoretical computer science;programming language;algorithm	PL	-21.7209855910247	29.69415136834292	38329
4de4f9932f986ef07e959ce61e1e5a9fac5f3303	scalable approach for mining association rules from structured xml data	xml documents;itemsets;flex algorithm;association rules xml java;web;association rules;data mining;data representation;xml data mining internet;internet;association rule;xml;xml document;flex algorithm association rules mining structured xml data data representation web xml documents;data mining association rules xml itemsets java transaction databases database languages computer science information technology data preprocessing;association rules xml java data mining;flexible printed circuits;association rules mining;java;structured xml data	XML has become the standard for data representation on the web. This expansion in reputation has prompted the need for a technique to access XML documents. Many techniques have been proposed to tackle the problem of mining XML data we study the various techniques to mine XML data and yet We presented a java based implementation of FLEX algorithm for mining XML data	algorithm;association rule learning;data (computing);document object model;java;parsing;simple api for xml;xml	Ashraf Abazeed;Ali Mamat;Md Nasir Sulaiman;Hamidah Ibrahim	2009	2009 2nd Conference on Data Mining and Optimization	10.1109/DMO.2009.5341918	xml catalog;xml validation;binary xml;xml encryption;xml base;simple api for xml;xml;xml schema;streaming xml;computer science;document structure description;xml framework;soap;data mining;xml database;xml schema;database;xml signature;world wide web;xml schema editor;cxml;efficient xml interchange	DB	-33.54537738892536	6.787681893561361	38341
320f43ff63aeb54b92adbe7a2166b856cc2c4735	soft subdivision search in motion planning, ii: axiomatics		We propose to design motion planning algorithms with a strong form of resolution completeness, called resolution-exactness. Such planners can be implemented using soft predicates within the subdivision paradigm. The advantage of softness is that we avoid the Zero problem and other issues of exact computation. Soft Subdivision Search (SSS) is an algorithmic framework for such planners. There are many parallels between our framework and the well-known Probabilistic Road Map (PRM) framework. Both frameworks lead to algorithms that are practical, flexible, extensible, with adaptive and local complexity. Our several recent papers have demonstrated these favorable properties on various non-trivial motion planning problems. In this paper, we provide a general axiomatic theory underlying these results. We also address the issue of subdivision in non-Euclidean configuration spaces, and how exact algorithms can be recovered using soft methods.	analysis of algorithms;axiomatic system;computation;heuristic;homology (biology);iros;kinematic chain;kinodynamic planning;motion planning;parallels desktop for mac;programming paradigm;randomized algorithm;statistical relational learning;subdivision surface;whole earth 'lectronic link	Chee-Keng Yap	2015		10.1007/978-3-319-19647-3_2	mathematical optimization;combinatorics;mathematics;algorithm	Robotics	-17.52185784849989	14.538152484077811	38350
18f42872c29dfddaa20c46755fb8159008b75b7a	on proving confluence modulo equivalence for constraint handling rules		Previous results on proving confluence for Constraint Handling Rules are extended in two ways in order to allow a larger and more realistic class of CHR programs to be considered confluent. Firstly, we introduce the relaxed notion of confluence modulo equivalence into the context of CHR: while confluence for a terminating program means that all alternative derivations for a query lead to the exact same final state, confluence modulo equivalence only requires the final states to be equivalent with respect to an equivalence relation tailored for the given program. Secondly, we allow non-logical built-in predicates such as var/1 and incomplete ones such as is/2, that are ignored in previous work on confluence. To this end, a new operational semantics for CHR is developed which includes such predicates. In addition, this semantics differs from earlier approaches by its simplicity without loss of generality, and it may also be recommended for future studies of CHR. For the purely logical subset of CHR, proofs can be expressed in first-order logic, that we show is not sufficient in the present case. We have introduced a formal meta-language that allows reasoning about abstract states and derivations with meta-level restrictions that reflect the non-logical and incomplete predicates. This language represents subproofs as diagrams, which facilitates a systematic enumeration of proof cases, pointing forward to a mechanical support for such proofs. The Project is supported by The DanishCouncil for IndependentResearch, Natural Sciences, Grant No. DFF4181-00442. The second author’s contribution received funding from the European Union Seventh Framework Programme (FP7/2007-2013) under Grant Agreement No. 318337, ENTRA—Whole-Systems Energy Transparency.	abstract interpretation;admissible rule;canonical account;confluence (abstract rewriting);constraint handling rules;constraint logic programming;correctness (computer science);critical pair (logic);diagram;first-order logic;first-order predicate;formal language;futures studies;linux/rk;modulo operation;newman's lemma;operational semantics;overlap–add method;parallel computing;recursive definition;reification (computer science);relevance;rewriting;semiconductor industry;turing completeness;unification (computer science)	Henning Christiansen;Maja H. Kirkeby	2016	Formal Aspects of Computing	10.1007/s00165-016-0396-9	discrete mathematics;mathematics;algorithm	PL	-14.494366637698473	17.826871646956487	38389
43502238b4a35561e599f302a31f607352ff4e08	epistemic theories and the interpretation of gödel's incompleteness theorems	prouvabilite;provability;logique et philosophie du langage;godel s theorem;theoreme de godel;logique epistemique;epistemic logic;philosophical logics and philosophyof language;these de church	"""L'A. presente un certain nombre de controverses sur les theoremes de Godel a l'interieur d'une theorie axiomatique de l'arithmetique contenant une notion epistemique representee par l'operateur """" il est prouvable que| """" l'A. developpe notamment une interpretation epistemique de la these de Church."""	gödel;theory	William N. Reinhardt	1986	J. Philosophical Logic	10.1007/BF00243392	epistemic modal logic;philosophy;epistemology;mathematics;algorithm	Logic	-12.704022110885077	10.286014373823317	38407
e9d6b16a2e333aaffb06d81d893dbf470ec9076f	optimized encodings of fragments of type theory in first-order logic	first order logic;optimized encodings;type theory	first order logic;optimized encodings;type theory	first-order logic;first-order reduction;type theory	Tanel Tammet;Jan M. Smith	1998	J. Log. Comput.	10.1093/logcom/8.6.713	discrete mathematics	Logic	-13.526420580531848	14.418172490420979	38414
2fe65e1887c1c52637714ce28d97079416665373	local liveness for compositional modeling of fair reactive systems	closed system;satisfiability;transition systems;reactive system;modular verification;open system	We argue that the standard constraints on liveness conditions in nonblocking trace models|machine closure for closed systems, and receptiveness for open systems|are unnecessarily weak and complex, and that liveness should, instead, be speci ed by augmenting transition systems with acceptance conditions that satisfy a locality constraint. First, locality implies machine closure and receptiveness, and thus permits the composition and modular veri cation of live transition systems. Second, while machine closure and receptiveness are based on in nite games, locality is based on repeated nite games, and thus easier to check. Third, no expressive power is lost by the restriction to local liveness conditions. We illustrate the appeal of local liveness using the model of Fair Reactive Systems, a nonblocking trace model of communicating processes.	closed system;expressive power (computer science);liveness;locality of reference	Rajeev Alur;Thomas A. Henzinger	1995		10.1007/3-540-60045-0_49	real-time computing;reactive system;computer science;theoretical computer science;distributed computing;closed system;open system;satisfiability	Logic	-8.38912310563396	23.243557266318856	38464
bdd7df7fff14ad6877c005ae30082ff986238f84	the category of constraint systems is cartesian-closed	information systems analog computers concurrent computing logic programming electronic mail equations power system modeling computer languages heart machinery;database theory constraint theory;first order;constraint theory;constraint system;information system;database theory;recursive domain equations cartesian closure database theory constraint systems gentzen style sequents propositional scott information systems minimal first order structure variables existential quantification substitution morphisms constsys smyth power domain	The development of constraint programming has brought to the forefront the notion of constraint systems as certain first-order systems of partial information [JL87,Sar89,SRP91]. We give a general definition of constraint systems utilizing Gentzen-style sequents. Constraint systems can be regarded as enriching the propositional Scott information systems with manimal first-order structure: the notion of variables, existential quantification and substitution. We take as morphisms approximable maps that are generic in all but finitely many variables. We show that the resulting structure forms a category (called ConstSys). Furthermore, the structure of Scott information systems lifts smoothly to the first-order setting we show that the category is Cartesian-closed, and other usual functors over Scott information systems (lifting, sums, Smyth power-domain) are also definable and recursive domain equations involving these functors can be solved.	cartesian closed category;constraint programming;existential quantification;first-order predicate;information system;lambda lifting;map;microsoft forefront;recursion;smoothing	Vijay A. Saraswat	1992		10.1109/LICS.1992.185546	constraint logic programming;scott domain;database theory;discrete mathematics;computer science;first-order logic;mathematics;programming language;information system;algorithm;scott information system	Logic	-9.194270469816368	16.14961295977244	38468
4d9659afad4f50591cb650fdd945f6a537c90321	coral - control, relations and logic	query language;storage management	CORAL is a modular declarative query language/programming language that supports general Horn clauses with complex terms, set-grouping, aggregation, negation, and relations with tuples that contain (universally quantified) variables. Support for persistent relations is provided by using the EXODUS storage manager. A unique feature of CORAL is that it provides a wide range of evaluation strategies and allows users to optionally tailor execution of a program through high-level annotations. A CORAL program is organized as a collection of modules, and this structure is used as the basis for expressing control choices. CORAL has an interface to C++, and uses the class structure of C++ to provide extensibility. FinaUy, CORAL supports a command sublanguage, in which statements are evaluated in a user-specified order. The statements can be queries, updates, production-system style rules, or any command that can be typed in at the CORAL	c++;declarative programming;exodus;extensibility;high- and low-level;horn clause;programming language;query language;sublanguage;universal quantification	Raghu Ramakrishnan;Divesh Srivastava;S. Sudarshan	1992			natural language processing;data control language;computer science;data mining;database;rdf query language;query language	DB	-27.543179565452103	16.656627215102475	38534
23affe71d15f21dc18c6a13fdc013ff746079263	free mv-modules		In this paper, by considering the notion of MV -modules, which is the structure that naturally correspond to lu-modules over lu-rings, we present the definitions of finitely generated and free MV -modules. Also, we define the notions of Ak-module and free Ak-module, where A is a PMV -algebra and k ∈ N. In a special case, we obtain a general representation for a free Ak-module. In the follow, by considering the notion of free objects, we obtain a method to construct a free object on a nonempty set in Ak-modules. Finally, we present the definitions of invariant dimension property and Ak-invariant dimension property in PMV -algebras and prove that every PMV -algebra has the Ak-invariant dimension property.	essence;lu decomposition;mv-algebra;turing completeness	Rajab Ali Borzooei;S. Saidi Goraghani	2016	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-162128	mathematics;artificial intelligence;machine learning	Logic	-4.875562065313831	16.313092864143538	38561
66b4d5e4a241a68c75067174a3f53291d6e63403	advanced storage and retrieval of xml multimedia documents	gestion document;multimedia;gestion documento;xml language;data representation;data model;indexing;indexation;indizacion;next generation;information system;systeme information;langage xml;lenguaje xml;sistema informacion;document management	Multimedia documents are more and more expressed in XML as data representation and exchange services. In this paper, we describe the data and execution models of XML Multimedia Document management as a part of the PHASME information engine. The core data model exploits the Extended Binary Graph structure (so called EBG structure); we present the storage and indexing services. The goals of the on-going project are to tackle the increase of multilingual multimedia documents within the PHASME prototype as key advances to prepare the next generation of information engines.	core data;data (computing);data model;mathematical optimization;next-generation network;prototype;silk road;xml	Jérôme Godard;Frédéric Andrès;Kinji Ono	2002		10.1007/3-540-36233-9_6	xml catalog;xml validation;xml encryption;xml base;search engine indexing;xml;data model;streaming xml;computer science;document structure description;document management system;xml schema;database;external data representation;xml signature;world wide web;xml schema editor;information retrieval;information system;efficient xml interchange	DB	-33.66162817763908	7.379422386364651	38631
bba0163513dbba1ebc090bdfefe161c1edf271bd	new architecture of fuzzy database management systems	management system;fuzzy database	Fuzzy relational data bases have been extensively studied in a theoretical level. Unfortunately, the repercussions of these works on the practical plan are negligible. Medina et al. have developed a server named fuzzy SQL, supporting flexible queries and based on a theoretic model called GEFRED. This server has been programmed in PL/SQL language under Oracle database management systems. To model the flexible queries and the concept of fuzzy attributes, an extension of the SQL language named fuzzy SQL has been defined. The FSQL language extends the SQL language, to support the flexible queries, with many fuzzy concepts. The FRDB is supposed has already been defined by the user. In this paper, we extend the work of medina et al. to present a new architecture of fuzzy DBMS based on the GEFRED model. This architecture is based on the concept of weak coupling with the DBMS Oracle. It permits, in particular, the description, the manipulation and the interrogation of FRDB in FSQL language.	futures and promises;fuzzy concept;oracle database;pl/sql;prototype;relational database;sql;server (computing);theory	Amel Grissa-Touzi;Mohamed Ali Ben Hassine	2009	Int. Arab J. Inf. Technol.		data definition language;pl/sql;sql;stored procedure;data manipulation language;computer science;query by example;operating system;data mining;management system;database;programming language;world wide web;fuzzy control language;null;algorithm	DB	-30.2945533325181	10.093823001897341	38665
16797688375b0b5bd425ee76317ebb80870aa476	solving quantified verification conditions using satisfiability modulo theories	search space;68t15;first order;quantifier instantiation;satisfiability modulo theories;legacy system;automated theorem proving;first order logic;quantified verification conditions	First-order logic provides a convenient formalism for describing a wide variety of verification conditions. Two main approaches to checking such conditions are pure first-order automated theorem proving (ATP) and automated theorem proving based on satisfiability modulo theories (SMT). Traditional ATP systems are designed to handle quantifiers easily, but often have difficulty reasoning with respect to theories. SMT systems, on the other hand, have built-in support for many useful theories, but have a much more difficult time with quantifiers. One clue on how to get the best of both worlds can be found in the legacy system Simplify which combines built-in theory reasoning with quantifier instantiation heuristics. Inspired by Simplify and motivated by a desire to provide a competitive alternative to ATP systems, this paper describes a methodology for reasoning about quantifiers in SMT systems. We present the methodology in the context of the Abstract DPLL Modulo Theories framework. Besides adapting many of Simplify’s techniques, we also introduce a number of new heuristics. Most important is the notion of instantiation level which provides an effective mechanism for prioritizing and managing the large search space inherent in quantifier instantiation techniques. These techniques have been implemented in the SMT system CVC3. Experimental results show that our methodology enables CVC3 to solve a significant number of quantified benchmarks that were not solvable with previous approaches.	automated theorem proving;dpll algorithm;decision problem;first-order logic;first-order predicate;formal system;heuristic (computer science);legacy system;modulo operation;quantifier (logic);satisfiability modulo theories;theory;universal instantiation	Yeting Ge;Clark W. Barrett;Cesare Tinelli	2009	Annals of Mathematics and Artificial Intelligence	10.1007/s10472-009-9153-6	discrete mathematics;computer science;artificial intelligence;first-order logic;mathematics;programming language;algorithm	AI	-19.196721232972106	14.075453975608607	38668
c875223fe908071360324345d02ebe7b867cc0f1	infinite computations in concurrent constraint programming	concurrent constraint programming	Abstract   We present a fully abstract model for concurrent constraint programming which besides describing the results of terminating computations also describes the results of those non-terminating computations which are fair with respect to the parallel agents. The justification of the recursive definition of the model is given in terms of the least fixpoint of a function which is continuous with respect to reverse set-inclusion.		Frank S. de Boer;Maurizio Gabbrielli	1997	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(05)80167-7	constraint logic programming;concurrent constraint logic programming;constraint programming;constraint satisfaction;computer science;theoretical computer science;programming language;algorithm	Logic	-14.938268040752185	21.485245821570917	38669
268a5cd37312cd2d6fdf2c527a4832d53e12538f	volt: a lazy grounding framework for solving very large maxsat instances		Very large MaxSAT instances, comprising 10 clauses and beyond, commonly arise in a variety of domains. We present VOLT, a framework for solving such instances, using an iterative, lazy grounding approach. In each iteration, VOLT grounds a subset of clauses in the MaxSAT problem, and solves it using an off-the-shelf MaxSAT solver. VOLT provides a common ground to compare and contrast different lazy grounding approaches for solving large MaxSAT instances. We cast four diverse approaches from the literature on information retrieval and program analysis as instances of VOLT. We have implemented VOLT and evaluate its performance under different state-of-the-art MaxSAT solvers.	information retrieval;iteration;lazy evaluation;maximum satisfiability problem;program analysis;solver	Ravi Mangal;Xin Zhang;Aditya V. Nori;Mayur Naik	2015		10.1007/978-3-319-24318-4_22	computer science;volt;maximum satisfiability problem;discrete mathematics;program analysis;ground;theoretical computer science;solver	AI	-20.080786242812877	14.051702493447467	38673
39aa59f3676248887a2b420c4e2c39b0eb543e7d	a hybrid approach for scalable sub-tree anonymization over big data using mapreduce on cloud	privacy preservation;journal article;data anonymization;big data;mapreduce;cloud computing	In big data applications, data privacy is one of the most concerned issues because processing large-scale privacy-sensitive data sets often requires computation resources provisioned by public cloud services. Sub-tree data anonymization is a widely adopted scheme to anonymize data sets for privacy preservation. Top–Down Specialization (TDS) and Bottom–Up Generalization (BUG) are two ways to fulfill sub-tree anonymization. However, existing approaches for sub-tree anonymization fall short of parallelization capability, thereby lacking scalability in handling big data in cloud. Still, either TDS or BUG individually suffers from poor performance for certain valuing of k-anonymity parameter. In this paper, we propose a hybrid approach that combines TDS and BUG together for efficient sub-tree anonymization over big data. Further, we design MapReduce algorithms for the two components (TDS and BUG) to gain high scalability. Experiment evaluation demonstrates that the hybrid approach significantly improves the scalability and efficiency of sub-tree anonymization scheme over existing approaches.	big data;mapreduce;scalability	Xuyun Zhang;Chang Liu;Surya Nepal;Chi Yang;Wan-Chun Dou;Jinjun Chen	2014	J. Comput. Syst. Sci.	10.1016/j.jcss.2014.02.007	big data;cloud computing;computer science;data mining;database;world wide web	DB	-30.688498922536457	20.279843891969545	38689
89c0c3af839bd3fc9a6cd26f4b26a333f360b7bc	deductive database systems: applications and programming	deductive databases		deductive database;relational database management system	Carlo Zaniolo	1990			database theory;programming language	DB	-30.531478625199167	9.623049276582774	38692
9cc84692d90a04833a74fbd11a4feae8473fe368	branching processes of petri nets	modelizacion;programmation;red petri;processus branchement;simultaneidad informatica;concurrent program;programacion;modelisation;branching process;concurrency;programa competidor;complete lattice;petri net;modeling;simultaneite informatique;programming;reseau petri;programme concurrent;partial order	The notion of a branching process is introduced, as a formalization of an initial part of a run of a Petri net, including nondeterministic choices. This generalizes the notion of a process in a natural way. It is shown that the set of branching processes of a Petri net is a complete lattice, with respect to the natural notion of partial order. The largest element of this lattice is the unfolding of the Petri net.	petri net;unfolding (dsp implementation)	Joost Engelfriet	1991	Acta Informatica	10.1007/BF01463946	partially ordered set;programming;branching process;combinatorics;discrete mathematics;systems modeling;stochastic petri net;concurrency;complete lattice;computer science;mathematics;process architecture;petri net;algorithm	Logic	-8.332727746814614	22.208546063107867	38733
3a58f87fb4bd6acaf90cce645d9ea942b764e225	probabilistic processing of interval-valued sensor data	hidden markov model;probabilistic inference;intervals	When dealing with sensors with different time resolutions, it is desirable to model a sensor reading as pertaining to a time interval rather than a unit of time. We introduce two variants on the Hidden Markov Model in which this is possible: a reading extends over an arbitrary number of hidden states. We derive inference algorithms for the models, and analyse their efficiency. For this, we introduce a new method: we start with an inefficient algorithm directly derived from the model, and visually optimize it using a sum-factor diagram.	algorithm;artificial intelligence;diagram;graphical user interface;hidden markov model;linear algebra;markov chain;mathematical optimization;operational semantics;probabilistic semantics;rewriting;sensor	Sander Evers;Maarten M. Fokkinga;Peter M. G. Apers	2008		10.1145/1402050.1402060	interval;computer science;machine learning;hidden semi-markov model;pattern recognition;markov model;hidden markov model	AI	-23.47754374199898	13.985950516452219	38755
e9ddc36f832142e9c55410ef490d87b16d9a4816	modularité et symétrie pour les systèmes répartis; application au langage csp. (modularity and symmetry for distributed system; application to the csp language)			distributed computing;modularity (networks)	Luc Bougé	1987				AI	-17.150226360965455	15.834345413726522	38758
be0a4bf9f5c892ee2659639f8e6d2563ad5e1a6e	the synthetic approach to decision table conversion	dynamic programming;computer program;decision tree;dynamic program;optimal programs;decision trees;decision tables;branch and bound;decision table	Previous approaches to the problem of automatically converting decision tables to computer programs have been based on decomposition. At any stage, one condition is selected for testing, and two smaller problems (decision tables with one less condition) are created. An optimal program (with respect to average execution time or storage space, for example) is located only through implicit enumeration of all possible decision trees using a technique such as branch-and-bound. The new approach described in this paper uses dynamic programming to synthesize an optimal decision tree from which a program can be created. Using this approach, the efficiency of creating an optimal program is increased substantially, permitting generation of optimal programs for decision tables with as many as ten to twelve conditions.	branch and bound;computer program;decision table;decision tree;dynamic programming;heuristic;run time (program lifecycle phase);synthetic intelligence	Helmut Schumacher;Kenneth C. Sevcik	1976	Commun. ACM	10.1145/360238.360245	decision table;optimal decision;influence diagram;decision tree learning;computer science;theoretical computer science;machine learning;decision tree;incremental decision tree;programming language;algorithm;weighted sum model;grafting;decision matrix	PL	-14.782281832573476	31.841234619754253	38790
4eefc20e398155b594cbc80cb5e8931a1b01e6f5	avoiding infinite recursion with stratified aspects	programming language;expressive power;aspect oriented programming;allgemeine werke;000 informatik;language extension;informationswissenschaft	Infinite recursion is a known problem of aspect-oriented programming with AspectJ: if no special precautions are taken, aspects advising aspects can easily and unintentionally advise themselves. We present a compiler for an extension of the AspectJ programming language that avoids self reference by associating aspects with levels, and by automatically restricting the scope of pointcuts used by an aspect to join points of lower levels. We report on a case study using our language extension, and provide numbers of the changes necessary for migrating existing applications to it. Our results suggest that we can make programming with AspectJ simpler and safer, without restricting its expressive power unduly.	aspect-oriented programming;aspectj;call stack;code refactoring;compiler;dom inspector;debugging;design by contract;infinite loop;java;pointcut;programmer;programming idiom;programming language;recursion (computer science);self-reference;tetris	Eric Bodden;Florian Forster;Friedrich Steimann	2006			aspect-oriented programming;programming domain;computer science;theoretical computer science;programming language;algorithm	PL	-25.750923465692683	29.555735852963995	38794
4124fe681f3fb81fa887c37b81a8fb9f0f606112	formalization of vhdl synthesis procedure in higher-order logic	very large scale integration;logic design;higher order logic;formal verification;design automation;formal semantics;hardware description language;process design;theorem proving;solids	VHDL [7] is an IEEE standard hardware description language intended for use in all phases of the creation of electronic systems. LAMBDA [5] is a general-purpose theorem-proving based CAD tool that integrates design and verification. The goal of the research presented in this paper is to provide a linkage between VHDL and LAMBDA, i.e. to synthesize VHDL descriptions using LAMBDA. Our approach is to identify a synthesizable subset of VHDL and define its formal semantics. Based on the semantics defined, a set of semantics equations are derived f o r each VHDL model. We then translate those semantics equations into a set of equational formulas which are acceptable t o LAMBDA as a specification of a design. A library of correctly synthesized components corresponding to most VHDL primitive operators is also established. Those components are used as building blocks during synthesis.	computer-aided design;denotational semantics;general-purpose modeling;hardware description language;linkage (software);semantics (computer science);vhdl	Xinhua Wang;Edward P. Stabler	1991	1991., International Workshop on the HOL Theorem Proving System and Its Applications		process design;logic synthesis;higher-order logic;electronic design automation;formal verification;vhdl;computer science;theoretical computer science;formal semantics;solid;automated theorem proving;very-large-scale integration;hardware description language;programming language;algorithm	Logic	-14.907136623430864	29.857776013810295	38806
c0a39b1b64100b929ec77d33232513ec72089a2e	english as a formal specification language	formal specification;formal specification language;look ahead;theorem prover;formal specification specification languages;first order;specification languages;authoring on the fly;controlled language;theorem prover english computer processable controlled natural language peng controlled grammar controlled lexicon grammatical restrictions ecole look ahead text editor domain specific content words discourse representations structures anaphora presuppositions first order predicate logic steamroller puzzle otter;domain specificity;formal specifications natural languages formal languages writing logic testing humans application software australia vocabulary manuals	PENG is a computer-processable controlled natural language designed for writing unambiguous and precise specifications. PENG covers a strict subset of standard English and is precisely defined by a controlled grammar and a controlled lexicon. In contrast to other controlled languages, the author does not need to know the grammatical restrictions explicitly. ECOLE, a look-ahead text editor, indicates the restrictions while the specification is written. The controlled lexicon contains domain-specific content words that can be defined by the author on the fly and predefined function words. Specifications written in PENG can be deterministically translated into discourse representations structures to cope with anaphora and presuppositions and also into first-order predicate logic. To test the formal properties of PENG, we reformulated Schubert’s steamroller puzzle in PENG, translated the resulting specification via discourse representation structures into first-order predicate logic with equality, and proved the steamroller’s conclusion with OTTER, a standard theorem prover.	anaphora (linguistics);automated theorem proving;controlled grammar;controlled natural language;definition;first-order logic;first-order predicate;formal language;formal specification;high- and low-level;knowledge acquisition;lexicon;need to know;on the fly;specification language;text editor	Rolf Schwitter	2002		10.1109/DEXA.2002.1045903	natural language processing;computer science;artificial intelligence;first-order logic;formal specification;database;automated theorem proving;programming language;algorithm	NLP	-25.664212078918723	18.458727437189406	38836
09f8c0a661636dd1c70309b82aaf363245b58fc9	defeasibility in clp(q) through generalized slack variables	linear equations	This paper presents a defeasible constraint solver for the domain of linear equations, disequations and inequalities over the body of rational/real numbers. As extra requirements resulting from the incorporation of the solver into an Incremental Hierarchical Constraint Solver (IHCS) scenario we identiied: a)the ability to refer to individual constraints by a label, b) the ability to report the (minimal) cause for the unsatissability of a set of constraints, and c) the ability to undo the eeects of a formerly activated constraint. We develop the new functionalities after starting the presentation with a general architecture for defeasible constraint solving, through a solved form algorithm that utilizes a generalized, incremental variant of the Simplex algorithm, where the domain of a variable can be restricted to an arbitrary interval. We demonstrate how generalized slacks form the basis for the computation of explanations regarding the cause of unsatissability and/or entailment in terms of the constraints told, and the possible deactivation of constraints as demanded by the hierarchy handler.	computation;constraint satisfaction problem;defeasible reasoning;emoticon;linear equation;requirement;simplex algorithm;slack variable;solver;undo	Christian Holzbaur;Francisco Menezes;Pedro Barahona	1996		10.1007/3-540-61551-2_76		AI	-18.765579894589884	7.61736669157749	38889
5890e52ff50e1245514d2af78463261e5433371e	bounded functional interpretation and feasible analysis	proof theory;feasibility analysis	In this article we study applications of the bounded functional interpretation to theories of feasible arithmetic and analysis. The main results show that the novel interpretation is sound for considerable generalizations of weak König’s lemma, even in the presence of very weak induction. Moreover, when combined with Cook and Urquhart’s variant of the functional interpretation, one obtains effective versions of conservation results regarding weak König’s lemma which have been so far only obtained non-constructively.	inductive reasoning;könig's lemma;theory	Fernando Ferreira;Paulo Oliva	2007	Ann. Pure Appl. Logic	10.1016/j.apal.2006.07.002	combinatorics;discrete mathematics;proof theory;mathematics;programming language	Logic	-10.449730546023545	14.923516770454595	38944
b14a78f5f5ff834ed0eb184c45471c1cd2b2c2a6	analyzing security protocols with secrecy types and logic programs	lenguaje programacion;theorie type;process calculi;protocole transmission;programming language;cryptographic protocols;cryptographic protocol;logical programming;software engineering;secrecy properties;process calculus;protocolo transmision;tipificacion;logic programming;algebra proceso;typing;programmation logique;criptografia;cryptography;type theory;reseau communication ordinateur;typage;algebre processus;genie logiciel;langage programmation;cryptographie;logic programs;process algebra;programacion logica;ingenieria informatica;security protocol;transmission protocol	We study and further develop two language-based techniques for analyzing security protocols. One is based on a typed process calculus; the other, on untyped logic programs. Both focus on secrecy properties. We contribute to these two techniques, in particular by extending the former with a flexible, generic treatment of many cryptographic operations. We also establish an equivalence between the two techniques.	logic programming	Martín Abadi;Bruno Blanchet	2005	J. ACM	10.1145/1044731.1044735	process calculus;computer science;artificial intelligence;theoretical computer science;cryptographic protocol;mathematics;programming language;algorithm	Theory	-23.395427078196434	30.12327244364008	38968
105f58bcfd70f504614112083d60ace642ff608d	structures for abstract rewriting	rewrite system;axiomatization;abstract completion procedure;abstract deduction procedure;abstract rewriting;rewrite systems;modus ponens	When rewriting is used to generate convergent and complete rewrite systems in order to answer the validity problem for some theories, all the rewriting theories rely on a same set of notions, properties, and methods. Rewriting techniques have been used mainly to answer the validity problem of equational theories, that is, to compute congruences. Recently, however, they have been extended in order to be applied to other algebraic structures such as preorders and orders. In this paper, we investigate an abstract form of rewriting, by following the paradigm of logical-system independency. To achieve this purpose, we provide a few simple conditions (or axioms) under which rewriting (and then the set of classical properties and methods) can be modeled, understood, studied, proven, and generalized. This enables us to extend rewriting techniques to other algebraic structures than congruences and preorders such as congruences closed under monotonicity and modus ponens. We introduce convergent rewrite systems that enable one to describe deduction procedures for their corresponding theory, and we propose a Knuth-Bendix–style completion procedure in this abstract framework.	abstract rewriting system;axiomatic system;church–rosser theorem;confluence (abstract rewriting);critical pair (logic);decision problem;first-order predicate;formal system;ibm system r;knuth–bendix completion algorithm;linear algebra;michael sipser;natural deduction;newman's lemma;programming paradigm;rewrite (programming);theory	Marc Aiguier;Diane Bahrami	2006	Journal of Automated Reasoning	10.1007/s10817-006-9065-7	discrete mathematics;modus ponens;computer science;mathematics;semi-thue system;programming language;knuth–bendix completion algorithm;confluence;algorithm	Logic	-12.302040877672962	15.614112196906985	38971
010faf892790cd8fb0b925b3d83dcec8edc61496	completeness guarantees for incomplete ontology reasoners: theory and practice	cs lo	To achieve scalability of query answering, the developers of Semantic Web applications are often forced to use incomplete OWL 2 reasoners, which fail to derive all answers for at least one query, ontology, and data set. The lack of completeness guarantees, however, may be unacceptable for applications in areas such as health care and defence, where missing answers can adversely affect the application’s functionality. Furthermore, even if an application can tolerate some level of incompleteness, it is often advantageous to estimate how many and what kind of answers are being lost. In this paper, we present a novel logic-based framework that allows one to check whether a reasoner is complete for a given query Q and ontology T —that is, whether the reasoner is guaranteed to compute all answers to Q w.r.t. T and an arbitrary data set A. Since ontologies and typical queries are often fixed at application design time, our approach allows application developers to check whether a reasoner known to be incomplete in general is actually complete for the kinds of input relevant for the application. We also present a technique that, given a query Q, an ontology T , and reasoners R1 and R2 that satisfy certain assumptions, can be used to determine whether, for each data set A, reasoner R1 computes more answers to Q w.r.t. T and A than reasoner R2. This allows application developers to select the reasoner that provides the highest degree of completeness forQ and T that is compatible with the application’s scalability requirements. Our results thus provide a theoretical and practical foundation for the design of future ontology-based information systems that maximise scalability while minimising or even eliminating incompleteness of query answers.	information system;ontology (information science);requirement;scalability;semantic web;semantic reasoner;web ontology language	Bernardo Cuenca Grau;Boris Motik;Giorgos Stoilos;Ian Horrocks	2012	J. Artif. Intell. Res.	10.1613/jair.3470	computer science;semantic reasoner;theoretical computer science;data mining;database	DB	-24.529042467095742	8.05858280193222	39021
de0af37ccc9e4ed7122df1715977f1f3e81f91a5	a formalization of brouwer's argument for bar induction		Brouwer was a founder of intuitionism and he developed intuitionistic mathematics in 1920’s. In particular, he proved the uniform continuity theorem using the fan theorem in 1927, which was derived from a stronger theorem called bar induction. For this principle Brouwer gave a justification which was an important source of BHK-interpretation, but it depends on an assumption which we call “the fundamental assumption” (FA). Since FA was neither explained or justified, many people have thought that Brouwer’s argument is highly controversial. In this paper, we propose a way of formalizing Brouwer’s argument using a method in infinitary proof theory. Also, based on our formalization, we give an explanation and justification of FA from the viewpoint of the practice of intuitionistic mathematics.	bar induction;brouwer fixed-point theorem	Ryota Akiyoshi	2018		10.1007/978-3-662-57669-4_4	bar induction;calculus;proof theory;computer science;discrete mathematics;intuitionism;uniform continuity	NLP	-12.01763097132852	4.871905662230153	39024
2b02d717c70165c925a07a00188d50d7a0142047	scalable reachability analysis via automated dynamic netlist-based hint generation	automatic hints generation;dynamic hints generation;resource allocation;bdd based reachability analysis;computability;resource reduction enhanced reachability analysis automated dynamic netlist based hint generation sat based algorithms largely displaced bdd based verification technique bdd based reachability analysis reachability engines partitioning technique scheduling technique breadth first fixedpoint computation underapproximate computation image iterations preimage iterations peak bdd size reduction resource requirements reachability computation scalability;formal methods;data structures boolean functions heuristic algorithms algorithm design and analysis reachability analysis reactive power scalability;formal verification;binary decision diagrams;tree searching binary decision diagrams computability formal verification reachability analysis resource allocation;scalable formal methods;tree searching;reachability analysis	While SAT-based algorithms have largely displaced BDD-based verification techniques due to their typically higher scalability, there are classes of problems for which BDDbased reachability analysis is the only existing method for an automated solution. Nonetheless, reachability engines require a high degree of tuning to perform well on challenging benchmarks. In addition to clever partitioning and scheduling techniques, the use of hints has been proposed to decompose an otherwise breadth-first fixedpoint computation into a series of underapproximate computations, requiring a larger number of (pre-)image iterations though often significantly reducing peak BDD size and thus resource requirements. In this paper, we introduce a novel approach to boost the scalability of reachability computation: automated netlist-based hint generation. Experiments confirm that this approach can yield significant resource reductions; often over an order of magnitude on complex problems compared to reachability analysis without hints, and even compared to SAT-based proof techniques.	algorithm;benchmark (computing);binary decision diagram;breadth-first search;computation;iteration;netlist;reachability;requirement;scalability;scheduling (computing);variable splitting	Jiazhao Xu;Mark Williams;Hari Mony;Jason Baumgartner	2014	Formal Methods in System Design	10.1007/s10703-014-0213-0	formal methods;formal verification;resource allocation;computer science;theoretical computer science;computability;programming language;algorithm	SE	-13.804402586437691	29.82648003703906	39071
2b33f9fdbaae7ecc861f48c36e539a524119f918	toward an improved downward refinement operator for inductive logic programming		In real-world supervised Machine Learning tasks, the learned theory can be deemed as valid only until there is evidence to the contrary (i.e., new observations that are wrongly classified by the theory). In such a case, incremental approaches allow to revise the existing theory to account for the new evidence, instead of learning a new theory from scratch. In many cases, positive and negative examples are provided in a mixed and unpredictable order, which requires generalization and specialization refinement operators to be available for revising the hypotheses in the existing theory when it is inconsistent with the new examples. The space of Datalog Horn clauses under the OI assumption allows the existence of refinement operators that fulfill desirable properties. However, the versions of these operators currently available in the literature are not able to handle some refinement tasks. The objective of this work is paving the way for an improved version of the specialization operator, aimed at extending its applicability.	datalog;first-order logic;first-order predicate;horn clause;inductive logic programming;machine learning;natural deduction;partial template specialization;predicate (mathematical logic);prototype;refinement (computing);requirement;supervised learning	Stefano Ferilli	2014			operator (computer programming);machine learning;inductive logic programming;computer science;artificial intelligence	AI	-16.963015473683303	7.0001238422978185	39116
64beb06a5e8058b81c4888894e6db47402b8702d	understanding sql stored procedures: a complete guide to sql/psm			sql;sql/psm;stored procedure	Jim Melton	1998				PL	-31.64065608886245	8.584391152637162	39197
d6965083e592b1cd26602b4f958be6e118b5092a	schema mapping for interoperability in xml-based multidatabase	hypermedia markup languages;database system;extensible markup language;data integrity;multidatabase system;integrable model;heterogeneous databases;data representation;data model;xml based integration model interoperability xml based multidatabase systems data integration distributed database systems heterogeneous database systems schema mapping data representation multidatabase common data model;distributed databases;data models distributed databases hypermedia markup languages open systems;schema mapping;open systems;xml data models database systems object oriented modeling relational databases database languages algebra educational institutions computer science prototypes;data models	* This work was supported by National High Performance Computing Foundation of China under grant 99319. Abstract The main aim of a multidatabase system (MDBS) is to achieve data integration and interoperability among distributed and heterogeneous database systems. But data model heterogeneity and schema heterogeneity make this a challenging task. Multidatabase users can only view the global schemas whose real data come from local database systems. Thus, mappings from global schemas to local schemas should be established. At the same time, the recent emergence of XML has shown great attractability as a new standard for data representation and exchange on the web. Data model based on XML becomes more suitable for integrating different types of systems. This paper firstly introduces a multidatabase common data model based on XML, named XML-based Integrated Data Model (XIDM). Then, an approach of schema mappings based on XIDM in MDBSs has been presented. Finally, the illustration and implementation of schema mappings in a multidatabase prototype – Panorama are also discussed.	data (computing);data model;emergence;global variable;heterogeneous database system;interoperability;prototype;schema evolution;xml	Ruixuan Li;Zhengding Lu;Weijun Xiao;Bing Li;Wei Wu	2003		10.1109/DEXA.2003.1232029	xml validation;data modeling;xml;semi-structured model;relax ng;xml schema;data model;streaming xml;computer science;document structure description;data integrity;data mining;xml database;xml schema;database;external data representation;open system;xml schema editor;distributed database;cxml;information retrieval;efficient xml interchange	DB	-33.42086293426204	12.321648268576626	39200
a9c5ce801e56420d6b70801c33477009be62f0ee	first-order eq-logic	eq algebraeq logicmathematical fuzzy logic	This paper represents the third step in the development of EQ-logics. Namely, after developing propositional and higher-order EQ-logics, we focus also on predicate one. First, we give a brief overview of the propositional EQ-logic and then develop syntax and semantics of predicate EQ-logic. Finally, we prove completeness by constructing a model of a consistent theory of EQ-logic from the syntactical material, as usual.	boolean algebra;description logic;first-order logic;first-order predicate;fuzzy set;linear algebra;logic programming;logical connective;mv-algebra;relational operator;residuated lattice	Martin Dyba;Vilém Novák	2013		10.2991/eusflat.2013.35	syntax;predicate (grammar);algorithm;semantics;completeness (statistics);mathematics	Theory	-13.592170859524115	12.749173826097856	39229
55ae0067cbf2674410618292ce9f6e4eba8b8b12	currency and concurrency in the cobol data base facility	cobol		cobol	Robert W. Engles	1976			programming language;database;cobol;computer science;concurrency;currency	DB	-29.654070613065294	26.995369011604677	39245
cd966b9c6946de148addc9aea639ec40a2058e97	tableaux for synchronous systems of knowledge and time with interactions	synchronous system		interaction	Clare Dixon;Michael Fisher	1997			computer science;algorithm	NLP	-13.324058117081066	11.733148939596111	39330
1ad049a793b851d7c1393e3dc06e9fb58664fb2e	deontic logic and possible worlds semantics: a historical sketch	deontic logic;possible worlds	"""This paper describes and compares the first steps in modern semantic theory for deontic logic which appeared in works of Stig Kanger, Jaakko Hintikka, Richard Montague and Saul Kripke in late 50s and early 60s. Moreover, some further developments as well as systematizations are also noted. Although the history of logical studies concerning normative notions goes back to the Middle Ages (see [-14]), deontic logic as a separate branch of formal logic is a relatively young field. Most of contemporary work in deontic logic has been stimulated by G. H. von Wright's classic paper Deontic logic [22] but related ideas were independently developed by J. Kalinowski (see [11]) in Poland and O. Becker (see [2]) in Germany. Von Wright and his first followers, for instance A. N. Prior [20], A. R. Anderson [1] and J. E. Fenstad [-4] as well as Becker and Kalinowski represented the syntactic approach to deontic logic. The situation changed about 1957 when the so called possible world semantics arrived. The semantic theory known as possible world semantics or Kripke-semantics has many fathers. The modern history of possible world semantics begins in 1957 (its prehistory goes to ideas of Leibniz, Wittgenstein and Carnap) when S. Kanger [12], [13] and J. Hintikka [9] appeared; note that [13] and [9] are directly devoted to deontic logic. M. Guillaume published his [-6] in 1958. S. Kripke presented his extremely impressive (so impressive that we have the name """"Kripke-semantics"""") version of possible world semantics in [15], published in 1959. In 1960, R. Montague joined this company with his [19]; this paper, based on Montague's talk delivered in 1955, contains a section on deontic logic. Although my description of the beginnings of possible world semantics is too brief to be complete, it sufficiently shows that a close link between deontic logic and possible world semantics was quickly observed. To sum up: three important writings in which possible world semantics came into being, namely Kanger [13], Hintikka [9] and Montague [19], are entirely or partly connected with deontic logic; also Kripke made in [16] an important remark concerning deontic logic. Since late 50s, possible world semantics has become a standard semantic device in deontic logic. My aim here is to describe the first attempts to establish possible world semantics for deontic logic and, to some extent, subsequent developments in this area. I restrict myself, however, to the so called monadic"""	deontic logic;donald becker;kripke semantics;monad (functional programming);montague grammar;possible world;security technical implementation guide	Jan Wolenski	1990	Studia Logica	10.1007/BF00935603	philosophy;epistemology;artificial intelligence;deontic logic;mathematics;possible world;algorithm	AI	-11.160379273043862	6.741894704193001	39361
95ca08a0bda7be85bba5d10dc709a964b4873944	an independence relation for sets of secrets	nondeducibility;information flow;formal system;formal logic	A relation between two secrets, known in the literature as nondeducibility, was originally introduced by Sutherland. We extend it to a relation between sets of secrets that we call independence. This paper proposes a formal logical system for the independence relation, proves the completeness of the system with respect to a semantics of secrets, and shows that all axioms of the system are logically independent.	agent-based model;communications of the acm;computer security;data-flow analysis;formal system;imperative programming;multi-agent system;predicate (mathematical logic);symposium on operating systems principles;syntactic predicate	Sara Miner More;Pavel Naumov	2010	Studia Logica	10.1007/s11225-010-9223-0	formal system;discrete mathematics;information flow;philosophy;epistemology;mathematics;logic;algorithm	NLP	-14.359980969941937	10.664986572490955	39365
137baa97bc51cb713cda67e02aa1249a40d593b0	process algebra under the light of wolfram's nks	formal specification;random numbers;process algebra;cellular automata;software engineering practices	The strong intellectual investment behind the definition of process algebras and the high abstraction level they can attain in formal specification still contrasts with their degree of penetration into software engineering practice, but also with the relatively limited number of other fields of fundamental science where these models have played some role. An emerging area in which process algebras might lend themselves to attractive investigations is Wolfram’s ’New Kind of Science’ (NKS). In this short note we start discussing possible motivations and preliminary steps for placing process algebra under this new light, and for exploring its versatility by NKS-style experiments.	a new kind of science;abstraction layer;experiment;formal specification;process calculus;software engineering	Tommaso Bolognesi	2006	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2005.12.106	cellular automaton;process calculus;computer science;formal specification;mathematics;programming language;algorithm	SE	-24.87844860565647	20.48303253743741	39379
856ea8dea4a8de5e55208b163c2a13e66146d42b	semantics and correctness of classes of deterministic and nondeterministic recursive programs	nondeterministic recursive program		correctness (computer science);recursion	Jean H. Gallier	1978			natural language processing;programming language;nondeterministic algorithm	Theory	-10.501708745408576	23.278623211647773	39430
913949d6513d74bd32e24c69c82b8da1ed9fb81f	refined rules termination analysis through transactions	database system;computacion informatica;active database;active data bases;rules termination;termination analysis;ciencias basicas y experimentales;static analysis;grupo a;transactions	We propose two methods for rule termination analysis within active databases. The first method relies on evolution graphs simulating rule processing statically and considering both rule activation and deactivation. The evolution graphs provide a more accurate analysis than traditional graph-based approaches. The second method refines the former one considering the updates forming transactions that trigger active rules. We show that several termination analysis methods are captured with our approaches. The algorithms testing rules termination (with and without transactions) are presented considering both event-preserving and event-consuming policies and immediate rule execution. The proposed approaches turn out to be practical and general with respect to the various rule languages and thus they may be applied to several database systems.	termination analysis	Danilo Montesi;Elisa Bertino;Maria Bagnato	2003	Inf. Syst.	10.1016/S0306-4379(02)00025-X	computer science;termination analysis;data mining;database;static analysis;algorithm	DB	-18.084916985466627	23.022336424408497	39447
f53d6102fc953c67d82c45182040ba0e7c998ad3	concurrent constraint programming based on functional programming (extended abstract)	lenguaje programacion;programmation sous contrainte;programming language;concurrent program;logical programming;functional programming;programmation logique;programa competidor;langage programmation;programmation fonctionnelle;programacion logica;programacion funcional;programme concurrent	We will show how the operational features of logic programming can be added as conservative extensions to a functional base language with call by value semantics. We will address both concurrent and constraint logic programming [9, 2, 18]. As base language we will use a dynamically typed language that is obtained from SML by eliminating type declarations and static type checking. Our approach can be extended to cover all features of Oz [6, 15]. The experience with the development of Oz tells us that the outlined approach is the right base for the practical development of concurrent constraint programming languages. It avoids unnecessary duplication of concepts by reusing functional programming as core technology. Of course, it does not unify the partly incompatible theories behind functional and logic programming. They both contribute at a higher level of abstraction to the understanding of different aspects of the class of programming languages proposed here.	concurrent computing;concurrent constraint logic programming;constraint programming;functional programming;programming language;theory;type system;value semantics	Gert Smolka	1998		10.1007/BFb0053559	concurrent constraint logic programming;constraint programming;declarative programming;computer science;artificial intelligence;functional logic programming;programming paradigm;procedural programming;symbolic programming;inductive programming;fifth-generation programming language;programming language;functional programming;logic programming;algorithm	PL	-20.978193759456847	23.006384461646157	39525
e03f82b69f466121c8b682b28b3adf0a56dd30e2	automatic abstraction for verification of timed circuits and systems	verification;automatic proving;red petri;timed system;espace etat;demostracion automatica;asynchronous circuit;demonstration automatique;circuit asynchrone;vhdl;state space;systeme temporise;circuito asincrono;verificacion;espacio estado;petri net;reseau petri	This paper presents a new approach for verification of asynchronous circuits by using automatic abstraction. It attacks the state explosion problem by avoiding the generation of a flat state space for the whole design. Instead, it breaks the design into blocks and conducts verification on each of them. Using this approach, the speed of verification improves dramatically.		Hao Zheng;Eric Mercer;Chris J. Myers	2001		10.1007/3-540-44585-4_16	embedded system;real-time computing;verification;asynchronous circuit;vhdl;physical verification;computer science;state space;high-level verification;runtime verification;petri net;intelligent verification;algorithm;functional verification	Logic	-15.932954317925448	28.01538560358793	39532
f4d6a9d2b4b22b5f320cca5e02344b26a5337998	compilation of higher-order concurrent programs into first-order programs on recursive graph rewriting model	graph rewriting;first order;higher order		first-order predicate;graph rewriting;recursion (computer science)	Masaki Murakami	2007				Logic	-21.779132984173497	21.91335834482042	39547
07cd4ca9d7292d989686ae0cdd8d414f01d0486c	an inductive approach to assertional mining of web ontology revision	description logic;knowledge base;first order logic	This paper proposes an inductive learning method for maintaining a web-based ontology by incorporating newly generated concepts from assertional knowledge (A-Box). The ontology used in this approach is represented by DAML+OIL. This ontology is translated into a form acceptable for the FACT system, a Description Logic (DL) reasoner, and is compiled into a knowledge base as a T-Box, a terminological knowledge description. Inductive learning is used for integrating the A-Box, where positive and negative examples submitted by human users are stored. Inductive Logic Programming (ILP) is used in order to induce concepts consistent with positive examples and to exclude negative ones. Such induced concepts are explored in order to find where they are positioned in the concept hierarchy in the T-Box, and the original ontology is revised. ILP can provide new concepts for DLs even though they may have richer expressiveness since DL is a decidable fragment of first-order logic. The induced concepts could be also utilized for predicting novel assertions from human users.	belief revision;compiler;daml+oil;description logic;first-order logic;first-order predicate;inductive logic programming;inductive reasoning;knowledge base;semantic reasoner;web application	Chieko Nakabasami	2002			dynamic logic (modal logic);inductive programming;natural language processing;description logic;machine learning;statistical relational learning;logic programming;computer science;inductive logic programming;semantic reasoner;autoepistemic logic;artificial intelligence	AI	-23.787003084384516	8.55538418911691	39556
9c857b8951888ece688fc72f3de2b1fb7209fd0d	action refinement for real-time concurrent processes with urgency	equivalence relation;specification language;real time;partially ordered set;system modeling;process algebra	We propose an action refinement approach for real-time concurrent processes with urgent interactions, where a partial-order setting, timed bundle event structures, is used as the system model and a real-time LOTOS-like process algebra is used as the specification language. We show that the refinement approach has the commonly expected properties: (1) The behaviour of the refined process can be inferred compositionally from the behaviour of the original process and from the behaviour of the processes substituted for actions; (2) The timed extensions of pomset trace equivalence and history preserving bisimulation equivalence are both congruences under the refinement; (3) The syntactic and semantic refinements coincide up to the aforementioned equivalence relations with respect to a cpo-based denotational semantics.		Guangping Qin;Jinzhao Wu	2005	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2005.09.002	partially ordered set;computer science;programming language;algorithm	Logic	-10.39785726622642	24.374229210237406	39561
ae26a88fa393558a181a93a5aa8e5eef20421f05	discovering block-structured process models from event logs - a constructive approach	block structured process models;fitness;soundness;process discovery	Process discovery is the problem of, given a log of observed behaviour, finding a process model that ‘best’ describes this behaviour. A large variety of process discovery algorithms has been proposed. However, no existing algorithm guarantees to return a fitting model (i.e., able to reproduce all observed behaviour) that is sound (free of deadlocks and other anomalies) in finite time. We present an extensible framework to discover from any given log a set of block-structured process models that are sound and fit the observed behaviour. In addition we characterise the minimal information required in the log to rediscover a particular process model. We then provide a polynomial-time algorithm for discovering a sound, fitting, block-structured model from any given log; we give sufficient conditions on the log for which our algorithm returns a model that is languageequivalent to the process model underlying the log, including unseen behaviour. The technique is implemented in a prototypical tool.	algorithm;deadlock;process modeling;time complexity	Sander J. J. Leemans;Dirk Fahland;Wil M. P. van der Aalst	2013		10.1007/978-3-642-38697-8_17	soundness;computer science;data science;machine learning;data mining;mathematics;business process discovery	ML	-7.877605533094089	26.74562249494418	39569
192890973de59d35c021c78db736ab29f99ce57f	efficient fixpoint computation in linear tabling	linear tabling;optimization technique;prolog;tabling;recursion;optimization techniques;memoization	Early resolution mechanisms proposed for tabling such as OLDT rely on suspension and resumption of subgoals to compute fixpoints. Recently, a new resolution framework called linear tabling has emerged as an alternative tabling method. The idea of linear tabling is to use iterative computation rather than suspension to compute fixpoints. Although linear tabling is simple, easy to implement, and superior in space efficiency, the current implementations are several times slower than XSB, the state-of-the-art implementation of OLDT, due to re-evaluation of looping subgoals. In this paper, we present a new linear tabling method and propose several optimization techniques for fast computation of fixpoints. The optimization techniques significantly improve the performance by avoiding redundant evaluation of subgoals, re-application of clauses, and reproduction of answers in iterative computation. Our implementation of the method in B-Prolog not only consumes an order of magnitude less stack space than XSB for some programs but also compares favorably well with XSB in speed.	b-prolog;computation;fixed point (mathematics);iterative method;mathematical optimization;memoization;prolog;xsb	Neng-Fa Zhou;Taisuke Sato	2003		10.1145/888251.888277	recursion;memoization;computer science;theoretical computer science;programming language;prolog;algorithm	DB	-16.01013071416169	23.49270168119606	39574
79bd98469dda48cccb33ab7f47c426f8e056a368	automata-theoretic and bounded model checking for linear temporal logic	g5 artikkelivaitoskirja	In this work we study methods for model checking the temporal logic LTL. The focus is on the automata-theoretic approach to model checking and bounded model checking. We begin by examining automata-theoretic methods to model check LTL safety properties. The model checking problem can be reduced to checking whether the language of a finite state automaton on finite words is empty. We describe an efficient algorithm for generating small finite state automata for so called non-pathological safety properties. The presented implementation is the first tool able to decide whether a formula is non-pathological. The experimental results show that treating safety properties can benefit model checking at very little cost. In addition, we find supporting evidence for the view that minimising the automaton representing the property does not always lead to a small product state space. A deterministic property automaton can result in a smaller product state space even though it might have a larger number states. Next we investigate modular analysis. Modular analysis is a state space reduction method for modular Petri nets. The method can be used to construct a reduced state space called the synchronisation graph. We devise an on-the-fly automata-theoretic method for model checking the behaviour of a modular Petri net from the synchronisation graph. The solution is based on reducing the model checking problem to an instance of verification with testers. We analyse the tester verification problem and present an efficient on-the-fly algorithm, the first complete solution to tester verification problem, based on generalised nested depth-first search. We have also studied propositional encodings for bounded model checking LTL. A new simple linear sized encoding is developed and experimentally evaluated. The implementation in the NuSMV2 model checker is competitive with previously presented encodings. We show how to generalise the LTL encoding to a more succint logic: LTL with past operators. The generalised encoding compares favourably with previous encodings for LTL with past operators. Links between bounded model checking and the automatatheoretic approach are also explored.	algorithm;automaton;depth-first search;experiment;finite-state machine;linear temporal logic;logic gate;model checking;nusmv;petri net;product state;propositional calculus;state space	Timo Latvala	2005			model checking;linear temporal logic;interval temporal logic;computation tree logic;abstraction model checking	Logic	-13.124290097919312	25.893543000439404	39589
2c9d5094c728508d6cb5aedc2ff67dc8a21d175a	generation of abstract programming interfaces from syntax definitions	code generation;data type	This paper describes how an Abstract Programming Interface (API) and its implementation can be generated from the syntax definition of a data type. In particular we describe how a grammar (in SDF) can be used to generate a library of access functions that manipulate the parse trees of terms over this syntax. Application of this technique in the ASF+SDF Meta-Environment has resulted in the elimination of 47% of the handwritten code, thus greatly improving both maintainability of the tools and their flexibility with respect to changes in the parse tree format. Although the focus is on ATerms, the issues discussed and the techniques described are more generic and are relevant in related areas such as XML data-binding. © 2003 Elsevier Inc. All rights reserved.	asf+sdf meta-environment;application programming interface;parse tree;parsing;xml data binding	Hayco de Jong;Pieter A. Olivier	2002	J. Log. Algebr. Program.	10.1016/j.jlap.2003.12.002	natural language processing;abstract syntax;data type;computer science;programming language;homoiconicity;algorithm;code generation;abstract syntax tree	PL	-28.237632957151916	26.097658987053876	39602
cbb12c016a49e0e07bd53c7013b1f1928733550a	sequentiality in bounded biorders	full abstraction;higher order functions	We study a notion ofboundedstable biorder, showing that the monotone and stable functions on such biorders are sequential . We construct bounded biorder models of a range of sequential, higher-order functional calculi, including unary PCF, (typed and untyped) call-by-value and lazyλ-calculi, and non-deterministic SPCF. We prove universality and full abstraction results for these models by reduction to the case of unary PCF, for which we give a simple new argument to show that any order-extensional and sequential model is universal.	denotational semantics;programming computable functions;unary operation;universality probability;monotone	James Laird	2005	Fundam. Inform.		discrete mathematics;computer science;mathematics;programming language;higher-order function;algorithm	Logic	-11.163841984617411	18.7758296236909	39626
a4f2b620d1ca5e6806e84cda0a9ca5e01ea32bc2	what is expected of an object-oriented data model?	object oriented data model;object oriented database	First consider a set of deductive databases (DDBs), each of which consists of de nite clauses. Consider each DDB as a set object with a type f dc g, where dc speci es a language. We can add an object identi er to each DDB and introduce an extent hierarchy [Ma] between object identi ers. By using such a hierarchy, we can express various kinds of DDBs under the hierarchy, construct new DDBs dynamically by changing the hierarchy or modifying a DDB, and pose a query to a	data model;deductive database;dortmund data bank	Kazumasa Yokota	1989			method;intelligent database;object model;semi-structured model;data model;data access object;database model;database design;object definition language	DB	-29.56886761106294	10.597486998124948	39694
31dbc9d2e09bf59760c131f3794c20a28d665c89	probability implication in the logics of classical and quantum mechanics	quantum logic;foundations of quantum mechanics;quantum mechanics;conditional probability	One of the debated problems of the ‘logical’ approach to foundations of quantum mechanics concerns the existence of an implication in quantum logic. In this paper I try to introduce some new kind of quantum implication, defined via the quantum conditional probability. It is shown, however, that this probability implication cannot be considered as a propositional connective acting in the standard logic of classical or quantum mechanics. Nevertheless it deserves some attention, also because of its close analogy to the counterfactual implication of Stalnaker.	quantum mechanics	Slawomir Bugajski	1978	J. Philosophical Logic	10.1007/BF00245923	quantum operation;quantum logic;supersymmetric quantum mechanics;quantum probability;conditional probability;quantum discord;open quantum system;quantization;consistent histories;mathematics;mathematical physics;quantum dissipation;interpretations of quantum mechanics;categorical quantum mechanics;quantum process;quantum dynamics;quantum statistical mechanics;quantum mechanics	Theory	-11.91822850802941	4.522771443348235	39710
3d7c85b7455396a2f06864673c39670a937e5913	nemo+: object-oriented constraint programming environment based on subdefinite models	tipo dato;logical programming;data type;constraint satisfaction;specification language;satisfaction contrainte;programmation logique;object oriented;declarative languages;constraint programming;oriente objet;lenguaje especificacion;satisfaccion restriccion;type donnee;programacion logica;orientado objeto;langage specification	In this paper we examine a constraint programming environment NeMo+, which embraces a C++ - based technique. It includes a high-level object-oriented declarative language for specification of data types and constraints. To solve systems of constraints, NeMo+ uses the method of so-called subdefinite models, which we briefly review in the first part of the paper. The architecture of NeMo+ and its main capabilities are presented in the paper.	constraint programming	Igor Shvetsov;Vitaly Telerman;Dmitry Ushakov	1997		10.1007/BFb0017465	constraint programming;constraint satisfaction;specification language;data type;computer science;artificial intelligence;programming language;object-oriented programming;algorithm	PL	-23.463543558329537	19.678335434014784	39750
60cd7fc97010b68417b6595738d3bb3335f1c761	first-order syntactic characterizations of minimal entailment, domain-minimal entailment, and herbrand entailment	first order;closed world assumption;model theory	This paper investigates a consistent versioncwa s of Reiter's closed-world assumptioncwa. It provides a syntactic characterization of minimal entailment ⊢ min : for every ∨-sentence ϕ and for every ∨-theory Σ, $$\varphi \varepsilon cwa_S \left( \sum \right)iff\sum \vdash _{min} \varphi .$$ A version of this characterization remains valid if not all relations are subject to minimization. These two characterizations do not use the domain-closure axiom nor the unique-names assumptionuna, although they may be easily modified to ones that incorporateuna. A similar result for Herbrand entailment ⊢ Her , by means of generalized domain-closure axiomdca s , is provided: for every ∨-sentence ϕ and every ∨-theory Σ, $$\varphi \varepsilon dca_S \left( \sum \right)iff\sum \vdash _{Her} \varphi .$$ Finally, a syntactic characterization of domain-minimal entailment ⊢ dom in terms of a versionmda s of minimal-domain assumption is shown: for every ∨-sentence ϕ and for every ∨-theory Σ, $$\varphi \varepsilon mda_S \left( \sum \right)iff\sum \vdash _{dom} \varphi .$$ The proving power of these entailments is then evaluated. In particular, it is shown that (1) neither ⊢ min nor its versions are strong enough to derive positive sentences from Σ unless they are first-order provable from Σ however, a double application of ⊢ min has enough power to derive such positive sentences; (2) ⊢ Her has the strength of infinitary rule of inference but cannot derive existential nor quantifier-free sentences from Σ unless they are first-order provable from Σ (3) ⊢ Her and ⊢ dom can derive from Σ certain positive facts about = which are otherwise unprovable from Σ and (4) ⊢ dom cannot derive from Σ sentences without positive occurrences of = unless they are first-order provable from Σ. Moreover, the paper relatescwa s to Reiter'scwa and to Minker's generalized closed-world assumptionGCWA and its extension.	first-order predicate;herbrand award;maxima and minima;provable security;quantifier (logic)	Marek A. Suchenek	1993	Journal of Automated Reasoning	10.1007/BF00881837	closed-world assumption;textual entailment;computer science;artificial intelligence;first-order logic;preferential entailment;model theory	NLP	-9.798360491450742	14.10998080502793	39776
18bef3fc23f4d02ac31c25d1c3270c0e1b2c8965	a coinductive framework for infinitary rewriting and equational reasoning	004;infinitary rewriting coinduction;article in monograph or in proceedings	We present a coinductive framework for defining infinitary analogues of equational reasoning and rewriting in a uniform way. We define the relation =, a notion of infinitary equational reasoning, and →∞, the standard notion of infinitary rewriting as follows: = := νR. (=R ∪ R)∗ →∞ := μR. νS. (→R ∪ R)∗ ◦ S where μ and ν are the least and greatest fixed-point operators, respectively, and where R := { 〈f(s1, . . . , sn), f(t1, . . . , tn)〉 | f ∈ Σ, s1 R t1, . . . , sn R tn } ∪ Id . The setup captures rewrite sequences of arbitrary ordinal length, but it has neither the need for ordinals nor for metric convergence. This makes the framework especially suitable for formalizations in theorem provers. 1998 ACM Subject Classification D.1.1 Applicative (Functional) Programming, D.3.1 Formal Definitions and Theory, F.4.1 Mathematical Logic, F.4.2 Grammars and Other Rewriting Systems, I.1.1 Expressions and Their Representation, I.1.3 Languages and Systems	applicative programming language;automated theorem proving;coinduction;functional programming;least fixed point;ordinal data;rewrite (programming);rewriting	Jörg Endrullis;Helle Hvid Hansen;Dimitri Hendriks;Andrew Polonsky;Alexandra Silva	2015		10.4230/LIPIcs.RTA.2015.143	discrete mathematics;mathematics;algorithm	Logic	-12.569930090811448	16.821137864290005	39790
586293b2ccd7e2c9b4ec90723eeda9ef756b29dd	in-place update in a dataflow synchronous language: a retiming-enabled language experiment	in place update;synchronous language;functional semantics;array;lustre	Dataflow synchronous languages such as Lustre have a purely functional semantics. This incurs a high overhead when dealing with arrays, as they have to be copied at each update. We propose to tackle this problem at the source, by constraining programs so that every functional array definition can be optimized into an in-place update. Our solution handles aliasing between function arguments. It also allows more programs with in-place updates to be accepted thanks to a new retiming framework, effectively rescheduling computations across time steps. Our proposed language and compilation method enforces zero-copy purely functional arrays while preserving expressiveness and programmer control through explicit copies.	aliasing;compiler;computation;dataflow programming;in-place algorithm;lustre;overhead (computing);programmer;retiming;zero-copy	Ulysse Beaugnon;Albert Cohen;Marc Pouzet	2016		10.1145/2906363.2906379	lustre;parallel computing;real-time computing;computer science;operating system;programming language	PL	-19.264930405716015	30.92529221860628	39800
0ba1eab6976176f7c9b53dfdcc882dde9dc73f67	a dsl for eer data model specification		In this paper we present a domain specific language (DSL) for Extended Entity-Relationship (EER) data model approach, named EERDSL. EERDSL is a part of our Multi-Paradigm Information System Modeling Tool (MIST) that provides EER database schema specification at the conceptual level and its transformation into a relational data model, or a class model. EERDSL modeling concepts are specified by Ecore, one of the commonly used approaches to create meta-models. In the paper we present both textual and graphical notations of EERDSL. Since only few modeling constraints may be described at the level of abstract syntax, we use Object Constraint Language (OCL) to specify complex validation rules for EER models.	abstract syntax;data model;database schema;digital subscriber line;domain-specific language;enhanced entity–relationship model;information system;object constraint language;relational model	Milan Celikovic;Vladimir Dimitrieski;Slavica Aleksic;Sonja Ristic;Ivan Lukovic	2014			database;relational model;data model;object constraint language;database schema;notation;validation rule;abstract syntax;domain-specific language;computer science	SE	-32.37317681770241	13.09942956728975	39815
1551fe5ea7e891ca2af436d089d5fa46aba224d5	decomposing opacity		Transactional memory (TM) algorithms are subtle and the TM correctness conditions are intricate. Decomposition of the correctness condition can bring modularity to TM algorithm design and verification. We present a decomposition of opacity called markability as a conjunction of separate intuitive invariants. We prove the equivalence of opacity and markability. The proofs of markability of TM algorithms can be aided by and mirror the algorithm design intuitions. As an example, we prove the markability and hence opacity of the TL2 algorithm. In addition, based on one of the invariants, we present lower bound results for the time complexity of TM algorithms.	algorithm design;correctness (computer science);time complexity;transactional memory;turing completeness	Mohsen Lesani;Jens Palsberg	2014		10.1007/978-3-662-45174-8_27		PL	-11.196706315349623	21.210807829772484	39823
22f8cced7b0eb41315ef1457df497ed2d6ae7e5e	synthesis of an efficient tactical theorem prover for the game of go	theorem prover;game of go	Permission to make digital/hard copy of part or all of this work for personal or classroom use is granted without fee provided that the copies are not made or distributed for profit or commercial advantage, the copyright notice, the title of the publication, and its date appear, and notice is given that copying is by permission of the ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.	automated theorem proving	Tristan Cazenave	1998	ACM Comput. Surv.	10.1145/289121.289139	combinatorial game theory;computer science;automated theorem proving;programming language;algorithm	DB	-21.023201248328466	19.58735946778204	39826
1371b98322b3713a78aa058bb6d75a257bc2271f	proof theoretic reasoning in system p	proof theoretic reasoning;system p.	This paper investigates how the rules of System P might be used in order to construct proofs for default rules which take into account the bounds on the probabilities of the consequents of the defaults. The paper defines a proof system, shows that it is sound, and then discusses at length the completeness of the system, and the kind of proofs that it can generate.	proof calculus;theory	Simon Parsons;Rachel A. Bourne	1999			algorithm	Logic	-11.116625400582997	13.774258450910642	39833
20e7ff9db252c954e1f0fa14b52bd2316097a322	a test selection language for co-opn specifications	e banking system;formal specification;concurrent computing;performance evaluation;software prototyping;temporal logic;prototypes;human development;formal specifications;formal specification language;system under test;test selection language;object oriented programming;bank data processing;automatic generation;banking system;input output;java prototype;program testing;concurrent systems;software prototyping specification languages formal specification program testing object oriented programming petri nets temporal logic concurrency control bank data processing software reusability;system testing object oriented modeling logic testing prototypes petri nets formal specifications performance evaluation java humans concurrent computing;object oriented;specification languages;concurrent system;software reusability;concurrency control;logic testing;java prototype test selection language co opn specification concurrent object oriented petri net formal specification language concurrent system temporal logic software reusability e banking system;system testing;humans;co opn specification;concurrent object oriented petri net;petri nets;petri net;object oriented modeling;java	In this paper we propose a test language that allows expressing test intentions for CO-OPN (concurrent object-oriented Petri nets) specifications - a formal specification language designed to handle large complex concurrent systems. Our test language is based on temporal logic formulas for expressing graphs of input/output pairs - the inputs correspond to operations performed on the system and the outputs to the observable results of those operations. We encapsulate the temporal logic using a language of constraints, which purpose is to shape the tests that are to be produced. In this paper we discuss the syntax and provide the semantics of this test language. One of our main worries while designing the test language were to keep it modular in order to promote reusability. Another worry was to be able to cope with non-determinism coming from the system under test. We illustrate managing non-determinism as well as other features of our language by showing how we can generate tests for the login part of an e-banking system. A framework for editing CO-OPN specifications exists and one of its features is the possibility of automatically generating high level Java prototypes that can be completed/extended by human developers. We discuss the applicability and the usefulness of our test language while verifying systems built using this methodology.	co-opn;commitment ordering;concurrency (computer science);formal specification;high-level programming language;input/output;java;login;nondeterministic algorithm;observable;online banking;petri net;specification language;system under test;temporal logic;test engineer;verification and validation	Levi Lucio;Luis Pedro;Didier Buchs	2005	16th IEEE International Workshop on Rapid System Prototyping (RSP'05)	10.1109/RSP.2005.9	real-time computing;concurrent computing;object language;specification language;data control language;computer science;formal specification;database;programming language;object-oriented programming;programming language specification;petri net	SE	-33.33021302504934	29.399039762508984	39924
8bd47b7d6400561d34c90bee90f460a7a1eb4d6f	choiceless, pointless, but not useless: dualities for preframes	domain theory;set theory;natural duality;locally compact	We provide the appropriate common ‘(pre)framework’ for various central results of domain theory and topology, like the Lawson duality of continuous domains, the Hofmann–Lawson duality between continuous frames and locally compact sober spaces, the Hofmann–Mislove theorems about continuous semilattices of compact saturated sets, or the theory of stably continuous frames and their topological manifestations. Suitable objects for the pointfree approach are quasiframes, i.e., up-complete meet-semilattices with top, and preframes, i.e., meetcontinuous quasiframes. We introduce the pointfree notion of locally compact wellfiltered preframes, show that they are just the continuous preframes (using a slightly modified definition of continuity) and establish several natural dualities for the involved categories. Moreover, we obtain various characterizations of preframes having duality. Our results hold in ZF set theory without any choice principles.	domain theory;duality (optimization);fenchel's duality theorem;scott continuity;zermelo–fraenkel set theory	Marcel Erné	2007	Applied Categorical Structures	10.1007/s10485-006-9029-4	locally compact space;stone duality;mathematical analysis;discrete mathematics;topology;domain theory;mathematics;locally compact group;set theory;algebra	AI	-6.634377112975275	10.991737914924997	39953
1be37ab7b64c78351e20952d4261033328ecd69c	abstract semantic differencing for numerical programs		We address the problem of computing semantic differences between a program and a patched version of the program. Our goal is to obtain a precise characterization of the difference between program versions, or establish their equivalence when no difference exists.	autoregressive integrated moving average	Nimrod Partush;Eran Yahav	2013		10.1007/978-3-642-38856-9_14	theoretical computer science;equivalence (measure theory);abstract interpretation;computer science;symbolic execution	SE	-15.879003249907598	21.91758915347203	39986
9b7e43060c7ace1f8beafc9feadabf21c83b1d52	overview of an entity-relationship based database management system	database management system;entity relationship		database;entity–relationship model;management system	Tok Wang Ling;Mong-Li Lee	1992			data administration;database;entity–relationship model;data management;database design;computer science	DB	-32.23790010854323	10.266299357892118	39988
de284a12f3973cd90d66e12b7f3a9fa4c978e6ee	rewriting context-free families of string diagrams		String diagrams provide a convenient graphical framework which may be used for equational reasoning about morphisms of monoidal categories. However, unlike term rewriting, which is the standard way of reasoning about the morphisms of monoidal categories, rewriting string diagrams results in shorter equational proofs, because the string diagrammatic representation allows us to formally establish equalities modulo any rewrite steps which follow from the monoidal structure. Manipulating string diagrams by hand is a time-consuming and errorprone process, especially for large string diagrams. This can be ameliorated by using software proof assistants, such as Quantomatic. However, reasoning about concrete string diagrams may be limiting and in some scenarios it is necessary to reason about entire (infinite) families of string diagrams. When doing so, we face the same problems as for manipulating concrete string diagrams, but in addition, we risk making further mistakes if we are not precise enough about the way we represent (infinite) families of string diagrams. The primary goal of this thesis is to design a mathematical framework for equational reasoning about infinite families of string diagrams which is amenable to computer automation. We will be working with context-free families of string diagrams and we will represent them using context-free graph grammars. We will model equations between infinite families of diagrams using rewrite rules between context-free grammars. Our framework represents equational reasoning about concrete string diagrams and context-free families of string diagrams using double-pushout rewriting on graphs and context-free graph grammars respectively. We will prove that our representation is sound by showing that it respects the concrete semantics of string diagrammatic reasoning and we will show that our framework is appropriate for software implementation by proving important decidability properties.	context-free grammar;context-free language;diagram;empty string;modulo operation;proof assistant;rewrite (programming);rewriting;string (computer science)	Vladimir Nikolaev Zamdzhiev	2016	CoRR		prefix grammar;semi-thue system	Logic	-13.555363090843315	19.57540957351561	39992
99ccb4e25b2dc9f63cad36b41e87594624ce7320	combining top-down and bottom-up approaches for automated discovery of typed programs		Automated program discovery has been mainly approached via Genetic Programming, which represents the search space of programs implicitly by a collection of individuals. In this paper, we propose a way of representing program search space explicitly, with a top-down hierarchy of semi-constructed programs. Together with a bottom-up generation procedure, we maintain an overview of the overall search space structure, while being able to quickly get samples from the fringe. Moreover, having a type system with parametric polymorphism allows us to limit the state space size using type level programming while keeping a complete control over program sizes. Our approach can naturally be used for searching the space of programs using tree search methods. We back this claim by a simple experiment utilizing a Monte-Carlo Tree Search algorithm, though other search methods (such as A∗) might be used as well.	bottom-up parsing;bottom-up proteomics;experiment;genetic programming;hindley–milner type system;monte carlo method;monte carlo tree search;open content;parametric polymorphism;search algorithm;semiconductor industry;state space;symbolic regression;top-down and bottom-up design;tree traversal	Tomás Kren;Josef Moudrík;Roman Neruda	2017	2017 IEEE Symposium Series on Computational Intelligence (SSCI)	10.1109/SSCI.2017.8285209	machine learning;tree traversal;state space;genetic programming;hierarchy;parametric polymorphism;monte carlo tree search;top-down and bottom-up design;computer science;artificial intelligence	SE	-18.013532641366442	29.769086341211786	40062
0316c772cc74e0c5841d51a30db622279f6a58c1	unfolding of products of symmetrical petri nets	construccion modular;file attente;unfolding;deploiement;red petri;maquina estado finito;despliegue;verification modele;queue;general techniques;program verification;symetrie;symmetry;verificacion programa;model checking;construction modulaire;modular construction;simetria;machine etat fini;verification programme;petri net;fila espera;finite state machine;reseau petri	This paper presents a general technique for the modular construction ofcomplete prefixes adapted to systems composed ofP etri nets. This construction is based on a definition ofa well-adapted order allowing combination. Moreover, the proposed technique takes into account the symmetries ofthe system to minimize the size ofthe produced complete prefixes. Finally, the technique has been instantiated in an efficient algorithm for systems combining finite state machines and k-bounded queues with k a priori known or not.	net (polyhedron);petri net	Jean-Michel Couvreur;Sébastien Grivet;Denis Poitrenaud	2001		10.1007/3-540-45740-2_9	model checking;discrete mathematics;computer science;symmetry;finite-state machine;programming language;petri net;queue;algorithm	Logic	-7.66555802611386	24.374516320523036	40113
c96a8bcd908fde6763abaead105cd4171c6f4081	a fully abstract bidomain model of unary fpc	algorithmique;lambda calculus;metalangage;full abstraction;metalanguage;algorithmics;algoritmica;informatique theorique;lambda calculo;interpretation abstraite;abstract interpretation;lambda calcul;computer theory;informatica teorica;metalenguaje	"""We present a fully abstract and effectively presentable model of unary FPC (a version of FPC with lifting rather than lifted sums) in a category of bicpos and continuous and stable functions. We show universality for the corresponding model of unary PCF, and then show that this implies full abstraction for unary FPC. We use a translation into this metalanguage to show that the """"canonical"""" bidomain model of the lazy λ-calculus (with seqential convergence testing) is fully abstract."""	denotational semantics;unary operation	James Laird	2003		10.1007/3-540-44904-3_15	metalanguage;computer science;lambda calculus;programming language;algorithmics;algorithm	Logic	-11.849421632922041	18.142613641034426	40178
f3774c716abf58bd32f925a60467a0c24f27732a	structuralism as a philosophy of mathematical practice	philosophy of mathematics;science philosophy;philosophie des sciences	This paper compares the statement ‘Mathematics is the study of structure’ with the actual practice of mathematics. We present two examples from contemporary mathematical practice where the notion of structure plays different roles. In the first case a structure is defined over a certain set. It is argued firstly that this set may not be regarded as a structure and secondly that what is important to mathematical practice is the relation that exists between the structure and the set. In the second case, from algebraic topology, one point is that an object can be a place in different structures. Which structure one chooses to place the object in depends on what one wishes to do with it. Overall the paper argues that mathematics certainly deals with structures, but that structures may not be all there is to mathematics.	linear algebra	Jessica Carter	2007	Synthese	10.1007/s11229-007-9169-6	mathematics education;philosophy of mathematics;applied mathematics;philosophy;epistemology;mathematics;abstract structure	SE	-11.30760381947994	5.58764722331904	40264
aad42b5ab09e6f774bec06a10a6a71a6f4b038ca	sysobjc: c extension for development of object-oriented operating systems	predicate classes;object oriented language;predicate dispatching;system programming;operating system;low level programming;object oriented;polymorphism;c;data structure	Object-oriented operating systems prefer to consider every data structure as an object. However, data structures predefined by the hardware's manufacturer do not fit into this scheme. First of all, traditional object-oriented languages do not support objects with user-defined representation, which would be needed to these data structures as objects. Secondly, classic object-oriented languages have an overhead unacceptable for operating systems. To overcome both problems we introduce predicate classes into C as an extension of the standard structures. Inheritance is based on overlapping reserved fields without extending the object in size, which enables polymorphism by value, a very important feature in system programming. Dispatcher routines of methods add no significant overhead compared to standard C.	data structure;operating system;overhead (computing);system programming	Ádám Balogh;Zoltán Csörnyei	2006		10.1145/1215995.1216000	method;parallel computing;real-time computing;data structure;computer science;object;operating system;programming language;object-oriented programming;algorithm	PL	-24.707441973329047	28.45369465709319	40290
95c1af3786ea7dffbcb13a87edd561662f1baad3	fluxcapacitor: efficient time-travel text search	increasing number;efficient time-travel text search;fluxcapacitor prototype;prime example;time-travel text search;user-specified time point;keyword query;temporally versioned text collection;temporal dimension;text collection	An increasing number of temporally versioned text collections is available today with Web archives being a prime example. Search on such collections, however, is often not satisfactory and ignores their temporal dimension completely. Time-travel text search solves this problem by evaluating a keyword query on the state of the text collection as of a userspecified time point. This work demonstrates our approach to efficient time-travel text search and its implementation in the FluxCapacitor prototype.	archive;prototype;version control	Klaus Berberich;Srikanta J. Bedathur;Thomas Neumann;Gerhard Weikum	2007			full text search;computer science;concept search;data mining;database;web search query;world wide web;information retrieval	Web+IR	-31.59137752386836	4.224035844356439	40291
7a34d74dfd38695f7c46e4f268cd83a4220bf378	formes: an object and time oriented system for music composition and synthesis	programming language;real time control;object oriented programming;complex system;message passing	It is well known [Winograd79] that the development and use of complex systems was stifled by the inadequacy of ordinary programming languages. Music Composition and Synthesis (MCS) by computer offers an appropriate example of this “complexity barrier”.  Object-Oriented programming matches a lot of MCS requirements: an object-oriented programming environment, called Formes, has been developed at IRCAM, including original features like precise control of Time and Hierarchy of events.  In this paper the structure and implementation of the system Formes are presented. Particular attention is given to a functional approach of message passing and an extension of the class concept to real-time control structure.  Two musical examples and a meta-circular definition of Formes are developed.	circular definition;complex systems;control flow;functional approach;integrated development environment;message passing;music construction set;programming language;real-time clock;requirement	Pierre Cointe;Xavier Rodet	1984		10.1145/800055.802024	complex systems;message passing;real-time control system;programming domain;reactive programming;computer science;artificial intelligence;functional logic programming;programming paradigm;inductive programming;programming language;object-oriented programming;algorithm	PL	-29.42507997416349	31.12758396033893	40299
460fde1d2392018960a0e25f0bb04723da820de3	von neumann categories	quantum teleportation;monoidal category;tensor category;double cone;monoidal structure	Quantum Mechanics (Abramsky, Coecke) Quantum mechanics is reformulated away from the notion of C ∗-algebras and expressed in abstract, categorical terms. The categorical structure in question is that of a compact closed dagger category. So we have a symmetric monoidal category, with dual objects, i.e. an A∗ with arrows I → A⊗ A∗ and A∗ ⊗ A → I inducing closed structure (Kelly). We furthermore assume an involutive contravariant endofunctor † : C → C which is the identity on objects, and interacts correctly with the monoidal structure. The primary examples are Rel, the category of sets and relations, and Hilbfd , the category of finite-dimensional Hilbert spaces. Richard Blute Von Neumann Categories Abstract Quantum Mechanics IIQuantum Mechanics II The authors show for example that compact closed dagger categories provide sufficient structure to model protocols such as quantum teleportation or entanglement swapping. The correctness of the interpretation basically just amounts to the coherence equations of the theory. But, this encoding does not take into account that protocols take place in space-time, and relativistic effects may be significant. A straightforward modification of the definition of AQFT would be to assign to each open in spacetime a compact closed dagger category. Much of the above structure is easily lifted to this level. The problem is with expressing Einstein Causality. Richard Blute Von Neumann Categories	causality;correctness (computer science);epr paradox;hilbert space;kelly criterion;paging;quantum entanglement;quantum mechanics;quantum teleportation;rel	Richard Blute;Marc Comeau	2015	Applied Categorical Structures	10.1007/s10485-014-9375-6	von neumann algebra;affiliated operator;topology;von neumann's theorem;pure mathematics;mathematics;abelian von neumann algebra;tomita–takesaki theory;algebra	Logic	-8.723804649504935	10.999651990262931	40320
ff453085398cf532fdd1f4d6159bb014ade4e72d	on a theory for ac0 and the strength of the induction scheme	ac0;logspace;bounded arithmetic;herbrand s theorem	We de ne a fragment of Primitive Recursive Arithmetic by replacing the de ning axioms for primitive recursive functions by those for functions in some speci c complexity class. In this note we consider such theory for AC 0 . We present a model-theoretical property of this theory, by means of which we are able to characterize its provably total functions. Next we consider the problem of how strong the induction scheme can be in this theory. 0 Introduction Primitive recursive arithmetic (PRA) is a rst order theory which consists of all primitive recursive functions together with their de ning axioms and induction scheme for all bounded formulae. It is known that PRA is preserved under substructures, namely if M is a model of PRA then any substructure of M is also a model of PRA. It follows using Herbrand's theorem and the theorem of Los and Tarski that if '(x; y) is a bounded formula such that PRA ` 8x9y'(x; y) then PRA ` 8x'(x; f(x)) holds for some primitive recursive function f . From the point of view of computational complexity, it is interesting to nd analogous systems for complexity classes of functions such as PTIME,LOGSPACE,AC 0 , etc. In this paper, we shall give such a system AC 0 CA for AC 0 which consists of the symbols for the functions in AC 0 together with their de ning axioms and appropriately chosen induction schemes so that the system is preserved under substructures. The main idea for choosing the induction scheme is to be able to compute a witness for a formula of a certain complexity by an algorithm which can be carried out within the given complexity class. So it is necessary to nd more elaborate technique as the complexity class in concern becomes smaller. Clote and Takeuti [7] de ned Buss-like weak rst order theories corresponding to small Boolean circuit complexity classes. Their systems have a weak induction scheme together with some additional axiom schemes, while our system have builtin functions for these complexity classes. F.Ferreira [8] introduced a similar theory called Th FO + which corresponds to the class AC 0 . Unlike our system AC 0 CA, his theory is de ned in the stringlanguage setting. But we prefer to take Buss-like setting for the following reason. 1991 Mathematics Subject Classi cation. 03C62, 03F30, 68Q15.	ac0;algorithm;boolean circuit;circuit complexity;complexity class;computable function;computational complexity theory;inductive reasoning;l (complexity);mathematical induction;norm (social);p (complexity);physical review a;primitive recursive function;recursion;recursive language	Saturo Kuroda	1998	Math. Log. Q.	10.1002/malq.19980440312	combinatorics;herbrand's theorem;mathematical analysis;discrete mathematics;primitive recursive function;mathematics;μ-recursive function;primitive recursive arithmetic;μ operator;algorithm;algebra	Theory	-7.057880934025937	14.293369878321341	40395
3bcde45e8300cfe199ff0ee503436b84a2b1cc7f	the undecidability of the disjunction property of propositional logics and other related problems		u0027How can we recognize, given axioms and inference rules of a calculus, whether the calculus has such-and-such property?u0027 A question of this kind arises whenever we deal with a new logic system. For large families of logics, this question may be considered as an algorithmic problem, and a property is called decidable in a given family if there exists an algorithm which is capable of deciding, for a finite axiomatics of a calculus in the family, whether or not it has the property. In the class of intermediate propositional logics, for instance, nontrivial properties such as the tabularity, pretabularity, and interpolation property (Maksimova [1972, 1977]) are decidable. However, for many other important propertiesdecidability, finite model property, disjunction property, Hallden-completeness, etc.-effective criteria were not found in spite of considerable efforts. In this paper we show that the difficulties in investigating these properties in the classes of intermediate logics and normal modal logics containing S4 are of principal nature, since all of them turn out to be algorithmically undecidable. In other words, there are no algorithms which, given a finite set of axioms of an intermediate or modal calculus, can recognize whether or not it is decidable, Hallden-complete, has the finite model or disjunction property. The first results concerning the undecidability of properties of calculi seem to have been obtained by Linial and Post [1949], who proved the undecidability of the problem of equivalence to classical calculus in the class of all propositional calculi with the same language as the classical one and the two inference rules: modus ponens and substitution. Kuznetsov [1963] generalized this result having proved the undecidability of the problem of equivalence to any fixed intermediate calculus (for instance, to intuitionistic calculus or even the inconsistent one). However, these results will not hold if we confine ourselves only to the class of intermediate logics, though the problem of equivalence to the undecidable intermediate calculus of Shehtman [1978] is clearly undecidable in this class as well. Thomason [1982] proved the undecidability of Kripke completeness in the class of all normal modal logics. Chagrova [1991] established the undecidability of the	undecidable problem	Alexander V. Chagrov;Michael Zakharyaschev	1993	J. Symb. Log.		propositional formula;monoidal t-norm logic;zeroth-order logic;t-norm fuzzy logics;absorption;modal μ-calculus;discrete mathematics;classical logic;resolution;horn-satisfiability;tautology;intuitionistic logic;intermediate logic;disjunction introduction;łukasiewicz logic;mathematics;propositional variable;well-formed formula;propositional calculus;algorithm;autoepistemic logic;satisfiability	Theory	-10.022706622736168	14.084841848127635	40414
0e30a0fa6c3609c23b9826198c0cf7ea511587a1	toward an infinitary logic of domains: abramsky logic for transition systems	domain theory;logica formal;distributive lattice;infinitary logic;topological space;logic;coordination language;formal method;sober space;modal logic;espace sobre;logique abramsky;logique lindenbaum;transition systems;formal logic;lindenbaum logic;espace topologique;abramsky logic;logique formelle;logique arite infinie;logique;logica;espacio topologico	We give a new characterization of sober spaces in terms of their completely distributive lattice of saturated sets. This characterization is used to extend Abramsky’s results about a domain logic for transition systems. The Lindenbaum algebra generated by the Abramsky finitary logic is a distributive lattice dual to an SFP-domain obtained as a solution of a recursive domain equation. We prove that the Lindenbaum algebra generated by the infinitary logic is a completely distributive lattice dual to the same SFP-domain. As a consequence soundness and completeness of the infinitary logic is obtained for a class of transition systems that is computational interesting. 1991 Mathematics Subject Classification: 03B45, 03B70, 03C75, 03G10, 06D10, 55M05, 68Q55 1991 ACM Computing Classification System: D.3.1, F.3.1, F.4.1	acm computing classification system;assertion (software development);business logic;denotational semantics;domain theory;hennessy–milner logic;interpretation (logic);logic programming;mathematics subject classification;parallel language;plotkin bound;power domains;recursion (computer science);semantics (computer science);shared variables;simulated fluorescence process algorithm;transition system;type system	Marcello M. Bonsangue;Joost N. Kok	1999	Inf. Comput.	10.1006/inco.1999.2827	predicate logic;discrete mathematics;formal methods;intermediate logic;mathematics;logic;algorithm	Theory	-12.100641122262568	13.907158277852213	40427
80cc450e8f3e2c391180cd8bd084ce749ca078a8	formal verification for feature-based composition of workflows		We present FeatureAgda, a framework for specifying and proving properties of feature-based composition of workflows implemented in the Feature-Oriented Software Production Lines paradigm. The resulting workflows allow for adaptation at runtime by changing the set of enabled features. Our framework is based on Agda, which is both a theorem prover and a programming language. It relies on dependent types to support the modular definition of features. While promoting the separation of concerns, we obtain a single artefact written entirely in Agda, allowing family-level formal verification. As a practical application of our approach, we demonstrate a case study from the healthcare domain implementing a complex medication prescription workflow. Our setting allows the workflow to be changed to accommodate the needs of a particular doctor or clinic while having trustworthiness through formal verification.	apl;agda;automated theorem proving;computable analysis;computable function;concurrency (computer science);coq (software);correctness (computer science);dependent type;experimental system;formal verification;idris;interaction;lisp machine;neural binding;programming language;programming paradigm;prototype;run time (program lifecycle phase);separation of concerns;software product line;trust (emotion);type system;eric	Stephan Adelsberger;Bashar Igried;Markus Moser;Vadim Savenkov;Anton Setzer	2018	2018 14th European Dependable Computing Conference (EDCC)	10.1109/EDCC.2018.00039	separation of concerns;programming language;agda;automated theorem proving;formal verification;medication prescription;modular design;software;workflow;computer science;distributed computing	PL	-31.13351696094888	29.7939592438751	40435
b586b6f870ee276a6159fa71dd7fe25a6a785b80	fault recovery of manufacturing systems based on controller reconfiguration	automatic control;manufacturing systems;control systems;abelian group;degradation;petri net regulation controller scheme;incidence matrix;discrete event system;redundancy;condition monitoring;discrete event system fault recovery discrete manufacturing system controller reconfiguration petri net regulation controller scheme;error correction;fault detection;fault tolerance;controller reconfiguration;discrete event systems;discrete manufacturing system;control engineering computing;petri nets;petri nets control engineering computing discrete event systems fault tolerance manufacturing systems;petri net;manufacturing system;manufacturing systems control systems automatic control redundancy petri nets fault detection error correction degradation condition monitoring fault diagnosis;fault recovery;fault diagnosis	This paper addresses fault recovery of discrete manufacturing systems through the reconfiguration of the controller. It is assumed that the system is controlled using a Petri net regulation controller scheme. Based on this scheme, this paper presents a novel control reconfiguration technique for fault recovery. The main contribution of this paper is an incidence matrix based technique and an abelian group of Petri net modules to recompute the controller each time that a resource fails	control reconfiguration;discrete manufacturing;fault tolerance;incidence matrix;interplanet;petri net	Mildreth Alcaraz-Mejia;Ernesto López-Mellado;Antonio Ramírez-Treviño	2006	2006 IEEE/SMC International Conference on System of Systems Engineering	10.1109/SYSOSE.2006.1652303	control engineering;real-time computing;engineering;control system;control reconfiguration;automatic control;control theory;petri net	Robotics	-5.560500584232195	28.82690150174555	40465
9870b416ead3bf16c7a32b9df55d0fe2af5e89fd	the absolute consistency problem of xml schema mappings with data values between restricted dtds		This paper proposes a restricted class of DTDs, called MDC-DTDs, such that the absolute consistency problem of XML schema mappings is solvable in polynomial time. An XML schema mapping is a triple consisting of a source DTD, a target DTD, and a set of dependencies, and represents a set of pairs of source and target documents that satisfy the dependencies. An XML schema mapping is said to be absolutely consistent if every source document has a corresponding target document. To the best of the authors’ knowledge, tractability results have been obtained only for XML schema mappings between nested-relational DTDs, which are a proper subclass of nonrecursive, disjunction-free DTDs. MDC-DTDs are a proper superclass of nested-relational DTDs, where recursion and disjunction are allowed but every label can appear at most once or unboundedly many times as children of a node. We show that the absolute consistency problem of XML schema mappings between MDC-DTDs is solvable in polynomial time if the dependencies are specified by (1) self, child, parent, following-sibling, preceding-sibling, and data value equality operators; or (2) self, child, following-sibling, preceding-sibling, qualifier, and data value equality operators.	cobham's thesis;decision problem;p (complexity);polynomial;recursion;time complexity;xml schema;xpath	Yasunori Ishihara;Hayato Kuwada;Toru Fujiwara	2014		10.1007/978-3-319-10073-9_26	xml validation;relax ng;data mining;xml schema;database	DB	-23.53160217260992	10.720120489745684	40473
62fa6cf01a15e996766d244cc9307ed779872f77	understanding and extending incremental determinization for 2qbf		Incremental determinization is a recently proposed algorithm for solving quantified Boolean formulas with one quantifier alternation. In this paper, we formalize incremental determinization as a set of inference rules to help understand the design space of similar algorithms. We then present additional inference rules that extend incremental determinization in two ways. The first extension integrates the popular CEGAR principle and the second extension allows us to analyze different cases in isolation. The experimental evaluation demonstrates that the extensions significantly improve the performance.	algorithm;powerset construction;quantifier (logic)	Markus N. Rabe;Leander Tentrup;Cameron Rasmussen;Sanjit A. Seshia	2018		10.1007/978-3-319-96142-2_17	rule of inference;computer science;theoretical computer science;alternation (linguistics)	Logic	-14.722407947702253	22.61215341495385	40488
41660532a5e0a4c6c6a29cd752ed3f745bb220ac	integrating rules and ontologies in the first-order stable model semantics (preliminary report)	first order;stable model semantics	We present an approach to integrating rules and ontologies on the basis of the first-order stable model semantics defined by Ferraris, Lee and Lifschitz. We show that a few existing integration proposals can be uniformly related to the first-order stable model semantics.	first-order logic;first-order predicate;ontology (information science);pointer (computer programming);semiconductor industry;stable model semantics;turing completeness;vladimir lifschitz	Joohyung Lee;Ravi Palla	2011		10.1007/978-3-642-20895-9_27	stable model semantics;computer science;artificial intelligence;formal semantics;first-order logic;data mining;database;programming language;well-founded semantics;operational semantics;denotational semantics;algorithm	AI	-16.15349284095942	13.629613371571029	40538
1d1dd0662d8bfbc782d03de3994770489c1a20a0	pointer programs and undirected reachability	locally ordered graphs;undirected s t reachability pointer programs logspace algorithms structured data abstract pointers logarithmic size memory registers locally ordered graphs;registers automata computer science computational modeling computer languages logic programming turing machines size control testing;turing machines;pointer program;generators;abstract pointers;undirected reachability;programming language;logarithmic size memory registers;cayley graph;data mining;pointer programs;artificial neural networks;computational modeling;undirected s t reachability;registers;logspace algorithms;cayley graph pointer program undirected reachability finite model theory;data structures;transitive closure;finite model theory;turing machines data structures reachability analysis;reachability analysis;concrete;structured data;radio access networks	Pointer programs are a model of structured computation within LOGSPACE. They capture the common description of LOGSPACE algorithms as programs that take as input some structured data (e.g. a graph) and that store in memory only a constant number of pointers to the input (e.g. to the graph nodes). In this paper we study undirected s-t-reachability for a class of pure pointer programs in which one can work with a constant number of abstract pointers, but not with arbitrary data, such as memory registers of logarithmic size. In earlier work we have formalised this class as a programming language PURPLE that features a for all-loop for iterating over the input structure and thus subsumes other formalisations of pure pointer programs, such as Jumping Automata on Graphs JAGs and Deterministic Transitive Closure logic (DTC-logic) for locally ordered graphs. In this paper we show that PURPLE cannot decide undirected s-t-reachability, even though there does exist a LOGSPACE-algorithm for this problem by Reingold's theorem. As a corollary we obtain that DTC-logic for locally ordered graphs cannot express undirected s-t-reachability.	apl;algorithm;automaton;computation;data structure;graph (discrete mathematics);iterated function;iterator;l (complexity);logic programming;pointer (computer programming);programming language;reachability;sl (complexity);software development process;traverse;transitive closure	Ulrich Schöpp;Martin Hofmann	2008	2009 24th Annual IEEE Symposium on Logic In Computer Science	10.1109/LICS.2009.41	discrete mathematics;concrete;data structure;data model;finite model theory;l;computer science;turing machine;theoretical computer science;cayley graph;processor register;programming language;computational model;transitive closure;artificial neural network;algorithm	Logic	-11.91501146720071	23.91715706684875	40647
c1858417f020faa68f46d3171e463647de149f99	logic programs for primitive recursive sets	logic programs	Meyer and Ritchie have previously given a description of primitive recursive functions by loop-programs In this paper a class of logic programs is described which computes the primitive recursive sets on Herbrand universes Furthermore, an internal description of primitive recursive functions and sets on Herbrand universes is given		Urs-Martin Künzi	1993	J. Log. Comput.	10.1093/logcom/3.4.401	dynamic logic;zeroth-order logic;discrete mathematics;description logic;computer science;bunched logic;predicate functor logic;computational logic;signature;axiomatic semantics;multimodal logic;algorithm;autoepistemic logic	Theory	-13.933965546916173	15.057181705349006	40702
3d0c86287aa20ea81c9759d2531210117a66991a	verification of context-sensitive knowledge and action bases		Knowledge and Action Bases (KABs) have been recently proposed as a formal framework to capture the dynamics of systems which manipulate Description Logic (DL) Knowledge Bases (KBs) through action execution. In this work, we enrich the KAB setting with contextual information, making use of different context dimensions. On the one hand, context is determined by the environment using context-changing actions that make use of the current state of the KB and the current context. On the other hand, it affects the set of TBox assertions that are relevant at each time point, and that have to be considered when processing queries posed over the KAB. Here we extend to our enriched setting the results on verification of rich temporal properties expressed in μ-calculus, which had been established for standard KABs. Specifically, we show that under a run-boundedness condition, verification stays decidable and does not incur in any additional cost in terms of worst-case complexity. We also show how to adapt syntactic conditions ensuring run-boundedness so as to account for contextual information, taking into account context-dependent activation of TBox assertions.	best, worst and average case;context-sensitive language;description logic;tbox;worst-case complexity	Diego Calvanese;Ismail Ilkan Ceylan;Marco Montali;Ario Santoso	2014		10.1007/978-3-319-11558-0_36	computer science;artificial intelligence;data mining;communication	AI	-18.5174097733813	8.105504955903031	40722
