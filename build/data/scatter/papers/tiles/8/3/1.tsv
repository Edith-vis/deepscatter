id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
7bd26e9fbb0d7664e945487c57cb41de851d3d21	a novel confidence-based framework for multiple expert decision fusion	decision fusion;character recognition	A novel confidence-based parallel multiple expert decision combination framework is introduced. The traditional approaches to parallel multiple source decision fusion either take no account of the confidence of each decision by each participating expert in the combined framework or only exploit the confidences associated with each decision. But it is entirely possible to incorporate more additional priori information in the form of various confidence indices that can be estimated from the performances of various participating experts. The confidence-based parallel multiple expert decision combination framework proposed here addresses this shortcoming. Very encouraging results have been obtained by implementing this proposed framework in combining decisions of multiple experts applied to the problem of handwritten and machine printed character recognition.	optical character recognition;performance;printing	Fuad Rahman;Michael C. Fairhurst	1998		10.5244/C.12.21	decision analysis;decision engineering;computer science;machine learning;pattern recognition;data mining;business decision mapping	ML	-16.92145493353409	-70.00453620340768	20949
8fd7eeb4fb70a4cadf5539c9ec08ab92cc440a39	semantic analysis for enhanced medical retrieval		Medical search technologies are crucial to enable the user to rapidly and effectively discover useful information from massive medical and clinical data. Because of the complexity of medical terminology, traditional information search methods have not fully expressed the intention of the query request and explored the potential semantic knowledge in the document. In this paper, we propose a multi-analysis approach by considering the medical ontology as a semantic resource, which can excavate latent semantic information of a user's query request. In addition, we also recognize topics of medical documents to express text contents for providing support for calculating the similarity between query keywords and documents. Our experiments on PubMed medical article collections show that the semantic-based multi-analysis approach is feasible and efficient compared with other traditional approaches in medical retrieval.	baseline (configuration management);experiment;ontology components;pubmed;semantic similarity	Yangyang Kang;Jianqiang Li;Ji-Jiang Yang;Qing Wang;Zhihua Sun	2017	2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2017.8122762	artificial intelligence;medical terminology;machine learning;semantic memory;ontology (information science);semantics;data modeling;information retrieval;ontology;computer science;unified modeling language	DB	-32.04773465407226	-67.78628618253488	20980
889fd7010f400f63ec50d5fce83c5273a9b519db	a methodology to enhance spatial understanding of disease outbreak events reported in news articles	geographical information;geographic information;health surveillance;information system;natural language processing;spatial information;public health surveillance;disease outbreak	PURPOSE The emergence and re-emergence of disease outbreaks of international concern in the last several years has raised the importance of health surveillance systems that exploit the open media for their timely and precise detection of events. However, one of the key barriers faced by current event-based health surveillance systems is in identifying fine-grained terms for an outbreak's geographical location. In this article, we present a method to tackle this problem by associating each reported event with the most specific spatial information available in a news report. This would be useful not only for health surveillance systems, but also for other event-centered processing systems.   METHODS To develop an automated spatial attribute annotation system, we first created a gold standard corpus for training a machine learning model. Since the qualitative analysis on data suggested that the event class might have an impact on the spatial attribute annotation, we also developed an event classification system to incorporate event class information into the spatial attribute annotation model. To automatically recognize the spatial attribute of events, several approaches, ranging from a simple heuristic technique to a more sophisticated approach based on a state-of-the-art Conditional Random Fields (CRFs) model were explored. Different feature sets were incorporated into the model and compared.   RESULTS The evaluations were conducted on 100 outbreak news articles. Spatial attribute recognition performance was evaluated based on three metrics; precision, recall and the harmonic mean of precision and recall (F-score). Among three strategies proposed in this article, the CRF model appeared to be the most promising for spatial attribute recognition with a best performance of 85.5% F-score (86.3% precision and 84.7% recall).   CONCLUSION We presented a methodology for associating each event in media outbreak reports with their spatial attribute at the finest level of granularity. Our goal has been to provide a means for enhancing the spatial understanding of outbreak-related events. Evaluation studies showed promising results for automatic spatial attribute annotation. In the future, we plan to explore more features, such as semantic correlation between words, that maybe useful for the spatial attribute annotation task.	annotation;attribute grammar;body of uterus;classification;conditional random field;emergence;evaluation;f1 score;geographic information system;heuristic;location (geography);machine learning;precision and recall;protocol event	Hutchatai Chanlekha;Nigel Collier	2010	International journal of medical informatics	10.1016/j.ijmedinf.2010.01.014	medicine;computer science;artificial intelligence;data science;data mining;spatial analysis;outbreak;information retrieval;information system	Web+IR	-20.470586219784135	-58.50382996768158	21107
8159fa7727322545b120d35e6a59446f031103a4	active exploration for learning rankings from clickthrough data	search engine;active exploration;bayesian approach;online learning;web search;user behavior;synthetic data;learning to rank;clickthrough data	We address the task of learning rankings of documents from search enginelogs of user behavior. Previous work on this problem has relied onpassively collected clickthrough data. In contrast, we show that anactive exploration strategy can provide data that leads to much fasterlearning. Specifically, we develop a Bayesian approach for selectingrankings to present users so that interactions result in more informativetraining data. Our results using the TREC-10 Web corpus, as well assynthetic data, demonstrate that a directed exploration strategy quicklyleads to users being presented improved rankings in an online learningsetting. We find that active exploration substantially outperformspassive observation and random exploration.	interaction	Filip Radlinski;Thorsten Joachims	2007		10.1145/1281192.1281254	bayesian probability;computer science;data science;machine learning;data mining;learning to rank;search engine;statistics;synthetic data	ML	-32.52695563235365	-52.11531483920134	21184
db6b036561407a7a6e2f27ccb495e38cc15e9c59	dynasim: a matlab toolbox for neural modeling and simulation	gnu octave;code generation;code:matlab;dynamical systems;graphical user interface;neural models;neuroscience gateway	DynaSim is an open-source MATLAB/GNU Octave toolbox for rapid prototyping of neural models and batch simulation management. It is designed to speed up and simplify the process of generating, sharing, and exploring network models of neurons with one or more compartments. Models can be specified by equations directly (similar to XPP or the Brian simulator) or by lists of predefined or custom model components. The higher-level specification supports arbitrarily complex population models and networks of interconnected populations. DynaSim also includes a large set of features that simplify exploring model dynamics over parameter spaces, running simulations in parallel using both multicore processors and high-performance computer clusters, and analyzing and plotting large numbers of simulated data sets in parallel. It also includes a graphical user interface (DynaSim GUI) that supports full functionality without requiring user programming. The software has been implemented in MATLAB to enable advanced neural modeling using MATLAB, given its popularity and a growing interest in modeling neural systems. The design of DynaSim incorporates a novel schema for model specification to facilitate future interoperability with other specifications (e.g., NeuroML, SBML), simulators (e.g., NEURON, Brian, NEST), and web-based applications (e.g., Geppetto) outside MATLAB. DynaSim is freely available at http://dynasimtoolbox.org. This tool promises to reduce barriers for investigating dynamics in large neural models, facilitate collaborative modeling, and complement other tools being developed in the neuroinformatics community.	anatomical compartments;benchmark (computing);brian;central processing unit;chronux;coder device component;cognition disorders;compiler;complement system proteins;computation (action);computational neuroscience;computer cluster;data transfer object;emoticon;estimation theory;eutropiichthys murius;event-driven programming;gnu octave;graphical user interface;human brain project;ibm notes;imagery;interoperability;large;linux;matlab;mex file;mathematical optimization;mathematics;medical device incompatibility problem;microsoft windows;multi-core processor;nest (neural simulation tool);neuroml;neuroinformatics;neuron;neuroscience discipline;object-based language;open-source software;operating system;parallel computing;particle filter;physical object;population parameter;population model;python;rapid prototyping;research support as topic;sbml;software documentation;specification;sulfanilamide;supercomputer;synapses;synaptic package manager;usability;user interface device component;version control;web application;wolfram alpha;workaround;benefit;macos	Jason S. Sherfey;Austin E. Soplata;Salva Ardid;Erik A. Roberts;David A. Stanley;Benjamin R. Pittman-Polletta;Nancy J Kopell	2018		10.3389/fninf.2018.00010	data mining;matlab;network model;software;computer architecture;gnu octave;computer science;neuroinformatics;sbml;modeling and simulation;graphical user interface	HPC	-7.220818451690749	-59.35741348664355	21187
2ae71d8e1f51a7918fdd3682a6256f95f4980462	gestures as point clouds: a $p recognizer for user interface prototypes	n;hungarian;comparing classifiers;p;point clouds;hausdorff;euclidean;1;multistrokes;gesture recognition	"""Rapid prototyping of gesture interaction for emerging touch platforms requires that developers have access to fast, simple, and accurate gesture recognition approaches. The $-family of recognizers ($1, $N) addresses this need, but the current most advanced of these, $N-Protractor, has significant memory and execution costs due to its combinatoric gesture representation approach. We present $P, a new member of the $-family, that remedies this limitation by considering gestures as clouds of points. $P performs similarly to $1 on unistrokes and is superior to $N on multistrokes. Specifically, $P delivers >99% accuracy in user-dependent testing with 5+ training samples per gesture type and stays above 99% for user-independent tests when using data from 10 participants. We provide a pseudocode listing of $P to assist developers in porting it to their specific platform and a """"cheat sheet"""" to aid developers in selecting the best member of the $-family for their specific application needs."""	cheat sheet;finite-state machine;gesture recognition;point cloud;pseudocode;rapid prototyping;user interface	Radu-Daniel Vatavu;Lisa Anthony;Jacob O. Wobbrock	2012		10.1145/2388676.2388732	p;computer vision;haplogroup n;simulation;speech recognition;computer science;artificial intelligence;operating system;machine learning;gesture recognition;point cloud;hausdorff space	HCI	-8.225083325621789	-67.4960848874674	21257
152eb04a9a16a3f3c5c32ea67bca4d8643cb55eb	holstep: a machine learning dataset for higher-order logic theorem proving		Large computer-understandable proofs consist of millions of intermediate logical steps. The vast majority of such steps originate from manually selected and manually guided heuristics applied to intermediate goals. So far, machine learning has generally not been used to filter or generate these steps. In this paper, we introduce a new dataset based on Higher-Order Logic (HOL) proofs, for the purpose of developing new machine learning-based theorem-proving strategies. We make this dataset publicly available under the BSD license. We propose various machine learning tasks that can be performed on this dataset, and discuss their significance for theorem proving. We also benchmark a set of simple baseline machine learning models suited for the tasks (including logistic regression, convolutional neural networks and recurrent neural networks). The results of our baseline models show the promise of applying machine learning to HOL theorem proving.	artificial neural network;automated theorem proving;bsd;baseline (configuration management);benchmark (computing);convolutional neural network;hol (proof assistant);heuristic (computer science);logistic regression;machine learning;recurrent neural network	Cezary Kaliszyk;François Chollet;Christian Szegedy	2017	CoRR		computer science;artificial intelligence;online machine learning;machine learning;data mining;mathematics;computational learning theory;active learning;algorithm;statistics	ML	-15.834512780098285	-70.25944112889917	21290
851e7b462e225414aa5dda7f626055e3d98bfe2b	human-competitive tagging using automatic keyphrase extraction	new domain;traditional evaluation technique;automatic tagging;human-competitive tagging;research area;new algorithm;human taggers;statistical keyphrase extraction;state-of-the-art keyphrase extraction algorithm;automatic keyphrase extraction;utilizes semantic information;extracts tag;computer science	This paper connects two research areas: automatic tagging on the web and statistical keyphrase extraction. First, we analyze the quality of tags in a collaboratively created folksonomy using traditional evaluation techniques. Next, we demonstrate how documents can be tagged automatically with a state-of-the-art keyphrase extraction algorithm, and further improve performance in this new domain using a new algorithm, “Maui”, that utilizes semantic information extracted from Wikipedia. Maui outperforms existing approaches and extracts tags that are competitive with those assigned by the best performing human taggers.	algorithm;decision tree;folksonomy;information extraction;keyword extraction;machine learning;tag (metadata);tf–idf;wikipedia	Olena Medelyan;Eibe Frank;Ian H. Witten	2009			computer science;data mining;world wide web;information retrieval	NLP	-26.76424928162974	-64.81678324557252	21300
6d5744993cb49411a5f4ec0f0933770bc5307808	alicante at clef 2009 robust-wsd task		In this paper we explore the use of semantic classes in an information retrieval system in order to improve the results in the RobustWSD task at CLEF 2009. Thus, we use two different ontologies of semantic classes (WordNet domain and Basic Level Concepts) in order to re-rank the retrieved documents and obtain better recall and precision. Finally, we implement a new method to weight the expanded terms taking into account the weights of the original query terms and their relations in WordNet with respect to the new ones which have demonstrated to improve the results.	experiment;information retrieval;ontology (information science);precision and recall;smoothing;software propagation;type class;web services for devices;web search engine;wordnet	Javi Fernández;Rubén Izquierdo;José M. Gómez	2009				AI	-31.18904939385568	-65.57514699226522	21446
26dccd1b1db7b3c73487a16bbe9ca74244d54870	personal web space	uniform resource locators computer architecture service oriented architecture software systems information management world wide web search engines monitoring dictionaries information retrieval;cache storage;information resources;search engine;query processing information resources cache storage search engines relevance feedback information retrieval system evaluation dictionaries;query processing;search engines;information retrieval;software systems;dictionaries;information retrieval system evaluation;functional requirement;evolutionary process;user performance software system personal web space pws www bookmarks url addresses proxy server caching high interest pages evolutionary process importance rank search engine responses user input freshness monitoring data dictionary key word queries information caching information retrieval;relevance feedback;proxy server	This paper describes the functional requirement and architecture of a software system called Personal Web Space (PWS). A PWS is a system to manage the informa­ tion from the Web, for either leisure or work related use. Similar to bookmarks, a PWS stores a set of URL addresses bur, in addition, it caches. on a proxy server pages of high imerest. PWS is developed through a evolutionary process that is outlined in the paper. First, a method is prescribed to rank a page (or an URL address) based on the degree of imporlance to a person as determined by search engine responses and user input. Second, the information stored in a pws must be refreshed periodically to keep up with the new state of information available on network; therefore a technique is proposed for determining and monitoring freshness. Thirdly, a pws .evolves through enlargement to include more information·.domains, or refinement to con­ centrate on smaller sub-domains. The paper describes how enlargement and refinement operations are mapped on to the stored PWS data dictionary. Finally, key-word queries can be issued against a PWS to get interesting information quickly. In this sense, a PWS uses a combination of infor­ mation caching, information retrieval and bookmarks tech­ nique to enhance significantly user performance on the Web.	cache (computing);data dictionary;functional requirement;information retrieval;microsoft personal web server;proxy server;refinement (computing);replay attack;server (computing);software system;web search engine;world wide web	Yangjun Chen;Tony Liu;Paul G. Sorenson	2002		10.1109/ICDCSW.2002.1030765	computer science;operating system;database;distributed computing;world wide web;computer security;information retrieval;search engine	Web+IR	-29.987863138443778	-52.38869858586887	21447
372c9970705fbcc0f454f2658af2d1a802ea102b	left-handed or right-handed? a data-driven approach to analysing characteristics of handedness based on language use		Numerous studies have identified differences between left-handed and right-handed people, especially in the fields of psychology and neuroscience. Using a social media setting, this paper presents a data-driven approach to explore whether a person’s handedness can be identified given his or her writing, and shows handedness characteristics that can be inferred from language.	experiment;social media	H. D. Choe;Rada Mihalcea	2016			artificial intelligence;cognitive psychology;machine learning;computer science;social media;on language;data-driven	NLP	-8.757443073116056	-78.42140840409377	21451
8b58e6816424c14de657bae5ed21f3e4a96bf58f	tat: an author profiling tool with application to arabic emails	text processing;machine learning	This paper reports on the application of the Text Attribution Tool (TAT) to profiling the authors of Arabic emails. The TAT system has been developed for the purpose of language-independent author profiling and has now been trained on two email corpora, English and Arabic. We describe the overall TAT system and the Machine Learning experiments resulting in classifiers for the different author traits. Predictions for demographic and psychometric author traits show improvements over the baseline for some of the author traits with both the English and the Arabic data. Arabic presents particular challenges for NLP and this paper describes more specifically the text processing components developed to handle Arabic emails.	algorithm;baseline (configuration management);batman: arkham asylum;email;experiment;language-independent specification;machine learning;natural language processing;profiling (computer programming);text corpus	Dominique Estival;Tanja Gaustad;Son Bao Pham;Will Radford;Ben Hutchinson	2007			natural language processing;speech recognition;computer science;data science	NLP	-22.119635583067364	-69.33088054954379	21460
e169481199faabe93af58864480039c4214da286	terminological enrichment for non-interactive mt evaluation	user needs;machine translation	In a previous study (Dabbadie, Mustafa, Timimi, 2001) we set a methodology for non interactive machine translation evaluation on big corpora, assuming that the goal of the translation was a simple understanding of the original message. The source text, in French, provided by INRA (Institut National pour la Recherche Agronomique i.e. National Institute for Agronomic Research) deals with biotechnology and animal reproduction. It has been translated into English by REVERSO. The output of the system (i.e. the result of the assembling of several components), as opposed to its individual modules or specific components (i.e. analysis, generation, grammar, lexicon, core, etc.), has been evaluated. In the present study we will recall the methodology and results obtained in the case of simple translation by REVERSO with no terminological enrichment and compare them to the results obtained after terminological enrichment. The aim of this study is to evaluate the impact of specific terminology when integrated to an MT System and after having run the system with a basic bilingual dictionary.	bilingual dictionary;gene ontology term enrichment;interactive machine translation;lexicon;linear algebra;text corpus	Marianne Dabbadie;Widad Mustafa El Hadi;Ismaïl Timimi	2002			natural language processing;computer science;data mining;linguistics;machine translation	NLP	-29.719421959456987	-74.95897635251939	21490
ac28ae8e80d4cc4fbfa25612d83f799c0196e116	crowd-sourced iterative annotation for narrative summarization corpora		We present an iterative annotation process for producing aligned, parallel corpora of abstractive and extractive summaries for narrative. Our approach uses a combination of trained annotators and crowd-sourcing, allowing us to elicit human-generated summaries and alignments quickly and at low cost. We use crowd-sourcing to annotate aligned phrases with the text-to-text generation techniques needed to transform each phrase into the other. We apply this process to a corpus of 476 personal narratives, which we make available on the Web.	amazon mechanical turk;automatic summarization;crowdsourcing;iteration;iterative method;natural language generation;natural language processing;parallel text;rewrite (programming);text corpus;the turk;world wide web	Kathy McKeown;Jessica Ouyang;Serina Chang	2017			computer science;natural language processing;automatic summarization;artificial intelligence;information retrieval;narrative;annotation	NLP	-30.591887524140315	-73.19893100110181	21579
197f53cfad2d4480b2258e13f88946697490ee3f	creating large-scale argumentation structures for dialogue systems		We are planning to develop argumentative dialogue systems that can discuss various topics with people by using large-scale argumentation structures. In this paper, we describe the creation process of these argumentation structures. We created ten structures each having more than 2000 nodes of five topics in English and five topics in Japanese. We analyzed the created structures for their characteristics and investigated the differences between the two languages. We conducted an evaluation experiment to ascertain that the structures can be applied to dialogue systems. We conducted another experiment to use the created argumentation structures as training data for augmenting the current argumentation structures.	dialog system	Kazuki Sakai;Akari Inago;Ryuichiro Higashinaka;Yuichiro Yoshikawa;Hiroshi Ishiguro;Junji Tomita	2018			artificial intelligence;natural language processing;argumentation theory;computer science	NLP	-26.892155627603273	-72.6720833447421	21637
978a23d508f3deb1c53cc0a6f901ba97ad7f99bf	content-based histopathology image retrieval using a kernel-based semantic annotation framework	kernels;semantic annotation;semantic representation;automatic image annotation;histopathology;auto annotation;biomedical imaging;image annotation;medical image;image representation;feature integration;automatic annotation;next generation;kernel method;kernel alignment;digital image;experimental evaluation;histology;biomedical images;image retrieval	Large amounts of histology images are captured and archived in pathology departments due to the ever expanding use of digital microscopy. The ability to manage and access these collections of digital images is regarded as a key component of next generation medical imaging systems. This paper addresses the problem of retrieving histopathology images from a large collection using an example image as query. The proposed approach automatically annotates the images in the collection, as well as the query images, with high-level semantic concepts. This semantic representation delivers an improved retrieval performance providing more meaningful results. We model the problem of automatic image annotation using kernel methods, resulting in a unified framework that includes: (1) multiple features for image representation, (2) a feature integration and selection mechanism (3) and an automatic semantic image annotation strategy. An extensive experimental evaluation demonstrated the effectiveness of the proposed framework to build meaningful image representations for learning and useful semantic annotations for image retrieval.	addresses (publication format);archive;automatic image annotation;collections (publication);digital image;feature integration theory;high- and low-level;histopathology;image retrieval;kernel (operating system);kernel method;medical imaging;next-generation network;programming paradigm;query by example;question (inquiry);unified framework;visual descriptor	Juan C. Caicedo;Fabio A. González;Eduardo Romero	2011	Journal of biomedical informatics	10.1016/j.jbi.2011.01.011	computer vision;kernel method;visual word;image retrieval;computer science;pattern recognition;automatic image annotation;information retrieval;digital image	Vision	-14.242683885809408	-57.63911272224218	21652
2bbf385c1bfc32664abac3391d3ad4efe5ce87dc	cluster-based prediction of user ratings for stylistic surface realisation		Surface realisations typically depend on their target style and audience. A challenge in estimating a stylistic realiser from data is that humans vary significantly in their subjective perceptions of linguistic forms and styles, leading to almost no correlation between ratings of the same utterance. We address this problem in two steps. First, we estimate a mapping function between the linguistic features of a corpus of utterances and their human style ratings. Users are partitioned into clusters based on the similarity of their ratings, so that ratings for new utterances can be estimated, even for new,unknownusers. In a second step, the estimated model is used to re-rank the outputs of a number of surface realisers to produce stylistically adaptive output. Results confirm that the generated styles are recognisable to human judges and that predictive models based on clusters of users lead to better rating predictions than models based on an average population of users.	cluster analysis;computer cluster;eye tracking;multimodal interaction;predictive modelling;recommender system;scalability;sentiment analysis;spatial variability;speech corpus;text corpus	Nina Dethlefs;Heriberto Cuayáhuitl;Helen F. Hastie;Verena Rieser;Oliver Lemon	2014			natural language processing;speech recognition;computer science	Web+IR	-16.540357967251634	-78.80108910342533	21693
f95ae0efdfce2e349cb1dbffeb8cbbfbe3c6ad48	nict's neural and statistical machine translation systems for the wmt18 news translation task.		This paper presents the NICT’s participation to the WMT18 shared news translation task. We participated in the eight translation directions of four language pairs: EstonianEnglish, Finnish-English, Turkish-English and Chinese-English. For each translation direction, we prepared state-of-the-art statistical (SMT) and neural (NMT) machine translation systems. Our NMT systems were trained with the transformer architecture using the provided parallel data enlarged with a large quantity of back-translated monolingual data that we generated with a new incremental training framework. Our primary submissions to the task are the result of a simple combination of our SMT and NMT systems. Our systems are ranked first for the Estonian-English and FinnishEnglish language pairs (constraint) according to BLEU-cased.	bleu;experiment;performance;satisfiability modulo theories;simultaneous multithreading;statistical machine translation;transformer	Benjamin Marie;Rui Wang;Atsushi Fujita;Masao Utiyama;Eiichiro Sumita	2018			architecture;machine translation;natural language processing;artificial intelligence;ranking;computer science	NLP	-22.075392017677316	-74.96435270578138	21709
a0a5e4494e589c40bd6fccc456b9915442398662	understanding counterfactuality: a review of experimental evidence for the dual meaning of counterfactuals		Cognitive and linguistic theories of counterfactual language comprehension assume that counterfactuals convey a dual meaning. Subjunctive-counterfactual conditionals (e.g., 'If Tom had studied hard, he would have passed the test') express a supposition while implying the factual state of affairs (Tom has not studied hard and failed). The question of how counterfactual dual meaning plays out during language processing is currently gaining interest in psycholinguistics. Whereas numerous studies using offline measures of language processing consistently support counterfactual dual meaning, evidence coming from online studies is less conclusive. Here, we review the available studies that examine online counterfactual language comprehension through behavioural measurement (self-paced reading times, eye-tracking) and neuroimaging (electroencephalography, functional magnetic resonance imaging). While we argue that these studies do not offer direct evidence for the online computation of counterfactual dual meaning, they provide valuable information about the way counterfactual meaning unfolds in time and influences successive information processing. Further advances in research on counterfactual comprehension require more specific predictions about how counterfactual dual meaning impacts incremental sentence processing.	computation;counterfactual conditional;dual;electroencephalography;eye tracking;failure;increment;information processing;linguistics;list comprehension;neuroimaging;online and offline;psycholinguistics;resonance;tom;theory;fmri	Eugenia Kulakova;Mante S. Nieuwland	2016		10.1111/lnc3.12175	psychology;counterfactual conditional;linguistics;counterfactual thinking;communication;social psychology	NLP	-8.599813809856762	-77.99052684877581	21762
92d332feea9edbd202d9059a7654aad5a31149ea	integrating multi-source bilingual information for chinese word segmentation in statistical machine translation	bilingual information;chinese segmentation;statistical machine translation	Chinese texts are written without spaces between the words, which is problematic for Chinese-English statistical machine translation (SMT). The most widely used approach in existing SMT systems is apply a fixed segmentations produced by the off-the-shelf Chinese word segmentation (CWS) systems to train the standard translation model. Such approach is sub-optimal and unsuitable for SMT systems. We propose a joint model to integrate the multi-source bilingual information to optimize the segmentations in SMT. We also propose an unsupervised algorithm to improve the quality of the joint model iteratively. Experiments show that our method improve both segmentation and translation performance in different data environment. © Springer-Verlag 2013.	statistical machine translation;text segmentation	Wei Chen;Wei Wei;Zhenbiao Chen;Bo Xu	2013		10.1007/978-3-642-41491-6_7	natural language processing;speech recognition;example-based machine translation;linguistics;machine translation;rule-based machine translation;machine translation software usability	NLP	-22.001009324600922	-76.85898069721803	21775
aceab244dd5290376976b04cadd6b62cd3c6ec86	similarity detection among longer texts by matching keywords found in segments	natural text;similarity detection among textual documents;document topic similarity;document content segmentation	Similarity detection among textual data is becoming more important with the spread of Internet and growth of textual data on it. The field of our research are long texts as this domain requires different and more sophisticated approaches when compared to the standard methods that are well working on shorter texts. Identifying similarity among longer texts usually requires identifying similarity among smaller segments that can be found in these texts. In this paper we propose our own approach aimed to segment textual documents written in natural language. The segmentation process we propose is based on analysing positions of important words in document content. By grouping these important words we create segments that do or do not overlap. We carried several experiments with our approach on the corpus of students' bachelor and master thesis. The results we present here prove that the proposed method is suitable for detecting similarity among longer textual documents. Although the target language of our experiments is Slovak, our approach can be easily applied on other languages as well.	compiler;experiment;internet;natural language;sensor;text corpus	Tomás Kucecka;Daniela Chudá	2014		10.1145/2659532.2659635	natural language processing;computer science;data mining;world wide web;information retrieval	NLP	-25.044812756755913	-66.08363680058771	21785
026795cdf5b7347f31f571bd812a36065550739d	intra-firm information flow: a content-structure perspective	information flow;social network analysis;natural language processing	This paper endeavors to bring together two largely disparate areas of research. On one hand, text mining methods treat each document as an independent instance despite the fact that in many text domains, documents are linked and their topics are correlated. For example, web pages of related topics are often connected by hyperlinks and scientific papers from related fields are typically linked by citations. On the other hand, Social Network Analysis (SNA) typically treats edges between nodes according to ”flat” attributes in binary form alone. This paper proposes a simple approach that addresses both these issues in data mining scenarios involving corpora of linked documents. According to this approach, after assigning weights to the edges between documents, based on the content of the documents associated with each edge, we apply standard SNA and network theory tools to the network. The method is tested on the Enron email corpus and successfully discovers the central people in the organization and the relevant communications between them. Furthermore, Our findings suggest that due to the non-conservative nature of information, conservative centrality measures (such as PageRank) are less adequate here than non-conservative centrality measures (such as eigenvector centrality). Keywords— Natural language processing, social network analysis, information flow	data mining;eigenvector centrality;email;hyperlink;information flow (information theory);natural language processing;network theory;pagerank;scientific literature;social network analysis;text corpus;text mining;web page	Yakir Berchenko;Or Daliot;Nir N. Brueller	2011		10.1007/978-3-642-24800-9_6	social network analysis;information flow;computer science;artificial intelligence;machine learning;data mining;world wide web	Web+IR	-25.831225899877985	-58.12928686848982	21859
b16311e861beb6f7db78a4a890b528ff362a682f	training recurrent neural networks to learn lexical encoding and thematic role assignment in parsing mandarin chinese sentences	cluster algorithm;learning experience;supervised learning;connectionistic parsing of mandarin chinese;backpropagation;automatic generation;thematic role assignment;scaling up;mandarin chinese;learning methods;thematic roles;recurrent network;simple recurrent network;recurrent neural networks;recurrent neural network;extended backpropagation learning;distributed lexical encodings	In this paper, we used the extended backpropagation learning method on the recurrent networks to learn lexical encodings and thematic role assignment tasks in parsing Mandarin Chinese sentences. In order to objectively evaluate the learning performance, the training and test sentences are automatically generated from tentatively designed sentence templates. Three learning experiments were carried out. In the first two experiments, we used a small lexicon of 33 words to observe the learned encodings and learning performances. 75 training sentences and 6503 test sentences were used for the supervised learning on a simple recurrent network (SRN) and a modified SRN (MSRN), respectively, in the first experiment. The experimental results showed the MSRN performed better than the SRN. In the second experiment, we focused on the investigation of the learning of thematic role assignment of sentences with embedded clause. 100 training sentences and 6609 test sentences were used. MSRN performed well on compound sentences of POST type, but not so satisfactorily on MIDDLE type. To analyze the learning of lexical encodings, a merge clustering algorithm and Kohonen’s feature map methods were used. The results showed the words belonging to the same category tended to have similar representations. In the third experiment, we attempted to scale up the experiment. The lexicon size increases from 33 to 82. 900 training sentences and 54 194 test sentences were used. We observe quite consistent performance in experiment 2 after scaling up.	algorithm;artificial neural network;backpropagation;cluster analysis;embedded system;experiment;image scaling;lexicon;parsing;performance;recurrent neural network;super robot monkey team hyperforce go!;supervised learning;teuvo kohonen	Tung-Bo Chen;Koong H. C. Lin;Von-Wun Soo	1997	Neurocomputing	10.1016/S0925-2312(97)00007-6	natural language processing;speech recognition;computer science;recurrent neural network;machine learning;supervised learning	NLP	-16.53521069766684	-75.71101572083121	21870
98215162d9b51b06e817b387d91f56ab033633a6	domain information for fine-grained person name categorization	search engine;ontology population;semantic information;named entity recognition;named entity recognizer;coarse grained;natural language processing;named entity;question answering	Named Entity Recognition became the basis of many Natural Language Processing applications. However, the existing coarsegrained named entity recognizers are insufficient for complex applications such as Question Answering, Internet Search engines or Ontology population. In this paper, we propose a domain distribution approach according to which names which occur in the same domains belong to the same fine-grained category. For our study, we generate a relevant domain resource by mapping and ranking the words from the WordNet glosses to their WordNetDomains. This approach allows us to capture the semantic information of the context around the named entity and thus to discover the corresponding fine-grained name category. The presented approach is evaluated with six different person names and it reaches 73% f-score. The obtained results are encouraging and perform significantly better than a majority baseline.	baseline (configuration management);categorization;f1 score;finite-state machine;lexical definition;named entity;natural language processing;question answering;wordnet	Zornitsa Kozareva;Sonia Vázquez;Andrés Montoyo	2008		10.1007/978-3-540-78135-6_26	natural language processing;question answering;computer science;data mining;entity linking;weak entity;information retrieval;search engine	AI	-26.444777888777992	-70.07284203114047	21883
4024e445d30186c10771329def8a3177506e1bab	hotspotting - a probabilistic graphical model for image object localization through crowdsourcing		Object localization is an image annotation task which consists of finding the location of a target object in an image. It is common to crowdsource annotation tasks and aggregate responses to estimate the true annotation. While for other kinds of annotations consensus is simple and powerful, it cannot be applied to object localization as effectively due to the task’s rich answer space and inherent noise in responses. We propose a probabilistic graphical model to localize objects in images based on responses from the crowd. We improve upon natural aggregation methods such as the mean and the median by simultaneously estimating the difficulty level of each question and skill level of	aggregate data;algorithm;automatic image annotation;computer vision;crowdsourcing;graphical model;heuristic;natural language processing;sparse matrix	Mahyar Salek;Yoram Bachrach;Peter B. Key	2013				Vision	-15.346548544075864	-65.4811418119028	21890
e6e7c83b92c056910bebe2d088f95ad6365a1fd7	search and classification based language model adaptation	broadcast transcription;lm rescoring;topic adaptation	Adaptation techniques in language modeling have shown growing potentials in improving speech recognition performance. For topic adaptation, a set of pre-defined topic-specific language models are typically used, and adaptation is achieved through adjusting the interpolation weights. However, mismatch between the test data and the pre-defined models inevitably exists and is left untreated in the static approach. Instead of tuning the parameters in the existing models, this paper describes a method that dynamically extracts relevant documents from training sources according to intermediate decoding hypotheses to build new targeted language models. Different from general search-based document collection, a new and effective ranking method is used here for candidate extraction. The targeted language models are interpolated with the static topic language models and a general language model, and used for lattice rescoring. The proposed adaptation technique is implemented in a state-of-the-art Mandarin broadcast transcription system, and evaluated on the GALE task. We show that static topic adaptation reduces the relative character error rate by 4.9%. It is further shown that the proposed dynamic adaptation technique attains an additional 10.3% reduction in error rate. Index Terms – LM Rescoring, Topic Adaptation, Broadcast Transcription	archive;broadcast domain;decoding methods;interpolation;language model;mac os x 10.3 panther;medical transcription;speech recognition;super robot monkey team hyperforce go!;test data;transcription (software)	Qin Shi;Stephen M. Chu;Wen Liu;Hong-Kwang Jeff Kuo;Yi Liu;Yong Qin	2008			language identification;speech recognition;natural language processing;language model;computer science;artificial intelligence	NLP	-20.635736673234337	-79.12220248467246	22034
a1b9aa6a7b163fc4850022caff7f7bb0d2ad237a	extracting adverse drug reactions using deep learning and dictionary based approaches.		Drug labels contain detailed information about the drugs including their safety concerns which are regulated by the United States Food and Drug Administration (FDA). Adverse drug reactions (ADR) are adverse reactions associated with a specific drug. Automatic extraction of ADRs could help FDA greatly regulate drug safety. In this study, we employed an integrated approach of machine learning (ML)-based and dictionary/rule-based methods to recognize ADR terms and normalize these terms to MedDRA Preferred Terms. The machine learning approach was used for the identification of the entities and is based on a recently proposed deep learning architecture. The model includes bi-directional Long Short-Term Memory (Bi-LSTM), a Convolutional Neural Network (CNN), and Conditional Random Fields (CRF). Alternatively, a dictionaryand rule-based approach was also used to identify ADR terms. MedDRA terms were added as a dictionary to SciMiner, our in-house text-mining system, and multiple rules for term expansion and exclusion to increase coverage and accuracy were implemented. The best performance was achieved using a combined approach: ADRs were first identified by the ML-based approach and then normalized to MedDRA Preferred Terms by the dictionaryand rule-based approach. Our system achieved 76.97% F1 score on the entity detection task and 82.58% micro-averaged F1 score on the ADR normalization task in the TAC 2017 ADR challenge.	artificial neural network;conditional random field;convolutional neural network;data dictionary;data pre-processing;deep learning;entity;f1 score;find-a-drug;flight recorder;logic programming;long short-term memory;machine learning;meddra;preprocessor;sensor;text mining;word embedding	Mert Tiftikci;Arzucan Özgür;Yongqun He;Junguk Hur	2017			data mining;information retrieval;drug;deep learning;computer science;artificial intelligence	AI	-23.679945214367603	-69.83999163440468	22039
8b7bf181bb8b42a47680fe8165b8c1335eee4b8d	automated translation of a literary work: a pilot study		Current machine translation (MT) techniques are continuously improving. In specific areas, post-editing (PE) can enable the production of high-quality translations relatively quickly. But is it feasible to translate a literary work (fiction, short story, etc) using such an MT+PE pipeline? This paper offers an initial response to this question. An essay by the American writer Richard Powers, currently not available in French, is automatically translated and post-edited and then revised by non-professional translators. In addition to presenting experimental evaluation results of the MT+PE pipeline (MT system used, automatic evaluation), we also discuss the quality of the translation output from the perspective of a panel of readers (who read the translated short story in French, and answered a survey afterwards). Finally, some remarks of the official French translator of R. Powers, requested on this occasion, are given at the end of this article.	compiler;google summer of code;jean;machine translation;mind;postediting;rev;spreadsheet;surround sound	Laurent Besacier;Lane Schwartz	2015			simulation	NLP	-31.417046479585835	-74.30930136435748	22063
756ff7f5fc446d5832cf8b62c07723c9207a34a5	profiling phishing emails based on hyperlink information	hyperlink information;machine learning algorithms;phishing emails profiling;unsolicited e mail computer crime learning artificial intelligence pattern classification support vector machines;electronic mail;support vector machines;classifier predictions;multilabel classification problem;prediction algorithms;phishing emails profiling hyperlink information multilabel classification problem classifier predictions boosting algorithm adaboost support vector machine;computer crime;browsers;html;boosting algorithm;servers;feature extraction;adaboost;pattern classification;support vector machine;unsolicited e mail;learning artificial intelligence;electronic mail html feature extraction servers machine learning algorithms browsers prediction algorithms	In this paper, a novel method for profiling phishing activity from an analysis of phishing emails is proposed. Profiling is useful in determining the activity of an individual or a particular group of phishers. Work in the area of phishing is usually aimed at detection of phishing emails. In this paper, we concentrate on profiling as distinct from detection of phishing emails. We formulate the profiling problem as a multi-label classification problem using the hyperlinks in the phishing emails as features and structural properties of emails along with who is (i.e. DNS) information on hyperlinks as profile classes. Further, we generate profiles based on classifier predictions. Thus, classes become elements of profiles. We employ a boosting algorithm (AdaBoost) as well as SVMto generate multi-label class predictions on three different datasets created from hyperlink information in phishing emails. These predictions are further utilized to generate complete profiles of these emails. Results show that profiling can be done with quite high accuracy using hyperlink information.	adaboost;algorithm;cross-validation (statistics);email;experiment;hyperlink;multi-label classification;phishing;profiling (computer programming);statistical classification	John Yearwood;Musa A. Mammadov;Arunava Banerjee	2010	2010 International Conference on Advances in Social Networks Analysis and Mining	10.1109/ASONAM.2010.56	support vector machine;computer science;machine learning;data mining;internet privacy;world wide web	SE	-19.87223725314157	-55.57958328652911	22233
da23dc065f165d84939f0a43c76adb278d7cc1d4	physiology-based affect recognition for computer-assisted intervention of children with autism spectrum disorder	physiological sensing;autismo;modelizacion;teoria cognitiva;outil logiciel;interfase usuario;software tool;human computer interaction;analisis estadistico;fisiologia;support vector machines;user interface;autism spectrum disorder;chainage donnee;autism;hombre;cognitive theory;psychology;autism intervention;theorie cognitive;man machine system;enfant;physiologie;modelisation;classification a vaste marge;physiology;statistical analysis;emotion emotionality;nino;autisme;analyse statistique;data link;human;affect recognition;child;pattern recognition;sistema hombre maquina;emotion emotivite;angustia ansiedad;interface utilisateur;psychologie;angoisse anxiete;emocion emotividad;reconnaissance forme;support vector machine;maquina ejemplo soporte;vector support machine;reconocimiento patron;herramienta software;anxiety;modeling;psicologia;homme;systeme homme machine;ligazon datos	Generally, an experienced therapist continuously monitors the affective cues of the children with Autism Spectrum Disorders (ASD) and adjusts the course of the intervention accordingly. In this work, we address the problem of how to make the computer-based ASD intervention tools affect-sensitive by designing therapist-like affective models of the children with ASD based on their physiological responses. Two computer-based cognitive tasks are designed to elicit the affective states of liking, anxiety, and engagement that are considered important in autism intervention. A large set of physiological indices are investigated that may correlate with the above affective states of children with ASD. In order to have reliable reference points to link the physiological data to the affective states, the subjective reports of the affective states from a therapist, a parent, and the child himself/herself were collected and analyzed. A support vector machines (SVM)-based affective model yields reliable prediction with approximately 82.9% success when using the therapist's reports. This is the first time, to our knowledge, that the affective states of children with ASD have been experimentally detected via physiology-based affect recognition technique.		Changchun Liu;Karla Conn Welch;Nilanjan Sarkar;Wendy Stone	2008	Int. J. Hum.-Comput. Stud.	10.1016/j.ijhsc.2008.04.003	support vector machine;computer science;artificial intelligence	HCI	-7.12599723335939	-71.13214628702055	22245
0470303953de1d19765423f719d8314e3cb91278	using appraisal groups for sentiment analysis	opinion mining;sentiment classification;review classification;appraisal theory;text classification;sentiment analysis;shallow parsing	Little work to date in sentiment analysis (classifying texts by `positive' or `negative' orientation) has attempted to use fine-grained semantic distinctions in features used for classification. We present a new method for sentiment classification based on extracting and analyzing appraisal groups such as ``very good'' or ``not terribly funny''. An appraisal group is represented as a set of attribute values in several task-independent semantic taxonomies, based on Appraisal Theory. Semi-automated methods were used to build a lexicon of appraising adjectives and their modifiers. We classify movie reviews using features based upon these taxonomies combined with standard ``bag-of-words'' features, and report state-of-the-art accuracy of 90.2%. In addition, we find that some types of appraisal appear to be more significant for sentiment classification than others.	bag-of-words model;lexicon;sentiment analysis;taxonomy (general)	Casey Whitelaw;Navendu Garg;Shlomo Argamon	2005		10.1145/1099554.1099714	appraisal theory;computer science;pattern recognition;data mining;information retrieval;sentiment analysis	Web+IR	-25.199536056679552	-69.66819816785214	22300
cf6eef2ac9ca1a76d0e70006ca9bc3cb3fb80b0b	recognizing open-vocabulary relations between objects in images		How can we describe the relations between objects in a picture? As recent deep neural networks have exhibited impressive performance in identifying individual entities in a picture, in this study we turn our attention to recognize inter-object relations. To recognize open-domain relations, (a) we propose collecting relational concepts automatically from an image-text corpus. In addition, using collected relational instances, (b) we train a classifier to recognize inter-object relations. A relation recognition experiment conducted in our study suggests that relative information calculated from objects improves relation recognition effectively.	artificial neural network;bitext word alignment;commonsense knowledge (artificial intelligence);computer vision;deep learning;emoticon;entity;finite-state machine;image retrieval;statistical classification;statistical machine translation;text corpus;vocabulary	Masayasu Muraoka;Sumit Maharjan;Masaki Saito;Kota Yamaguchi;Naoaki Okazaki;Takayuki Okatani;Kentaro Inui	2016			natural language processing;artificial intelligence;computer science;vocabulary	AI	-17.132231937226823	-70.44890472546868	22366
c2f8ab71b34981c219f20d71fb549de27549e346	automatically integrating heterogeneous ontologies from structured web pages	web pages;knowledge models;ontologies;semantic matching;data schema;data models	This article presents an automated approach to integrate multiple analogous ontologies extracted from structured web pages into a common ontology. These ontologies from heterogeneous systems exhibit rich diversity in appearances, structures, terminologies and granularities. We design a unified similarity paradigm that can collect the implicit and explicit evidences that exhibit coherences among ontology and instance, semantic and structure, as well as linguistic and syntactic features. The similarity between ontology elements is derived from three aspects such as intension, extension and context, denoted by <INT, EXT, CXT>, where INT and EXT include corresponding weighted contents from their offspring, and CXT is relevant to evidences shown in their ancestors. The similarity in each aspect is calculated by means of their semantic overlapping and syntactic comparability. We develop a top-down matching algorithm based on matching space selection and similarity reuse; the algorithm facilitates less error-prone mappings and lower computational cost.	algorithm;algorithmic efficiency;cognitive dimensions of notations;computation;customer relationship management;intension;ontology (information science);programming paradigm;syntactic predicate;top-down and bottom-up design;web page	Shiren Ye;Tat-Seng Chua	2007	Int. J. Semantic Web Inf. Syst.	10.4018/jswis.2007040105	data modeling;data web;computer science;ontology;artificial intelligence;semantic web;social semantic web;web page;data mining;semantic web stack;database;website parse template;database schema;information retrieval;semantic analytics	Web+IR	-29.674050877058328	-66.2871341222917	22380
0eea5d61ac5d99430107f451cee86d8da4a5f28e	hierarchical auto-tagging: organizing q&a knowledge for everyone	auto tagging;text classification;hierarchical classification;oshiete goo;keyword extraction;knowledge sharing;community question answering;question answering	We propose a hierarchical auto-tagging system, TagHats, to improve users' knowledge sharing. Our system assigns three different levels of tags to Q&A documents: category, theme, and keyword. Multiple category tags can organize a document according to multiple viewpoints, and multiple theme and keyword tags can identify what the document is about clearly. Moreover, these hierarchical tags will be helpful in organizing documents to support everyone because different users have different demands in terms of tag specificity. Our system consists of a hierarchical classification method for assigning category and theme tags, a new keyword extraction method that considers the structure of Q&A documents, and a new method for selecting theme tag candidates from each category. Experiments with the documents of Oshiete! goo demonstrate that our system is able to assign hierarchical tags to the documents appropriately and is capable of outperforming baseline methods significantly.	baseline (configuration management);grey goo;keyword extraction;norm (social);organizing (structure);sensitivity and specificity;theme (computing)	Kyosuke Nishida;Ko Fujimura	2010		10.1145/1871437.1871697	natural language processing;question answering;computer science;data mining;world wide web;information retrieval	Web+IR	-25.966137989387125	-67.22274812689868	22382
c549f502ac61851d4d3682eda4b5dcb2b908203d	a comparative study on learning to rank with computational methods	web search query;artificial neural networks (ann);support vector machines (svm);multivariate adaptive regression spline (mars);conic multivariate adaptive regression splines (cmars)	Learning to rank is a supervised learning problem that aims to construct a ranking model. The most common application of learning to rank is to rank a set of documents against a query. In this work, we focus on pointwise approach and compare the performances of four computational methods in developing ranking models using several criteria such as accuracy, stability and robustness. The experimental results show that Multivariate Adaptive Regression Splines (MARS) and Artificial Neural Networks (ANN) are effective methods for learning to rank problem and provide promising results.	artificial neural network;computation;eisenstein's criterion;emoticon;f1 score;learning to rank;multivariate adaptive regression splines;neural network software;performance;precision and recall;ranking (information retrieval);receiver operating characteristic;robustness (computer science);robustness testing;supervised learning;test data	Inci Batmaz;Pinar Senkul;Gulsah Serdar	2017	2017 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2017.8258135	support vector machine;learning to rank;artificial intelligence;supervised learning;multivariate adaptive regression splines;artificial neural network;robustness (computer science);machine learning;computer science;ranking;hidden markov model	ML	-21.258748636762135	-63.079869288262756	22421
3bb33bebe871e66c9aaf3cc966474913eb9f5cdc	content-based filtering enhanced by human visual attention applied to clothing recommendation	databases;retail data processing clothing content based retrieval recommender systems;biological system modeling;semantics;frequency measurement;visualization;visual attention recommendation systems clothing similarity visual features;content based image recommendation systems content based filtering human visual attention clothing recommendation recommendation system rs netflix amazon textual attributes visual features clothes profile visual attention similarity;visual features;clothing similarity;clothing;visualization clothing databases semantics frequency measurement proposals biological system modeling;visual attention;proposals;recommendation systems	Recommendation systems (RS) are important applications to help consumers to find interesting products in large databases available on line. A wide range of RS can be easily found such as Netflix and Amazon. In this paper, we propose a novel content-based approach for clothing recommendation, termed CRESA, which combines textual attributes, visual features and human visual attention to compose the clothes profile. Traditionally, the RS uses textual and/or visual features to derive the similarity measure between two products. However some parts of the image may call much more attention of users than others. We believe that the characterization of this phenomenon can improve the quality of the recommendation systems, especially when applied to the clothing recommendation problem. With this purpose, in this work we propose weighting the similarity of visual attention between corresponding parts of products with the measures conventionally used in content-based image recommendation systems. The experimental results showed that our approach reached the best accuracy rates when compared to the baseline approaches.	baseline (configuration management);computation;computational model;database;experiment;eye tracking;partial template specialization;recommender system;similarity measure;tracking system	Ernani Viriato de Melo;Emilia Alves Nogueira;Denise Guliato	2015	2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI)	10.1109/ICTAI.2015.98	visualization;computer science;clothing;data mining;semantics;multimedia	DB	-16.740334711493055	-53.606079645702835	22487
6a85761345a366948cf9a30bffb808ffeaea67d5	towards a lexicon-grammar of polish: extraction of verbo-nominal collocations from corpora	feasibility study	In the paper we present a contribution to the SyntLex longterm-project aiming at a lexicon-grammar for Polish. A corpus-based method is presented for computer-assisted improvement or/and verification of verbo-nominal lexicongrammars (in application to Polish). Feasibility study.	collocation;lexicon;nominal type system;text corpus	Zygmunt Vetulani;Tomasz Obrêbski;Grazyna Vetulani	2007			natural language processing;artificial intelligence;multiword expression;computer science;grammar;lexicon	NLP	-29.41115611896933	-77.83808284081164	22534
1842453619aae0ee5be318d85f1b7c0fa5eeec87	predicting political preference of twitter users	preference change;political preference;election campaign;language model;party candidate;political party;twitter user;predefined preference;general election;prediction model;human annotators	We study the problem of predicting the political preference of users on the Twitter network, showing that the political preference of users can be predicted from their Twitter behavior towards political parties. We show this by building prediction models based on a variety of contextual and behavioral features, training the models by resorting to a distant supervision approach and considering party candidates to have a predefined preference towards their respective parties. A language model for each party is learned from the content of the tweets by the party candidates, and the preference of a user is assessed based on the alignment of user tweets with the language models of the parties. We evaluate our work in the context of two real elections: 2012 Albertan and 2013 Pakistani general elections. In both cases, we show that our model outperforms, in terms of the F-measure, sentiment and text classification approaches and is at par with the human annotators. We further use our model to analyze the preference changes over the course of the election campaign and report results that would be difficult to attain by human annotators.	collaborative filtering;document classification;emoticon;experiment;f1 score;hashtag;language model;mike lesser;sensor	Aibek Makazhanov;Davood Rafiei;Muhammad Waqar	2013	2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2013)	10.1007/s13278-014-0193-5	public relations;political science;advertising;social psychology	NLP	-20.397364982726696	-68.33436195138786	22548
7eb6d0ff7175395fbe37050d9d321efcfe04cf52	variable-length sequence language model for large vocabulary continuous dictation machine	n gramme;speech;langage;n gram;sequence model;modele de langage;reconnaissance de la parole;modele de sequence;speech recognition;parole;sequence;language model	In natural language, some sequences of words are very frequent. A classical language model, like n-gram, does not adequately take into account such sequences, because it underestimates their probabilities. A better approach consists in modeling word sequences as if they were individual dictionary elements. Sequences are considered as additional entries of the word lexicon, on which language models are computed. In this paper, we present two methods for automatically determining frequent phrases in unlabeled corpora of written sentences. These methods are based on information theoretic criteria which insure a high statistical consistency. Our models reach their local optimum since they minimize the perplexity. One procedure is based only on the n-gram language model to extract word sequences. The second one is based on a class n-gram model trained on 233 classes extracted from the eight grammatical classes of French. Experimental tests, in terms of perplexity and recognition rate, are carried out on a vocabulary of 20000 words and a corpus of 43 million words extracted from the “Le Monde” newspaper. Our models reduce perplexity by more than 20% compared with n-gram (n R3) and multigram models. In terms of recognition rate, our models outperform n-gram and multigram models.	dictionary;information theory;language model;lexicon;local optimum;n-gram;natural language;perplexity;text corpus;vocabulary	Imed Zitouni;Jean-François Mari;Kamel Smaïli;Jean Paul Haton	1999			natural language processing;cache language model;speech recognition;computer science;linguistics;language model	NLP	-24.618218750616375	-78.58554730694877	22586
3b4eab8b288405b0dab5ed1f07cd55a2f410bbd4	tree-to-tree neural networks for program translation		Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to employ deep neural networks toward tackling this problem. We observe that program translation is a modular procedure, in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder. We evaluate the program translation capability of our tree-to-tree model against several state-of-the-art approaches. Compared against other neural translation models, we observe that our approach is consistently better than the baselines with a margin of up to 15 points. Further, our approach can improve the previous state-of-the-art program translation approaches by a margin of 20 points on the translation of real-world projects.	artificial neural network;deep learning;ecosystem;encoder;legacy code;random neural network;rendering (computer graphics);vocabulary	Xinyun Chen;Chang Liu;Dawn Xiaodong Song	2018			artificial intelligence;computer science;machine learning;baseline (configuration management);artificial neural network;modular design;intuition;legacy code	ML	-17.877673681109236	-74.81020299961642	22619
19e19a109f59faca530a5fcd3377a0e4a0a1281a	distributional vectors encode referential attributes		Distributional methods have proven to excel at capturing fuzzy, graded aspects of meaning (Italy is more similar to Spain than to Germany). In contrast, it is difficult to extract the values of more specific attributes of word referents from distributional representations, attributes of the kind typically found in structured knowledge bases (Italy has 60 million inhabitants). In this paper, we pursue the hypothesis that distributional vectors also implicitly encode referential attributes. We show that a standard supervised regression model is in fact sufficient to retrieve such attributes to a reasonable degree of accuracy: When evaluated on the prediction of both categorical and numeric attributes of countries and cities, the model consistently reduces baseline error by 30%, and is not far from the upper bound. Further analysis suggests that our model is able to “objectify” distributional representations for entities, anchoring them more firmly in the external world in measurable ways.	baseline (configuration management);distributional semantics;encode;entity	Abhijeet Gupta;Gemma Boleda;Marco Baroni;Sebastian Padó	2015			data mining	NLP	-27.18129828190952	-70.68407030482773	22623
4def31ac7af95e44d88383a512cdc909c9b63da5	similarity analysis of cell movements in video microscopy	trajectory data visualization shape vectors embryo couplings;biology computing;pattern recognition i 5 3 clustering similarity measures;pattern recognition i 5 3 clustering 8212;computer graphics;interaction techniques;computer graphics i 3 6 methodology techniques interaction techniques;3d cell movement representation similarity analysis cell movements video microscopy complex organism shape structures feature structures coherent behavior 2d projections;trajectory;shape;vectors;data visualization;embryo;similarity measures;computer graphics biology computing;couplings;computer graphics i 3 6 methodology techniques trajectory;pattern recognition i 5 3 clustering similarity measures computer graphics i 3 6 methodology techniques interaction techniques	Modern 3D+T video microscopy techniques enable biologists to acquire data of living organisms with unprecedented resolution in time and space. These datasets contain a wealth of biologically relevant and quantifiable information, e.g. the movements of all individual cells in a complex organism. However, extraction, validation, and analysis of this information are both challenging and time-consuming. In this paper, we present a computational technique that classifies and validates similar patterns of cell movements and cell divisions in organisms that consist of up to thousands of cells. Our algorithm determines tracking paths of traced cells that exhibit similar features and shape structures. These similarity values are assigned to our cluster algorithm that clusters paths into groups of coherent behavior. The data can be interactively explored in 2D projections and a 3D cell movement representation. For the first time, this visualization allows biologists to exhaustively assess similarities and differences in division patterns and cell migration on the scale of an entire organism. For validation, we applied our method on a synthetic dataset and two real datasets including zebrafish periods from blastula stage to early epiboly and growing zebrafish tail. We show that our method succeeds in detecting similarities based on shape and cell-movement based features.	2d to 3d conversion;3d computer graphics;algorithm;cluster analysis;coherence (physics);computation;data acquisition;interactivity;sensor;synthetic intelligence;usability	Jens Fangerau;Burkhard Hockendorf;Joachim Wittbrodt;Heike Leitte	2012	2012 IEEE Symposium on Biological Data Visualization (BioVis)	10.1109/BioVis.2012.6378595	computer vision;computer science;bioinformatics;machine learning	Visualization	-5.263856718519068	-52.44575548226756	22637
5731ebdea6b7da2f38199d081beab6c680e5d5a1	collecting spontaneously spoken queries for information retrieval		Motivated to realize the speech-driven information retrieval systems that accept spontaneously spoken queries, we developed a method to collect such speech data derived from the pre-defined search topics that had been systematically constructed for IR research. In order to evaluate both our method and the performance of the document retrieval by using the spontaneously spoken queries, we took place two experiments of collecting the speech data by our method using publicly available test collections of evaluating document retrieval. The first preliminary experiment took place with relatively small number of search topics selected from the NTCIR-3Web retrieval collection, which had been constructed for the TREC-style evaluation workshop, in order to test our method. The second experiment took place with all of the search topics released from the NTCIR-4 Web task to participate the formal run of the evaluation. The information about the collected data and the result of the evaluation with respect to both the speech recognition accuracy and the precision of document retrieval by using the collected data are presented in this paper.	document retrieval;experiment;information retrieval;speech recognition;text retrieval conference	Tomoyosi Akiba;Atsushi Fujii;Katunobu Itou	2004			computer science;information retrieval;document retrieval;small number	Web+IR	-31.77901535319323	-63.381117067633554	22653
817e39028a56decc6f8f9fb558c55c9e88fd5669	embedding cardinality constraints in neural link predictors		Neural link predictors learn distributed representations of entities and relations in a knowledge graph. They are remarkably powerful in the link prediction and knowledge base completion tasks, mainly due to the learned representations that capture important statistical dependencies in the data. Recent works in the area have focused on either designing new scoring functions or incorporating extra information into the learning process to improve the representations. Yet the representations are mostly learned from the observed links between entities, ignoring commonsense or schema knowledge associated to the relations in the graph. A fundamental aspect of the topology of relational data is the cardinality information, which bounds the number of predictions given for a relation between a minimum and maximum frequency. In this paper, we propose a new regularisation approach to incorporate relation cardinality constraints to any existing neural link predictor without affecting their efficiency or scalability. Our regularisation term aims to impose boundaries on the number of predictions with high probability, thus, structuring the embeddings space to respect commonsense cardinality assumptions resulting in better representations. Experimental results on Freebase, WordNet and YAGO show that, given suitable prior knowledge, the proposed method positively impacts the predictive accuracy of downstream link prediction tasks.		Emir Muñoz;Pasquale Minervini;Matthias Nickles	2018	CoRR	10.1145/3297280.3297502	artificial intelligence;cardinality;computer science;machine learning;relational database;knowledge base;scalability;commonsense knowledge;embedding;schema (psychology);wordnet	ML	-16.218172316746553	-66.79894300274846	22655
568081925d924826bae14dc7043b4c36766d1df9	relda for micro-blog topic mining based on relationship structure	deformable models;data mining;clustering algorithms;probabilistic logic;twitter;data models	Micro-blog is a sort of open social network platform. The massive real-time messages that Twitter users publish through the micro-blog platform every day are called tweets, which are characterized with instantaneity and subjectivity and imply some topic information that contain the users' daily activities and hot news. Because of the short length and non-prominent topics of tweets, the traditional mining and clustering methods are not very effective. In order to solve the difficulty of topics mining, we try to find the relationship structure by the dependent of tweets, and propose reLDA (the abbreviation of relationship LDA) model based on the relationship structure. In particular, we build the tree structure for tweets, then deduce and improve the process of Gibbs Sampling by this model, at last we carry out the topic mining of tweets that captured by Twitter API, the result of experiment proves that our model is effective.	algorithm;application programming interface;blog;cluster analysis;experiment;gibbs sampling;opensocial;parallel computing;real-time web;social network;tree structure	Lijun Cai;Wenjian Tao;Lei Chen;Tingqin He	2016	2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)	10.1109/FSKD.2016.7603408	data modeling;computer science;data science;machine learning;data mining;probabilistic logic;cluster analysis;world wide web	DB	-23.4835323477134	-52.11410244534425	22657
469503eabc95439c86848ba2b2dbe03cf7ca4dce	combining punctuation and disfluency prediction: an empirical study		Punctuation prediction and disfluency prediction can improve downstream natural language processing tasks such as machine translation and information extraction. Combining the two tasks can potentially improve the efficiency of the overall pipeline system and reduce error propagation. In this work1, we compare various methods for combining punctuation prediction (PU) and disfluency prediction (DF) on the Switchboard corpus. We compare an isolated prediction approach with a cascade approach, a rescoring approach, and three joint model approaches. For the cascade approach, we show that the soft cascade method is better than the hard cascade method. We also use the cascade models to generate an n-best list, use the bi-directional cascade models to perform rescoring, and compare that with the results of the cascade models. For the joint model approach, we compare mixedlabel Linear-chain Conditional Random Field (LCRF), cross-product LCRF and 2layer Factorial Conditional Random Field (FCRF) with soft-cascade LCRF. Our results show that the various methods linking the two tasks are not significantly different from one another, although they perform better than the isolated prediction method by 0.5–1.5% in the F1 score. Moreover, the clique order of features also shows a marked difference.	baseline (configuration management);conditional random field;downstream (software development);f1 score;gene prediction;information extraction;machine translation;natural language processing;one-class classification;propagation of uncertainty;software propagation;telephone switchboard	Xuancong Wang;Khe Chai Sim;Hwee Tou Ng	2014			natural language processing;speech recognition;computer science;machine learning;pattern recognition	NLP	-21.779856205145663	-75.15459593242608	22720
6ebd199548dbb3fdf1cb4f5b1fc7321e78a58ba9	automatic rumors identification on sina weibo	training;monitoring;feature extraction;computer science;twitter;entertainment industry	In this paper, we study the problem of detecting rumors spreading in the social networks. Different from the most of the previous works on identifying rumors in Twitter, we select Sina Weibo, the China's major microblog system, as our target. We use two interfaces named “@Weibopiyao” and “Weibo Misinformation-Declaration” from Sina Weibo to help us construct high accuracy training dataset. We analyze data types of microblogs based on their content and the role and possible social impacts of different types of microblogs in rumors spreading. Leveraging our findings, we then focus on detecting social news rumors on Weibo. A new method is proposed to annotate the collected data from Weibo automatically, and three new features for identifying social news rumors are proposed. Experimental results illustrate the efficacy and efficiency of the methods and features proposed in this paper.	sensor;social network	Gang Liang;Jin Yang;Chun Xu	2016	2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)	10.1109/FSKD.2016.7603402	feature extraction;computer science;machine learning;internet privacy;world wide web	NLP	-20.656705282527103	-55.28785356955896	22725
1e63fb3cc60a2d07c1ee7fc206764eebe02353b4	visual attention and structural choice in sentence production across languages	c800 psychology;bf psychology;visual attention	To represent the complexity of a visually perceived event, viewers need to attend selectively to different aspects of the event and its associated entities. Spoken descriptions of such complex events must encode the corresponding perceptual properties. This review discusses how the speaker’s attentional focus on one of the referents in a given event influences the structural choice in languages with different degrees of word order flexibility. First, we will discuss whether English speakers prefer to map visually salient referents onto a prominent grammatical role (e.g., Subject) or to a prominent linear position in the sentence (e.g., the sentential starting point). Comparison of this evidence with research in free word-order languages (Russian and Finnish) suggests the existence of a mapping mechanism wherein perceptual salience predominantly affects grammatical-role assignment and, to a lesser extent, assignment of linear positions.		Andriy Myachykov;Dominic Thompson;Christoph Scheepers;Simon Garrod	2011	Language and Linguistics Compass	10.1111/j.1749-818X.2010.00265.x	psychology;natural language processing;linguistics;communication;social psychology	NLP	-8.988883688754003	-77.47551716222586	22859
07d17a4c588d2c875789fe1ae8334c9b198816a3	communicative signals promote abstract rule learning by 7-month-old infants	social and behavioral sciences	Infants’ ability to detect patterns in speech input is central to their acquisition of language, and recent evidence suggests that their cognitive faculties may be specifically tailored to this task: Seven-month-olds reliably abstract rule-like structures (e.g., ABB vs. ABA) from speech, but not other stimuli. Here we ask what drives this speech advantage. Specifically, we propose that infants’ learning from speech is driven by their representation of speech as a communicative signal. As evidence for this claim, we report an experiment in which 7-month-old infants (N=28) learned rules from a novel sound (sine-wave tones) introduced as a communicative signal, but failed to learn the same rules from tones presented in non-communicative contexts. These findings highlight the powerful influence of social-communicative contexts on infants’ learning.	aba problem	Brock Ferguson;Casey Lew-Williams	2014			psychology;developmental psychology;communication;social psychology	NLP	-7.62497371600238	-79.3910584071112	22873
cce70f333e80a2d5e2dce2b1346481beec381e93	caliph & emir: mpeg-7 photo annotation and retrieval	multimedia;metadata;image annotation;semantic description;content based image retrieval;mpeg 7;open source	Caliph & Emir are Java-based applications for image annotation and retrieval. They implement a large part of MPEG-7 descriptors and support annotation and retrieval based on the descriptors. Manual annotation is based on text and the MPEG-7 semantic description scheme. Automatic extraction of low level features and existing metadata is also supported. Retrieval features include: linear search, content based image retrieval, textual metadata and graph indexing, and two-dimensional repository visualization.	automatic image annotation;content-based image retrieval;java;linear search;mpeg-7	Mathias Lux	2009		10.1145/1631272.1631456	visual word;image retrieval;computer science;database;metadata;automatic image annotation;world wide web;information retrieval	Web+IR	-13.845908591955729	-57.377958185878704	22879
15fcecf899acf594ebb7b04ba2df49aa4adc4799	a word at a time: computing word relatedness using temporal semantic analysis	semantic similarity;temporal dynamics;time series;language resources;temporal information;semantic relatedness;temporal semantics;word relatedness;empirical evaluation;computational semantics;explicit semantic analysis;semantic analysis	"""Computing the degree of semantic relatedness of words is a key functionality of many language applications such as search, clustering, and disambiguation. Previous approaches to computing semantic relatedness mostly used static language resources, while essentially ignoring their temporal aspects. We believe that a considerable amount of relatedness information can also be found in studying patterns of word usage over time. Consider, for instance, a newspaper archive spanning many years. Two words such as """"war"""" and """"peace"""" might rarely co-occur in the same articles, yet their patterns of use over time might be similar. In this paper, we propose a new semantic relatedness model, Temporal Semantic Analysis (TSA), which captures this temporal information. The previous state of the art method, Explicit Semantic Analysis (ESA), represented word semantics as a vector of concepts. TSA uses a more refined representation, where each concept is no longer scalar, but is instead represented as time series over a corpus of temporally-ordered documents. To the best of our knowledge, this is the first attempt to incorporate temporal evidence into models of semantic relatedness. Empirical evaluation shows that TSA provides consistent improvements over the state of the art ESA results on multiple benchmarks."""	archive;benchmark (computing);cluster analysis;esa;explicit semantic analysis;file spanning;semantic similarity;text corpus;time series;word-sense disambiguation	Kira Radinsky;Eugene Agichtein;Evgeniy Gabrilovich;Shaul Markovitch	2011		10.1145/1963405.1963455	natural language processing;semantic similarity;semantic computing;semantic integration;explicit semantic analysis;semeval;semantic search;computer science;semantic compression;semantic property;computational semantics	Web+IR	-27.491896547651166	-65.81178222634976	22912
8cebd4ca9f57824e9a0b281b78448901b699466d	annotating the meaning of discourse connectives by looking at their translation: the translation-spotting technique		The various meanings of discourse connectives like while and however are difficult to identify and annotate, even for trained human annotators. This problem is all the more important since connectives are salient textual markers of cohesion and need to be correctly interpreted for many Natural Language Processing applications. In this paper, we suggest an alternative route to reach a reliable annotation of connectives, by making use of the information provided by their translation in large parallel corpora. This method thus replaces the difficult explicit reasoning involved in traditional sense annotation by an empirical clustering of the senses emerging from the translations. We argue that this method has the advantage of providing more reliable reference data than traditional sense	causal filter;cluster analysis;cognitive tutor;cohesion (computer science);compiler;experiment;logical connective;natural language processing;parallel text;text corpus;word-sense disambiguation	Bruno Cartoni;Sandrine Zufferey;Thomas Meyer	2013	D&D			NLP	-26.537218046783696	-72.98040263718883	22915
c5be5422855e1e6cef989d10c9b30f1d318d3457	from argumentation mining to stance classification		Argumentation mining and stance classification were recently introduced as interesting tasks in text mining. In this paper, a novel framework for argument tagging based on topic modeling is proposed. Unlike other machine learning approaches for argument tagging which often require large set of labeled data, the proposed model is minimally supervised and merely a one-to-one mapping between the pre-defined argument set and the extracted topics is required. These extracted arguments are subsequently exploited for stance classification. Additionally, a manuallyannotated corpus for stance classification and argument tagging of online news comments is introduced and made available. Experiments on our collected corpus demonstrate the benefits of using topic-modeling for argument tagging. We show that using Non-Negative Matrix Factorization instead of Latent Dirichlet Allocation achieves better results for argument classification, close to the results of a supervised classifier. Furthermore, the statistical model that leverages automatically-extracted arguments as features for stance classification shows promising results.	latent dirichlet allocation;machine learning;non-negative matrix factorization;off topic;one-to-one (data model);statistical model;supervised learning;text corpus;text mining;topic model	Parinaz Sobhani;Diana Inkpen;Stan Matwin	2015			natural language processing;speech recognition;computer science;artificial intelligence;machine learning;data mining	NLP	-19.947143014142906	-66.64410401820332	22937
2c37223eb7280047960239c47e16aad1dc42b3e9	casee: a hierarchical event representation for the analysis of videos	proposed work;proposed event representation;hierarchical case representation;novel representation;natural language representation;event detection;temporal event-tree;hierarchical event representation;multithreaded event;case representation;proposed representation	A representational gap exists between low-level measurements (segmentation, object classification, tracking) and high-level understanding of video sequences. In this paper, we propose a novel representation of events in videos to bridge this gap, based on the CASE representation of natural languages. The proposed representation has three significant contributions over existing frameworks. First, we recognize the importance of causal and temporal relationships between sub-events and extendCASE to allow the representation of temporal structure and causality between sub-events. Second, in order to capture both multi-agent and multi-threaded events, we introduce a hierarchical CASE representation of events in terms of sub-events and case-lists. Last, for purposes of implementation we present the concept of a temporal event-tree, and pose the problem of event detection as subtree pattern matching. By extending CASE, a natural language representation, for the representation of events, the proposed work allows a plausible means of interface between users and the computer. We show two important applications of the proposed event representation for the automated annotation of standard meeting video sequences, and for event detection in extended videos of railroad crossings. Introduction Human community and society are built upon the ability to share experiences of events. Hence, in the enterprize of machine vision, the ability to represent and share observed events must be one of the ultimate, if most abstract, goals. With computer vision techniques maturing sufficiently to provide reliable low-level descriptions of scenes, the necessity of developing semantically meaningful descriptions of these low-level descriptors is becoming increasingly pressing. In this work, one primary objective is to present a coherent representation of events, as a means to encode the relationships between agents and objects participating in an event. We also emphasize, in particular, a representation that allows computers to share observations with other computers and also with humans, in terms of events. An eventis defined as a collection of actions performed by one or more agents.Agentsare animates that can perform actions independently or dependently (e.g. people or robots). The practical need for formal representation of events is best illustrated Copyright c © 2004, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. through possible applications. These applications include: (1) Surveillance: By definition, surveillance applications require the detection of peculiar events. Event representations can be used for prior definition of what constitutes an interesting event in any given domain, allowing automation of area surveillance, (2) Video Indexing and Event Browsing: Given a query for a certain event (defined in terms of an event representation), similar instances can be retrieved from a database of annotated clips, (3) Annotation: In the spirit of MPEG-7, video sequences may be annotated autonomously based on their content, (4) Domain Understanding: It is noted that causality is an abstract that cannot be directly inferred from a single video sequence. Through the use of event representations, causality can be inferred between events in a single domain (e.g. surveillance of airports) across several extended video sequences for domain understanding. In literature, a variety of approaches have been proposed for the detection of events in video sequences. Most of these approaches can be arranged into two categories based on the semantic significance of their representations. This distinction is important, since it determines whether humans can exploit the representation for communication. Approaches where representations do not take on semantic meaning include Causal events (Brand 1997), Force dynamics (Siskind 2000), Stochastic Context Free Grammars (Bobick and Ivanov 1998), Spatio-temporal Derivatives (ZelnikManor and Irani 2001), and geometric properties and appearance (Malliot, Thonnat, and Boucher 2003). While they differ in approaches, the representations they employ do not lend themselves directly to interpretation or interface to humans. Learning methods such as Bayesian Networks and Hidden Markov Models (Ivanov and Bobick 2000) have been widely used in the area of activity recognition. A known drawback of learning methods is that they usually require large training sets of events and variation in data may require complete re-training. Similarly, there is no straightforward method of expanding the domain, once training has been completed. On the other hand, semantically significant approaches like the state machines (Koller, Heinze, and Nagel 1991), and PNF Networks (Pinhanez and Bobick 1998) provide varying degrees of representation to the actions and agents involved in the events. What is missing in these representations is coherence in describing low-level measurements as ‘events’. Can these representations be used to share knowledge between two KNOWLEDGE REPRESENTATION & REASONING 263 systems? Can events be compared on the basis of these representations? How are these representations related to human understanding of events? Can a human communicate his or her observation of an event to a computer or vice versa? By extending automatic generation of a natural language ontology to event representation of a video, a plausible interface between the human and the computer is facilitated. One such natural language representation called CASE was proposed by Fillmore (Fillmore 1968) for language understanding. The basic unit of this representation is a caseframe that has several elementary cases, such as an agentive, an instrumental, and a predicate. Using these caseframes Fillmore analyzed languages, treating all languages generically. However, CASE was primarily used for syntactic analysis of natural languages, and while it provides a promising foundation for event representation it has several limitations for that end. Firstly, since events are typically made up of a hierarchy of sub-events it is impossible to describe them as a succession of case-frames. Second, these sub-events often have temporal and causal relationships between them, andCASE provides no mechanisms to represent these relationships. Furthermore, there might be simultaneous dependent or independent sub-events with multiple agentives, and change of location and instrumentals during events. CASE was first investigated for event representation (Neumann 1989), but the author did not investigate the temporal structure of events as the author was not concerned with eventdetection. More recently (Kojima, Tamura, and Fukunaga 2001) addressed some shortcomings in CASE for single person event detection with, SO(source prefixed to case),GO(goal prefixed to case) and SUB (child frame describing a sub-event).SOand GOare prefixed to theLOC (locative) case mostly describing the source and destination locations of the agent in the event. A concept hierarchy of action rules (case-frames) was used to determine an action grammar (ontology) for the sequence of events. Also, using case-frames based on events, they reconstructed the event sequence in the form of sentences. Their method worked well for single person action analysis using the CASE representation. However, this work did not address important issues of temporal and causal relationships. Moreover, no mechanisms were proposed for multiple-agents or multithreaded events. We propose three critical extensions to CASE for the representation of events: (1) accommodating multiple agents and multi-threaded event, (2) supporting the inclusion of temporal information ortemporal logicinto the representation, and (3) supporting the inclusion of causalrelationships between events as well. We also propose a novel event-tree representation, based on temporal relationships, for the detection of events in video sequences. Hence, unlike almost all previous work, we use both temporal structure and an environment descriptor simultaneously to represent an event. The Extended CASE framework: CASE In this section, the three extensions to the CASE framework are presented. Firstly, in order to capture both multi-agent and multi-thread events, we introduce a hierarchical CASE representation of events in terms of sub-events and caselists. Secondly, since the temporal structure of events is critical to understanding and hence representing events, we introduce temporal logic into the CASE representation based on the interval algebra in (Allen and Ferguson 1994). Lastly, we recognize the importance of causal relationships between sub events and extend CASE to allow the representation of such causality between sub-events. Multi-Agent, Multi-Thread Representation Except in constrained domains, events typically involve multiple agents engaged in several dependent or independent actions. Thus any representation of events must be able to capture the composite nature of real events. To represent multiple objects, we introduce the idea of case-lists of elements for a particular case. For example, if there are more than one agents involved in an event we add both in a caselist within AG, [ PRED: move,AG:{ person1, person2 }, ...] As in (Kojima, Tamura, and Fukunaga 2001), we use SUB to represent a sub-event that occurs during an event. However, this representation offers no means to represent veralsubevents or multiple threads. To represent multiple threads we add them to a list of sub-events in the SUB case. An example is shown below, “While Jack stole from the cashier, Bonnie robbed from the bank as Clyde was talking to the cashier” [ PRED: steal,AG: Jack,D: cashier,SUB:{ [ PRED: rob,AG: Bonnie,OBJ: bank ], [ PRED: talk, AG: { Clyde, cashier } ] }	activity recognition;allen's interval algebra;artificial intelligence;bayesian network;browsing;causality;coherence (physics);computer vision;computer-aided software engineering;encode;event tree;hidden markov model;high- and low-level;in re boucher;knowledge representation and reasoning;mpeg-7;machine vision;markov chain;multi-agent system;multithreading (computer architecture);natural language understanding;parsing;pattern matching;predicate (mathematical logic);prenex normal form;re-order buffer;robot;sid meier's railroad tycoon;succession;temporal logic;thread (computing);tree (data structure)	Asaad Hakeem;Yaser Sheikh;Mubarak Shah	2004				AI	-9.307279558947982	-65.35721679763797	22961
842b67d49765eba913a7a9c7bb1cb3b923adc8a6	mining soft-matching rules from textual data	instance based learning;word frequency;rule induction;text mining;rule based;document retrieval;matching method	Text miningconcerns the discovery of knowledge from unstructured textual data. One important task is the discovery of rules that relate specific words and phrases. Although existing methods for this task learn traditional logical rules, soft-matching methods that utilize word-frequency information generally work better for textual data. This paper presents a rule induction system, T EXTRISE, that allows for partial matching of text-valued features by combining rule-based and instance-based learning. We present initial experiments applying T EXTRISE to corpora of book descriptions and patent documents retrieved from the web and compare its results to those of traditional rule and instance based methods.	data mining;database;experiment;information extraction;instance-based learning;logic programming;rule induction;spatial variability;substring;text corpus;text mining	Un Yong Nahm;Raymond J. Mooney	2001			document retrieval;text mining;association rule learning;computer science;artificial intelligence;machine learning;pattern recognition;data mining;word lists by frequency;linguistics;information retrieval	AI	-27.284788084661862	-66.48711101838565	22984
42b2ec4c3884d514a982529633804e843c9b5a23	enhancing of chemical compound and drug name recognition using representative tag scheme and fine-grained tokenization	biological patents;biomedical journals;text mining;europe pubmed central;citation search;computer applications in chemistry;citation networks;theoretical and computational chemistry;computational biology bioinformatics;research articles;abstracts;open access;life sciences;clinical guidelines;full text;rest apis;orcids;europe pmc;documentation and information in chemistry;biomedical research;bioinformatics;literature search	"""BACKGROUND The functions of chemical compounds and drugs that affect biological processes and their particular effect on the onset and treatment of diseases have attracted increasing interest with the advancement of research in the life sciences. To extract knowledge from the extensive literatures on such compounds and drugs, the organizers of BioCreative IV administered the CHEMical Compound and Drug Named Entity Recognition (CHEMDNER) task to establish a standard dataset for evaluating state-of-the-art chemical entity recognition methods.   METHODS This study introduces the approach of our CHEMDNER system. Instead of emphasizing the development of novel feature sets for machine learning, this study investigates the effect of various tag schemes on the recognition of the names of chemicals and drugs by using conditional random fields. Experiments were conducted using combinations of different tokenization strategies and tag schemes to investigate the effects of tag set selection and tokenization method on the CHEMDNER task.   RESULTS This study presents the performance of CHEMDNER of three more representative tag schemes-IOBE, IOBES, and IOB12E-when applied to a widely utilized IOB tag set and combined with the coarse-/fine-grained tokenization methods. The experimental results thus reveal that the fine-grained tokenization strategy performance best in terms of precision, recall and F-scores when the IOBES tag set was utilized. The IOBES model with fine-grained tokenization yielded the best-F-scores in the six chemical entity categories other than the """"Multiple"""" entity category. Nonetheless, no significant improvement was observed when a more representative tag schemes was used with the coarse or fine-grained tokenization rules. The best F-scores that were achieved using the developed system on the test dataset of the CHEMDNER task were 0.833 and 0.815 for the chemical documents indexing and the chemical entity mention recognition tasks, respectively.   CONCLUSIONS The results herein highlight the importance of tag set selection and the use of different tokenization strategies. Fine-grained tokenization combined with the tag set IOBES most effectively recognizes chemical and drug names. To the best of the authors' knowledge, this investigation is the first comprehensive investigation use of various tag set schemes combined with different tokenization strategies for the recognition of chemical entities."""	biocreative;biological science disciplines;categories;chemicals;conditional random field;disease;entity;experiment;indexes;inside outside beginning;literature;machine learning;name;named-entity recognition;onset (audio);rule (guideline);silo (dataset);substance abuse detection;tokenization (data security);tracer	Hong-Jie Dai;Po-Ting Lai;Yung-Chun Chang;Richard Tzong-Han Tsai	2015		10.1186/1758-2946-7-S1-S14	text mining;medical research;medicine;computer science;bioinformatics;data science;data mining;information retrieval	NLP	-23.596974871501832	-67.90049453004136	23015
fd458e7109e8ebac2f59d399981054957078c7a4	encoding temporal information for time-aware link prediction		Most existing knowledge base (KB) embedding methods solely learn from time-unknown fact triples but neglect the temporal information in the knowledge base. In this paper, we propose a novel time-aware KB embedding approach taking advantage of the happening time of facts. Specifically, we use temporal order constraints to model transformation between time-sensitive relations and enforce the embeddings to be temporally consistent and more accurate. We empirically evaluate our approach in two tasks of link prediction and triple classification. Experimental results show that our method outperforms other baselines on the two tasks consistently.	baseline (configuration management);entity;knowledge base;model transformation;valid time	Tingsong Jiang;Tianyu Liu;Tao Ge;Lei Sha;Sujian Li;Baobao Chang;Zhifang Sui	2016			artificial intelligence;natural language processing;computer science;machine learning;encoding (memory)	AI	-16.524091164088876	-66.65362489889644	23055
4f1470f58ef143db98e2493faf43b564cb1c1658	retrieval from context trees				Ben Wegbreit	1975	Inf. Process. Lett.	10.1016/0020-0190(75)90046-0	frame problem;theoretical computer science;natural language processing;artificial intelligence;computer science;data structure	DB	-31.181979995947067	-78.01456668704365	23074
59d48c7d876e15f3950126b93f79540351de50db	the web information extraction for update summarization based on shallow parsing	information extraction;information retrieval;text analysis;data mining;english texts web information extraction update summarization shallow parsing text information extraction static documents information update;data mining feature extraction syntactics tagging green products real time systems natural language processing;text analysis information retrieval internet;internet;information extraction shallow parsing web texts updated information;feature extraction;updated information;natural language processing;web texts;shallow parsing;web information extraction;real time systems	Traditional text information extraction methods mainly act on static documents and are difficult to reflect the dynamic evolvement of information update on the web. To address this challenge, this work proposes a new method based on shallow parsing with rules. The rules are generated according to the syntactic features of English texts, such as the tense of verbs, the usages of modal verbs and so on. The latest novel information in English news texts is extracted correctly, to meet the needs of users for accessing to updated information of the developing events quickly and effectively. Performance results show the improvement of the proposed scheme in this work.	information extraction;machine learning;modal logic;part-of-speech tagging;shallow parsing;world wide web	Min Peng;Xiaoxiao Ma;Ye Tian;Ming Yang;Hua Long;Quanchen Lin;Xiaojun Xia	2011	2011 International Conference on P2P, Parallel, Grid, Cloud and Internet Computing	10.1109/3PGCIC.2011.26	natural language processing;computer science;data mining;information retrieval	NLP	-25.516567029117947	-64.4398768979301	23232
41c65ea16a6139ec32ead0fb05a0b299ce8d2f0e	tighter integration of rule-based and statistical mt in serial system combination	chinese-to-english mt task;statistical mt;target language output;european language pair;target language;serial system combination;smt system;input language;previous work;1-best translation;tighter integration;rule-based mt;mt quality;rule based	L'accès à ce site Web et l'utilisation de son contenu sont assujettis aux conditions présentées dans le site LISEZ CES CONDITIONS ATTENTIVEMENT AVANT D'UTILISER CE SITE WEB. Access and use of this website and the material on it are subject to the Terms and Conditions set forth at	world wide web	Nicola Ueffing;Jens Stephan;Evgeny Matusov;Loïc Dugast;George F. Foster;Roland Kuhn;Jean Senellart;Jin Yang	2008			rule-based system;natural language processing;speech recognition;transfer-based machine translation;computer science;rule-based machine translation	ML	-29.203948041537117	-77.96210117537856	23243
e0455e926db006661af7c8403fd745055b1d52eb	topic adaptation for the automatic translation of news articles (adaptation thématique pour la traduction automatique de dépêches de presse) [in french]		Résumé. L’utilisation de méthodes statistiques en traduction automatique (TA) implique l’exploitation de gros corpus parallèles représentatifs de la tâche de traduction visée. La relative rareté de ces ressources fait que la question de l’adaptation au domaine est une problématique centrale en TA. Dans cet article, une étude portant sur l’adaptation thématique des données journalistiques issues d’une même source est proposée. Dans notre approche, chaque phrase d’un document est traduite avec le système de traduction approprié (c.-à-d. spécifique au thème dominant dans la phrase). Deux scénarios de traduction sont étudiés : (a) une classification manuelle, reposant sur la codification IPTC ; (b) une classification automatique. Nos expériences montrent que le scénario (b) conduit à des meilleures performances (à l’aune des métriques automatiques), que le scénario (a). L’approche la meilleure pour la métrique BLEU semble toutefois consister à ne pas réaliser d’adaptation ; on observe toutefois qu’adapter permet de lever certaines ambiguïtés sémantiques. Abstract. Statistical approaches used in machine translation (MT) require the availability of large parallel corpora for the task at hand. The relative scarcity of thes resources makes domain adaptation a central issue in MT. In this paper, a study of thematic adaptation for News texts is presented. All data are produced by the same source : News articles. In our approach, each sentence is translated with the appropriate translation system (specific to the dominant theme for the sentence). Two machine translation scenarios are considered : (a) a manual classification, based on IPTC codification ; (b) an automatic classification. Our experiments show that scenario (b) leads to better performance (in terms of automatic metrics) than scenario (a) . The best approach for the BLEU metric however seems to dispense with adaptation alltogether. Nonetheless, we observe that domain adaptation sometimes resolves some semantic ambiguities .	automatic summarization;bleu;council for educational technology;domain adaptation;emoticon;experiment;linear algebra;machine translation;meme;parallel text;performance;programmation automatique des formules;text corpus	Souhir Gahbiche-Braham;Hélène Bonneau-Maynard;François Yvon	2014				NLP	-27.33760628945191	-76.64607811036954	23250
664a94d644f65816209b7eaa74c1765b2ecd6eda	learning linear precedence rules	linear precedence grammar;linear precedence rule;learning linear precedence rule;appropriate natural language expression;immediate dominance;particular immediate dominance grammar;output linear precedence rule;natural language	"""A system is descril)ed which learns fl'om examples the Linear Precedence rules in an Immedia te Dominance/Linear Precedence grammar . Given a particular hnmediate Dominance g rammar and hierarchies of feature values potentially rel evant for linearization (=the systelu's bias), the leanler generates appropriate naturM language expressions to be ewduated as positive or negative by a teacher, and produces as output IAnear Precedence rules which can be directly used 1)y the gralnmar. 1 I n t r o d u c t i o n The lnanual cotnpilation of a. sizable g rammar is a difficult and t ime-consuming task. An important subtask is the construction of word ordering rules in the grammar . '[ 'hough some languages are proclaimed as having simple ordering rules, e.g. either complete scrambling or strictly """"fixed"""" order, most languages exhibit quite complex regularities (Steele, 198l), and even the rigid word order languages (like 1,;nglish) and those with to tal scrambling (like Warlpiri; cf. (H ale, 1983) may show intricate rules (Kashket, 11981); hence the need for their automat ic acquisition. 'Fhis I;ask however, to the best of our knowledge, has not heen previously addressed. This paper describes a prograln which, given a g rammar with no ordering relations, l)roduces as outpu_t a set of linearization, or Linear Precedence, rules which can be directly employed by that grammar. The learning step uses the version space algorithm, a familiar techlfique from ma.chine learning for learning from examples. In contrast to most previous uses of the algorithnl for various learning tasks, which rely on priorly given classified examples, our learner generates itself tile training instances olte at a tinie, and they are then classed as positive or negative by a teacher. A selective generation of training instances is employe, d which facilitates the learning by minimizing the nu,nl)er of evaluations that the teacher"""	algorithm;compiler-compiler;computation;computational linguistics;hough transform;id/lp grammar;microsoft word for mac;signal-to-noise ratio;version space learning	Vladimir Pericliev	1996			natural language processing;id/lp grammar;operator-precedence grammar;computer science;machine learning;linguistics;natural language	NLP	-10.497388130761294	-75.2579685410466	23273
b5f45116ca393bc29eefc86192dc40b536d350f7	comparison of multi-episode video summarisation algorithms	multimedia multi episode video automatic video summaries simulated user principle;video summaries;video coding;automated evaluation;eurecom ecole d ingenieur telecommunication centre de recherche graduate school research center communication systems;multimedia communication;video coding multimedia communication;humans multimedia communication data mining event detection motion pictures position measurement singular value decomposition automatic control robustness predictive models;multimedia content analysis;image similarity	This paper presents a comparison of some methodologies for the automatic construction of video summaries. The work is based on the Simulated User Principle to evaluate the quality of a video summary in a way, which is automatic, yet related to user's perception. The method is studied for the case of multi-episode video. Where we don’t only describe what is important in a video, but rather what distinguishes this video from the others. Experimental results are presented to support the proposed ideas.	algorithm	Itheri Yahiaoui;Bernard Mérialdo;Benoit Huet	2001		10.1109/MMSP.2001.962776	video compression picture types;subjective video quality;computer vision;h.263;computer science;video quality;theoretical computer science;video tracking;multimedia;video processing;smacker video;pevq	Vision	-13.629265086151518	-53.82691021167635	23306
3aac2ed5a81c2e541de5ecb10bc41cc2e8d00e56	towards abstraction from extraction: multiple timescale gated recurrent unit for summarization		In this work, we introduce temporal hierarchies to the sequence to sequence (seq2seq) model to tackle the problem of abstractive summarization of scientific articles. The proposed Multiple Timescale model of the Gated Recurrent Unit (MTGRU) is implemented in the encoderdecoder setting to better deal with the presence of multiple compositionalities in larger texts. The proposed model is compared to the conventional RNN encoderdecoder, and the results demonstrate that our model trains faster and shows significant performance gains. The results also show that the temporal hierarchies help improve the ability of seq2seq models to capture compositionalities better without the presence of highly complex architectural hierarchies.	automatic summarization;end-to-end principle;random neural network;requirement;scientific literature	Minsoo Kim;Moirangthem Dennis Singh;Minho Lee	2016		10.18653/v1/W16-1608	real-time computing;simulation;computer science;artificial intelligence;machine learning	AI	-16.966363302669603	-73.25498777084708	23331
5f9a58625c06505ec027a824fd5bb1e0195e4c2b	monolingual and multilingual question answering on european legislation	question answering	This paper documents the participation of the Research Institute for Artificial Intelligence to the CLEF 2010 ResPubliQA lab. We answered questions in Romanian and English from Romanian documents of Acquis Communautaire and the European Parliament Proceedings. We extend the report from the previous ResPubliQA participation by introducing multi-factored paragraph relevance score training onto English-Romanian QA. We also investigate how our monolingual parametric QA system developed for the last year’s ResPubliQA track scales up to current challenges.	artificial intelligence;estimation theory;multi-environment real-time;question answering;relevance;scalability;software quality assurance;test set	Radu Ion;Alexandru Ceausu;Dan Stefanescu;Dan Tufis;Elena Irimia;Verginica Barbu Mititelu	2010			artificial intelligence;paragraph;parliament;information retrieval;natural language processing;data mining;computer science;question answering;romanian;legislation	AI	-31.48567497315553	-72.34957451613751	23363
dce7951b9a05a7a59ac9627354dd7f11caa47c6b	unsupervised word categorization using self-organizing maps and automatically extracted morphs	automatic;finnois;analyse amas;categorisation;linguistique;young children;finlandes;resource management;finnish;semantics;hombre;intelligence artificielle;automatico;segmentation;semantica;semantique;classification;enfant;gestion recursos;categorizacion;analyse syntaxique;linguistica;matrice creuse;cluster analysis;analisis morfologico;nino;analisis sintaxico;syntactic analysis;morphological analysis;human;child;automatique;autoorganizacion;representacion parsimoniosa;gestion ressources;analyse morphologique;artificial intelligence;self organization;self organized map;analisis cluster;inteligencia artificial;sparse matrix;sparse representation;clasificacion;segmentacion;autoorganisation;matriz dispersa;categorization;homme;representation parcimonieuse;linguistics	Automatic creation of syntactic and semantic word categorizations is a challenging problem for highly inflecting languages due to excessive data sparsity. Moreover, the study of colloquial language resources requires the utilization of fully corpus-based tools. We present a completely automated approach for producing word categorizations for morphologically rich languages. Self-Organizing Map (SOM) is utilized for clustering words based on the morphological properties of the context words. These properties are extracted using an automated morphological segmentation algorithm called Morfessor. Our experiments on a colloquial Finnish corpus of stories told by young children show that utilizing unsupervised morphs as features leads to clearly improved clusterings when compared to the use of whole context words as features.	algorithm;categorization;cluster analysis;experiment;microsoft word for mac;organizing (structure);self-organizing map;sparse matrix;text corpus;unsupervised learning	Mikaela Klami;Krista Lagus	2006		10.1007/11875581_109	natural language processing;self-organization;el niño;sparse matrix;biological classification;morphological analysis;computer science;artificial intelligence;resource management;machine learning;parsing;sparse approximation;semantics;cluster analysis;automatic transmission;segmentation;categorization	NLP	-26.788500152947936	-78.16169016861915	23397
fe1077f6b79e14457db77d7477a477f40f87e7e6	dynamic neural turing machine with continuous and discrete addressing schemes		We extend the neural Turing machine (NTM) model into a dynamic neural Turing machine (D-NTM) by introducing trainable address vectors. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies, including both linear and nonlinear ones. We implement the D-NTM with both continuous and discrete read and write mechanisms. We investigate the mechanisms and effects of learning to read and write into a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU controller. We provide extensive analysis of our model and compare different variations of neural Turing machines on this task. We show that our model outperforms long short-term memory and NTM variants. We provide further experimental results on the sequential MNIST, Stanford Natural Language Inference, associative recall, and copy tasks.	addressing scheme;controllers;experience;experiment;feedforward neural network;inference;location-based service;long short-term memory;mnist database;machine translation;memory cell (binary);ntm-it gene;natural language;non-deterministic turing machine;nonlinear system;random neural network	Çaglar Gülçehre;A. P. Sarath Chandar;Kyunghyun Cho;Yoshua Bengio	2018	Neural Computation	10.1162/neco_a_01060	artificial intelligence;machine learning;memory address;deep learning;feed forward;theoretical computer science;turing machine;reinforcement learning;recall;mnist database;computer science;memory cell	ML	-16.58396162536388	-74.9169896190988	23415
5938ef3bf0f5a9bfd246356d6bbf9d9a59b32ae0	uncertainty and term selection in text categorization	classification algorithm;uncertainty;information retrieval;term selection;term frequency;text classification;stiff terms;noisy terms;text categorization	"""This paper discusses the notion of Uncertainty, which has a prominent place in the theory and experimental practice of modern Physics. It argues that the awareness of Uncertainty may also be of tremendous importance to the field of Information Retrieval, and in particular Text Categorization.As an application of Uncertainty in Text Categorization, a new criterion for Term Selection is described, which is based on the Uncertainty in Term Frequency across categories. This criterion allows to distinguish between low-quality (or """"noisy"""") and high-quality (""""stiff"""") terms.We describe an experiment investigating the effect of eliminating noisy and stiff terms in the context of text classification. In the experiment we applied the Rocchio and Winnow classification algorithms to a collection of newspaper items, a mono-classified subset of the well-known Reuters 21578 corpus.This investigation shows that both the local elimination of noisy terms and the global elimination of stiff terms can be used for Term Selection in Text Categorization."""	categorization;document classification	Charles M. E. E. Peters;Cornelis H. A. Koster	2003	International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems	10.1142/S0218488503001977	uncertainty;computer science;machine learning;pattern recognition;tf–idf;information retrieval	Robotics	-25.044686441826904	-62.716315868164386	23423
133c0508f3b81c33438646838450a8c4d66bef35	innovative use of ontologies: association of physiological pathways to corresponding computational models and disparate data types			computation;computational model;ontology (information science)	Nazanin Tabesh-Saleki;Brian E. Carlson;Christopher T. Thompson;Maxwell Lewis Neal;Daniel L. Cook;Mary Shimoyama	2013			disparate system;data science;ontology (information science);computational model;data mining;computer science	NLP	-4.847648290578492	-63.350313760651154	23428
69e353fc39d5e76907fabe2a57e584c45f6c0978	on the complexity of id/lp parsing			id/lp grammar;parsing	G. Edward Barton	1985	Computational Linguistics			NLP	-30.025451140712327	-78.61379442881156	23462
b49e048b0fe8896337a201bccdc0a2cdee1c1816	italian and spanish null subjects. a case study evaluation in an mt perspective		Thanks to their rich morphology, Italian and Spanish allow pro-drop pronouns, i.e., non lexically-realized subject pronouns. Here we distinguish between two different types of null subjects: personal pro-drop and impersonal pro-drop. We evaluate the translation of these two categories into French, a non pro-drop language, using Its-2, a transfer-based system developed at our laboratory; and Moses, a statistical system. Three different corpora are used: two subsets of the Europarl corpus and a third corpus built using newspaper articles. Null subjects turn out to be quantitatively important in all three corpora, but their distribution varies depending on the language and the text genre though. From a MT perspective, translation results are determined by the type of pro-drop and the pair of languages involved. Impersonal pro-drop is harder to translate than personal pro-drop, especially for the translation from Italian into French, and a significant portion of incorrect translations consists of missing pronouns.	europarl corpus;galaxy morphological classification;internal transcribed spacer;moses;text corpus	Lorenza Russo;Sharid Loáiciga;Asheesh Gulati	2012			natural language processing;morphology (linguistics);subject pronoun;artificial intelligence;newspaper;computer science	NLP	-27.57732009150655	-75.60199025671035	23476
ac143ce7c39896682e267574fa312df433147f00	determining reliability of subjective and multi-label emotion annotation through novel fuzzy agreement measure		The paper presents a new fuzzy agreement measure γf for determining the agreement in multi-label and subjective annotation task. In this annotation framework, one data item may belong to a category or a class with a belief value denoting the degree of confidence of an annotator in assigning the data item to that category. We have provided a notion of disagreement based on the belief values provided by the annotators with respect to a category. The fuzzy agreement measure γf has been proposed by defining different fuzzy agreement sets based on the distribution of difference of belief values provided by the annotators. The fuzzy agreement has been computed by studying the average agreement over all the data items and annotators. Finally, we elaborate on the computation γf measure with a case study on emotion text data where a data item (sentence) may belong to more than one emotion category with varying belief values.	computation;data item;multi-label classification;text corpus	Plaban Kumar Bhowmick;Anupam Basu;Pabitra Mitra	2010			fuzzy logic;natural language processing;data mining;computation;computer science;artificial intelligence;annotation;pattern recognition;sentence	NLP	-26.39060511126946	-68.6619940477538	23477
57261b35db5089bed4925662e2e10377b4a2b0eb	detecting malapropisms using measures of contextual fitness		While detecting simple language errors (e.g. misspellings, number agreement, etc.) is nowadays standard functionality in all but the simplest text-editors, other more complicated language errors might go unnoticed. A difficult case are errors that come in the disguise of a valid word that fits syntactically into the sentence. We use the Wikipedia revision history to extract a dataset with such errors in their context. We show that the new dataset provides a more realistic picture of the performance of contextual fitness measures. The achieved error detection quality is generally sufficient for competent language users who are willing to accept a certain level of false alarms, but might be problematic for non-native writers who accept all suggestions made by the systems. We make the full experimental framework publicly available which will allow other scientists to reproduce our experiments and to conduct follow-up experiments. RÉSUMÉ. Alors que la détection d’erreurs simples est aujourd’hui une fonctionnalité standard des traitements de texte un peu évolués, de nombreuses erreurs restent difficiles à repérér. C’est souvent le cas lorsque la forme correcte est remplacée par une autre forme valide et syntaxiquement plausible en contexte. Nous avons utilisé les révisions de Wikipédia pour extraire automatiquement une listes d’erreurs de ce type. Ces données permettent de se faire une meilleure idée de l’utilité réelle des indicateurs standard de conformité contextuelle, qu’ils soient linguistiques ou statistiques. Les taux de détection obtenus sont généralement suffisants pour des scripteurs compétents qui seraient prêts à accepter un certain niveau de fausses alarmes ; ils restent problématiques pour des scripteurs nonnatifs. L’ensemble du dispositif expérimental utilisé pour ce travail sera rendu public, ce qui permettra à d’autres chercheurs de reproduire nos expériences et d’approfondir nos résultats.	error detection and correction;experiment;fits;fitness function;les trophées du libre;linear algebra;sensor;wikipedia	Torsten Zesch	2012	TAL		art;artificial intelligence;algorithm;cartography	ML	-26.921469858220046	-78.4047636618244	23591
5c4572806a1aefb90a77dd54d247883d3ace6e11	frigram: a french interaction grammar	model theoretic syntax;formal grammar;interaction grammar;polarity	We present Frigram, a French grammar with a large coverage, written in the formalism of Interaction Grammars. The originality of the formalism lies in its system of polarities, which expresses the resource sensitivity of natural languages and which is used to guide syntactic composition. We focus the presentation on the principles of the grammar, its modular architecture, the link with a lexicon independent of the formalism and the companion property, which helps to guarantee the consistency of the whole grammar.		Guy Perrier;Bruno Guillaume	2015	J. Language Modelling	10.15398/jlm.v3i1.93	grammar systems theory;natural language processing;id/lp grammar;generative grammar;synchronous context-free grammar;link grammar;operator-precedence grammar;regular grammar;computer science;affix grammar;regular tree grammar;phrase structure rules;extended affix grammar;emergent grammar;linguistics;relational grammar;attribute grammar;communication;adaptive grammar;mildly context-sensitive grammar formalism;combinatory categorial grammar;lexical grammar;head-driven phrase structure grammar	NLP	-29.887608529818024	-79.87155114842335	23604
a3337144563179ead9c5d041711f3ab4618cc907	content-based retrieval using heuristic search	optimal solution;content based indexing;search method;image indexing;heuristic search;mmir;image indexing retrieval;video database;content based retrieval;object relational;heuristic algorithm;content based indexing retrieval	The fast growth of multimedia information in image and video databases has triggered research on efficient retrieval methods. This paper deals with structural queries, a type of content-based retrieval where similarity is not defined on visual properties such as color and texture, but on object relations in space. We pr opose the application of heuristic algorithms which provide good, but not necessarily optimal, solutions in a pre-determined time period, and compare our approach with systematic search methods which are guaranteed to find optimal solutions but require exponential time in the worst case. The quality of the output is calculated using a relation framework which is an extension of Allen’s relations. With this framework our methods can be applied in multiple resolutions and dimensions, thus covering a wide range of applications in spatial, multimedia and video systems.	algorithm;best, worst and average case;database;heuristic;time complexity	Dimitris Papadias;Marios Mantzourogiannis;Panos Kalnis;Nikos Mamoulis;Ishfaq Ahmad	1999		10.1145/312624.312673	heuristic;visual word;heuristic;computer science;data mining;database;information retrieval	Web+IR	-11.32437104278967	-57.76123848665682	23612
0a5b93e8b8dd4de29620b2d7e7db1a7d1e6e2fc1	sinai at imageclefmed 2008	visual search;indexation;query expansion;image retrieval	This paper describes the SINAI team participation in the ImageCLEF campaign. In this paper we only explain the experiments accomplished in the medical task. We have experimented with query expansion and the text information of the collection. For expansion, we carry out experiments using MeSH ontology and UMLS separately. With respect to text collection, we have used three different collections, one with caption and title, other with caption, title and the text of the section where the image appears, and the third with the full article. Moreover, we have experimented with mixed search, textual and visual search, using the FIRE software for image retrieval. The use of FIRE and MeSH expansion with the minimal collection (only caption and title) obtains the best results in the track.	experiment;fire;image retrieval;query expansion	Manuel Carlos Díaz-Galiano;Miguel Ángel García Cumbreras;María Teresa Martín-Valdivia;Luis Alfonso Ureña López;Arturo Montejo Ráez	2008			query expansion;visual search;image retrieval;computer science;data mining;world wide web;information retrieval	NLP	-32.175442028196635	-63.20278735644484	23615
a371f871bb26f048254781ae72643af3e412abf1	twittercracy: exploratory monitoring of twitter streams for the 2016 u.s. presidential election cycle	conference publication;machine learning;statistics	We present TwitterCracy, an exploratory search system that allows users to search and monitor across the Twitter streams of political entities. Its exploratory capabilities stem from the application of lightweight time-series based clustering together with biased PageRank to extract facets from tweets and presenting them in a manner that facilitates exploration.		Muhammad Atif Qureshi;Arjumand Younus;Derek Greene	2016		10.1007/978-3-319-46131-1_16	computer science;data science;machine learning;data mining;world wide web;statistics	HCI	-24.19939989317209	-53.9996062841409	23617
11ac9d0984809a563a66496aca93b01d7a038b68	human rating of emotional expressions - scales vs. preferences		Human ratings of emotional expressions are the foundation for building and training automatic affect recognition systems. We compare two rating schemes for labeling emotional expressions: likert scales and pair-wise preferences. A statistical analysis shows that while there is a strong correlation between the two schemes, there are also frequent mismatches. Our findings indicate that the schemes perform differently well per affect label. We discuss reasons for this and outline planned future work based on the findings.	data mining;ground truth	Marco Pasch;Andrea Kleinsmith;Monica Landoni	2014		10.5220/0004727602400245	social psychology	NLP	-11.987900880710187	-73.7085804162043	23687
fa57c70cf001a4e119f1cfc4672b66881ea1ce57	ecnu at semeval-2018 task 3: exploration on irony detection from tweets via machine learning and deep learning methods		The paper describes our submissions to task 3 in SemEval 2018. There are two subtasks: Subtask A is a binary classification task to determine whether a tweet is ironic, and Subtask B is a fine-grained classification task including four classes. To address them, we explored supervised machine learning method alone and in combination with neural networks.	algorithm;artificial neural network;binary classification;deep learning;machine learning;overfitting;semeval;supervised learning;trustworthy computing	Zhenghang Yin;Feixiang Wang;Man Lan;Wenting Wang	2018			natural language processing;machine learning;semeval;deep learning;irony;computer science;artificial intelligence	NLP	-21.51807463648386	-69.87862547506813	23694
f9182dc18b0e85cd56a9e4be8d1a0a3b9e4f5561	building a situation-based language knowledge base	semantic annotation;efficient algorithm;language resources;large scale;natural language processing;knowledge base	Language resources are very important for natural language processing research and applications. This paper will introduce our ongoing research work to build a situation-based language knowledge base for the Chinese language, based on two basic language resources: three Chinese semantic lexicons and a large scale Chinese treebank. We developed a supporting platform to make full use of the abundant information contained in current Chinese semantic lexicons so as to gradually summarize the complete situation descriptions, organize them as situation network and build corresponding descriptive definition dictionary for different concepts. We explored an efficient algorithm to link from syntax to semantics so as to introduce suitable semantic explanations into current Chinese treebank and gradually build a situation-based semantically-annotated corpus. All these research work will lay a good foundation for the computational infrastructure in Chinese natural language processing.	algorithm;dictionary;knowledge base;lexicon;natural language processing;treebank	Qiang Zhou;Zushun Chen	2005		10.1007/978-3-540-30586-6_36	natural language processing;language identification;knowledge base;semantic computing;universal networking language;question answering;data control language;computer science;data mining;semantic compression;linguistics;temporal annotation	NLP	-31.93836774077758	-70.11445568411628	23697
55d537104c2616f38408ef6463d5c523e20e2fd1	learning translation templates from examples	analogical reasoning;translation templates;research paper;machine learning;target language;source language;machine translation	This paper proposes a mechanism for learning lexical level correspondences between two languages from a set of translated sentence pairs The proposed mechanism is based on an analogical reasoning between two translation examples Given two translation examples the similar parts of the sentences in the source language must correspond to the similar parts of the sentences in the target language Similarly the di erent parts should correspond to the respective parts in the translated sentences The correspondences between the similarities and also di erences are learned in the form of translation templates The approach has been implemented and tested on a small training dataset and produced promising results for further investigation Copyright c Elsevier Science Ltd	compiler;statistical machine translation	H. Altay Güvenir;Ilyas Cicekli	1998	Inf. Syst.	10.1016/S0306-4379(98)00017-9	dynamic and formal equivalence;computer-assisted translation;natural language processing;speech recognition;transfer-based machine translation;example-based machine translation;computer science;machine learning;machine translation;rule-based machine translation;machine translation software usability	AI	-20.25133260472478	-79.88968628082878	23703
3b0ab829e61e5ed171dfe50de2f2f66d2546881f	a dom tree alignment model for mining parallel data from the web	mined parallel sentence;parallel hyperlinks;parallel data acquisition;mining coverage;new web mining scheme;dom tree;parallel web document;dom tree alignment model;parallel dom tree;new mining scheme;previous mining scheme;data acquisition;web pages;document object model;web mining	This paper presents a new web mining scheme for parallel data acquisition. Based on the Document Object Model (DOM), a web page is represented as a DOM tree. Then a DOM tree alignment model is proposed to identify the translationally equivalent texts and hyperlinks between two parallel DOM trees. By tracing the identified parallel hyperlinks, parallel web documents are recursively mined. Compared with previous mining schemes, the benchmarks show that this new mining scheme improves the mining coverage, reduces mining bandwidth, and enhances the quality of mined parallel sentences.	data acquisition;document object model;hyperlink;mined;recursion;web mining;web page;world wide web	Lei Shi;Cheng Niu;Ming Zhou;Jianfeng Gao	2006			document object model;web mining;static web page;computer science;web page;data mining;database;data acquisition;world wide web	ML	-27.7973724711936	-56.083553461409934	23707
341b7e86e582b0cc7c43a1d6a6b0cf3878d815c6	ontology-supported web recommender for scholar information	databases;search engine;reliability;ontologies data mining information analysis statistical analysis java internet hybrid intelligent systems information systems mathematical model artificial neural networks;search engines;spss clementine;vocabulary;index structure;scholar information;data mining;ontologies artificial intelligence;information recommendation;web sites data mining indexing information filters internet ontologies artificial intelligence;internet;indexing;web sites;spss clementine ontology web recommender;web page indexing structure;ontologies;reliability and validity;ontology supported web recommender;information filters;web recommender;domain ontology;information recommendation ontology supported web recommender scholar information web page indexing structure search engines data mining spss clementine domain ontology java;ontology;recommender systems;java	In this quickly developed and shifting era of Internet, how to make use of webpage indexing structure or search engines which let information demanders fast and precisely search and extract out advantage information has become extremely important capability in users on the Web. This paper combined a data mining tool SPSS Clementine with the domain ontology to mine out usefully important information from huge datum, and then to employ Java to develop an information recommender for scholars--- Onto Recommender, in which can recommend suitably important information to scholars. The preliminary experiment outcomes proved the reliability and validation of the recommender achieving the regular-level outcomes of information recommendation, and accordingly proved the feasibility of the related techniques proposed in this paper.	clementine;data mining;experiment;geodetic datum;java;microsoft sql server;ontology (information science);recommender system;sie (file format);spss;web page;web search engine;world wide web	Sheng-Yuan Yang;Chun-Liang Hsu	2009	2009 Ninth International Conference on Hybrid Intelligent Systems	10.1109/HIS.2009.61	computer science;data mining;world wide web;information retrieval	Robotics	-29.471466580191898	-53.298336144027154	23722
66020ed3793f9657962b3986c6205e340d5961e1	nine features in a random forest to learn taxonomical semantic relations	relazioni semantiche;semantic distribuzionale	ROOT9 is a supervised system for the classification of hypernyms, co-hyponyms and random words that is derived from the already introduced ROOT13 (Santus et al., 2016). It relies on a Random Forest algorithm and nine unsupervised corpus-based features. We evaluate it with a 10-fold cross validation on 9,600 pairs, equally distributed among the three classes and involving several Parts-Of-Speech (i.e. adjectives, nouns and verbs). When all the classes are present, ROOT9 achieves an F1 score of 90.7%, against a baseline of 57.2% (vector cosine). When the classification is binary, ROOT9 achieves the following results against the baseline: hypernyms-co-hyponyms 95.7% vs. 69.8%, hypernyms-random 91.8% vs. 64.1% and co-hyponyms-random 97.8% vs. 79.4%. In order to compare the performance with the state-of-the-art, we have also evaluated ROOT9 in subsets of the Weeds et al. (2014) datasets, proving that it is in fact competitive. Finally, we investigated whether the system learns the semantic relation or it simply learns the prototypical hypernyms, as claimed by Levy et al. (2015). The second possibility seems to be the most likely, even though ROOT9 can be trained on negative examples (i.e., switched hypernyms) to drastically reduce this bias.	algorithm;baseline (configuration management);cosine similarity;cross-validation (statistics);f1 score;ontology components;random forest;taxonomy (general);word lists by frequency	Enrico Santus;Alessandro Lenci;Tin-Shing Chiu;Qin Lu;Chu-Ren Huang	2016	CoRR		natural language processing;computer science;artificial intelligence;machine learning;pattern recognition;data mining	NLP	-21.919383975541034	-71.43408612466199	23753
a65bbf57933e7d6cdc2dbb7e845f91256b2f9473	building earth mover's distance on bilingual word embeddings for machine translation	bilingual corpus filtering;word translation;earth mover s distance;machine translation	Following their monolingual counterparts, bilingual word embeddings are also on the rise. As a major application task, word translation has been relying on the nearest neighbor to connect embeddings cross-lingually. However, the nearest neighbor strategy suffers from its inherently local nature and fails to cope with variations in realistic bilingual word embeddings. Furthermore, it lacks a mechanism to deal with manyto-many mappings that often show up across languages. We introduce Earth Mover’s Distance to this task by providing a natural formulation that translates words in a holistic fashion, addressing the limitations of the nearest neighbor. We further extend the formulation to a new task of identifying parallel sentences, which is useful for statistical machine translation systems, thereby expanding the application realm of bilingual word embeddings. We show encouraging performance on both tasks.	holism;resultant;statistical machine translation;vocabulary;word embedding	Meng Zhang;Yang Liu;Huan-Bo Luan;Maosong Sun;Tatsuya Izuha;Jie Hao	2016			natural language processing;earth mover's distance;speech recognition;example-based machine translation;word error rate;computer science;machine learning;machine translation	AI	-19.8571525126524	-74.74285892280902	23770
28ca98363d1c88846115dbd59cc2a39f7ccf0dc8	unsupervised discovery and extraction of semi-structured regions in text via self-information	semi structured information extraction	We describe a general method for identifying and extracting information from semi-structured regions of text embedded within a natural language document. These regions encode information according to ad hoc schemas and visual cues, instead of using the grammatical and presentational conventions of normal sentential language. Examples include tables, key-value listings, or repeated enumerations of properties. Because of their generally non-sentential nature, these regions can present problems for standard information extraction algorithms. Unlike previous work in table extraction, which relies on a relatively noiseless two-dimensional layout, our aim is to accommodate a wide variety of structure types. Our approach for identifying semi-structured regions is an unsupervised one, based on scoring unusual regularity inside the document. As content in semi-structured regions are governed by a schema, the occurrence of features encompassing textual content and visual appearance would be unusual compared to those seen in sentential language. Regularity refers to repetition of these unusual features, as semi-structured regions commonly encode more than a single row or group of information. To score this, we present a measure based on expected self-information, derived from statistics over patterns of textual categories and visual layout. We describe the results of an initial study to assess the ability of these measures to detect semi-structured text in a corpus culled from the web, and show that this measure outperform baseline methods on an average precision measure. We present initial work that uses these significant patterns to generate extraction rules, and conclude with a discussion of future directions.	algorithm;attribute–value pair;baseline (configuration management);database schema;encode;embedded system;hoc (programming language);information extraction;information retrieval;natural language;self-information;semi-structured data;semiconductor industry;structured text;table (database);text corpus;unsupervised learning	Eric Yeh;John Niekrasz;Dayne Freitag	2013		10.1145/2509558.2509576	computer science;pattern recognition;data mining;information retrieval	NLP	-29.568150892655037	-69.21083223865205	23773
c01ca0b1b0b04813bafa7fc708b6e8ea65bfcf9f	macs: automatic counting of objects based on shape recognition	traitement automatise;analisis imagen;software;logiciel;computerized processing;tratamiento informatico;shape recognition;comptage particule;tratamiento automatizado;pattern recognition;logicial;image analysis;contaje particula;reconnaissance forme;reconocimiento patron;analyse image;traitement informatique;particle counting;automated processing	Summary: MACS is a tool for obtaining basic measurements of cell domains and for automatic counting of particles like colloidal gold probes. Availability: Freely available by anonymous ftp. URL: ftp:llftp.univ-rennes J .frlpubllaboslcrmJM'ACS.tar.Z Contact: jrolland@univ-rennesI ftIn cell biology, the precise localization of molecules has been achieved in electron microscopy by using fine probes which can be covalently attached to molecules able to bind specific sites (Roth etal, 1978; Hainfeldef ai, 1991). Colloidal gold probes are mostly used in immunoelectron microscopy where multiple labeling is possible by using gold particles of diameter usually ranging from 5 to 15 nm(Bendayan, 1982). Qualitative and quantitative analyses are possible by counting particles inside a given area or along the cell membrane (Griffiths and Hopeler, 1986), although counting gold particles in low-magnification electron images can be a tedious operation. To overcome this problem, we have developed a software named MACS (Measurements and Automatic Counting Software). MACS run on UNIX workstations. To be portable, the graphical interface has been developed with Motif 1.2, a toolkit from the Open Software Foundation (OSF) written for the X-window System. The software is written in C (Kernighan and Ritchie, 1978). The main functions provided by MACS are loading, windowing, measuring and counting particles (manual and automatic). Images used by MACS are stored as 1 byte pixel arrays, they have no header and their size must be specified before each loading. Images of 512 x 512 pixels size or less are displayed immediately in the display area, while images larger than 512 x 512 pixels should go through a 0.5 zooming process before being displayed. The operation for windowing an image of maximum size 512x512 pixels is performed by using the mouse. For an '7b whom correspondence should be addressed image of maximum size 1024x 1024 pixels, the user is automatically connected to a specific windowing area where the image is displayed with a factor of 0.5. Within this area, the image can be cut either by giving the upper left and bottom right coordinates, or by simply using the mouse. An image of maximum size can also be automatically windowed and saved on disk files as four quadrants of 512 x 512 pixels. Measuring sequences are performed using the mouse buttons. Length and surface measurements (given in pixels or nm) are standard functions which are performed manually. It is possible to average several measurements. Automatic counting is based on the cross-correlation function, a method which provides selective detection, and used for comparing features and pattern recognition (Frank, 1980; Stoschek and Hegerl, 1997). The cross-correlation function <t>/m of two images /] and ii is represented in the following way:	automatic control;byte;cross-correlation;diameter (qualifier value);electron microscopy;gold colloid;graphical user interface;header of a document;interface device component;interferometric microscopy;large;motif;mouse button;name;pattern recognition;physical object;pixel;plasma membrane;realms of the haunting;subatomic particle;the c programming language;uniform resource locator;unix;window function;workstation;x window system;magnetic cell separation system	Jean-Paul Rolland;P. Bon;D. Thomas	1997	Computer applications in the biosciences : CABIOS	10.1093/bioinformatics/13.5.563	computer vision;image analysis;computer science;computer graphics (images)	Graphics	-5.453877457598089	-57.1297032525429	23788
2e97db7ef85cb74526f75bc4b9e52ab3e4e2825d	generalizable intention prediction of human drivers at intersections		Effective navigation of urban environments is a primary challenge remaining in the development of autonomous vehicles. Intersections come in many shapes and forms, making it difficult to find features and models that generalize across intersection types. New and traditional features are used to train several intersection intention models on real-world intersection data, and a new class of recurrent neural networks, Long Short Term Memory networks (LSTMs), are shown to outperform the state of the art. The models predict whether a driver will turn left, turn right, or continue straight up to 150 m with consistent accuracy before reaching the intersection. The results show promise for further use of LSTMs, with the mean cross validated prediction accuracy averaging over 85% for both three and four-way intersections, obtaining 83% for the highest throughput intersection.	artificial neural network;autonomous robot;high-throughput computing;long short-term memory;machine learning;recurrent neural network;supervised learning;throughput;unbalanced circuit	Derek J. Phillips;Tim Allan Wheeler;Mykel J. Kochenderfer	2017	2017 IEEE Intelligent Vehicles Symposium (IV)	10.1109/IVS.2017.7995948	long short term memory;throughput;data modeling;hidden markov model;feature extraction;machine learning;recurrent neural network;artificial intelligence;engineering	Robotics	-10.59789361262683	-67.5349464060114	23790
3c78b47bee5408aaa372963525b0390890a0c8bf	multiview clustering: a late fusion approach using latent models	information retrieval;data fusion;serveur institutionnel;multi view clustering;archive institutionnelle;latent models;open access;archive ouverte unige;cluster model;cybertheses;institutional repository;correlation analysis	Multi-view clustering is an important problem in information retrieval due to the abundance of data offering many perspectives and generating multi-view representations. We investigate in this short note a late fusion approach for multi-view clustering based on the latent modeling of cluster-cluster relationships. We derive a probabilistic multi-view clustering model outperforming an early-fusion approach based on multi-view feature correlation analysis.	cluster analysis;information retrieval	Eric Bruno;Stéphane Marchand-Maillet	2009		10.1145/1571941.1572103	document clustering;computer science;data science;data mining;sensor fusion;cluster analysis;probabilistic latent semantic analysis;brown clustering;information retrieval;conceptual clustering	AI	-25.177970048858654	-60.244983905689885	23803
cb7f45a0915bfc833f36523b22bd12bab766d8a8	sysevr: a framework for using deep learning to detect software vulnerabilities		The detection of software vulnerabilities (or vulnerabilities for short) is an important problem that has yet to be tackled, as manifested by many vulnerabilities reported on a daily basis. This calls for machine learning methods to automate vulnerability detection. Deep learning is attractive for this purpose because it does not require human experts to manually define features. Despite the tremendous success of deep learning in other domains, its applicability to vulnerability detection is not systematically understood. In order to fill this void, we propose the first systematic framework for using deep learning to detect vulnerabilities. The framework, dubbed Syntax-based, Semantics-based, and Vector Representations (SySeVR), focuses on obtaining program representations that can accommodate syntax and semantic information pertinent to vulnerabilities. Our experiments with 4 software products demonstrate the usefulness of the framework: we detect 15 vulnerabilities that are not reported in the National Vulnerability Database. Among these 15 vulnerabilities, 7 are unknown and have been reported to the vendors, and the other 8 have been “silently” patched by the vendors when releasing newer versions of the products.	deep learning;experiment;machine learning;national vulnerability database;patch (computing);relevance;section 508 amendment to the rehabilitation act of 1973;vulnerability (computing)	Zhen Li;Deqing Zou;Shouhuai Xu;Hai Jin;Yawei Zhu;Zhaoxuan Chen;Sujuan Wang;Jialai Wang	2018	CoRR		national vulnerability database;machine learning;deep learning;software;artificial intelligence;syntax;mathematics;vulnerability	Security	-4.8266031117197645	-69.52666737465029	23823
f02fd0c9d8cfa321470ad08b57b51f8c2641cac6	improving the performance of question answering with semantically equivalent answer patterns	satisfiability;question answering system;question reformulations;semantic relations;natural language processing;question answering	In this paper, we discuss a novel technique based on semantic constraints to improve the performance and portability of a reformulation-based question answering system. First, we present a method for acquiring semantic-based reformulations automatically. The goal is to generate patterns from sentences retrieved from the Web based on lexical, syntactic and semantic constraints. Once these constraints have been defined, we present a method to evaluate and re-rank candidate answers that satisfy these constraints using redundancy. The two approaches have been evaluated independently and in combination. The evaluation on 493 questions from TREC-11 shows that the automatically acquired semantic patterns increase the MRR by 26%, the re-ranking using semantic redundancy increases the MRR by 67%, and the two approaches combined increase the MRR by 73%. This new technique allows us to avoid the manual work of formulating semantically equivalent reformulations; while still increasing performance. 2007 Elsevier B.V. All rights reserved.	lexicon;question answering;redundancy (engineering);software portability;world wide web	Leila Kosseim;Jamileh Yousefi	2008	Data Knowl. Eng.	10.1016/j.datak.2007.07.010	natural language processing;question answering;computer science;data mining;database;information retrieval;satisfiability	NLP	-29.381887883416656	-69.6780484876214	23838
139f4aad228c43e095808b7b24eeb634c9da58f0	rumor has it: identifying misinformation in microblogs	large-scale dataset;large amount;identifying rumor;rumor detection;online social media;mean average precision;online misinformation;annotated tweet;large network;false information	A rumor is commonly defined as a statement whose true value is unverifiable. Rumors may spread misinformation (false information) or disinformation (deliberately false information) on a network of people. Identifying rumors is crucial in online social media where large amounts of information are easily spread across a large network by sources with unverified authority. In this paper, we address the problem of rumor detection in microblogs and explore the effectiveness of 3 categories of features: content-based, network-based, and microblog-specific memes for correctly identifying rumors. Moreover, we show how these features are also effective in identifying disinformers, users who endorse a rumor and further help it to spread. We perform our experiments on more than 10,000 manually annotated tweets collected from Twitter and show how our retrieval model achieves more than 0.95 in Mean Average Precision (MAP). Finally, we believe that our dataset is the first large-scale dataset on rumor detection. It can open new dimensions in analyzing online misinformation and other aspects of microblog conversations.	emergence;experiment;information retrieval;linear function;mean squared error;meme;network topology;social media;statistical model	Vahed Qazvinian;Emily Rosengren;Dragomir R. Radev;Qiaozhu Mei	2011			internet privacy;world wide web	NLP	-20.728580478167313	-55.26385871337452	23848
31daf095d1b665f3e92973e6c152a4e39ca4c935	astexviewertm: a visualisation aid for structure-based drug design	graphical programming;structure based drug design	AstexViewerTM is a Java molecular graphics program that can be used for visualisation in many aspects of structure-based drug design. This paper describes its functionality, implementation and examples of its use. The program can run as an Applet in a web browser allowing structures to be displayed without installing additional software. Applications of its use are described for visualisation and as part of a structure based design platform. The software is being made freely available to the community and may be downloaded from http://www.astex-technology.com/AstexViewer.	applet;graphics software;java;molecular graphics	Michael J. Hartshorn	2002	Journal of Computer-Aided Molecular Design	10.1023/A:1023813504011	simulation;chemistry;human–computer interaction;computer science;visual programming language;world wide web	SE	-5.234153142892292	-58.2204833993573	23859
94abff7ab5b1b8af4eaa771fbf5275fddf5dafa3	abstract anaphora resolution in danish	individual nps;english dialogue;danish pronoun;abstract object;danish dialogue;dialogue collection;abstract anaphora resolution	In this paper I describe the use of Danish pronouns and deictics in dialogues. Then I present an adaptation to Danish of Eckert and Strubeu0027s algorithm for resolving anaphora referring to individual NPs and abstract objects in English dialogues (Eckert and Strube, 1999b; Eckert and Strube, 1999a). The adapted algorithm is tested on four Danish dialogues from two dialogue collections and the results obtained are evaluated.	anaphora (linguistics)	Costanza Navarretta	2000			natural language processing;computer science;linguistics;communication	NLP	-28.388894438885764	-78.68264683849733	23868
b87976ecb9aed3e57aa374bd848f6fb3bf754e70	an efficient text summarizer using lexical chains	automatic text summarization;original document;useful summary;lexical chain extraction algorithm;information content;html document;future summary generation research;efficient text summarizer;lexical chain;previous research;short summary	We present a system which uses lexical chains as an intermediate representation for automatic text summarization. This system builds on previous research by implementing a lexical chain extraction algorithm in linear time. The system is reasonably domain independent and takes as input any text or HTML document. The system outputs a short summary based on the most salient concepts from the original document. The length of the extracted summary can be either controlled automatically, or manually based on length or percentage of compression. While still under development, the system provides useful summaries which compare well in information content to human generated summaries. Additionally, the system provides a robust test bed for future summary generation research. 1 I n t r o d u c t i o n Automatic text summarization has long been viewed as a two-step process. First, an intermediate representation of the summary must be created. Second, a natural language representation of the summary must be generated using the intermediate representation(Sparek Jones, 1993). Much of the early research in automatic text summarization has involved generation of the intermediate representation. The natural language generation problem has only recently received substantial attention in the context of summarization.	algorithm;automatic summarization;html;intermediate representation;jones calculus;markov chain;natural language generation;self-information;testbed;time complexity	H. Gregory Silber;Kathleen F. McCoy	2000			natural language processing;speech recognition;computer science;information retrieval	NLP	-29.840679814351844	-69.55195971243003	23889
06f49ea374358a8d254678f7a6ad1503418eb3b0	global ranking by exploiting user clicks	search engine;sequential supervised learning;supervised learning;data collection;conditional random field;user clicks;implicit relevance feedback;experimental evaluation;user interaction;learning to rank;sliding window	It is now widely recognized that user interactions with search results can provide substantial relevance information on the documents displayed in the search results. In this paper, we focus on extracting relevance information from one source of user interactions, i.e., user click data, which records the sequence of documents being clicked and not clicked in the result set during a user search session. We formulate the problem as a global ranking problem, emphasizing the importance of the sequential nature of user clicks, with the goal to predict the relevance labels of all the documents in a search session. This is distinct from conventional learning to rank methods that usually design a ranking model defined on a single document; in contrast, in our model the relational information among the documents as manifested by an aggregation of user clicks is exploited to rank all the documents jointly. In particular, we adapt several sequential supervised learning algorithms, including the conditional random field (CRF), the sliding window method and the recurrent sliding window method, to the global ranking problem. Experiments on the click data collected from a commercial search engine demonstrate that our methods can outperform the baseline models for search results re-ranking.	algorithm;baseline (configuration management);clickstream;conditional random field;interaction;learning to rank;machine learning;relevance;result set;session (web analytics);supervised learning;web search engine	Shihao Ji;Ke Zhou;Ciya Liao;Zhaohui Zheng;Gui-Rong Xue;Olivier Chapelle;Gordon Sun;Hongyuan Zha	2009		10.1145/1571941.1571950	sliding window protocol;ranking;computer science;machine learning;data mining;supervised learning;world wide web;conditional random field;information retrieval;learning to rank;search engine;data collection	Web+IR	-27.130976148738263	-54.199713337200706	23901
6af903a52dfbe01416fbdce13b8abccf5e8e5067	building a wordnet for sinhala	article abstract	Sinhala is one of the official languages of Sri Lanka and is used by over 19 million people. It belongs to the Indo-Aryan branch of the Indo-European languages and its origins date back to at least 2000 years. It has developed into its current form over a long period of time with influences from a wide variety of languages including Tamil, Portuguese and English. As for any other language, a WordNet is extremely important for Sinhala to take it into the digital era. This paper is based on the project to develop a WordNet for Sinhala based on the English (Princeton) WordNet. It describes how we overcame the challenges in adding Sinhala specific characteristics which were deemed important by Sinhala language experts to the WordNet while keeping the structure of the original English WordNet. It also presents the details of the crowdsourcing system we developed as a part of the project consisting of a NoSQL database in the backend and a web-based frontend. We conclude by discussing the possibility of adapting this architecture for other languages and the road ahead for the Sinhala WordNet and Sinhala NLP.	crowdsourcing;indo;information retrieval;natural language processing;nosql;sinhala keyboard;web application;wordnet	Indeewari Wijesiri;Malaka Gallage;Buddhika Gunathilaka;Madhuranga Lakjeewa;Daya C. Wimalasuriya;Gihan Dias;Rohini Paranavithana;Nisansa de Silva	2014			natural language processing;information retrieval	NLP	-33.27059919720213	-71.48971267962656	23920
a3c634e17979e0fe6130706c3e6827feeee80a5c	japanese tree adjoining grammar and its application to on-line help system neoassist			tree-adjoining grammar	Kuniaki Uehara	1990			programming language;tree-adjoining grammar;computer science	NLP	-30.094884265223225	-79.01329806440033	23997
0b5acdd997cfc213dced0b4e11594f6ab94b4d7d	comparing eeg/erp-like and fmri-like techniques for reading machine thoughts	settore inf 01 informatica;settore ing inf 05 sistemi di elaborazione delle informazioni;coarse grained	fMRI and ERP/EEG are two different sources for scanning the brain for building mind state decoders. fMRI produces accurate images but it is expensive and cumbersome. ERP/EEG is cheaper and potentially wearable but it gives more coarse-grain data. Recently the metaphor between machines and brains has been introduced in the context of mind state decoders: the “readers for machines’ thoughts”. This metaphor gives the possibility for comparing mind state decoder methods in a more controlled setting. In this paper, we compare the fMRI and ERP/EEG in the context of building “readers for machines’ thoughts”. We want assess if the cheaper ERP/EEG can be competitive with fMRI models for building decoders for mind states. Experiments show that accuracy of “readers” based on ERP/EEG-like data are considerably lower than the one of those based on fMRI-like images.	erp;electroencephalography;wearable computer	Fabio Massimo Zanzotto;Danilo Croce	2010		10.1007/978-3-642-15314-3_13	psychology;computer science;artificial intelligence;machine learning;communication;algorithm;cognitive science	NLP	-13.681098500860799	-75.91954453149555	23998
de643d781cfb7736094b598b109a52dbc3b8da53	who are the american vegans related to brad pitt?: exploring related entities	wikipedia;distributional semantics;entity graph;entity relatedness;entity recommendation	In this demo, we present Entity Relatedness Graph (EnRG), a focused related entities explorer, which provides the users with a dynamic set of filters and facets. It gives a ranked lists of related entities to a given entity, and clusters them using the different filters. For instance, using EnRG, one can easily find the American vegans related to Brad Pitt or Irish universities related to Semantic Web. Moreover, EnRG helps a user in discovering the provenance for implicit relations between two entities. EnRG uses distributional semantics to obtain the relatedness scores between two entities.	distributional semantics;entity;my life as a teenage robot;semantic web	Nitish Aggarwal;Kartik Asooja;Housam Ziad;Paul Buitelaar	2015		10.1145/2740908.2742851	natural language processing;computer science;data mining;brand;weak entity;information retrieval	ML	-28.199829095671078	-63.37262774925485	24008
d0fdc28667351402257c3034caa06169367b8365	statistical machine translation in low resource settings		My thesis will explore ways to improve the performance of statistical machine translation (SMT) in low resource conditions. Specifically, it aims to reduce the dependence of modern SMT systems on expensive parallel data. We define low resource settings as having only small amounts of parallel data available, which is the case for many language pairs. All current SMT models use parallel data during training for extracting translation rules and estimating translation probabilities. The theme of our approach is the integration of information from alternate data sources, other than parallel corpora, into the statistical model. In particular, we focus on making use of large monolingual and comparable corpora. By augmenting components of the SMT framework, we hope to extend its applicability beyond the small handful of language pairs with large amounts of available parallel text.	bilingual dictionary;language technology;lexicon;parallel text;statistical machine translation;statistical model;text corpus	Ann Irvine	2013			natural language processing;bioinformatics;data science	NLP	-24.778348604836278	-74.37601285851044	24021
12ad828ce165614c28ae955bca236d11b95f1521	some basic data structures and algorithms for chemical generic programming	data structure;generic programming	Here, we report a template library used for molecular operation, the Molecular Handling Template Library (MHTL). The library includes some generic data structures and generic algorithms, and the two parts are associated with each other by two concepts: Properties and Molecule. The concept Properties describes the interface to access objects' properties, and the concept Molecule describes the minimum requirement for a molecular class. Data structures include seven models of Properties, each using a different method to access properties, and two models of molecular classes. Algorithms include molecular file manipulation subroutines, SMARTS language interpreter and matcher functions, and molecular OpenGL rendering functions.	algorithm;class;clinical use template;data structure;generic drugs;generic programming;opengl;physical object;smiles arbitrary target specification;subroutine	Wei Zhang;Tingjun Hou;Xuebin Qiao;Xiaojie Xu	2004	Journal of chemical information and computer sciences	10.1021/ci049938s	chemistry;computer science;theoretical computer science;database;programming language;generic programming;algorithm	PL	-5.895659152425409	-58.83137763898866	24029
a1ac88680f53903c4a0fcefba5f2da4dafa6b884	ranking and selecting terms for text categorization via svm discriminate boundary	svm discriminate boundary;smaller number;distinct term;feature ranking;important term;text categorization;redundant term;natural language document categorization;higher classification performance;classifying document;classification performance;technology management;support vector machines;classification;industrial engineering;feature selection;content management;natural languages;engineering management;text analysis;support vector machine;natural language;statistics	The problem of natural language document categorization consists in classifying documents into predetermined categories based on their contents. Each distinct term, or word, in the documents is a feature for representing a document. In general, the number of terms may be extremely large and the dozens of redundant terms may be included, which may deteriorate the performance of classification. In this paper, an SVM based feature ranking and selecting method for text categorization is proposed. The contribution of each term for classification is calculated based on the nonlinear discriminate boundary generated by support vector machine (SVM). The results of experiments on the Reuters-21S78 dataset show that the proposed method achieves higher classification performance than existing feature selection based on LSI and x/sup 2/ statistics values.	categorization;cluster analysis;document classification;document retrieval;experiment;feature selection;gradient;international conference on machine learning;language model;natural language;nonlinear system;numerical analysis;radial basis function kernel;statistical classification;support vector machine;yang	Tien-Fang Kuo;Yasutoshi Yajima	2005	2005 IEEE International Conference on Granular Computing	10.1109/GRC.2005.1547341	machine learning;data mining;boosting methods for object categorization;structured support vector machine;categorization;artificial intelligence;ranking svm;feature selection;support vector machine;computer science;text mining;pattern recognition;ranking	Vision	-21.691148746069114	-64.20187157054194	24036
825b06d04234675dfd33c79d9aca070e77016d43	sequence-to-sequence models can directly translate foreign speech		We present a recurrent encoder-decoder deep neural network architecture that directly translates speech in one language into text in another. The model does not explicitly transcribe the speech into text in the source language, nor does it require supervision from the ground truth source language transcription during training. We apply a slightly modified sequence-to-sequence with attention architecture that has previously been used for speech recognition and show that it can be repurposed for this more complex task, illustrating the power of attention-based models. A single model trained end-to-end obtains state-of-the-art performance on the Fisher Callhome Spanish-English speech translation task, outperforming a cascade of independently trained sequence-to-sequence speech recognition and machine translation models by 1.8 BLEU points on the Fisher test set. In addition, we find that making use of the training data in both languages by multi-task training sequence-to-sequence speech translation and recognition models with a shared encoder network can improve performance by a further 1.4 BLEU points.	artificial neural network;bleu;computer multitasking;deep learning;encoder;end-to-end principle;ground truth;machine translation;network architecture;recurrent neural network;speech recognition;test set;transcription (software)	Ron J. Weiss;Jan Chorowski;Navdeep Jaitly;Yonghui Wu;Zhifeng Chen	2017			speech analytics;machine learning;speech translation;natural language processing;encoder;artificial intelligence;speech recognition;machine translation;bleu;architecture;artificial neural network;pattern recognition;computer science;evaluation of machine translation	NLP	-17.668655518465894	-75.38055112700258	24064
dac4e79a81a16917f97ece5cd1da0a1f0fa55cda	similarity searching in statistical figures based on extracted meta data	data mining shape image analysis prototypes feature extraction biomedical imaging focusing histograms computer science petroleum;information extraction;meta data similarity searching statistical images information extraction;query formulation;statistical figures;statistical images similarity searching statistical figures extracted meta data query formulation;statistical images;meta data;similarity searching;query formulation image retrieval meta data;similarity search;extracted meta data;image retrieval	Similarity searching is an excellent approach for getting information from subjective materials like images or videos. Some excellent works on special domains have done. We focus on statistical images. These kinds of images have some excellent features that can be clearly extractable and useable in similarity searching. But there no significant work has been done in this area. So we have done some preliminary works in this domain. By some extensive analysis we classify images of this domain in some sub domains and also identified the nature of features those can be considered as silent. We develop a prototype based on this analysis where we store extracted features information of a statistical images as meta data. Then we devise some strategy to do similarity searching using standard query formulation.	prototype;usability	Mohammad Mahdi Hassan;Wasfi G. Al-Khatib	2007	Computer Graphics, Imaging and Visualisation (CGIV 2007)	10.1109/CGIV.2007.76	computer science;pattern recognition;data mining;information retrieval	Vision	-14.207424761876336	-58.30517780662582	24068
265445208740ba6a3bcaec22eddccd8dab1f67d6	tourism network comments sentiment analysis and early warning system based on ontology		Mine Tourism information and opinion, intelligent analysis user emotion, to improve tourism products and services, is the key to the success of tourism e-commerce. This paper embarks from the tourism network review information, researches how to build the microblog emotional vocabulary ontology and how to classify emotion based on Naive Bayes classification algorithm, implements a tourism network comments sentiment analysis and early warning system based on ontology. It not only save a large amount of manpower and material resources, but also have a certain reference value to establish reasonable tourism policy.	sentiment analysis	Yanxia Yang;Xiaoli Lin	2016		10.1007/978-3-319-42294-7_76	data science;data mining;world wide web	NLP	-22.62702138205563	-57.261729582496216	24138
7dffa594d15237e18cf86e9e9656b879ed742416	thuir at trec 2003: novelty, robust and web		This is the second time that Tsinghua University Information Retrieval Group (THUIR) participates in TREC. In this year, we took part in four tracks: novelty, robust, web and HARD, describing in following sections, respectively. A new IR system named TMiner has been built on which all experiments have been performed. In the system, Primary Feature Model (PFM) has been proposed and combined with BM2500 term weighting , which led to encouraging results. Word-pair searching has also been performed and improves system precision. Both approaches are described in robust experiments (section 2), and they were also used in web track experiments.	computational hardness assumption;experiment;feature model;information retrieval;text retrieval conference	Min Zhang;Chuan Lin;Yiqun Liu;Leo Zhao;Shaoping Ma	2003				Web+IR	-31.9297434063986	-63.35339232514445	24197
866d328149a3e2094e1f4c076d30cff6dec319f4	exploring semantic information in hindi wordnet for hindi dependency parsing		In this paper, we present our efforts towards incorporating external knowledge from Hindi WordNet to aid dependency parsing. We conduct parsing experiments on Hindi, an Indo-Aryan language, utilizing the information from concept ontologies available in Hindi WordNet to complement the morpho-syntactic information already available. The work is driven by the insight that concept ontologies capture a specific real world aspect of lexical items, which is quite distinct and unlikely to be deduced from morpho-syntactic information such as morph, POS-tag and chunk. This complementing information is encoded as an additional feature for data driven parsing and experiments are conducted. We perform experiments over datasets of different sizes. We achieve an improvement of 1.1% (LAS) when training on 1,000 sentences and 0.2% (LAS) on 13,371 sentences over the baseline. The improvements are statistically significant at p<0.01. The higher improvements on 1,000 sentences suggest that the semantic information could address the data sparsity problem.	baseline (configuration management);experiment;indo;ontology (information science);parsing;sparse matrix;synonym ring;wordnet	Sambhav Jain;Naman Jain;Aniruddha Tammewar;Riyaz Ahmad Bhat;Dipti Misra Sharma	2013			natural language processing;speech recognition;computer science;linguistics	NLP	-22.44001415146129	-72.5299355612563	24229
3b15f60a5d752bf42893ac77de3e795f76f5e178	cluster analysis and classification of named entities.		This paper presents a statistics-based and language independent unsupervised approach for clustering possible named entities. We describe and motivate the features and statistical filters used by our clustering process. Using the Model-Based Clustering Analysis software we obtained different clusters of named entities. The method was applied to Bulgarian and English. For some clusters, precision is close to 100%; this helps human validation and saves time. Other clusters still need further refinement. Based on the obtained clusters, it is possible to classify new named entities.	cluster analysis;data validation;named entity;named-entity recognition;refinement (computing);unsupervised learning	Joaquim Ferreira da Silva;Zornitsa Kozareva;José Gabriel Pereira Lopes	2004			software;artificial intelligence;natural language processing;bulgarian;cluster analysis;computer science;pattern recognition	NLP	-24.372863917790415	-70.98618126199447	24271
26447b5eb4e3a27d38d1cf9a3a8a93766b7cb202	supervised dynamic topic models for associative topic extraction with a numerical time series	text mining;time series analysis;topic models	A series of events generates multiple types of time series data, such as numeric and text data over time, and the variations of the data types capture the events from different angles. This paper aims to integrate the analyses on such numerical and text time-series data influenced by common events with a single model to better understand the events. Specifically, we present a topic model, called a supervised dynamic topic model (sDTM), which finds topics guided by a numerical time series. We applied sDTM to financial indexes and financial news articles. First, sDTM identifies topics associated with the characteristics of time-series data from the multiple types of data. Second, sDTM predicts numerical time-series data with a higher level of accuracy than does the iterative model, which is supported by lower mean squared errors.	dynamic topic model;iterative and incremental development;iterative method;numerical analysis;text corpus;time series	Sungrae Park;Wonsung Lee;Il-Chul Moon	2015		10.1145/2809936.2809938	computer science;data science;machine learning;data mining	DB	-23.92050431899015	-55.80063409968895	24300
1e8c9ec3a92aeb32620c8d346b97d76c68f40f96	a map-based web search interface using point of interest aggregation		With advent of a mobile computing, the pattern of information search has been changed. Search queries through mobile devices increase; 30% of Google’s organic search queries come from mobile devices, and local search, which seeks information with geospatial constraints, also increases. As of 2013 local search on mobile phones continues to grow up to 60% since 2010. However, a large number of web documents cannot be exposed to local search even though they refer to a point of interest (POI) just because they are not explicitly geo-tagged. We are interested in connecting typical web documents to spatial search based on POIs by geotagging web documents. In this paper, we present a map-based web search system that serves geospatial search queries for non-geotagged documents. The proposed system provides with fine-grained local search for typical web pages mentioning several POIs and supports semantic search in accordance with their spatial relation of inclusion.	point of interest;web search engine	Kwangsoon Jung;Sang Chul Ahn;Heedong Ko	2014		10.1007/978-3-319-07731-4_35	world wide web	Web+IR	-31.48666550200018	-52.82958670434145	24313
847b8fce30fd1ec218988aa2c9734f181b37ad57	robust latent semantic exploration for image retrieval in social media	l 2 1;word to vector;image retrieval	With the speedy development of social media, more and more multimedia data are generated by users with tags associated. The tag information provides the extra cue to link multimedia data in addition to the multimedia content itself. However, the manually added tags are always with noise and not correct enough. Moreover, the semantically similar tags exist massively but cannot be accounted for well. This paper proposes a new algorithm to robustly combine multimedia content and associated tags by mining the latent semantic which takes into account the semantically similar tags. The l2;1 norm is proposed to employ in latent semantic indexing for a more robust latent space, and a word-to-vector based clustering method is proposed to address the massive tags with similar meaning. The experiments on extensive data demonstrate the proposed method. Compared to the existing latent semantic based methods, the algorithm proposed a more robust model to deal with noise. & 2015 Elsevier B.V. All rights reserved.	algorithm;cluster analysis;experiment;image retrieval;latent semantic analysis;mathematical optimization;social media;sparse matrix;word2vec	Liujuan Cao;Fanglin Wang	2015	Neurocomputing	10.1016/j.neucom.2015.02.082	image retrieval;computer science;data mining;probabilistic latent semantic analysis;world wide web;information retrieval	AI	-25.870722032793942	-59.61843430251471	24406
d84b6e03429bc659aff2502d447a79ad433d991d	developing simplified chinese psychological linguistic analysis dictionary for microblog	traditional chinese;text analysis;microblog;simplified chinese;liwc	The words that people use could reveal their emotional states, intentions, thinking styles, individual differences, etc. LIWC (Linguistic Inquiry and Word Count) has been widely used for psychological text analysis, and its dictionary is the core. The Traditional Chinese version of LIWC dictionary has been released, which is a translation of LIWC English dictionary. However, Simplified Chinese which is the world's most widely used language has subtle differences with Traditional Chinese. Furthermore, both English LIWC dictionary and Traditional Chinese version dictionary were both developed for relatively formal text. Microblog has become more and more popular in China nowadays. Original LIWC dictionaries take less consideration on microblog popular words, which makes it less applicable for text analysis on microblog. In this study, a Simplified Chinese LIWC dictionary is established according to LIWC categories. After translating Traditional Chinese dictionary into Simplified Chinese, five thousand words most frequently used in microblog are added into the dictionary. Four graduate students of psychology rated whether each word belonged in a category. The reliability and validity of Simplified Chinese LIWC dictionary were tested by these four judges. This new dictionary could contribute to all the text analysis on microblog in future.	academy;chinese dictionary;cognition;external validity	Rui Gao;Bibo Hao;He Li;Yusong Gao;Tingshao Zhu	2013		10.1007/978-3-319-02753-1_36	psychology;natural language processing;speech recognition;linguistics	NLP	-22.78551358668547	-59.24920659372816	24544
94a6b606c9d85d255f65b9006410a3641d41033e	building ip geolocation database from online used market articles		An IP geolocation database provides location information for each IP address block. Though free and commercial IP geolocation databases are available, their accuracy and coverage are not clearly known for non US countries. In this paper, for the wider and more accurate IP geolocation service in Korea, we present a crowd-sourcing IP geolocation database build-up method by analyzing messages of an online used market, Ruliweb that reveals IP addresses and location information. From 195,937 posts on the Ruliweb website over 29 months, we can construct a Korean IP geolocation database that achieves the district-level granularity and outperforms commercial services in the accuracy. Though every online website does not provide IP address information, our crowd-sourcing methodology is useful for IP geolocation database management in Internet or mobile services.	classless inter-domain routing;crowdsourcing;database;geolocation software	Hyunsu Mun;Youngseok Lee	2017	2017 19th Asia-Pacific Network Operations and Management Symposium (APNOMS)	10.1109/APNOMS.2017.8094175	computer network;world wide web;computer science;ip address management;the internet;database;granularity;geolocation;bogon filtering	Metrics	-25.224324810281384	-52.206969414312375	24597
66bfa865d59b81bef4c4a13b223b566e2e2acb16	applications of data selection via cross-entropy difference for real-world statistical machine translation		We broaden the application of data selection methods for domain adaptation to a larger number of languages, data, and decoders than shown in previous work, and explore comparable applications for both monolingual and bilingual crossentropy difference methods. We compare domain adapted systems against very large general-purpose systems for the same languages, and do so without a bias to a particular direction. We present results against real-world generalpurpose systems tuned on domain-specific data, which are substantially harder to beat than standard research baseline systems. We show better performance for nearly all domain adapted systems, despite the fact that the domainadapted systems are trained on a fraction of the content of their general domain counterparts. The high performance of these methods suggest applicability to a wide variety of contexts, particularly in scenarios where only small supplies of unambiguously domain-specific data are available, yet it is believed that additional similar data is included in larger heterogenous-content general-domain corpora.	baseline (configuration management);cross entropy;domain adaptation;domain-specific language;general-purpose markup language;statistical machine translation;text corpus	Amittai Axelrod;QingJun Li;William D. Lewis	2012			speech recognition;computer science;machine learning;data mining	ML	-19.540403789923914	-74.8589651335891	24627
6a716a562d19ccb9f9fbaa3750e80a3c5fdb65aa	comparative news summarization using linear programming	comparative news summarization;linear programming model;comparative evidence;news topic;cross-topic concept pair;optimization problem;comparative news summary;comparable news topic	Comparative News Summarization aims to highlight the commonalities and differences between two comparable news topics. In this study, we propose a novel approach to generating comparative news summaries. We formulate the task as an optimization problem of selecting proper sentences to maximize the comparativeness within the summary and the representativeness to both news topics. We consider semantic-related cross-topic concept pairs as comparative evidences, and consider topic-related concepts as representative evidences. The optimization problem is addressed by using a linear programming model. The experimental results demonstrate the effectiveness of our proposed model.	automatic summarization;linear programming;machine learning;mathematical optimization;optimization problem;programming model	Xiaojiang Huang;Xiaojun Wan;Jianguo Xiao	2011			computer science;data science;data mining;information retrieval	AI	-26.46999635593207	-61.72687789205919	24644
d3fec6d9e7f1f06707b59e16046d7dcc084754e9	a study on a spatio-temporal data structure for managing video data from monitoring cameras	generic algorithm;query formulation;virtual reality;support system;spatio temporal data;monitoring;search key spatio temporal data structure video data management monitoring cameras support system disaster prevention multimedia virtual large wall;video cameras;query formulation spatial data structures video cameras monitoring disasters virtual reality multimedia databases temporal databases visual databases;monitoring cameras large scale systems layout videoconference disaster management humans large screen displays cities and towns streaming media;indexation;multimedia databases;spatial data structures;temporal databases;disasters;visual databases	In this paper the authors discusses about video processing which has been more concentrated on moving objects in the video. Video objects refer to the semantics that are used to denote the spatialtemporal region. They discuss about some approaches which have been done in the past and then compare with the proposed approach. In the previous work, few approaches were based on low level features and others were based on high level semantics. The main aim of the paper is to develop an efficient video data model which introduces the concept of Video Object Action (VOA) and Elementary Video Object Motion (EVOM) for bridging the gap between low level features with semantics.	bridging (networking);dvd region code;data model;data structure;high-level programming language;video processing	Yiqun Wang;Yoshinori Hijikata;Shogo Nishida	2003		10.1109/AINA.2003.1192920	computer vision;disaster;genetic algorithm;computer science;artificial intelligence;video tracking;data mining;database;virtual reality;temporal database	AI	-13.439346267117722	-55.13565147461641	24645
2283872af70c0fa98b94d98b4ea59fc4629f9591	a model for evaluating the quality of user-created documents	supervised classification;feature selection	In this paper, we propose a model for evaluating the quality of general user-created documents. The model is based on supervised classification approach, in which output scores are considered as quality of given document. In order to utilize both textual and non-textual attributes of documents, we incorporated a number of objectively measurable, real-valued features selected upon predefined criteria for quality. Experiments on two datasets of real world documents show that textual features are stable indicators for evaluating documents’ quality. Some features are inferred to be effective for general kinds of documents.	experiment;machine learning;supervised learning	Linh Hoang;Jung-Tae Lee;Young-In Song;Hae-Chang Rim	2008		10.1007/978-3-540-68636-1_54	computer science;machine learning;pattern recognition;data mining;feature selection;information retrieval	Web+IR	-21.115437949686317	-63.00046831846895	24657
0e606940e0ff1162fef582baeb000ffa95c6004d	transfer learning based cross-lingual knowledge extraction for wikipedia		Wikipedia infoboxes are a valuable source of structured knowledge for global knowledge sharing. However, infobox information is very incomplete and imbalanced among the Wikipedias in different languages. It is a promising but challenging problem to utilize the rich structured knowledge from a source language Wikipedia to help complete the missing infoboxes for a target language. In this paper, we formulate the problem of cross-lingual knowledge extraction from multilingual Wikipedia sources, and present a novel framework, called WikiCiKE, to solve this problem. An instancebased transfer learning method is utilized to overcome the problems of topic drift and translation errors. Our experimental results demonstrate that WikiCiKE outperforms the monolingual knowledge extraction method and the translation-based method.	compiler;crostata;linked data;wiki;wikipedia	Zhigang Wang;Zhixing Li;Juan-Zi Li;Jie Tang;Jeff Z. Pan	2013			natural language processing;computer science;machine learning;data mining;knowledge extraction;information retrieval;infobox	NLP	-18.683034303735923	-66.69671786322589	24676
4c242ac47291de9a7c1c7948304158b747b288a1	techniques for gigabyte-scale n-gram based information retrieval on personal computers	personal computer;inverted index;information retrieval;space time;hash table;on the fly;high performance	In this paper, we discuss the implementation techniques that allowed us to use n-gram based retrieval methods on a gigabyte corpus on commodity personal computer hardware. While such techniques have been used before in wordbased systems, n-gram systems have different challenges primarily caused by the significantly larger number of unique terms in the corpus. Our work shows that using appropriately tuned gamma compression, extensible hash tables and significant amounts of precalculation on the inverted index allows the indexing of a one gigabyte multilingual corpus in a commodity workstation with 256 MB of memory. Response time for full-document queries on this system is approximately 20 seconds for 1 KB documents while providing the same retrieval precision and recall as previous n-gram based systems. We also discuss the space-time tradeoffs we encountered in building a high performance n-gram based retrieval engine. Because of the larger term count our corpus had nearly 1 million unique terms and over 700 million postings we deliberately chose methods that reduced space at the cost of increasing retrieval time, primarily through onthe-fly calculations and decompression. We found that, perhaps somewhat counter-intuitively, compressed on-disk indices were actually faster than uncompressed indices because of the reduced time necessary to transfer information off the disk.	computer hardware;data compression;document;experiment;gamma correction;gigabyte;grams;hash table;information retrieval;inverted index;n-gram;norm (social);personal computer;precision and recall;responsiveness;tell-tale;text corpus;workstation	Ethan L. Miller;Dan Shen;Junli Liu;Charles K. Nicholas;Ting Chen	1999			hash table;parallel computing;inverted index;computer science;theoretical computer science;operating system;space time;database;distributed computing;programming language;information retrieval	Web+IR	-32.792787364290994	-57.59418846575729	24687
b6ba53b10dd3b115dadde4f77a2d6c45a0e876f6	a graph-based textual entailment method aware of real-world knowledge	semantic similarity;graph based representation;textual entailment	In this paper we propose an unsupervised methodology to solve the textual entailment task, that extracts facts associated to pair of sentences. Those extracted facts are represented as a graph. Then, two graph-based representations of two sentences may be further compared in order to determine the type of textual entailment judgment that they hold. The comparison method is based on graph-based algorithms for finding sub-graphs structures inside another graph, but generalizing the concepts by means of a real world knowledge database. The performance of the approach presented in this paper has been evaluated using the data provided in the Task 1 of the SemEval 2014 competition, obtaining 79i¾?% accuracy.	commonsense knowledge (artificial intelligence);textual entailment	Saúl León;Darnes Vilariño Ayala;David Pinto;Mireya Tovar;Beatríz Beltrán	2015		10.1007/978-3-319-19264-2_21	natural language processing;text graph;semantic similarity;textual entailment;computer science;pattern recognition;data mining;graph;graph database	AI	-26.74729940651299	-64.89403197346297	24703
293d98364755367cfae3c5a7ee16dd52338674a1	skeleton key: image captioning by skeleton-attribute decomposition		Recently, there has been a lot of interest in automatically generating descriptions for an image. Most existing language-model based approaches for this task learn to generate an image description word by word in its original word order. However, for humans, it is more natural to locate the objects and their relationships first, and then elaborate on each object, describing notable attributes. We present a coarse-to-fine method that decomposes the original image description into a skeleton sentence and its attributes, and generates the skeleton sentence and attribute phrases separately. By this decomposition, our method can generate more accurate and novel descriptions than the previous state-of-the-art. Experimental results on the MS-COCO and a larger scale Stock3M datasets show that our algorithm yields consistent improvements across different evaluation metrics, especially on the SPICE metric, which has much higher correlation with human ratings than the conventional metrics. Furthermore, our algorithm can generate descriptions with varied length, benefiting from the separate control of the skeleton and attributes. This enables image description generation that better accommodates user preferences.	algorithm;language model;spice;user (computing)	Yufei Wang;Zhe L. Lin;Xiaohui Shen;Scott Cohen;Garrison W. Cottrell	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.780	word order;computer vision;artificial intelligence;skeleton (computer programming);machine learning;pattern recognition;computer science;semantics;recurrent neural network;sentence;closed captioning	Vision	-17.009073217984476	-70.86564792919832	24726
0e58a497b8331027e5fd3505b4b1e079b130f789	a bottom up approach to category mapping and meaning change		In this article, we use an automated bottom-up approach to identify semantic categories in an entire corpus. We conduct an experiment using a word vector model to represent the meaning of words. The word vectors are then clustered, giving a bottom-up representation of semantic categories. Our main finding is that the likelihood of changes in a word’s meaning correlates with its position within its cluster.	bottom-up parsing;experiment;microsoft word for mac;top-down and bottom-up design;word embedding	Haim Dubossarsky;Yulia Tsvetkov;Chris Dyer;Eitan Grossman	2015			top-down and bottom-up design;artificial intelligence;pattern recognition;mathematics	NLP	-25.001360608725847	-72.3975183527562	24914
b5ead09dff01fed4e5b99a56b5a0297bb500b596	domain adaptive answer extraction for discussion boards	discussion board;answer extraction;sequential patterns;sequential pattern;question answering	Answer extraction from discussion boards is an extensively studied problem. Most of the existing work is focused on supervised methods for extracting answers using similarity features and forum-specific features. Although this works well for the domain or forum data that it has been trained on, it is difficult to use the same models for a domain where the vocabulary is different and some forum specific features may not be available. In this poster, we report initial results of a domain adaptive answer extractor that performs the extraction in two steps: a) an answer recognizer identifies the sentences in a post which are likely to be answers, and b) a domain relevance module determines the domain significance of the identified answer. We use domain independent methodology that can be easily adapted to any given domain with minimum effort.	emoticon;finite-state machine;information extraction;randomness extractor;relevance;supervised learning;vocabulary	Ankur Gandhe;Dinesh Raghu;Rose Catherine	2012		10.1145/2187980.2188097	question answering;computer science;data science;problem domain;data mining;algorithm	NLP	-26.286087303102985	-69.5711493618043	24939
63fdd7422bc7e7f8289c47a3a7a889758608a652	discovering irrelevance in the blogosphere through blog search	link structure;social network services;search problem;google;blogosphere;irrelevance;spam;content based;measurement;query processing;search engines;blog search;blogosphere irrelevance discovering;web sites data mining internet query processing search engines search problems unsolicited e mail;google blog search blogosphere irrelevance discovering web 2 0 technologies information sharing medium search problem web search spam general topic queries;information sharing medium;data mining;splogs;information sharing;web 2 0 technologies;internet;web sites;general topic queries;web search;search problems;unsolicited e mail;communities;google blog search;blogs;blogs search problems measurement search engines google social network services communities;blogosphere splogs irrelevance content based link structure blog search	Web 2.0 technologies have given birth to the blogosphere, which is an information sharing medium by the users for the users. Furthermore, these technologies have also expanded the search problem to a new form of search known as blog search. Similar to Web search, blog search has been affected by spam which affects the quality of search results. This paper approaches the relevant blog problem in the top search results against the general topic queries. It pursues a study of irrelevant blogs appearing in the top search results of Google Blog Search for the blog spot domains. We define metrics for irrelevant blogs by observing the qualitative relevance of content and by analyzing the link structure of those blogs. Our preliminary results show an overall recall of 0.875 with a precision of 1.0 for finding irrelevant blogs in the top 15 search results against six general topic queries on Google Blog Search.	blog;blogger;blogosphere;coupling (computer programming);relevance;search problem;spamming;statistical classification;web 2.0;web search engine	Muhammad Atif Qureshi;Arjumand Younus;Nasir Touheed;M. Shahid Qureshi;Muhammad Saeed	2011	2011 International Conference on Advances in Social Networks Analysis and Mining	10.1109/ASONAM.2011.84	spam blog;spam;organic search;the internet;semantic search;search engine optimization;search problem;computer science;phrase search;data mining;internet privacy;search analytics;world wide web;information retrieval;algorithm;measurement;scraper site	Web+IR	-29.841152549538275	-54.94690684310385	25082
e0c9d2af8c96b5c922c113d705d8f5732c0fa1c9	exploiting multilingual corpora simply and efficiently in neural machine translation		In this paper, we explore a simple approach for “Multi-Source Neural Machine Translation” (MSNMT) which only relies on preprocessing a N-way multilingual corpus without modifying the Neural Machine Translation (NMT) architecture or training procedure. We simply concatenate the source sentences to form a single, long multisource input sentence while keeping the target side sentence as it is and train an NMT system using this preprocessed corpus. We evaluate our method in resource poor as well as resource rich settings and show its effectiveness (up to 4 BLEU using 2 source languages and up to 6 BLEU using 5 source languages) and compare them against existing approaches. We also provide some insights on how the NMT system leverages multilingual information in such a scenario by visualizing attention. We then show that this multi-source approach can be used for transfer learning to improve the translation quality for single-source systems without using any additional corpora thereby highlighting the importance of multilingual-multiway corpora in low resource scenarios. We also extract and evaluate a multilingual dictionary by a method that utilizes the multi-source attention and show that it works fairly well despite its simplicity.	bleu;bilingual dictionary;bitext word alignment;black box;concatenation;display resolution;multi-source;neural machine translation;open-source software;preprocessor;text corpus	Raj Dabre;Fabien Cromierès;Sadao Kurohashi	2018	JIP	10.2197/ipsjjip.26.406	natural language processing;transfer of learning;machine translation;theoretical computer science;deep learning;computer science;artificial intelligence	NLP	-18.56964146152754	-75.03440428076242	25086
01cf13f3bc7a70f1e6c2fc065324bc8e83df9c2b	adaptive ranking of web pages	search engine;quadratic program;web pages;quadratic programming applications;linear constraint;pagerank;learning pagerank;adaptive pagerank determinations;point of view	In this paper, we consider the possibility of altering the PageRank of web pages, from an administrator's point of view, through the modification of the PageRank equation. It is shown that this problem can be solved using the traditional quadratic programming techniques. In addition, it is shown that the number of parameters can be reduced by clustering web pages together through simple clustering techniques. This problem can be formulated and solved using quadratic programming techniques. It is demonstrated experimentally on a relatively large web data set, viz., the WT10G, that it is possible to modify the PageRanks of the web pages through the proposed method using a set of linear constraints. It is also shown that the PageRank of other pages may be affected; and that the quality of the result depends on the clustering technique used. It is shown that our results compared well with those obtained by a HITS based method.	cluster analysis;experiment;pagerank;quadratic programming;viz: the computer game;web page	Ah Chung Tsoi;Gianni Morini;Franco Scarselli;Markus Hagenbuchner;Marco Maggini	2003		10.1145/775152.775203	computer science;theoretical computer science;web page;data mining;world wide web;quadratic programming;information retrieval;search engine	DB	-27.243480099001847	-55.81141674740698	25093
071f47b7bc5830643e31dbed82e0375bf9b26559	ad hoc retrieval experiments using wordnet and automatically constructed thesauri	information retrieval system	This paper describe our method in automatic-adhoc task of TREC-7. We propose a method to improve the performance of information retrieval system by expanded the query using 3 diffferent types of thesaurus. The expansion terms are taken from handcrafted thesaurus (WordNet), co-occurrence-based automatically constructed thesaurus, and syntactically predicate-argument based automatically constructed thesaurus.	hoc (programming language);information retrieval;thesaurus (information retrieval);wordnet	Rila Mandala;Takenobu Tokunaga;Hozumi Tanaka;Akitoshi Okumura;Kenji Satoh	1998			concept search;information retrieval;data mining;computer science;wordnet	NLP	-30.081600023970967	-70.08788162686369	25116
cdf9a355ca5c71651ae8f69bc62598bc7bdfca53	performance evaluation of information retrieval models in bug localization on the method level	vsm performance evaluation;text model developer;method level bug localization;lsi;latent dirichlet analysis;vector space model;query processing;information retrieval;machine learning methods applied to big data analytics;semantic content extraction and analytics languages and techniques;text analysis;inference mechanisms;documentation comments;text categorization and topic recognition discovery collection and extraction of information in big data sources machine learning methods applied to big data analytics natural language processing methodologies performance and benchmarking for big data processing and analytics semantic content extraction and analytics languages and techniques;bug localization programmers;performance and benchmarking for big data processing and analytics;discovery collection and extraction of information in big data sources;lda;large scale integration;statistical analysis;collaboration systems;vsm;big data;text analysis big data inference mechanisms program debugging query processing statistical analysis;latent semantic indexing;big data collaboration systems information retrieval models method level bug localization statistical inference collaboration systems vector space model vsm latent semantic indexing lsi latent dirichlet analysis lda documentation comments bug reports vsm performance evaluation text model developer bug localization programmers;text categorization and topic recognition;statistics;statistical inference;bug reports;information retrieval models;program debugging;natural language processing methodologies;computer bugs;object oriented modeling;sociology;big data collaboration systems;computer bugs sociology statistics large scale integration information retrieval big data object oriented modeling	This study uses statistical inference to compare the performance of three text models used for bug localization in collaboration systems: Vector Space Model (VSM), Latent Semantic Indexing (LSI), and Latent Dirichlet Analysis (LDA) on the method level. After the three models are compared we confirm that VSM is the superior model. We then, point out which external factors i.e. methods lengths, queries lengths, methods documentation comments, products names and components names mentioned in bug reports affect VSM performance. We conclude that VSM performance is positively correlated with most of the tested factors. We believe our results can be helpful to: (i) text models developers, to understand the strengths and limitations of VSM for future development; (ii) bug localization programmers using classical VSM, to understand improved ways to prepare methods extracted from big data collaboration systems and (iii) bug reporters, to follow the most efficient methods presented in this work in reporting bugs to enhance the information retrieval process.	big data;bug tracking system;documentation;information retrieval;performance evaluation;programmer;software bug;text mining;viable system model	Mai Alduailij;Mona Al-Duailej	2015	2015 International Conference on Collaboration Technologies and Systems (CTS)	10.1109/CTS.2015.7210439	statistical inference;latent semantic indexing;software bug;big data;computer science;data science;data mining;value stream mapping;vector space model;information retrieval;statistics	SE	-28.20836196288077	-66.54843089045671	25171
1e3d40b156b472db8f15c325670e2cba2c0ce57b	active information retrieval for linking twitter posts with political debates	information retrieval;active learning;twitter tagging feature extraction labeling media joining processes;feature extraction;textual content active information retrieval linking twitter posts political debates microblogging social networks short messages;text similarity;feature extraction active learning information retrieval text similarity;social networking online information retrieval politics	Users of microblogging social networks produce millions of short messages every day. Retrieving relevant information to a particular event from this sheer volume of data is not a trivial task. In this paper, we present a framework for the retrieval of Twitter posts that are relevant to a set of political debates. Our main contribution is the proposal of a set of strategies for involving the user in the retrieval process, so that by presenting to her meaningful posts to be labeled, the method achieves a noticeably higher accuracy. The correct retrieval or labeling could be provided by an external information source such as a domain expert, or simulated with an oracle. A key aspect of active retrieval methods is to request the labels of the instances that help improve the retrieval accuracy the most, while keeping the number of labeling requests to a minimum. The proposed strategies for selecting labeling requests make use of the textual content of tweets and their structural information. The experimental results show the advantages of the proposed methods and the effectiveness of the selection strategies for involving the user in the retrieval process.	automatic target recognition;experiment;hashtag;information retrieval;information source;mike lesser;oracle database;simulation;social network;subject-matter expert;test set	Raheleh Makki;Axel J. Soto;Stephen Brooks;Evangelos E. Milios	2015	2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)	10.1109/ICMLA.2015.142	relevance;cognitive models of information retrieval;feature extraction;computer science;machine learning;data mining;adversarial information retrieval;active learning;world wide web;information retrieval;human–computer information retrieval	Web+IR	-25.47694408717134	-54.48146615296712	25203
4fe8abea6aae2e362c525734f8fc3758a8c490d3	retrieving geo-location of videos with a divide & conquer hierarchical multimodal approach	flickr;location;video annotation;placing task;geotags	This paper presents a strategy to identify the geographic location of videos. First, it relies on a multi-modal cascade pipeline that exploits the available sources of information, namely the user's upload history, his social network and a visual-based matching technique. Second, we present a novel divide & conquer strategy to better exploit the tags associated with the input video. It pre-selects one or several geographic area of interest of higher expected relevance and performs a deeper analysis inside the selected area(s) to return the coordinates most likely to be related to the input tags. The experiments were conducted as part of the MediaEval 2012 Placing Task. Our approach, which differs significantly from the other submitted techniques, achieves the best results on this benchmark when considering the same amount of external information, i.e. when not using any gazetteers nor any other kind of external information.	benchmark (computing);experiment;geographic coordinate system;geolocation;modal logic;multimodal interaction;relevance;social network;upload	Michele Trevisiol;Hervé Jégou;Jonathan Delhumeau;Guillaume Gravier	2013		10.1145/2461466.2461468	computer vision;computer science;data mining;geotagging;multimedia;location;world wide web	Web+IR	-25.57234129862134	-52.92936503622627	25212
a3e26a5135c3f1ef22b90e9f892729c81eeb8aad	constructing test collections by inferring document relevance via extracted relevant information	information retrieval;relevance assessment;nuggets;evaluation	"""The goal of a typical information retrieval system is to satisfy a user's information need---e.g., by providing an answer or information """"nugget""""---while the actual search space of a typical information retrieval system consists of documents---i.e., collections of nuggets. In this paper, we characterize this relationship between nuggets and documents and discuss applications to system evaluation.  In particular, for the problem of test collection construction for IR system evaluation, we demonstrate a highly efficient algorithm for simultaneously obtaining both relevant documents and relevant information. Our technique exploits the mutually reinforcing relationship between relevant documents and relevant information, yielding document-based test collections whose efficiency and efficacy exceed those of typical Cranfield-style test collections, while also generating sets of highly relevant information."""	algorithm;information needs;information retrieval;relevance	Shahzad Rajput;Matthew Ekstrand-Abueg;Virgil Pavlu;Javed A. Aslam	2012		10.1145/2396761.2396783	relevance;computer science;evaluation;data mining;world wide web;information retrieval	Web+IR	-33.58943103408913	-57.915851120206376	25239
20f7e203460c02cb299aa69e6105e20434a8952d	spatial constraint for image location estimation	position descriptor;visual word group;location estimation;bag of words	Nowadays, image location has been widely used in many application scenarios for large geo-tagged image corpora. As to images which are not geographically tagged, we can estimate their locations with the help of the large geo-tagged image set by content based image retrieval. In this paper, we propose a global feature clustering and local feature refinement based image location estimation approach. We exploit spatial information by processing useful visual words. In this process, visual word groups are generated. Moreover to improve the retrieval performance, spatial constraint is utilized to code the relative position of visual words. Here we generate a position descriptor for each visual word. Experiments show the effectiveness of our proposed approach.	cluster analysis;content-based image retrieval;experiment;microsoft word for mac;refinement (computing);text corpus;visual word	Yisi Zhao;Xueming Qian	2015		10.1145/2671188.2749327	computer vision;visual word;computer science;bag-of-words model;machine learning;pattern recognition;information retrieval	Vision	-16.99459125473237	-58.57714758247453	25263
79451c4a5cef327416667c8e6b27bf678698f7d5	towards effective parsing with neural networks: inherent generalisations and bounded resource effects	syntactic parsing;neural networks;systematicity;resource effects;article;neural network;structured representations	This article explores how the effectiveness of learning to parse with neural networks can be improved by including two architectural features relevant to language: generalisations across syntactic constituents and bounded resource effects. A number of neural network parsers have recently been proposed, each with a different approach to the representational problem of outputting parse trees. In addition, some of the parsers have explicitly attempted to capture an important regularity within language, which is to generalise information across syntactic constituents. A further property of language is that natural bounds exist for the number of constituents which a parser need retain for later processing. Both the generalisations and the resource bounds may be captured in architectural features which enhance the effectiveness and efficiency of learning to parse with neural networks. We describe a number of different types of neural network parser, and compare them with respect to these two features. These features are both explicitly present in the Simple Synchrony Network parser, and we explore and illustrate their impact on the process of learning to parse in some experiments with a recursive grammar.	artificial neural network;dmz (computing);experiment;natural language;parse tree;parsing;recursion;recursive grammar	Peter C. R. Lane;James Henderson	2003	Applied Intelligence	10.1023/A:1023820807862	natural language processing;parser combinator;speech recognition;computer science;artificial intelligence;machine learning;parsing;top-down parsing;artificial neural network	NLP	-18.307293662764895	-73.56113169072698	25322
0db01127c3a919df8891d85c270109894c67e3f8	exploiting morphological regularities in distributional word representations		We present an unsupervised, language agnostic approach for exploiting morphological regularities present in high dimensional vector spaces. We propose a novel method for generating embeddings of words from their morphological variants using morphological transformation operators. We evaluate this approach on MSR word analogy test set (Mikolov et al., 2013d) with an accuracy of 85% which is 12% higher than the previous best known system.	language-independent specification;test set;unsupervised learning;word lists by frequency	Arihant Gupta;Syed Sarfaraz Akhtar;Avijit Vajpayee;Arjit Srivastava;Madan Gopal Jhanwar;Manish Shrivastava	2017			machine learning;natural language processing;computer science;artificial intelligence	NLP	-20.59078655548765	-72.19542214532069	25344
7d1384643a320b6ceb43ca8cce2460fc386ceef8	gcon: a graph-based technique for resolving ambiguity in query translation candidates	meetings and proceedings;book chapter;translation;co occurrence measure;disambiguation;graph analysis;ambiguity;query translation;cross language information retrieval	In the field of cross-language information retrieval (CLIR), the resolution of lexical ambiguity is a key challenge. Common mechanisms for the translation of query terms from one language to another typically produce a set of possible translation candidates, rather than some authoritative result. Correctly reducing a list of possible candidates down to a single translation is an enduring problem. Thus far, solutions have concentrated upon the use of the use of term co-occurrence information to guide the process of resolving translation-based ambiguity. In this paper we introduce a new disambiguation strategy which employs a graph-based analysis of generated co-occurrence data to determine the most appropriate translation for a given term.	cross-language information retrieval;word-sense disambiguation	Dong Zhou;Mark Truran;Tim J. Brailsford;Helen Ashman;James Goulding	2008		10.1145/1363686.1364054	natural language processing;translation;power graph analysis;transfer-based machine translation;example-based machine translation;computer science;database;rule-based machine translation;world wide web;information retrieval	Web+IR	-29.229334747395928	-67.12935203819464	25364
649f95d1b5fd72c532f6f28ab8e2c630a162ec3b	representations and architectures in neural sentiment analysis for morphologically rich languages: a case study from modern hebrew		This paper empirically studies the effects of representation choices on neural sentiment analysis for Modern Hebrew, a morphologically rich language (MRL) for which no sentiment analyzer currently exists. We study two dimensions of representational choices: (i) the granularity of the input signal (token-based vs. morpheme-based), and (ii) the level of encoding of vocabulary items (string-based vs. character-based). We hypothesise that for MRLs, languages where multiple meaning-bearing elements may be carried by a single space-delimited token, these choices will have measurable effects on task perfromance, and that these effects may vary for different architectural designs: fully-connected, convolutional or recurrent. Specifically, we hypothesize that morpheme-based representations will have advantages in terms of their generalization capacity and task accuracy, due to their better OOV coverage. To empirically study these effects, we develop a new sentiment analysis benchmark for Hebrew, based on 12K social media comments, and provide two instances thereof: token-based and morpheme-based. Our experiments show that the effect of representational choices vary with architectural types. While fully-connected and convolutional networks slightly prefer token-based settings, RNNs benefit from a morphemebased representation, in accord with the hypothesis that explicit morphological information may help generalize. Our endeavor also delivers the first state-of-the-art broad-coverage sentiment analyzer for Hebrew, with over 89% accuracy, alongside an established benchmark to further study the effects of linguistic representation choices on neural networks’ task performance.	artificial neural network;benchmark (computing);delimiter;experiment;media resource locator;sentiment analysis;social media;text-based (computing);vocabulary	Adam Amram;Anat Ben-David;Reut Tsarfaty	2018			artificial intelligence;hebrew;natural language processing;computer science;sentiment analysis	NLP	-18.59236455590551	-73.55908519160982	25370
38f0bc6399a3d5826bbb2101b35bd7d8f0653223	transferring coreference chains through word alignment		This paper investigates the problem of automatically annotating resources with NP coreference information using a parallel corpus, English-Romanian, in order to transfer, through word alignment, coreference chains from the English part to the Romanian part of the corpus. The results show that we can detect Romanian referential expressions and coreference chains with over 80% F-measure, thus using our method as a preprocessing step followed by manual correction as part of an annotation effort for creating a large Romanian corpus with coreference information is worthwhile.	bitext word alignment;data structure alignment;parallel text;preprocessor	Oana Postolache;Dan Cristea;Constantin Orasan	2006			speech recognition;artificial intelligence;natural language processing;computer science;preprocessor;expression (mathematics);coreference;annotation	NLP	-25.048857278812697	-75.66573950993369	25391
63d5aed87e1fcf2e18402770215c620860b37c4b	cross-cultural differences in categorical memory errors	categories;long term memory;memory errors;cognition;culture	Cultural differences occur in the use of categories to aid accurate recall of information. This study investigated whether culture also contributed to false (erroneous) memories, and extended cross-cultural memory research to Turkish culture, which is shaped by Eastern and Western influences. Americans and Turks viewed word pairs, half of which were categorically related and half unrelated. Participants then attempted to recall the second word from the pair in response to the first word cue. Responses were coded as correct, as blanks, or as different types of errors. Americans committed more categorical errors than did Turks, and Turks mistakenly recalled more non-categorically related list words than did Americans. These results support the idea that Americans use categories either to organize information in memory or to support retrieval strategies to a greater extent than Turks and suggest that culture shapes not only accurate recall but also erroneous distortions of memory.	attempt;categories;distortion;memory disorders;microsoft word for mac	Aliza J. Schwartz;Aysecan Boduroglu;Angela H. Gutchess	2014	Cognitive science	10.1111/cogs.12109	psychology;cognitive psychology;cognition;developmental psychology;memory errors;encoding specificity principle;communication;social psychology;culture	HCI	-8.850482709802048	-78.49836561033806	25419
f7e21caff88064459456dc35a141eaba4fe7bd62	natural language inspired approach for handwritten text line detection in legacy documents	text layout;natural language processing;document layout analysis;handwritten text recognition;handwritten text line detection;text line detection accuracy;text line detection;vertical layout language model;handwritten legacy document;parallel text line	Document layout analysis is an important task needed for handwritten text recognition among other applications. Text layout commonly found in handwritten legacy documents is in the form of one or more paragraphs composed of parallel text lines. An approach for handwritten text line detection is presented which uses machinelearning techniques and methods widely used in natural language processing. It is shown that text line detection can be accurately solved using a formal methodology, as opposed to most of the proposed heuristic approaches found in the literature. Experimental results show the impact of using increasingly constrained ”vertical layout language models” in text line detection accuracy.	document layout analysis;edge detection;heuristic (computer science);language model;natural language processing;optical character recognition;parallel text;transcription (software)	Vicente Bosch;Alejandro Héctor Toselli;Enrique Vidal	2012			natural language processing;text graph;speech recognition;computer science;document layout analysis;noisy text analytics;data mining	NLP	-24.05561367565955	-78.02276412541816	25421
0075fa4252239cd6738465401caff6917610fe2d	the first version of the oaei complex alignment benchmark		We present the first version of the complex benchmark of the Ontology Alignment Evaluation Initiative campaigns. This benchmark is composed of four datasets from different domains (conference, hydrology, geoscience and agronomy) and covers different evaluation strategies.	benchmark (computing);ontology alignment	Élodie Thiéblin;Michelle Cheatham;Cássia Trojahn dos Santos;Ondrej Sváb-Zamazal;Lu Zhou	2018				ML	-32.3619171289456	-65.00844777774526	25464
3974d74645302570096722ab9358dc52a846b048	mining arguments from 19th century philosophical texts using topic based modelling		In this paper we look at the manual analysis of arguments and how this compares to the current state of automatic argument analysis. These considerations are used to develop a new approach combining a machine learning algorithm to extract propositions from text, with a topic model to determine argument structure. The results of this method are compared to a manual analysis.	algorithm;approximation;automatic identification and data capture;crowdsourcing;deep learning;heuristic (computer science);inferential programming;inferential theory of learning;machine learning;np-completeness;next-generation network;simple features;surround sound;test data;topic model	John Lawrence;Chris Reed;Colin Allen;Simon McAlister;Andrew Ravenscroft	2014			natural language processing;computer science;artificial intelligence;algorithm	NLP	-31.36254159013172	-77.15355577985679	25475
8cca312a319bfba154106ee37844d76ad1762e91	cerebroviz: an r package for anatomical visualization of spatiotemporal brain data		Summary Spatiotemporal transcriptomic profiling has provided valuable insight into the patterning of gene expression throughout the human brain from early fetal development to adulthood. When combined with prior knowledge of a disease's age at onset and region-specificity, these expression profiles have provided the necessary context to both strengthen putative gene-disease associations and infer new associations. While a wealth of spatiotemporal expression data exists, there are currently no tools available to visualize this data within the anatomical context of the brain, thus limiting the intuitive interpretation of many such findings. We present cerebroViz, an R package to map spatiotemporal brain data to vector graphic diagrams of the human brain. Our tool allows rapid generation of publication-quality figures that highlight spatiotemporal trends in the input data, while striking a balance between usability and customization. cerebroViz is generalizable to any data quantifiable at a brain-regional resolution and currently supports visualization of up to thirty regions of the brain found in databases such as BrainSpan, GTEx and Roadmap Epigenomics.   Availability and Implementation cerebroViz is freely available through GitHub ( https://github.com/ethanbahl/cerebroViz ). The tutorial is available at http://ethanbahl.github.io/cerebroViz/.   Contacts ethan-bahl@uiowa.edu or jacob-michaelson@uiowa.edu.   Supplementary information Supplementary data are available at Bioinformatics online.	bioinformatics;dvd region code;database;databases;diagram;epigenomics;fetal development of the mammalian embryo or fetus;gene expression profiling;genotype-tissue expression program;geographic information systems;imagery;inference;mental association;nih roadmap initiative tag;onset (audio);parkinson disease;parsing expression grammar;r language;sensitivity and specificity;usability;vector graphics	Ethan Bahl;Tanner Koomar;Jacob J. Michaelson	2016		10.1093/bioinformatics/btw726	clinical psychology;visualization;data science;human brain;limiting;computer science;bioinformatics	Visualization	-6.249040875041129	-60.95852191745144	25493
c37366f10c81798961b92fdc7f801e2effc337cc	evaluation of automatic caption segmentation	caption segmentation	Captions are typically segmented in a way that respects grammatical boundaries and makes them more readable. However, the growth of online video content with captions generated from transcripts means that this segmentation process is often ignored. This study evaluates the effects of text segmentation on caption readability, and proposes a program to automatically segment captions using a parser. The parser-segmented captions readability is also evaluated and compared to human-segmented captions and arbitrarily- segmented captions. Results indicate segmentation influences sentence recall, though other wise little difference is found between the different kinds of captioning.	digital video;human-readable medium;text segmentation;video clip	James M. Waller;Raja S. Kushalnagar	2016		10.1145/2982142.2982205	natural language processing;speech recognition;computer science;multimedia	NLP	-24.165165826288476	-80.02811467924413	25522
8cdcc319e6b45b3490124ce4e967a68a855401ea	umass amherst and ut austin @ the trec 2009 relevance feedback track	feature extraction;internet;algorithms;feedback;sampling;information retrieval;accuracy;expansion	We present a new supervised method for estimating term-based retrieval models and apply it to weight expansion terms from relevance feedback. While previous work on supervised feedback [Cao et al., 2008] demonstrated significantly improved retrieval accuracy over standard unsupervised approaches [Lavrenko and Croft, 2001, Zhai and Lafferty, 2001], feedback terms were assumed to be independent in order to reduce training time. In contrast, we adapt the AdaRank learning algorithm [Xu and Li, 2007] to simultaneously estimate parameterization of all feedback terms. While not evaluated here, the method can be more generally applied for joint estimation of both query and feedback terms. To apply our method to a large web collection, we also investigate use of sampling to reduce feature extraction time while maintaining robust learning.	algorithm;consistency model;experiment;feature extraction;markov random field;negative feedback;positive feedback;relevance feedback;sampling (signal processing);supervised learning;ut-vpn;unsupervised learning;web search engine	Marc-Allen Cartright;Jangwon Seo;Matthew Lease	2009			computer science;the internet;information retrieval;data mining;sampling (statistics);feature extraction;parametrization;machine learning;artificial intelligence;relevance feedback	Web+IR	-18.279680082870595	-63.086272206587395	25534
450d21ff5519eaa481a50a7767f5b29c3a694047	predicting co-occurrence restrictions by using semantic classifications in the lexicon	expert system;general principle;russian word;natural language processing;non-obligatory dependent;combinatory option;benefactive object;semantic class;semantic feature;co-occurrence restriction;semantic classification	"""In this paper we investigate general principles of constructing semantic classifications that yield useful predictions combinatory options Several semantic Russian words are concernin~ of words. classes of discussed, implemented in an expert system named """"Lex i co~raphe r"""", the """"Lexicographer"""" is supposed to provide its users with all kind of information concerning some 15.000 most common Russian words. Alon~ with morphological, syntactic infor.~tion conventional system information and semantic usually stored in dictionaries, the should contain about referential characteristics of words and about restraints in combinability with other words in syntactic constructions of different types. In its final version """"Lexicographer"""" should provide the users with all sorts of bibliographical information system beinff conceived as an aid both in the area of natural language processin~ arid in traditional lexicography. Semantic Features proposed regulate co-occurence of verbs with their non-obli?atory dependents-such as Modifiers of place or time; Instrumental and Benefactive objects and the like. (concern i n~ both i nd i v i dual words and semantic classes of words) and with concordances made on the bas i s of a suffi c i ent ly re presentat i ve corpus of Russian texts. One of the basic components of the system is its lexicon; the lexicon contains information not only about individual lexer~s, but also about sen~ntic and syntactic classes of lexemes. Thus, for nominal lexe~es such features are ~iven as: """"NATURAL CLASS"""", """"ARTEFACT"""", """"MASS TERM"""", """"SET"""", """"BODY PART"""" and the like."""	concordance (publishing);dictionary;expert system;information system;lex (software);lexical analysis;lexicography;lexicon;natural language;system information (windows);visual artifact	Elena V. Paducheva;Ekaterina V. Rakhilina	1990			natural language processing;semantic similarity;semantic computing;speech recognition;computer science;semantic compression;linguistics	AI	-32.66213958900052	-71.62044479013997	25573
556ddc9887c9754b2ab77e5e2831cb7eb96af992	creating a coreference resolution system for italian.	system design;coreference resolution;languages other than english;open source	This paper summarizes our work on creating a full-scale core ference resolution (CR) system for Italian, using BART – an o pen-source modular CR toolkit initially developed for English corpora . We discuss our experiments on language-specific issues of t he task. As our evaluation experiments show, a language-agnostic syst em (designed primarily for English) can achieve a performan ce level in high forties (MUC F-score) when re-trained and tested on a new lan gu ge, at least on gold mention boundaries. Compared to this level, we can improve our F-score by around 10% introducing a small num ber of language-specific changes. This shows that, with a mod ular coreference resolution platform, such as BART, one can stra ightforwardly develop a family of robust and reliable syste ms for various languages. We hope that our experiments will encourage rese arch rs working on coreference in other languages to create their own full-scale coreference resolution systems – as we have ment ion d above, at the moment such modules exist only for very fe w languages other than English.	experiment;field electron emission;full scale;language-independent specification;message understanding conference;mod database;text corpus	Massimo Poesio;Olga Uryupina;Yannick Versley	2010			natural language processing;speech recognition;computer science;linguistics;systems design	NLP	-28.5934927213681	-75.91087129044914	25608
6297f2fa76e87b7f073870714e26022966764fc2	transpath: representation learning for heterogeneous information networks via translation mechanism		In this paper, we propose a novel network representation learning model TransPath to encode heterogeneous information networks (HINs). Traditional network representation learning models aim to learn the embeddings of a homogeneous network. TransPath is able to capture the rich semantic and structure information of a HIN via meta-paths. We take advantage of the concept of translation mechanism in knowledge graph which regards a meta-path, instead of an edge, as a translating operation from the first node to the last node. Moreover, we propose a user-guided meta-path sampling strategy which takes users’ preference as a guidance, which could explore the semantics of a path more precisely, and meanwhile improve model efficiency via the avoidance of other noisy and meaningless meta-paths. We evaluate our model on two large-scale real-world data sets database systems and logic programming (DBLP) and YELP, and two benchmark tasks similarity search and node classification. We observe that TransPath outperforms other state-of-the-art baselines consistently and significantly.	benchmark (computing);database;encode;feature learning;knowledge graph;logic programming;machine learning;sampling (signal processing);similarity search	Yang Fang;Xiang Zhao;Zhen Tan;Weidong Xiao	2018	IEEE Access	10.1109/ACCESS.2018.2827121	task analysis;and gate;machine learning;distributed computing;semantics;noise measurement;data set;computer science;nearest neighbor search;artificial intelligence;feature learning;graph	AI	-16.114848148438803	-66.36849846803855	25614
379197f981b871e98d4ea8f4e7a9fc60ab2d7990	a probability model for image annotation	image autoannotation predicate;relevance model;probability;cross media relevance model probability model automatic image annotation image retrieval by keywords statistical model image autoannotation predicate image visual tokens word to word correlation conditional probability;automatic image annotation;image annotation;statistical model;image visual tokens;probability working environment noise oceans image retrieval clouds vocabulary birds snow digital images digital photography;statistical analysis;statistical analysis image retrieval probability;image retrieval by keywords;automatic annotation;visual features;word to word correlation;probability model;conditional probability;cross media relevance model;image retrieval	Automatic image annotation is a promising solution to enable more effective image retrieval by keywords. Traditionally, statistical models for image auto-annotation predicate each annotated keyword independently without considering the correlation of words. In this paper, we propose a novel probability model, in which the correspondence between keywords and image visual tokens/regions and the word-to-word correlation are well combined. We employ the conditional probability to express two kinds of correlation uniformly and obtain the correspondence between keyword and visual feature with the cross-media relevance model (CMRM). Experiments conducted on standard Corel dataset demonstrate the effectiveness of the proposed method for image automatic annotation.	automatic image annotation;corel linux;image retrieval;relevance;statistical model	Yong Ge;Richang Hong;Zhiwei Gu;Rong Zhang;Xiuqing Wu	2007	2007 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2007.4284778	natural language processing;statistical model;conditional probability;computer science;pattern recognition;probability;automatic image annotation;information retrieval;statistics	Vision	-15.716378602963996	-62.76323588489914	25625
db6c932acb3fb8dda8b0701f617da7798e583eef	investigating the semantics of frame elements	informative sense label;high number;classification precision;enriched fe information;frame element;frame elements;existing semantic role repository;semantic role;generic semantic type;typical fe filler	Compared to other existing semantic role repositories, FrameNet is characterized by an extremely high number of roles or Frame Elements (FEs), which amount to 8,884 in the last resource release. This represents an interesting issue to investigate both from a theoretical and a practical point of view. In this paper, we analyze the semantics of frame elements by automatically assigning them a set of synsets characterizing the typical FE fillers. We show that the synset repository created for each FE can adequately generalize over the fillers, while providing more informative sense labels than just one generic semantic type. We also evaluate the impact of the enriched FE information on a semantic role labeling task, showing that it can improve classification precision, though at the cost of lower recall.	babelnet;framenet;information;semeval;semantic role labeling;semiconductor industry;supervised learning;synonym ring;wikipedia;wordnet;yago	Sara Tonelli;Volha Bryl;Claudio Giuliano;Luciano Serafini	2012		10.1007/978-3-642-33876-2_13	computer science;data mining;information retrieval	NLP	-26.311747353287608	-70.4016598907653	25642
fb2caf2fbb19c506941371402ea00d2d4a3afffc	temporal classification of medical events	sequence information;maximum entropy classifier;temporal classification;discrete time-bins;overall tagging accuracy;temporal feature;hospital admission date;clinical narrative;present result;conditional random fields;medical event	We investigate the task of assigning medical events in clinical narratives to discrete time-bins. The time-bins are defined to capture when a medical event occurs relative to the hospital admission date in each clinical narrative. We model the problem as a sequence tagging task using Conditional Random Fields. We extract a combination of lexical, section-based and temporal features from medical events in each clinical narrative. The sequence tagging system outperforms a system that does not utilize any sequence information modeled using a Maximum Entropy classifier. We present results with both handtagged as well as automatically extracted features. We observe over 8% improvement in overall tagging accuracy with the inclusion of sequence information.	baseline (configuration management);conditional random field;information;lexicon;multinomial logistic regression;principle of maximum entropy;product binning	Preethi Raghavan;Eric Fosler-Lussier;Albert M. Lai	2012			sequence labeling;geography;pattern recognition;data mining;information retrieval	NLP	-24.137394889346922	-70.29874023746912	25643
93ebec91227046b156860dd3cd1dd661131eb152	multilingual part-of-speech tagging with weightless neural networks	part of speech tagging;weightless neural networks	Training part-of-speech taggers (POS-taggers) requires iterative time-consuming convergence-dependable steps, which involve either expectation maximization or weight balancing processes, depending on whether the tagger uses stochastic or neural approaches, respectively. Due to the complexity of these steps, multilingual part-of-speech tagging can be an intractable task, where as the number of languages increases so does the time demanded by these steps. WiSARD (Wilkie, Stonham and Aleksander's Recognition Device), a weightless artificial neural network architecture that proved to be both robust and efficient in classification tasks, has been previously used in order to turn the training phase faster. WiSARD is a RAM-based system that requires only one memory writing operation to train each sentence. Additionally, the mechanism is capable of learning new tagged sentences during the classification phase, on an incremental basis. Nevertheless, parameters such as RAM size, context window, and probability bit mapping, make the multilingual part-of-speech tagging task hard. This article proposes mWANN-Tagger (multilingual Weightless Artificial Neural Network tagger), a WiSARD POS-tagger. This tagger is proposed due to its one-pass learning capability. It allows language-specific parameter configurations to be thoroughly searched in quite an agile fashion. Experimental evaluation indicates that mWANN-Tagger either outperforms or matches state-of-art methods in accuracy with very low standard deviation, i.e., lower than 0.25%. Experimental results also suggest that the vast majority of the languages can benefit from this architecture.		Hugo C. C. Carneiro;Felipe Maia Galvão França;Priscila Machado Vieira Lima	2015	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2015.02.012	speech recognition;computer science;artificial intelligence;machine learning	NLP	-18.558294811854605	-76.17719885793521	25654
89035d459e1a47791ffb3d0bbdba4adf291b43cb	research on text network representation	syntactic network;text network representation;electronic text document;vector space model;text analysis semantic networks;text processing;semantic network;text analysis;semantic networks;text classification;co occurrence network;lexical network;co occurrence network text network representation text processing text structure information lexical network syntactic network semantic network text classification electronic text document;text structure information;indexing text processing text categorization lifting equipment internet information filtering information filters resource management frequency information retrieval	Text representation is the basis of text processing. Most current text representation model didn't consider of the words' relations and result in the loss of text's structure information, which is important to understand the text. This paper proposed a novel text representation model, which uses lexical network to represent the text and retains the text's structure. According to the different levels of words' relations, co-occurrence network, syntactic network and semantic network are introduced. The text network representation was applied into text classification to measure the representation ability of this model. The experiment result shows that our text network representation is prior to vector space model.	document classification;semantic network	Jianyi Liu;Jinghua Wang;Cong Wang	2008	2008 IEEE International Conference on Networking, Sensing and Control	10.1109/ICNSC.2008.4525402	natural language processing;text graph;text simplification;full text search;text mining;explicit semantic analysis;computer science;artificial intelligence;noisy text analytics;pattern recognition;semantic network;information retrieval;co-occurrence networks	NLP	-25.314596757751083	-64.37961266023557	25699
55555629226f3750fde03e47ae3f5bf10dbd9d7b	enhancing twitter spam accounts discovery using cross-account pattern mining	spam pattern;unsolicited electronic mail;spam tweets;tweeting pattern;spam detection twitter spam accounts discovery cross account pattern mining sybil accounts fake account advertising campaign url resource sharing spam detector spamming activity spam pattern tweeting pattern real data evaluation spam tweets;sybil accounts;real data evaluation;spam detector;data mining;companies;resource sharing;social networking online;fake account;twitter unsolicited electronic mail uniform resource locators companies videos correlation;cross account pattern mining;unsolicited e mail;correlation;twitter spam accounts discovery;twitter;uniform resource locators;unsolicited e mail advertising data mining security of data social networking online;security of data;spam detection;videos;spamming activity;url;advertising;advertising campaign	Twitter generates the majority of its revenue from advertising. Third parties usually pay to have their products advertised on Twitter through tweets, accounts and trends. However, spammers can use Sybil accounts (fake accounts) to advertise and avoid paying for it. Sybil accounts are highly active on Twitter performing advertising campaigns to serve their clients. They aggressively try to reach a large audience to maximize their influence. These accounts have similar behavior if controlled by the same master. Most of their spam tweets include a shortened URL to trick users into clicking on it. Also, since they share resources with each other, they tend to tweet similar trending topics to attract a larger audience. However, some Sybil accounts do not spam aggressively to avoid being detected, rendering it difficult for traditional spam detectors to be effective in detecting Sybil accounts with low spamming activities. In this paper, we investigate additional criteria - spam patterns, to measure the similarity across accounts on Twitter. We propose an algorithm to define the correlation among accounts by investigating their tweeting patterns and content. Our real data evaluation reveals that, given known some initially labelled spam tweets, this approach can detect additional spam tweets and spam accounts that are correlated to the initially labelled spam tweets, which are not detected by traditional spam detection approaches otherwise.	algorithm;centrality;computation;data mining;sensor;social network;spamming;sybil attack	Ioana-Alexandra Bara;Carol J. Fung;Thang N. Dinh	2015	2015 IFIP/IEEE International Symposium on Integrated Network Management (IM)	10.1109/INM.2015.7140327	uniform resource locator;forum spam;shared resource;advertising campaign;computer science;social spam;spambot;internet privacy;world wide web;correlation;sping	Security	-20.365950868722596	-54.88368604829483	25734
69e01aa6ee9fa49a0b3dd040ec6aa67b988012ca	finding translations in scanned book collections	score function;book collections;translation detection;bepress selected works;unique words;information storage and retrieval digital libraries translation detection sequence alignment unique words book collections;word order;longest common subsequence;its sequences;paired comparison;sequence alignment	This paper describes an approach for identifying translations of books in large scanned book collections with OCR errors. The method is based on the idea that although individual sentences do not necessarily preserve the word order when translated, a book must preserve the linear progression of ideas for it to be a valid translation. Consider two books in two different languages, say English and German. The English book in the collection is represented by the sequence of words (in the order they appear in the text) which appear only once in the book. Similarly, the book in German is represented by its sequence of words which appear only once. An English-German dictionary is used to transform the word sequence of the English book into German by translating individual words in place. It is not necessary to translate all the words and this method works even with small dictionaries. Both sequences are now in German and can, therefore, be aligned using a Longest Common Subsequence (LCS) algorithm. We describe two scoring functions TRANS-cs and TRANS-its which account for both the LCS length and the lengths of the original word sequences. Experiments demonstrate that TRANS-its is particularly successful in finding translations of books and outperforms several baselines including metadata search based on matching titles and authors. Experiments performed on a Europarl parallel corpus for four language pairs, English-Finnish, English-French, English-German, English-Spanish, and a scanned book collection of 50K English-German books show that the proposed method retrieves translations of books with an average MAP score of 1.0 and a speed of 10K book pair comparisons per second on a single core.	algorithm;baseline (configuration management);book;color gradient;dictionary;longest common subsequence problem;optical character recognition;parallel text;scoring functions for docking	Ismet Zeki Yalniz;R. Manmatha	2012		10.1145/2348283.2348347	word order;natural language processing;pairwise comparison;speech recognition;computer science;sequence alignment;longest common subsequence problem;score;world wide web;information retrieval;statistics	NLP	-30.331594692176537	-72.14350353534148	25747
625077ff90c8c19b030f68428a5bb5e59fa9dd29	nlel-maat at clef-ip	intellectual property;passage retrieval;question answering	This report presents the work carried out at NLE Lab for the CLEF-IP 2009 competition. We adapted the JIRS passage retrieval system for this task, with the objective to exploit the stylistic characteristics of the patents. Since JIRS was developed for the Question Answering task and this is the first time its model was used to compare entire documents, we had to carry out some transformations on the patent documents. The obtained results are not good and show that the modifications adopted in order to use JIRS represented a wrong choice, compromising the performance of the retrieval system.	question answering	Santiago Correa;Davide Buscaldi;Paolo Rosso	2009		10.1007/978-3-642-15754-7_52	natural language processing;question answering;computer science;artificial intelligence;data mining;world wide web;information retrieval;intellectual property	Web+IR	-32.06463985496893	-65.04597562598128	25808
5a1d3fe11d26eacd9c862f4a3099407df578614a	automated discovery of dependencies between logical components in document image understanding	image recognition;document image understanding;visual models;image databases;optical character recognition;image understanding;publishing;computational strategy;logical component dependence discovery;text analysis;ocr document image understanding logical component dependence discovery document image recognition visual models machine learning computational strategy separate and parallel conquer search multi page documents wisdom system;image recognition text analysis image analysis optical character recognition software xml system testing publishing image databases digital images optical devices;divide and conquer methods;optical character recognition software;visual modeling;machine learning;wisdom system;search problems document image processing optical character recognition learning artificial intelligence divide and conquer methods;xml;ocr;document image processing;system testing;image analysis;search problems;document processing;learning artificial intelligence;multi page documents;separate and parallel conquer search;digital images;document image recognition;optical devices	Document image understanding denotes the recognition of semantically relevant components in the layout extracted from a document image. This recognition process is based on some visual models, whose manual specification can be a highly demanding task. In order to automatically acquire these models, we propose the application of machine learning techniques. In this paper, problems raised by possible dependencies between concepts to be learned are illustrated and solved with a computational strategy based on the separate-and-parallel-conquer search. The approach is tested on a set of real multi-page documents processed by the system WISDOM++. New results confirm the validity of the proposed strategy and show some limits of the learning system used in this work.	computation;computer vision;document processing;engine control unit;first-order logic;first-order predicate;machine learning;predicate (mathematical logic);semantics (computer science);unary operation	Donato Malerba;Floriana Esposito;Francesca A. Lisi;Oronzo Altamura	2001		10.1109/ICDAR.2001.953778	natural language processing;computer vision;xml;image analysis;speech recognition;document processing;computer science;machine learning;data mining;publishing;optical character recognition;system testing;information retrieval;digital image	AI	-13.744632560815923	-63.348722840829275	25858
aed412fc5409acd46e375bc854d47d58e11be250	hapax: probabilistic part-of-speech tagging in xquery and xforms		• They work in batch mode, not interactively. • They generally model text as a flat sequence of characters; for most, XML markup must be removed before data are submitted to the tagger, and afterwards merged back into the output. • They are consequently unable to exploit information in XML markup — for example, that “Brown” is here a proper noun and “Essex” there a place name. • They tag each word token in the input with their best guess at the correct POS; by default, they do not distinguish lowand highprobability guesses. • They cannot accept partially tagged input. In consequence, the human annotator cannot help them by providing hints on some words. • They operate on words, not smaller segments.	batch processing;brill tagger;hapax legomenon;interactivity;markup language;part-of-speech tagging;xforms toolkit;xml;xquery	C. M. Sperberg-McQueen	2017			hapax legomenon;natural language processing;probabilistic logic;xforms;xquery;part-of-speech tagging;computer science;artificial intelligence	NLP	-25.12326307930993	-78.33319092251915	25864
85b3e05beec5b35d6c2130e344cd62062bd69441	contribution to semantic analysis of arabic language	correct sense;information retrieval;information retrieval measure;lesk algorithm;ambiguous word;arabic language;particular sense;new approach;exact string-matching algorithm;arabic word;adequate sense	In this paper we propose a new approach for determining the adequate sense of Arabic words. For that, we propose an algorithm based on information retrieval measures to identify the context of use that is the most closest to the sentence containing the word to be disambiguated. The contexts of use represent a set of sentences that indicates a particular sense of the ambiguous word. These contexts are generated using the words that define the senses of the ambiguous words, the exact string-matching algorithm and the corpus. We use the measures ‎employed in the domain of information retrieval: Harman, Croft, and Okapi combined to the Lesk ‎algorithm to ‎assign the correct sense of those proposed.	approximate string matching;gloss (annotation);information retrieval;lesk algorithm;lexical definition;stemming;string searching algorithm;unsupervised learning;word sense;word-sense disambiguation	Anis Zouaghi;Mounir Zrigui;Georges Antoniadis;Laroussi Merhbene	2012	Adv. Artificial Intellegence	10.1155/2012/620461	natural language processing;computer science	NLP	-26.981825880983344	-71.93736707134902	25889
02c0629abbfa7c1f7569f836437b5a4214afba0c	an introduction to language processing with perl and prolog	computer and information science;natural sciences;language processing;datavetenskap datalogi;computer science	This comprehensive NLP textbook is strongly algorithm-oriented and designed for talented computer programmers who might or might not be linguists. The book occupies a market niche in between that of Jurafsky and Martin (2008) and my own humble effort (Covington 1994); it resembles the latter in approach and the former in scope. Perhaps more than either of those, Nugues’s book is also useful to working professionals as a handbook of techniques and algorithms. Everything is here—everything, that is, except speech synthesis and recognition; phonetics receives only a four-page summary. Those wanting to start an NLP course by covering phonetics in some depth should consider Coleman (2005) as well as Jurafsky and Martin (2008). After a brief overview, Nugues covers corpus linguistics, markup languages, text statistics, morphology, part-of-speech tagging (two ways), parsing (several ways), semantics, and discourse. “Neat” and “scruffy” approaches are deftly interleaved and compared. Unification-based grammar, event semantics, and tools such as WordNet and the Penn Treebank are covered in some detail. The syntax section includes dependency grammar and even the very recent work of Nivre (2006), as well as partial parsing and statistical approaches. Many important algorithms are presented ready to run, or nearly so, as Prolog or Perl code. If, for example, you want to build a Cocke–Kasami–Younger parser, this is the place to look for directions. Explanations are lucid and to-the-point. Here is an example. Nugues is discussing the fact that, if you sample a corpus for n-grams, some will not occur in your sample at all, but it would be a mistake to consider the unseen ones to be infinitely rare (frequency 0). Thus the counts need to be adjusted:	book;money;perl;prolog	Pierre Nugues	2006		10.1007/3-540-34336-9	natural language processing;natural language programming;question answering;computer science;programming language;language technology;high-level programming language	NLP	-31.224703626934062	-74.37805831981264	25923
35a2c008b445985267e4807df6eb17178cdc5ee5	ainlp at ntcir-6: evaluations for multilingual and cross-lingual information retrieval		In this paper, a multilingual cross-lingual information retrieval (CLIR) system is presented and evaluated in NTCIR-6 project. We use the language-independent indexing technology to process the text collections of Chinese, Japanese, Korean, and English languages. Different machine translation systems are used to translate the queries for bilingual and multilingual CLIR. The experimental results are discussed to analyze the performances of our system. The effectiveness of query translations for bilingual and multilingual CLIR is discussed. In the evaluations, the English version of topics performed better CLIR results to retrieve the Korean text collections than the Chinese version did. However, the Chinese version of topics performed better cross-language information retrieval results to retrieve the Japanese text collections than the English version did.	cross-language information retrieval;language-independent specification;machine translation;performance	Chen-Hsin Cheng;Reuy-Jye Shue;Hung-Lin Lee;Shu-Yu Hsieh;Guann-Cyun Yeh;Guo-Wei Bian	2007			human–computer information retrieval;machine translation;information retrieval;search engine indexing;computer science	Web+IR	-33.210474309612884	-64.30956975716619	25930
f722bb7b36f4195301ab2cc07488a8912deda034	word confidence estimation for smt n-best list re-ranking		This paper proposes to use Word Confidence Estimation (WCE) information to improve MT outputs via N-best list reranking. From the confidence label assigned for each word in the MT hypothesis, we add six scores to the baseline loglinear model in order to re-rank the N-best list. Firstly, the correlation between the WCE-based sentence-level scores and the conventional evaluation scores (BLEU, TER, TERp-A) is investigated. Then, the N-best list re-ranking is evaluated over different WCE system performance levels: from our real and efficient WCE system (ranked 1st during last WMT 2013 Quality Estimation Task) to an oracle WCE (which simulates an interactive scenario where a user simply validates words of a MT hypothesis and the new output will be automatically re-generated). The results suggest that our real WCE system slightly (but significantly) improves the baseline while the oracle one extremely boosts it; and better WCE leads to better MT quality.	bleu;baseline (configuration management);boosting (machine learning);experiment;feature selection;lexicon;log-linear model;postediting;simulation;simultaneous multithreading	Ngoc-Quang Luong;Laurent Besacier;Benjamin Lecouteux	2014		10.3115/v1/W14-0301	speech recognition;computer science;machine learning;data mining	NLP	-23.04386276498504	-75.97123466509932	25988
aa127963438f491731291086cf33ed3494c6fb80	acquiring entailment pairs across languages and domains: a data analysis	data analysis;weed management;logistic regression model	Entailment pairs are sentence pairs of a premise and a hypothesis, where the premise textually entails the hypothesis. Such sentence pairs are important for the development of Textual Entailment systems. In this paper, we take a closer look at a prominent strategy for their automatic acquisition from newspaper corpora, pairing first sentences of articles with their titles. We propose a simple logistic regression model that incorporates and extends this heuristic and investigate its robustness across three languages and three domains. We manage to identify two predictors which predict entailment pairs with a fairly high accuracy across all languages. However, we find that robustness across domains within a language is more difficult to achieve.	elegant degradation;emoticon;heuristic;logistic regression;overfitting;text corpus;textual entailment;tf–idf;wikipedia	Manaal Faruqui;Sebastian Padó	2011			natural language processing;computer science;artificial intelligence;data mining	NLP	-22.160017608834593	-71.37768906795687	25998
5da69c5b6bfb86d2ace18103c09e8b8f495fd945	analyzing parallelism and domain similarities in the marec patent corpus	sentence-parallel data;processed data;patent document section;analyzing parallelism;document section;large multilingual patent corpus;patent text;domain similarity;marec patent corpus;parallel data;patent document;patent topic	Statistical machine translation of patents requires large amounts of sentence-parallel data. Translations of patent text often exist for parts of the patent document, namely title, abstract and claims. However, there are no direct translations of the largest part of the document, the description or background of the invention. We document a twofold approach for extracting parallel data from all patent document sections from a large multilingual patent corpus. Since language and style differ depending on document section (title, abstract, description, claims) and patent topic (according to the International Patent Classification), we sort the processed data into subdomains in order to enable its use in domain-oriented translation, e.g. when applying multi-task learning. We investigate several similarity metrics and apply them to the domains of patent topic and patent document sections. Product of our research is a corpus of 23 million parallel German-English sentences extracted from the MAREC patent corpus and a descriptive analysis of its subdomains.	computer multitasking;marec;multi-task learning;peer-to-patent;software patent;statistical machine translation;text corpus	Katharina Wäschle;Stefan Riezler	2012		10.1007/978-3-642-31274-8_2	natural language processing;computer science;data mining;information retrieval	NLP	-29.221871276487065	-70.40979189592755	26013
7e091641c44ee7313d0c6884263765c373d1495a	dynamic similarity threshold in authorship verification: evidence from classical arabic		Abstract Many Authorship Verification Machine Learning-based algorithms rely on establishing a similarity threshold θ between a candidate text and known texts in terms of one or more linguistic features. Documents that score below that threshold are rejected as not written by the same author. Current definitions of θ rely on both negative and positive training input. An algorithm that relies exclusively on positive training data, and dynamically calculates θ for each verification problem performs with good accuracy, tested using a training and evaluation corpus from Classical Arabic.		Hossam Ahmed	2017		10.1016/j.procs.2017.10.103	machine learning;dynamic similarity;training set;classical arabic;computer science;pattern recognition;artificial intelligence	NLP	-21.196894943284313	-70.92994047230226	26040
39dde3dea8c8a7d8db1c2eb416ffe6daed1af59e	hierarchical phrase-based translation	statistical machine translation;system performance;context free grammar;computational linguistics;linguistique informatique	We present a statistical machine translation model that uses hierarchical phrasesphrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a parallel text without any syntactic annotations. Thus it can be seen as combining fundamental ideas from both syntax-based translation and phrase-based translation. We describe our system's training and decoding methods in detail, and evaluate it for translation speed and translation accuracy. Using BLEU as a metric of translation accuracy, we find that our system performs significantly better than the Alignment Template System, a state-of-the-art phrase-based system.	ats;bleu;context-free language;decoding methods;monotone polygon;parallel text;statistical machine translation;synchronous context-free grammar;text corpus	David Chiang	2007	Computational Linguistics	10.1162/coli.2007.33.2.201	rouge;dynamic and formal equivalence;computer-assisted translation;natural language processing;synchronous context-free grammar;speech recognition;transfer-based machine translation;example-based machine translation;computer science;computational linguistics;evaluation of machine translation;linguistics;machine translation;rule-based machine translation;context-free grammar;machine translation software usability	NLP	-22.177456591906857	-77.16527057379714	26041
08c8d354d5cc43d4079a4d6617283a21a9b7582d	semantic similarity measures to disambiguate terms in medical text		Computing the semantic similarity accurately between words is an important but challenging task in the semantic web field. However, the semantic similarity measures involve the comprehensiveness of knowledge learning and the sufficient training of words of both high and low frequency. In this study, an approach MedSim is presented for semantic similarity measures to identify synonym terms in medical text with effectiveness and accuracy well-balanced. Experimental results on Chinese medical text demonstrate that our proposed method has robust superiority over competitors for synonym identification.		Kai Lei;Jiyue Huang;Shangchun Si;Ying Shen	2018		10.1007/978-3-030-04239-4_36	natural language processing;machine learning;semantic similarity;word embedding;artificial intelligence;semantic web;synonym;feature selection;computer science	NLP	-26.639710121346685	-66.80680538033778	26053
7843e1ba788399b18a2058a3b549e78dbb0a0f16	recent developments on standardisation of mpeg-7 visual signature tools	histograms;visual fingerprint;image signature descriptor;mobile visual search framework;testing;transform coding;content identification;visual media content;standardisation;video coding;indexes;visualization;video signature descriptor;feature extraction;transform coding visualization robustness testing feature extraction indexes histograms;visual search;telecommunication standards;moving picture experts group;mobile visual search framework mpeg 7 visual signature tools moving picture experts group image signature descriptor video signature descriptor visual media content fingerprints robust hashes;robust hashes;video coding telecommunication standards;robustness;mpeg;visual signature;mpeg content identification visual signature standardization visual fingerprint;fingerprints;moving pictures expert group;standardization;mpeg 7 visual signature tools	This paper presents the latest developments and possible new directions for future work in standardisation of Visual Signature Tools within the Moving Picture Experts Group (MPEG). The tools, which include the Image Signature descriptor and the recently completed Video Signature descriptor, form a part of the MPEG-7 specification. They enable fast and robust detection of duplicate or derived visual media content, images and videos. Descriptors of this type are sometimes also referred to as fingerprints or robust hashes. Here we mainly focus on introducing the technology behind the recently completed Video Signature Tools and describe some recent developments and demonstration applications for the Image Signature Tools. Finally, we briefly present MPEG exploratory investigations on requirements of searching for different images containing the same visual objects within the mobile visual search framework.	fingerprint;mpeg-7;moving picture experts group;requirement;visual objects	Paul Brasnett;Stavros Paschalakis;Miroslaw Bober	2010	2010 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2010.5583215	computer vision;computer science;multimedia;programming language;information retrieval;standardization;statistics	Vision	-12.610343831931123	-56.57384333526347	26134
50183daf4a09c88cc97ba85ab3b0fa144ae583e6	unsupervised multi-feature tag relevance learning for social image retrieval	tag relevance learning;social image retrieval;universiteitsbibliotheek;multi feature;local features;visual features;social tagging;image retrieval	Interpreting the relevance of a user-contributed tag with respect to the visual content of an image is an emerging problem in social image retrieval. In the literature this problem is tackled by analyzing the correlation between tags and images represented by specific visual features. Unfortunately, no single feature represents the visual content completely, e.g., global features are suitable for capturing the gist of scenes, while local features are better for depicting objects. To solve the problem of learning tag relevance given multiple features, we introduce in this paper two simple and effective methods: one is based on the classical Borda Count and the other is a method we name UniformTagger. Both methods combine the output of many tag relevance learners driven by diverse features in an unsupervised, rather than supervised, manner.  Experiments on 3.5 million social-tagged images and two test sets verify our proposal. Using learned tag relevance as updated tag frequency for social image retrieval, both Borda Count and UniformTagger outperform retrieval without tag relevance learning and retrieval with single-feature tag relevance learning. Moreover, the two unsupervised methods are comparable to a state-of-the-art supervised alternative, but without the need of any training data.	gist;image retrieval;relevance;unsupervised learning	Xirong Li;Cees Snoek;Marcel Worring	2010		10.1145/1816041.1816044	visual word;image retrieval;computer science;pattern recognition;data mining;information retrieval	Web+IR	-16.256389191709054	-61.47964291830967	26135
61777bb512c5c1327778d9d7698e1994bdf5ca9a	bilingual word representations with monolingual quality in mind		Recent work in learning bilingual representations tend to tailor towards achieving good performance on bilingual tasks, most often the crosslingual document classification (CLDC) evaluation, but to the detriment of preserving clustering structures of word representations monolingually. In this work, we propose a joint model to learn word representations from scratch that utilizes both the context coocurrence information through the monolingual component and the meaning equivalent signals from the bilingual constraint. Specifically, we extend the recently popular skipgram model to learn high quality bilingual representations efficiently. Our learned embeddings achieve a new state-of-the-art accuracy of 80.3 for the German to English CLDC task and a highly competitive performance of 90.7 for the other classification direction. At the same time, our models outperform best embeddings from past bilingual representation work by a large margin in the monolingual word similarity evaluation.	cluster analysis;connected limited device configuration;display resolution;document classification;emoticon;n-gram	Thang Luong;Hieu Pham;Christopher D. Manning	2015			natural language processing;speech recognition;computer science;linguistics	NLP	-19.685285283103518	-74.26678257936923	26197
a5b80d149c19c5a2fd476de7078d6a8780a958e3	extracting phrases in vietnamese document for summary generation	pragmatics;syntactics barium semantics text recognition satellites presses pragmatics;barium;semantics;rhetorical relation;text analysis;presses;text summarization;discourse tree;discourse structure;elementary discourse unit;syntactics;satellites;discourse tree vietnamese phrase extraction text summarization elementary discourse unit textual span;computational linguistics;phrase extraction;text recognition;vietnamese;text analysis computational linguistics natural language processing;natural language processing;vietnamese text summarization discourse structure rhetorical relation;textual span	This paper describes an approach to Vietnamese text summarization, concentrated on the discourse structure of the text. Based on characteristics of Vietnamese, we propose rules for segmenting text into elementary discourse units (edus) and for recognizing discourse relations between textual spans. The score of an edu is computed based on the discourse tree. The edus with highest scores are chosen to put in the summary. Experiments show that this method can give promising results.	automatic summarization;cyk algorithm;experiment;parsing	Huong Thanh Le;Rathany Chan Sam;Phuc Trong Nguyen	2010	2010 International Conference on Asian Language Processing	10.1109/IALP.2010.8	natural language processing;text mining;speech recognition;vietnamese;computer science;computational linguistics;semantics;linguistics;barium;pragmatics	NLP	-24.923179472154537	-65.38026800663452	26208
3ad0c3e0705f54d5b09f956235c9baa4bef8eb2e	a flexible search-by-similarity algorithm for content-based image retrieval		This paper introduces a flexible search strategy for image or image category retrieval in large databases. The process is based on a search-by-similarity method in which both request and dissimilarity measure are updated thanks to user interaction. The system copes with complex queries combining relevant images scattered in the database. The effectiveness of the method is emphasized by a quality assessment realized for a large and heterogeneous image database.	algorithm;content-based image retrieval;heterogeneous database system;image noise;multimodal interaction;refinement (computing);relevance feedback	Jérome Fournier;Matthieu Cord	2002			visual word;automatic image annotation;content-based image retrieval;feature detection (computer vision);image texture;image retrieval;computer vision;artificial intelligence;computer science;pattern recognition	Vision	-12.704378566481608	-58.08062593914487	26240
3ffe25db0188a8548fac9823e4d0f4ad642fd600	modeling selectional preferences of verbs and nouns in string-to-tree machine translation		We address the problem of mistranslated predicate-argument structures in syntaxbased machine translation. This paper explores whether knowledge about semantic affinities between the target predicates and their argument fillers is useful for translating ambiguous predicates and arguments. We propose a selectional preference feature based on the selectional association measure of Resnik (1996) and integrate it in a string-to-tree decoder. The feature models selectional preferences of verbs for their core and prepositional arguments as well as selectional preferences of nouns for their prepositional arguments. We compare our features with a variant of the neural relational dependency language model (RDLM) (Sennrich, 2015) and find that neither of the features improves automatic evaluation metrics. We conclude that mistranslated verbs, errors in the target syntactic trees produced by the decoder and underspecified syntactic relations are negatively impacting these features.	evaluation of machine translation;feature model;language model;statistical machine translation	Maria Nadejde;Alexandra Birch;Philipp Koehn	2016			noun;machine translation;natural language processing;linguistics;artificial intelligence;computer science	NLP	-24.24681001880311	-74.9127165844367	26259
a192057a85cd1d8627bcabdac82f6a79995b6e83	from once upon a time to happily ever after: tracking emotions in mail and books	lexicon;enron corpus;email;fairy tales;sentiment analysis;google books corpus;emotion analysis	In this paper, we show how sentiment analysis can be used in tandem with effective visualizations to quantify and track emotions in mail and books. We study a number of specific datasets and show, among other things, how collections of texts can be organized for affect-based search and how books portray different entities through co-occurring emotion words. Analysis of the Enron Email Corpus reveals that there are marked differences across genders in how they use emotion words in work-place email. Finally, we show that fairy tales have more extreme emotion densities than novels.	book;crowdsourcing;email;entity;lexicon;microsoft word for mac;sadness;tag cloud;web search query	Saif Mohammad	2012	Decision Support Systems	10.1016/j.dss.2012.05.030	computer science;data mining;multimedia;advertising;world wide web;sentiment analysis	NLP	-22.00249720316204	-59.56213930146955	26279
b49f588818a71608e21506589615a1a7a17b0197	an meg study of temporal characteristics of semantic integration in japanese noun phrases	mismatching;magnetoencefalografia;tecnologia electronica telecomunicaciones;noun;integrated circuit;time course;noun phrase;japonais;frase;semantic integration;semantics;circuito integrado;semantica;semantique;semantic interpretation;higher order;sentence;magnetoencephalography;desadaptacion;genitive numerical classifier;signal classification;classification signal;procesador;phrase;desadaptation;classification automatique;analisis semantico;tecnologias;processeur;grupo a;analyse semantique;automatic classification;clasificacion automatica;japones;magnetoencephalographie;processor;circuit integre;semantic analysis;japanese	Many studies of on-line comprehension of semantic violations have shown that the human sentence processor rapidly constructs a higher-order semantic interpretation of the sentence. What remains unclear, however, is the amount of time required to detect semantic anomalies while concatenating two words to form a phrase with very rapid stimuli presentation. We aimed to examine the time course of semantic integration in concatenating two words in phrase structure building, using magnetoencephalography (MEG). In the MEG experiment, subjects decided whether two words (a classifier and its corresponding noun), presented each for 66 ms, form a semantically correct noun phrase. Half of the stimuli were matched pairs of classifiers and nouns. The other half were mismatched pairs of classifiers and nouns. In the analysis of MEG data, there were three primary peaks found at approximately 25 ms (M1), 170 ms (M2) and 250 ms (M3) after the presentation of the target words. As a result, only the M3 latencies were significantly affected by the stimulus conditions. Thus, the present results indicate that the semantic integration in concatenating two words starts from approximately 250 ms.	magnetoencephalography;semantic integration	Hirohisa Kiguchi;Nobuhiko Asakura	2008	IEICE Transactions	10.1093/ietisy/e91-d.6.1656	natural language processing;noun;japanese;semantic interpretation;noun phrase;semantic integration;speech recognition;computer science;integrated circuit;semantics;magnetoencephalography	NLP	-9.932567936097964	-78.14037165812609	26333
998a6f897f87667caa8afd1c3d909eb063f72d7f	pre- and in-parsing models for neural empty category detection		▶ Perhaps. Deep grammar formalisms provide more transparent interface to semantics ▶ Hard to prove. Grammar Formalism are heterogeneous and hard to be compared. ▶ Modeling empty category help dependency parsing. ▷ Our CoNLL paper: Zhang, Sun and Wan (2017) ▷ The dependency tree representation is augmented with empty nodes, which corresponds to unpronounced nominal words ▷ Data-driven parsing based on global linear models	artificial neural network;explanatory combinatorial dictionary;linear model;natural language processing;nonlinear system;parsing;precondition;sun ray	Weiwei Sun;Xiaojun Wan;Yufei Chen;Yuanyuan Zhao	2018			natural language processing;computer science;artificial intelligence;empty category;parsing	NLP	-19.22067252381472	-74.69623309589508	26354
a53815cf0c9cbb54a23b11f73c7d532910afa750	the excitement open platform for textual inferences		This paper presents the Excitement Open Platform (EOP), a generic architecture and a comprehensive implementation for textual inference in multiple languages. The platform includes state-of-art algorithms, a large number of knowledge resources, and facilities for experimenting and testing innovative approaches. The EOP is distributed as an open source software.	algorithm;computational linguistics;exchange online protection;experiment;high-level programming language;interoperability;open platform;open-source software;pipeline (computing);software architecture;textual entailment	Bernardo Magnini;Roberto Zanoli;Ido Dagan;Kathrin Eichler;Günter Neumann;Tae-Gil Noh;Sebastian Padó;Asher Stern;Omer Levy	2014			computer science;theoretical computer science;open platform;database;world wide web	SE	-32.90442643486414	-73.23097460431786	26375
5ff1f26095c3e3bdc6d0864f976e049cf43a19ba	the icecite research paper management system		We present Icecite, a new fully web-based research paper management system (RPMS). Icecite facilitates the following otherwise laborious and time-consuming steps typically involved in literature research: automatic metadata and reference extraction, on-click reference downloading, shared annotations, offline availability, and full-featured search in metadata, full texts, and annotations. None of the many existing RPMSs provides this feature set. For the metadata and reference extraction, we use a rule-based approach combined with an index-based approximate search on a given reference database. An extensive quality evaluation, using DBLP and PubMed as reference databases, shows extraction accuracies of above 95%. We also provide a small user study, comparing Icecite to the state-of-the-art RPMS Mendeley as well as to an RPMS-free baseline.	1-click;approximation algorithm;baseline (configuration management);bibliographic database;dbl-browser;download;error analysis (mathematics);logic programming;management system;mendeley;online and offline;pdfbox;portable document format;pubmed;usability testing;web application	Hannah Bast;Claudius Korzen	2013		10.1007/978-3-642-41154-0_30	computer science;data mining;database;world wide web;information retrieval	NLP	-31.45550158304216	-60.1484388256543	26466
a212e2bec6f9f91761fd83c252623fecc9d32107	the yahoo query treebank, v. 1.0		1 General This dataset release accompanies Pinter et al. (2016) which describes the motivation and grammatical theory 1. Please cite that paper when referencing the dataset. The dataset may be accessed via the Yahoo Webscope homepage 2 under Linguistic Data as dataset L-28. The description in Section 2 is included within the dataset as a Readme. The dataset is sure to have annotation errors which are not covered by the special cases specified in this document. Please approach the first author for any corrections and they will appear in the next release. See Section 4 for known errors. User queries annotated for syntactic dependency parsing, version 1.0. These queries were issued on all search engines between 2012 and 2014 and led the searcher to click on a result link to a question page on the Yahoo Answers site 3. 2.1 Full description This dataset contains two files: ydata-search-parsed-queries-dev-v1 0.txt 1,000 queries, 5,344 tokens ydata-search-parsed-queries-test-v1 0.txt 4,000 queries, 26,015 tokens These files differ in their level of annotation, but share the schema. They contain tab-delimited lines, each representing a single token in a Web query. The tokens in each query are given sequentially, and queries are given in order of an arbitrarily-selected numeric ID (with no empty lines between queries). The field schema is detailed in Table 1.	delimiter;parsing;readme;treebank;web search engine;yahoo! answers	Yuval Pinter;Roi Reichart;Idan Szpektor	2016	CoRR		linguistics	NLP	-32.21932386957835	-62.19525390670534	26469
b1fea576641b429d1890c04f39bab143e8b07835	a semantic graph model for text representation and matching in document mining	document clustering;document mining;semantic understanding;doctoral thesis;text representation;similarity measure;electrical computer engineering	The explosive growth in the number of documents produced daily necessitates the development of effective alternatives to explore, analyze, and discover knowledge from documents. Document mining research work has emerged to devise automated means to discover and analyze useful information from documents. This work has been mainly concerned with constructing text representation models, developing distance measures to estimate similarities between documents, and utilizing that in mining processes such as document clustering, document classification, information retrieval, information filtering, and information extraction. #R##N#Conventional text representation methodologies consider documents as bags of words and ignore the meanings and ideas their authors want to convey. It is this deficiency that causes similarity measures to fail to perceive contextual similarity of text passages due to the variation of the words the passages contain, or at least perceive contextually dissimilar text passages as being similar because of the resemblance of words the passages have. #R##N#This thesis presents a new paradigm for mining documents by exploiting semantic information of their texts. A formal semantic representation of linguistic inputs is introduced and utilized to build a semantic representation scheme for documents. The representation scheme is constructed through accumulation of syntactic and semantic analysis outputs. A new distance measure is developed to determine the similarities between contents of documents. The measure is based on inexact matching of attributed trees. It involves the computation of all distinct similarity common sub-trees, and can be computed efficiently. It is believed that the proposed representation scheme along with the proposed similarity measure will enable more effective document mining processes.#R##N#The proposed techniques to mine documents were implemented as vital components in a mining system. A case study of semantic document clustering is presented to demonstrate the working and the efficacy of the framework. Experimental work is reported, and its results are presented and analyzed.		Khaled Bashir Shaban	2006			concept mining;natural language processing;semantic similarity;text mining;explicit semantic analysis;document clustering;computer science;data mining;information retrieval	NLP	-28.813984206774634	-63.20491771801427	26482
9d347f639995c71e794b70c795085f84764af14d	semantic video classification based on subtitles and domain terminologies	wordnet domains;text classification;video classification;natural language processing	In this paper we explore an unsupervised approach to classify video content by analyzing the corresponding subtitles. The proposed method is based on the WordNet lexical database and the WordNet domains and applies natural language processing techniques on video subtitles. The method is divided into several steps. The first step includes subtitle text preprocessing. During the next steps, a keyword extraction method and a word sense disambiguation technique are applied. Subsequently, the WordNet domains that correspond to the correct word senses are identified. The final step assigns category labels to the video content based on the extracted domains. Experimental results with documentary videos show that the proposed method is quite effective in discovering the correct category for each video.	digital video;keyword extraction;lexical database;natural language processing;preprocessor;unsupervised learning;word sense;word-sense disambiguation;wordnet	Polyxeni Katsiouli;Vassileios Tsetsos;Stathes Hadjiefthymiades	2007			information retrieval;subtitle;natural language processing;semeval;word-sense disambiguation;keyword extraction;computer science;preprocessor;lexical database;wordnet;artificial intelligence	AI	-25.547732751280627	-65.72822551398082	26503
9f184e65211b10403f6db2c6c62d264bec5aae42	learning chinese word representations from glyphs of characters		In this paper, we propose new methods to learn Chinese word representations. Chinese characters are composed of graphical components, which carry rich semantics. It is common for a Chinese learner to comprehend the meaning of a word from these graphical components. As a result, we propose models that enhance word representations by character glyphs. The character glyph features are directly learned from the bitmaps of characters by convolutional auto-encoder(convAE), and the glyph features improve Chinese word representations which are already enhanced by character embeddings. Another contribution in this paper is that we created several evaluation datasets in traditional Chinese and made them public.	bitmap;glyph;graphical user interface	Tzu-Ray Su;Hung-yi Lee	2017			natural language processing;computer science;machine learning;artificial intelligence;bitmap;chinese characters;semantics;glyph	NLP	-19.815006653352494	-72.6300253089238	26508
3c591b5bfbaa692f867267eed2f64584f592b2a3	combining signals for cross-lingual relevance feedback		We present a new cross-lingual relevance feedback model that improves a machine-learned ranker for a language with few training resources, using feedback from a better ranker for a language that has more training resources. The model focuses on linguistically non-local queries, such as [world cup] and [copa mundial], that have similar user intent in different languages, thus allowing the low-resource ranker to get direct relevance feedback from the high-resource ranker. Our model extends prior work by combining both queryand document-level relevance signals using a machine-learned ranker. On an evaluation with web data sampled from a real-world search engine, the proposed cross-lingual feedback model outperforms two state-of-the-art models across two different low-resource languages.	anchor text;baseline (configuration management);bilingual dictionary;clickstream;document;domain adaptation;learning to rank;query expansion;relevance feedback;web page;web search engine	Kristen Parton;Jianfeng Gao	2012		10.1007/978-3-642-35341-3_31	natural language processing;computer science;artificial intelligence;machine learning;data mining;world wide web;information retrieval	NLP	-21.0896122404509	-66.91741294116066	26525
62c6852befaf72c0743d3605d433284f9f9c0566	edinburgh-stanford trec-2003 genomics track		We describe our participation in both tasks in the 2003 TREC Genomics track. For the primary task we concentrated mainly upon query expansion and species-specific document searching. An analysis of the variance of possible retrieval results suggested that the official TREC-supplied test set is only a crude approximation of the true system performance. The secondary task we treated as an extraction problem, using a maximum entropy scorer trained on GeneRIF sentences as positives and other sentences as negatives. While our results were not always equivalent to the actual GeneRIFs, on biological grounds many of them appeared better descriptors than the GeneRIFs themselves.	approximation;computational genomics;generif;query expansion;test set	Miles Osborne;Mark Cuminskey;Gail Sinclair;Matthew Smillie;Bonnie L. Webber;Jeffrey T. Chang;Nipun Mehra;Veronica Rotemberg;Russ B. Altman	2003			information retrieval;genomics;computer science	NLP	-28.739828768852554	-69.8663028070719	26658
5d7e179f1543108f06f09ba801ae70ba38900c5d	semantic similarity applied to spoken dialogue summarization	noun portion;novel approach;semantic similarity metrics;essential content;switchboard dialogue;idf baselines;knowledge source;dialogue summarization;noun sense;semantic similarity;noun	We present a novel approach to spoken dialogue summarization. Our system employs a set of semantic similarity metrics using the noun portion of WordNet as a knowledge source. So far, the noun senses have been disambiguated manually. The algorithm aims to extract utterances carrying the essential content of dialogues. We evaluate the system on 20 Switchboard dialogues. The results show that our system outperforms LEAD, RANDOM and TF*IDF baselines.	algorithm;semantic similarity;telephone switchboard;tf–idf;wordnet	Iryna Gurevych;Michael Strube	2004			natural language processing;noun;semantic similarity;computer science;linguistics	NLP	-27.33110533337047	-70.80957998662527	26758
75c2196a5b2aa05c7dc77571cfd3f2bdd931b4ed	scalable text classification as a tool for personalization		We consider scalability issues of the text classification problem where by using (multi)-labeled training documents, we try to build classifiers that assign documents into classes permitting classification in multiple classes. A new class of classification problems; called ‘scalable’, is introduced, with applications on web mining. Scalable classification utilizes newly classified instances in order to improve the accuracy of future classifications and capture changes in semantic representation of different topics. In addition, definition of different similarity classes is allowed, resulting in a ‘per-user’ classification procedure. Such an approach provides a new methodology for building personalized applications. This is due to the fact that the user becomes a part of the classification procedure. We explore solutions for the scalable text classification problem and introduce an algorithm that exploits a new text analysis technique that decomposes documents into the vector representation of their sentences according to the user expertise. Finally, a web-based personalized news categorization system that bases upon this approach is presented.	algorithm;categorization;computer;document classification;information needs;personalization;scalability;systems science;web application;web mining	Ioannis Antonellis;Christos Bouras;Vassilis Poulopoulos	2009	Comput. Syst. Sci. Eng.		personalization;information retrieval;scalability;data mining;computer science	Web+IR	-20.892435595354605	-62.07111437053119	26789
3b1aa96b3cbd788aebf75666c2167bdb0292d2cb	reordering constraint based on document-level context	context document;patent translation task;document-level context;reordering constraint;bleu point;japanese-english translation;english-japanese translation;noun phrase;source sentence;phrase-based statistical machine translation	One problem with phrase-based statistical machine translation is the problem of longdistance reordering when translating between languages with different word orders, such as Japanese-English. In this paper, we propose a method of imposing reordering constraints using document-level context. As the documentlevel context, we use noun phrases which significantly occur in context documents containing source sentences. Given a source sentence, zones which cover the noun phrases are used as reordering constraints. Then, in decoding, reorderings which violate the zones are restricted. Experiment results for patent translation tasks show a significant improvement of 1.20% BLEU points in JapaneseEnglish translation and 1.41% BLEU points in English-Japanese translation.	bleu;coherence (physics);statistical machine translation	Takashi Onishi;Masao Utiyama;Eiichiro Sumita	2011			natural language processing;speech recognition;computer science;linguistics	NLP	-22.404243719863373	-76.55746725360981	26805
25a94a3c3cd05a4a07b7da6354269f857f380b4e	quantifying origin and character of long-range correlations in narrative texts	correlations;multifractals;natural language;consciousness;hypertext	In natural language using short sentences is considered efficient for communication. However, a text composed exclusively of such sentences looks technical and reads boring. A text composed of long ones, on the other hand, demands significantly more effort for comprehension. Studying characteristics of the sentence length variability (SLV) in a large corpus of world-famous literary texts shows that an appealing and aesthetic optimum appears somewhere in between and involves selfsimilar, cascade-like alternation of various lengths sentences. A related quantitative observation is that the power spectra S(f) of thus characterized SLV universally develop a convincing ‘1/f’ scaling with the average exponent β ≈ 1/2, close to what has been identified before in musical compositions or in the brain waves. An overwhelming majority of the studied texts simply obeys such fractal attributes but especially spectacular in this respect are hypertext-like, ”stream of consciousness” novels. In addition, they appear to develop structures characteristic of irreducibly interwoven sets of fractals called multifractals. Scaling of S(f) in the present context implies existence of the ∗Corresponding author Email address: stanislaw.drozdz@ifj.edu.pl (Stanis law Drożdż) Preprint submitted to Journal of Information Sciences October 15, 2015 long-range correlations in texts and appearance of multifractality indicates that they carry even a nonlinear component. A distinct role of the full stops in inducing the long-range correlations in texts is evidenced by the fact that the above quantitative characteristics on the long-range correlations manifest themselves in variation of the full stops recurrence times along texts, thus in SLV, but to a much lesser degree in the recurrence times of the most frequent words. In this latter case the nonlinear correlations, thus multifractality, disappear even completely for all the texts considered. Treated as one extra word, the full stops at the same time appear to obey the Zipfian rank-frequency distribution, however.	email;fractal;hypertext;image scaling;irreducibility;level of measurement;mike lesser;multifractal system;natural language;neural oscillation;nonlinear system;self-similarity;spatial variability;zipf's law	Stanislaw Drozdz;Pawel Oswiecimka;Andrzej Kulig;Jaroslaw Kwapien;Katarzyna Bazarnik;Iwona Grabska-Gradzinska;Jan Rybicki;Marek Stanuszek	2016	Inf. Sci.	10.1016/j.ins.2015.10.023	natural language processing;speech recognition;hypertext;computer science;artificial intelligence;machine learning;consciousness;mathematics;natural language;algorithm	NLP	-12.140217559542625	-78.39386592414871	26862
03086d9b8396c3edb6623276f188f104bda149f5	a naïve approach for monolingual question answering		This paper talks about the system which we have submitted for the ResPubliQA task. We participated in building the QA system for en-en part. We followed a different method for each question type. In this paper we outline the methods which we adapted and the results which we obtained.	naivety;question answering	Rohit G. Bharadwaj;Surya Ganesh Veeravalli;Vasudeva Varma	2009			natural language processing;question answering;artificial intelligence;computer science	NLP	-22.746359502816233	-70.47091447349868	26883
dc2985d4d8cd82560fcecd1f59e741fddb79cb38	a novel web pages classification model based on integrated ontology	concept vector space;ontology integration;text classification model;word vector space	The main existed problem in the traditional text classification methods is can't use the rich semantic information in training data set. This paper proposed a new text classification model based SUMO (The Suggested Upper Merged Ontology) and WordNet ontology integration. This model utilizes the mapping relations between WordNet synsets and SUMO ontology concepts to map terms in document-words vector space into the corresponding concepts in ontology, forming document-concepts vector space, based this, we carry out a text classification experiment. Experiment results show that the proposed method can greatly decrease the dimensionality of vector space and improve the text classification performance. © 2011 Springer-Verlag.		Bai Rujiang;Wang Xiaoyue;Hu Zewen	2011		10.1007/978-3-642-27207-3_1	upper ontology;bibliographic ontology;ontology inference layer;ontology;data science;ontology-based data integration;world wide web;owl-s;website parse template;information retrieval;process ontology;suggested upper merged ontology	Web+IR	-28.47813496802356	-58.86453083986394	26953
33893d7be0c4f28a6fa90f05d0e3158b62f45976	a comparison between two spanish sentiment lexicons in the twitter sentiment analysis task		Sentiment analysis aims to determine people’s opinions towards certain entities (e.g., products, movies, people, etc.). In this paper we describe experiments performed to determine sentiment polarity on tweets of the Spanish corpus used in the TASS workshop. We explore the use of two Spanish sentiment lexicons to find out the effect of these resources in the Twitter sentiment analysis task. Rule based and supervised classification methods were implemented and several variations over those approaches were performed. The results show that the information of both lexicons improve the accuracy when is provided as a feature to a Naive Bayes classifier. Despite the simplicity of the proposed strategy, the supervised approach obtained better results than several participant teams of the TASS workshop and even the rule based approach overpass the accuracy of one team which used a supervised algorithm.	lexicon;sentiment analysis	Omar Juárez Gambino;Hiram Calvo	2016		10.1007/978-3-319-47955-2_11	natural language processing;linguistics;world wide web;sentiment analysis	NLP	-21.923042159976433	-69.05747851775244	27023
fa8075ef330458b8ae626da1ca1442bd395cb438	reconocimiento temporal para el italiano combinando técnicas de aprendizaje automático y adquisición automática de conocimiento	informacion temporal;automatic acquisition of knowledge;info eu repo semantics article;temporal information;aprendizaje automatico;machine learning;reconocimiento de expresiones temporales;adquisicion automatica del conocimiento;temporal expression recognition	This paper presents the automatic extension of TERSEO system to other languages combined with the use of Machine Learning (ML) techniques. In particular, in this paper, Italian temporal expression recognition is treated and two different ML techniques have been proven: Maximum Entropy Model and Hidden Markov Model. Every system has been evaluated independently and combined afterwards in order to analyze if the system is improving the results in the combination without increasing the number of erroneous expressions in the same percentage. TERSEO system was previously combined with ML techniques for English obtaining good results. In this paper, TERSEO plus ML has been evaluated for Italian. When TERSEO system is combined with different ML techniques, the results are quite successful, taking into account that the automatic extension of TERSEO system for Italian has not been manually supervised and the whole process has been automatically performed	boolean expression;hidden markov model;machine learning;markov chain;principle of maximum entropy;temporal logic	Estela Saquete Boró;Óscar Ferrández;Patricio Martínez-Barco;Rafael Muñoz	2006	Procesamiento del Lenguaje Natural		computer science;artificial intelligence	SE	-26.348504487704044	-77.51668271436272	27135
8aa6a6c6a121355dc8757ef9a313425f4ecf1ac3	dictionary-based sentiment analysis applied to a specific domain	u10 methodes mathematiques et statistiques;c30 documentation et information;000 autres themes;u40 methodes de releve	The web and social media have been growing exponentially in recent years. We now have access to documents bearing opinions expressed on a broad range of topics. This constitutes a rich resource for natural language processing tasks, particularly for sentiment analysis. Nevertheless, sentiment analysis is usually difficult because expressed sentiments are usually topic-oriented. In this paper, we propose to automatically construct a sentiment dictionary using relevant terms obtained from web pages for a specific domain. This dictionary is initially built by querying the web with a combination of opinion terms, as well as terms of the domain. In order to select only relevant terms we apply two measures AcroDefMI3 and TrueSkill. Experiments conducted on different domains highlight that our automatic approach performs better for specific cases.	algorithm;data dictionary;experiment;lexicon;natural language processing;sentiment analysis;social media;web page;window function;world wide web	Laura Cruz;Juan Ochoa;Mathieu Roche;Pascal Poncelet	2016		10.1007/978-3-319-55209-5_5	natural language processing;computer science;data mining;information retrieval;sentiment analysis	NLP	-30.381134896989497	-66.71216371549416	27143
916e919adda7c13b6cb54d5f7d37be43108722d3	sentence embedding evaluation using pyramid annotation		"""Word embedding vectors are used as input for a variety of tasks. Choosing the right model and features for producing such vectors is not a trivial task and different embedding methods can greatly affect results. In this paper we repurpose the """"Pyramid Method"""" annotations used for evaluating automatic summarization to create a benchmark for comparing embedding models when identifying paraphrases of text snippets containing a single clause. We present a method of converting pyramid annotation files into two distinct sentence embedding tests. We show that our method can produce a good amount of testing data, analyze the quality of the testing data, perform test on several leading embedding methods, and finally explain the downstream usages of our task and its significance."""	automatic summarization;benchmark (computing);downstream (software development);pyramid (image processing);word embedding	Tal Baumel;Raphael Cohen;Michael Elhadad	2016		10.18653/v1/W16-2526	computer science;pattern recognition;data mining;information retrieval	NLP	-25.30047525310091	-68.3081732923509	27147
6837438d46880add2bd5239c8a0a1a313b461801	annotation and analysis of discourse relations, temporal relations and multi-layered situational relations in japanese texts		This paper proposes a methodology for building a specialized Japanese data set for recognizing temporal relations and discourse relations. In addition to temporal and discourse relations, multilayered situational relations that distinguish generic and specific states belonging to different layers in a discourse are annotated. Our methodology has been applied to 170 text fragments taken from Wikinews articles in Japanese. The validity of our methodology is evaluated and analyzed in terms of degree of annotator agreement and frequency of errors.	experiment;natural language processing	Kimi Kaneko;Saku Sugawara;Koji Mineshima;Daisuke Bekki	2016			natural language processing;artificial intelligence;situational ethics;computer science;annotation	NLP	-28.415484761651737	-72.67721975315878	27228
558a9795bcb682b8778f9c12848f5dc08f8fb01c	kosac: a full-fledged korean sentiment analysis corpus	conference paper	This paper aims to introduce the Korean Sentiment Analysis Corpus named KOSAC. KOSAC is a corpus consisting of 332 news articles taken from the Sejong Syntactic Parsed Corpus. These sentences have been manually-tagged for sentimental features. The corpus includes 7,713 sentence subjectivity tags and 17,615 opinionated expression tags based on the annotation scheme called KSML which reflects the characteristics of the Korean language. The results of sentence subjectivity and polarity classification experiements using the corpus show the wide possibilities of application the KSML scheme and the tagged information of the KOSAC comprehensively to other corpus. What is innovative about our work is that it pulls together both the concept of private states and nested-sources into one linguistic annotation scheme. We believe that this corpus could be used by researchers as a gold standard for various NLP tasks related to sentiment analysis.	experiment;natural language processing;sentiment analysis;text corpus;treebank	Hayeon Jang;Munhyong Kim;Hyopil Shin	2013			sentiment analysis;syntax;natural language processing;parsing;subjectivity;artificial intelligence;annotation;sentence;computer science	NLP	-28.57037687286411	-73.858465570781	27240
8cc85543c41d67a2a4066926cbedaeab11f3ab77	dependency relations as source context in phrase-based smt	phrase based smt;syntactic dependencies;conference paper;machine translating;memory based learning	The Phrase-Based Statistical Machine Translation (PB-SMT) model has recently begun to include source context modeling, under the assumption that the proper lexical choice of an ambiguous word can be determined from the context in which it appears. Various types of lexical and syntactic features such as words, parts-of-speech, and supertags have been explored as effective source context in SMT. In this paper, we show that position-independent syntactic dependency relations of the head of a source phrase can be modeled as useful source context to improve target phrase selection and thereby improve overall performance of PB-SMT. On a Dutch—English translation task, by combining dependency relations and syntactic contextual features (part-of-speech), we achieved a 1.0 BLEU (Papineni et al., 2002) point improvement (3.1% relative) over the baseline.	bleu;baseline (configuration management);experiment;image scaling;lexical choice;meteor;part-of-speech tagging;satisfiability modulo theories;statistical machine translation	Rejwanul Haque;Sudip Kumar Naskar;Antal van den Bosch;Andy Way	2009			natural language processing;speech recognition;computer science;programming language;dependency grammar	NLP	-21.930105372442696	-76.56943353193196	27279
0324fdde1d8702da4ecaed7fb61f027f7ef58795	searching for the x-factor: exploring corpus subjectivity for word embeddings		We explore the notion of subjectivity, and hypothesize that word embeddings learnt from input corpora of varying levels of subjectivity behave differently on natural language processing tasks such as classifying a sentence by sentiment, subjectivity, or topic. Through systematic comparative analyses, we establish this to be the case indeed. Moreover, based on the discovery of the outsized role that sentiment words play on subjectivity-sensitive tasks such as sentiment classification, we develop a novel word embedding SentiVec which is infused with sentiment information from a lexical resource, and is shown to outperform baselines on such tasks.	natural language processing;text corpus;word embedding	Maksim Tkatchenko;Chong Cher Chia;Hady W. Lauw	2018			artificial intelligence;natural language processing;computer science;computational linguistics;subjectivity	NLP	-20.764915347855	-71.43955170349997	27284
2d7e0794fe95fc9197fad6f4fa879022300f91bc	multimedia automatic annotation by mining label set correlation		Organizing and retrieving multimedia data heavily rely on the relevant textual descriptions. Multimedia automatic annotation, which assigns text labels to multimedia samples, has been widely studied. Among others, search-based annotation methods are well suited for annotation tasks on large-scale datasets and are studied in depth because of their simplicity and scalability. However, classical search based annotation methods address this problem by treating each label independently, which ignores the correlation between different labels in the assigned label set. This paper aims to integrate the relevant information of the label set with respect to the multimedia content and the inner correlated information of the label set into a joint learning framework. We evaluate the performance of the proposed method on MIRFLICKR-25000 and NUS-WIDE datasets. Experimental results show that the proposed annotation method achieves excellent performance.	approximation algorithm;baseline (configuration management);goto;heuristic;information;scalability;search algorithm	Feng Tian;Xukun Shen;Xianmei Liu	2017	Multimedia Tools and Applications	10.1007/s11042-017-5170-3	computer science;data mining;image retrieval;information retrieval;scalability;multimedia;annotation	Web+IR	-17.742750769473655	-59.662040846946056	27350
7a547353febf10f1e6fa2d7d2904bd476ed5383f	application of computational media aesthetics methodology to extracting color semantics in film	computational methods;lecture recording;color;indexing of information;event detection;multimedia systems;multimedia in education;probabilistic model;clustering method;authoring;data storage equipment;similarity measure;collaborative workspace;annotation capturing;multi layer recording	Using film grammar as the underpinning, we study the extraction of structures in video based on color using a wide configuration of clustering methods combined with existing and new similarity measures. We study the visualisation of these structures, which we call Scene-Cluster Temporal Charts and show how it can bring out the interweaving of different themes and settings in a film. We also extract color events that filmmakers use to draw/force a viewer's attention to a shot/scene. This is done by first extracting a set of colors used rarely in film, and then building a probabilistic model for color event detection. We demonstrate with experimental results from ten movies that our algorithms are effective in the extraction of both scene-cluster temporal charts and color events.	3d film;algorithm;chart;cluster analysis;color;statistical model	Ba Tu Truong;Svetha Venkatesh;Chitra Dorai	2002		10.1145/641007.641079	statistical model;computer vision;computer science;machine learning;multimedia;world wide web;statistics	NLP	-23.782591632983834	-58.945522056926166	27417
4cbd81db83c6f70a741fd1082ae3413133f8bd95	arabic text classification using n-gram frequency statistics a comparative study.	dissimilarity measure;text classification;comparative study;statistical techniques	This paper presents the results of classifying Arabic text documents using the N-gram frequency statistics technique employing a dissimilarity measure called the “Manhattan distance”, and Dice’s measure of similarity. The Dice measure was used for comparison purposes. Results show that N-gram text classification using the Dice measure outperforms classification using the Manhattan measure.	document classification;n-gram;taxicab geometry	Laila Khreisat	2006			speech recognition;pattern recognition	Web+IR	-22.933702926563253	-64.94838809547666	27455
22bf8b3d5684087448a92ef758050807aeb8d0b8	annotation manuelle de matchs de foot : oh la la la ! l'accord inter-annotateurs ! et c'est le but ! (manual annotation of football matches : inter-annotator agreement ! gooooal !) [in french]		Manual Annotation of Football Matches : Inter-annotator Agreement ! Gooooal ! We present here an annotation campaign of commentaries of football matches in French. The annotation was done from a very heterogeneous text corpus of both match minutes and video commentary transcripts. We show how the intraand inter-annotator agreement can be used efficiently during the whole campaign by proposing a definition of the markables suited to our type of task, as well as emphasizing the importance of using it appropriately. We also show how some clues, collected through statistical analyses, could be used to help correcting the annotations. These statistical analyses are then used to assess the impact of the source modality (written or spoken) on the cost and quality of the annotation process. MOTS-CLÉS : annotation manuelle, accords inter-annotateurs.	inter-rater reliability;linear algebra;modality (human–computer interaction);norm (social);text corpus	Karën Fort;Vincent Claveau	2012			speech recognition;geography;world wide web;cartography	NLP	-28.13975817380576	-75.79274007744539	27465
a742f7a4771d70865359f259ba64eec9bf8926fa	narrative schema as world knowledge for coreference resolution	narrative schema;scripts;coreference resolution;natural language processing	In this paper we describe the system with which we participated in the CoNLL-2011 Shared Task on modelling coreference. Our system is based on a cluster-ranking model proposed by Rahman and Ng (2009), with novel semantic features based on recent research on narrative event schema (Chambers and Jurafsky, 2009). We demonstrate some improvements over the baseline when using schema information, although the effect varied between the metrics used. We also explore the impact of various features on our system’s performance.	baseline (configuration management);commonsense knowledge (artificial intelligence);database schema;natural language processing	Joseph Irwin;Mamoru Komachi;Yuji Matsumoto	2011			natural language processing;computer science;linguistics;communication	NLP	-26.55707578783294	-71.4272035890452	27612
72fa586d652126e2c94c000eae628eb86a22da81	preliminary report of iii&cyut for ntcir-11 mednlp-2		We construct a supervised learning system to participate MedNLP2 task in NTCIR-11 that find the keyword out correctly at right position and normalize to identify unique id in ICD10 [4]. In our system, We pick part-of-speech tagging (POS) [1] as feature to train machine learning models based on Conditional Random Fields (CRF) [3] for named entities extraction, then construct a hierarchical classifier to determine ICD code of the terms.	conditional random field;hierarchical classifier;machine learning;named entity;part-of-speech tagging;supervised learning;unique key	Liang-Pu Chen;Hsiang Lun Lin;Yan Shen Lai;Ping-Che Yang	2014			computer science;machine learning;pattern recognition;data mining	NLP	-23.22036874465053	-70.54836693322477	27629
4852bc91c90c0ee819843f6838c4083af6dced34	adaptive visual information retrieval by changing visual vocabulary sizes in context of user intentions	vocabulary visualization indexes streaming media vectors feature extraction conferences;human computer interaction;vocabulary;content based image retrieval adaptive visual information retrieval visual vocabulary sizes user intention context visual content internet retrieval adaptation intentionality degree;visual information retrieval;retrieval adaptation;internet;user intentions;information need;bag of visual words;content based retrieval;vocabulary content based retrieval human computer interaction image retrieval internet;retrieval adaptation user intentions bag of visual words;image retrieval	The overwhelming availability of visual content on the Internet poses a serious problem: although there are huge resources to tap, users often cannot find the content they actually want. The information need of a user is based on the user's intention or the goal s/he wants to achieve. In this paper we distinguish between users that have clearly defined, specific information needs and goals and those that have a broader interest and less specific needs. We present a novel approach to retrieval adaptation based on the degree of intentionality. Our tests on two well known data sets demonstrate the potential of our approach.	algorithm;cluster analysis;content-based image retrieval;feature vector;fuzzy logic;hierarchical clustering;information needs;information retrieval;intentionality;internet;run time (program lifecycle phase);vagueness;vocabulary;way to go	Marian Kogler;Mathias Lux;Oge Marques	2011	2011 Workshop on Multimedia on the Web	10.1109/MMWeb.2011.13	document retrieval;computer vision;visual word;relevance;cognitive models of information retrieval;computer science;multimedia;information retrieval;human–computer information retrieval	Web+IR	-15.351703657106608	-58.55752760445638	27634
66fa058b919385320a66233abc264f6fb3c9276e	openeval: web information query evaluation		In this paper, we investigate information validation tasks that are initiated as queries from either automated agents or humans. We introduce OpenEval, a new online information validation technique, which uses information on the web to automatically evaluate the truth of queries that are stated as multiargument predicate instances (e.g., DrugHasSideEffect(Aspirin, GI Bleeding))). OpenEval gets a small number of instances of a predicate as seed positive examples and automatically learns how to evaluate the truth of a new predicate instance by querying the web and processing the retrieved unstructured web pages. We show that OpenEval is able to respond to the queries within a limited amount of time while also achieving high F1 score. In addition, we show that the accuracy of responses provided by OpenEval is increased as more time is given for evaluation. We have extensively tested our model and shown empirical results that illustrate the effectiveness of our approach compared to related techniques.	correctness (computer science);experiment;exploit (computer security);f1 score;online algorithm;web page;web search engine;world wide web	Mehdi Samadi;Manuela M. Veloso;Manuel Blum	2013			computer science;machine learning;data mining;database;information retrieval	Web+IR	-30.290374311824966	-54.95740101744717	27637
14908a18ff831005b6b4fc953ce61e1b4e7b54ee	practical text classification with large pre-trained language models		Multi-emotion sentiment classification is a natural language processing (NLP) problem with valuable use cases on realworld data. We demonstrate that large-scale unsupervised language modeling combined with finetuning offers a practical solution to this task on difficult datasets, including those with label class imbalance and domain-specific context. By training an attention-based Transformer network (Vaswani et al. 2017) on 40GB of text (Amazon reviews) (McAuley et al. 2015) and fine-tuning on the training set, our model achieves a 0.69 F1 score on the SemEval Task 1:E-c multidimensional emotion classification problem (Mohammad et al. 2018), based on the Plutchik wheel of emotions (Plutchik 1979). These results are competitive with state of the art models, including strong F1 scores on difficult (emotion) categories such as Fear (0.73), Disgust (0.77) and Anger (0.78), as well as competitive results on rare categories such as Anticipation (0.42) and Surprise (0.37). Furthermore, we demonstrate our application on a real world text classification task. We create a narrowly collected text dataset of real tweets on several topics, and show that our finetuned model outperforms general purpose commercially available APIs for sentiment and multidimensional emotion classification on this dataset by a significant margin. We also perform a variety of additional studies, investigating properties of deep learning architectures, datasets and algorithms for achieving practical multidimensional sentiment classification. Overall, we find that unsupervised language modeling and finetuning is a simple framework for achieving high quality results on realworld sentiment classification.		Neel Kant;Raul Puri;Nikolai Yakovenko;Bryan Catanzaro	2018	CoRR			NLP	-20.437018690127655	-71.11904621789365	27645
c2a197db326a38a9b7531283263a420cbc0d686c	image retrieval based on high level concept detection and semantic labelling	cbir;concept detection;image annotation;classification;semantic labelling;support vector machine	This paper presents a novel approach to high-level concept detection and retrieval in images based on a combination of visual thesaurus and multi-class supervised learning. The visual thesaurus includes both conceptual and spatial location information of semantic concepts that are key to image labelling. Our image annotation (or labelling) process includes segmenting and building an image signature. The visual thesaurus is then built using a multi-class supervised SYM classifier. Algorithm for spatial location matching is included. Similarity matching during retrieval is performed on both the content as well as the location information using the standard Euclidean distance. Corel data set was used for experimentation and results were compared with two related approaches to visual thesaurus and image retrieval.	high-level programming language;image retrieval	Buddhika Madduma;Sheela Ramanna	2012	Intelligent Decision Technologies	10.3233/IDT-2012-0135	support vector machine;computer vision;visual word;biological classification;image retrieval;computer science;machine learning;pattern recognition;automatic image annotation;information retrieval	AI	-14.190712134987345	-59.428344584461236	27659
0a6803dba54e512213b0f819e43893f7b10c3517	hierarchical text summarization for wap-enabled mobile devices	mobile device;text summarization;hierarchical summarization;mobile devices	We present WAP MEAD, a WAP-enabled text summarization system. It incorporates a state-of-the art text summarizer enhanced to produce hierarchical summaries that are appropriate for various types of mobile devices, including cellular phones.	automatic summarization;mobile device;mobile phone	Dragomir R. Radev;Omer Kareem;Jahna Otterbacher	2005		10.1145/1076034.1076191	multi-document summarization;computer science;automatic summarization;mobile device;multimedia;world wide web;information retrieval	HCI	-16.66096269413227	-55.02884533485899	27663
fb767befa1d36dfa9e4658a34ef2c6cc18464aa6	icl kbp approaches to knowledge base population at tac2010		This paper reports the ICL KBP team participated in the TAC2010-Knowledge Base Popolation Track. We submitted results for Entity Linking task and Slot Filling task. For Entity Linking, we implemented a simple unsupervised method to select the candidate entities in the Wikipedia Reference Knowledge Base for the given query document which describes the query name-string. For Slot Filling, we treat it as a binary classification problem for each candidate slot value and use type constraint to filter the results. Experimental results reveals that our method reaches the median values of all participants for both tasks.	binary classification;entity linking;icl;knowledge base;unsupervised learning;wikipedia	Yang Song;Zhengyan He;Houfeng Wang	2010			computer science;bioinformatics;data mining;world wide web	NLP	-30.297978257940844	-64.2648209456686	27704
57b1ae67a0d5f6965e5a7afced8004be9808991d	clustering algorithm based on semantic distance for xml documents	heterogeneous semantic distance documents clustering;databases;document clustering;cluster algorithm;pattern clustering;document handling;clustering algorithm;clustering algorithms xml databases heuristic algorithms dictionaries educational institutions application software information science data engineering computer science;xml document handling pattern clustering;document type definition clustering algorithm semantic distance xml document global semantic dictionary dtd cluster;satisfiability;heterogeneous;semantic distance;heuristic algorithms;global semantic dictionary;dictionaries;xml;clustering algorithms;xml document;dtd cluster;approximation methods;documents clustering;document type definition;conferences	As the information grows exponentially, it has become a new and basic requirement to reduce the querying area efficiently and accurately for information querying. This paper proposes a semantic distance based clustering algorithm for XML documents. It discusses the algorithm in two steps, Firstly, it forms some DTD clusters with all heterogeneous DTD documents by using the global semantic dictionary, Secondly, it computes the semantic distance between XML documents which corresponded certain DTD cluster, then build some finally XML clusters according threshold value given beforehand. Users can locate document cluster and query within this area without extending all over XML documents, and the querying results satisfying the users' requirements can be returned rapidly. The experiments show that this algorithm has good categorization function, and can facilitate information querying.	algorithm;categorization;cluster analysis;dictionary;experiment;requirement;semantic similarity;xml;xml database	Lingxian Yang;Jinguang Gu;Heping Chen	2009	2009 First International Workshop on Database Technology and Applications	10.1109/DBTA.2009.134	xml validation;xml encryption;simple api for xml;xml;document clustering;computer science;document structure description;machine learning;data mining;database;cluster analysis;xml signature;xml schema editor;information retrieval;efficient xml interchange	DB	-28.535073869863673	-57.769620651861324	27722
d46cf088055a3af9d665b52f35e28072cb01fef3	an improved graph model for chinese spell checking		In this paper, we propose an improved graph model for Chinese spell checking. The model is based on a graph model for generic errors and two independentlytrained models for specific errors. First, a graph model represents a Chinese sentence and a modified single source shortest path algorithm is performed on the graph to detect and correct generic spelling errors. Then, we utilize conditional random fields to solve two specific kinds of common errors: the confusion of “在” (at) (pinyin is ‘zai’ in Chinese), “再” (again, more, then) (pinyin: zai) and “的” (of) (pinyin: de), “地” (-ly, adverb-forming particle) (pinyin: de), “得” (so that, have to) (pinyin: de). Finally, a rule based system is exploited to solve the pronoun usage confusions: “她” (she) (pinyin: ta), “他” (he) (pinyin: ta) and some others fixed collocation errors. The proposed model is evaluated on the standard data set released by the SIGHAN Bake-off 2014 shared task, and gives competitive result. ∗This work was partially supported by the National Natural Science Foundation of China (No. 60903119, No. 61170114, and No. 61272248), the National Basic Research Program of China (No. 2013CB329401), the Science and Technology Commission of Shanghai Municipality (No. 13511500200), the European Union Seventh Framework Program (No. 247619), the Cai Yuanpei Program (CSC fund 201304490199 and 201304490171), and the art and science interdiscipline funds of Shanghai Jiao Tong University (A study on mobilization mechanism and alerting threshold setting for online community, and media image and psychology evaluation: a computational intelligence approach). †Corresponding author.	collocation;computational intelligence;conditional random field;dijkstra's algorithm;online community;rule-based system;shortest path problem;spell checker	Yang Xin;Hai Zhao;Yuzhu Wang;Zhongye Jia	2014		10.3115/v1/W14-6825	natural language processing;null model;computer science;machine learning;moral graph;algorithm	NLP	-28.16625515309142	-74.54018455606726	27726
51ae365abb5aded1745b6e67672e2521eadf04ef	emotion mining using semantic similarity				Rafiya Jan;A. A. Khan	2018	IJSE	10.4018/IJSE.2018070101	psychology;artificial intelligence;semantic similarity;machine learning	ML	-24.30099406227065	-61.1451463011155	27754
10ea0f001d647afcc0afc7e3132fdcee1112b3d1	a link graph-based approach to identify forum spam	web spam;forum spam;link graph;security	Web spammers have taken note of the popularity of public forums such as blogs, wikis, webboards, and guestbooks. They are now exploiting them with the purpose of driving traffic to their malicious or fraudulent websites, such as those used for phishing, distributing malware, or selling counterfeit pharmaceuticals. A popular technique they use is to spam these forums with URLs to their spam websites. We consider the problem of classifying URLs posted to forums as spam or legitimate by considering the link structure of the graph rooted at the posted URL. We investigate various graph metrics and associated metadata to analyze link structures. To lessen noisy structural characteristics of the link graphs for spam classification, we also examine two techniques: differing depths and aggregating sub-graphs of the link graphs. Our results show that a support vector machine classifier based on combinations of graph metrics and metadata of link graphs can achieve a pragmatically high performance in forum spam detection. Copyright © 2014 John Wiley & Sons, Ltd.	anti-spam techniques;bigraph;blog;guestbook;john d. wiley;malware;phishing;spamming;support vector machine;wiki	Youngsang Shin;Steven Myers;Minaxi Gupta;Predrag Radivojac	2015	Security and Communication Networks	10.1002/sec.970	spam blog;forum spam;computer science;spamdexing;information security;spamming;social spam;spambot;spam and open relay blocking system;data mining;internet privacy;world wide web;sping	Metrics	-20.14499111393592	-55.414886065565625	27770
7dd5ccb65241fab47a8b6c8f51ba719ffd5c9509	sinai-emma: vectores de palabras para el análisis de opiniones en twitter		In this work, a polarity classification system is developed for the task 1 of workshop TASS 2015 by the SINAI-EMMA team. Our system takes advantage of 5 linguistic resources for building vectors of words. The results encourage us to continue studying the contribution of vectors of words to Sentiment Analysis.	emma;sentiment analysis;tass times in tonetown	Eugenio Martínez-Cámara;Miguel Ángel García Cumbreras;María Teresa Martín-Valdivia;Luis Alfonso Ureña López	2015			political science	NLP	-21.762907690652078	-69.15190581924901	27792
ffd74e6910f4ae59e52c39001bb135038b64c2df	deep models for converting sarcastic utterances into their non sarcastic interpretation		Sarcasm is a form of speech in which the the implied sentiment is the opposite of literal meaning. In this paper, we present the task of sarcasm interpretation, defined as converting a sarcastic utterance into its non-sarcastic (literal) interpretation. We present three approaches for the task: (a) a rule-based approach that considers sarcasm as a form of dropped negation and associate negation words with verbs present in the sarcastic utterance, (b) statistical machine translation-based (SMT) approach that address the sarcasm interpretation task as monolingual machine translation and (c) three deep learning-based (DL) architectures, Encoder-Decoder Network, Attention Network and Pointer Generator Network. We also discuss the scope of future work to further enhance the proposed models for sarcasm interpretation.		Abhijeet Dubey;Aditya Joshi;Pushpak Bhattacharyya	2019		10.1145/3297001.3297043	machine translation;pointer (computer programming);literal and figurative language;deep learning;sarcasm;negation;linguistics;artificial intelligence;utterance;computer science	NLP	-28.43969269894779	-79.16512598442996	27856
6c367514e7adbcd957d27aa53900f7a2c4093855	research on extracting conceptual frameworks of sentence groups	computers;professional activities;kernel;natural languages;data mining;conceptual framework;or phrases;domain sentence categories;semantic chunks;natural language processing knowledge acquisition;sentence groups;knowledge acquisition;semantic chunks conceptual framework extraction sentence groups serial sentence categories natural language processing domain sentence categories;conceptual framework extraction;acoustics concrete natural languages humans context modeling logic joining processes books education kernel;humans;natural language processing;serial sentence categories;knowledge based systems	A sentence group is a processing unit between a sentence and a paragraph or an article. It is described with three aspects: domain, situation and background. Domain is the conceptual category of a sentence group. Situation is the conceptual framework of a sentence group. The conceptual framework of a sentence group is defined with several serial sentence categories (SCs), the semantic chunks in the SCs, and the conceptual restriction of those semantic chunks. Extracting the framework should base on the knowledge of domain sentence categories and then fill the framework with the right words or phrases in sentences, according to the SCs and semantic chunks. This paper introduces the description of conceptual frameworks of sentence groups. It also shows how to extract the elements in a framework based on analyzing the SCs of sentences. Finally, some aspects must be improved to reach a higher processing performance.		XiangFeng Wei;Jianming Miao;Quan Zhang	2009	2009 IEEE International Conference on Granular Computing	10.1109/GRC.2009.5255054	natural language processing;semantic role labeling;kernel;speech recognition;computer science;knowledge-based systems;data mining;conceptual framework;natural language	Vision	-31.880400759693657	-79.91571525658291	27912
493564e3498f4fdaf4a6c5aef40646363630c864	multi-class composite n-gram language model using multiple word clusters and word successions	language model	In this paper, a new language model, the Multi-Class Composite N-gram, is proposed to avoid a data sparseness problem in small amount of training data. The Multi-Class Composite Ngram maintains an accurate word prediction capability and reliability for sparse data with a compact model size based on multiple word clusters, so-called Multi-Classes. In the Multi-Class, the statistical connectivity at each position of the N-grams is regarded as word attributes, and one word cluster each is created to represent positional attributes. Furthermore, by introducing higher order word N-grams through the grouping of frequent word successions, Multi-Class N-grams are extended to Multi-Class Composite N-grams. In experiments, the MultiClass Composite N-grams result in 9.5% lower perplexity and a 16% lower word error rate in speech recognition with a 40% smaller parameter size than conventional word 3-grams.	experiment;grams;language model;n-gram;neural coding;perplexity;sparse matrix;speech recognition;word error rate	Shuntaro Isogai;Katsuhiko Shirai;Hirofumi Yamamoto;Yoshinori Sagisaka	2001			natural language processing;speech recognition;factored language model;word error rate;computer science;pattern recognition;language model	NLP	-20.60062614015744	-75.81025470203849	27914
58f8ce450271dff5d4e07419bc031fa54d0d62ac	alignment link projection using transformation-based learning	new word-alignment approach;alignment system;alignment error rate;new alignment link;transformation-based learning;english-spanish data;english-chinese data;word alignment system;alignment link projection approach;relative reduction;word alignment	We present a new word-alignment approach that learns errors made by existing word alignment systems and corrects them. By adapting transformationbased learning to the problem of word alignment, we project new alignment links from already existing links, using features such as POS tags. We show that our alignment link projection approach yields a significantly lower alignment error rate than that of the best performing alignment system (22.6% relative reduction on EnglishSpanish data and 23.2% relative reduction on English-Chinese data).	bitext word alignment;brown corpus;cluster analysis;data structure alignment;end-to-end principle;machine translation;microsoft word for mac;point of sale;protologism;supervised learning	Necip Fazil Ayan;Bonnie J. Dorr;Christof Monz	2005			computer vision;computer science;machine learning;pattern recognition	NLP	-20.927333598020446	-76.74318804344648	27917
31cfa5e971a933414fe16c37fcc692e075d2b1ba	pattern recognition method for classification of agricultural scientific papers in polish		Calculation of text similarity is an essential task for the text analysis and classification. It be can based, e.g., on Jaccard, cosine or other similar measures. Such measures consider the text as a bag-of-words and, therefore, lose some syntactic and semantic features of its sentences. This article presents a different measure based on the so-called artificial sentence pattern (ASP) method. This method has been developed to analyze texts in the Polish language which has very rich inflection. Therefore, ASP has utilized syntactic and semantic rules of the Polish language. Nevertheless, we argue that it admits extensions to other languages. As a result of the analysis, we have obtained several hypernodes which contain the most important words. Each hypernode corresponds to one of the examined documents, the latter being published papers from agriculture domain written in Polish. Experimental results obtained from that set of papers have been described and discussed. Those results have been visually illustrated using graphs of hypernodes and compared with Jaccard and cosine measures.	pattern recognition	Piotr Wrzeciono;Waldemar Karwowski	2018		10.1007/978-3-030-00692-1_43	jaccard index;polish;artificial intelligence;inflection;syntax;pattern recognition;text mining;computer science;graph;sentence	Vision	-26.77937375311333	-69.5711834820725	27928
1e8c3b6517a9d309b0cfe4faa0b4e05cbe324aab	semeval-2016 task 9: chinese semantic dependency parsing		This paper describes the SemEval-2016 Shared Task 9: Chinese semantic Dependency Parsing. We extend the traditional treestructured representation of Chinese sentence to directed acyclic graphs that can capture richer latent semantics, and the goal of this task is to identify such semantic structures from a corpus of Chinese sentences. We provide two distinguished corpora in the NEWS domain with 10,068 sentences and the TEXTBOOKS domain with 14,793 sentences respectively. We will first introduce the motivation for this task, and then present the task in detail including data preparation, data format, task evaluation and so on. At last, we briefly describe the submitted systems and analyze these results.	algorithm;data pre-processing;directed acyclic graph;parsing;semeval;text corpus	Wanxiang Che;Yanqiu Shao;Ting Liu;Yu Ding	2012				NLP	-22.836244148448973	-69.95947039223836	27940
9979ab353468c90712455949c73e79548a186f11	towards a process mining approach to grammar induction for digital libraries - syntax checking and style analysis		Since most content in Digital Libraries and Archives is text, there is an interest in the application of Natural Language Processing (NLP) to extract valuable information from it in order to support various kinds of user activities. Most NLP techniques exploit linguistic resources that are language-specific, costly and error prone to produce manually, which motivates research for automatic ways to build them.		Stefano Ferilli;Sergio Angelastro	2019		10.1007/978-3-030-11226-4_23	information retrieval;world wide web;digital library;process mining;grammar induction;computer science;exploit	SE	-29.80069458179409	-71.20887992388124	27963
dbfa906b051ac36638690167228fb69ac5aabcb0	sentence similarity measures for fine-grained estimation of topical relevance in learner essays		We investigate the task of assessing sentencelevel prompt relevance in learner essays. Various systems using word overlap, neural embeddings and neural compositional models are evaluated on two datasets of learner writing. We propose a new method for sentencelevel similarity calculation, which learns to adjust the weights of pre-trained word embeddings for a specific task, achieving substantially higher accuracy compared to other relevant baselines.	neural oscillation;relevance	Marek Rei;Ronan Cummins	2016		10.18653/v1/W16-0533	natural language processing;computer science;artificial intelligence;machine learning;pattern recognition	NLP	-20.693684932221277	-72.33172709515013	27995
3e7ca8147afb433ce5b50a5c44cc0346074a6598	the short stories corpus: notebook for pan at clef 2015		In this work we describe the construction of a plagiarism detection/text reuse corpus submitted for the PAN-2015 Evaluation Lab. Our corpus consists of four different text reuse scenarios namely, (1) no-plagiarism, (2) story-retelling, (3) synonym-replacement and (4) character-substitution. Among these scenarios the most interesting one is story retelling through it we find patterns of textual similarity between story retellings. We use Grimm brothers fairy tales as described in the Project Gutenberg as the source of our documents. The corpus consists of 200 pairs of documents, with 50 document pairs for each type of text reuse. Empirical observation shows interesting patterns of textual similarity within the corpus. Furthermore, plagiarism detection using various approaches shows the difficulty of detection of various groups within the corpus.	text corpus	Faisal Alvi;Mark Stevenson;Paul D. Clough	2015			reuse;natural language processing;clef;speech recognition;art;artificial intelligence;plagiarism detection	NLP	-31.237869616789	-72.17711145182504	28042
2be63869d9b30a6e2d732eaaca089624272f1c87	an agent for web information dissemination based on a genetic algorithm	legislation;information sources;genetic algorithms information filtering information retrieval information filters legislation software agents feedback web mining text mining face detection;text mining;information retrieval;data mining;software agents;information dissemination;brazilian federal revenue web information dissemination genetic algorithm personal agent information retrieval techniques user feedback user interests brazilian tax legislation;web mining;genetic algorithm;genetic algorithms;information agent;user interfaces information retrieval information dissemination data mining genetic algorithms software agents;user interfaces	We present and evaluate the architecture of a personal agent that mines web information sources and retrieves documents according to user’s interests. The agent represents and retrieves documents using classical information retrieval techniques and uses user’s feedback and genetic algorithms to learn and adapt to changes in user’s interests. The agent was customized to help professionals that need to be up to date with the Brazilian Tax Legislation, stored at the Brazilian Federal Revenue site. Experiments discussed on the paper shown that the agent has good precision and recall rates when retrieving documents, and is also able to successfully detect changes in user’s interests.	genetic algorithm;information retrieval;intelligent agent;physical information;precision and recall	Max Streicher Vallim;Juan Manuel Adán Coello	2003		10.1109/ICSMC.2003.1244486	web mining;text mining;genetic algorithm;relevance;computer science;artificial intelligence;information filtering system;data mining;world wide web;information retrieval;human–computer information retrieval	Web+IR	-29.988987361986894	-52.25965569944222	28049
b5a292cb680a2ba1306440c8a2e9bbcd370b87d8	automatic sense tagging using parallel corpora.	coarse grained;parallel corpora	This article reports the results of an analysis of translation equivalents in six languages from different language families, automatically extracted from an on-line 7-way parallel corpus of George Orwell’s Nineteen EightyFour. The goal is to determine sense distinctions that can be used to automatically sense-tag the data. Our results show that sense distinctions derived from cross-lingual information correspond to those made by human annotators, especially at the coarse-grained level. We also show that the reliability of sense assignments at finer-grained levels is comparable for human annotators and those produced automatically with cross-lingual data.	online and offline;orwell;parallel text;text corpus	Nancy Ide;Tomaz Erjavec;Dan Tufis	2001			natural language processing;artificial intelligence;computer science	NLP	-26.79454287416524	-74.983530919426	28056
28c87a02d924e4aa90ac99d6ca9ed0da1a6bbc0c	sg-wrap: a schema-guided wrapper generator	rule refiner;hypermedia markup languages;web pages;user interfaces hypermedia markup languages internet;usability testing;schema acquirer;information retrieval;html data mining xml data preprocessing internet councils spatial databases user interfaces web pages uniform resource locators;data type descriptors;sg wrap;rule generator;data mining;rule refiner sg wrap schema guided wrapper generator internet world wide web html pages web wrapper technology user interaction html document data type descriptors schema acquirer rule generator;html pages;html;web wrapper technology;internet;data extraction;spatial databases;xml;councils;world wide web;html document;uniform resource locators;user interaction;data preprocessing;user interfaces;schema guided wrapper generator	Extracting data from Web pages using wrappers is a fundamental problem arising in a large variety of applications of vast practical interests. There are two main issues relevant to Web-data extraction, namely wrapper generation and wrapper maintenance. In this paper, we propose a novel schema-guided approach to the problem of automatic wrapper maintenance. It is based on the observation that despite various page changes, many important features of the pages are preserved, such as syntactic patterns, annotations, and hyperlinks of the extracted data items. Our approach uses these preserved features to identify the locations of the desired values in the changed pages, and repair wrappers correspondingly by inducing semantic blocks from the HTML tree. Our intensive experiments on real Web sites show that the proposed approach can effectively maintain wrappers to extract desired data with high accuracies.	contain (action);database;html;information source;internet;page (document);radionuclide generators;semiconductor industry;shrink wrap contract;suicidegirls;world wide web	Xiaofeng Meng;Hongjun Lu;Haiyan Wang;Mingzhe Gu	2002		10.1109/ICDE.2002.994743	the internet;xml;html;computer science;web page;data mining;database;data pre-processing;user interface;world wide web;information retrieval	DB	-29.764038984985586	-54.515863000825924	28064
67aefff9fc149501ae67a7e175a11655f240ea68	automated assessment of english-learner writing		In this thesis, we investigate automated assessment (AA) systems of free text that automatically analyse and score the quality of writing of learners of English as a second (or other) language. Previous research has employed techniques that measure, in addition to writing competence, the semantic relevance of a text written in response to a given prompt. We argue that an approach which does not rely on task-dependent components or data, and directly assesses learner English, can produce results as good as promptspecific models. Furthermore, it has the advantage that it may not require re-training or tuning for new prompts or assessment tasks. We evaluate the performance of our models against human scores, manually annotated in the Cambridge Learner Corpus, a subset of which we have released in the public domain to facilitate further research on the task. We address AA as a supervised discriminative machine learning problem, investigate methods for assessing different aspects of writing prose, examine their generalisation to different corpora, and present state-of-the-art models. We focus on scoring general linguistic competence and discourse coherence and cohesion, and report experiments on detailed analysis of appropriate techniques and feature types derived automatically from generic text processing tools, on their relative importance and contribution to performance, and on comparison with different discriminative models, whilst also experimentally motivating novel feature types for the task. Using outlier texts, we examine and address validity issues of AA systems and, more specifically, their robustness to subversion by writers who understand something of their workings. Finally, we present a user interface that visualises and uncovers the ‘marking criteria’ represented in AA models, that is, textual features identified as highly predictive of a learner’s level of attainment. We demonstrate how the tool can support their linguistic interpretation and enhance hypothesis formation about learner grammars, in addition to informing the development of AA systems and further improving their performance.	abductive reasoning;command-line interface;discriminative model;experiment;item unique identification;machine learning;relevance;subversion;text corpus;user interface;while	Helen Yannakoudakis	2013			robustness (computer science);discriminative model;natural language processing;artificial intelligence;generalization;text processing;computer science;rule-based machine translation;subversion;user interface;linguistic competence	NLP	-26.481683872029087	-73.66104344214	28067
8158d4b4de878a2b063c71c50c103bda5d44a617	apprendre à ordonner la frontière de crawl pour le crawling orienté		Focused crawling consists in searching and retrieving a set of documents relevant to a specific domain of interest from the Web. Such crawlers prioritize their fetches by relying on a crawl frontier ordering strategy. In this article, we propose to learn this ordering strategy from annotated data using learning-to-rank algorithms. Such approach allows us to cope with tunneling and to integrate a large number of heterogeneous features to guide the crawler. We describe a novel method to learn a domain-independent ranking function for topical Web crawling. We validate the relevance of our approach on “large” crawls of 40,000 documents on a set of 15 topics from the OpenDirectory, and show that our approach provides an increase in precision (harvest rate) of up to 10% compared to a baseline Shark Search algorithm. Finally, we discuss future leads regarding the application of learning-to-rank to focused Web crawling. MOTS-CLÉS : Crawling orienté, Apprentissage de fonction d’ordonnancement, Recherche d’Information sur le Web.	apple open directory;baseline (configuration management);focused crawler;learning to rank;linear algebra;ranking (information retrieval);relevance;search algorithm;tunneling protocol;web crawler;world wide web	Clément de Groc;Xavier Tannier	2014		10.24348/sdnri.2014.CORIA-16	history;performance art	Web+IR	-30.61927737576273	-61.91482909061526	28073
1a4fcacfe690ccfff36b7f226366d8fbb297ba1b	improving the quality of minority class identification in dialog act tagging		We present a method of improving the performance of dialog act tagging in identifying minority classes by using per-class feature optimization and a method of choosing the class based not on confidence, but on a cascade of classifiers. We show that it gives a minority class F-measure error reduction of 22.8%, while also reducing the error for other classes and the overall error by about 10%.	class consciousness;computation;language technology;mathematical optimization;sensor;dialog	Adinoyi Omuya;Vinodkumar Prabhakaran;Owen Rambow	2013			speech recognition;computer science;machine learning;data mining	ML	-22.65480206851411	-70.80795980688772	28083
914b313dd98a158b699523df9516c95d92f2cdbf	language models based on semantic composition	long-range semantic dependency;standard n-gram language model;semantic composition;composition task;structured language model;n-gram model;spatial semantic representation;novel statistical language model;composition model;underlying semantic space;language model	In this paper we propose a novel statistical language model to capture long-range semantic dependencies. Specifically, we apply the concept of semantic composition to the problem of constructing predictive history representations for upcoming words. We also examine the influence of the underlying semantic space on the composition task by comparing spatial semantic representations against topic-based ones. The composition models yield reductions in perplexity when combined with a standard n-gram language model over the n-gram model alone. We also obtain perplexity reductions when integrating our models with a structured language model.	brian;display resolution;interaction;interpolation;language model;mathematical structure;microsoft word for mac;n-gram;perplexity;sparse matrix;statistical model;trigram;victor allis;victor animatograph corporation	Jeff Mitchell;Mirella Lapata	2009			natural language processing;language identification;semantic similarity;semantic computing;multinet;speech recognition;universal networking language;explicit semantic analysis;semantic search;computer science;pattern recognition;semantic compression;semantic equivalence;linguistics;probabilistic latent semantic analysis;semantic gap;language model	NLP	-18.776364666248888	-76.94086970033094	28100
dc5452dfeb526b95c0258580376ea638b770f450	information retrieval and extraction from the web: the crossmarc approach	information retrieval	The paper presents the CROSSMARC approach for the complex task of identification of interesting web sites and web pages and the extraction of information from them. This task is hard because most of the information on the Web today is in the form of HTML documents, which are designed for presentation purposes and not for automatic extraction systems. This task becomes even harder in a multilingual context, where web pages in different languages need to be considered. CROSSMARC approach focuses on the easy customization of web information retrieval and extraction technology to new domains and languages. This is achieved by adopting and implementing an open, multi-lingual and multi-agent architecture that integrates the CROSSMARC components into a web-based prototype system, as well as by providing an infrastructure that facilitates customization of its components to new domains and languages.	agent architecture;html;information retrieval;multi-agent system;prototype;web application;web page;world wide web	Vangelis Karkaletsis;Constantine D. Spyropoulos	2004			web service;web application security;web development;web modeling;data web;web mapping;html;web design;web standards;computer science;web navigation;social semantic web;web page;semantic web stack;database;web intelligence;web 2.0;world wide web;information extraction;information retrieval;web server	Web+IR	-30.146321548021746	-56.81499053250393	28150
deb991b4d2f89ce65ccda9fa33db43c2dd69f568	a new semi-supervised hierarchical active clustering based on ranking constraints for analysts groupization	ranked constraints;groupization;personalization;constraint;semi supervised hierarchical clustering;olap log files;data warehouse	The groupization aims to enrich the individual preferences using similar individual’s data. It may efficiently adapt the query results to the user expectations. In this paper, we aim to optimally identify the analyst’ groups in a data warehouse. For that reason, we study the similarity between the selected queries in the analytical history. To enhance the quality of derived groups of analysts, we introduce a new method of semi-supervised hierarchical clustering under constraints ranking for handling cases when some constraints are more important than others and must be firstly enforced during the groupization process. Four axis for group identification are distinguished: (i) the function exerted, (ii) the granted responsibilities to accomplish goals, (iii) the source of groups identification, (iv) the dynamicity of discovered groups. Carried out experiments on real log files used for decision-maker groupization in data warehouse confirm the soundness of our approach. Our findings demonstrate that groupization improves upon personalization for several group types, mainly for function-based groupization and explicitly identified groups.	algorithm;apache axis;cluster analysis;data logger;experiment;hierarchical clustering;personalization;semi-supervised learning;semiconductor industry	Eya Ben Ahmed;Ahlem Nabli;Faïez Gargouri	2012	Applied Intelligence	10.1007/s10489-012-0407-3	mathematical optimization;computer science;machine learning;data warehouse;data mining;personalization;database;constraint;world wide web	ML	-27.576751522678993	-58.68667597755652	28155
992978151443318db980bf721cced5c9a8130ac7	visualization on financial terms via risk ranking from financial reports	financial report;stock return volatility;10 k corpus;text ranking;financial reporting	This paper attempts to deal with a ranking problem with a collection of financial reports. By using the text information in the reports, we apply learning-to-rank techniques to rank a set of companies to keep them in line with their relative risk levels. The experimental results show that our ranking approach significantly outperforms the regression-based one. Furthermore, our ranking models not only identify some financially meaningful words but suggest interesting relations between the text information in financial reports and the risk levels among companies. Finally, we provide a visualization interface to demonstrate the relations between financial risk and text information in the reports. This demonstration enables users to easily obtain useful information from a number of financial reports.	information visualization;learning to rank	Ming-Feng Tsai;Chuan-Ju Wang	2012			financial modeling;accounting management;data mining	Web+IR	-24.56052998834131	-56.666364110993065	28280
d0bd3cc0cc770ed3ddc8c512520e15cf653e7d99	email classification into relevant category using neural networks		In the real world, many online shopping websites or service provider have single email-id where customers can send their query, concern etc. At the back-end service provider receive million of emails every week, how they can identify which email is belonged of a particular department? This paper presents an artificial neural network (ANN) model that is used to solve this problem and experiments are carried out on user personal Gmail emails datasets. This problem can be generalised as typical Text Classification or Categorization [8].	categorization;document classification;email;experiment;gmail;neural networks;online shopping;statistical classification	Deepak Kumar Gupta;Shruti Goyal	2018	CoRR		service provider;machine learning;mathematics;artificial intelligence;categorization;artificial neural network	ML	-20.042065460825825	-53.93753531272331	28298
851b3a2d5eccbbaa818615c557c594e0e230894e	business terminology in multilingual vocabulary management system for pc dos	management system		dos;management system;vocabulary	Michal Tasiemski	1993			natural language processing;artificial intelligence;knowledge management;terminology;computer science;management system;vocabulary	Vision	-32.16435917623905	-77.25112760761805	28317
6066bc5d097582a47b1b9ab70d5a02e03995191a	better twitter summaries?		This paper describes an approach to improve summaries for a collection of Twitter posts created using the Phrase Reinforcement (PR) Algorithm (Sharifi et al., 2010a). The PR algorithm often generates summaries with excess text and noisy speech. We parse these summaries using a dependency parser and use the dependencies to eliminate some of the excess text and build better-formed summaries. We compare the results to those obtained using the	algorithm;offset binary;parsing	Joel Judd;Jugal K. Kalita	2013				NLP	-23.246026768810985	-75.20112565354174	28329
602f136c556da01c291e70fa5ab446a39d2f03d3	rumor identification with maximum entropy in micronet		The widely used applications of Microblog, WeChat, and other social networking platforms (that we call MicroNet) shorten the period of information dissemination and expand the range of information dissemination, which allows rumors to cause greater harm and have more influence. A hot topic in the information dissemination field is how to identify and block rumors. Based on the maximum entropy model, this paper constructs the recognition mechanism of rumor information in the micronetwork environment. First, based on the information entropy theory, we obtained the characteristics of rumor information using the maximum entropy model. Next, we optimized the original classifier training set and the feature function to divide the information into rumors and nonrumors. Finally, the experimental simulation results show that the rumor identification results using this method are better than the original classifier and other related classification methods.		Suisheng Yu;Mingcai Li;Fengming Liu	2017	Complexity	10.1155/2017/1703870	mathematics;microblogging;artificial intelligence;entropy (information theory);machine learning;principle of maximum entropy;training set;rumor;social media;social network;information dissemination	ML	-19.973822196584905	-56.66418400594762	28554
1a8ff5e8994bd8daef38a770a3c6bbc499362689	minería de opiniones basado en la adaptación al español de anew sobre opiniones acerca de hoteles	opinion mining;computacion informatica;pln;lexicon;filologias;info eu repo semantics article;informacion documentacion;linguistica;ciencias basicas y experimentales;sentiment analysis;anew;analisis de sentimiento;grupo a;ciencias sociales;grupo b;mineria de opinion;nlp	Receantly, the Opinions Mining (OM) has shown a high tendency of research due to large-scale production of opinions and comments from users over the Internet. Companies and organizations, in general terms, are interested in knowing what is the reputation they have in social networks, blogs, wikis and other web sites. So far, the vast majority of research involving systems MO in English. For this reason, the scientific community is interested in researching different to this language. This article is about the construction of a mining system views in Spanish based on comments given by different clients and hotels. The system works on the lexical approach using Spanish adaptation of affective standards for English words (ANEW). These standards are based on evaluations conducted in the dimensions of valence, arousal and dominance. For the construction of the system took into account the phases of extraction, preprocessing of texts, identification of feelings and the respective ranking of the opinion using ANEW. System experiments were made on labeling a corpus from the spanish version of Tripadvisor. As a result, precision exceeding 94% was obtained at similar systems.	blog;experiment;internet;linear algebra;preprocessor;social network;text corpus;wiki	Carlos Nelson Henriquez Miranda;Jaime Alberto Guzmán Luna;Dixon Salcedo Morillo	2016	Procesamiento del Lenguaje Natural		natural language processing;phospholamban;computer science;sentiment analysis	ML	-23.14146367934999	-61.28172747262885	28588
85cb6b384ce9e1d0b6d3ac334b588dbb1821d722	recurrent neural networks for incremental disfluency detection		For dialogue systems to become robust, they must be able to detect disfluencies accurately and with minimal latency. To meet this challenge, here we frame incremental disfluency detection as a word-by-word tagging task and, following their recent success in Spoken Language Understanding tasks, we test the performance of Recurrent Neural Networks (RNNs). We experiment with different inputs for RNNs to explore the effect of context on their ability to detect edit terms and repair disfluencies effectively. Although not eclipsing the state of the art in terms of utterance-final performance, RNNs achieve good detection results, requiring no feature engineering and using simple input vectors representing the incoming utterance as their training input. Furthermore, RNNs show very good incremental properties with low latency and very good output stability, surpassing previously reported results in these measures.	artificial neural network;dialog system;feature engineering;natural language understanding;neural networks;recurrent neural network	Julian Hough;David Schlangen	2015			latency (engineering);latency (engineering);artificial intelligence;computer science;pattern recognition;recurrent neural network;spoken language;utterance;feature engineering	NLP	-17.173238892719947	-76.446907469555	28705
b19cfc62015d38884ef9ac4d4987d345104a68eb	consolidating client names in the lobbying disclosure database using efficient clustering techniques	lobbying disclosure data;string similarity;levenshtein distance	A fuzzy-matching clustering algorithm is applied to clustering similar client names in the lobbying Disclosure Database. Due to errors and inconsistencies in manual typing, the name of a client often has multiple representations including erroneously spelled names and sometimes shorthand forms, presenting difficulties in associating lobbying activities and interests with one single client. Therefore, there is a need to consolidate various forms of names of the same client into one group/cluster. For efficient clustering, we applied a series of preprocessing techniques before calculating the string distance between two client names. An optimized threshold selection has been adopted, which helps improve clustering accuracy. A single linkage hierarchical clustering technique has been introduced to cluster the client names. The algorithm proves to be effective in clustering similar client names. It also helps to find the representative name for a particular client cluster.	algorithm;client (computing);cluster analysis;hierarchical clustering;linkage (software);preprocessor;string metric	Rajan Kumar Kharel;Niju Shrestha;Chengcui Zhang;Grant T. Savage;Ariel D. Smith	2014		10.1145/2638404.2638506	computer science;data mining;database;world wide web	SE	-27.72357483082419	-60.095436606086494	28730
82c0a132639cba1c5ddd4bd86d0516fe505216e8	forest-based translation rule extraction	translation quality;bleu points improvement;linguistically syntax-based system;translation rule extraction;forest-based translation rule extraction;state-of-the-art tree-to-string system;hierarchical system;bleu point;machine translation;rule set quality;30-best parses	Translation rule extraction is a fundamental problem in machine translation, especially for linguistically syntax-basedsystems that need parse trees from either or both sides of the bitext. The current dominant practice only uses 1-best trees, which adversely affects the rule set quality due to parsing errors. So we propose a novel approach which extracts rules from a packed foresthat compactly encodes exponentially many parses. Experiments show that this method improves translation quality by over 1 BLEU point on a state-of-the-art tree-to-string system, and is 0.5 points better than (and twice as fast as) extracting on 30best parses. When combined with our previous work on forest-based decoding, it achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero by 0.7 points.	algorithm;bleu;baseline (configuration management);ibm notes;machine translation;ork;parallel text;parsing;rule induction	Haitao Mi;Liang Huang	2008			natural language processing;bleu;transfer-based machine translation;computer science;machine learning;data mining;machine translation;hierarchical control system;rule-based machine translation;algorithm	NLP	-21.268101914896622	-76.84505422538673	28731
9bc257c329c850f242c2e30f436db60c07982f64	aspect extraction from product reviews using category hierarchy information		Aspect extraction is a task to abstract the common properties of objects from corpora discussing them, such as reviews of products. Recent work on aspect extraction is leveraging the hierarchical relationship between products and their categories. However, such effort focuses on the aspects of child categories but ignores those from parent categories. Hence, we propose an LDA-based generative topic model inducing the two-layer categorical information (CAT-LDA), to balance the aspects of both a parent category and its child categories. Our hypothesis is that child categories inherit aspects from parent categories, controlled by the hierarchy between them. Experimental results on 5 categories of Amazon.com products show that both common aspects of parent category and the individual aspects of subcategories can be extracted to align well with the common sense. We further evaluate the manually extracted aspects of 16 products, resulting in an average hit rate of 79.10%.	align (company);generative model;text corpus;topic model	Minghui Qiu;Yinfei Yang;Cen Chen;Forrest Sheng Bao	2017			data science;artificial intelligence;natural language processing;computer science;hierarchy;computational linguistics	NLP	-19.755693564328766	-68.71028133184227	28754
607d5928a249bfd4b4d58f85e1f74d59df8e4bcc	degraded text recognition using word collocation	experimental analysis;word recognition;water;water transport	"""A relaxation-based algorithnl is proposed that improves the performance of a text recognition technique by propagating the influence of word collocation statistics. Word collocation refers to the likelihood that two words co-occur within a fixed distance of one another. For example, in a story about water transportation, it is highly likely that the word """"river"""" will occur within ten words on either side of the word """"boat."""" The proposed algorithm receives groups of visually similar decisions (called neighborhoods) for words in a running text that are COlnputed by a word recognition algorithm. The position of decisions within the neighborhoods are modified based on how often they co-occur with decisions in the neig!l~Q:t:bp2g~~()f_()th~r )J:~~IJ?y wo-rds-.-ThIs--proces-sis-Tter-ate~fa number of times effectively propagating the influence of the collocation statistics across an input text. This improves on a strictly local analysis by allowing for strong collocations to reinforce weak (but related) collocations elsewhere. An experimental analysis is discussed in which the algorithm is applied to improving text recognition results that are less than 60 percent correct. The correct rate is effectively improved to 90 percent or better in all cases."""	algorithm;collocation;linear programming relaxation;optical character recognition	Tao Hong;Jonathan J. Hull	1994		10.1117/12.171121	natural language processing;speech recognition;computer science;communication	NLP	-14.147015945258138	-79.75996123470595	28760
29223817ede05f81fd65be1753818b888d8613ab	cde-iiith at semeval-2016 task 12: extraction of temporal information from clinical documents using machine learning techniques		In this paper, we demonstrate our approach for identification of events, time expressions and temporal relations among them. This work was carried out as part of SemEval-2016 Challenge Task 12: Clinical TempEval. The task comprises six sub-tasks: identification of event spans, time spans and their attributes, document time relation and the narrative container relations among events and time expressions. We have participated in all six subtasks. We have provided with a manually annotated dataset which comprises of training dataset (293 documents), development dataset (147 documents) and 151 documents as test dataset. We have submitted our work as two systems for the challenge. One system is developed using machine learning techniques, Conditional Random Fields (CRF) and Support Vector machines (SVM) and the other system is developed using deep neural network (DNN) techniques. The results show that both systems have given relatively same performance on these tasks.	artificial neural network;conditional random field;deep learning;machine learning;regular expression;semeval;support vector machine	Veera Raghavendra Chikka	2016			artificial intelligence;computer science;machine learning;natural language processing;information retrieval;relationship extraction;semeval	NLP	-23.090698961512743	-69.86192076374338	28807
b95c29e612b2238928d1784f393156f5cb9f1511	what makes a research article newsworthy?		There has been tremendous growth in the amount of scientific literature being published every year. Yet, very little of it receives press coverage. Mentions by news outlets often establish the relevance the research has to society in general. In the present study, we focused on better understanding the factors that contribute to a research article’s newsworthiness. We have built three classifiers to predict the likelihood of research article receiving online press coverage, based on features that quantify the attention it has received on various online platforms. The Random Forest classifier performed best with an accuracy rate of 0.92.	random forest;relevance;scientific literature	Harish Varma Siravuri;Hamed Alhoori	2017		10.1002/pra2.2017.14505401163		HCI	-20.4535011574625	-53.49979670496141	28977
0d7e3191b05d94b9ed24fa16c600527b1312286c	word semantic similarity calculation based on word2vec		In order to solve the problem of poor universality and the absence of contextual information in word similarity calculation based on dictionary, this paper proposes a semantic similarity computation method based on Word2vec. This method improves HowNet and Tongyici Cilin, and also adds the word vector model as a weighing parameter to calculate the word similarity, after compares the similarity of the words by assigning different weights to the three methods. Through experimental comparison, the Pearson coefficient of the algorithm and the artificial value is 0.892, and the method can cover most words so that it can effectively solve the problem of the similarity of the word calculation in the dictionary.		Xiaolin Jin;Shuwu Zhang;Jie Liu	2018	2018 International Conference on Control, Automation and Information Sciences (ICCAIS)	10.1109/ICCAIS.2018.8570612		Robotics	-25.675868467119262	-67.1341281088339	28978
7732045db6d0edf1c9de055bfd438d91355486d6	affective content detection in sitcom using subtitle and audio	dvd video;audio signal processing;image segmentation;video signal processing;speech processing;vocabulary;video analysis;divx videos;video segmentation;event detection;dialogue script partition;domain knowledge;sitcom videos;content analysis;digital videos;video script;emotion related vocabularies;audio event detection;video semantic analysis;vocabulary audio signal processing image segmentation speech processing video signal processing;digital video;subtitle files;point of view;videos event detection humans hidden markov models intelligent networks computer networks variable speed drives cameras vocabulary production;affective content analysis;subtitle file analysis affective content detection digital videos subtitle files dvd video divx videos video segmentation dialogue script partition emotion related vocabularies video script audio event detection video semantic analysis affective content analysis sitcom videos;affective content detection;subtitle file analysis;semantic analysis	From a personalized media point of view, many users favor a flexible tool to quickly browse the affective content in a video. Such affective content may cause audiences' strong reactions or special emotional experiences, such as anger, sadness, fear, joy and love. This paper attempts to extract affective content for digital videos by analyzing the subtitle files of DVD/DivX videos and utilize audio event to assist affective content detection. Firstly, videos are segmented by dialogue script partition. Compared to traditional video shot, video segmented by scripts is not affected by camera changes and shooting angles and easy to include video segments with compact content. Secondly, emotion-related vocabularies in video script are detected to locate affective video content. Using script to directly access video content avoids complex video analysis. Thirdly, audio event detection is utilized to assist affective content detection. Compared with traditional video semantic analysis, affective content analysis puts much more emphasis on the audience's reactions and emotions. Initial experiments are carried on sitcom videos because its simple video structure provides useful domain knowledge. The experimental results demonstrate that subtitle file analysis and audio event detection provides effective and efficient clues to determine the emotional content of the videos	browsing;digital video;experience;experiment;personalization;point of view (computer hardware company);sadness;video content analysis;vocabulary	Min Xu;Liang-Tien Chia;Haoran Yi;Deepu Rajan	2006	2006 12th International Multi-Media Modelling Conference	10.1109/MMMC.2006.1651312	computer vision;speech recognition;content analysis;audio signal processing;computer science;speech processing;multimedia;image segmentation;dvd-video;domain knowledge	Vision	-13.914304348350974	-55.403773272711774	29016
2b9befc4351e6a43049241d62a55e86bbfd6aa6d	multi-document summarization based on rhetorical structure: sentence extraction and evaluation	summarization generation multidocument summarization sentence extraction sentence evaluation multidocument rhetorical structure multidocument automatic summarization task multidocument representation cross structure theory information fusion theory multidocument information fusion;sentence extraction;summarization generation;automatic summarization;multidocument information fusion;structuration theory;text analysis;multi document summarization;sentence evaluation;document representation;multidocument rhetorical structure;information fusion theory;data mining computer science data structures buildings fusion power generation fuses testing internet;multidocument representation;cross structure theory;multidocument automatic summarization task;information fusion;multidocument summarization	A Multi-document Rhetorical Structure (MRS) is proposed for multi-document automatic summarization task. This structure can represent interrelationship between text units at different levels of granularity and can describe simultaneously the happen and change of various events. MRS simplify traditional multi-document representation in cross structure theory and supplement change and distribution information of events topics which cannot be obtained in information fusion theory. Concretely, a series of algorithms including building MRS, multi-document information fusion based MRS and summarization generation are proposed. The capability of concurrently fuse multiple knowledge sources of MRS strategies is testified by sets of experiments and shows good result.	algorithm;automatic summarization;branch and bound;convergence insufficiency;experiment;limit analysis;minimal recursion semantics;multi-document summarization;sentence extraction	Yong-Dong Xu;Xiaolong Wang;Tao Liu;Zhi-Ming Xu	2007	2007 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2007.4413925	natural language processing;multi-document summarization;computer science;automatic summarization;pattern recognition;information retrieval	Robotics	-26.722892674739544	-65.8180456731139	29106
87d48d1c40e9fac8d0d886a913fbf9ee857d84fe	laval university and lakehead university at trec dynamic domain 2015: combination of techniques for subtopics coverage				Robin Joganah;Luc Lamontagne;Richard Khoury	2015			information retrieval;data mining;computer science	Web+IR	-31.997176828005053	-63.026332765208814	29151
09d4bac51c78fe2c3400522b2c8b12fc711f5ffc	theory of keyblock-based image retrieval	information retrieval;texture features;clustering;codebook;indexation;content based image retrieval;keyblock;image retrieval	"""The success of text-based retrieval motivates us to investigate analogous techniques which can support the querying and browsing of image data. However, images differ significantly from text both syntactically and semantically in their mode of representing and expressing information. Thus, the generalization of information retrieval from the text domain to the image domain is non-trivial. This paper presents a framework for information retrieval in the image domain which supports content-based querying and browsing of images. A critical first step to establishing such a framework is to construct a codebook of """"keywords"""" for images which is analogous to the dictionary for text documents. We refer to such """"keywords"""" in the image domain as """"keyblocks."""" In this paper, we first present various approaches to generating a codebook containing keyblocks at different resolutions. Then we present a keyblock-based approach to content-based image retrieval. In this approach, each image is encoded as a set of one-dimensional index codes linked to the keyblocks in the codebook, analogous to considering a text document as a linear list of keywords. Generalizing upon text-based information retrieval methods, we then offer various techniques for image-based information retrieval. By comparing the performance of this approach with conventional techniques using color and texture features, we demonstrate the effectiveness of the keyblock-based approach to content-based image retrieval."""	code;codebook;content-based image retrieval;dictionary;information retrieval;text-based (computing)	Lei Zhu;Aibing Rao;Aidong Zhang	2002	ACM Trans. Inf. Syst.	10.1145/506309.506313	image texture;visual word;image retrieval;computer science;machine learning;codebook;pattern recognition;data mining;cluster analysis;automatic image annotation;information retrieval	Web+IR	-13.773386105404109	-58.23909619627638	29164
c4f49b95e8b23faff8fbf5dc778a4eaeda22c344	corpus based unsupervised labeling of documents	semi supervised learning;world wide web	Text categorization involves mapping of documents to a fixed set of labels. A similar but equally important problem is that of assigning labels to large corpora. With a deluge of documents from sources like the World Wide Web, manual labeling by domain experts is prohibitively expensive. The problem of reducing effort in labeling of documents has warranted a lot of investigation in the past. Most of this work involved some kind of supervised or semisupervised learning. This motivates the need to find automatic methods for annotating documents with labels. In this work we explore a novel method of assigning labels to documents without using any training data. The proposed method uses clustering to build semantically related sets that are used as candidate labels to documents. This technique could be used for labeling large corpora in an unattended fashion.	categorization;cluster analysis;document classification;semi-supervised learning;supervised learning;text corpus;world wide web	Delip Rao;P Deepak;Deepak Khemani	2006			artificial intelligence;natural language processing;semi-supervised learning;cluster analysis;categorization;information retrieval;computer science;training set	Web+IR	-23.949815611135545	-65.11918967993132	29196
f8eb13d1b8a2e4d646654c9919fc84b5c7c4ce87	semantic information processing for multi-party interaction	semantic representation;semantic information;machine learning	We present ongoing research efforts using semantic representations and processing, combined with machine learning approaches to structure, understand, summarize etc. the multimodal information available from multi-party meeting recordings. In the AMI and AMIDA projects1, we are working with our partners on numerous aspects of analysing, structuring and understanding multimodal recordings of multiparty meetings. We are working on applications based on this understanding that allow browsing of meetings as well as supporting tools that can be used during meetings. The AMI and AMIDA corpora of well over 100 hours of meetings are freely available for research purposes. Semantic Analysis: The main application of semantic analysis is through NLU components that work on the ASR output with semantic parsers. The target semantics are encoded in domain specific ontologies, used to represent the content of the discussion. Other semantic frameworks represent various aspects of the discourse, e.g., dialogue and negotiation acts. These are typically analysed by machine learning systems. Semantic Processing: An important aspect of analysing the (meeting) documents are various levels of segmentation. On the highest level, these are topic segments that typically last several minutes. On the lowest level, these are dialogue acts that loosely correspond to utterances or sentences. The task of semantic processing is twofold: first, the underlying structure, e.g., statements and responses must be determined; second, a discourse model with inferences over these structures is used to determine the current state of the discussion and thus the final results of a meeting. As an example, we present a complete system that understands and summarizes the decisions made during a meeting. Semantics–based Presentations: Based on the results of semantic processing, we can generate various summaries of meetings. As examples, we present short abstracts generated using NLG techniques as well as longer storyboards that combine extractive summarization technology with semantic discourse understanding. The latter can be presented as meeting browsers, allowing direct access to the entire meeting recording. 1 This work was partly supported by the European Union 6th FWP IST Integrated Project AMIDA (Augmented Multi-party Interaction with Distance Access), IST-033812, http://www.amidaproject.org V. Matoušek and P. Mautner (Eds.): TSD 2009, LNAI 5729, p. 14, 2009. c © Springer-Verlag Berlin Heidelberg 2009	information processing;lecture notes in computer science;machine learning;multimodal interaction;natural language generation;natural language understanding;ontology (information science);parsing;random access;semantic web;semantic analysis (compilers);semantic analysis (knowledge representation);springer (tank);storyboard;tsd;text corpus	Tilman Becker	2009		10.1007/978-3-642-04208-9_4	semantic data model;natural language processing;semantic interoperability;semantic similarity;semantic computing;semantic integration;explicit semantic analysis;semantic grid;computer science;machine learning;social semantic web;semantic web stack;semantic compression;semantic technology;information retrieval;semantic analytics	NLP	-33.267785253885506	-76.6917135506544	29224
11c8d08f1a03db5b782df6094c9a6b648cf2892c	domain-based lexicon enhancement for sentiment analysis		General knowledge sentiment lexicons have the advantage of wider term coverage. However, such lexicons typically have inferior performance for sentiment classification compared to using domain focused lexicons or machine learning classifiers. Such poor performance can be attributed to the fact that some domain-specific sentiment-bearing terms may not be available from a general knowledge lexicon. Similarly, there is difference in usage of the same term between domain and general knowledge lexicons in some cases. In this paper, we propose a technique that uses distant-supervision to learn a domain focused sentiment lexicon. The technique further combines general knowledge lexicon with the domain focused lexicon for sentiment analysis. Implementation and evaluation of the technique on Twitter text show that sentiment analysis benefits from the combination of the two knowledge sources. The technique also performs better than state-of-the-art machine learning classifiers trained with distantsupervision dataset.	lexicon;machine learning;sentiment analysis;social media	Aminu Muhammad;Nirmalie Wiratunga;Robert Lothian;Richard Glassey	2013			sentiment analysis;lexicon;artificial intelligence;pattern recognition;general knowledge;computer science	AI	-21.697082035391457	-71.28527039619186	29247
6eca51984758f4650ff4c95f95f3477b0a883cc0	quantitative analysis of art market using ontologies, named entity recognition and machine learning: a case study		In the paper we investigate new approaches to quantitative art market research, such as statistical analysis and building of market indices. An ontology has been designed to describe art market data in a unified way. To ensure the quality of information in the knowledge base of the ontology, data enrichment techniques such as named entity recognition (NER) or data linking are also involved. By using techniques from computer vision and machine learning, we predict a style of a painting. This paper comes with a case study example being a detailed validation of our approach.	auction algorithm;computer vision;expect;gene ontology term enrichment;knowledge base;linker (computing);machine learning;named entity;named-entity recognition;ontology (information science);refinement (computing)	Dominik Filipiak;Henning Agt-Rickauer;Christian Hentschel;Agata Filipowska;Harald Sack	2016		10.1007/978-3-319-39426-8_7	natural language processing;computer science;data science;data mining;entity linking	AI	-33.115998111866595	-67.2693818097444	29258
3d0102a34cd3afe046f23e6f53a9a9c7d8eac77e	webis at the trec 2011 session track		In this paper we give a brief overview of the Webis group’s participation in the TREC 2011 Sessions track with an extended version of our last year’s approach [HSV10]. The basic idea can be described as a conservative query expansion based on terms used in previous queries or terms contained in clicked snippets. Furthermore, a query’s result set is reduced by removing documents shown for previous queries or documents containing important terms from nonclicked snippets.	query expansion;result set	Matthias Hagen;Jan Graßegger;Maximilian Michel;Benno Stein	2011				Web+IR	-32.06158261585316	-62.607540188984935	29282
805966d8bf7eafaa605b44076e1847e875e5b732	word sense induction using graphs of collocations	associated collocation;different sense;associated word;semeval-2007 wsi task;traditional graph-based approach;target word;co-occurrence frequency;word sense induction;resulting graph;word sense;vertex corresponds	Word Sense Induction (WSI) is the task of identifying the different senses (uses) of a target word in a given text. Traditional graph-based approaches create and then cluster a graph, in which each vertex corresponds to a word that co-occurs with the target word, and edges between vertices are weighted based on the co-occurrence frequency of their associated words. In contrast, in our approach each vertex corresponds to a collocation that co-occurs with the target word, and edges between vertices are weighted based on the co-occurrence frequency of their associated collocations. A smoothing technique is applied to identify more edges between vertices and the resulting graph is then clustered. Our evaluation under the framework of SemEval-2007 WSI task shows the following: (a) our approach produces less sense-conflating clusters than those produced by traditional graph-based approaches, (b) our approach outperforms the existing state-of-the-art results.	collocation method;emoticon;semeval;smoothing;vertex (geometry);wafer-scale integration;word sense;word-sense induction	Ioannis P. Klapaftis;Suresh Manandhar	2008		10.3233/978-1-58603-891-5-298	natural language processing;graph power;multiple edges;level structure;graph labeling;degree;mixed graph;multigraph;machine learning;hypercube graph;pattern recognition;cycle graph;vertex;path graph;path;wheel graph;complement graph;semi-symmetric graph;neighbourhood;strength of a graph	NLP	-26.313667414679465	-64.96088709363991	29320
66147d813812fecbd2f51df6cc6db994118db898	word sense disambiguation using statistical models and wordnet	statistical model;natural language processing;hidden markov model;maximum entropy;part of speech	One of the main problems in Natural Language Processing is lexical ambiguity, words often have multiple lexical functionalities (i.e. they can have various parts-of-speech) or have several semantic meanings. Nowadays, the semantic ambiguity problem, most known as Word Sense Disambiguation, is still an open problem in this area. The accuracy of the different approaches for semantic disambiguation is much lower than the accuracy of the systems which solve other kinds of ambiguity, such as part-of-speech tagging. Corpus-based approaches have been widely used in nearly all natural language processing tasks. In this work, we propose a Word Sense Disambiguation system which is based on Hidden Markov Models and the use of W rdNet . Some experimental results of our system on the SemCorcorpus are	hidden markov model;markov chain;natural language processing;part-of-speech tagging;statistical model;word sense;word-sense disambiguation;wordnet	Antonio Molina;Ferran Plà;Encarna Segarra;Lidia Moreno	2002			artificial intelligence;part of speech;natural language processing;speech recognition;open problem;principle of maximum entropy;factored language model;hidden markov model;ambiguity;semeval;computer science;wordnet	NLP	-23.876998244823646	-73.67562560225593	29337
e09a39232217e81af46315f73b7ab94a5eeabbba	the semantics and syntax of russian pronominal structure: a feature breakdown	russian pronominal structure;german sich selbst;additional transmissional deictic marking;feature breakdown;coling bonn;author discusses pronominality;special semantic stratum;interrogative pronoun;direct anaphoric effect;singulative identificational deixis;singulative identificational pronominality	"""Elaborating one of the points of his Coling ~onn 1986 paper, the author discusses prononfn~lity, which is due to a special semantic stratum, singulatire identifieational deixis. Personal, reflexive and interrogative pronouns have additional transn[issional deictic markings, but singulative identificational pronominality alone has a direct anaphorie effect and tends to reinforce syntactically. This ,explains, for instance, German sich selbst. i. In several publications of mine /van Schooncveld 1982/, /idem forthcoming-A/, /idem forthc~ning-C/ ]i have had occasion to mention in passing the specia] nature of pronouns. Pronouns have their own type of deixis. 2. Deixis is traditionally used for the type of meaning which results from the once-occurring speech act, Sauss~e' s parole, being recodified, that is, being incorporated back into the langue. In adverbs lJJ<e here [~nd now the code refers to actual applications o--o-f--J±self-?We cannot identify the referent of here and now without taking recourse to our having T~-~tifie~-~he given speech act. 11~e speech act becomes a part of the narrated situation /Bru~nann 1904/, /Lyons 1977, pp.638 sqq./. 3. It is an empirical fact, however, that language can also indicate an identification act which takes place during the narrated situation. Thus we have to do in the preposition out with a space which l~s been identified (an in-spa~ and subsequent].y discarded as irrelevant ~ the next identification, so that """"out"""" is the resulting meaning. What is iraportent he~. is that all identifications, subsequent ones as well as the first, may take place in the ~m~ated situation, not necessarily during the speech (the transmission) act. Do we have to do with deixis in the case of identification in the narrated situation? l~%at depends on how we use the word deixis. If we understand by deixis the reincorporation of an identification, act into the code, that is, the prerogation, by the first identifier, of the identification of the referent, then we can speak of deixis also in the second case~ when the entire identification procedure takes place Jn the narrated situation alone. But that is not the way the term deixis has been used traditJo~[].ly by linguists. In the traditional conceptualiza'i:ion, deixis is of a tranmnissional nature. If~ however, we are. to use the term deixis for any recodJfication of the initial identification of the refer(u~t, including identifiuation in the narrated situation, """"then we must distinguish between mere identificational deixis (in the narrated situation), and the traditional type of deixis, which I call tran~nissional deixis. In pronouns we have to do with a deixis of the identificational variety. 4. Of both types of deixis, transmissional and identificational~ there exist two varieties: m] uTmh~rked type and a singulative tqfpe /van Schooneveld forth;oming-A/. In the singulative type~ all identifications take place at the same moment. Pronominal deixis is singulatively identificational. Singultative transmissional delxis gives rise to references to the process of the transmission, that is, the pzx~ess of pronunciation itself. It regulates~ for i[~stance, the relation between grammatical morphemes (endings) and lexical morphemes. In singulative identificational deixis all identifications in the narrated situation must take place simultaneously with the first one. 5. It is obvious that singulative ~ansn[issional deixis is the type of deixis which is at the basis of demonstrative pronouns. In """"the framework of the demons±~ative pronouns our vantage point is that of an identifier operating within the narrated situation; in the personal pronom~s the speech si'ttmtion is involved at the same time. In the personal pronom%s, we have to do with a semm~tie mechanism more complex than that of the demonstrative pronouns. The personal pronouns set up objects as being on a par with speaker and receiver, since they define these objects (referents) further in terms of the speech situation. In using the first person the speaker looks at his own image in the nfirror of the singulative r~rrated situation. Mutatis mutandis """"the same happens in the second and third person. 8. Pronouns in general are n~rked by singulative identificational plurality (plur""""'). Singulative identificational plurality means that there is multiple synchronized identifiability of the referent. A singu]ative (mini-)narrated situation is created. The referent of the demonstrative pronoun 'that' equals a (mini-)narrated situation. Personal pronouns are, being pronouns, marked by plur""""' , but a further specification (subclassification) is done by transmissional semantic tea hines. 7. At this point, I should mention t]~t I have empirically concluded that Russian semantic values can be expressed in terms of six Semantic features which occur on each of the four deictic levels mentioned; i.~., by 24 semantic features. These features constitute an ordered set in that each succeeding teatime incorporates the information given by its predecessor /van Schooneveld 1983/, /idem forthcoming-C/. .P!urality, the first feat[~e, instructs the recelver to perform more tl~nn one identification on one object or to identify more than one object. DimensJ~nality singles out from a plurality of identifications a subset which is distinct from its peers. Preidentity indicates that the first identification must be assumed to have been performed earlier than other identifications in the narrated or speech situation. Extension reidentifies an identified element and impl~elative unaffectedness by the identification situation; cancellation signalizes the complement of an originally identified set, and objec~ tiveness indicates that the referent can be at any frcm the element initially identified. • II! 8. The third person is, additlonally to plur , marked by transmissional plurality (plu~""""). In the pronouns, plur"""" creates a reference in the speech situation which must not necessarily be identified with the speaker or the receiver but can be repeatedly (i.e. a multiple of times) identified. The second' person is, additionally to plur""""' , n~rked by (non-singul~tive) transmissionS, dimensionality (dim""""). Dim"""" says that there is a subset of elements in the speech situation which have a property in common. What else constitutes a more conspicuous subset of actants in the speech situation than those who actively participate in it? The second person indicates the direct participant in the speech situa-"""	actant;existential quantification;first-person (video games);identifier;pronom;relevance;speech synthesis;virtual camera system	C. H. van Schooneveld	1988			natural language processing;computer science;linguistics	NLP	-12.006403260834984	-78.94275268831076	29350
b0bf95263c287f71bf146d9ae50342dc3168df5b	mining semantic networks for knowledge discovery	text mining;rule based;semantic network;text analysis;semantic networks;data mining;data mining computer hacking text mining computer worms computer networks knowledge engineering organizing user interfaces algorithm design and analysis knowledge based systems;knowledge based systems data mining semantic networks text analysis frame based representation;frame based representation;network structure;content mining algorithms semantic networks mining knowledge discovery concept frame graphs rule based algorithm text documents networked knowledge base text mining;knowledge based systems;knowledge base;knowledge discovery	This paper addresses the problem of mining a class of semantic networks, called Concept Frame Graphs (CFG’s), for knowledge discovery from text. This new representation is motivated by the need to capture richer text content so that non-trivial mining tasks can be performed. We first define the CFG representation and then describe a rulebased algorithm for constructing a CFG from text documents. Treating the CFG as a networked knowledge base, we propose new methods for text mining. On a specific task of discovering the top companies in an area, we observe that our approach leads to simpler content mining algorithms, once the CFG has been constructed. Moreover, exploiting the network structure of CFG results in significant improvements in precision and recall.	algorithm;context-free grammar;frame language;knowledge base;precision and recall;semantic network;text mining	Kanagasabai Rajaraman;Ah-Hwee Tan	2003		10.1109/ICDM.2003.1250995	concept mining;web mining;semantic computing;text mining;software mining;computer science;artificial intelligence;data science;data mining;knowledge extraction;data stream mining;semantic network;molecule mining;information retrieval;co-occurrence networks	ML	-24.44001883036814	-63.636903870979204	29402
0f8ea1d9c537464be248238ff7d8670c41e32b37	the ratf formula (kwok's formula): exploiting average term frequency in cross-language retrieval	term frequency		ratf;tf–idf	Ari Pirkola;Erkka Leppänen;Kalervo Järvelin	2002	Inf. Res.		information retrieval;knowledge management;computer science;ratf	Web+IR	-30.39386037481659	-64.89673949496009	29413
d05e2051ad1d143cbb82ae2f1c9d12012c9b0081	approximate address matching	databases;libraries;address matching;address standardization;databases cities and towns libraries measurement standards organizations accuracy approximation algorithms;tf idf weight;address management;inverse document frequency;measurement;text analysis geographic information systems information retrieval string matching;vector space model;standards organizations;approximation algorithms;information retrieval;edit distance;text analysis;term frequency;address comparison;address standards;string similarity;accuracy;address correction;geographic information systems;term weighting;address capturing process;term weighted dissimilarity;cities and towns;term frequency inverse document frequency weighting address matching address management address capturing process address standards string similarity measurement address comparison edit distance vector space model term weighted dissimilarity;tf idf weight address matching address correction address standardization string similarity edit distance vector space model;string matching;similarity measure;term frequency inverse document frequency weighting;string similarity measurement	Address management is a major challenge for many organizations, as errors occur frequently in the address capturing process, and address standards and usages may vary among different databases. Rather than comparing house number, street, city and province individually, we use a string similarity measurement to perform address comparison, which enables us to combine the edit distance with the vector space model to search for potentially matching address candidates by associating them with a similarity matching score. Upon evaluating the strengths and weaknesses of these techniques, we introduce an algorithm for effective address matching, called Term-Weighted Dissimilarity, which combines edit distance similarity with Term Frequency-Inverse Document Frequency weighting. We implement this algorithm in software and show its effectiveness via a real application for address matching and correction based on Canada Post’s address standard.	address space layout randomization;addressing mode;algorithm;data deduplication;database;edit distance;house numbering;library;string metric	Dengyue Li;Shengrui Wang;Zhen Mei	2010	2010 International Conference on P2P, Parallel, Grid, Cloud and Internet Computing	10.1109/3PGCIC.2010.43	computer science;data mining;database;world wide web	DB	-33.27845840050001	-64.38665982652789	29457
cdbe4465b4beeda6b8136de011bebe3af36da9a1	主題語言模型於大詞彙連續語音辨識之研究 (on the use of topic models for large-vocabulary continuous speech recognition) [in chinese].		本論文研究使用主題資訊之語言模型(Language Model)。當語言模型用於大詞彙連續語 音辨識時,其主要的任務是藉由已解碼歷史詞序列資訊來預測下一個候選詞出現的可能 性。傳統的 N 連(N-gram)語言模型容易受限於模型參數過多的問題,僅能用來擷取短距 離的詞彙接連資訊,並不能考慮完整的歷史詞序列之語意資訊。因此,近十幾年來許多 研究學者陸續提出各式主題模型(Topic Model),包括討論文件與詞之關係的機率式潛藏 語意分析(Probabilistic Latent Semantic Analysis, PLSA)和潛藏狄利克里分配(Latent Dirichlet Allocation, LDA),以及討論詞虛擬文件與詞關係的詞主題模型(Word Topic Model, WTM)。這些模型主要都是透過一組潛藏的主題機率分布來描述文件與詞、或者 詞虛擬文件與詞之間的關係,用以擷取出歷史詞序列長距離的潛藏語意資訊。本論文提 出一種新的主題模型,稱之為詞相鄰模型(Word Vicinity Model, WVM),它直接地基於 語言中詞與詞相互關聯資訊以建構一個機率式的潛藏主題空間,並且透過線性模型結合 的方式建立歷史詞序列之主題模型來預測下一個候選詞出現的可能性,藉此輔助傳統 N 連語言模型。實驗結果顯示本論文所提出的詞相鄰模型不僅相較大部分主題模型具有較 低的模型參數量,同時能對於僅使用三連語言模型的基礎大詞彙連續語音辨識系統也有 相當程度的語音辨識率提升。	language model;latent dirichlet allocation;probabilistic latent semantic analysis;speech recognition;topic model;vocabulary	Kuan-Yu Chen;Berlin Chen	2009			natural language processing;speech recognition	NLP	-21.32334807330959	-79.09564651500096	29493
e0560139ef36521a652c9affc17879c8f1e01a61	bag-of-words with aggregated temporal pair-wise word co-occurrence for human action recognition	info eu repo semantics article;temporal constraints;human action recognition;bag of words	http://dx.doi.org/10.1016/j.patrec.2014.07.014 0167-8655/ 2014 Elsevier B.V. All rights reserved. q This paper has been recommended for acceptance by G. Borgefors. ⇑ Corresponding author at: Department of Computer Languages and Systems, Universitat Jaume I, 12071 Castelló de la Plana, Spain. E-mail addresses: pagusti@uji.es (P. Agustí), vtraver@uji.es (V.J. Traver), pla@uji. es (F. Pla). Pau Agustí, V. Javier Traver ⇑, Filiberto Pla	bag-of-words model in computer vision;experiment;linear algebra	Pau Agustí;V. Javier Traver;Filiberto Pla	2014	Pattern Recognition Letters	10.1016/j.patrec.2014.07.014	computer vision;speech recognition;computer science;artificial intelligence;bag-of-words model;machine learning;pattern recognition	AI	-22.43018774248156	-69.1018302799897	29502
3615560164bf7545c932bef771183cee2754a140	tree-structured conditional random fields for semantic annotation	modelizacion;anotacion;ajustamiento modelo;semantic annotation;empirical study;ontologie;methode empirique;web pages;red www;probabilidad condicional;metodo arborescente;metodo empirico;web semantique;modele lineaire;probabilite conditionnelle;empirical method;reseau web;semantics;annotation;modelo lineal;probabilistic approach;semantica;semantique;ajustement modele;modelisation;internet;enfoque probabilista;approche probabiliste;web semantica;model matching;tree structure;linear model;conditional random field;semantic web;world wide web;tree structured method;ontologia;dependent data;methode arborescente;parameter estimation;modeling;conditional probability;ontology	The large volume of web content needs to be annotated by ontologies (called Semantic Annotation), and our empirical study shows that strong dependencies exist across different types of information (it means that identification of one kind of information can be used for identifying the other kind of information). Conditional Random Fields (CRFs) are the state-of-the-art approaches for modeling the dependencies to do better annotation. However, as information on a Web page is not necessarily linearly laid-out, the previous linear-chain CRFs have their limitations in semantic annotation. This paper is concerned with semantic annotation on hierarchically dependent data (hierarchical semantic annotation). We propose a Tree-structured Conditional Random Field (TCRF) model to better incorporate dependencies across the hierarchically laid-out information. Methods for performing the tasks of model-parameter estimation and annotation in TCRFs have been proposed. Experimental results indicate that the proposed TCRFs for hierarchical semantic annotation can significantly outperform the existing linear-chain CRF model.	approximation algorithm;conditional random field;estimation theory;ontology (information science);support vector machine;web content;web page	Jie Tang;MingCai Hong;Juan-Zi Li;Bangyong Liang	2006		10.1007/11926078_46	semantic computing;image retrieval;computer science;ontology;data mining;database;semantics;temporal annotation;empirical research;world wide web;statistics	Web+IR	-13.723351804676733	-61.72156401960544	29524
665d02b67d234b9013d93920e44d0612388c3686	description and recognition of form and automated form data entry	databases;robustness databases;frame line detection automated form data entry form description method frame lines printed forms form recognition;form recognition;knowledge representation business forms document image processing knowledge acquisition;line detection;automated form data entry;knowledge acquisition;business forms;frame lines;document image processing;robustness;frame line detection;printed forms;knowledge representation;form description method	In this paper we present a form description method, in which frame lines are used to constitute a so-called frame template, which reflects the structure of a form either topologically or geometrically. Relevant item traversal algorithm is then proposed to locate and label form's items. We have also developed a robust and fast frame line detection method to make this form description practical for form recognition. Experimental results show our approach provides an effective way to convert printed forms into computerized format or collect information for database from printed forms.		Jinhui Liu;Xiaoqing Ding;Youshou Wu	1995		10.1109/ICDAR.1995.601963	knowledge representation and reasoning;computer vision;computer science;data mining;robustness	AI	-13.250900525272504	-63.49703740404737	29552
1fd11d9cdef97337fc36bd5df73c08a15ba8c44b	improving named entity translation combining phonetic and semantic similarities	semantic similarity		named entity	Fei Huang;Stephan Vogel;Alexander H. Waibel	2004			machine learning;artificial intelligence;natural language processing;semantic similarity;named entity;entity linking;computer science	NLP	-24.531716535095487	-72.17804409800722	29558
45675d290a378c03879119a8f6cc961396894584	exploiting spatial descriptions in visual scene analysis		The reliable automatic visual recognition of indoor scenes with complex object constellations using only sensor data is a nontrivial problem. In order to improve the construction of an accurate semantic 3D model of an indoor scene, we exploit human-produced verbal descriptions of the relative location of pairs of objects. This requires the ability to deal with different spatial reference frames (RF) that humans use interchangeably. In German, both the intrinsic and relative RF are used frequently, which often leads to ambiguities in referential communication. We assume that there are certain regularities that help in specific contexts. In a first experiment, we investigated how speakers of German describe spatial relationships between different pieces of furniture. This gave us important information about the distribution of the RFs used for furniture–predicate combinations, and by implication also about the preferred spatial predicate. The results of this experiment are compiled into a computational model that extracts partial orderings of spatial arrangements between furniture items from verbal descriptions. In the implemented system, the visual scene is initially scanned by a 3D camera system. From the 3D point cloud, we extract point clusters that suggest the presence of certain furniture objects. We then integrate the partial orderings extracted from the verbal utterances incrementally and cumulatively with the estimated probabilities about the identity and location of objects in the scene, and also estimate the probable orientation of the objects. This allows the system to significantly improve both the accuracy and richness of its visual scene representation.	adult fanconi syndrome;compiler;computation;computational model;description;extraction;frame (physical object);jumbo frame;physical object;point cloud;probability;psycholinguistics;radio frequency;reference frame (video);scanning;spatial reference system	Leon Ziegler;Katrin Johannsen;Agnes Swadzba;Jan Peter de Ruiter;Sven Wachsmuth	2012	Cognitive Processing	10.1007/s10339-012-0460-1	computer vision;simulation;communication	Vision	-7.92814671073999	-75.6250752876626	29658
17397faeb00ad566006cac78080173f063da712d	concept extraction for online shopping	web pages;supervised learning;e commerce;naive bayes;keyphrase extraction;gold standard;online shopping;concept extraction;user experience;automatic keyphrase extraction;extraction method	Online shopping has been more and more popular nowadays. Online shopping starts with research and shopping research starts with search. In order to provide a more streamlined user experience in shopping related research, it is critical for e-commerce sites to accurately identify what a Web page is talking about. Concept extraction is a nice solution for this purpose. In this paper, we investigate two concept extraction methods: Automatic Concept Extractor (ACE) and Automatic Keyphrase Extraction (KEA). ACE is an unsupervised method that looks at both text and HTML tags. We upgrade ACE into Improved Concept Extractor (ICE) with significant improvements. KEA is a supervised learning system. It first builds a Naive Bayes model from training documents where concepts are manually assigned. The trained model is then used to automatically find concepts in new documents. In order to evaluate the two systems, we create a gold standard by manually assigning concepts to each page in the collection. We tune different parameters of ICE and KEA to generate concepts. And we use precision, recall and F1 to evaluate the concepts. The experimental results demonstrate that ICE significantly outperforms KEA in concept extraction for online shopping.	ace;e-commerce;html;naive bayes classifier;online shopping;paging;randomness extractor;supervised learning;unsupervised learning;user experience;web page	Yongzheng Zhang;Rajyashree Mukherjee;Benny Soetarman	2012		10.1145/2346536.2346545	engineering;data science;data mining;world wide web	AI	-23.947891578050974	-58.341318454709835	29700
4f246caf0f8fc4836402e9fb585bc0a6ea749819	using the wiktionary graph structure for synonym detection	measures of information	This paper presents our work on using the graph structure of Wiktionary for synonym detection. We implement semantic relatedness metrics using both a direct measure of information flow on the graph and a comparison of the list of vertices found to be “close” to a given vertex. Our algorithms, evaluated on ESL 50, TOEFL 80 and RDWP 300 data sets, perform better than or comparable to existing semantic relatedness measures.	algorithm;markov chain;semantic similarity;testbed;word lists by frequency	Timothy Weale;Chris Brew;Eric Fosler-Lussier	2009			computer science;pattern recognition;data mining;information retrieval	NLP	-26.861735533931455	-64.55199116017194	29742
acd3d308af7292020e1f26906c7d5601ce43adc4	emory university at trec liveqa 2016: combining crowdsourcing and learning-to-rank approaches for real-time complex question answering		This paper describes the two QA systems we developed to participate in the TREC LiveQA 2016 shared task. The first run represents an improvement of our fully automatic real-time QA system from LiveQA 2015, Emory-QA. The second run, Emory-CRQA, which stands for Crowd-powered Real-time Question Answering, incorporates human feedback, in real-time, to improve answer candidate generation and ranking. The base Emory-QA system uses the title and the body of a question to query Yahoo! Answers, Answers.com, WikiHow and general web search and retrieve a set of candidate answers along with their topics and contexts. This information is used to represent each candidate by a set of features, rank them with a trained LambdaMART model, and return the top ranked candidates as an answer to the question. The second run, Emory-CRQA, integrates a crowdsourcing module, which provides the system with additional answer candidates and quality ratings, obtained in near real-time (under one minute) from a crowd of workers When Emory-CRQA receives a question, it is forwarded to the crowd, who can start working on the answer in parallel with the automatic pipeline. When the automatic pipeline is done generating and ranking candidates, a subset of them is immediately sent to the same workers who have been working on answering the questions. Workers than rate the quality of all humanor systemgenerated candidate answers. The resulting ratings as well as original system scores are used as features for the final re-ranking module, which returns the highest scoring answer. The official run results of the tasks indicate promising improvements for both runs compared to the best performing system from LiveQA 2015. Additionally, they demonstrate the effectiveness of the introduced crowdsourcing module, which allowed us to achieve an improvement of ∼20% in average answer score over a fully automatic Emory-QA system.	crowdsourcing;learning to rank;performance;question answering;real-time clock;real-time computing;real-time locating system;real-time transcription;software quality assurance;text retrieval conference;web search engine	Denis Savenkov;Eugene Agichtein	2016			learning to rank;data mining;information retrieval;data science;computer science;complex question;crowdsourcing	Web+IR	-31.66018257324318	-64.51474537343628	29745
843e443aab36c36075fbb5e407ded2d79b6ec853	bottom-up named entity recognition using two-stage machine learning method		This paper proposes Japanese bottom-up named entity recognition using a twostage machine learning method. Most work has formalized Named Entity Recognition as a sequential labeling problem, in which only local information is utilized for the label estimation, and thus a long named entity consisting of several morphemes tends to be wrongly recognized. Our proposed method regards a compound noun (chunk) as a labeling unit, and first estimates the labels of all the chunks in a phrasal unit (bunsetsu) using a machine learning method. Then, the best label assignment in the bunsetsu is determined from bottom up as the CKY parsing algorithm using a machine learning method. We conducted an experimental on CRL NE data, and achieved an F measure of 89.79, which is higher than previous work.	algorithm;bottom-up parsing;dynamic programming;machine learning;named entity;top-down and bottom-up design	Hirotaka Funayama;Tomohide Shibata;Sadao Kurohashi	2009			natural language processing;speech recognition;computer science;pattern recognition	AI	-23.131654157863494	-74.28981401607935	29751
e45e82ca3c0371c254067f810e620a6a1875a348	domain-specific modeling: a food and drink gazetteer		Our goal is to build a Food and Drink (FD) gazetteer that can serve for classification of general, FD-related concepts, efficient faceted search or automated semantic enrichment. Fully supervised design of domain-specific models ex novo is not scalable. Integration of several ready knowledge bases is tedious and does not ensure coverage. Completely data-driven approaches require a large amount of training data, which is not always available. For general domains (such as the FD domain), re-using encyclopedic knowledge bases like Wikipedia may be a good idea. We propose here a semi-supervised approach that uses a restricted Wikipedia as a base for the modeling, achieved by selecting a domain-relevant Wikipedia category as root for the model and all its subcategories, combined with expert and data-driven pruning of irrelevant categories.	domain-specific language;domain-specific modeling;faceted classification;gene ontology term enrichment;knowledge base;national address gazetteer;relevance;scalability;semi-supervised learning;semiconductor industry;wikipedia	Andrey Tagarev;Laura Tolosi;Vladimir Alexiev	2017	Trans. Computational Collective Intelligence	10.1007/978-3-319-59268-8_9	data mining;domain-specific modeling;scalability;faceted search;categorization;training set;computer science	AI	-32.31207587442161	-67.28111840439152	29785
3ca384d8a9b110dbcf4911577ef24f9e49fdd948	distributional learning and lexical category acquisition: what makes words easy to categorize?		In this study, results of computational simulations on English child-directed speech are presented to uncover what distributional properties of words make it easier to group them into lexical categories. This analysis provides evidence that words are easier to categorize when (i) they are hard to predict given the contexts they occur in; (ii) they occur in few different contexts; and (iii) their contextual distributions have a low entropy, meaning that they tend to occur more often in one of the contexts they occur in. This profile fits that of content words, especially nouns and verbs, which is consistent with developmental evidence showing that children learning English start by forming a noun and a verb category. These results further characterize the role of distributional information in lexical category acquisition and confirm that it is a robust, reliable, and developmentally plausible source to learn lexical categories.	categorization;computer simulation;distributional semantics;fits	Giovanni Cassani;Robert Grimm;Steven Gillis;Walter Daelemans	2017			cognitive psychology;part of speech;categorization;psychology	NLP	-10.265250792167542	-77.50541523311841	29790
0c1703202956fbfad41a180ffe3f3cf9685cc743	structured text modification using guided inference	programming by example;text entry;structure prediction;empirical evaluation;end user programming	"""We describe a technique that allows end-users to specify automated transformations of structured text by inferring an underlying model. Inference is achieved with a novel algorithm, Structured Prediction by Partial Match (SPPM), a generalisation of the well-known PPM approach to predictive text entry and compression. We created two simple applications, as examples of """"first steps"""" end-user programming tasks that can be achieved using SPPM. In empirical evaluations, each of these applications proved to be substantially superior to equivalent facilities in leading commercial products."""	algorithm;end-user development;predictive text;profile-guided optimization;structured prediction;structured text;whole earth 'lectronic link	Luke Church;Alan F. Blackwell	2008			computer science;machine learning;pattern recognition;data mining	ML	-13.344802360760012	-67.27947298404992	29866
2bde12d9f5a5b89ca11fb64f03243ee9386e7d8c	clinical language annotation, modeling, and processing toolkit (clamp) - a user-centric nlp system			natural language processing	Ergin Soysal;Jingqi Wang;Min Jiang;Yonghui Wu;Hua Xu	2015			clamp;natural language processing;annotation;artificial intelligence;user-centered design;computer science	NLP	-32.41537541293404	-77.06563516508413	29891
f6b0ab486254832da9bb1f1182fdd520879cc01f	a new method for assessing gloss based on digital imaging			digital imaging;gloss (annotation)	Wei Ji;M. Ronnier Luo;Cai Fu Li;Guihua Cui;Michael R. Pointer	2004			computer vision;gloss (annotation);artificial intelligence;digital imaging;computer science	EDA	-11.955939288064705	-63.762711958717944	29905
812b3fe750a3729b8edc385f5e912c00bc200832	sentiment and authority analysis in conversational content		This paper deals with mining conversational content from the social media. It focused on two issues: opinion and emotion classification and identification of authoritative reviewers. The paper also describes applications representing the results obtained in the given areas. Authority identification can be used by organizations to search for experts in their specific areas to employ them. The opinion and emotion analysis can be useful for providing decision-making support.		Kristína Machová;Martin Mikula;Martina Szabóová;Marián Mach	2018	Computing and Informatics		theoretical computer science;natural language processing;computer science;emotion classification;social media;artificial intelligence	HCI	-22.811471241474905	-56.45208865258273	29906
e923e85b2d491c37bac08258deff0af485ab71b9	discriminative corpus weight estimation for machine translation	discriminative corpus weight estimation;translation model parameter;training corpus;training bitext;discriminative objective function;training data quality;translation model;arabic-english translation;current statistical machine translation;discriminative corpus weight;low quality training data;parameter estimation;col;data quality;machine translation;objective function	Current statistical machine translation (SMT) systems are trained on sentencealigned and word-aligned parallel text collected from various sources. Translation model parameters are estimated from the word alignments, and the quality of the translations on a given test set depends on the parameter estimates. There are at least two factors affecting the parameter estimation: domain match and training data quality. This paper describes a novel approach for automatically detecting and down-weighing certain parts of the training corpus by assigning a weight to each sentence in the training bitext so as to optimize a discriminative objective function on a designated tuning set. This way, the proposed method can limit the negative effects of low quality training data, and can adapt the translation model to the domain of interest. It is shown that such discriminative corpus weights can provide significant improvements in Arabic-English translation on various conditions, using a state-of-the-art SMT system.	data quality;data structure alignment;discriminative model;estimation theory;human body weight;loss function;optimization problem;parallel text;rule induction;sensor;statistical machine translation;test set;the sentence	Spyridon Matsoukas;Antti-Veikko I. Rosti;Bing Zhang	2009			natural language processing;speech recognition;transfer-based machine translation;data quality;computer science;machine learning;pattern recognition;machine translation;rule-based machine translation;estimation theory;statistics	NLP	-20.3961955639404	-77.62967579881453	29963
f1df595152f020e866c941238fdea2a9d35d6e18	a hybrid ensemble pruning approach based on consensus clustering and multi-objective evolutionary algorithm for sentiment classification		Sentiment analysis is a critical task of extracting subjective information from online text documents. Ensemble learning can be employed to obtain more robust classification schemes. However, most approaches in the field incorporated feature engineering to build efficient sentiment classifiers.#R##N##R##N#The purpose of our research is to establish an effective sentiment classification scheme by pursuing the paradigm of ensemble pruning. Ensemble pruning is a crucial method to build classifier ensembles with high predictive accuracy and efficiency. Previous studies employed exponential search, randomized search, sequential search, ranking based pruning and clustering based pruning. However, there are tradeoffs in selecting the ensemble pruning methods. In this regard, hybrid ensemble pruning schemes can be more promising.#R##N##R##N#In this study, we propose a hybrid ensemble pruning scheme based on clustering and randomized search for text sentiment classification. Furthermore, a consensus clustering scheme is presented to deal with the instability of clustering results. The classifiers of the ensemble are initially clustered into groups according to their predictive characteristics. Then, two classifiers from each cluster are selected as candidate classifiers based on their pairwise diversity. The search space of candidate classifiers is explored by the elitist Pareto-based multi-objective evolutionary algorithm.#R##N##R##N#For the evaluation task, the proposed scheme is tested on twelve balanced and unbalanced benchmark text classification tasks. In addition, the proposed approach is experimentally compared with three ensemble methods (AdaBoost, Bagging and Random Subspace) and three ensemble pruning algorithms (ensemble selection from libraries of models, Bagging ensemble selection and LibD3C algorithm). Results demonstrate that the consensus clustering and the elitist pareto-based multi-objective evolutionary algorithm can be effectively used in ensemble pruning. The experimental analysis with conventional ensemble methods and pruning algorithms indicates the validity and effectiveness of the proposed scheme.	cluster analysis;consensus clustering;evolutionary algorithm	Aytug Onan;Serdar Korukoglu;Hasan Bulut	2017	Inf. Process. Manage.	10.1016/j.ipm.2017.02.008	data mining;consensus clustering;sentiment analysis;exponential search;adaboost;cluster analysis;computer science;instability;pruning (decision trees);machine learning;ensemble learning;pattern recognition;artificial intelligence	AI	-20.77862293835825	-63.649868513565714	29966
35b48d63ce26f084314bc9a1d295d31b2630eb35	a step-by-step model for human detection of four compositional-based translation errors	meaning compositionality;translation errors;omission;meaning unit precision algorithm;under translation;addition;over translation;error detection	In this paper, we describe a step-by-step model for human detection of four compositional translation errors between two segments in source and target language. The model is based on the application of a meaning unit precision algorithm for two groups of symmetrical translation errors and on the acceptability assessment of the candidate target segment for over-translation and under-translation errors. Although we borrowed the conceptual framework from automatic assessment of machine translations like Papineni (2002), the algorithm is not intended for use in assessment of automatic machine translation but of human translation. Its purpose is to describe some common human translation errors that are compositional. The analysis described reflects professional state of the art practices and hopefully will be of interest in training professional translators and in the formalization and systematic description of meaning-based translation methods that will be made possible in the future when the processing of meaning units will be feasible.	algorithm;compiler;machine translation	Éric André Poirier	2014		10.1145/2668260.2668288	dynamic and formal equivalence;natural language processing;synchronous context-free grammar;error detection and correction;transfer-based machine translation;example-based machine translation;computer science;rule-based machine translation;addition;algorithm	NLP	-27.00402146823681	-74.83318667770797	30053
ce83d1648c6ba67f6f776513dad0fab516895f85	performance of decision trees on arabic text categorization	decision tree		categorization;decision tree;document classification	Fouzi Harrag;Eyas Al-Qawasmah	2009	JDIM		arabic;data mining;decision tree;categorization;natural language processing;artificial intelligence;computer science	ML	-23.026255427013687	-68.37124265494819	30113
db625c4c26c7df67c9099e78961d479532628ec7	all-in text: learning document, label, and word representations jointly		Conventional multi-label classification algorithms treat the target labels of the classification task as mere symbols that are void of an inherent semantics. However, in many cases textual descriptions of these labels are available or can be easily constructed from public document sources such as Wikipedia. In this paper, we investigate an approach for embedding documents and labels into a joint space while sharing word representations between documents and labels. For finding such embeddings, we rely on the text of documents as well as descriptions for the labels. The use of such label descriptions not only lets us expect an increased performance on conventional multi-label text classification tasks, but can also be used to make predictions for labels that have not been seen during the training phase. The potential of our method is demonstrated on the multi-label classification task of assigning keywords from the Medical Subject Headings (MeSH) to publications in biomedical research, both in a conventional and in a zero-shot learning setting.	algorithm;document classification;multi-label classification;wikipedia	Jinseok Nam;Eneldo Loza Mencía;Johannes Fürnkranz	2016			natural language processing;computer science;machine learning;data mining;information retrieval	AI	-24.269152468612997	-67.79521698321167	30153
06f5ea1eba3dc53945ca976cf44734ac5625e67d	context-sensitive recognition for emerging and rare entities		We present a novel named entity recognition (NER) system, and its participation in the Emerging and Rare Entity Recognition shared task, hosted at the 2017 EMNLP Workshop on Noisy User Generated Text (W-NUT). With a specialized evaluation highlighting performance on rare, and sparsely-occurring named entities, this task provided an excellent opportunity to build out a newly-developed statistical algorithm and benchmark it against the state-of-the-art. Powered by flexible context features of word forms, our system’s capacity for identifying neverbefore-seen entities made it well suited for the task. Since the system was only developed to recognize a limited number of named entity types, its performance was lower overall. However, performance was competitive on the categories trained, indicating potential for future development.	algorithm;benchmark (computing);empirical methods in natural language processing;named entity	Jake Williams;Giovanni Santia	2017			natural language processing;computer science;artificial intelligence	NLP	-31.02755466720624	-72.60738329818112	30166
ca0332d5c05cbdedaf8c29e3bdad14f371c77118	using embedding masks for word categorization		Word embeddings are widely used nowadays for many NLP tasks. They reduce the dimensionality of the vocabulary space, but most importantly they should capture (part of) the meaning of words. The new vector space used by the embeddings allows computation of semantic distances between words, while some word embeddings also permit simple vector operations (e.g. summation, difference) resembling analogical reasoning. This paper proposes a new operation on word embeddings aimed to capturing categorical information by first learning and then applying an embedding mask for each analyzed category. Thus, we conducted a series of experiments related to categorization of words based on their embeddings. Several classical approaches were compared together with the one introduced in the paper which uses different embedding masks learnt for each category.	categorization;computation;experiment;mask (computing);memory-level parallelism;natural language processing;variable shadowing;vocabulary;word embedding;wordnet	Stefan Ruseti;Traian Rebedea;Stefan Trausan-Matu	2016		10.18653/v1/W16-1623	discrete mathematics;machine learning;pattern recognition;mathematics	NLP	-19.842175437610734	-72.67571344846853	30191
55117be67c7313863c74067cd71f1a79a2eb19ad	neural program search: solving programming tasks from description and examples		We present a Neural Program Search, an algorithm to generate programs from natural language description and a small number of input / output examples. The algorithm combines methods from Deep Learning and Program Synthesis fields by designing rich domain-specific language (DSL) and defining efficient search algorithm guided by a Seq2Tree model on it. To evaluate the quality of the approach we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs. We show that our algorithm significantly outperforms a sequence-to-sequence model with attention baseline.	baseline (configuration management);deep learning;digital subscriber line;domain-specific language;input/output;natural language;program synthesis;search algorithm;semiconductor industry;synthetic intelligence	Illia Polosukhin;Alexander Skidanov	2018	CoRR		program synthesis;machine learning;artificial intelligence;deep learning;natural language;computer science;digital subscriber line;small number;search algorithm	AI	-17.83998308928863	-74.6086503653996	30279
d482b906372bed8ae1d06edf0533c8c5dbea6849	proceedings of the lexsem+logics workshop 2016		Lexical semantics continues to play an important role in driving research directions in NLP, with the recognition and understanding of context becoming increasingly important in delivering successful outcomes in NLP tasks. Besides traditional processing areas such as word sense and named entity disambiguation, the creation and maintenance of dictionaries, annotated corpora and resources have become cornerstones of lexical semantics research and produced a wealth of contextual information that NLP processes can exploit. New efforts both to link and construct from scratch such information - as Linked Open Data or by way of formal tools coming from logic, ontologies and automated reasoning - have increased the interoperability and accessibility of resources for lexical and computational semantics, even in those languages for which they have previously been limited. LexSem+Logics 2016 combines the 1st Workshop on Lexical Semantics for Lesser-Resources Languages and the 3rd Workshop on Logics and Ontologies. The accepted papers in our program covered topics across these two areas, including: the encoding of plurals in Wordnets, the creation of a thesaurus from multiple sources based on semantic similarity metrics, and the use of cross-lingual treebanks and annotations for universal part-of-speech tagging. We also welcomed talks from two distinguished speakers: on Portuguese lexical knowledge bases (different approaches, results and their application in NLP tasks) and on new strategies for open information extraction (the capture of verb-based propositions from massive text corpora).	accessibility;automated reasoning;computational semantics;dictionary;entity linking;game semantics;information extraction;interoperability;linked data;mike lesser;named entity;natural language processing;ontology (information science);part-of-speech tagging;semantic similarity;text corpus;thesaurus;treebank;word sense;word-sense disambiguation	Steven Neale;Valeria de Paiva;Arantxa Otegi;Alexandre Rademaker	2016	CoRR		natural language processing;lexical markup framework;computer science;artificial intelligence;data mining;linguistics;programming language;information retrieval	NLP	-30.664246286388156	-74.99757909315875	30315
668314f4ce570ee9e12ae1bb7cba98e310d3e895	improving historical spelling normalization with bi-directional lstms and multi-task learning.		Natural-language processing of historical documents is complicated by the abundance of variant spellings and lack of annotated data. A common approach is to normalize the spelling of historical words to modern forms. We explore the suitability of a deep neural network architecture for this task, particularly a deep bi-LSTM network applied on a character level. Our model compares well to previously established normalization algorithms when evaluated on a diverse set of texts from Early New High German. We show that multi-task learning with additional normalization data can improve our model’s performance further.	algorithm;american and british english spelling differences;artificial neural network;baseline (configuration management);computer multitasking;conditional random field;database normalization;deep learning;encoder;experiment;historical document;long short-term memory;multi-task learning;natural language processing;network architecture;neural machine translation;norma;statistical machine translation;test set;text corpus;transduction (machine learning)	Marcel Bollmann;Anders Søgaard	2016			machine learning;normalization (statistics);architecture;natural language processing;artificial neural network;artificial intelligence;computer science;spelling;multi-task learning	NLP	-19.984279602881603	-74.00460040130129	30437
9131a333b5a98d339a45b77562bdf278411a3713	a content and social network approach of bibliometrics analysis across domains	information retrieval;temporal information;social network;hybrid approach;content;bibliometrics;world wide web;bibliometric analysis;co authorship network	Bibliometrics data contain rich co-authorship network, text and temporal information. In this work, we employ a hybrid approach that incorporating content and social network similarity to conduct a bibliometrics analysis across the information retrieval and World Wide Web domains using the DBLP dataset.	bibliometrics;dbl-browser;information retrieval;social network;world wide web	Christopher C. Yang;Xuning Tang	2012		10.1145/2132176.2132270	geography;bibliometrics;data mining;world wide web;information retrieval	Web+IR	-26.587807653782424	-57.98838268035659	30457
1c85528612c9fa90ac8a775072ec5710328a829e	incorporating discrete translation lexicons into neural machine translation		Neural machine translation (NMT) often makes mistakes in translating low-frequency content words that are essential to understanding the meaning of the sentence. We propose a method to alleviate this problem by augmenting NMT systems with discrete translation lexicons that efficiently encode translations of these low-frequency words. We describe a method to calculate the lexicon probability of the next word in the translation candidate by using the attention vector of the NMT model to select which source word lexical probabilities the model should focus on. We test two methods to combine this probability with the standard NMT probability: (1) using it as a bias, and (2) linear interpolation. Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time. 1	bleu;encode;experiment;lexicon;linear interpolation;neural machine translation;text corpus	Philip Arthur;Graham Neubig;Satoshi Nakamura	2016			natural language processing;speech recognition;transfer-based machine translation;computer science	NLP	-19.123622453907686	-75.75877774558963	30510
72913c4a2be03bfb32932cf889b5202e1a09750e	two methods to incorporate 'local morphosyntactic' features in hindi dependency parsing		In this paper we explore two strategies to incorporate local morphosyntactic features in Hindi dependency parsing. These features are obtained using a shallow parser. We first explore which information provided by the shallow parser is most beneficial and show that local morphosyntactic features in the form of chunk type, head/non-head information, chunk boundary information, distance to the end of the chunk and suffix concatenation are very crucial in Hindi dependency parsing. We then investigate the best way to incorporate this information during dependency parsing. Further, we compare the results of various experiments based on various criterions and do some error analysis. All the experiments were done with two data-driven parsers, MaltParser and MSTParser, on a part of multi-layered and multi-representational Hindi Treebank which is under development. This paper is also the first attempt at complete sentence level pars-	concatenation;error analysis (mathematics);experiment;shallow parsing;treebank	Bharat Ram Ambati;Samar Husain;Sambhav Jain;Dipti Misra Sharma;Rajeev Sangal	2010			natural language processing;parser combinator;speech recognition;computer science;bottom-up parsing;linguistics;top-down parsing	NLP	-22.05323487227292	-76.71190895051454	30529
12cc937343a65fa601dc35b408e5f8443ecc1da8	pre-translation for neural machine translation		Recently, the development of neural machine translation (NMT) has significantly improved the translation quality of automatic machine translation. While most sentences are more accurate and fluent than translations by statistical machine translation (SMT)-based systems, in some cases, the NMT system produces translations that have a completely different meaning. This is especially the case when rare words occur. When using statistical machine translation, it has already been shown that significant gains can be achieved by simplifying the input in a preprocessing step. A commonly used example is the pre-reordering approach. In this work, we used phrase-based machine translation to pre-translate the input into the target language. Then a neural machine translation system generates the final hypothesis using the pre-translation. Thereby, we use either only the output of the phrase-based machine translation (PBMT) system or a combination of the PBMT output and the source sentence. We evaluate the technique on the English to German translation task. Using this approach we are able to outperform the PBMT system as well as the baseline neural MT system by up to 2 BLEU points. We analyzed the influence of the quality of the initial system on the final result.	bleu;baseline (configuration management);compiler;concatenation;ensemble forecasting;neural machine translation;preprocessor;statistical machine translation	Jan Niehues;Eunah Cho;Thanh-Le Ha;Alexander H. Waibel	2016			natural language processing;synchronous context-free grammar;speech recognition;transfer-based machine translation;example-based machine translation;computer science;rule-based machine translation	NLP	-21.80572961584757	-77.01700470965997	30658
25ca49f729d2d2d36859c19100a35361dc1854e4	compositional learning of relation paths embedding for knowledge base completion		Large-scale knowledge bases have currently reached impressive sizes; however, these knowledge bases are still far from complete. In addition, most of the existing methods for knowledge base completion only consider the direct links between entities, ignoring the vital impact of the consistent semantics of relation paths. In this paper, we study the problem of how to better embed entities and relations of knowledge bases into different low-dimensional spaces by taking full advantage of the additional semantics of relation paths, and we propose a compositional learning model of relation path embedding (RPE). Specifically, with the corresponding relation and path projections, RPE can simultaneously embed each entity into two types of latent spaces. It is also proposed that type constraints could be extended from traditional relation-specific constraints to the new proposed path-specific constraints. The results of experiments show that the proposed model achieves significant and consistent improvements compared with the state-of-the-art algorithms.	algorithm;entity;experiment;knowledge base;question answering;relationship extraction	Xixun Lin;Yanchun Liang;Renchu Guan	2016	CoRR		artificial intelligence;theoretical computer science;machine learning;algorithm	AI	-16.24821119698669	-66.72451164921162	30662
52ebf89baf678e47bbd82014e22ee65f7a46b4a9	all, and only, the errors: more complete and consistent spelling and ocr-error correction evaluation.	area under the curve;evaluation metric;error correction	Some time in the future, some spelling error correction syst em will correct all the errors, and only the errors. We need ev aluation metrics that will tell us when this has been achieved and that can help guide us there. We survey the current practice in the form of t he evaluation scheme of the latest major publication on spelling correcti on in a leading journal. We are forced to conclude that while t e metric used there can tell us exactly when the ultimate goal of spelling c orrection research has been achieved, it offers little in th e way of directions to be followed to eventually get there. We propose to consist ently use the well-known metrics Recall and Precision, as co mbined in the F score, on 5 possible levels of measurement that should g uide us more informedly along that path. We describe briefly w hat is then measured or measurable at these levels and propose a fra mework that should allow for concisely stating what it is one performs in one’s evaluations. We finally contrast our preferred metr ics to Accuracy, which is widely used in this field to this day a nd to the Area-Under-the-Curve, which is increasingly finding accep tance in other fields.	error detection and correction;f1 score;level of measurement;precision and recall;qr code;whole earth 'lectronic link	Martin Reynaert	2008			error detection and correction;area under the curve;computer science;algorithm	Web+IR	-31.144538180660604	-72.99226964934452	30683
e09d77ebacde3163016e549b12b06355811957f1	metagui 3: a graphical user interface for choosing the collective variables in molecular dynamics simulations	molecular dynamics;natural sciences;clustering;gui;free energy	Abstract Molecular dynamics (MD) simulations allow the exploration of the phase space of biopolymers through the integration of equations of motion of their constituent atoms. The analysis of MD trajectories often relies on the choice of collective variables (CVs) along which the dynamics of the system is projected. We developed a graphical user interface (GUI) for facilitating the interactive choice of the appropriate CVs. The GUI allows: defining interactively new CVs; partitioning the configurations into microstates characterized by similar values of the CVs; calculating the free energies of the microstates for both unbiased and biased (metadynamics) simulations; clustering the microstates in kinetic basins; visualizing the free energy landscape as a function of a subset of the CVs used for the analysis. A simple mouse click allows one to quickly inspect structures corresponding to specific points in the landscape. Program summary Program Title: METAGUI 3 Program Files doi: http://dx.doi.org/10.17632/wyxjndwkbp.1 Licensing provisions: GPLv3 Programming language: Tcl/Tk, Fortran Journal reference of previous version: METAGUI [1] Does the new version supersede the previous version?: No Nature of problem: Choose the appropriate collective variables for describing the thermodynamics and kinetics of a biomolecular system through biased and unbiased molecular dynamics. Solution method: Provide an environment to compute and visualize free energy surfaces as a function of collective variables, interactively defined. Additional comments: METAGUI 3 is not a standalone program but a plugin that provides analysis features within VMD (version 1.9.2 or higher). [1] X. Biarnes, F. Pietrucci, F. Marinelli, A. Laio, METAGUI. A VMD interface for analyzing metadynamics and molecular dynamics simulations, Computer Physics Communications 183 (2012) 203–211.	alessandro vespignani;cluster analysis;cost per mille;cross-reference;data visualization;event (computing);fabio paternò;gnu;graphical user interface;https;interactivity;iteration;linear algebra;metadynamics;molecular dynamics;simulation	Toni Giorgino;Alessandro Laio;Alex Rodriguez	2017	Computer Physics Communications	10.1016/j.cpc.2017.04.009	ministate;plug-in;cluster analysis;fortran;phase space;energy landscape;theoretical computer science;computer science;metadynamics;graphical user interface	Graphics	-6.8094883690726205	-58.047828609683656	30722
0d70b3b0e8436b1be79b81e40cbcf171eb2e651d	approximate n-gram markov model for natural language generation	markov model;natural language generation;machine translation	This paper proposes an Approximate n-gram Markov Model for bag generation. Directed word association pairs with distances are used to approximate (n-1)-gram and n-gram training tables. This model has parameters of word association model, and merits of both word association model and Markov Model. The training knowledge for bag generation can be also applied to lexical selection in machine translation design.	approximation algorithm;machine translation;markov chain;markov model;n-gram;natural language generation	Hsin-Hsi Chen;Yue-Shi Lee	1994	CoRR		natural language processing;markov chain;maximum-entropy markov model;speech recognition;computer science;machine learning;pattern recognition;linguistics;machine translation;markov algorithm;markov process;markov model;variable-order markov model	NLP	-21.443927963469076	-78.39887876246023	30733
491ec7a6def19a06cbea30fab3fd1043bd503502	adapting text simplification decisions to different text genres and target users	simplificacion de textos;computacion informatica;filologias;004 informatik;method adaptation;adaptacion de metodos;info eu repo semantics article;informacion documentacion;linguistica;sentence classification;text simplification;ciencias basicas y experimentales;grupo a;ciencias sociales;grupo b;clasificacion de oraciones	We investigate sentence deletion and split decisions in Spanish text simplification for two different corpora aimed at different groups of users. We analyse sentence transformations in two parallel corpora of original and manually simplified texts for two different types of users and then conduct two classification experiments: classifying between those sentences to be deleted and those to be kept; and classifying between sentences to be split and those to be left unsplit. Both experiments were first run on each of the two corpora separately and then run by using one corpus for the training and the other for testing. The results indicated that both sentence decision systems could be successfully trained on one corpus and then used for a different text genre in a text simplification system aimed at a different target population.	experiment;parallel text;text corpus;text simplification	Sanja Stajner;Horacio Saggion	2013	Procesamiento del Lenguaje Natural		natural language processing;text simplification;speech recognition;computer science;linguistics	NLP	-26.734831115555963	-77.72673044279452	30778
aec00bf04bf4f721980fd7f6859083e3f01647a6	string transduction with target language models and insertion handling		Many character-level tasks can be framed as sequence-to-sequence transduction, where the target is a word from a natural language. We show that leveraging target language models derived from unannotated target corpora, combined with a precise alignment of the training data, yields state-of-the art results on cognate projection, inflection generation, and phoneme-to-grapheme conversion.	compiler;experiment;insertion sort;language model;natural language;text corpus;transducer;transduction (machine learning)	Garrett Nicolai;Saeed Najafi;Grzegorz Kondrak	2018	CoRR		natural language processing;artificial intelligence;transduction (genetics);cognate;natural language;inflection;language model;computer science;training set	NLP	-21.546433689728882	-77.5818756272001	30787
2ab8c4f841f67dbf74bea4bbc89b64e17f589ee1	una demostración de onoma, el conjugador en línea de verbos y neologismos verbales en español	herramienta en linea;computacion informatica;conjugator;filologias;info eu repo semantics article;informacion documentacion;verb neologism;linguistica;ciencias basicas y experimentales;online tool;conjugador;neologismo verbal;grupo a;ciencias sociales;grupo b	We present Onoma, an online tool to conjugate and analyze Spanish verbs and verb neologisms. Its development results from of an important linguistic study in which the Spanish verbal system is approached with an innovative methodology. Both the evaluation and the noticeable success confirm its validity and interest.	neologism;unique name assumption	Eduardo Basterrechea;Luz Rello;Rodrigo Alarcón	2011	Procesamiento del Lenguaje Natural		linguistics;computer science	Logic	-27.946538465003623	-77.43849712009677	30793
c0c7e0169996c2cac65f19cad9f4916436d28e8a	finding associations between concepts in an unstructured corpus of texts (invited address)	belief networks;deductive text mining;interestingness;information extraction;text mining;learning artificial intelligence data mining belief networks;bayesian methods;patterns extraction;ontologies building;natural languages;data mining;delta modulation;data mining ontologies delta modulation text mining taxonomy floods natural languages performance analysis bayesian methods;rules induction;taxonomy;performance analysis;ontologies;inductive text mining;floods;patterns extraction interestingness inductive text mining deductive text mining information extraction rules induction ontologies building;learning artificial intelligence	Presents an application based on an evaluation of the interestingness of the rules induced from examples using inductive text mining (ITM). The better-known deductive text mining is called information extraction, and amounts to finding instances of a predefined pattern in a set of texts. ITM looks for unknown patterns or rules to discover inside a set of texts. We mainly discuss two of the problems of ITM: building ontologies of concepts, and extracting patterns.		Yves Kodratoff	2001		10.1109/DEXA.2001.953073	natural language processing;delta modulation;text mining;bayesian probability;computer science;ontology;artificial intelligence;pattern recognition;data mining;natural language;information extraction;taxonomy	NLP	-24.412748450958944	-63.995488422099896	30813
8df0e41c4338bff354b1bdd2dd55d4efed912692	reconstructing ddc for interactive classification	interactive classification;data collection;taxonomy reconstruction;dewey decimal classification;large scale;hierarchical classification;real world application;library of congress;classification system;trimming machine;bibliographic data;automatic classification;user interaction;text categorization	The automated text categorization (TC) has made prominent progress in recent years. However, seldom work is done on automatic classification with library classification systems, the largest and most sophisticated classification systems people ever built, such as the Dewey Decimal classification (DDC). The library classification is a very laborious and time-consuming job that requires qualification and good training.  The large-scale classification schemes, such as the DDC, impose several obstacles to the state-of-art TC technologies, including very deep hierarchy, data sparseness, and skewed category distribution. These problems characterize large corpora of real-world applications and it is very hard, if not impossible, to obtain satisfactory results. In this paper, we propose a novel algorithm to reconstruct classification schemes according to the document density and category distribution, and to transform the category hierarchy into a balanced virtual taxonomy by merging sparse categories, lopping dense branches and flattening the hierarchy. To make the classification performance acceptable to real-world applications, we also propose an interactive classification model that only needs two or three times of user interaction. Extensive experiments are conducted on a 10-year bibliographic data collection of the Library of Congress to verify the proposed methodology.	algorithm;categorization;dewey decimal classification;display data channel;document classification;experiment;interactive storytelling;library classification;neural coding;sparse matrix;text corpus	Meng Chen Lee	2007		10.1145/1321440.1321462	natural language processing;web query classification;computer science;artificial intelligence;data science;machine learning;multiclass classification;dewey decimal classification;data mining;database;world wide web;one-class classification;information retrieval;statistics;library classification;data collection	Web+IR	-23.70813880752858	-65.28329166028306	30816
6d9a073629e5ac0b857e8064182f6fe9c7f410b9	semantic browsing of video surveillance databases through online generic indexing	database indexing;multicamera context semantic browsing video surveillance database online generic indexing eads urbanview indexing mining platform scalable indexing architecture;event recognition;moving object;video surveillance forensics;video surveillance data mining database indexing image retrieval video databases;video databases;video surveillance;video streaming;content based indexing;real time;data mining;surveillance database mining and browsing;video surveillance indexing data security smart cameras streaming media real time systems object detection distributed databases character recognition information retrieval;semantic metadata;indexing;streaming media;image color analysis;indexation;heavy traffic;detection rate;video content based indexing;surveillance database mining and browsing video surveillance forensics video content based indexing;video database;camera network;object identification;cameras;tracking;real time systems;image retrieval	This paper gives a thorough overview of EADS UrbanVIEW indexing and mining platform aimed at providing police forces and security officers with advanced tools to efficiently browse large video surveillance databases for investigation purposes. A scalable indexing architecture that works indifferently with smart or classical camera networks as well as for real-time or a posteriori indexing has been designed and implemented. We introduce the concept of Online Generic Indexing Strategy (OGIS) aimed at systematically enriching each video stream with real-time extracted generic metadata allowing to dramatically decrease post-event investigation time. The indexing strategy relies on the systematic detection, tracking and characterization of all observed moving objects. Semantic and non semantic metadata produced by embedded or distributed video analytics modules can be used either to browse the distributed video databases or as inputs to higher level characterization modules (object identification, multi-camera back-tracking, event recognition…). Once a first observation of an object of interest has been found, it can be forward and backward tracked thanks to an interactive multi-stream player taking into account the multi-camera context. Our platform has been assessed on the NGSIM and I-LIDS datasets which consist of real heavy traffic images, showing both high recall and high detection rates in its retrieval capabilities.	browsing;categorization;closed-circuit television;database;embedded system;image analysis;image quality;linux intrusion detection system;real life;real-time clock;real-time locating system;scalability;streaming media;video content analysis	Denis Marraud;Benjamin Cepas;Livier Reithler	2009	2009 Third ACM/IEEE International Conference on Distributed Smart Cameras (ICDSC)	10.1109/ICDSC.2009.5289366	database index;search engine indexing;image retrieval;computer science;video tracking;tracking;internet privacy;world wide web;information retrieval	DB	-12.376421271925452	-55.10869936557892	30832
12fc6f855f58869cf81743b9be0df1380c17f4d0	exploiting redundancy in question answering	experimental tests;informational inference;conceptual space;information flow;passage retrieval;multiple choice;test collection;question answering	Our goal is to automatically answer brief factual questions of the form ``When was the Battle of Hastings?'' or ``Who wrote The Wind in the Willows?''. Since the answer to nearly any such question can now be found somewhere on the Web, the problem reduces to finding potential answers in large volumes of data and validating their accuracy. We apply a method for arbitrary passage retrieval to the first half of the problem and demonstrate that answer redundancy can be used to address the second half. The success of our approach depends on the idea that the volume of available Web data is large enough to supply the answer to most factual questions multiple times and in multiple contexts. A query is generated from a question and this query is used to select short passages that may contain the answer from a large collection of Web data. These passages are analyzed to identify candidate answers. The frequency of these candidates within the passages is used to ``vote'' for the most likely answer. The approach is experimentally tested on questions taken from the TREC-9 question-answering test collection. As an additional demonstration, the approach is extended to answer multiple choice trivia questions of the form typically asked in trivia quizzes and television game shows.	experiment;question answering;world wide web	Charles L. A. Clarke;Gordon V. Cormack;Thomas R. Lynam	2001		10.1145/383952.384024	multiple choice;natural language processing;information flow;question answering;computer science;data mining;database;world wide web;information retrieval	Web+IR	-32.14860270520262	-61.484535977100435	30905
0522efc4600a4ebb10bae6140b5c78ed785e1d1b	semi-automatic, data-driven construction of multimedia ontologies	scene cut detection;features extraction;ontologies videos automatic speech recognition content based retrieval layout data mining face detection statistics frequency entropy;video signal processing;statistics multimedia systems video signal processing speech recognition meta data feature extraction indexing image retrieval content based retrieval;video retrieval;layout;data mining;metadata extraction;multimedia systems;trec video retrieval benchmark;video indexing;automatic speech recognition;visual content;indexing;feature extraction;ontology engineering;visual content multimedia ontologies data driven approach scene cut detection automatic speech recognition metadata extraction features extraction content based retrieval text information statistics trec video retrieval benchmark video indexing;automatic indexing;text information;statistics;speech recognition;multimedia ontologies;meta data;ontologies;entropy;face detection;frequency;content based retrieval;videos;data driven approach;image retrieval	In this paper we investigate semi-automatic construction of multimedia ontologies using a data-driven approach. We start with a collection of videos for which we wish to build an ontology (an explicit specification of a domain). Each video is pre-processed: scene cut detection, automatic speech recognition (ASR), and metadata extraction are performed. In addition we automatically index the videos based on visual content by extracting syntactic (e.g., color, texture, etc.) and semantic features (e.g., face, landscape, etc.). We then combine standard tools for ontology engineering and tools in contentbased retrieval to semi-automatically build ontologies. In the first stage we process the text information available with the videos (ASR, metadata, and annotations, if any). Stop words (e.g., a, on, the) are eliminated and statistics (e.g., frequency, TFIDF, and entropy) are computed for all terms. Based on this data we manually select concepts and relationships to include in the ontology. Then we use content-based retrieval tools to assign multimedia entities (e.g., shots, videos, collections of videos) to concepts, properties, or relationships in the ontology, and to select multimedia entities as concepts, relationships, or properties in the ontology. We explore this methodology to construct multimedia ontologies from 24 hours of educational films from the 1940s-1960s used in the TREC video retrieval benchmark and discuss the problems encountered and future directions.	benchmark (computing);entity;entropy (information theory);ontology (information science);ontology engineering;semiconductor industry;shot transition detection;speech recognition;text retrieval conference;text-based (computing);texture mapping;tf–idf	Alejandro Jaimes;John R. Smith	2003		10.1109/ICME.2003.1221034	upper ontology;layout;search engine indexing;entropy;face detection;feature extraction;image retrieval;computer science;ontology;frequency;data mining;metadata;world wide web;information retrieval;process ontology	Web+IR	-14.52529015668512	-56.55327342989067	30906
d05f4f7a5422d659e68283c2b114f97afe77f6a4	learning relationship between speech and image	kernel;image databases;training;speech;testing;hidden markov models;robots	Currently, studies on learning relationship between objects focus on the text domain. There are a few researchers who focus on relationship learning between objects in other domains. In these researches, they have tried to represent the qualitative description of structure of objects, and the symbolic relationship between them. This output provides symbolic meaning to the inter-object relationships which are useful for subsequent common sense reasoning and decision making. In this paper, we propose an approach to learn the relationship between a speech signal and an image. In other words, this approach can classify objects based on speech input.	commonsense reasoning	The Duy Bui;Quang-Trung Nguyen	2016	2016 Eighth International Conference on Knowledge and Systems Engineering (KSE)	10.1109/KSE.2016.7758037	natural language processing;robot;kernel;computer science;speech;artificial intelligence;machine learning;linguistics;software testing;hidden markov model	AI	-11.559851150032099	-72.35587174363458	30955
9426fcb06966466372f59fd2c865b8d691ed9317	towards scalable summarization and visualization of large text corpora (abstract only)	information extraction;text analytics;statistical model;relevance ranking;graph visualization;off the shelf;natural language processing;open source	Society is awash with problems requiring the analysis of vast quantities of text and data. From detecting flu trends out of twitter conversations to finding scholarly works answering specific questions, we rely more and more on computers to process text for us. Text analytics is the application of computational, mathematical, and statistical models to derive information from large quantities of data coming primarily as text. Our project provides fast and effective text-analytics tools for large document collections, such as the blogosphere. We use natural language processing and database techniques to extract, collect, analyze, visualize, and archive information extracted from text. We focus on discovering relationships between entities (people, places, organizations, etc.) mentioned in one or more sources (blog posts or news articles). We built a custom solution using mostly off-the-shelf, open-source tools to provide a scalable platform for users to search and analyze large text corpora. Currently, we provide two main outlets for users to discover these relations: (1) full-text search over the documents and (2) graph visualizations of the entities and their relationships. This provides the user with succinct and easily digestible information gleaned from the corpus as a whole. For example, we can easily pose queries like which companies were bought by Google? as entity:google relation:bought. The extracted data is stored on a combination of the noSQL database CouchDB and Apache's Lucene. This combination is justified as our work-flow consists of offline batch insertions with almost no updates. Because we support specialized queries, we can forgo the flexibility of traditional SQL solutions and materialize all necessary indices, which are used to quickly query large amounts of de-normalized data using MapReduce. Lucene provides a flexible and powerful query syntax to yield relevant ranked results to the user. Moreover, its indices are synchronized by a process subscribed to the list of database changes published by CouchDB. The graph visualizations rely on CouchDB's ability to export the data in any format: we currently use a customized graph visualization relying on XML data. Finally, we use memcached to further improve the performance, especially for queries involving popular entities.	apache couchdb;archive;blog;blogosphere;computer;entity;graph drawing;mapreduce;memcached;natural language processing;nosql;online and offline;open-source software;sql;scalability;sensor;statistical model;text corpus;text mining;xml	Tyler Sliwkanich;Douglas Schneider;Aaron Yong;Mitchell Home;Denilson Barbosa	2012		10.1145/2213836.2213970	text graph;statistical model;computer science;data mining;database;graph drawing;programming language;information extraction;information retrieval	DB	-25.34146659822787	-55.61455098904361	31022
6411896c0c7ec5601b2e0ae832c6a682182a2eb5	the interpretation of dream meaning: resolving ambiguity using latent semantic analysis in a small corpus of text	dream content analysis;word2vec;latent semantic analysis	Computer-based dreams content analysis relies on word frequencies within predefined categories in order to identify different elements in text. As a complementary approach, we explored the capabilities and limitations of word-embedding techniques to identify word usage patterns among dream reports. These tools allow us to quantify words associations in text and to identify the meaning of target words. Word-embeddings have been extensively studied in large datasets, but only a few studies analyze semantic representations in small corpora. To fill this gap, we compared Skip-gram and Latent Semantic Analysis (LSA) capabilities to extract semantic associations from dream reports. LSA showed better performance than Skip-gram in small size corpora in two tests. Furthermore, LSA captured relevant word associations in dream collection, even in cases with low-frequency words or small numbers of dreams. Word associations in dreams reports can thus be quantified by LSA, which opens new avenues for dream interpretation and decoding.	body of uterus;categories;dreams;embedding;latent semantic analysis;lichen sclerosus et atrophicus;mental association;microsoft word for mac;skip list;text corpus;word lists by frequency;gram	Edgar Altszyler;Mariano Sigman;Diego Fernández Slezak	2017	Consciousness and Cognition	10.1016/j.concog.2017.09.004	word usage;word2vec;ambiguity;dream interpretation;dream;natural language processing;artificial intelligence;content analysis;latent semantic analysis;computer science;word lists by frequency	NLP	-25.521593013722285	-72.50420402817583	31029
d61c0b9d86f60ce2220be9ee2baf5009c5ce8841	summac: a text summarization evaluation	automatic text summarization;filologias;evaluation method;text summarization;statistical significance;statistical method;term frequency;linguistica;lessons learned;present day;accuracy analysis;grupo a	The TIPSTER Text Summarization Evaluation (SUMMAC) has developed several new extrinsic and intrinsic methods for evaluating summaries. It has established definitively that automatic text summarization is very effective in relevance assessment tasks on news articles. Summaries as short as 17% of full text length sped up decision-making by almost a factor of 2 with no statistically significant degradation in accuracy. Analysis of feedback forms filled in after each decision indicated that the intelligibility of present-day machine-generated summaries is high. Systems that performed most accurately in the production of indicative and informative topic-related summaries used term frequency and co-occurrence statistics, and vocabulary overlap comparisons between text passages. However, in the absence of a topic, these statistical methods do not appear to provide any additional leverage: in the case of generic summaries, the systems were indistinguishable in accuracy. The paper discusses some of the tradeoffs and challenges faced by the evaluation, and also lists some of the lessons learned, impacts, and possible future directions. The evaluation methods used in the SUMMAC evaluation are of interest to both summarization evaluation as well as evaluation of other 'output-related' NLP technologies, where there may be many potentially acceptable outputs, with no automatic way to compare them.	automatic summarization;elegant degradation;generic programming;information;intelligibility (philosophy);natural language processing;relevance;tf–idf;vocabulary	Inderjeet Mani;Gary Klein;David House;Lynette Hirschman;Therese Firmin;Beth Sundheim	2002	Natural Language Engineering	10.1017/S1351324901002741	natural language processing;multi-document summarization;computer science;data science;automatic summarization;data mining;statistical significance;tf–idf;information retrieval	Web+IR	-28.511767013148543	-68.60408620367453	31118
38abf9285ce45839cb17c14ad22ce9087fead1a2	electrodermal activity applied to violent scenes impact measurement and user profiling	motion pictures physiology context sensitivity correlation skin;violent scenes;motion pictures;sensors;video signal processing;video signal processing sensors;physiological signals violent scenes affective computing film annotation;skin;physiological signals;film annotation;sensitivity;physiology;correlation;context;movie electrodermal activity user profiling violent scene identification violent excerpt extraction audiovisual cue human centered manner commercial sensor violent scene impact measurement violent scene detection problem manual annotations user sensitivity;affective computing	Identifying violent scenes in a movie may be of high interest as soon as the associated content has to be shown to a specific audience, as children for instance. However, defining what a violent scene is as well as extracting the violent excerpts from the only audiovisual cue are two hard tasks. In this article, we propose a pilot study to evaluate the interest of an approach based on the use of the electrodermal activity (EDA) to address these problems in an objective and human-centered manner. Assuming a consumer context, we especially focus on the use of a commercial sensor to capture the EDA. A comparison with a more professional device is initially performed to validate the accuracy of the commercial sensor. Two main aspects of the violent scene impact measurement with the EDA are then discussed. To tackle the violent scene detection problem, a methodology to correlate the EDA information with manual annotations of violent excerpts is first proposed. The user sensitivity to violence is then addressed and an approach to identify different profiles in the audience is qualitatively proposed. The advantages and limitations of such an approach are discussed and potential improvements are finally proposed.		Julien Fleureau;Cédric Penet;Philippe Guillotel;Claire-Hélène Demarty	2012	2012 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/ICSMC.2012.6378302	computer vision;sensitivity;computer science;sensor;artificial intelligence;control theory;affective computing;multimedia;skin;correlation	Visualization	-16.55628922955884	-53.41303248968613	31122
4b45dcb163e331be96dc838d0eb5afa6388f0127	title-based approach to relation discovery from wikipedia.		With the advent of the Web and the explosion of available textual data, the field of domain ontology engineering has gained more and more importance. The last decade, several successful tools for automatically harvesting knowledge from web data have been developed, but the extraction of taxonomic and non taxonomic ontological relationships is still far from being fully solved. This paper describes a new approach which extracts ontological relations from Wikipedia. The non-taxonomic relations extraction process is performed by analyzing the titles which appear in each document of the studied corpus. This method is based on regular expressions which appear in titles and from which we can extract not only the two arguments of the relationships but also the labels which describe the relations. The resulting set of labels is used in order to retrieve new relations by analyzing the title hierarchy in each document. Other relations can be extracted from titles and subtitles containing only one term. An enrichment step is also applied by considering each term which appears as a relation argument of the extracted links in order to discover new concepts and new relations. The experiments have been performed on French Wikipedia articles related to the medical field. The precision and recall values are encouraging and seem to validate	experiment;gene ontology term enrichment;ontology (information science);ontology engineering;precision and recall;regular expression;taxonomy (general);text corpus;wikipedia;world wide web	Rim Zarrad;Narjes Doggaz;Ezzeddine Zagrouba	2013		10.5220/0004547400700080	data mining;world wide web;information retrieval	NLP	-29.852660442474832	-66.4898234888179	31136
2860424604a7a0a010014f62138a09c0aea31a02	a novel interactive object recognition approach for real-time-type video transmission	object recognition;video signal processing;image matching;real time;image classification;skin color;content analysis;image colour analysis;model matching;object recognition videoconference;multimedia communication;video transmission;visual features;real time systems object recognition interactive systems video signal processing image matching image colour analysis image classification knowledge based systems;information system;user interaction;interactive systems;knowledge based systems;vcpvs interactive object recognition real time type video transmission semantic content analysis video information system multimedia communities low level visual features user interaction stochastic skin color model skin color classification knowledge based approach dress region video character popularity voting system;real time systems;knowledge base	In recent years, semantic content analysis for the construction of video information systems has been a major research topic in the diverse multimedia communities. In this paper, an interactive object recognition approach for real-time-type (RTT) video transmission is proposed to generate semantic content with the video character as the basic unit based on low-level visual features. The basic idea is the model matching between character models and the dress features of the video character, which is specified by user interaction. A stochastic skin-color model is used for skin color classification and a knowledge-based approach is proposed to estimate the dress region of the specified character. Furthermore, a practical application, called video characters' popularity voting system (VCPVS), is constructed by applying the RTT object recognition approach and implemented to show the practicality of the proposed approach.	high- and low-level;information system;knowledge-based systems;outline of object recognition;real-time clock;real-time transcription	Xiaomeng Wu;Wenli Zhang;Tamon Sadasue;Shunsuke Kamijo;Masao Sakauchi	2005	First International Conference on Distributed Frameworks for Multimedia Applications	10.1109/DFMA.2005.6	computer vision;knowledge base;contextual image classification;content analysis;computer science;artificial intelligence;cognitive neuroscience of visual object recognition;video tracking;multimedia;information system	Vision	-13.21926959978289	-55.51226645428312	31144
0d9e54118cf596c7bfe809a2a2cbb0482d5c6fee	ecue: a spam filter that uses machine learning to track concept drift	concept drift;software engineering;artificial intelligent;text classification;electrical and computer engineering;spam filtering;machine learning;spam filter;technical report;computer science;ecue	While text classification has been identified for some time as a promising application area for Artificial Intelligence, so far few deployed applications have been described. In this paper we present a spam filtering system that uses example-based machine learning techniques to train a classifier from examples of spam and legitimate email. This approach has the advantage that it can personalise to the specifics of the user’s filtering preferences. This classifier can also automatically adjust over time to account for the changing nature of spam (and indeed changes in the profile of legitimate email). A significant software engineering challenge in developing this system was to ensure that it could interoperate with existing email systems to allow easy managment of the training data over time. This system has been deployed and evaluated over an extended period and the results of this evaluation are presented here.	anti-spam techniques;artificial intelligence;authentication;concept drift;defense in depth (computing);document classification;email filtering;filter (signal processing);interoperability;lazy evaluation;machine learning;naive bayes classifier;software deployment;software engineering;spamming;support vector machine	Sarah Jane Delany;Padraig Cunningham;Barry Smyth	2006			computer science;technical report;concept drift;bag-of-words model;machine learning;data mining;world wide web	AI	-18.738477677030776	-56.72823070892137	31224
cc8dbff2bf9085aad150749f189fc05f316ae1cf	text mining and temporal trend detection on the internet for technology assessment: model and tool		In today’s world, organizations conduct technology assessment (TAS) prior to decision making about investments in existing, emerging, and hot technolo gies to avoid costly mistakes and survive in the hyper-competitive business environment. Relying on web search engines in looking for relevant information for TAS processes, decision makers face abundant unstructured information that limit their ability to assess technologies within a reaso nable time frame. Thus the following question arise s: how to extract valuable TAS knowledge from a divers e corpus of textual data on the web? To cope with this question, this paper presents a web-based mo el and tool for knowledge mapping. The proposed knowledge maps are constructed on the basi s of a novel method of co-word analysis, based on webometric web counts and a temporal trend detec tion algorithm which employs the vector space model (VSM). The approach is demonstrated and valid ated for a spectrum of information technologies. Results show that the research model ass ssments are highly correlated with subjective expert (n=136) assessment (r > 0.91), and with pred ictive validity value above 85%. Thus, it seems safe to assume that this work can probably be gener aliz d to other domains. The model contribution is emphasized by the current growing attention to the big-data phenomenon.	algorithm;big data;cognitive map;concept map;decision support system;european conference on information systems;knowledge management;text corpus;text mining;thermal-assisted switching;viable system model;web application;web page;web search engine	Elan Sasson;Gilad Ravid;Nava Pliskin	2014			text mining;computer science;data science;data mining;information retrieval	Web+IR	-25.388783381405275	-56.88433785373194	31239
7032bb242727d62cd46f28013edcc8182d97d6b8	probabilistic semantic network-based image retrieval using mmm and relevance feedback	probabilistic semantic network;mmm mechanism;semantic network;content based image retrieval;relevance feedback;image retrieval	The performance of content-based image retrieval (CBIR) systems is largely limited by the gap between the low-level features and high-level semantic concepts. In this paper, a probabilistic semantic network-based image retrieval framework using relevance feedback is proposed to bridge this gap, which not only takes into consideration the low-level image content features, but also learns high-level concepts from a set of training data, such as access frequencies and access patterns of the images. One of the distinct properties of our framework is that it exploits the structured description of visual contents as well as the relative affinity measurements among the images. Consequently, it provides the capability to bridge the gap between the low-level features and high-level concepts. Moreover, such high-level concepts can be learned off-line, and can be utilized and refined based on the user’s specific interest during the on-line retrieval process. Our experimental results demonstrate that the proposed framework can effectively assist in retrieving more accurate results for user queries.	affinity analysis;content-based image retrieval;high- and low-level;iteration;markov chain;markov model;online and offline;radio frequency;relevance feedback;semantic network	Mei-Ling Shyu;Shu-Ching Chen;Min Chen;Chengcui Zhang;Chi-Min Shu	2006	Multimedia Tools and Applications	10.1007/s11042-006-0023-5	visual word;image retrieval;computer science;data mining;semantic network;world wide web;information retrieval;divergence-from-randomness model	Vision	-15.157662205135232	-59.604448097205115	31313
5654ed765a69ec06474b706ea0dfb5f014f89ac8	a memory-based approach to anti-spam filtering for mailing lists	liste diffusion;evaluation function;filtering;filtrage;spam;naive bayes;filtrado;informacion;spam filtering;machine learning;unsolicited commercial e mail;memory based learning;empirical evaluation;text categorization;information;mailing list	This paper presents an extensive empirical evaluation of memory-based learning in the context of anti-spam filtering, a novel cost-sensitive application of text categorization that attempts to identify automatically unsolicited commercial messages that flood mailboxes. Focusing on anti-spam filtering for mailing lists, a thorough investigation of the effectiveness of a memory-based anti-spam filter is performed using a publicly available corpus. The investigation includes different attribute and distance-weighting schemes, and studies on the effect of the neighborhood size, the size of the attribute set, and the size of the training corpus. Three different cost scenarios are identified, and suitable cost-sensitive evaluation functions are employed. We conclude that memory-based anti-spam filtering for mailing lists is practically feasible, especially when combined with additional safety nets. Compared to a previously tested Naive Bayes filter, the memory-based filter performs on average better, particularly when the misclassification cost for non-spam messages is high.	anti-spam techniques;archive;attribute grammar;blocking (computing);categorization;cobham's thesis;database normalization;document classification;email filtering;evaluation function;experiment;google moderator;instance-based learning;kullback–leibler divergence;naive bayes classifier;preprocessor;probabilistic automaton;spamming;stacking;text corpus;tf–idf;usenet	Georgios Sakkis;Ion Androutsopoulos;Georgios Paliouras;Vangelis Karkaletsis;Constantine D. Spyropoulos;Panagiotis Stamatopoulos	2003	Information Retrieval	10.1023/A:1022948414856	filter;spam;naive bayes classifier;information;computer science;machine learning;evaluation function;data mining;world wide web;information retrieval	Web+IR	-20.41028088555195	-61.125900674015675	31359
862bfb5a02943539ab2c6d3a7f5ebb817cfb217e	document overlap detection system for distributed digital libraries	distributed system;digital library;overlap detection;suffix tree;string matching	In this paper we introduce the MatchDetectReveal(MDR) system, which is capable of identifying overlapping and plagiarised documents. Each component of the system is briefly described. The matching-engine component uses a modified suffix tree representation, which is able to identify the exact overlapping chunks and its performance is also presented.	digital library;library (computing);suffix tree	Krisztián Monostori;Arkady B. Zaslavsky;Heinz W. Schmidt	2000		10.1145/336597.336667	computer science;theoretical computer science;compressed suffix array;world wide web;information retrieval	HPC	-33.2215341323423	-60.20590789682441	31402
38376b1d128a02b6ca2c49f0fe4c1ecaa5e6929c	affective characterization of movie scenes based on multimedia content analysis and user's physiological emotional responses	content management;biomedical monitoring;audio signal processing;emotional video content affective characterization movie scenes multimedia content analysis user physiological emotional responses video clips physiological responses emotional arousal content based audio based features video based features feature extraction physiological signals audio video features multimedia features;motion pictures;video signal processing;signal analysis;audio video;emotion recognition;serveur institutionnel;emotion recognition and assessment;multimedia computing;physiological signals analysis;video indexing;affective personalization and characterization;video signal processing audio signal processing content management content based retrieval multimedia computing;archive institutionnelle;streaming media;feature extraction;open access;multimedia communication;multimedia indexing and retrieval;multimedia indexing;electromyography;archive ouverte unige;physiological signals analysis multimedia indexing and retrieval affective personalization and characterization emotion recognition and assessment affective computing;correlation;cybertheses;content based retrieval;institutional repository;physiological response;motion pictures layout indexing biomedical monitoring content based retrieval feature extraction multimedia computing signal analysis skin facial muscles;multimedia content analysis;affective computing	In this paper, we propose an approach for affective representation of movie scenes based on the emotions that are actually felt by spectators. Such a representation can be used for characterizing the emotional content of video clips for e.g. affective video indexing and retrieval, neuromarketing studies, etc. A dataset of 64 different scenes from eight movies was shown to eight participants. While watching these clips, their physiological responses were recorded. The participants were also asked to self-assess their felt emotional arousal and valence for each scene. In addition, content-based audio- and video-based features were extracted from the movie scenes in order to characterize each one. Degrees of arousal and valence were estimated by a linear combination of features from physiological signals, as well as by a linear combination of content-based features. We showed that a significant correlation exists between arousal/valence provided by the spectator's self-assessments, and affective grades obtained automatically from either physiological responses or from audio-video features. This demonstrates the ability of using multimedia features and physiological responses to predict the expected affect of the user in response to the emotional video content.		Mohammad Soleymani;Guillaume Chanel;Joep J. M. Kierkels;Thierry Pun	2008	2008 Tenth IEEE International Symposium on Multimedia	10.1109/ISM.2008.14	computer vision;audio signal processing;feature extraction;content management;computer science;affective computing;multimedia;correlation	Vision	-16.17153515568418	-53.18349887979987	31498
6b72812c298cf3985c67a5fefff6d125175439c5	lexit: a computational resource on italian argument structure	argument structure;subcategorization frames;computational linguistics	The aim of this paper is to introduce LexIt, a computational framework for the automatic acquisition and exploration of distributional information about Italian verbs, nouns and adjectives, freely available through a web interface at the address http://sesia.humnet.unipi.it/lexit. LexIt is the first large-scale resource for Italian in which subcategorization and semantic selection properties are characterized fully on distributional ground: in the paper we describe both the process of data extraction and the evaluation of the subcategorization frames extracted with LexIt.	computation;computational resource;user interface	Alessandro Lenci;Gabriella Lapesa;Giulia Bonansinga	2012			natural language processing;computer science;computational linguistics;linguistics;algorithm	NLP	-30.355134845138313	-75.47429201373096	31610
79e165199e91c09084ad0ce510ce2d234b27921b	video scene annotation based on web social activities	web community;weblog tools video scene annotation web social activities video content semantics web communities bulletin board system;multimedia computing internet;presses;transform coding;web service;data mining;web communities;pressing;multimedia computing;weblog;computer architecture;internet;web social activities;video content semantics;video scene annotation;video annotation;layout video sharing data mining content based retrieval image recognition automatic speech recognition mpeg 7 standard costs robustness pattern recognition;communities;bulletin board system;weblog tools;collaborative tagging;blogs	This article describes a mechanism to acquire the semantics of video content from the activities of Web communities that use a bulletin-board system and Weblog tools to discuss video scenes.	blog;digital video;folksonomy;hyperlink;hypermedia;java annotation;world wide web	Daisuke Yamamoto;Tomoki Masuda;Shigeki Ohira;Katashi Nagao	2008	IEEE MultiMedia	10.1109/MMUL.2008.67	web service;the internet;transform coding;computer science;bulletin board system;multimedia;pressing;internet privacy;law;world wide web	Vision	-15.082402713876332	-55.511190329732955	31615
82932c240cfd4a2970ae20d04e21b14414136461	automatic string replace by examples	search and replace;genetic programming	Search-and-replace is a text processing task which may be largely automated with regular expressions: the user must describe with a specific formal language the regions to be modified (search pattern) and the corresponding desired changes (replacement expression). Writing and tuning the required expressions requires high familiarity with the corresponding formalism and is typically a lengthy, error-prone process.  In this paper we propose a tool based on Genetic Programming (GP) for generating automatically both the search pattern and the replacement expression based only on examples. The user merely provides examples of the input text along with the desired output text and does not need any knowledge about the regular expression formalism nor about GP. We are not aware of any similar proposal. We experimentally evaluated our proposal on 4 different search-and-replace tasks operating on real-world datasets and found good results, which suggests that the approach may indeed be practically viable.	automatic programming;cognitive dimensions of notations;experiment;formal language;formal system;genetic programming;regular expression;semantics (computer science)	Andrea De Lorenzo;Eric Medvet;Alberto Bartoli	2013		10.1145/2463372.2463532	genetic programming;computer science;artificial intelligence;theoretical computer science;machine learning;data mining;algorithm	AI	-16.632973708091168	-79.51262594077295	31618
26c33bbe73f6ff185d783f9e5b3500ecdc9dd1b2	object matching on real-world problems		Object matching (also referred to as duplicate identification, record linkage, entity resolution or reference reconciliation) is a crucial task for data integration and data cleaning. The task is to detect multiple representations of the same real-world object. This is a challenging task particularly for objects that are highly heterogeneous and of limited data quality, e.g., regarding completeness and consistency of their descriptions. To gain a better overview about the current state of the art in object matching, we survey the existing frameworks and their evaluations. According to the defined criteria, we review various frameworks published in the literature. We characterize them in some detail and compare them with each other and with our own framework, FEVER. With FEVER we introduce a new generic and comprehensive framework for object matching and comparative object matching evaluation. FEVER offers numerous operators for constructing non-learning as well as learning-based match workflows. Moreover, FEVER allows match approaches to be automatically executed and evaluated under different parameter configurations. Therefore FEVER sets the platform for conducting a comparative evaluation on the relative effectiveness and efficiency of alternate match approaches. Despite the huge amount of recent research efforts on object matching there has not yet been such an evaluation. With FEVER we fill this gap and present an evaluation of existing implementations on challenging real-world match tasks. We use the FEVER framework to automatically execute the approaches and to find favourable parameter settings in a comparable way. We consider approaches both with and without using machine learning to find suitable parameterization and combination of similarity functions. In addition to approaches from the research community we also consider a state-of-the-art commercial object matching implementation. Our results indicate significant quality and efficiency differences between different approaches. We also find that some challenging matching tasks such as matching product offers from online shops are not sufficiently solved with conventional approaches based on the similarity of attribute values. Furthermore, this thesis addresses the product offer matching problem. Product of-	data quality;linkage (software);machine learning;matching (graph theory);online shopping;plasma cleaning	Hanna Köpcke	2014				Web+IR	-28.85245935690239	-64.64275800850932	31632
897cc9e9ad1846f24243cc445063a247f5e76221	design, construction and validation of an arabic-english conceptual interlingua for cross-lingual information retrieval		This paper describes the issues involved in extending a trans-lingual lexicon, the TextWise Conceptual Interlingua (CI), with Arabic terms. The Conceptual Interlingua is based on the Princeton English WordNet (Fellbaum, 1998). It is a central component in the cross-lingual information retrieval (CLIR) system CINDOR (Conceptual INterlingua for DOcument Retrieval). Arabic has a rich morphological system combining templatic and affixational paradigms for both inflectional and derivational morphology. This rich morphology poses a major challenge to the design and building of the Arabic CI and also its validation. This is because the available resources for Arabic, whether manually constructed bilingual lexicons or lexicons automatically derived from bilingual parallel corpora, exist at different levels of morphological representation. We describe here the issues and decisions made in the design and construction of the Arabic-English CI using different types of manual and automatic resources. We also present the results of an extensive validation of the Arabic CI and briefly discuss the evaluation of its use for CLIR on the TREC Arabic Benchmark collection.	benchmark (computing);cross-language information retrieval;galaxy morphological classification;lexicon;mathematical morphology;parallel text;source-to-source compiler;text retrieval conference;text corpus;wordnet	Nizar Habash;Clinton Mah;Sabiha Imran;Randy Calistri-Yeh;Páraic Sheridan	2006			natural language processing;artificial intelligence;arabic;computer science;information retrieval;interlingua	NLP	-30.195605416846615	-74.88020322680126	31657
08977303dac8084ea14737b2b10cba0b6c90af1f	overcoming schema heterogeneity between linked semantic repositories to improve coreference resolution	schema matching;background knowledge;coreference resolution	Schema heterogeneity issues often represent an obstacle for discovering coreference links between individuals in semantic data repositories. In this paper we present an approach, which performs ontology schema matching in order to improve instance coreference resolution performance. A novel feature of the approach is its use of existing instancelevel coreference links defined in third-party repositories as background knowledge for schema matching techniques. In our tests of this approach we obtained encouraging results, in particular, a substantial increase in recall in comparison with existing sets of coreference links.	algorithm;experiment;ontology alignment;schema evolution	Andriy Nikolov;Victoria S. Uren;Enrico Motta;Anne N. De Roeck	2009		10.1007/978-3-642-10871-6_23	natural language processing;computer science;data mining;information retrieval	AI	-30.58314114424225	-66.50887144141267	31702
a2c951b48a4b9018bf321a8ba7f320c0418862bb	design and implementation of chinese historical text mining system based on culturomics		Culturomics and Chinese text mining methods are of great significance for analyzing the development and evolution of Chinese history and culture. To help researchers analyze a large number of Chinese historical text data, a Chinese historical text mining system based on cultruomics is designed, which includes text data processing and analyzing subsystem, text data visualizing subsystem, and text data clustering and retrieval subsystem. First of all, our system preprocesses the text data, then visualizes the text data with the frequency of words line chart and word cloud, at last selects the text data through clustering and retrieval methods. It further supports researchers to discover knowledge from a large number of historical text data. We demonstrate its general performance on text data of Canton Customs into our system. The result shows that our system is feasible and effective.	culturomics;text mining	Lin Tang;Chonghui Guo	2016		10.1007/978-981-10-2053-7_28	data science;data mining;database	NLP	-25.143684263452176	-57.21519430045409	31708
97c0dd7bff90e1f2e4b9bbc919433faa6ff092d3	utilizing annotated wikipedia article titles to improve a rule-based named entity recognizer for turkish		Named entity recognition is one of the information extraction tasks which aims to identify named entities such as person/ location/organization names along with some numeric and temporal expressions in free natural language texts. In this study, we target at named entity recognition from Turkish texts on which information extraction research is considerably rare compared to other well-studied languages. The effects of utilizing annotated Wikipedia article titles to enrich the lexical resources of a rule-based named entity recognizer for Turkish are discussed after evaluating the enriched named entity recognizer against its initial version. The evaluation results demonstrate that the presented extension improves the recognition performance on different text genres, particularly on historical and financial news text sets for which the initial recognizer has not been engineered for. The current study is significant as it is the first study to address the utilization of Wikipedia articles as an information source to improve named entity recognition on Turkish texts.	finite-state machine;named entity;wikipedia	Dilek Küçük	2013		10.1007/978-3-642-40769-7_59	natural language processing;speech recognition;computer science;entity linking;information retrieval	NLP	-27.484353334694415	-69.71231100941353	31720
668b1d615822e636c1938cf2870a27ebd1ee8570	extraction et analyse de l'impact émotionnel des images	emotions;psycho visual tests;colors;artificial neural network	This paper proposes a method to extract the emotional impact of images. Emotions are often associated with facial expressions, but we decided not to consider this feature as first emotional characteristic of natural images, which, in general, does not contain faces. Using this statement, our tests have been done on a new image database composed of low semantic diversified images. The complexity of emotion modeling was considered in classification process through psycho-visual tests. The twenty five observers assessed the nature and the power of emotions felt. We used an artificial neural network for classification. The average success rate is 56,15% ; that is really relevant regarding the equivalent results in the literrature.	artificial neural network;regular expression	Syntyche Gbèhounou;François Lecellier;Christine Fernandez-Maloigne	2012	Traitement du Signal	10.3166/ts.29.409-432	emotion;computer science;machine learning;artificial neural network	AI	-11.858975686721035	-70.01421798635228	31734
ab246d49e696f19b7a95f2f0f7fde1d9d4599208	a novel weight for recommendation: item quality	filtering;content based filtering;motion pictures;hybrid filtering approach;item quality;information filters information filtering;information filtering;bayesian methods;user feedback;frequency measurement;weight measurement;user profile;item quality recommendation weight collaborative filtering;weight;collaboration feedback information filtering information filters industrial training recommender systems proposals application software motion pictures information retrieval;chapters;collaborative filtering;user feedbacks item quality content based filtering collaborative filtering hybrid filtering approach;feature extraction;recommendation;information filters;user feedbacks	The many researches of recommendation technique already have been performed. These techniques were used mostly content-based filtering, collaborative filtering or hybrid filtering approach. They are performed the recommendation using userpsilas rating, user similarity, itempsilas features, user profiles. They consider several features of user and user profiles but they do not consider a quality of items itself. Because they donpsilat be able to efficiently define the quality definition of all item and they donpsilat be able to easily get user feedbacks. To solve the difficulty of the measurement of item quality and apply item quality as a weight to recommendation techniques, we measure item quality through popularity of item and user awareness. And we propose an approach to apply item quality to recommendation.	collaborative filtering;feedback;recommender system;user profile	Sunghoon Cho;Moo-hun Lee;Jeongseok Kim;Bong-hoi Kim;Eui-in Choi	2008	2008 International Conference on Computational Sciences and Its Applications	10.1109/ICCSA.2008.58	filter;feature extraction;bayesian probability;computer science;collaborative filtering;machine learning;multimedia;weight;world wide web;information retrieval	DB	-28.155992895114093	-52.534636291987226	31760
ac7f8ed5b02330b526f08dac225127d3d27941af	icarus: intelligent content-based retrieval of 3d scene	information retrieval;virtual reality;geometric reasoning;description logic;knowledge representation;content based retrieval	We present a tool for the analysis, classification and contentbased retrieval of 3D scene. The system ICARUS analyzes files in VRML format, searching for the presence of complex 3D-objects and relative geometrical relationships between them. Descriptions of the virtual scenes are classified in a Terminological System, and reasoning mechanisms are used for querying.	color;computer-aided design;event (computing);logical relations;prototype;statistical classification;vrml;yak (cryptography)	Raffaella Colaci;Marco Schaerf	2000			computer vision;description logic;computer science;artificial intelligence;data mining;virtual reality;information retrieval	AI	-12.921660955671474	-58.04543350597241	31770
89146152deeb50f9730cc452e8c34e6f02cd41b3	a probabilistic classification approach for lexical textual entailment	text categorization	The textual entailment task – determining if a given text entails a given hypothesis – provides an abstraction of applied semantic inference. This paper describes first a general generative probabilistic setting for textual entailment. We then focus on the sub-task of recognizing whether the lexical concepts present in the hypothesis are entailed from the text. This problem is recast as one of text categorization in which the classes are the vocabulary words. We make novel use of Naïve Bayes to model the problem in an entirely unsupervised fashion. Empirical tests suggest that the method is effective and compares favorably with state-of-the-art heuristic scoring approaches.	categorization;document classification;heuristic;lexical substitution;naive bayes classifier;textual entailment;unsupervised learning;vocabulary	Oren Glickman;Ido Dagan;Moshe Koppel	2005			natural language processing;text graph;textual entailment;computer science;machine learning;pattern recognition	AI	-20.803868221598655	-71.82172219492523	31854
994860512f7910ac4cd13903a1c01c39083e8763	a neurofuzzy approach to active learning based annotation propagation for 3d object databases	categories and subject descriptors according to acm ccs i 5 2 pattern recognition design methodology;active learning;object database	Most existing Content-based Information Retrieval (CBIR) systems using semantic annotation, either annotate all the objects in a database (full annotation) or a manually selected subset (partial annotation) in order to increase the system's performance. As databases become larger, the manual effort needed for full annotation becomes unaffordable. In this paper, a fully automatic framework for partial annotation and annotation propagation, applied to 3D content, is presented. A part of the available 3D objects is automatically selected for manually annotation, based on their 'information content'. For the non-annotated objects, the annotation is automatically propagated using a neurofuzzy model, which is trained during the manual annotation process and takes into account the information hidden into the already annotated objects. Experimental results show that the proposed method is effective, fast and robust to outliers. The framework can be seen as another step towards bridging the semantic gap between low-level geometric characteristics (content) and intuitive semantics (context).	software propagation	Michalis Lazaridis;Petros Daras	2008		10.2312/3DOR/3DOR08/049-056	minimum information required in the annotation of models;image retrieval;computer science;data mining;database;temporal annotation;automatic image annotation;information retrieval	Vision	-14.651748023759513	-59.54344833937283	31968
491190cac6ea738ccc0dea0dfa5d50c0c1bf0661	a color-action perceptual approach to the classification of animated movies	experimental tests;average precision;color perception;video indexing;action content;animated genre classification;genre classification;color properties	We address a particular case of video genre classification, namely the classification of animated movies. This task is achieved using two categories of content descriptors, temporal and color based, which are adapted to this particular content. Temporal descriptors, like rhythm or action, are quantifying the perception of the action content at different levels. Color descriptors are determined using color perception which is quantified in terms of statistics of color distribution, elementary hues, color properties (e.g. amount of light colors, cold colors, etc.) and color relationship. The potential of the proposed descriptors to the classification task has been proved through experimental tests conducted on more than 159 hours of video footage. Despite the high diversity of the video material, the proposed descriptors achieve an average precision and recall ratios up to 90% and 92%, respectively, and a global correct detection ratio up to 92%.	color vision;information retrieval;precision and recall;temporal logic	Bogdan Ionescu;Constantin Vertan;Patrick Lambert;Alexandre Benoit	2011		10.1145/1991996.1992006	computer vision;computer science;multimedia;color vision	Vision	-12.075004273456797	-70.06983176151124	31996
79cfad8f0078235a4b682ff6916f0dc7c39d2e0d	multimodal photo annotation and retrieval on a mobile phone	multimodal indexing;real time;mobile phone;digital image management;indexation;mobile search and retrieval;image retrieval	Mobile phones are becoming multimedia devices. It is common to observe users capturing photos and videos on their mobile phones on a regular basis. As the amount of digital multimedia content expands, it becomes increasingly difficult to find specific images in the device. In this paper, we present a multimodal and mobile image retrieval prototype named MAMI (Multimodal Automatic Mobile Indexing). It allows users to annotate, index and search for digital photos on their phones via speech or image input. Speech annotations can be added at the time of capturing photos or at a later time. Additional metadata such as location, user identification, date and time of capture is stored in the phone automatically. A key advantage of MAMI is tha it is implemented as a stand-alone application which runs in real-time on the phone. Therefore, users can search for photos in their personal archives without the need of connectivity to a server. In this paper, we compare multimodal and monomodal approaches for image retrieval and we propose a novel algorithm named the Multimodal Redundancy Reduction (MR2) Algorithm. In addition to describing in detail the proposed approaches, we present our experimental results and compare the retrieval accuracy of monomodal versus multimodal algorithms.	algorithm;archive;image retrieval;mobile phone;multimodal interaction;prototype;real-time transcription;server (computing)	Xavier Anguera Miró;Jiejun Xu;Nuria Oliver	2008		10.1145/1460096.1460127	computer vision;mobile search;image retrieval;computer science;pattern recognition;multimedia;world wide web;information retrieval	Web+IR	-16.16380374215483	-55.024927847036146	32000
1b8ff2e4919232600f6fea6d38099497e9d238f4	extracting the semantic content of web pages via repeated structures	hypermedia markup languages;semantic content extraction repeated structure segmentation semantic block detection html data web page semantics model;information retrieval;repeated structure semantic modeling web page;nickel semantics web pages feature extraction html data mining visualization;web sites hypermedia markup languages information retrieval;semantic modeling;web sites;repeated structure;web page	Web pages may carry semantics that are very important to the authors and the readers. Due to many reasons, the authors may insert contents that are irrelevant to the underlying semantics of the page to different positions of the page, such as advertizements, guide bars, links. As a result, it may not lead good effect by using all the data of a web page to model its semantics. In this paper, we propose a framework that can extract the real semantic content from web pages via repeated structures of the HTML data. Our algorithm first detect the real semantic blocks in web pages via repeated structure segmentation, then extracts the real semantic content of the pages from real semantic blocks.	algorithm;html;relevance;web page	Zheng He;Hangzai Luo;Jianping Fan;Xiao Qiao Liu	2013	2013 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)	10.1109/ICMEW.2013.6618450	semantic computing;static web page;web modeling;site map;semantic web rule language;data web;html;web design;semantic search;computer science;dynamic web page;semantic web;social semantic web;web page;semantic web stack;database;world wide web;website parse template;information retrieval;semantic analytics	DB	-29.054983675608756	-54.57753142770992	32051
afb484afb0ff94b50ca1445399c8a5aed3efe215	a set of handwriting families: style recognition	pattern classification handwriting recognition fuzzy logic;handwriting characterization;pertinent observations;handwriting recognition;style recognition;handwriting families;fuzzy logic;variability space handwriting families style recognition variability handwriting characterization text recognition process pertinent observations semantic aspect handwritten amounts fuzzy partition;handwriting recognition character recognition humans hidden markov models density measurement extraterrestrial measurements image analysis writing databases standardization;text recognition process;handwritten amounts;pattern classification;variability;variability space;semantic aspect;fuzzy partition	In this paper, we analyse the variability of handwritings. The aim is to determine what sort of observations gives a first degree of handwriting characterization before initiating a tezt recognition process. In the case of handwriting consisting of few words, such literal amounts on cheques, this first degree of characterization can be obtained for each word, independent of signification, by eztracting the measures of some pertinent observations. Outcomes of this characterization are, to a certain eztent, a distinction between significans which characterise the author and signification which is the semantic aspect. Based on an analysis of 980 diflerent handwritten amounts, it is shown that theae measures define a variability space of nonuniform density. A fizzy partition of the set of 9788 words of the database is proposed, which allows to regroup handwriting styles into a small number of specific families. *	handwriting recognition;literal (mathematical logic);relevance;spatial variability	Jean-Pierre Crettez	1995		10.1109/ICDAR.1995.599041	fuzzy logic;natural language processing;speech recognition;computer science;machine learning;pattern recognition;handwriting recognition	Vision	-15.262866314693706	-78.6883147114556	32053
860be66e3d514984fa0ec85121af7b82759aee61	a study on dependency tree kernels for automatic extraction of protein-protein interaction	protein-protein interaction;phrase structure tree;dependency tree structure;extraction task;extended dependency tree;dependency tree kernel;effective approach;various relation extraction;effective technique;automatic extraction;proposed dt structure;new dependency tree;higher accuracy	Kernel methods are considered the most effective techniques for various relation extraction (RE) tasks as they provide higher accuracy than other approaches. In this paper, we introduce new dependency tree (DT) kernels for RE by improving on previously proposed dependency tree structures. These are further enhanced to design more effective approaches that we call mildly extended dependency tree (MEDT) kernels. The empirical results on the protein-protein interaction (PPI) extraction task on the AIMed corpus show that tree kernels based on our proposed DT structures achieve higher accuracy than previously proposed DT and phrase structure tree (PST) kernels.	kernel (operating system);kernel method;overfitting;phrase structure rules;pixel density;planar separator theorem;program structure tree;relationship extraction	Faisal Md. Chowdhury;Alberto Lavelli;Alessandro Moschitti	2011			computer science;machine learning;pattern recognition;data mining;tree kernel	NLP	-20.56205792950873	-74.5213801239774	32066
f06016efa442b29715616d9b99fcfbbfd65ed315	clustering terms in the bayesian network retrieval model: a new approach with two term-layers	bayesian network;term clustering;learning;information retrieval system;information retrieval model;learning methods;retrieval model;information retrieval models;bayesian networks	The retrieval performance of an information retrieval system usually increases when it uses the relationships among the terms contained in a given document collection. However, this creates the problem of how to obtain these relationships efficiently, and how to then use them to retrieve documents given a user’s query. This paper presents a new retrieval model based on a Bayesian network that represents and exploits term relationships, overcoming these two drawbacks. An efficient learning method to capture these relationships, based on term clustering, as well as their use for retrieval purposes, is also shown. © 2004 Elsevier B.V. All rights reserved.	archive;bayesian network;cluster analysis;information retrieval	Luis M. de Campos;Juan M. Fernández-Luna;Juan F. Huete	2004	Appl. Soft Comput.	10.1016/j.asoc.2003.11.003	document retrieval;query expansion;relevance;document clustering;cognitive models of information retrieval;standard boolean model;computer science;machine learning;bayesian network;data mining;adversarial information retrieval;okapi bm25;term discrimination;vector space model;data retrieval;information retrieval;human–computer information retrieval;divergence-from-randomness model	Web+IR	-30.169117110721693	-59.06960043407371	32094
34418d0fcd71cf6db21f5d11a11c1e9fd4e4638b	query-independent evidence in home page finding	graph theory;keywords citation and link analysis;search engines;information retrieval;journal article;query languages;link analysis;web information retrieval;online searching citation and link analysis;document contents;web search;connectivity;data storage equipment;anchor text;websites;citation and link analysis;optimality condition;test collection	Hyperlink recommendation evidence, that is, evidence based on the structure of a web's link graph, is widely exploited by commercial Web search systems. However there is little published work to support its popularity. Another form of query-independent evidence, URL-type, has been shown to be beneficial on a home page finding task. We compared the usefulness of these types of evidence on the home page finding task, combined with both content and anchor text baselines. Our experiments made use of five query sets spanning three corpora---one enterprise crawl, and the WT10g and VLC2 Web test collections.We found that, in optimal conditions, all of the query-independent methods studied (in-degree, URL-type, and two variants of PageRank) offered a better than random improvement on a content-only baseline. However, only URL-type offered a better than random improvement on an anchor text baseline. In realistic settings, for either baseline, only URL-type offered consistent gains. In combination with URL-type the anchor text baseline was more useful for finding popular home pages, but URL-type with content was more useful for finding randomly selected home pages. We conclude that a general home page finding system should combine evidence from document content, anchor text, and URL-type classification.	anchor text;baseline (configuration management);bigraph;directed graph;experiment;file spanning;home page;hyperlink;pagerank;randomness;text corpus;web search engine	Trystan Upstill;Nick Craswell;David Hawking	2003	ACM Trans. Inf. Syst.	10.1145/858476.858479	link analysis;anchor text;computer science;connectivity;graph theory;data mining;database;world wide web;information retrieval;query language	Web+IR	-32.55282312770392	-54.31456292080967	32123
0175b11c0588d03c113e801b5818fecc00160c5c	transfer learning via feature isomorphism discovery		Transfer learning has gained increasing attention due to the inferior performance of machine learning algorithms with insufficient training data. Most of the previous homogeneous or heterogeneous transfer learning works aim to learn a mapping function between feature spaces based on the inherent correspondence across the source and target domains or labeled instances. However, in many real world applications, existing methods may not be robust when the correspondence across domains is noisy or labeled instances are not representative. In this paper, we develop a novel transfer learning framework called Transfer Learning via Feature Isomorphism Discovery (abbreviated to TLFid), which owns high tolerance for noisy correspondence between domains as well as scarce or non-existing labeled instances. More specifically, we propose a feature isomorphism approach to discovering common substructures across feature spaces and learning a feature mapping function from the target domain to the source domain. We evaluate the performance of TLFid on the cross-lingual sentiment classification tasks. The results show that our method achieves significant improvement in terms of accuracy compared with the state-of-the-art methods.	algorithm;coefficient;computer vision;experiment;machine learning;mined;subgraph isomorphism problem	Shimin Di;Jingshu Peng;Yanyan Shen;Lei Chen	2018		10.1145/3219819.3220029	artificial intelligence;transfer of learning;computer science;machine learning;isomorphism;homogeneous;training set;subgraph isomorphism problem	ML	-16.4653633855223	-66.56646562014673	32136
67fd3b1c64c84745fb78a433f5ea975288c9586e	an evaluation of the web retrieval task at the third ntcir workshop	web retrieval	We have investigated the evaluation methods for measuring retrieval effectiveness of Web search engine systems, attempting to make them suitable for real Web environment. With this objective, we conducted ‘Web Retrieval Task’ at the Third NTCIR Workshop (‘NTCIR-3 WEB’) from 2001 to 2002 [1, 2, 3]. Using this NTCIR-3 WEB, we built a re-usable test collection that is suitable for evaluating Web search engine systems, and evaluated the retrieval effectiveness of a certain number of Web search engine systems. TREC Web Tracks [4] are well-known workshops that have an objective to research the retrieval of large-scale Web document data. Past TREC Web Tracks have used data sets extracted from ‘the Internet Archive’or pages gathered from the ‘.gov’ domain as document sets. They assessed the relevance only on information given in English. NTCIR-3 WEB was another workshop that has used 100-gigabyte and/or 10gigabyte document data that were mainly gathered from the ‘.jp’ domain. Relevance judgment was performed on the retrieved documents that are written in Japanese or English, partially considering hyperlinks. By considering the hyperlinks, a ‘hub page’ that gives out-links to multiple ‘authority pages’ [5] may be judged as relevant even if these do not include sufficient relevant information in them. 16 groups enrolled to participate in the NTCIR-3 WEB, and seven of these groups submitted run results.	hyperlink;relevance;text retrieval conference;usb hub;usability;web search engine;world wide web	Koji Eguchi;Keizo Oyama;Emi Ishida;Noriko Kando;Kazuko Kuriyama	2004	SIGIR Forum	10.1145/986278.986285	computer science	Web+IR	-31.82445256829799	-62.90530251685045	32158
d0f801613e06ab23166f3a0fb5e383b6a1fb66d8	extracting discriminative information from e-mail for spam detection inspired by immune system	libraries;electronic mail;biological immune system;training;construction industry;local concentration;term selection method;feature vector;10 fold cross validation;accuracy;discriminative information extraction;anti spam model;feature extraction;immune system;cross validation;unsolicited e mail feature extraction;10 fold cross validation discriminative information extraction e mail spam detection biological immune system local concentration feature extraction anti spam model term selection method;unsolicited e mail;bag of words;e mail;spam detection;sliding window;feature extraction electronic mail accuracy pathogens training construction industry libraries;pathogens	Inspired from Biological Immune System, we propose a local concentration based feature extraction (LC) approach for anti-spam. A general anti-spam model is built to incorporate the LC approach with term selection methods and classifiers. In the LC model, each message is divided into areas by a sliding window. At each area, a two-dimensional feature is constructed by calculating the concentrations of spam and legitimate email. Then all the features of each area are combined together as a whole feature vector. Several experiments are conducted on four benchmark corpora, by using 10-fold cross-validation. It is shown that the LC approach can extract the effective position correlated information from messages. Compared to the prevalent Bag-of-Words approach, the LC has better performance in terms of both accuracy and F1 measure. Most significantly, the LC approach can reduce feature dimensionality greatly and has much faster speed.	anti-spam techniques;bag-of-words model in computer vision;benchmark (computing);cross-validation (statistics);email;experiment;f1 score;feature extraction;feature vector;microsoft windows;spamming;text corpus	Yuanchun Zhu;Ying Tan	2010	IEEE Congress on Evolutionary Computation	10.1109/CEC.2010.5586290	sliding window protocol;immune system;feature vector;feature extraction;computer science;bag-of-words model;machine learning;pattern recognition;data mining;accuracy and precision;world wide web;cross-validation	SE	-19.754081065094574	-56.729884592317774	32159
b197adf1cf0102e24396af2459364a7c26ac8586	qursim: a corpus for evaluation of relatedness in short texts		 Collected a dataset of nearly 8,000 pairs of related Quranic verses from scholarly sources (i.e., Tafsir Ibn Kathir)  The majority of related verse pairs require deep semantic analysis and domain specific world knowledge to relate  As the related pairs preserve unique Chapter and Verse numbers, the dataset could be automatically generated for many translated languages. Dataset compilation process	commonsense knowledge (artificial intelligence);compiler;text corpus;verse protocol	Abdul-Baquee M. Sharaf;Eric Atwell	2012			machine translation;domain knowledge;artificial intelligence;natural language processing;corpus linguistics;computer science;commutative property;paraphrase	NLP	-28.119522573125092	-73.57066886126631	32218
c1efd29ddb6cb5cf82151ab25fbfc99e20354d9e	linking glove with word2vec		The Global Vectors for word representation (GloVe), introduced by Jeffrey Pennington et al. [3] is reported to be an efficient and effective method for learning vector representations of words. State-of-the-art performance is also provided by skip-gram with negative-sampling (SGNS) [2] implemented in the word2vec tool. In this note, we explain the similarities between the training objectives of the two models, and show that the objective of SGNS is similar to the objective of a specialized form of GloVe, though their cost functions are defined differently.	effective method;n-gram;sampling (signal processing);skip list;word2vec	Tianze Shi;Zhiyuan Liu	2014	CoRR		simulation;artificial intelligence	NLP	-20.613948436558786	-72.79804836903199	32227
a53c86a88315516740664924c1e47567f5bca4b8	overview of the ialp 2016 shared task on dimensional sentiment analysis for chinese words	training;testing;information services;internet;sentiment analysis;electronic publishing;knowledge based systems	This paper presents the IALP 2016 shared task on Dimensional Sentiment Analysis for Chinese Words (DSAW) which seeks to identify a real-value sentiment score of Chinese words in the both valence and arousal dimensions. Valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and arousal represents the degree of excitement and calm. Of the 22 teams registered for this shared task for two-dimensional sentiment analysis, 16 submitted results. We expected that this evaluation campaign could produce more advanced dimensional sentiment analysis techniques, especially for Chinese affective computing. All data sets with gold standards and scoring script are made publicly available to researchers.	affective computing;sentiment analysis;web standards	Liang-Chih Yu;Lung-Hao Lee;Kam-Fai Wong	2016	2016 International Conference on Asian Language Processing (IALP)	10.1109/IALP.2016.7875957	natural language processing;the internet;computer science;data mining;software testing;electronic publishing;world wide web;information system;sentiment analysis	NLP	-22.900351821632903	-66.6763868159272	32278
7e106b444d1353e1264df9e7a34027b0a528e43a	towards building a knowledge base of monetary transactions from a news collection		We address the problem of extracting structured representations of economic events from a large corpus of news articles, using a combination of natural language processing and machine learning techniques. The developed techniques allow for semi-automatic population of a financial knowledge base, which, in turn, may be used to support a range of data mining and exploration tasks. The key challenge we face in this domain is that the same event is often reported multiple times, with varying correctness of details. We address this challenge by first collecting all information pertinent to a given event from the entire corpus, then considering all possible representations of the event, and finally, using a supervised learning method, to rank these representations by the associated confidence scores. A main innovative element of our approach is that it jointly extracts and stores all attributes of the event as a single representation (quintuple). Using a purpose-built test set we demonstrate that our supervised learning approach can achieve 25% improvement in F1-score over baseline methods that consider the earliest, the latest or the most frequent reporting of the event.	baseline (configuration management);correctness (computer science);data mining;f1 score;formal verification;knowledge base;machine learning;natural language processing;relevance;semiconductor industry;supervised learning;test set	Jan R. Benetka;Krisztian Balog;Kjetil Nørvåg	2017	2017 ACM/IEEE Joint Conference on Digital Libraries (JCDL)	10.1109/JCDL.2017.7991575	information retrieval;data mining;supervised learning;ontology (information science);correctness;scholarly communication;computer science;knowledge base;knowledge-based systems;test set;population	NLP	-24.99963659532749	-63.836594423734546	32328
f4a3f6cc3d961e698ec018826e06555cbba55b06	impact analysis of the person in topic event mining.				Fenghuan Li;Dequan Zheng;Tiejun Zhao	2014	Austr. J. Intelligent Information Processing Systems		data science;data mining;information retrieval	ML	-24.869990121105413	-60.648016042451026	32352
2bb484318853fd43c60d0f58ce79104ee5283483	a simulation model of glucose-insulin metabolism and implementation on osg	metabolic pathways;enzymes;chemical reactions;metabolic network simulation;osg	In this paper, we present the design and implementation of a stand-alone tool for metabolic simulation and its deployment on the Open Science Grid (OSG). The case study model of glucose-insulin metabolism aims at the development of a real-time monitoring system that can assist patients with diabetes to handle their blood glucose profile and maintain healthy diet habit. This system is able to integrate custom-built SBML models along with users' food intake information and produces the estimation of ATP, Glucose, and Insulin for the given duration using numerical analysis and simulation. The tool has also been generalized to take into consideration of temporal genomic information and be flexible for simulation of any given biochemical models. After implementation on OSG, the results have demonstrated the effectiveness of numerical optimization for model selection and the feasibility of the proposed tool for the given metabolic simulation. The ATP-glucose and glucose-insulin correlations revealed by this tool can be promising for a variety of different application cases.	automated theorem proving;bioinformatics;computer simulation;database;model selection;numerical analysis;real-time clock;requirement;runge–kutta methods;sbml;simulation;software deployment	Milad Ghiasi Rad;Aditya Immaneni;Megan McCabe;Massimiliano Pierobon;Juan Cui	2017	2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)	10.1109/BIBM.2017.8217939	artificial intelligence;model selection;grid;machine learning;computer science;data modeling;sbml;open science;insulin metabolism	Robotics	-7.354184164839921	-59.38955606907833	32353
3b42edad0848dc45aa194e82ffedbfd4cad3a1d5	vectorweavers at semeval-2016 task 10: from incremental meaning to semantic unit (phrase by phrase)		This paper describes an experimental approach to Detection of Minimal Semantic Units and their Meaning (DiMSUM), explored within the framework of SemEval’16 Task 10. The approach is primarily based on a combination of word embeddings and parserbased features, and employs unidirectional incremental computation of compositional embeddings for multiword expressions.	computation;dynamic problem (algorithms);incremental computing;word embedding	Andreas Scherbakov;Ekaterina Vylomova;Fei Liu;Timothy Baldwin	2016			natural language processing;machine learning;artificial intelligence;computer science;computation;semeval;expression (mathematics);phrase	NLP	-21.63052954219424	-72.78730212687978	32406
fb548cbd3085a8407f9a8e7b5195b49c4a6acc40	a document profile for improving information retrieval systems		In recent year the great popularity that enjoys mobile technologies has led most users to become consumers and producers of information on the network. Many studies speak about this phenomenon as an activity that is capable of doubling or tripling existing content on an annual basis. The huge amount of information makes the current user oriented systems, like retrieval systems, recommender systems and others, become less e cient, especially when users require speci c information and answers according to their needs and preferences. These facts make necessary to equip these systems with proper Natural Language technologies able to provide the information that users demand adapted to each context and content type. In this article, it is presented a study of some Natural Language Processing technologies that can be useful to facilitate the proper identi cation of documents according to the user needs. For this purpose, it is designed a document pro le that will be able to represent semantic meta-data extracted from documents. The research is basically focused on the study of di erent language technologies in order to support the creation this novel document pro le proposal from semantic perspectives.	converge;ecosystem;information retrieval;international standard serial number;language technology;natural language processing;period-doubling bifurcation;recommender system	Antonio Guillén;Yoan Gutiérrez-Vázquez;Rafael Muñoz	2017	Research in Computing Science		world wide web;the internet;metadata;government;computer science	Web+IR	-31.38886221529119	-56.66321990953165	32542
d219c57db914389af520251684d67e3ffa54dde5	open-set classification for automated genre identification	one class svm;classifier ensembles;automated genre identification	Automated Genre Identification (AGI) of web pages is a problem of increasing importance since web genre (e.g. blog, news, eshops, etc.) information can enhance modern Information Retrieval (IR) systems. The state-of-the-art in this field considers AGI as a closed-set classification problem where a variety of web page representation and machine learning models have intensively studied. In this paper, we study AGI as an open-set classification problem which better formulates the real world conditions of exploiting AGI in practice. Focusing on the use of content information, different text representation methods (words and character n-grams) are tested. Moreover, two classification methods are examined, one-class SVM learners, used as a baseline, and an ensemble of classifiers based on random feature subspacing, originally proposed for author identification. It is demonstrated that very high precision can be achieved in open-set AGI while recall remains relatively high.	baseline (configuration management);blog;grams;information retrieval;machine learning;n-gram;null character;web page	Dimitrios A. Pritsos;Efstathios Stamatatos	2013		10.1007/978-3-642-36973-5_18	speech recognition;computer science;machine learning;pattern recognition;information retrieval	Web+IR	-20.872205401030975	-63.9457667304994	32543
90b0f3a337d11d14c2be30b68316d2d41a15c383	dynamic semantics for tense and aspect	situation calculus;dynamic semantics;contingent relation;dynamic logic;computational semantics;natural language;non-reified dynamic logic;recent claim;temporal category	"""A semantics for tense, modality, and aspect in natural language must capture causal and contingent relations between events and states as welt as merely temporal ones The paper investigates a non-reified dynamic logic based, formulation of the situation calculus is a formalism for a computational semantics for a. number of temporal categories in English and suggests that some recent claims that dynamic logics are inherently unsuitable for this purpose have taken too narrow a view of the situation calculi 1 T e m p o r a l O n t o l o g y The most important thing to observe about the temporal ontology implicit in natural language tense and aspect is that it IR not purely temporal To take a simple example the English perfect when predicated of an event like losing a watch says that some contextually retrievable consequences of the event in question hold at the time under discussion (Such consequences have sometimes been described under the heading of """"""""present relevance of the perfect ) As a result, conjoining such a perfect with a further clause denying those consequences is infelicitous (1) I have lost my watch (# but 1 have found it again) In this respect the perfect stands in contrast to the more purely temporal tenses, such as the past, which make no comparable claim about the consequences of the core event (2) Yesterday I lost my watrh (but 1 (have) found it again) It is because categories like the perfect are not purely temporal that it is usual to distinguish them from the tenses proper as ' aspects Another aspect whose meaning is not purelv temporal is the progressive or lmperfective The predication that it makes concerning the core event is a subtle one While the progressive clearly states that some event is ongoing at the time under discussion it is not necessarily the event that is actuallv mentioned Thus in a helow there seems to be a factive entailment about an event of wri t ing But in b, there 18 no such entailment concerning an event of writ ing a sonnet, for b is true even if the author was interrupted before he could complete the action Dowty [1979] named this rather surprising property of the progressive the imperfective paradox The imper fective paradox is a sign that we must distinguish various types or sorts of core event in natural language tempo ral ontology This system, which is described at greater length in [Steedman in press.], is briefly summarised as follows There are two key insights into this system which most theories either build upon or are forced to reinvent I he first concerns the temporal ontolog) itself and is usually attributed to Vendler [1967] though there are precedents in work by Jespersen Kenny and many earlier author itjes including Aristotle Vendler s taxonom) was importantly refined bv \erkuyl and Dowty, and ha.s been further extended by manv others Such taxonomies typically distinguish 'states from events' and divide the latter into a number of wr ts or types \endler dislin guished 'activities , (events which have duration but don t change state like heat* writing), achievements (events which have no duration but do chang* stat< like h eats amvmg), and accomplishments (which hav* duration and change state like heats writing a sonnet) Many authors have proposed recursive sort hierarchies Moens [1987, 1988] explained the aspectual sort hierarchy and possible coercions among Akiionsaricn m terms of a structure of the kind represented schematically in figure 1, representing an association in memory or the knowledge representation of all events with characteristic preparations and consequents, an idea that has 6ince been adopted in DR. Theory (Kamp and Reyle, [1993, p 557-570] Moens claimed Lhat the preparation is in Vendler's terms an activity, the consequent is a (perfect) state, and that the core event is an achievement Ihere is a great deal more to say about the status of these categories, but we wil l take it as read here, noting merely that we Bhall follow these authors m assuming that accomplishments like writing a sonnet are composites of an activity of writing and a culminating achievement of 1292 NATURAL LANGUAGE"""	causal filter;computational semantics;contingency (philosophy);course (navigation);denotational semantics;interrupt;knowledge representation and reasoning;mark steedman;modality (human–computer interaction);moravec's paradox;natural language;programming language;recursion;relevance;semantics (computer science);situation calculus;temporal logic;theory	Mark Steedman	1995			natural language processing;mathematics;operational semantics;algorithm	AI	-6.7160307398558565	-76.90823212201207	32588
25252ee840e81a57b2b72bcf743d8168c86bad67	le petit prince in unl		The present paper addresses the process and the results of the interpretation of the integral text of “Le Petit Prince” (Little Prince), the famous novel by Antoine de Saint-Exupéry, from French into UNL. The original text comprised 1,684 interpretation units (15,513 words), which were sorted according to their similarity, from the shortest to the longest ones, and which were then projected into a UNL graph structure, composed of semantic directed binary relations linking nodes associated to the synsets of the corresponding original lexical items. The whole UNL-ization process was carried-out manually and the results have been used as the main resource in a natural language generation project involving already 27 languages. .	dormand–prince method;natural language generation;prince;synonym ring;universal networking language	Ronaldo Martins	2012			artificial intelligence;natural language processing;speech recognition;natural language generation;computer science;binary relation;graph;lexical item	AI	-29.063933971483383	-69.5607071187386	32665
8ddc81b4ce53de15ec3658234cb524fc664bd76d	expanded semantic graph representation for matching related information of interest across free text documents	content management;graph theory;electronic mail;information retrieval;directed attributed graph;semantics context ontologies electronic mail vectors syntactics law enforcement;semantics;text analysis;information of interest;relatedness measures;node attribute list;inference mechanisms;text analysis content management graph theory inference mechanisms information retrieval;graph matching;semantic information structure information of interest semantic graph graph matching directed attributed graph adjacency matrix node attribute list relatedness measures;vectors;syntactics;law enforcement;ontologies;adjacency matrix;semantic information structure;context;law enforcement investigator email dataset expanded semantic graph representation information of interest free text documents graph matching semantic structures poor match identification poor match elimination processing requirement reduction graph nodes graph edges inference dl safe rules abductive hypotheses syntactic patterns semantic content separation semantic context subgraph ground truthed subset;semantic graph	This research proposes an expanded semantic graph definition that serves as a basis for an expanded semantic graph representation and graph matching approach. This representation separates the content and context and adds a number of semantic structures that encapsulate inferred information. The expanded semantic graph approach facilitates finding additional matches, identifying and eliminating poor matches, and prioritizing matches based on how much new knowledge is provided. By focusing on information of interest, doing pre-processing, and reducing processing requirements, the approach is applicable to problems where related information of interest is sought across a massive body of free text documents. Key aspects of the approach include (1) expanding the nodes and edges through inference using DL-Safe rules, abductive hypotheses, and syntactic patterns, (2) separating semantic content into nodes and semantic context into edges, and (3) applying relatedness measures on a node, edge, and sub graph basis. Results from tests using a ground-truthed subset of a large dataset of law enforcement investigator emails illustrate the benefits of these approaches.	abductive reasoning;email;graph (abstract data type);matching (graph theory);preprocessor;requirement;semantic web	James R. Johnson;Anita Miller;Latifur Khan;Bhavani M. Thuraisingham	2012	2012 IEEE Sixth International Conference on Semantic Computing	10.1109/ICSC.2012.45	natural language processing;semantic similarity;semantic computing;content management;computer science;ontology;graph theory;machine learning;pattern recognition;abstract semantic graph;data mining;database;semantics;world wide web;graph database;information retrieval;adjacency matrix;matching	DB	-24.120911277785755	-62.53008053759094	32691
0270cbad20f8416df91d56629bcf3f963e898429	transformation from publications to diabetes ontology using topic-based assertion discovery		During the last decade, we have seen an explosive growth in the number of bio-medical publications. In this paper, we present an Assertion Discovery framework that aims to transform from PubMed publications (diabetes domain) to an ontology, called Diabetes Publication Ontology (DPO). The assertions in the DPO ontology were mapped and integrated with ones in existing diabetes ontologies. The Assertion Discovery framework consists of three main components: (i) Assertion Discovery, (ii) Assertion Alignment, and (iii) Assertion Integration. The proposed approach for ontology generation was based on Stanford CoreNLP for Natural Language Processing, OpenIE (Open Information Extraction) for relation extraction, LDA (Latent Dirichlet Allocation) for topic modeling, and OWL API for ontology generation on the Spark parallel engine. We presented a web-based application for searching diabetes publications as well as retrieving the assertions from the diabetes publications through the DPO ontology.	apache spark;british informatics olympiad;distributed computing;entity;information extraction;latent dirichlet allocation;linear discriminant analysis;natural language processing;ontology (information science);ontology learning;pubmed;relationship extraction;scalability;scientific literature;topic model;web ontology language;web application	Rohithkumar Nagulapati;Mayanka Chandrashekar;Yugyung Lee	2018	2018 IEEE International Conference on Healthcare Informatics Workshop (ICHI-W)	10.1109/ICHI-W.2018.00009	data mining;ontology;latent dirichlet allocation;information extraction;topic model;assertion;ontology (information science);computer science;relationship extraction	AI	-33.691477712872484	-67.5976548310223	32718
5da5952da2fceac1823393ee2e0bdde0e0a02d2b	from one tree to a forest: a unified solution for structured web data extraction	information structure;web data extraction;search engine;information extraction;vertical knowledge;site level information;false positive;structured data	Structured data, in the form of entities and associated attributes, has been a rich web resource for search engines and knowledge databases. To efficiently extract structured data from enormous websites in various verticals (e.g., books, restaurants), much research effort has been attracted, but most existing approaches either require considerable human effort or rely on strong features that lack of flexibility. We consider an ambitious scenario -- can we build a system that (1) is general enough to handle any vertical without re-implementation and (2) requires only one labeled example site from each vertical for training to automatically deal with other sites in the same vertical? In this paper, we propose a unified solution to demonstrate the feasibility of this scenario. Specifically, we design a set of weak but general features to characterize vertical knowledge (including attribute-specific semantics and inter-attribute layout relationships). Such features can be adopted in various verticals without redesign; meanwhile, they are weak enough to avoid overfitting of the learnt knowledge to seed sites. Given a new unseen site, the learnt knowledge is first applied to identify page-level candidate attribute values, while inevitably involve false positives. To remove noise, site-level information of the new site is then exploited to boost up the true values. The site-level information is derived in an unsupervised manner, without harm to the applicability of the solution. Promising experimental performance on 80 websites in 8 distinct verticals demonstrated the feasibility and flexibility of the proposed solution.	book;database;entity;interaction;overfitting;row (database);unsupervised learning;web resource;web search engine;wrapper (data mining)	Qiang Hao;Rui Cai;Yanwei Pang;Lei Zhang	2011		10.1145/2009916.2010020	natural language processing;simulation;type i and type ii errors;data model;computer science;machine learning;data mining;world wide web;information extraction;information retrieval;search engine	Web+IR	-24.228111789054797	-63.002839505705076	32748
48ef181956a748301dd40f3f97c690795a11c886	adaptive duplicate detection using learnable string similarity measures	distance function;duplicate detection;distance metric learning;data integrity;vector space;svm applications;distance metric;data cleaning;trained similarity measures;support vector machine;similarity measure;string edit distance;record linkage	The problem of identifying approximately duplicate records in databases is an essential step for data cleaning and data integration processes. Most existing approaches have relied on generic or manually tuned distance metrics for estimating the similarity of potential duplicates. In this paper, we present a framework for improving duplicate detection using trainable measures of textual similarity. We propose to employ learnable text distance functions for each database field, and show that such measures are capable of adapting to the specific notion of similarity that is appropriate for the field's domain. We present two learnable text similarity measures suitable for this task: an extended variant of learnable string edit distance, and a novel vector-space based measure that employs a Support Vector Machine (SVM) for training. Experimental results on a range of datasets show that our framework can improve duplicate detection accuracy over traditional techniques.	database;edit distance;plasma cleaning;string metric;support vector machine	Mikhail Bilenko;Raymond J. Mooney	2003		10.1145/956750.956759	metric;computer science;machine learning;pattern recognition;normalized compression distance;data mining;mathematics;jaro–winkler distance	ML	-29.694598441866923	-70.45696826919779	32771
2aa9925cf16009e5e49d7dffe4b56bfd6c3a8890	technicolor/inria/imperial college london at the mediaeval 2012 violent scene detection task		This paper presents the work done in Technicolor, INRIA and Imperial College London regarding the Affect Task at MediaEval 2012. This task aims at detecting violent shots in movies. Four different systems and a fusion of three of them are proposed in this paper.	sensor	Cédric Penet;Claire-Hélène Demarty;Mohammad Soleymani;Guillaume Gravier;Patrick Gros	2012			visual arts;simulation;engineering;cartography	NLP	-7.384864780593546	-69.29327960160798	32853
d383e987bc8d9a636dff24c34abe5496dd2c9595	rev: extracting entity relations from world wide web	average precision;relation extraction with verification;relation extraction;world wide web;extraction rules;entity relation extraction;minimum cover patterns	Quantities of valuable relation knowledge are contained in textual documents on the World Wide Web. However, those data are always organized in semi-structured text and cannot be used directly. We develop an automatic and effective approach to extract relations from World Wide Web, which just requires a few user specified seed instances as input. Those instances are used to generate extraction rules that in turn result in new instances. And in order to improve the reliability of results, an effective method is proposed to assess new extracted instances. This paper introduces the approach in details and the experimental results show that the approach achieves an average precision of 98.67% and can preferably complete the relation extraction task.	effective method;information retrieval;rev;relationship extraction;semi-supervised learning;semiconductor industry;structured text;world wide web	Chao Chen;Zhe Yang;Xin Lin	2012		10.1145/2184751.2184761	relationship extraction;computer science;data mining;database;world wide web;information retrieval	NLP	-27.992165408357113	-63.964789928351884	32878
e17b1677ef323d5bc5722f0ecfe7351ee3b61aa0	analysis of typical annotation problems in bilingual case grammar treebank construction		In recent years, the study of machine translation has made great progress. However there are still many things to do for machine translation to reach the semantic level. In this paper, case grammar’s features that could well describe the semantic relationships in sentences were concluded. 23 thousand annotation errors that occurred in treebank construction were analyzed. 13 typical problems were summarized and the corresponding revolutions were proposed. The application of case grammar may contribute a new way of thinking for machine translation.	treebank	Hongying Zan;Wanli Chen;Kunli Zhang;Yuxiang Jia	2015		10.1007/978-3-319-27194-1_53	natural language processing;treebank;linguistics;programming language	NLP	-30.761777562733588	-74.92364732532923	32917
582ad65f041c508cc5b0cc0f0edfa332590c23be	part of speech tagging with min-max modular neural networks	min max neural network;parallel learning;overlearning;part of speech tagging;modular neural network;thai corpus;pos tagging	A parts of speech (POS) tagging system using neural networks has been developed by Ma and colleagues. This system can tag unlearned data with a much higher accuracy than that of the Hidden Markov Model (HMM), which is the most popular method of POS tagging. It does so by learning a small Thai corpus on the order of 10,000 words that are ambiguous as to their POSs. However, the threelayer perceptron used in the system has slow convergence and low learning accuracy even on such a small amount of data. It is therefore difficult to improve accuracy by incrementing the epoch of learning or by increasing the amount of learning data. To solve this problem, the tagging system of this paper makes use of the min-max modular (M) neural network of Lu and colleagues. This new system learns faster and has a higher learning accuracy compared with the old one, by decomposing large, complicated POS tagging problems into many smaller, easier problems. Learning accuracy can be improved by using the same learning data and larger data sets can be learned, which results in a much higher tagging accuracy. © 2002 Wiley Periodicals, Inc. Syst Comp Jpn, 33(7): 3039, 2002; Published online in Wiley InterScience (www.interscience. wiley.com). DOI 10.1002/scj.1139	artificial neural network;digi-comp i;hidden markov model;john d. wiley;lu decomposition;markov chain;maxima and minima;modular neural network;part-of-speech tagging;perceptron;tag (metadata);text corpus	Qing Ma;Bao-Liang Lu;Hitoshi Isahara;Michinori Ichikawa	2002	Systems and Computers in Japan	10.1002/scj.1139	overlearning;speech recognition;computer science;artificial intelligence;machine learning;algorithm	ML	-17.981545795757135	-76.58985748783763	32932
127fa78e5b264d750daf2ba521e635f5dbe372f7	isti@trec microblog track 2012: real-time filtering through supervised learning		Our approach to the microblog filtering task is based on learning a relevance classifier from an initial training set of relevant and non relevant tweets, generated by using a simple retrieval method. The classifier is then retrained using the (simulated) user feedback collected during the training process, in order to improve its accuracy as the filtering process goes on. In the official runs the system scored low effectiveness values, suffering a strong imbalance toward recall.	real-time locating system;relevance;simulation;statistical classification;supervised learning;test set	Giacomo Berardi;Andrea Esuli;Diego Marcheggiani	2012			computer science;microblogging;filter (signal processing);supervised learning;information retrieval;data mining;machine learning;social media;artificial intelligence;classifier (linguistics);training set;recall	Vision	-20.23422028940708	-66.29078479333488	32937
593d1fbfdcb009e8c78dcfa0b91d9cab14cf1592	research of web data mining based on fuzzy logic and neural networks	web documents;pattern clustering;fuzzy reasoning;neural networks;neural nets;data mining fuzzy logic neural networks internet information retrieval fuzzy reasoning web search uncertainty web mining machine learning;training;pattern clustering data mining fuzzy logic internet neural nets;data mining;fuzzy reasoning web data mining web document classification web document clustering fuzzy logic neural networks;fuzzy logic;simulation experiment;web document clustering;artificial neural networks;internet;classification algorithms;clustering algorithms;web data mining;web search;web document classification;web search result clustering web data mining fuzzy logic neural network web document classification;algorithm design and analysis;web search result clustering;neural network	Web document classification and clustering are two crucial sections in Web data mining. The models, algorithms and simulation experiments for both Web document classification and clustering have been studied separately to support for the personalized services and to overcome the deficiencies and shortcomings of the same type’s algorithms in the paper. The Web document classification based on fuzzy reasoning with comprehensive weights and Web search result clustering based on fuzzy logic and neural networks are presented for Web data mining to obtain easily understood, robust and low-priced solutions by exploring the greatest possible extents of imprecision, uncertainty, fuzzy reasoning and partial correctness. The experiments have demonstrated that the established intelligent Web information mining system here makes Web document classification and clustering more accurate, more credible and more rapid than the exciting ones.	algorithm;cluster analysis;correctness (computer science);data mining;document classification;experiment;fuzzy logic;neural networks;personalization;simulation;world wide web	Limin Ren	2009	2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2009.344	fuzzy logic;algorithm design;web modeling;web query classification;the internet;document clustering;fuzzy clustering;computer science;machine learning;social semantic web;data mining;cluster analysis;web intelligence;information retrieval;artificial neural network	ML	-24.157961845142214	-57.5963413264853	32953
422c635e0a93dee29d50ca393d61267391e3a523	reading to learn: an investigation into language understanding		One of the most important methods by which human beings learn is by reading. While in its full generality, the reading task is still too difficult a capability to be implemented in a computer, significant (if partial) approaches to the task are now feasible. Our goal in this project was to study issues and develop solutions for this task by working with a reduced version of the problem, namely working with text written in a simplified version of English (a Controlled Language) rather than full natural language. Our experience and results reveal that even this reduced version of the task is still challenging, and we have uncovered several major insights into this challenge. We describe our work and analysis, present a synthesis and evaluation of our work, and make several recommendations for future work in this area. Our conclusion is that ultimately, to bridge the “knowledge gap”, a pipelined approach is inappropriate, and that to address the knowledge requirements for good language understanding an iterative (bootstrapped) approach is the most promising way forward.	controlled natural language;extensibility;grammatical evolution;hand coding;iterative method;knowledge integration;natural language processing;natural language understanding;pipeline (computing);question answering;requirement;text simplification	Peter Clark;Philip Harrison;John A. Thompson;Rick Wojcik;Tom Jenkins;David J. Israel	2007			bootstrapping;generality;computer science;machine learning;natural language;natural language processing;artificial intelligence	NLP	-26.63704707229987	-73.04712307918084	32993
8d2c6cefe876d62ca53a88f63ccb8e0eae464efb	classifier-based tense model for smt		Tense of one sentence can indicate the time when an event takes place. Therefore, it is very useful for natural language processing tasks such as Machine Translation (MT). However, the mapping of tense in MT is a very challenging problem as the usage of tenses varies from one language to another. Aiming at translating one language (source) which lacks overt tense markers into another language (target) whose tense information is easily recognized, we propose to use a classifier-based tense model to keep the main tense in target side consistent with the one in source side. Furthermore, we present a simple and effective way to help this model by expanding more phrase pairs with different tenses. Experimental results demonstrate our methods significantly improve translation accuracy. TITLE AND ABSTRACT IN ANOTHER LANGUAGE (CHINESE)	automatic differentiation;baseline (configuration management);evaluation of machine translation;natural language processing;satisfiability modulo theories;statistical classification	Zhengxian Gong;Min Zhang;Chew Lim Tan;Guodong Zhou	2012			natural language processing;computer science;linguistics	NLP	-24.69631488184671	-74.45960690624325	33012
b2695f497ba152189a5290a6574b8730a7c7ecea	extrinsic evaluation on automatic summarization tasks: testing affixality measurements for statistical word stemming	automatic summarization;cortex;morphological segmentation;affixality measurements;statistical stemming	This paper presents some experiments of evaluation of a statistical stemming algorithm based on morphological segmentation. The method estimates affixality of word fragments. It combines three indexes associated to possible cuts. This unsupervised and language-independent method has been easily adapted to generate an effective morphological stemmer. This stemmer has been coupled with Cortex, an automatic summarization system, in order to generate summaries in English, Spanish and French. Summaries have been evaluated using ROUGE. The results of this extrinsic evaluation show that our stemming algorithm outperforms several classical systems.	automatic summarization;stemming	Carlos-Francisco Méndez-Cruz;Juan-Manuel Torres-Moreno;Alfonso Medina Urrea;Gerardo Sierra	2012		10.1007/978-3-642-37798-3_5	natural language processing;speech recognition;computer science;automatic summarization;machine learning;pattern recognition;cortex	NLP	-24.417141224042307	-75.97981622959577	33020
850ca3a36b298bd35155350009561052e390c27d	improving learning in networked data by combining explicit and mined links	relational data;information sources;web pages;text mining;semi supervised learning;research paper;networked learning;difference set;similarity measure;relational learning	This paper is about using multiple types of information for classification of networked data in a semi-supervised setting: given a fully described network (nodes and edges) with known labels for some of the nodes, predict the labels of the remaining nodes. One method recently developed for doing such inference is a guilt-byassociation model. This method has been independently developed in two different settings–relational learning and semi-supervised learning. In relational learning, the setting assumes that the networked data has explicit links such as hyperlinks between webpages or citations between research papers. The semi-supervised setting assumes a corpus of non-relational data and creates links based on similarity measures between the instances. Both use only the known labels in the network to predict the remaining labels but use very different information sources. The thesis of this paper is that if we combine these two types of links, the resulting network will carry more information than either type of link by itself. We test this thesis on six benchmark data sets, using a within-network learning algorithm, where we show that we gain significant improvements in predictive performance by combining the links. We describe a principled way of combining multiple types of edges with different edge-weights and semantics using an objective graph measure called node-based assortativity. We investigate the use of this measure to combine text-mined links with explicit links and show that using our approach significantly improves performance of our classifier over naively combining these two types of links. Motivation Recent years have seen a lot of attention on classification with networked data in various domains and settings (e.g., (Cortes, Pregibon, & Volinsky 2001; Blum et al. 2004; Macskassy & Provost forthcoming; Wang & Zhang 2006)). Networked data is data, generally of the same type such as web-pages or text documents, that are connected via various explicit relations such as one paper citing another, hyperlinks between web-pages, or people calling each other. This paper concerns itself mainly with the problem of withinnetwork classification: given a partially labeled network (some nodes have been labeled), label the rest of the nodes in the network. There have been two separate thrusts of work in this area; one assumes that the data is already in the form of a network such as a web-site, a citation graph, or a calling graph (e.g., (Taskar, Segal, & Koller 2001; Cortes, Pregibon, & Volinsky 2001; Macskassy & Provost 2003)). The second Copyright c © 2007, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. area of work has not been cast as a network learning problem, but rather in the area of semi-supervised learning in a transductive setting (Blum & Chawla 2001; Joachims 2003; Zhu, Ghahramani, & Lafferty 2003; Blum et al. 2004; Wang & Zhang 2006). These works assume that you are given a corpus of instances (consisting of labeled and unlabeled instances) and need to first create the links (e.g., given a set of text documents, generate pairwise similarity scores and create a link between two documents if their similarity score is above a given threshold), and then apply withinnetwork classification on this created network. In both scenarios, as mentioned above, the assumption is that the final graph is fully specified (all nodes and edges are known), that the labels of some of the nodes are labeled and that the task is to predict the labels of the remaining nodes. In the work presented here we focus on the case where this is all that is used at classification time. We note that both thrusts of work in this area have independently developed near-identical algorithms to address this classification task: Guilt-by-association (or homophily-based) models with approximate inference techniques have been used in statistical relation learning (Macskassy & Provost forthcoming) and harmonic functions with exact inference have been used in the semi-supervised setting (Zhu, Ghahramani, & Lafferty 2003; Wang & Zhang 2006). These methods empirically produce near-identical results with respect to predictions of nodes. The main idea of this paper stems from the realization that the two existing approaches both ignore information that is readily available. The work in statistical relational learning has ignored local attributes altogether and focused on the univariate case where only the labels are used. Contrast this with the work in the semi-supervised work, where they have no relations and create links using only local attributes. The thesis of this paper is that augmenting an existing network (such as a web-site or citation-graph) with links mined from the local attributes ought to increase the information in the network and hence improve the performance of the network classifier. We will show that a naive augmentation, while generally better than either network alone, can be further improved by using objective scaling measures for how to combine the two types of networks. We will show our results on six benchmark data sets, where we augment an existing network by adding K edges from each entity to the Homophily in the context of this paper is the likelihood that a node of a specific class will link to another node of the same class.	2-satisfiability;approximation algorithm;artificial intelligence;assortativity;benchmark (computing);blum axioms;citation graph;hyperlink;image scaling;mined;oscillator representation;semi-supervised learning;semiconductor industry;sid meier's alpha centauri;statistical classification;statistical relational learning;supervised learning;tagged union;text corpus;wang and landau algorithm;wang tile	Sofus A. Macskassy	2007			text mining;relational database;computer science;artificial intelligence;data science;machine learning;web page;data mining;difference set	AI	-17.435859834437156	-64.53927564297207	33032
f0cebe68c9a90e6fd053ab5b58fc886a5be711d3	webrat: supporting agile knowledge retrieval through dynamic, incremental clustering and automatic labelling of web search result sets	search engine;labeling web search data visualization data mining search engines information retrieval displays interactive systems user interfaces proposals;meta information webrat agile knowledge retrieval interactive system incremental clustering automatic labelling web search result search result visualization thematic clustering query refinement search engines information retrieval infromation structure;query processing;search engines;meta information;information retrieval;real time;incremental clustering;environmental conditions;query refinement;search result visualization;data mining;thematic clustering;dynamic clustering;data visualisation;internet;data structures interactive systems internet query processing search engines data visualisation;interactive system;data structures;automatic labelling;agile knowledge retrieval;displays;infromation structure;data visualization;web search result;on the fly;web search;webrat;proposals;interactive systems;user interfaces;labeling	WebRat is an interactive system for visualising and refining and refining search result sets. Documents matching a query are dynamically clustered on the fly and visualised as a contour map of islands. Thematic clusters are built, analysed, and visualised in real time. Users can interactively explore the visualisation and refine queries by selecting from the keywords and clusters presented to them. WebRat does not rely on precalculated meta data. Instead, necessary information is directly extracted from query result representations provided by search engines, as for example ranked lists of document snippets. The system is language-independent and can easily be adapted to a number of data sources and visualisation modes. WebRat supports agile knowledge retrieval by transforming unstructured information input into a representation enriched with structure and meta information even when environmental conditions and user demands change frequently and rapidly.	agile software development;web search engine	Michael Granitzer;Wolfgang Kienreich;Vedran Sabol;Gisela Dösinger	2003		10.1109/ENABL.2003.1231426	computer science;data mining;database;world wide web;information retrieval;data visualization;search engine	Web+IR	-31.938585097500813	-56.85986061632381	33062
684be9e9bd41d148158c64ba811c08f66b58092a	learning to rank for information retrieval	data preprocessing;learning theory;information retrieval;learning to rank;loss function	Learning to rank for Information Retrieval (IR) is a task to automatically construct a ranking model using training data, such that the model can sort new objects according to their degrees of relevance, preference, or importance. Many IR problems are by nature ranking problems, and many IR technologies can be potentially enhanced by using learning-to-rank techniques. The objective of this tutorial is to give an introduction to this research direction. Specifically, the existing learning-to-rank algorithms are reviewed and categorized into three approaches: the pointwise, pairwise, and listwise approaches. The advantages and disadvantages with each approach are analyzed, and the relationships between the loss functions used in these approaches and IR evaluation measures are discussed. Then the empirical evaluations on typical learning-to-rank methods are shown, with the LETOR collection as a benchmark dataset, which seems to suggest that the listwise approach be the most effective one among all the approaches. After that, a statistical ranking theory is introduced, which can describe different learning-to-rank algorithms, and be used to analyze their query-level generalization abilities. At the end of the tutorial, we provide a summary and discuss potential future work on learning to rank.	information retrieval;learning to rank	Tie-Yan Liu	2011		10.1007/978-3-642-14267-3	ranking;computer science;data science;machine learning;learning theory;data mining;data pre-processing;ranking svm;information retrieval;learning to rank;statistics;loss function	Web+IR	-19.752024527450267	-62.408256344514605	33096
caa8a41d58e386c56f56d46bbe79df9cb1087338	ecnu at semeval-2017 task 1: leverage kernel-based traditional nlp features and neural networks to build a universal model for multilingual and cross-lingual semantic textual similarity		To model semantic similarity for multilingual and cross-lingual sentence pairs, we first translate foreign languages into English, and then build an efficient monolingual English system with multiple NLP features. Our system is further supported by deep learning models and our best run achieves the mean Pearson correlation 73.16% in primary track.	artificial neural network;deep learning;kernel (operating system);natural language processing;semantic similarity	Junfeng Tian;Zhiheng Zhou;Man Lan;Yuanbin Wu	2017		10.18653/v1/S17-2028	artificial neural network;leverage (finance);computer science;machine learning;kernel (linear algebra);artificial intelligence;natural language processing;semeval	NLP	-20.230606505888748	-71.93207446927558	33185
5b56009f4d090c98d14cdd00daa36dc527540e5a	low-level natural language technique for arabic text processing	natural language		natural language	Arafat Awajan	2001			natural language programming;information extraction;computer science;language technology;temporal annotation;universal networking language;natural language processing;text graph;language identification;artificial intelligence;text segmentation	NLP	-30.32659400621313	-77.43237520463417	33304
a75edf8124f5b52690c08ff35b0c7eb8355fe950	authentic emotion detection in real-time video	mimica;bayesian network;interfase usuario;base donnee;learning algorithm;human computer interaction;medicion automatica;decision tree;emotional intelligence;facies;user interface;mimique;real time;decision bayes;database;base dato;hombre;automatic measurement;algorithme apprentissage;percepcion;mesure automatique;arbol decision;bayes decision;reseau bayes;senal video;paradigm;signal video;machine learning;emotion emotionality;red bayes;temps reel;human;bayes network;paradigme;video signal;tiempo real;emotion emotivite;interface utilisateur;emocion emotividad;perception;facial expression;paradigma;algoritmo aprendizaje;arbre decision;homme	There is a growing trend toward emotional intelligence in humancomputer interaction paradigms. In order to react appropriately to a human, the computer would need to have some perception of the emotional state of the human. We assert that the most informative channel for machine perception of emotions is through facial expressions in video. One current difficulty in evaluating automatic emotion detection is that there are currently no international databases which are based on authentic emotions. The current facial expression databases contain facial expressions which are not naturally linked to the emotional state of the test subject. Our contributions in this work are twofold: First, we create the first authentic facial expression database where the test subjects are showing the natural facial expressions based upon their emotional state. Second, we evaluate the several promising machine learning algorithms for emotion detection which include techniques such as Bayesian Networks, SVMs, and Decision trees.	algorithm;bayesian network;database;decision tree;emotion recognition;information;machine learning;machine perception;norm (social);programming paradigm;real-time transcription;spontaneous order;support vector machine;tracking system	Yafei Sun;Nicu Sebe;Michael S. Lew;Theo Gevers	2004		10.1007/978-3-540-24837-8_10	computer vision;speech recognition;computer science;artificial intelligence;machine learning;bayesian network	AI	-7.135238203408115	-71.24881291982011	33317
aef77ed17e86eb4610d6f87d72b30161b534a43e	resolving api mentions in informal documents		Developer forums contain opinions and information related to the usage of APIs. API names in forum posts are often not explicitly linked to their official resources. Automatic linking of an API mention to its official resources can be challenging for various reasons, such as, name overloading. We present a technique, ANACE, to automatically resolve API mentions in the textual contents of forum posts. Given a database of APIs, we first detect all words in a forum post that are potential references to an API. We then use a combination of heuristics and machine learning to eliminate false positives and to link true positives to the actual APIs and their resources.	application programming interface;baseline (configuration management);function overloading;heuristic (computer science);machine learning;programming language;web search engine	Gias Uddin;Martin P. Robillard	2017	CoRR		world wide web;computer science;heuristics;data mining	NLP	-29.27970931084231	-55.88614647962465	33456
9fd679680068e764018013b21851a7daf447492e	semantic annotation architecture for accessible multimedia resources	content management;software;semantic annotation;subtitling task;resource manager;resource management;data management;ieee multimedia;multimedia resource;accessible content;data management semantic annotation subtitling audio description accessible content ontologies ieee multimedia software;multimedia systems;audio description task semantic annotation architecture multimedia resource multimedia content subtitling task;audio description task;internet;subtitling;ontologies;multimedia content;internet content management resource management;audio description;semantic annotation architecture	The paper discusses a platform that enables the semantic annotation of multimedia content when the subtitling and audio description tasks are being carried out.	audio description;faceted classification;ontology (information science);overhead (computing);semantic search;user interface;vocabulary	Fernando Paniagua Martín;Ángel García-Crespo;Ricardo Colomo Palacios;Belén Ruíz-Mezcua	2011	IEEE MultiMedia	10.1109/MMUL.2010.43	the internet;content management;computer science;ontology;resource management;multimedia;law;world wide web;information retrieval	Visualization	-16.395319072119207	-55.58447824256994	33463
04298e3715f6cb0a95cddb26b69f4b7835ce11a6	exploration of text collections with hierarchical feature maps	text classification;document classification	Document classification is one of the central issues in information retrieval research. The aim is to uncover similarities between text documents. In other words, classification techniques are used to gain insight in the structure of the various data items contained in the text archive. In this paper we show the results from using a hierarchy of self-organizing maps to perform the text classification task. Each of the individual self-organizing maps is trained independently and gets specialized to a subset of the input data. As a consequence, the choice of this particular artificial neural network model enables the tme establishment of a document taxonomy. The benefit of this approach is a straightforward representation of document similarities combkd with drantatically reduced training time. In particular, the hierarchical representation of document collections is appealing because it is the underlying organizational principle in use by librarians providing the necessary familiarity for the user. The massive reduction in the time needed to train the artificial neural network together with its highly accurate clustering remdts makea it a challenging alternative to conventional approaches.	archive;artificial neural network;cluster analysis;document classification;information retrieval;librarian;network model;organizing (structure);self-organization;self-organizing map;semantic similarity;statistical classification	Dieter Merkl	1997		10.1145/258525.258564	computer science;pattern recognition;data mining;information retrieval;library classification	Web+IR	-20.148225936181568	-63.51698315131482	33465
5856b9f7ecb92c6dbb8b6a3fcbc0c1a69994f1b0	intent-based categorization of search results using questions from web q&a corpus	intent based categorization;user intent detection;web search;information need	User intent is defined as a user's information need. Detecting intent in Web search helps users to obtain relevant content, thus improving their satisfaction. We propose a novel approach to instantiating intent by using adaptive categorization producing predicted intent probabilities. For this, we attempt to detect factors by which intent is formed, called intent features, by using a Web Q&A corpus. Our approach was motivated by the observation that questions related to queries are effective for finding intent features. We extract set of categories and their intent features automatically by analyzing questions within Web Q&A corpus, and categorize search results using these features. The advantages of our intent-based categorization are twofold, (1) presenting the most probable intent categories to help users clarify and choose starting points for Web searches, and (2) adapting sets of intent categories for each query. Experimental results show that distilled intent features can efficiently describe intent categories, and search results can be efficiently categorized without any human supervision.	categorization	Soungwoong Yoon;Adam Jatowt;Katsumi Tanaka	2009		10.1007/978-3-642-04409-0_19	information needs;semantic search;computer science;pattern recognition;data mining;world wide web	NLP	-27.669752452720648	-54.4852502358809	33471
44e0a7d4f3039e6eeef8873362c5ab0bd6ef235c	recurrent neural networks with limited numerical precision		Recurrent Neural Networks (RNNs) produce state-of-art performance on many machine learning tasks but their demand on resources in terms of memory and computational power are often high. Therefore, there is a great interest in optimizing the computations performed with these models especially when considering development of specialized low-power hardware for deep networks. One way of reducing the computational needs is to limit the numerical precision of the network weights and biases. This has led to different proposed rounding methods which have been applied so far to only Convolutional Neural Networks and Fully-Connected Networks. This paper addresses the question of how to best reduce weight precision during training in the case of RNNs. We present results from the use of different stochastic and deterministic reduced precision training methods applied to three major RNN types which are then tested on several datasets. The results show that the weight binarization methods do not work with the RNNs. However, the stochastic and deterministic ternarization, and pow2-ternarization methods gave rise to low-precision RNNs that produce similar and even higher accuracy on certain datasets therefore providing a path towards training more efficient implementations of RNNs in specialized hardware.	baseline (configuration management);binary image;computation;convolutional neural network;experiment;language model;like button;low-power broadcasting;machine learning;microsoft outlook for mac;network performance;neural networks;numerical analysis;overfitting;quantization (signal processing);random neural network;recurrent neural network;requirement;rounding;speech recognition;time complexity	Joachim Ott;Zhouhan Lin;Shih-Chii Liu;Yoshua Bengio	2016	CoRR		computer science;artificial intelligence;theoretical computer science;machine learning	NLP	-15.683669902597961	-76.15795046511411	33473
69cab8382666eb66c56084d634b946548021c2cc	self-consistency as an inductive bias in early language acquisition		In this paper we introduce an inductive bias for language acquisition under a view where learning of the various levels of linguistic structure takes place interactively. The bias encourages the learner to choose sound systems that lead to more “semantically coherent” lexicons. We quantify this coherence using an intrinsic and unsupervised measure of predictiveness called “self-consistency.”We found self-consistency to be optimal under the true phonemic inventory and the correct word segmentation in English and Japanese.	algorithm;bayesian network;coherence (physics);inductive bias;interactivity;inventory;lexicon;text segmentation;unsupervised learning	Abdellah Fourtassi;Ewan Dunbar;Emmanuel Dupoux	2014			language acquisition;speech corpus;cognitive psychology;lexicon;phone;natural language processing;inductive bias;utterance;speech recognition;artificial intelligence;computer science;lexical item;text segmentation	NLP	-10.972745931791595	-77.74472340477034	33587
28105f29871aa490082f3d756961d41ab65d2d4c	low-resource named entity recognition with cross-lingual, character-level neural conditional random fields		Low-resource named entity recognition is still an open problem in NLP. Most stateof-the-art systems require tens of thousands of annotated sentences in order to obtain high performance. However, for most of the world’s languages it is unfeasible to obtain such annotation. In this paper, we present a transfer learning scheme, whereby we train character-level neural CRFs to predict named entities for both high-resource languages and low-resource languages jointly. Learning character representations for multiple related languages allows transfer among the languages, improving F1 by up to 9.8 points over a loglinear CRF baseline.	baseline (configuration management);conditional random field;experiment;named entity;natural language processing;sample complexity	Ryan Cotterell;Kevin Duh	2017			artificial intelligence;computer science;pattern recognition;conditional random field;named-entity recognition	NLP	-19.203524848757965	-73.20285910468395	33609
019ecabbec2fe7bb14a07f11a97cb5362e107a4a	real-time string filtering of large databases implemented via a combination of artificial neural networks	approximate string matching;real time;self organized map;intrusion detection system;artificial neural network	A novel approach to real-time string filtering of large databases is presented. The proposed approach is based on a combination of artificial neural networks and operates in two stages. The first stage employs a self-organizing map for performing approximate string matching and retrieving those strings of the database which are similar to (i.e. assigned to the same SOM node as) the query string. The second stage employs a harmony theory network for comparing the previously retrieved strings in parallel with the query string and determining whether an exact match exists. The experimental results demonstrate accurate, fast and database-size independent string filtering which is robust to database modifications. The proposed approach is put forward for general-purpose (directory, catalogue and glossary search) and Internet (e-mail blocking, intrusion detection systems, URL and username classification) applications.	artificial neural network;neural networks;real-time clock	Tatiana Tambouratzis	2007		10.1007/978-3-540-71629-7_20	intrusion detection system;string kernel;approximate string matching;commentz-walter algorithm;computer science;theoretical computer science;machine learning;boyer–moore string search algorithm;data mining;string metric;artificial neural network;string searching algorithm	ML	-18.715908605728117	-59.05733960868747	33616
afa7b50311e86b6faa0a4b420c76a687520333bc	chinese personal name disambiguation based on person modeling		This document presents the bakeoff results of Chinese personal name in the First CIPS-SIGHAN Joint Conference on Chinese Language Processing. The authors introduce the frame of person disambiguation system LJPD, which uses a new person model. LJPD was built in short time, and it is not given enough training and adjustment. Evaluation on LJPD shows that the precision is competitive, but the recall is very low. It has more space for further improvement.	word-sense disambiguation	Hua-Ping Zhang;Zhi-Hua Liu;Qian Mo;Heyan Huang	2010			internet privacy;personal name;psychology	NLP	-26.118159368259814	-67.66888007373292	33635
27a884cf5be609b52afedb78d6aaa8e7c93c51cb	early detection of topical expertise in community question answering	expertise finding;universiteitsbibliotheek;user profiling;community question answering	We focus on detecting potential topical experts in community question answering platforms early on in their lifecycle. We use a semi-supervised machine learning approach. We extract three types of feature: (i) textual, (ii) behavioral, and (iii) time-aware, which we use to predict whether a user will become an expert in the longterm. We compare our method to a machine learning method based on a state-of-the-art method in expertise retrieval. Results on data from Stack Overflow demonstrate the utility of adding behavioral and time-aware features to the baseline method with a net improvement in accuracy of 26% for very early detection of expertise.	baseline (configuration management);machine learning;question answering;semi-supervised learning;semiconductor industry;sensor;stack overflow;supervised learning	David van Dijk;Manos Tsagkias;Maarten de Rijke	2015		10.1145/2766462.2767840	computer science;data mining;world wide web;information retrieval	ML	-21.4339657962323	-57.8099965131903	33662
0d3424e4f3726f52e53060dc88d1bbb19c526303	persian/arabic baffletext captcha	completely automated public turing test to tell computers and human apart captcha;persian and arabic text;internet security.;optical character recognition ocr;baffletext	Nowadays, many daily human activities such as education, trade, talks, etc are done by using the Internet. In such things as registration on Internet web sites, hackers write programs to make automatic false registration that waste the resources of the web sites while it may also stop it from functioning. Therefore, human users should be distinguished from computer programs. To this end, this paper presents a method for distinction of Persian and Arabic-language users from computer programs based on Persian and Arabic texts. Our proposed algorithm is based on adding a background to the image of a meaningless Persian/Arabic randomly generated word. This method relies on the difficulty of automatic separation of background from Persian/Arabic writing, due to the presence of many diacritical dots and signs. In this method, the image of a random meaningless Persian or Arabic word is shown to the user and he is asked to type it. Considering that the presently available Persian and Arabic OCR programs cannot identify these words, the word can be identified only by a Persian or Arabiclanguage user. This method also can be used to prevent program attacks, resource waste and performance reduction. The proposed method has been implemented by the Java language. The generated words are tested, using ReadIris and Omnipage OCR systems. These OCR systems were unable to recognize these words.	algorithm;book;captcha;computer program;internet;java;omnipage;optical character recognition;procedural generation;separation kernel	Mohammad Hassan Shirali-Shahreza;Mohammad Shirali-Shahreza	2006	J. UCS	10.3217/jucs-012-12-1783	the internet;hacker;computer science;arabic;natural language processing;persian;artificial intelligence;captcha;java	AI	-5.862183014227947	-69.57293000596599	33698
3d40e2cd0769af50b028976d6efd9e03239e71b0	gaze movement-driven random forests for query clustering in automatic video annotation		In the recent years, the rapid increase of the volume of multimedia content has led to the development of several automatic annotation approaches. In parallel, the high availability of large amounts of user interaction data, revealed the need for developing automatic annotation techniques that exploit the implicit user feedback during interactive multimedia retrieval tasks. In this context, this paper proposes a method for automatic video annotation by exploiting implicit user feedback during interactive video retrieval, as this is expressed with gaze movements, mouse clicks and queries submitted to a content-based video search engine. We exploit this interaction data to represent video shots with feature vectors based on aggregated gaze movements. This information is used to train a classifier that can identify shots of interest for new users. Subsequently, we propose a framework that during testing: a) identifies topics (expressed by query clusters), for which new users are searching for, based on a novel clustering algorithm and b) associates multimedia data (i.e., video shots) to the identified topics using supervised classification. The novel clustering algorithm is based on random forests and is driven by two factors: first, by the distance measures between different sets of queries and second by the homogeneity of the shots viewed during each query cluster defined by the clustering procedure; this homogeneity is inferred from the performance of the gaze-based classifier on these shots. The evaluation shows that the use of aggregated gaze data can be exploited for video annotation purposes.	algorithm;cluster analysis;emoticon;feature vector;high availability;machine learning;random forest;statistical classification;supervised learning;web search engine	Stefanos Vrochidis;Ioannis Patras;Yiannis Kompatsiaris	2015	Multimedia Tools and Applications	10.1007/s11042-015-3221-1	computer vision;computer science;machine learning;multimedia;world wide web;information retrieval	Web+IR	-26.07766732160069	-54.22643019010931	33724
5ffb7edcf4dde804db33efcce8af85946ea754b6	extracting bibliographical data for pdf documents with hmm and external resources	information extraction;hidden markov model;pdf documents;bibliographical information	Purpose – The purpose of this paper is to propose an automatic metadata extraction and retrieval system to extract bibliographical information from digital academic documents in portable document formats (PDFs). Design/methodology/approach – The authors use PDFBox to extract text and font size information, a rule-based method to identify titles, and an Hidden Markov Model (HMM) to extract the titles and authors. Finally, the extracted titles and authors (possibly incorrect or incomplete) are sent as query strings to digital libraries (e.g. ACM, IEEE, CiteSeerX, SDOS, and Google Scholar) to retrieve the rest of metadata. Findings – Four experiments are conducted to examine the feasibility of the proposed system. The first experiment compares two different HMM models: multi-state model and one state model (the proposed model). The result shows that one state model can have a comparable performance with multi-state model, but is more suitable to deal with real-world unknown states. The second experiment show...	hidden markov model;portable document format	Wen-Feng Hsiao;Te-Min Chang;Thomas Erwin	2014	Program	10.1108/PROG-12-2011-0059	computer science;data science;data mining;information retrieval	NLP	-30.771638479753175	-64.89432424638451	33745
f2e7598464a0b9376771ffc4ba243233ee12c677	incorporating chinese characters of words for lexical sememe prediction		Sememes are minimum semantic units of concepts in human languages, such that each word sense is composed of one or multiple sememes. Words are usually manually annotated with their sememes by linguists, and form linguistic common-sense knowledge bases widely used in various NLP tasks. Recently, the lexical sememe prediction task has been introduced. It consists of automatically recommending sememes for words, which is expected to improve annotation efficiency and consistency. However, existing methods of lexical sememe prediction typically rely on the external context of words to represent the meaning, which usually fails to deal with low-frequency and out-of-vocabulary words. To address this issue for Chinese, we propose a novel framework to take advantage of both internal character information and external context information of words. We experiment on HowNet, a Chinese sememe knowledge base, and demonstrate that our framework outperforms state-of-the-art baselines by a large margin, and maintains a robust performance even for low-frequency words.	word lists by frequency	Huiming Jin;Hao Zhu;Zhiyuan Liu;Ruobing Xie;Maosong Sun;Fen Lin;Leyu Lin	2018			computer science;word sense;natural language processing;artificial intelligence;knowledge base;chinese characters;sememe;annotation	NLP	-24.337914998210614	-73.86094866502509	33813
2f4bbcdebbe6b0343775861802792c35ba41ee5f	thecerealkiller at semeval-2016 task 4: deep learning based system for classifying sentiment of tweets on two point scale		In this paper, we propose a deep learning system for classification of tweets on a two-point scale. Our architecture consists of a multilayered recurrent neural network having gated recurrent units. The network is pre-trained with a weakly labeled dataset of tweets to learn the sentiment specific embeddings. Then it is fine tuned on the given training dataset of the task 4B in SemEval-2016. The network does very little pre-processing for raw tweets and no post-processing at all. The proposed system achieves 3rd rank on the leaderboard of task 4B.	artificial neural network;deep learning;preprocessor;recurrent neural network;semeval;video post-processing	Vikrant Yadav	2016			artificial intelligence;machine learning;natural language processing;computer science;semeval;deep learning	ML	-19.264630706213275	-71.39903147311861	33836
a538a05864a23e2f80f9b003d5ecbdfb8025b954	usfd at semeval-2016 task 6: any-target stance detection on twitter with autoencoders		This paper describes the University of Sheffield’s submission to the SemEval 2016 Twitter Stance Detection weakly supervised task (SemEval 2016 Task 6, Subtask B). In stance detection, the goal is to classify the stance of a tweet towards a target as “favor”, “against”, or “none”. In Subtask B, the targets in the test data are different from the targets in the training data, thus rendering the task more challenging but also more realistic. To address the lack of target-specific training data, we use a large set of unlabelled tweets containing all targets and train a bag-of-words autoencoder to learn how to produce feature representations of tweets. These feature representations are then used to train a logistic regression classifier on labelled tweets, with additional features such as an indicator of whether the target is contained in the tweet. Our submitted run on the test data achieved an F1 of 0.3270.	autoencoder;bag-of-words model;logistic regression;semeval;test data;test set	Isabelle Augenstein;Andreas Vlachos;Kalina Bontcheva	2016			computer science;artificial intelligence;natural language processing;semeval	NLP	-21.719664772563966	-69.98927342077384	33854
9a68c04149137df268a77ce46bd29952f5d7363d	query-by-shape interface for content based image retrieval	manuals;web based application query by shape interface content based image retrieval content based image retrieval interface query stage query objects graph image features;shape recognition content based retrieval graph theory image retrieval internet multimedia databases;shape;image edge detection;image color analysis;bicycles;query by shape content based imag retrieval multimedia databases graphical query;shape image retrieval image color analysis bicycles image edge detection manuals;image retrieval	In this paper we describe Query-by-Shape, a new Content-Based Image Retrieval interface. In this method objects are decomposed into features, each feature may consist of a color, a texture or a shape attributes. The query stage consists of comparing the query objects graph with objects from the database. The main advantage of the proposed solution is the ability to query for an object without having full knowledge about it. The proposed interface allows creating graphically drawn queries using simple, predefined shapes. Moreover, the method is able to automatically detect image features and to create a graph used for the query. Because users sometimes would like to get results representing not only exactly the same class of objects, but also similar objects, thus as a result a set of objects, ordered according to the similarity to the query, are returned. The method was implemented as a prototype web-based application. The initial, experimental results proved that the method is very promising, it identifies objects with a high precision.	color;content-based image retrieval;graph (discrete mathematics);prototype;texture mapping;web application	Stanislaw Deniziak;Tomasz Michno	2015	2015 8th International Conference on Human System Interaction (HSI)	10.1109/HSI.2015.7170652	image texture;sargable;computer vision;query optimization;query expansion;web query classification;visual word;ranking;image retrieval;computer science;query by example;web search query;automatic image annotation;world wide web;information retrieval;query language	DB	-12.470413174118423	-57.369169121925474	33863
9e505da0823307361f8cdd54d54bc3f72010b2e9	entity-centric stream filtering and ranking: filtering and unfilterable documents		Cumulative Citation Recommendation (CCR) is defined as: given a stream of documents on one hand and Knowledge Base (KB) entities on the other, filter, rank and recommend citation-worthy documents. The pipeline encountered in systems that approach this problem involves four stages: filtering, classification, ranking (or scoring), and evaluation. Filtering is only an initial step that reduces the web-scale corpus into a working set of documents more manageable for the subsequent stages. Nevertheless, this step has a large impact on the recall that can be attained maximally. This study analyzes in-depth the main factors that affect recall in the filtering stage. We investigate the impact of choices for corpus cleansing, entity profile construction, entity type, document type, and relevance grade. Because failing on recall in this first step of the pipeline cannot be repaired later on, we identify and characterize the citation-worthy documents that do not pass the filtering stage by examining their contents.	commonsense knowledge (artificial intelligence);content-control software;digital curation;document;entity;entity–relationship model;failure;interaction;kilobyte;knowledge base;profiling (computer programming);relevance;statistical classification;venue (sound system);wikipedia;working set	Gebrekirstos G. Gebremeskel;Arjen P. de Vries	2015		10.1007/978-3-319-16354-3_33	computer science;data mining;world wide web;information retrieval	Web+IR	-29.35182977378021	-60.02814846731712	33892
0084d2acd1448ea03c40a79f2d6b39bcca71c9aa	identifying comparable entities on the web	comparative analysis;information extraction;web search engine;large scale;web crawling;query logs;comparables	Web search engines are often presented with user queries that involve comparisons of real-world entities. Thus far, this interaction has typically been captured by users submitting appropriately designed keyword queries for which they are presented a list of relevant documents. Richer interactions that explicitly allow for a comparative analysis of entities represent a new potential direction to improve the search experience. With this in mind, we present an initial step of mining comparable entities from sources of information available to a large-scale Web search engine, namely, search query logs and documents from a Web crawl. Our mining methods generate a diverse set of comparables consisting of entities from a broad class of categories, such as medicines, appliances, electronics, and vacation destinations.	computer appliance;entity;interaction;norm (social);qualitative comparative analysis;web search engine	Alpa Jain;Patrick Pantel	2009		10.1145/1645953.1646198	qualitative comparative analysis;web mining;web modeling;web query classification;web search engine;computer science;web crawler;data mining;database;web search query;world wide web;information extraction;information retrieval;search engine	Web+IR	-30.400170027783933	-54.26954533278735	33895
69f34679e17e4ec89874d37d8521c2ac65ea7763	deriving insights from national happiness indices	conference publication;social networking online internet social aspects of automation;twitter indexes data visualization clustering algorithms cities and towns facebook noise measurement;sentire crowds national happiness indices online social media twitter geolocation information happiness index sentiment scores;social aspects of automation;data mining;visualization;internet;visualisation;indexation;social networking online;sentiment analysis;visualization sentiment analysis social media;social network analysis;twitter;social media	"""In online social media, individuals produce vast amounts of content which in effect """"instruments"""" the world around us. Users on sites such as Twitter are publicly broadcasting status updates that provide an indication of their mood at a given moment in time, often accompanied by geolocation information. A number of strategies exist to aggregate such content to produce sentiment scores in order to build a """"happiness index"""". In this paper, we describe such a system based on Twitter that maintains a happiness index for nine US cities. The main contribution of this paper is a companion system called Sentire Crowds that allows us to identify the underlying causes behind shifts in sentiment. This ability to analyze the components of the sentiment signal highlights a number of problems. It shows that sentiment scoring on social media data without considering context is difficult. More importantly, it highlights cases where sentiment scoring methods are susceptible to unexpected shifts due to noise and trending memes."""	aggregate data;bigram;clique graph;cluster analysis;dynamic data;geolocation;meme;sentiment analysis;social media;the wisdom of crowds;usability testing	Anthony Brew;Derek Greene;Daniel W. Archambault;Padraig Cunningham	2011	2011 IEEE 11th International Conference on Data Mining Workshops	10.1109/ICDMW.2011.61	visualization;computer science;data mining;internet privacy;world wide web;sentiment analysis	DB	-23.060701421955006	-54.00791811985087	33896
06f1e81253677b67a0b7c9415b1cd87fb202529e	unsupervised authorship analysis of phishing webpages	document handling;web sites computer crime document handling;computer crime html electronic mail measurement internet pattern matching;external information unsupervised authorship analysis phishing webpages phishing websites phishing attacks document salient features cyber attacks nuance expert knowledge;computer crime;web sites	Authorship analysis on phishing websites enables the investigation of phishing attacks, beyond basic analysis. In authorship analysis, salient features from documents are used to determine properties about the author, such as which of a set of candidate authors wrote a given document. In unsupervised authorship analysis, the aim is to group documents such that all documents by one author are grouped together. Applying this to cyber-attacks shows the size and scope of attacks from specific groups. This in turn allows investigators to focus their attention on specific attacking groups rather than trying to profile multiple independent attackers. In this paper, we analyse phishing websites using the current state of the art unsupervised authorship analysis method, called NUANCE. The results indicate that the application produces clusters which correlate strongly to authorship, evaluated using expert knowledge and external information as well as showing an improvement over a previous approach with known flaws.	computer cluster;cybercrime;e-commerce;high-level programming language;phishing;profiling (computer programming);stylometry;synergy;the australian;unsupervised learning;victoria (3d figure)	Robert Layton;Paul A. Watters;Richard Dazeley	2012	2012 International Symposium on Communications and Information Technologies (ISCIT)	10.1109/ISCIT.2012.6380857	computer science;data mining;internet privacy;world wide web	Web+IR	-19.98926819895982	-56.84393440428311	33918
537c8c4f519af08cb4330e57da8edc97923ddec0	lxgram: a deep linguistic processing grammar for portuguese	unification grammars;parsing;deep linguistic processing	In this paper we present LXGram, a general purpose grammar for the deep linguistic processing of Portuguese that delivers high precision grammatical analysis and detailed meaning representations. We present the main design features and evaluation results on the grammar’s coverage as well as its ability to produce correct grammatical analyses.	deep linguistic processing	Francisco Costa;António Branco	2010		10.1007/978-3-642-12320-7_11	natural language processing;generative grammar;synchronous context-free grammar;deep linguistic processing;computer science;affix grammar;emergent grammar;linguistics;relational grammar;programming language;attribute grammar;mildly context-sensitive grammar formalism	NLP	-28.960901253551647	-77.08568851085793	33938
86ac1c4d898c4a0e5e63fcfa4c75c456d4dea840	cross document person name disambiguation using entity profiles		Consolidating entity information spread across multiple documents is a critical problem now with the growing use of large open-domain documen t sources. Associating every entity in a corpus to a unique entry in a growing knowledge base serves a dual purpose of consolidating (disambiguatin g) entities as well as to build a rich growing knowledge source containing information a b ut each and every entity accumulated from several documents. With the pre s nce of ambiguous names, use of nominals and aliases, the task of hyper-taggin g n entity mentioned in a document to a node in a knowledge base requires the use o f context in addition to name matching rules. In this paper, we present an ap proach that computes a similarity between entities identified in a document with those in the knowledge base using a Vector Space Model utilizing document level entity profiles Information accumulated for each entity from the entire docum ent. The technique resulted in a TAC evaluation score of 71.9 at the TAC 2009 KBP track. The same technique was also successfully used in obtaining state of the art F-measures ( 93.95) in disambiguating person names by clustering the similarity values obtained using hierarchical agglomerative clustering.	cluster analysis;entity linking;hierarchical clustering;hyper-heuristic;knowledge base;topic model;viable system model;word-sense disambiguation	Harish Srinivasan;John Chen;Rohini K. Srihari	2009			information retrieval;data mining;entity linking;computer science	NLP	-29.190949425562483	-65.12229350494201	34026
080f78fc763a900f9813287d4b0435fffaba1a13	multi-document relationship fusion via constraints on probabilistic databases	probability;semantics;data fusion;data bases;probabilistic database;documents;multiple operation	Previous multi-document relationship extraction and fusion research has focused on single relationships. Shifting the focus to multiple relationships allows for the use of mutual constraints to aid extraction. This paper presents a fusion method which uses a probabilistic database model to pick relationships which violate few constraints. This model allows improved performance on constructing corporate succession timelines from multiple documents with respect to a multi-document fusion baseline.	baseline (configuration management);data mining;database model;existential quantification;f1 score;mutual exclusion;natural language processing;precision and recall;probabilistic database;relational database;relationship extraction;schedule (computer science);succession;text corpus;timeline	Gideon S. Mann	2007			computer science;probabilistic database;pattern recognition;probability;data mining;semantics;sensor fusion;information retrieval	AI	-27.918692712406948	-65.07388254781634	34097
6de7f30827fed1c68a40184b76a1d32ed75f4e3e	rule-based translation with statistical phrase-based post-editing	second workshop;statistical phrase-based post-editing;input text;statistical machine translation;rule-based translation;automatic post-editing strategy;statistical phrase-based system;previous campaign;machine translation system;portage mt system;rule-based mt system;machine translation;rule based	This article describes a machine translation system based on an automatic post-editing strategy: initially translate the input text into the target-language using a rule-based MT system, then automatically post-edit the output using a statistical phrase-based system. An implementation of this approach based on the SYSTRAN and PORTAGE MT systems was used in the shared task of the Second Workshop on Statistical Machine Translation. Experimental results on the test data of the previous campaign are presented.	logic programming;postediting;systran;statistical machine translation;test data	Michel Simard;Nicola Ueffing;Pierre Isabelle;Roland Kuhn	2007			computer-assisted translation;natural language processing;synchronous context-free grammar;speech recognition;transfer-based machine translation;example-based machine translation;computer science;evaluation of machine translation;pattern recognition;machine translation;rule-based machine translation;machine translation software usability	NLP	-23.035108776249896	-76.59232568216228	34103
8ea1d34d41fc5b2abcef92940d33f99cd70d5972	using multiple viewpoints to improve retrieval effectiveness in content-based image retrieval	mathematics;concurrent computing;biographies;information retrieval;image retrieval content based retrieval information retrieval computer science streaming media image representation biographies mathematics computer industry concurrent computing;computer industry;streaming media;image representation;computer science;content based image retrieval;content based retrieval;image retrieval	A viewpoint is any representational scheme on some collection of data objects, together with a mechanism for accessing this content. Thus a viewpoint is any structure from which we can elicit an informative result from a collection of data by presenting a query. A multiple viewpoint system allows a searcher to pose queries to one viewpoint and then change to another viewpoint while retaining context. This is a very general framework that is applicable to any information retrieval system.	content-based image retrieval;information retrieval	James C. French	2004		10.1109/MMSE.2004.86	document retrieval;visual word;concurrent computing;image retrieval;computer science;theoretical computer science;adversarial information retrieval;multimedia;programming language;automatic image annotation;information retrieval;human–computer information retrieval	Web+IR	-12.196946998616369	-57.82758380831951	34124
196b711d66a9d07b4a4bf13d3113dbedd4e6ca39	a deep learning model with hierarchical lstms and supervised attention for anti-phishing		Anti-phishing aims to detect phishing content/documents in a pool of textual data. This is an important problem in cybersecurity that can help to guard users from fraudulent information. Natural language processing (NLP) offers a natural solution for this problem as it is capable of analyzing the textual content to perform intelligent recognition. In this work, we investigate the state-of-theart techniques for text categorization in NLP to address the problem of anti-phishing for emails (i.e, predicting if an email is phishing or not). These techniques are based on deep learning models that have attracted much attention from the community recently. In particular, we present a framework with hierarchical long short-term memory networks (HLSTMs) and attention mechanisms to model the emails simultaneously at the word and the sentence level. Our expectation is to produce an effective model for anti-phishing and demonstrate the effectiveness of deep learning for problems in cybersecurity. Copyright c © by the paper’s authors. Copying permitted for private and academic purposes. In: R. Verma, A. Das. (eds.): Proceedings of the 1st AntiPhishing Shared Pilot at 4th ACM International Workshop on Security and Privacy Analytics (IWSPA 2018), Tempe, Arizona, USA, 21-03-2018, published at http://ceur-ws.org	categorization;computer security;deep learning;document classification;email;experiment;long short-term memory;natural language processing;phishing;text corpus;vocabulary	Minh Nguyen;Toàn Nguyên;Thien Huu Nguyen	2018	CoRR		guard (information security);theoretical computer science;deep learning;natural language processing;categorization;phishing;computer science;sentence;artificial intelligence	NLP	-21.407566017644534	-69.49286378871017	34223
1ec7096054c81a2cc3de74a808952df19a95d9fc	extending web search for online plagiarism detection	search engine;web search plagiarism search engines internet software tools portals data engineering information technology collaborative work centralized control;search engines;text segment extraction online plagiarism detection internet data overloading problem web search engines online detection system;information technology;information filtering;web search engine;internet;security of data information filtering internet search engines;web search;security of data;text segmentation;plagiarism detection	As information technologies advance, the data amount gathered on the Internet increases at an incredible rapid speed. To solve the data overloading problem, people commonly use Web search engines to find what they need. However, as search engines become an efficient and effective tool, plagiarists can grab, reassemble and redistribute text contents without much difficulty. In this paper, we develop an online detection system to reduce such misapplication of search engines. Specifically, suspicious documents are extracted and verified through the collaboration of our plagiarism detection system and search engines. With a proper design, extracted text segments are given different priorities when sending them to search engines as the ascertainment of plagiarism. This greatly reduces unnecessary and repetitive works when performing plagiarism detection.	internet;memory segmentation;web search engine	Yi-Ting Liu;Heng-Rui Zhang;Tai-Wei Chen;Wei-Guang Teng	2007	2007 IEEE International Conference on Information Reuse and Integration	10.1109/IRI.2007.4296615	search engine indexing;metasearch engine;web search engine;semantic search;computer science;web crawler;online search;data mining;database;search analytics;web search query;information technology;world wide web;information retrieval;search engine	Robotics	-30.49818826023078	-54.54955001939962	34275
b10427999fbde2d90e3541c477e2f6ba4c8f08cc	bridge video and text with cascade syntactic structure		We present a video captioning approach that encodes features by progressively completing syntactic structure (LSTM-CSS). To construct basic syntactic structure (i.e., subject, predicate, and object), we use a Conditional Random Field to label semantic representations (i.e., motions, objects). We argue that in order to improve the comprehensiveness of the description, the local features within object regions can be used to generate complementary syntactic elements (e.g., attribute, adverbial). Inspired by redundancy of human receptors, we utilize a Region Proposal Network to focus on the object regions. To model the final temporal dynamics, Recurrent Neural Network with Path Embeddings is adopted. We demonstrate the effectiveness of LSTM-CSS on generating natural sentences: 42.3% and 28.5% in terms of BLEU@4 and METEOR. Superior performance when compared to state-of-the-art methods are reported on a large video description dataset (i.e., MSR-VTT-2016).	audio description;cascading style sheets;conditional random field;hierarchical database model;long short-term memory;meteor;object detection;recurrent neural network;windows preinstallation environment	Guolong Wang;Zheng Qin;Kaiping Xu;Kai Huang;Shuxiong Ye	2018			natural language processing;computer science;cascade;syntax;artificial intelligence	Vision	-14.93067277379214	-69.62216200718746	34286
78919b0a16248c26cafd21c494ea2301810c9e95	on the contribution of word-level semantics to practical author name disambiguation		We demonstrate the utility of word embedding-based semantic similarity methods for Author Name Disambiguation.	internationalized domain name;semantic similarity;word embedding;word-sense disambiguation	Christoph Müller	2018		10.1145/3197026.3203912	information retrieval;semantic similarity;word embedding;semantics;author name;digital library;computer science	NLP	-28.04452512482811	-66.42563357746519	34303
ee941de3addf0e93851f0b652d731e852f032f21	german dialect identification in interview transcriptions		This paper presents three systems submitted to the German Dialect Identification (GDI) task at the VarDial Evaluation Campaign 2017. The task consists of training models to identify the dialect of SwissGerman speech transcripts. The dialects included in the GDI dataset are Basel, Bern, Lucerne, and Zurich. The three systems we submitted are based on: a plurality ensemble, a mean probability ensemble, and a meta-classifier trained on character and word n-grams. The best results were obtained by the meta-classifier achieving 68.1% accuracy and 66.2% F1score, ranking first among the 10 teams which participated in the GDI shared task.	distribution ensemble;ensemble kalman filter;error analysis (mathematics);f1 score;grams;graphics device interface;learning classifier system;n-gram;natural language processing;organizing (structure);statistical classification;switzerland	Shervin Malmasi;Marcos Zampieri	2017			linguistics;transcription (linguistics);speech recognition;computer science;german	NLP	-22.593510324032987	-70.76291775055337	34333
bd574663776edb230f7ef27f8e0c85c36266d2b3	domain adaptation via tree kernel based maximum mean discrepancy for user consumption intention identification			discrepancy function;domain adaptation	Xiao Ding;Bibo Cai;Ting Liu;Qiankun Shi	2018		10.24963/ijcai.2018/560	machine learning;domain adaptation;artificial intelligence;tree kernel;computer science	AI	-19.03363236628466	-67.1022392162504	34420
58a19b9aba018f1093fb59ecc5c9840964330a37	putting it all together: the xtrieval framework at grid@clef 2009	retrieval model	The Xtrieval framework, built at the Chemnitz University of Technology, aims at analyzing the impact of different retrieval models and methods on retrieval effectiveness. For the Grid@CLEF task 2009, the CIRCO framework was integrated into the Xtrieval framework. 15 runs were performed 15 runs in the three languages German, English, and French. For each language two different stemmers and two different retrieval models were used. One run was a fusion run combining the results of the four other experiments. Whereas the different runs demonstrated that the impact of the used retrieval technologies is highly depending on the corpus, the merged approach produced the best results in each language.		Jens Kürsten;Maximilian Eibl	2009		10.1007/978-3-642-15754-7_70	natural language processing;simulation;computer science;data mining;world wide web;information retrieval	Vision	-32.41705660595066	-64.77575736457105	34476
f7cbf7805115b81867af20c35d1ee286f9934618	proliferation and detection of blog spam	social network services;unsolicited electronic mail;classifier combination;spam posters ip addresses;ip blocks blog spam proliferation blog spam detection defensio logs support vector machine classifier spam posters ip addresses autonomous system numbers;support vector machines;blog spam detection;web browser network level security and protection;qa mathematics;autonomous system numbers;information services;autonomic system;support vector machine classifier;internet;blog spam proliferation;defensio logs;web browser;web sites;ip blocks;pattern classification;ip networks;network level security and protection;web sites internet information services ip networks support vector machines unsolicited electronic mail blogs social network services;support vector machine;unsolicited e mail;blogs;web sites pattern classification support vector machines unsolicited e mail	The ease of posting comments and links in blogs has attracted spammers as an alternative venue to conventional email. An experimental study investigates the nature and prevalence of blog spam. Using Defensio logs, the authors collected and analyzed more than one million blog comments during the last two weeks of June 2009. They used a support vector machine (SVM) classifier combined with heuristics to identify spam posters' IP addresses, autonomous system numbers (ASN), and IP blocks. Experimental results show that more than 75 percent of blog comments during the reporting period are spam. In addition, the results show that blog spammers likely operate from a few colocation facilities.	autonomous robot;autonomous system (internet);blog;colocation centre;email;experiment;heuristic (computer science);spamming;support vector machine;venue (sound system)	Saeed Abu-Nimeh;Thomas M. Chen	2010	IEEE Security & Privacy	10.1109/MSP.2010.113	spam blog;support vector machine;computer science;spamming;social spam;spambot;data mining;internet privacy;world wide web;sping	Security	-19.87832927578483	-55.75823236032006	34490
4fbd021b997f4acc545f02285dc16e5aba235316	blogharvest: blog mining and search framework		Beyond serving as online diaries, weblogs have evolved into complex social structures. Blogging software allows users to publish opinions on any topic without any constraints on the predefined schema. Analysis of linkage between blogs has indicated that community forming in blogosphere is not a random process but is a result of shared interests binding bloggers together. Learning, analysis and usage of the user's interest and social linkage from the blog is therefore necessary to provide useful search faculty on the blogosphere to bloggers and revenue generation opportunities like advertising to the blog service providers. In this paper, we demonstrate BlogHarvest which is a blog mining and search framework that extracts the interests of the blogger, finds and recommends blogs with similar topics and provides blog oriented search functionality. BlogHarvest uses classification, linkage & topic similarity based clustering and POS tagging based opinion mining for providing these features. Novel search interface is built to provide related blogs for queries along with the usual result ranking. Association rules found from POS tags are used to get the context of search for providing query expansion to get targeted results. By crawling the blogosphere and extract & index blog posts and linkage metadata; we have analyzed around 50000 blogs to tune our algorithms.	algorithm;blog;blogger;blogosphere;brown corpus;cluster analysis;linkage (software);online diary;part-of-speech tagging;query expansion;social structure;statistical classification;stochastic process	Mukul Joshi;Nikhil Belsare	2006			world wide web;sentiment analysis;query expansion;service provider;data mining;cluster analysis;blogosphere;metadata;computer science;association rule learning;schema (psychology)	ML	-25.004060742566544	-53.86742428563576	34503
133a0274c992c67ed6a84c8d41c7e4685f3ff0e3	an algorithm for building lexical semantic network and its application to polnet - polish wordnet project	lexical semantics	This paper presents the PolNet - Polish WordNet project which aims at a linguistically oriented ontology for Polish compatible with Princeton WordNet. We present the headlines of its methodology as well as its implementation using the DEBVisDic tool for WordNet building. We present the results obtained so far and discuss a number of related problems.	algorithm;semantic network;wordnet	Zygmunt Vetulani;Justyna Walkowska;Tomasz Obrêbski;Jacek Marciniak;Pawel Konieczka;Przemyslaw Rzepecki	2007		10.1007/978-3-642-04235-5_32	natural language processing;wordnet;extended wordnet;computer science;linguistics;information retrieval	NLP	-30.92538118394757	-71.93526211340729	34517
8e85fc1a66875651c758476671b9c0480347f86f	querying linguistic corpora with prolog		In this paper we demonstrate how Prolog can be used to query linguistically annotated corpora, combining the ease of dedicated declarative query languages and the flexibility of general-purpose languages. On the basis of a Prolog representation of the German Tüba-D/Z Treebank, we show how one can tally arbitrary features of (groups) of nodes, define queries that combine information from different layers of annotation and cross sentence boundaries, query ‘virtual annotation’ by transforming annotation on-thefly, and perform data driven error analysis. Almost all code needed for these case studies is contained in the paper.	error analysis (mathematics);general-purpose language;general-purpose markup language;library (computing);logico-linguistic modeling;online and offline;prettyprint;printing;programmer;prolog;query language;text corpus;treebank	Gerlof Bouma	2010			programming language;treebank;query language;sentence boundary disambiguation;prolog;computer science;annotation	NLP	-28.949510833448063	-79.48053858133997	34632
ef2498485bc2ccb7f9a4a0be2a298ccae16c1dde	query-adaptive fusion for multimodal search	busqueda informacion;detectors;metodo adaptativo;video databases;evaluation performance;query class dependent models multimedia indexing and retrieval multimodal search query adaptive fusion;algoritmo busqueda;performance evaluation;estrategia optima;low level visual features;query processing;query adaptive retrieval;search engines;information retrieval;algorithme recherche;query class dependent models;evaluacion prestacion;search algorithm;search methods;search method;video retrieval;pregunta documental;speech;search strategy;base donnee video;methode adaptative;web retrieval multimodal search query adaptive search metasearch video search textual speech transcripts low level visual features high level semantic concept detectors query adaptive retrieval text retrieval;textual speech transcripts;metasearch;multimedia computing;optimal strategy;etat actuel;query adaptive search;high level semantic concept detectors;indexing;recherche information;state of the art;indexation;adaptive method;spatial databases;multimedia communication;text retrieval;strategie recherche;indizacion;high definition video;video retrieval multimedia computing query processing;visual features;query;web retrieval;query adaptive fusion;multimedia indexing and retrieval;estado actual;web search;multimedia indexing;analisis semantico;analyse semantique;communication multimedia;video database;strategie optimale;video search;requete;search methods information retrieval web search search engines metasearch spatial databases visual databases speech detectors high definition video;multimodal search;semantic analysis;estrategia investigacion;visual databases	We conduct a broad survey of query-adaptive search strategies in a variety of application domains, where the internal retrieval mechanisms used for search are adapted in response to the anticipated needs for each individual query experienced by the system. While these query-adaptive approaches can range from meta-search over text collections to multimodal search over video databases, we propose that all such systems can be framed and discussed in the context of a single, unified framework. In our paper, we keep an eye towards the domain of video search, where search cues are available from a rich set of modalities, including textual speech transcripts, low-level visual features, and high-level semantic concept detectors. The relative efficacy of each of the modalities is highly variant between many types of queries. We observe that the state of the art in query-adaptive retrieval frameworks for video collections is highly dependent upon the definition of classes of queries, which are groups of queries that share similar optimal search strategies, while many applications in text and Web retrieval have included many advanced strategies, such as direct prediction of search method performance and inclusion of contextual cues from the searcher. We conclude that such advanced strategies previously developed for text retrieval have a broad range of possible applications in future research in multimodal video search.	application domain;bridging (networking);database;document retrieval;embedded system;emergence;federated search;high- and low-level;information retrieval;limited availability;mined;multimodal interaction;optical character recognition;relevance;search algorithm;sensor;sparse matrix;speech recognition;stepping level;unified framework;video clip;web search engine;weight function	Lyndon S. Kennedy;Shih-Fu Chang;Apostol Natsev	2008	Proceedings of the IEEE	10.1109/JPROC.2008.916345	search engine indexing;detector;metasearch engine;semantic search;computer science;speech;concept search;multimedia;world wide web;information retrieval;search algorithm	Web+IR	-15.53956329288833	-58.021392060419515	34634
d00943a1659e2b8248b91f27946989f82370a347	wikipedia2vec: an optimized tool for learning embeddings of words and entities from wikipedia		We present Wikipedia2Vec, an open source tool for learning embeddings of words and entities from Wikipedia. This tool enables users to easily obtain high-quality embeddings of words and entities from a Wikipedia dump with a single command. The learned embeddings can be used as features in downstream natural language processing (NLP) models. The tool can be installed via PyPI. The source code, documentation, and pretrained embeddings for 12 major languages can be obtained at http://wikipedia2vec.github.io.		Ikuya Yamada;Akari Asai;Hiroyuki Shindo;Hideaki Takeda;Yoshiyasu Takefuji	2018	CoRR			NLP	-32.7904739407511	-73.87149842811789	34662
cc54a917271532cc43a2b194a2b4ce91c2e63276	intuitive large image database browsing using perceptual similarity enriched by crowds	databases;retrieval;browsers;navigation;crowd sourcing;indexing;clustering;abstracts;similarity;perception;textures;images	The main objective of image browsers is to empower users to find a desired image with ease, speed and accuracy from a large database. In this pa- per we present a novel approach at creating an image browsing environment based on human perception with the aim of providing intuitive image naviga- tion. In our approach, similarity judgments form the basic structural organiza- tion for the images in our browser. To enrich this we have developed a scalable crowd sourced method of augmenting a database with a large number of additional samples by capturing human judgments from members of a crowd. Experiments were conducted involving two databases that demonstrate the effectiveness of our method as an intuitive, fast browsing environment for large image databases.	browsing	Stefano Padilla;Fraser Halley;David A. Robb;Mike J. Chantler	2013		10.1007/978-3-642-40246-3_21	computer vision;search engine indexing;navigation;similarity;computer science;multimedia;cluster analysis;perception;world wide web	Vision	-16.72194684625551	-57.882918816828564	34676
076a7e93bed8ed956bf75084b5e6adce11d89876	a novel text classification algorithm based on na&#239;ve bayes and kl-divergence	classification algorithm;neural networks;support vector machines;naive bayes;niobium;text classification;relative entropy;learning systems;naive bayes classifier;machine learning;classification algorithms;feature weighting;support vector machine classification;entropy;text categorization classification algorithms niobium algorithm design and analysis support vector machines support vector machine classification learning systems entropy decision trees neural networks;classification accuracy;bayes classifier;decision trees;algorithm design and analysis;text categorization	The Naive Bayes classifier is a popular machine learning method for text classification because it is fast and easy to implement and performs well. Its severe assumption that each feature word is independent with other feature words in a document makes higher efficiency possible but also adversely affects the quality of its results because some of feature words are interrelated. In this paper, in order to enhance the performance of the text classification, some solutions are proposed to some of the problems with Naïve Bayes classifiers. Based on the original Naive Bayes algorithm, we take feature weight into account and make it a factor and combine KL-divergence (relative entropy) between the words to improve Naïve Bayes classifier. The improved Naïve Bayes classification algorithm is called INBA. By theory and experiment analyses it is proved that INBA algorithm not only has advantages of Naïve Bayes classifier, but also results in higher classification accuracy, and the solutions are feasible, practical and effective.	algorithm;document classification;kullback–leibler divergence;machine learning;microsoft word for mac;naive bayes classifier	Baoyi Wang;Shaomin Zhang	2005	Sixth International Conference on Parallel and Distributed Computing Applications and Technologies (PDCAT'05)	10.1109/PDCAT.2005.36	statistical classification;probabilistic classification;bayes classifier;naive bayes classifier;bayesian programming;computer science;machine learning;pattern recognition;data mining;bayes error rate;artificial neural network	DB	-20.713664383535967	-64.2713023411752	34687
30f1ea3b4194dba7f957fd6bf81bcaf12dca6ff8	dynamic programming for linear-time incremental parsing	dynamic program;linear time	Incremental parsing techniques such as shift-reduce have gained popularity thanks to their efficiency, but there remains a major problem: the search is greedyand only explores a tiny fraction of the whole space (even with beam search) as opposed to dynamic programming. We show that, surprisingly, dynamic programming is in fact possible for many shift-reduce parsers, by merging “equivalent” stacks based on feature values. Empirically, our algorithm yields up to a five-fold speedup over a state-of-the-art shift-reduce dependency parser with no loss in accuracy. Better search also leads to better learning, and our final parser outperforms all previously reported dependency parsers for English and Chinese, yet is much faster.	algorithm;beam search;dynamic programming;parsing expression grammar;petri net;python;shift-reduce parser;speedup;time complexity;treebank	Liang Huang;Kenji Sagae	2010			natural language processing;time complexity;parser combinator;computer science;theoretical computer science;machine learning;programming language;algorithm	NLP	-20.043051772500956	-76.53604291221141	34691
72c2cc507bc7203bcb4eaf6a3df6e9e8f8514e31	a simple and effective approach to the story cloze test		In the Story Cloze Test, a system is presented with a 4-sentence prompt to a story, and must determine which one of two potential endings is the ‘right’ ending to the story. Previous work has shown that ignoring the training set and training a model on the validation set can achieve high accuracy on this task due to stylistic differences between the story endings in the training set and validation and test sets. Following this approach, we present a simpler fully-neural approach to the Story Cloze Test using skip-thought embeddings of the stories in a feed-forward network that achieves close to state-of-the-art performance on this task without any feature engineering. We also find that considering just the last sentence of the prompt instead of the whole prompt yields higher accuracy with our approach.	baseline (configuration management);feature engineering;feedforward neural network;test set	Siddarth Srinivasan;Richa Arora;Mark O. Riedl	2018			natural language processing;artificial intelligence;cloze test;machine learning;feature engineering;computer science;sentence;training set	NLP	-20.439515484833713	-71.77577021217779	34699
13681ed97ad13052850df90eaff145c2cfae30bb	fbk-irst at clef 2007	question answering	This report presents the outcomes of the activity carried out at FBK-irst for the participation in the CLEF-2007 Main QA track. Both the major improvements over last year’s version of the DIOGENE system, and the results achieved in the evaluation exercise are reported.		Milen Kouylekov;Matteo Negri;Bernardo Magnini;Bonaventura Coppola	2007			computer science;database;world wide web;information retrieval	Vision	-32.02982303707149	-63.66861753565185	34704
2321b45922dee0a3f002cb7befbe8bdb58408ed3	on the construction of a large scale chinese web test collection	search engine;web pages;zipf like law;large scale;chinese information retrieval;documents;power law;test collection	The lack of a large scale Chinese test collection is an obstacle to the Chinese information retrieval development. In order to address this issue, we built such a collection composed of millions of Chinese web pages, known as the Chinese Web Test collection with 100 gigabyte (CWT100g) in data volume, which is the largest Chinese web test collection as of this writing, and has been used by several dozen research groups besides being adopted in the evaluation of the SEWM-2004 Chinese Web Track[1] and the HTRDPE-2004[2]. We present the total solution for constructing a large scale test collection like the CWT100g. Further, we found that: 1) the distribution of the number of pages within sites obeys a Zipf-like law instead of a power law proposed by Adamic and Huberman [3, 4]; 2) and an appropriate filtering method on host alias will economize resources for about 25% while crawling pages. The Zipf-like law and the method of filtering host alias proposed in the paper will facilitate both to model the Web and to perfect a search engine. Finally, we report on the results of the SEWM-2004 Chinese Web Track.	benchmark (computing);document;gigabyte;information retrieval;list of toolkits;testbed;web page;web search engine;zipf's law	Hongfei Yan;Chong Chen;Bo Peng;Xiaoming Li	2008		10.1007/978-3-540-68636-1_12	power law;web search engine;computer science;web page;data mining;world wide web;information retrieval;search engine	Web+IR	-31.338943132484086	-55.98129704365871	34709
fe49d2819719e23fa08dece6ba5a3d6e1f9f8a75	mac: mining activity concepts for language-based temporal localization		We address the problem of language-based temporal localization in untrimmed videos. Compared to temporal localization with fixed categories, this problem is more challenging as the language-based queries not only have no predefined activity list but also may contain complex descriptions. Previous methods address the problem by considering features from video sliding windows and language queries and learning a subspace to encode their correlation, which ignore rich semantic cues about activities in videos and queries. We propose to mine activity concepts from both video and language modalities by applying the actionness score enhanced Activity Concepts based Localizer (ACL). Specifically, the novel ACL encodes the semantic concepts from verb-obj pairs in language queries and leverages activity classifiers’ prediction scores to encode visual concepts. Besides, ACL also has the capability to regress sliding windows as localization results. Experiments show that ACL significantly outperforms state-of-the-arts under the widely used metric, with more than 5% increase on both Charades-STA and TACoS datasets1.	encode;linked list;microsoft windows;natural language	Runzhou Ge;Jiyang Gao;Kan Chen;R. Nevatia	2018	CoRR		modalities;encode;machine learning;task analysis;semantics;feature extraction;artificial intelligence;visualization;computer science;subspace topology;correlation	Vision	-14.685595109047417	-69.68418543756547	34722
c7ebe9754da4f7eff56412683cba379b51360a19	automatically selecting the best dependency annotation design with dynamic oracles		This work introduces a new strategy to compare the numerous conventions that have been proposed over the years for expressing dependency structures and discover the one for which a parser will achieve the highest parsing performance. Instead of associating each sentence in the training set with a single gold reference, we propose to consider a set of references encoding alternative syntactic representations. Training a parser with a dynamic oracle will then automatically select among all alternatives the reference that will be predicted with the highest accuracy. Experiments on the UD corpora show the validity of this approach.	church encoding;experiment;parsing;test set;text corpus;urban dictionary	Guillaume Wisniewski;Ophélie Lacroix;François Yvon	2018			parsing;encoding (memory);computer science;oracle;machine learning;dependency grammar;training set;artificial intelligence;syntax;sentence;annotation	NLP	-22.435349301956613	-74.57771063895665	34731
35855f8513345fb1c466845927e2ed8822ca2d9c	#hardtoparse: pos tagging and parsing the twitterverse	sprakteknologi sprakvetenskaplig databehandling;datorlingvistik;malt;artificial intelligence;computational linguistics;language technology computational linguistics;twitter	We evaluate the statistical dependency parser, Malt, on a new dataset of sentences taken from tweets. We use a version of Malt which is trained on gold standard phrase structure Wall Street Journal (WSJ) trees converted to Stanford labelled dependencies. We observe a drastic drop in performance moving from our in-domain WSJ test set to the new Twitter dataset, much of which has to do with the propagation of part-of-speech tagging errors. Retraining Malt on dependency trees produced by a state-of-the-art phrase structure parser, which has itself been self-trained on web material, results in a significant improvement. We analyse this improvement by examining in detail the effect of the retraining on individual dependency types.	baseline (configuration management);brill tagger;dependency grammar;linear algebra;parser combinator;parsing;part-of-speech tagging;phrase structure rules;point of sale;software propagation;test set;the wall street journal;unsupervised learning	Jennifer Foster;Özlem Çetinoglu;Joachim Wagner;Joseph Le Roux;Stephen Hogan;Joakim Nivre;Deirdre Hogan;Josef van Genabith	2011			natural language processing;speech recognition;computer science;artificial intelligence;computational linguistics;machine learning;world wide web	NLP	-22.446756428081418	-75.200106157043	34745
f8f64abf988c51e21ca21e136175596b3c2c4481	exploiting cooccurrence on corpus and document level for fair crosslanguage retrieval	information retrieval system;information retrieval;word sense disambiguation;indexation;query translation;query expansion	In this paper we describe the methodology, architecture and implementation of the information retrieval system we have developed for the Robust WSD Task at CLEF 2008. Our system is based on an extensive query preprocessing step for homogenisation of the corpus queries. The preprocessing of queries includes: firstly, an query expansion step based on Wordnet Synonsyms or an Associative Index, secondly a query translation step based on corpus article cooccurrence in Wikipedia, and thirdly a standard disjunct index search in the CLEF corpus. The crosslanguage enabled system behaves thereby as much as possible fair over different languages. We apply the same preprocessing steps, independent of the query and corpus language, to all queries.	fairness measure;fundamental fysiks group;information retrieval;preprocessor;query expansion;web services for devices;wikipedia;word sense;word-sense disambiguation;wordnet	Andreas Juffinger;Roman Kern;Michael Granitzer	2008			natural language processing;query optimization;query expansion;web query classification;computer science;database;web search query;information retrieval;query language	Web+IR	-33.057473463742376	-64.17836415764435	34770
ba7db3c28a20adbb0b7cbf35bbfede6c02f403c8	hacia una semántica computacional de las anáforas demostrativas	generalized quantifiers;demonstratives;semantics;anaphora;spanish;discourse	Version:1.0 StartHTML:0000000267 EndHTML:0000003569 StartFragment:0000002583 EndFragment:0000003533 SourceURL:file://localhost/Users/raquelpalopbenlloch/Documents/PAPERS/CONFERENCIAS%202008-2009/PAPER%20PROJECT%20Linguam%C3%A1tica/Paper%20Linguam%C3%A1tica.doc                  Demonstratives exhibit a dual nature with respect to their discourse behavior. On the one hand, they behave as directly referential elements commonly accompanied by a pointing gesture in their canonical use. On the other hand, speakers make use of demonstratives to refer to a range of entities that have been previously mentioned in discourse (discourse anaphora), such as events, propositions or any other type of abstract entities apparently lacking any kind of spatio-temporal anchoring. In this paper, we propose a characterization of Spanish demonstrative determiners and pronouns as generalized quantifiers, which will allow us to account for their heterogeneous referential nature and the principal differences among these elements.	unique name assumption	Iker Zulaica-Hernandez;Javier Gutierrez-Rexach	2009	Linguamática		psychology;natural language processing;linguistics;communication	Logic	-8.784109847135278	-76.21409459723185	34775
eaf2636c133f33907c796fbe9930da24eb7f1adc	incorporating social media comments in affective video retrieval	affective video retrieval;information retrieval;dempster shafer theory of evidence;information fusion;social media	Affective video retrieval systems aim at finding video contents matching the desires and needs of users. Existing systems typically use the information contained in the video itself to specify its affect category. These systems either extract low-level features or build up higher-level attributes to train classification algorithms. However, using low-level features ignores global relations in data and constructing high-level features is time consuming and problem dependent. To overcome these drawbacks, an external source of information may be helpful. With the explosive growth and availability of social media, users’ comments could be such a valuable source of information. In this study, a new method for incorporating social media comments with the audio-visual contents of videos is proposed. Furthermore, for the combination stage a decision-level fusion method based on the Dempster–Shafer theory of evidence is presented. Experiments are carried out on the video clips of the DEAP (Database for Emotion Analysis using Physiological signals) dataset and their associated users’ comments on YouTube. Results show that the proposed system significantly outperforms the baseline method of using only the audio-visual contents for affective video retrieval.	algorithm;baseline (configuration management);high- and low-level;information science;information source;lexicon;machine learning;multimodal interaction;naive bayes classifier;oracle fusion architecture;recommender system;sentiment analysis;social media;text corpus;video clip	Shahla Nemati;Ahmad Reza Naghsh-Nilchi	2016	J. Information Science	10.1177/0165551515593689	social media;computer science;multimedia;world wide web;information retrieval	AI	-21.440906674098702	-58.04103850665656	34787
3b018f820e923da499af17a3102c39ec2c6e14b0	unsupervised metaphor paraphrasing using a vector space model		We present the first fully unsupervised approach to metaphor interpretation, and a system that produces literal paraphrases for metaphorical expressions. Such a form of interpretation is directly transferable to other NLP applications that can benefit from a metaphor processing component. Our method is different from previous work in that it does not rely on any manually annotated data or lexical resources. First, our method computes candidate paraphrases according to the context in which the metaphor appears, using a vector space model. It then uses a selectional preference model to measure the degree of literalness of the paraphrases. The system identifies correct paraphrases with a precision of 0.52 at top rank, which is a promising result for a fully unsupervised approach.	design of experiments;dirk brockmann;interpretation (logic);lexical substitution;lexicon;literal (mathematical logic);natural language processing;newton;scalability;test set;unsupervised learning;word lists by frequency	Ekaterina Shutova;Tim Van de Cruys;Anna Korhonen	2012			natural language processing;machine learning;pattern recognition	NLP	-26.046631000249832	-72.16862287512687	34796
084199521e3f4a05e70c397943da5c8586f769fd	co-training for topic classification of scholarly data		With the exponential growth of scholarly data during the past few years, effective methods for topic classification are greatly needed. Current approaches usually require large amounts of expensive labeled data in order to make accurate predictions. In this paper, we posit that, in addition to a research article’s textual content, its citation network also contains valuable information. We describe a co-training approach that uses the text and citation information of a research article as two different views to predict the topic of an article. We show that this method improves significantly over the individual classifiers, while also bringing a substantial reduction in the amount of labeled data required for training accurate classifiers.	algorithm;citation network;co-training;experiment;latent dirichlet allocation;natural language processing;semi-supervised learning;semiconductor industry;supervised learning;time complexity	Cornelia Caragea;Florin Adrian Bulgarov;Rada Mihalcea	2015			computer science;data science;data mining;information retrieval	NLP	-22.230196018633475	-62.329735560019806	34836
309bf94db239a1646a51a712ece2be3d82342c8a	an efficient approach to learning chinese judgment document similarity based on knowledge summarization		A previous similar case in common law systems can be used as a reference with respect to the current case such that identical situations can be treated similarly in every case. However, current approaches for judgment document similarity computation failed to capture the core semantics of judgment documents and therefore suffer from lower accuracy and higher computation complexity. In this paper, a knowledge block summarization based machine learning approach is proposed to compute the semantic similarity of Chinese judgment documents. By utilizing domain ontologies for judgment documents, the core semantics of Chinese judgment documents is summarized based on knowledge blocks. Then the WMD algorithm is used to calculate the similarity between knowledge blocks. At last, the related experiments were made to illustrate that our approach is very effective and efficient in achieving higher accuracy and faster computation speed in comparison with the traditional approaches.	algorithm;automatic summarization;computation;deep learning;experiment;machine learning;ontology (information science);semantic similarity	Yinglong Ma;Peng Zhang;Jiangang Ma	2018	CoRR		semantic similarity;machine learning;automatic summarization;artificial intelligence;data mining;computer science;ontology (information science);semantics;computation	NLP	-27.463497602272312	-66.45230739758107	34892
b25cc66e3340fec8413a1b23038a8e644144d5ea	emergence of vocal developmental sequences in a predictive coding model of speech acquisition		Learning temporal patterns among primitive speech sequences and being able to control the motor apparatus for effective production of the learned patterns are imperative for speech acquisition in infants. In this paper, we develop a predictive coding model whose objective is to minimize the sensory (auditory) and proprioceptive prediction errors. Temporal patterns are learned by minimizing the former while control is learned by minimizing the latter. The model is learned using a set of synthetically generated syllables, as in other contemporary models. We show that the proposed model outperforms existing ones in learning vocalization classes. It also computes the control/muscle activation which is useful for determining the degree of easiness of vocalization.	emergence;imperative programming;speech acquisition	Shamima Najnin;Bonny Banerjee	2016		10.21437/Interspeech.2016-1126	speech recognition	ML	-7.309773955315894	-79.40122168894165	34906
34918aa54b1452a429bc456726e518bd4447beb8	automatic generation of review content in specific domain of social network based on rnn		The online social network has become a favorable site where a large number of malicious netizens spread rumors and conduct malignant competition. In this paper, we set up a method for generating review content in specific domain of social networks, which uses a recurrent neural network model to generate the social network-style review. Taking Twitter platform as an example platform, we firstly classify the review text according to the sentence pattern; secondly, aiming at different categories, we design corresponding recurrent neural network model to generate the initial review text corresponding to sentence structures; finally, we conduct automatic replacement of the generated initial text through the relevance of subject terms to achieve the effect of better adapting to hot topics. This method is not only easy to operate and economical, but also can evade the most advanced detectors. In the same environment, it is superior to the existing technology and generates more than 85.2% of the output text with correct grammar and wise contents.	algorithm;artificial neural network;lock (computer science);long short-term memory;machine learning;malware;netizen;network model;pattern language;random neural network;recurrent neural network;relevance;sensor;social network	Yu Tai;Hui He;Weizhe Zhang;Yanguo Jia	2018	2018 IEEE Third International Conference on Data Science in Cyberspace (DSC)	10.1109/DSC.2018.00096	feature extraction;social network;machine learning;recurrent neural network;computer science;artificial intelligence;sentence	DB	-18.813298799915273	-69.34715381479006	34915
a219e501132a6cf8a05a8153c66f845b56406203	multiflex: a multilingual finite-state tool for multi-word units	multiflex;multi word units;finite state morphology;natural language processing	Multi-word units are linguistic objects whose idiosyncrasy calls for a lexicalized approach allowing to render their orthographic, inflectional and syntactic flexibility.  Multiflex  is a graph-based formalism answering this need by conflation of different surface realizations of the same underlying concept. Its implementation relies on a finite-state machinery with unification. It can be applied to the creation of linguistic resources for a high-quality natural language processing tasks.		Agata Savary	2009		10.1007/978-3-642-02979-0_27	natural language processing;speech recognition;computer science;linguistics	Logic	-28.865012692308127	-79.76485669171498	34922
6bc82095f4bd345820f00bc91f86c0f1f673b761	vector representations of multi-word terms for semantic relatedness	distributional similarity;natural language processing;semantic similarity and relatedness	This paper presents a comparison between several multi-word term aggregation methods of distributional context vectors applied to the task of semantic similarity and relatedness in the biomedical domain. We compare the multi-word term aggregation methods of summation of component word vectors, mean of component word vectors, direct construction of compound term vectors using the compoundify tool, and direct construction of concept vectors using the MetaMap tool. Dimensionality reduction is critical when constructing high quality distributional context vectors, so these baseline co-occurrence vectors are compared against dimensionality reduced vectors created using singular value decomposition (SVD), and word2vec word embeddings using continuous bag of words (CBOW), and skip-gram models. We also find optimal vector dimensionalities for the vectors produced by these techniques. Our results show that none of the tested multi-word term aggregation methods is statistically significantly better than any other. This allows flexibility when choosing a multi-word term aggregation method, and means expensive corpora preprocessing may be avoided. Results are shown with several standard evaluation datasets, and state of the results are achieved.	bag-of-words model;baseline (configuration management);choose (action);dimensionality reduction;display resolution;microsoft word for mac;n-gram;numerous;preprocessor;semantic similarity;singular value decomposition;text corpus;word embedding;word2vec;gram	Sam Henry;Clint Cuffy;Bridget T. McInnes	2018	Journal of biomedical informatics	10.1016/j.jbi.2017.12.006	information retrieval;artificial neural network;curse of dimensionality;semantic similarity;bag-of-words model;word2vec;computer science;dimensionality reduction;preprocessor;singular value decomposition;artificial intelligence;pattern recognition	NLP	-20.994120504779556	-73.43871904783262	34927
189da019d007c27601791717f792c90693285db7	extending the negex lexicon for multiple languages	sweden;computer and systems sciences;vocabulary controlled;systemvetenskap informationssystem och informatik;information systems;translating;semantics;france;medical records systems computerized;terminology as topic;germany;data och systemvetenskap;artificial intelligence;natural language processing	"""We translated an existing English negation lexicon (NegEx) to Swedish, French, and German and compared the lexicon on corpora from each language. We observed Zipf's law for all languages, i.e., a few phrases occur a large number of times, and a large number of phrases occur fewer times. Negation triggers """"no"""" and """"not"""" were common for all languages; however, other triggers varied considerably. The lexicon is available in OWL and RDF format and can be extended to other languages. We discuss the challenges in translating negation triggers to other languages and issues in representing multilingual lexical knowledge."""	lexicon;multilingualism;phrases;precipitating factors;programming languages;resource description framework;text corpus;web ontology language;zipf's law	Wendy W. Chapman;Dieter Hillert;Sumithra Velupillai;Maria Kvist;Maria Skeppstedt;Brian E. Chapman;Mike Conway;Melissa Tharp;Danielle L. Mowery;Louise Deléger	2013	Studies in health technology and informatics	10.3233/978-1-61499-289-9-677	natural language processing;speech recognition;computer science;linguistics	NLP	-30.01247878822181	-71.8679079183602	34936
0194e38faa68cbaf56bbc1dd4475eaf5ec2c9196	computing logical form on regulatory texts	scope-taking operator;scope comparison;logical form;computing logical form;abstract syntax tree;main step;regulatory text;modest-sized corpus;scope ambiguity;intermediate step;novel variant	The computation of logical form has been proposed as an intermediate step in the translation of sentences to logic. Logical form encodes the resolution of scope ambiguities. In this paper, we describe experiments on a modestsized corpus of regulation annotated with a novel variant of logical form, calledabstract syntax trees(ASTs). The main step in computing ASTs is to order scope-taking operators. A learning model for ranking is adapted for this ordering. We design features by studying the problem of comparing the scope of one operator to another. The scope comparisons are used to compute ASTs, with an F-score of 90.6% on the set of ordering decisons.	abstract syntax tree;claire;computation;embedded system;emily howell;experiment;heuristic (computer science);lexico;parse tree;parsing;peres–horodecki criterion;pragmatic theory of information;search algorithm;statistical model;word-sense disambiguation	Nikhil Dinesh;Aravind K. Joshi;Insup Lee	2011			natural language processing;computer science;theoretical computer science;machine learning;algorithm	NLP	-25.362427421563737	-76.2600835735605	34987
0f19f13daa94bcf7c118764147e7f2469df7e243	dynamic querying for pattern identification in microarray and genomic data	genes;premrna splicing;pre mrna splicing;splicing;genomics;sequences;splicing query processing pattern recognition genetics sequences macromolecules medical signal processing;expression profiles;mice;linear order;expression profile;query processing;time course;dynamic querying;interactive search;genomic data;timesearcher;genetics;genomics bioinformatics sequences splicing displays genetics frequency gene expression signal processing mice;gene expression;linear ordered sequences;signal processing;displays;dynamic query;macromolecules;pattern recognition;microarray;technical report;data sets;frequency;medical signal processing;premrna splicing dynamic querying pattern identification microarray genomic data data sets linear ordered sequences bioinformatics timesearcher genes expression profiles;pattern identification;bioinformatics	Data sets involving linear ordered sequences are a recurring theme in bioinformatics. Dynamic query tools that support exploration of these data sets can be useful for identifying patterns of interest. This paper describes the use of one such tool – TimeSearcher to interactively explore linear sequence data sets taken from two bioinformatics problems. Microarray time course data sets involve expression levels for large numbers of genes over multiple time points. TimeSearcher can be used to interactively search these data sets for genes with expression profiles of interest. The occurrence frequencies of short sequences of DNA in aligned exons can be used to identify sequences that play a role in the pre-mRNA splicing. TimeSearcher can be used to search these data sets for candidate splicing signals.	bioinformatics;interactivity;microarray	Harry Hochheiser;Eric H. Baehrecke;Stephen M. Mount;Ben Shneiderman	2003		10.1109/ICME.2003.1221346	macromolecule;genomics;gene expression;computer science;bioinformatics;technical report;frequency;signal processing;gene;microarray;data mining;sequence;rna splicing;data set;total order	Comp.	-5.770131785073241	-59.78190344606459	34996
616d7503d22c22974dc21b4929e55a3f996616c1	fast and effective spam sender detection with granular svm on highly imbalanced mail server behavior data	highly imbalanced mail server behavior data;support vector machines;unsolicited electronic mail support vector machines electronic mail filtering home appliances support vector machine classification international collaboration pattern analysis algorithm design and analysis computer science;class imbalance;spam sender detection;data mining;support vector;unsolicited e mail pattern classification support vector machines;reputation system;spam filtering;granular support vector machine spam filtering data mining class imbalance;pattern classification;unsolicited email;support vector machine;cost sensitive learning;unsolicited e mail;granular support vector machine;behavioral classification;behavioral classification spam filtering granular support vector machine spam sender detection highly imbalanced mail server behavior data unsolicited email	Unsolicited commercial or bulk emails or emails containing virus currently pose a great threat to the utility of email communications. A recent solution for filtering is reputation systems that can assign a value of trust to each IP address sending email messages. By analyzing the query patterns of each participating node, reputation systems can calculate a reputation score for each queried IP address and serve as a platform for global collaborative spam filtering for all participating nodes. In this research, we explore a behavioral classification approach based on spectral sender characteristics retrieved from such global messaging patterns. Due to the large amount of bad senders, this classification task has to cope with highly imbalanced data. In order to solve this challenging problem, a novel granular support vector machine - boundary alignment algorithm (GSVM-BA) is designed. GSVM-BA looks for the optima] decision boundary by repetitively removing positive support vectors from the training dataset and rebuilding another SVM. Compared to the original SVM algorithm with cost-sensitive learning, GSVM-BA demonstrates superior performance on spam IP detection, in terms of both effectiveness and efficiency	algorithm;business architecture;decision boundary;email filtering;granular computing;messaging pattern;reputation system;spamming;support vector machine	Yuchun Tang;Sven Krasser;Paul Judge;Yanqing Zhang	2006	2006 International Conference on Collaborative Computing: Networking, Applications and Worksharing	10.1109/COLCOM.2006.361856	support vector machine;computer science;data mining;internet privacy;world wide web	Metrics	-19.78215331887497	-55.28173319638083	35001
0b47b6ffe714303973f40851d975c042ff4fcde1	distributional clustering of english words	coarse sense distinction;relative frequency distribution;deterministic annealing;average context distribution;test data;annealing parameter increase;distributional clustering;class model;relative entropy;clustering word;cluster membership;english word;data clustering	"""We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical """"soft"""" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data. I N T R O D U C T I O N Methods for automatically classifying words according to their contexts of use have both scientific and practical interest. The scientific questions arise in connection to distributional views of linguistic (particularly lexical) structure and also in relation to the question of lexical acquisition both from psychological and computational learning perspectives. From the practical point of view, word classification addresses questions of data sparseness and generalization in statistical language models, particularly models for deciding among alternative analyses proposed by a grammar. It is well known that a simple tabulation of frequencies of certain words participating in certain configurations, for example of frequencies of pairs of a transitive main verb and the head noun of its direct object, cannot be reliably used for comparing the likelihoods of different alternative configurations. The problemis that for large enough corpora the number of possible joint events is much larger than the number of event occurrences in the corpus, so many events are seen rarely or never, making their frequency counts unreliable estimates of their probabilities. Hindle (1990) proposed dealing with the sparseness problem by estimating the likelihood of unseen events from that of """"similar"""" events that have been seen. For instance, one may estimate the likelihood of a particular direct object for a verb from the likelihoods of that direct object for similar verbs. This requires a reasonable definition of verb similarity and a similarity estimation method. In Hindle's proposal, words are similar if we have strong statistical evidence that they tend to participate in the same events. His notion of similarity seems to agree with our intuitions in many cases, but it is not clear how it can be used directly to construct word classes and corresponding models of association. Our research addresses some of the same questions and uses similar raw data, but we investigate how to factor word association tendencies into associations of words to certain hidden senses classes and associations between the classes themselves. While it may be worth basing such a model on preexisting sense classes (Resnik, 1992), in the work described here we look at how to derive the classes directly from distributional data. More specifically, we model senses as probabilistic concepts or clusters c with corresponding cluster membership probabilities p(clw ) for each word w. Most other class-based modeling techniques for natural language rely instead on """"hard"""" Boolean classes (Brown et al., 1990). Class construction is then combinatorially very demanding and depends on frequency counts for joint events involving particular words, a potentially unreliable source of information as noted above. Our approach avoids both problems. P r o b l e m S e t t i n g In what follows, we will consider two major word classes, 12 and Af, for the verbs and nouns in our experiments, and a single relation between them, in our experiments the relation between a transitive main verb and the head noun of its direct object. Our raw knowledge about the relation consists of the frequencies f~n of occurrence of particular pairs (v,n) in the required configuration in a training corpus. Some form of text analysis is required to collect such a collection of pairs. The corpus used in our first experiment was derived from newswire text automatically parsed by"""	cluster analysis;computer cluster;control theory;distortion;experiment;information source;kullback–leibler divergence;language model;natural language;neural coding;parsing;similarity measure;simulated annealing;table (information);test data;text corpus;text mining	Fernando Pereira;Naftali Tishby;Lillian Lee	1993			complete-linkage clustering;correlation clustering;constrained clustering;k-medians clustering;fuzzy clustering;computer science;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;cluster analysis;kullback–leibler divergence;single-linkage clustering;brown clustering;statistics	NLP	-25.531825310725594	-73.82612486296472	35013
af8b3a31fe92e4235a19b65e7275d1891a983668	monitoring disaster impact: detecting micro-events and eyewitness reports in mainstream and social media		This paper approaches the problem of monitoring the impact of the disasters by mining web sources for the events, caused by these disasters. We refer to these disaster effects as “micro-events”. Micro-events typically following a large disaster include casualties, damage on infrastructures, vehicles, services and resource supply, as well as relief operations. We present natural language grammar learning algorithms which form the basis for building micro-event detection systems from data, with no or minor human intervention, and we show how they can be applied to mainstream news and social media for monitoring disaster impact. We also experimented with applying statistical classifiers to distill, from social media situational updates on disasters, eyewitness reports from directly affected people. Finally, we describe a Twitter mining robot, which integrates some of these monitoring techniques and is intended to serve as a multilingual content hub for enhancing situational awareness.	algorithm;machine learning;natural language;sensor;social media;statistical classification;usb hub	Hristo Tanev;Vanni Zavarella;Josef Steinberger	2017				AI	-21.72129245852612	-56.04376129745127	35020
a6dd73c7b4d334b196d12d07763c74ffad9b5dea	clinicalkey: terminology driven semantic search.		In this Information Age, we have a variety of sources to fulfill our information needs. More and more of the data is available online, and search engines such as Google, provide us with powerful tools to find specific pieces of information. However, the data is growing exponentially resulting in an Information Overload [Bergamaschi & Guerra]. This phenomenon is all too common in the healthcare domain too where clinicians are spending increasing amounts of time filtering out useless information to find what they are looking for.	information needs;information overload;semantic search;web search engine	Sivaram Arabandi;Helen Moran	2012			information retrieval;terminology;semantic search;clinicalkey;computer science	Metrics	-32.14056809271933	-55.79787240464296	35026
b5f04f5fccd20994f6fb1d5e615dd363e0b24f41	web image retrieval via learning semantics of query image	cbir;textual semantics;semantics learning;semantic representation;learning artificial intelligence content based retrieval image retrieval internet;query image;web image retrieval;data mining;image retrieval large scale systems image databases laboratories pattern recognition automation internet information retrieval content based retrieval feedback;large scale;visualization;internet;machine learning;streaming media;image color analysis;feature selection web image retrieval semantics learning;feature extraction;semantic gap;multimedia communication;feature selection;learning artificial intelligence;machine learning web image retrieval semantic gap textual semantics query image cbir semantic representation similarity measure;similarity measure;content based retrieval;image retrieval	The performance of traditional image retrieval approaches remains unsatisfactory, as they are restricted by the wellknown semantic gap and the diversity of textual semantics. To tackle these problems, we propose an improved image retrieval framework when querying with an image. The framework considers not only the discriminative power of various visual properties but also the semantic representation of the query image. Given a query image, we first perform CBIR to obtain some visually similar image sets corresponding to different visual properties separately. Then, a semantic representation to the query image is learnt from each image set. The semantic consistence among the textual indexes of each image set is measured in order to judge the confidence of various visual properties and the obtained semantic representation in search. Obtaining these items, both visually and semantically relevant images are returned to the user by a combined similarity measure. Experiments on a large-scale web images demonstrate the effectiveness and potential of the proposed framework.	content-based image retrieval;experiment;similarity measure	Chuanghua Gui;Changsheng Xu;Hanqing Lu	2009	2009 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2009.5202782	feature detection;query expansion;visual word;the internet;visualization;feature extraction;image retrieval;computer science;machine learning;pattern recognition;data mining;feature selection;automatic image annotation;information retrieval;semantic gap	Vision	-15.254725925749538	-58.60265992954013	35028
02a2178517abc5e5edc0c4a10bedbdb569b9ebc3	kinematics-based extraction of salient 3d human motion data for summarization of choreographic sequences		Capturing, documenting and storing Intangible Cultural Heritage content has been recently enabled at unprecedented volume and quality levels through a variety of sensors and devices. When it comes to the performing arts, and mainly dance and kinesiology, the massive amounts of RGB-D and 3D skeleton data produced by video and motion capture devices the huge number of different types of existing dances and variations thereof, dictate the need for organizing, indexing, archiving, retrieving and analyzing dance-related cultural content in a tractable fashion and with lower computational and storage resource requirements. In this context, we present a novel framework based on kinematics modeling for the extraction of salient 3D human motion data from real-world choreographic sequences. Two approaches are proposed: a clustering-based method for the selection of the basic primitives of a choreography, and a kinematics-based method that generates meaningful summaries at hierarchical levels of granularity. The dance summarization framework has been successfully validated and evaluated with two real-world datasets and with the participation of dance professionals and domain experts.		Athanasios Voulodimos;Nikolaos D. Doulamis;Anastasios D. Doulamis;Ioannis Rallis	2018	2018 24th International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2018.8545078	skeleton (computer programming);automatic summarization;computer vision;search engine indexing;data mining;cluster analysis;artificial intelligence;choreography;dance;motion capture;kinematics;computer science	Robotics	-14.581448852475642	-56.95091207019614	35056
3ef5f2b717e60caeec6b82a8a56de9ea6acf9191	theory and practice of ambiguity labelling with a view to interactive disambiguation in text and speech mt	labelling ambiguity;mulitilingual data;ambiguity occurrence;ambiguous result;partial interactive disambiguation;ambiguity scope;speech mt;correct interpretation;dialogue transcription;automatic analyzer;ambiguity kernel	In many contexts, automatic analyzers cannot fully disambiguate a sentence or an utterance reliably, but can produce ambiguous results containing the correct interpretation. It is useful to study vatious properties of these ambiguities in the view of subsequent total or partial interactive disambiguation. We have proposed a technique for labelling ambiguities in texts and in dialogue transcriptions, and experimented it on multilingual data. It has been first necessary to define formally the very notion of ambiguity relative to a representation system, as well as associated concepts such as ambiguity kernel, ambiguity scope, ambiguity occurrence.	word-sense disambiguation	Christian Boitet;Mutsuko Tomokiyo	1996			natural language processing;pattern recognition	NLP	-28.057437871142255	-78.720734907592	35203
ba2fff5eac9b77cbb6883f35d680d0bdb6d1da25	word segmentation as graph partition		We propose a new approach to the Chinese word segmentation problem that considers the sentence as an undirected graph, whose nodes are the characters. One can use various techniques to compute the edge weights that measure the connection strength between characters. Spectral graph partition algorithms are used to group the characters and achieve word segmentation. We follow the graph partition approach and design several unsupervised algorithms, and we show their inspiring segmentation results on two corpora: (1) electronic health records in Chinese, and (2) benchmark data from the Second International Chinese Word Segmentation Bakeoff.	algorithm;benchmark (computing);computation;graph (discrete mathematics);graph partition;laplacian matrix;markov chain;point of view (computer hardware company);text corpus;text segmentation;unsupervised learning	Yuanhao Liu;Sheng Yu	2018	CoRR		machine learning;artificial intelligence;graph partition;segmentation;computer science;text segmentation;graph;sentence	NLP	-21.033851410310753	-73.89657999233667	35407
376529c7585902a0d4f64f160eae09ddcf22f323	developing learner corpus annotation for chinese grammatical errors	pragmatics;tongue;writing;computational linguistics;europe;usability;tagging	This study describes the construction of the TOCFL (Test Of Chinese as a Foreign Language) learner corpus, including the collection and grammatical error annotation of 2,837 essays written by Chinese language learners originating from a total of 46 different mother-tongue languages. We propose hierarchical tagging sets to manually annotate grammatical errors, resulting in 33,835 inappropriate usages. Our built corpus has been provided for the shared tasks on Chinese grammatical error diagnosis. These demonstrate the usability of our learner corpus annotation.	usability	Lung-Hao Lee;Li-Ping Chang;Yuen-Hsien Tseng	2016	2016 International Conference on Asian Language Processing (IALP)	10.1109/IALP.2016.7875980	natural language processing;speech recognition;usability;computer science;computational linguistics;corpus linguistics;text corpus;linguistics;writing;pragmatics	NLP	-28.028318850567604	-75.88734868549271	35415
1651b6d815aaf34a836bd67d1d21bcd7e17edf21	automatic extraction of fixed multiword expressions	sensibilidad contexto;collocations;lenguaje natural;learning algorithm;context aware;analisis estadistico;supervised learning;support vector machines;speech processing;langage naturel;tratamiento palabra;chaine caractere;traitement parole;automatic extraction;intelligence artificielle;algorithme apprentissage;probabilistic approach;classification;supervised machine learning;statistical analysis;enfoque probabilista;approche probabiliste;natural language;contexto;multiword expressions;cadena caracter;analyse statistique;part of speech;contexte;artificial intelligence;t score;inteligencia artificial;apprentissage supervise;sensibilite contexte;aprendizaje supervisado;algoritmo aprendizaje;clasificacion;context;character string	Fixed multiword expressions are strings of words which together behave like a single word. This research establishes a method for the automatic extraction of such expressions. Our method involves three stages. In the first, a statistical measure is used to extract candidate bigrams. In the second, we use this list to select occurrences of candidate expressions in a corpus, together with their surrounding contexts. These examples are used as training data for supervised machine learning, resulting in a classifier which can identify target multiword expressions. The final stage is the estimation of the part of speech of each extracted expression based on its context of occurence. Evaluation demonstrated that collocation measures alone are not effective in identifying target expressions. However, when trained on one million examples, the classifier identified target multiword expressions with precision greater than 90%. Part of speech estimation had precision and recall of over 95%.	bigram;collocation;machine learning;minimal working example;natural language processing;precision and recall;speech synthesis;statistical classification;supervised learning;text corpus	Campbell Hore;Masayuki Asahara;Yuji Matsumoto	2005		10.1007/11562214_50	natural language processing;support vector machine;standard score;speech recognition;string;biological classification;part of speech;computer science;artificial intelligence;machine learning;speech processing;linguistics;supervised learning;natural language	NLP	-25.848880372273758	-78.60162087683442	35434
30c8833127535d112347e7b4cab588b8924bc7b7	automatic sales lead generation from web data	corporate acquisitions;seminars;marketing and sales data mining companies decision making web sites consumer electronics corporate acquisitions training data seminars advertising;consumer electronics;data mining;companies;automatic generation;training data;named entity recognition;decision making process;web sites;world wide web;direct marketing;marketing and sales;new products;advertising	Speed to market is critical to companies that are driven by sales in a competitive market. The earlier a potential customer can be approached in the decision making process of a purchase, the higher are the chances of converting that prospect into a customer. Traditional methods to identify sales leads such as company surveys and direct marketing are manual, expensive and not scalable. Over the past decade the World Wide Web has grown into an information-mesh, with most important facts being reported through Web sites. Several news papers, press releases, trade journals, business magazines and other related sources are on-line. These sources could be used to identify prospective buyers automatically. In this paper, we present a system called ETAP (Electronic Trigger Alert Program) that extracts trigger events from Web data that help in identifying prospective buyers. Trigger events are events of corporate relevance and indicative of the propensity of companies to purchase new products associated with these events. Examples of trigger events are change in management, revenue growth and mergers & acquisitions. The unstructured nature of information makes the extraction task of trigger events difficult. We pose the problem of trigger events extraction as a classification problem and develop methods for learning trigger event classifiers using existing classification methods. We present methods to automatically generate the training data required to learn the classifiers. We also propose a method of feature abstraction that uses named entity recognition to solve the problem of data sparsity. We score and rank the trigger events extracted from ETAP for easy browsing. Our experiments show the effectiveness of the method and thus establish the feasibility of automatic sales lead generation using the Web data.	experiment;finite-state machine;named-entity recognition;need to know;online and offline;prospective search;relevance;scalability;sparse matrix;web page;weka;world wide web	Ganesh Ramakrishnan;Sachindra Joshi;Sumit Negi;Raghu Krishnapuram;Sreeram Balakrishnan	2006	22nd International Conference on Data Engineering (ICDE'06)	10.1109/ICDE.2006.28	decision-making;training set;computer science;direct marketing;data mining;database	DB	-20.93948108243912	-52.8876116084386	35436
9bf08f91d08e9302af10a61d4dc525513bd7d838	a sophisticated library search strategy using folksonomies and similarity matching	search engines;document retrieval	Libraries, private and public, offer valuable resources to library patrons. As of today the only way to locate information archived exclusively in libraries is through their catalogs. Library patrons, however, often find it difficult to formulate a proper query, which requires using specific keywords assigned to different fields of desired library catalog records, to obtain relevant results. These improperly formulated queries often yield irrelevant results or no results at all. This negative experience in dealing with existing library systems turn library patrons away from library catalogs; instead, they rely on Web search engines to perform their searches first and upon obtaining the initial information (such as titles, subject headings, or authors) on the desired library materials, they query library catalogs. This searching strategy is an evidence of failure of today’s library systems. In solving this problem, we propose an enhanced library system, which allows partial, similarity matching of (i) tags defined by ordinary users at a folksonomy site that describe the content of books and (ii) unrestricted keywords specified by an ordinary library patron in a query to search for relevant library catalog records. The proposed library system allows patrons posting a query Q using commonly-used words and ranks the retrieved results according to their	archive;book;controlled vocabulary;folksonomy;fuzzy set;image scaling;library (computing);library classification;library of congress subject headings;logical connective;public library;relevance;user-generated content;web search engine	Maria Soledad Pera;William B. Lund;Yiu-Kai Ng	2009	JASIST	10.1002/asi.21072	document retrieval;computer science;data mining;world wide web;information retrieval;information system	HPC	-32.392522901258324	-57.16499910305913	35447
d3511cd8946bf46de1b5a885fda4f57b639408a8	system for monitoring natural disasters using natural language processing in the social network twitter	databases;filtering;artificial intelligence;twitter;fires;natural language processing	This paper presents the design and implementation of an automated system that allows monitoring the social network Twitter, making a connection to the API, to filter content according to four categories (volcanic, telluric, fires and climatological) which affect Ecuador because of its geographical location, taking into account that these cannot be easily predicted, and stores all tweets in a database for analysis. The filtering process is performed by using the NLTK tool with which the frequency of a word is determined within a tweet, to be classified later in one of the proposed categories. The results for each category are displayed on a web page that contains real-time statistics of the database. This work provides access to information on natural disasters because they are classified.	application programming interface;freedom of information laws by country;location (geography);natural language toolkit;natural language processing;real-time web;social network;web page	Miguel Maldonado;Darwin Alulema;Derlin Morocho;Marida Proano	2016	2016 IEEE International Carnahan Conference on Security Technology (ICCST)	10.1109/CCST.2016.7815686	geography;data science;data mining;world wide web	Robotics	-23.299113639347222	-55.10761892285185	35461
6ba91b0ff46d181eb79fb254f35c7afd8e29b54c	word-level language identification and back transliteration of romanized text	transliteration;n grams;word level language classification	This paper presents the BMSCE team's participation in `FIRE Shared Task on Transliterated Search subtask-1'. Our Language Identification system is based on the n-grams approach and uses a tri-gram language identifier trained over a shared and collected training set to classify the language of a word at the. We use a rule based approach blended with simple dictionary search to back transliterate the Romanized Kannada word. We participated in the Bengali-English, Guajarati-English, Kannada-English, Malayalam-English and Tamil-English language tracks and have obtained 70-80% accuracy for the language pairs.	dictionary;grams;identifier;language identification;n-gram;structured english;test set;triangular function	Royal Denzil Sequiera;Shashank S. Rao;B. R. Shambavi	2014		10.1145/2824864.2824871	natural language processing;language identification;speech recognition;linguistics	NLP	-23.02235494714417	-70.97077954153275	35476
220a67d97b39463b2079b352007e4de1c53b6d8b	speculation detection for chinese clinical notes: impacts of word segmentation and embedding models	chinese nlp;speculation detection;word segmentation;word embedding;clinical nlp;natural language processing	Speculations represent uncertainty toward certain facts. In clinical texts, identifying speculations is a critical step of natural language processing (NLP). While it is a nontrivial task in many languages, detecting speculations in Chinese clinical notes can be particularly challenging because word segmentation may be necessary as an upstream operation. The objective of this paper is to construct a state-of-the-art speculation detection system for Chinese clinical notes and to investigate whether embedding features and word segmentations are worth exploiting toward this overall task. We propose a sequence labeling based system for speculation detection, which relies on features from bag of characters, bag of words, character embedding, and word embedding. We experiment on a novel dataset of 36,828 clinical notes with 5103 gold-standard speculation annotations on 2000 notes, and compare the systems in which word embeddings are calculated based on word segmentations given by general and by domain specific segmenters respectively. Our systems are able to reach performance as high as 92.2% measured by F score. We demonstrate that word segmentation is critical to produce high quality word embedding to facilitate downstream information extraction applications, and suggest that a domain dependent word segmenter can be vital to such a clinical NLP task in Chinese language.		Shaodian Zhang;Tian Kang;Xingting Zhang;Dong Wen;Noémie Elhadad;Jianbo Lei	2016	Journal of biomedical informatics	10.1016/j.jbi.2016.02.011	natural language processing;text segmentation;speech recognition;computer science;artificial intelligence;machine learning	NLP	-25.525059256658768	-74.38142323277992	35491
9523d525d14d545ee7e13006e00b69550ff8367d	cineast: a multi-feature sketch-based video retrieval engine	histograms;motion based video retrieval video retrieval content based information retrieval;semantics;video retrieval;video sequences;content based information retrieval;video retrieval content based retrieval image colour analysis image retrieval image sequences;feature extraction image color analysis vectors histograms video sequences image edge detection semantics;vectors;image edge detection;image color analysis;feature extraction;motion based video retrieval;general purpose video collection cineast multifeature sketch based video retrieval engine video collections use cases standard keyword search manual annotations content based image retrieval search paradigms academic prototypes commercial search engines sketch enabled devices intrinsic image features content based video retrieval video sequences color sketches	Despite the tremendous importance and availability of large video collections, support for video retrieval is still rather limited and is mostly tailored to very concrete use cases and collections. In image retrieval, for instance, standard keyword search on the basis of manual annotations and content-based image retrieval, based on the similarity to query image (s), are well established search paradigms, both in academic prototypes and in commercial search engines. Recently, with the proliferation of sketch-enabled devices, also sketch-based retrieval has received considerable attention. The latter two approaches are based on intrinsic image features and rely on the representation of the objects of a collection in the feature space. In this paper, we present Cineast, a multi-feature sketch-based video retrieval engine. The main objective of Cineast is to enable a smooth transition from content-based image retrieval to content-based video retrieval and to support powerful search paradigms in large video collections on the basis of user-provided sketches as query input. Cineast is capable of retrieving video sequences based on edge or color sketches as query input and even supports one or multiple exemplary video sequences as query input. Moreover, Cineast also supports a novel approach to sketch-based motion queries by allowing a user to specify the motion of objects within a video sequence by means of (partial) flow fields, also specified via sketches. Using an emergent combination of multiple different features, Cineast is able to universally retrieve video (sequences) without the need for prior knowledge or semantic understanding. The evaluation with a general purpose video collection has shown the effectiveness and the efficiency of the Cineast approach.	content-based image retrieval;emergence;feature vector;search algorithm;sketch;web search engine	Luca Rossetto;Ivan Giangreco;Heiko Schuldt	2014	2014 IEEE International Symposium on Multimedia	10.1109/ISM.2014.38	video compression picture types;computer vision;query expansion;visual word;feature extraction;image retrieval;computer science;machine learning;video tracking;histogram;semantics;multimedia;smacker video;motion compensation;video post-processing;automatic image annotation;information retrieval;statistics;human–computer information retrieval	Vision	-14.52528888052811	-56.81548321033218	35498
2baf77a9c2c5a37cb43a7e666eba04c6625f5ab8	database selection and result merging in p2p web search	search engine;pseudo relevance feedback;p2p;web search engine;web search;peer to peer	Intelligent Web search engines are extremely popular now. Currently, only commercial centralized search engines like Google can process terabytes of Web data. Alternative search engines fulfilling collaborative Web search on a voluntary basis are usually based on a blooming Peer-to-Peer (P2P) technology. In this paper, we investigate the effectiveness of different database selection and result merging methods in the scope of P2P Web search engine Minerva. We adapt existing measures for database selection and results merging, all directly derived from popular document ranking measures, to address the specific issues of P2P Web search. We propose a general approach to both tasks based on the combination of pseudo-relevance feedback methods. From experiments with TREC Web data, we observe that pseudo-relevance feedback improves quality of distributed information retrieval.	centralized computing;consistency model;database;experiment;information retrieval;peer-to-peer;ranking (information retrieval);relevance feedback;terabyte;web search engine	Sergey Chernov;Pavel Serdyukov;Matthias Bender;Sebastian Michel;Gerhard Weikum;Christian Zimmer	2005		10.1007/978-3-540-71661-7_3	web service;search engine indexing;web modeling;site map;database search engine;web query classification;data web;metasearch engine;web design;web search engine;semantic search;computer science;spamdexing;web crawler;peer-to-peer;data mining;database;search analytics;web search query;world wide web;information retrieval;search engine	Web+IR	-31.119995032241526	-55.20080628651379	35543
509d5a0f18f7cb386621b18f48f073c3a437c207	chinese patent mining based on sememe statistics and key-phrase extraction	statistical approach;key phrase;patent;sememe statistics;term frequency;data mining;knowledge structure	Recently, key-phrase extraction from patent document has received considerable attention. However, the current statistical approaches of Chinese key-phrase extraction did not realize the semantic comprehension, thereby resulting in inaccurate and partial extraction. In this study, a Chinese patent mining approach based on sememe statistics and key-phrase extraction has been proposed to extract key-phrases from patent document. The key-phrase extraction algorithm is based on semantic knowledge structure of HowNet, and statistical approach is adopted to calculate the chosen value of the phrase in the patent document. With an experimental data set, the results showed that the proposed algorithm had improvements in recall from 62% to 73% and in precision from 72% to 81% compared with term frequency statistics algorithm.		Bo Jin;Hongfei Teng;Yanjun Shi;Fuzheng Qu	2007		10.1007/978-3-540-73871-8_48	computer science;pattern recognition;data mining;tf–idf;information retrieval	ML	-26.819775671539816	-66.45253462569653	35566
be7c3ef51f12440ede91518ebf1ce87368167ca6	improved modeling of out-of-vocabulary words using morphological classes	morphological class;unknown history;perplexity improvement;clusters rare word;art kneser-ney model;class-based language model;similar morphology;out-of-vocabulary word;improved modeling	• Partition the vocabulary into a set of equivalence classes • Examples include semantic classes: last names, small dollar amounts, city names, . . . ; syntactic classes: part of speech (PoS) tags; or clusters: words with a similar distribution • Clusters usually work best (Niesler et al., 1998), but you need contextual information to define them • Model the word transition probability by a class transition probability and a word emission probability	brown clustering;cluster analysis;continuation;domain adaptation;experiment;galaxy morphological classification;kneser–ney smoothing;language model;machine translation;perplexity;statistical classification;unsupervised learning;vocabulary	Thomas Müller;Hinrich Schütze	2011			natural language processing;speech recognition;computer science	NLP	-24.312319076992928	-78.30906865415484	35567
53e4279933597937aed23eef196f714ab6b82d0c	issues in the unsupervised clustering of web documents			cluster analysis;web page	Mark P. Sinka	2006				NLP	-24.96140790886156	-61.315955669133956	35603
576be5ee93ceec0b550d95321b9a7e004f5ce704	automatic lexicon enhancement by means of corpus tagging		Using specialised text corpus to automatically enhance a general lexicon is the aim of this study. Indeed, having lexicons which offer maximal cover on a specific topic is an important benefit in many applications of Automatic Speech and Natural Language Processing. The enhancement of these lexicons can be made automatic as big corpora of specialised texts are available. A syntactic tagging process, based on 3-class and 3-gram language models, allows us to automatically allocate possible syntactic categories to the Out-Of-Vocabulary (OOV) words which are found in the corpus processed. These OOV words generally occur several times in the corpus, and a number of these occurrences can be important. By taking into account all the occurrences of an OOV word in a given text as a whole, we propose here a method for automatically extracting a specialised lexicon from a text corpus which is representative of a specific topic.	cpu cache;iso/iec 10967;language model;lexicon;maximal set;natural language processing;resultant;text corpus;transcription (software);vocabulary	Frédéric Béchet;Thierry Spriet;Marc El-Bèze	1997			natural language processing;speech recognition;text corpus;linguistics	NLP	-24.935891965706297	-76.73852996397643	35608
4799fa5b66cb144d1130d8c88aca7496cb58b984	extraction of chinese multiword expressions based on artificial neural network with feedbacks	present idiosyncratic feature;word alignment method;extraction system;satisfying performance;bilingual corpus;multiword expressions extraction;statistical machine translation;chinese multiword expression;artificial neural network;natural language processing;alignment information	Multiword Expressions present idiosyncratic features in the application of Natural Language Processing. This paper focuses on Multiword Expressions extraction from bilingual corpus with alignment information constructed by Statistical Machine Translation (SMT) and word alignment method. A pattern based extraction system and an Artificial Neural Network (ANN) with feedback are applied for extracting MWEs. The results show that both of these approaches can achieve satisfying performance.		Yiwen Fu;Naisheng Ge;Zhongguang Zheng;Shu Zhang;Yao Meng;Hao Yu	2012		10.1007/978-3-642-32695-0_65	natural language processing;speech recognition;computer science	NLP	-23.4617201358271	-77.68010094459908	35620
b2339a35b1f9796c7a3792b47ca0cd7f0fe6c69a	character-level deep conflation for business data analytics		Connecting different text attributes associated with the same entity (conflation) is important in business data analytics since it could help merge two different tables in a database to provide a more comprehensive profile of an entity. However, the conflation task is challenging because two text strings that describe the same entity could be quite different from each other for reasons such as misspelling. It is therefore critical to develop a conflation model that is able to truly understand the semantic meaning of the strings and match them at the semantic level. To this end, we develop a character-level deep conflation model that encodes the input text strings from character level into finite dimension feature vectors, which are then used to compute the cosine similarity between the text strings. The model is trained in an end-to-end manner using back propagation and stochastic gradient descent to maximize the likelihood of the correct association. Specifically, we propose two variants of the deep conflation model, based on long-short-term memory (LSTM) recurrent neural network (RNN) and convolutional neural network (CNN), respectively. Both models perform well on a real-world business analytics dataset and significantly outperform the baseline bag-of-character (BoC) model.	artificial neural network;backpropagation;baseline (configuration management);business analytics;convolutional neural network;cosine similarity;end-to-end principle;long short-term memory;random neural network;recurrent neural network;software propagation;stochastic gradient descent;string (computer science)	Zhe Gan;P. D. Singh;Ameet Joshi;Xiaodong He;Jianshu Chen;Jianfeng Gao;Li Deng	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7952551	natural language processing;computer science;artificial intelligence;data science;machine learning;data mining	DB	-17.509784166063067	-72.12937866035863	35636
9921cd659d843528395c331df6b40ccd2f1fcb60	is it positive or negative? on determining erp components	symbol manipulation;component definition;bioelectric potentials;half wave encoding;statistical analysis bioelectric potentials electroencephalography symbol manipulation data analysis encoding;general polarization;indexing terms;enterprise resource planning encoding statistics testing morphology delay data analysis histograms voltage control natural languages;event related brain potential;data analysis;word statistics;statistical analysis;language processing;half wave encoding negative erp components positive erp components event related brain potentials experimental condition control condition pattern morphology nonlinear data analysis language processing generalized polarity histogram symbolic dynamics;algorithms computer simulation electroencephalography evoked potentials humans models neurological models statistical reproducibility of results sensitivity and specificity signal processing computer assisted;coherence;symbolic dynamics;electroencephalography;event related potential erp;event related potentials erp;encoding	In most experiments using event-related brain potentials (ERPs), there is a straightforward way to define-on theoretical grounds-which of the conditions tested is the experimental condition and which is the control condition. If, however, theoretical assumptions do not give sufficient and unambiguous information to decide this question, then the interpretation of an ERP effect becomes difficult, especially if one takes into account that certain effects can be both a positivity or a negativity on the basis of the morphology of the pattern as well as with respect to peak latency (regard for example, N400 and P345). Exemplified with an ERP experiment on language processing, we present such a critical case and offer a possible solution on the basis of nonlinear data analysis. We show that a generalized polarity histogram, the word statistics of symbolic dynamics, is in principle able to distinguish negative going ERP components from positive ones when an appropriate encoding strategy, the half wave encoding is employed. We propose statistical criteria which allow to determine ERP components on purely methodological grounds.	erp;experiment;mathematical morphology;mcgurk effect;negativity (quantum mechanics);nonlinear system;polarity	Peter beim Graben;Stefan Frisch	2004	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2004.827558	natural language processing;symbolic dynamics;speech recognition;index term;coherence;electroencephalography;computer science;mathematics;data analysis;communication;physics;encoding;statistics	ML	-9.814508644418519	-79.2407690085863	35639
83190a833d03dea5ea82be7999ed1c048540eed5	where to stop reading a ranked list	evaluation measure	Abstract : We document our participation in the TREC 2008 Legal Track. This year we focused solely on selecting rank cut-offs for optimizing the given evaluation measure per topic.		Avi Arampatzis;Jaap Kamps	2008			computer science;data mining;world wide web;information retrieval	NLP	-31.82419993249775	-62.95686373024522	35641
80348ec60c91c8a62f7c6c72f1fde03cfb6dbed3	neural character-level dependency parsing for chinese		This paper presents a truly full character-level neural dependency parser together with a newly released character-level dependency treebank for Chinese, which has suffered a lot from the dilemma of defining word or not to model character interactions. Integrating full character-level dependencies with character embedding and human annotated characterlevel part-of-speech and dependency labels for the first time, we show an extra performance enhancement from the evaluation on Chinese Penn Treebank and SJTU (Shanghai Jiao Tong University) Chinese Character Dependency Treebank and the potential of better understanding deeper structure of Chinese sentences.	brown corpus;interaction;null character;parsing;point of sale;treebank	Haonan Li;Zhisong Zhang;Yuqi Ju;Hai Zhao	2018			machine learning;computer science;artificial intelligence;dependency grammar	AI	-18.669037119931787	-73.20539519509995	35642
4d035638f614dd648ef5c531797f3e826d329f13	modelling and detecting changes in user satisfaction	concept drift;query reformulation;information retrieval	Informational needs behind queries, that people issue to search engines, are inherently sensitive to external factors such as breaking news, new models of devices, or seasonal changes as 'black Friday'. Mostly these changes happen suddenly and it is natural to suppose that they may cause a shift in user satisfaction with presented old search results and push users to reformulate their queries. For instance, if users issued the query 'CIKM conference' in 2013 they were satisfied with results referring to the page cikm2013.org and this page gets a majority of clicks. However, the confernce site has been changed and the same query issued in 2014 should be linked to the different page cikm2014.fudan.edu.cn. If the link to the fresh page is not among the retrieved results then users will reformulate the query to find desired information.  In this paper, we examine how to detect changes in user satisfaction if some events affect user information goals but search results remained the same. We formulate a problem using concept drift detection techniques. The proposed method works in an unsupervised manner, we do not rely on any labelling. We report results of a large scale evaluation over real user interactions, that are collected by a commercial search engine within six months. The final datasets consist of more than sixty millions log entries. The results of our experiments demonstrate that by using our method we can accurately detect changes in user behavior. The detected drifts can be used to enhance query auto-completion, user satisfaction metrics, and recency ranking.	computer user satisfaction;concept drift;conference on information and knowledge management;experiment;interaction;rsa conference;ranking (information retrieval);sensor;unsupervised learning;web search engine	Julia Kiseleva;Eric Crestan;Riccardo Brigo;Roland Ditte	2014		10.1145/2661829.2661960	query expansion;web query classification;ranking;computer user satisfaction;computer science;artificial intelligence;concept drift;machine learning;data mining;database;world wide web;information retrieval	Web+IR	-31.398960263706055	-53.457052965198436	35675
57e44a952ed4c94ce7da3991ac0341d65068ffe8	a structure-based search engine for phylogenetic databases	databases;organisms;biology computing;information resources;search engine;query tree;history;phylogeny;search engines;information retrieval;phylogenetic databases;search methods;relational database;structure based search engine;searching;visualization;phylogenetic tree;taxa;web sites;search engines phylogeny databases organisms history search methods computer science web sites genetic mutations visualization;phylogenetic trees;world wide web;aligned sequence data search;relational databases;relational databases biology computing search engines information retrieval tree searching scientific information systems information resources;genetic mutations;computer science;tree searching;relational database structure based search engine phylogenetic databases phylogenetic trees organisms taxa searching keyword type search aligned sequence data search query tree query structure world wide web;keyword type search;scientific information systems;query structure	Phylogenetic trees are essential for understanding the relationships among organisms or taxa. Many of the current techniques for searching phylogenetic repositories allow the user to perform a keyword-type search or an aligned sequence data search, or to browse a hierarchical list of taxa. Here we describe a new search engine that allows the user to present an example phylogeny, or a query tree, and then searches a phylogenetic database for trees that contain the query structure. The presented search engine is fully operational and is available on the World Wide Web.	approximation algorithm;browsing;phylogenetic tree;phylogenetics;query language;treebase;user interface;web search engine;world wide web;xml	Huiyuan Shan;Katherine G. Herbert;William H. Piel;Dennis Shasha;Jason Tsong-Li Wang	2002		10.1109/SSDM.2002.1029699	phylogenetic tree;relational database;computer science;bioinformatics;data mining;database;information retrieval;search engine;phylogenetics	DB	-5.064134393626827	-59.68573523479827	35749
3fa3f8a8e49cc1f2e2ac57f5f956d442b660f302	acecloud: molecular dynamics simulations in the cloud	journal article	We present AceCloud, an on-demand service for molecular dynamics simulations. AceCloud is designed to facilitate the secure execution of large ensembles of simulations on an external cloud computing service (currently Amazon Web Services). The AceCloud client, integrated into the ACEMD molecular dynamics package, provides an easy-to-use interface that abstracts all aspects of interaction with the cloud services. This gives the user the experience that all simulations are running on their local machine, minimizing the learning curve typically associated with the transition to using high performance computing services.	abstract summary;amazon web services;amazona;cloud computing;computer simulation;interface device component;learning disorders;molecular dynamics;supercomputer	Matt J. Harvey;Gianni De Fabritiis	2015	Journal of chemical information and modeling	10.1021/acs.jcim.5b00086	simulation;chemistry;computer science;theoretical computer science;distributed computing	HPC	-6.582243760246327	-57.802691597848245	35763
51ae6ffb62ce022a99ce6f3fd7e92e6b31c72704	spelling error patterns in brazilian portuguese		Fifty years after Damerau set up his statistics for the distribution of errors in typed texts, his findings are still used in a range of different languages. Because these statistics were derived from texts in English, the question of whether they actually apply to other languages has been raised. We address this issue through the analysis of a set of typed texts in Brazilian Portuguese, deriving statistics tailored to this language. Results show that diacritical marks play a major role, as indicated by the frequency of mistakes involving them, thereby rendering Damerau's original findings mostly unfit for spelling correction systems, although still holding them useful, should one set aside such marks. Furthermore, a comparison between these results and those published for Spanish show no statistically significant differences between both languages—an indication that the distribution of spelling errors depends on the adopted character set rather than the language itself.	character encoding;null character;text corpus;text editor;typing	Priscila A. Gimenes;Norton Trevisan Roman;Ariadne M. B. R. Carvalho	2015	Computational Linguistics	10.1162/COLI_a_00216	natural language processing;speech recognition;computer science;linguistics	NLP	-28.0553551782523	-74.74918847243775	35796
f5c44dae5797d10e51609168e59eb474eed595c2	unsupervised anchor space generation for similarity measurement of general audio	unsupervised learning;audio segmentation;pattern clustering;document handling;audio document similarity measurement;audio signal processing;probability;anchor space;audio content analysis;audio segmentation audio similarity computation anchor space audio content analysis;audio retrieval;information retrieval;audio clip;indexing terms;spectral clustering;extraterrestrial measurements signal analysis feature extraction space technology layout asia fluctuations signal processing content based retrieval event detection;semantic category;feature extraction;unsupervised learning audio signal processing document handling feature extraction information retrieval pattern clustering probability;audio retrieval system;unsupervised anchor space generation;semantic space;audio document similarity measurement unsupervised anchor space generation audio clip semantic category probability spectral clustering audio retrieval system feature extraction;similarity measure;audio similarity computation	Reliably measuring similarity between audio clips is critical to many applications. As opposed to the conventional way of measuring audio similarity using low-level features directly, in this paper we consider the similarity computation using an anchor space. Each dimension of such a space corresponds to a semantic category (anchor). Mapping an audio clip onto this space results in a vector, which indicates the membership probability of this audio clip with respect to each semantic category. The more similar the mappings of two audio clips, the more similar they are. While an anchor space is typically generated in a supervised fashion, supervised approach is infeasible in many realistic scenarios where audio content semantics is too diverse or simply unknown a priori. We therefore propose an unsupervised approach to anchor space generation. There, spectral clustering is employed to cluster the audio clips with similar low-level features and then the obtained clusters are adopted as semantic categories. Using this semantic space for audio similarity computation shows a considerable accuracy improvement (7% on mAP) in an audio retrieval system, compared with the conventional low-level feature based approach.	cluster analysis;computation;google map maker;high- and low-level;spectral clustering;supervised learning;unsupervised learning	Lie Lu;Alan Hanjalic	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4517544	unsupervised learning;computer vision;audio mining;index term;audio signal processing;feature extraction;computer science;machine learning;pattern recognition;probability;information retrieval;spectral clustering	SE	-15.585531470696848	-58.44465530833447	35817
a3d15e2d312e2f000bf746bbded7e8a8731677ed	clustering of web search results based on an iterative fuzzy c-means algorithm and bayesian information criterion	belief networks;bayesian information criterion web document clustering fuzzy c means;pattern clustering;document handling;clustering performance web search results clustering iterative fuzzy c means algorithm bayesian information criterion web clustering engines web document clustering description centric algorithm ifcwr merge operation suffix tree clustering lingo moresque dataset ambient dataset precision metric recall metric f measure metric ssl k metric clustering quality;iterative methods;internet;clustering algorithms partitioning algorithms algorithm design and analysis web search bayes methods accuracy educational institutions;pattern clustering belief networks document handling internet iterative methods	The clustering of web search has become a very interesting research area among academic and scientific communities involved in information retrieval. Clustering of web search result systems, also called Web Clustering Engines, seek to increase the coverage of documents presented for the user to review, while reducing the time spent reviewing them. Several algorithms for web document clustering already exist, but results show there is room for more to be done. This paper introduces a new description-centric algorithm for clustering of web results called IFCWR. IFCWR initially selects a maximum estimated number of clusters using Forgy's strategy, then it iteratively merges clusters until results cannot be improved. Every merge operation implies the execution of Fuzzy C-Means for clustering results of web search and the calculus of Bayesian Information Criterion for automatically evaluating the best solution and number of clusters. IFCWR was compared against other established web document clustering algorithms, among them: Suffix Tree Clustering and Lingo. Comparison was executed on AMBIENT and MORESQUE datasets, using precision, recall, f-measure, SSLk and other metrics. Results show a considerable improvement in clustering quality and performance.	algorithm;bayesian information criterion;cluster analysis;computer performance;f1 score;fuzzy clustering;information retrieval;iterative method;lingo (programming language);software quality;suffix tree clustering;web page;web search engine	Carlos Alberto Cobos Lozada;Martha Mendoza;Milos Manic;Elizabeth León Guzman;Enrique Herrera-Viedma	2013	2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS)	10.1109/IFSA-NAFIPS.2013.6608452	correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;document clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;cluster analysis;brown clustering;dbscan;biclustering;affinity propagation;clustering high-dimensional data;conceptual clustering	Web+IR	-26.728912646823574	-56.55979532966023	35851
1ea806a4c3825ebb1c2bf346c887ef53a76549cc	joint neural entity disambiguation with output space search		In this paper, we present a novel model for entity disambiguation that combines both local contextual information and global evidences through Limited Discrepancy Search (LDS). Given an input document, we start from a complete solution constructed by a local model and conduct a search in the space of possible corrections to improve the local solution from a global view point. Our search utilizes a heuristic function to focus more on the least confident local decisions and a pruning function to score the global solutions based on their local fitness and the global coherences among the predicted entities. Experimental results on CoNLL 2003 and TAC 2010 benchmarks verify the effectiveness of our model.		Hamed Shahbazi;Xiaoli Z. Fern;Reza Ghaeini;Chao Ma;Rasha Mohammad Obeidat;Prasad Tadepalli	2018			machine learning;artificial intelligence;heuristic function;computer science;pattern recognition	AI	-18.14830272521896	-69.36237380638302	35861
3024eab4e3af7ad96f405984c4e7e59de3dc26db	morphological decomposition in arabic asr systems	morphologie;prononciation;reconnaissance automatique de la parole;automatic speech recognition;morphology;arabic;pronunciation;probabilites;arabe;computational linguistics;probabilities;linguistique informatique;pronunciation probabilities	In recent years, the use of morphological decomposition strategies for Arabic Automatic Speech Recognition (ASR) has become increasingly popular. Systems trained on morphologically decomposed data are often used in combination with standard word-based approaches, and they have been found to yield consistent performance improvements. The present article contributes to this ongoing research endeavour by exploring the use of the ‘Morphological Analysis and Disambiguation for Arabic’ (MADA) tools for this purpose. System integration issues concerning language modelling and dictionary construction, as well as the estimation of pronunciation probabilities, are discussed. In particular, a novel solution for morpheme-to-word conversion is presented which makes use of an N-gram Statistical Machine Translation (SMT) approach. System performance is investigated within a multi-pass adaptation/combination framework. All the systems described in this paper are evaluated on an Arabic large vocabulary speech recognition ∗F. Diehl Email address: {fd257, mjfg, mt126, pcw}@eng.cam.ac.uk (F. Diehl, M.J.F. Gales, M. Tomalin, and P.C. Woodland) Preprint submitted to Elsevier August 25, 2011 task which includes both Broadcast News and Broadcast Conversation test data. It is shown that the use of MADA-based systems, in combination with word-based systems, can reduce the Word Error Rates by up to 8.1% relative.	acoustic cryptanalysis;automated system recovery;baseline (configuration management);citation style language;concatenation;dictionary;email;endeavour (supercomputer);experiment;language model;n-gram;orthographic projection;speech recognition;statistical machine translation;system integration;test data;vocabulary;word error rate;word-sense disambiguation	Frank Diehl;Mark J. F. Gales;Marcus Tomalin;Philip C. Woodland	2012	Computer Speech & Language	10.1016/j.csl.2011.12.001	natural language processing;speech recognition;morphology;computer science;computational linguistics;arabic;linguistics	NLP	-21.6599810793896	-79.96445303454004	35917
e6c0c1a3209ff0aed3900d32016785babf925c80	fuzzy clustering for tv program classification	digital video broadcasting;textual information;pattern clustering;fuzzy similarity matrix;fuzzy logic pattern classification pattern clustering xml meta data feature extraction digital video broadcasting;tv broadcasting;digital tv;consumer electronics;fuzzy similarity matrix tv program group recommendation fuzzy clustering program classification xml program description metadata representation textual information symbolic information program feature extraction statistics broadcasting characteristics;data mining;broadcasting characteristics;program feature extraction;fuzzy logic;fuzzy clustering;data analysis;symbolic information;feature extraction;xml educational institutions feature extraction digital tv statistics buildings consumer electronics tv broadcasting data mining data analysis;program description;xml;pattern classification;statistics;tv program group recommendation;meta data;metadata representation;program classification;buildings	In order to achieve TV program group recommendation, an approach based on fuzzy clustering is proposed for program classification. We first describe the XML based program description metadata representation, in which both textual and symbolic information is included. Secondly it presents the program feature extraction and presentation method. A program is defined as two vectors, one is based on term statistics implying what the program is about, and the other reflects broadcasting characteristics of the program. Then the classifying approach based on fuzzy clustering is proposed. The approach goes: normalizing original data, building fuzzy similarity matrix, and then clustering. The final fuzzy similarity matrix is constructed by combining two fuzzy similarity matrices calculated from two different aspects.	anytime algorithm;cluster analysis;feature extraction;fuzzy clustering;movielens;similarity measure;tv-anytime;vector graphics;xml	Zhiwen Yu;Jianhua Gu;Xingshe Zhou;Zhiyi Yang	2004	International Conference on Information Technology: Coding and Computing, 2004. Proceedings. ITCC 2004.	10.1109/ITCC.2004.1286729	defuzzification;fuzzy clustering;fuzzy classification;computer science;fuzzy number;data mining;database;fuzzy associative matrix;fuzzy set operations;information retrieval	SE	-17.65640865346102	-54.1261932930795	35920
1ed2b2fcfba1b12f60e2b43d34137e9158a0a23f	beyond sentence-level semantic role labeling: linking argument structures in discourse	null instantiation;semeval;frame semantics;semantic roles	Semantic role labeling is traditionally viewed as a sentence-level task concerned with identifying semantic arguments that are overtly realized in a fairly local context (i.e., a clause or sentence). However, this local view potentially misses important information that can only be recovered if local argument structures are linked across sentence boundaries. One important link concerns semantic arguments that remain locally unrealized (null instantiations) but can be inferred from the context. In this paper, we report on the SemEval 2010 Task-10 on ‘‘Linking Events and Their Participants in Discourse’’, that addressed this problem. We discuss the corpus that was created for this task, which contains annotations on multiple levels: predicate argument structure (FrameNet and PropBank), null instantiations, and coreference. We also provide an analysis of the task and its difficulties.	compiler;framenet;interdependence;natural language processing;neural coding;organizing (structure);programming paradigm;propbank;semeval;semantic role labeling;semi-supervised learning;text corpus;universal instantiation	Josef Ruppenhofer;Russell Lee-Goldman;Caroline Sporleder;Roser Morante	2013	Language Resources and Evaluation	10.1007/s10579-012-9201-4	natural language processing;semantic role labeling;framenet;semeval;computer science;linguistics;frame semantics	NLP	-18.445028089899782	-72.91444063354875	35924
18cbed562327c721878d7e82c338d5c1b7ba2888	constructing large proposition databases	information extraction;information retrieval;semantics;datavetenskap datalogi;knowledge discovery representation	With the advent of massive online encyclopedic corpora such as Wikipedia, it has become possible to apply a systematic analysis to a wide range of documents covering a significant part of human knowledge. Using semantic parsers, it has become possible to extract such knowledge in the form of propositions (predicate–argument structures) and build large proposition databases from these documents. This paper describes the creation of multilingual proposition databases using generic semantic dependency parsing. Using Wikipedia, we extracted, processed, clustered, and evaluated a large number of propositions. We built an architecture to provide a complete pipeline dealing with the input of text, extraction of knowledge, storage, and presentation of the resulting propositions.	database;parsing;text corpus;wikipedia	Peter Exner;Pierre Nugues	2012			natural language processing;computer science;data mining;semantics;linguistics;information extraction;information retrieval	NLP	-31.849337991247246	-68.15962688008861	35929
5682ff2277ef707bbb8ea7544be41e01bbe9006e	digital waste sorting: a goal-based, self-learning approach to label spam email campaigns		Fast analysis of correlated spam emails may be vital in the effort of finding and prosecuting spammers performing cybercrimes such as phishing and online frauds. This paper presents a self-learning framework to automatically divide and classify large amounts of spam emails in correlated labeled groups. Building on large datasets daily collected through honeypots, the emails are firstly divided into homogeneous groups of similar messages (campaigns), which can be related to a specific spammer. Each campaign is then associated to a class which specifies the goal of the spammer, i.e. phishing, advertisement, etc. The proposed framework exploits a categorical clustering algorithm to group similar emails, and a classifier to subsequently label each email group. The main advantage of the proposed framework is that it can be used on large spam emails datasets, for which no prior knowledge is provided. The approach has been tested on more than 3200 real and recent spam emails, divided in more than 60 campaigns, reporting a classification accuracy of 97 % on the classified data.	algorithm;cluster analysis;cybercrime;dictionary writing system;email;experiment;honeypot (computing);malware;phishing;profiling (computer programming);refinement (computing);sorting;spamming;test set	Mina Sheikh Alishahi;Andrea Saracino;Mohamed Mejri;Nadia Tawbi;Fabio Martinelli	2015		10.1007/978-3-319-24858-5_1	data mining;internet privacy;world wide web	Web+IR	-19.683734319394592	-55.51426686976379	35994
8adf8ed13aef238713d78e52bff4876083892669	sampling strategies and active learning for volume estimation	high recall retrieval;twitter	"""This paper tackles the challenge of accurately and efficiently estimating the number of relevant documents in a collection for a particular topic. One real-world application is estimating the volume of social media posts (e.g., tweets) pertaining to a topic, which is fundamental to tracking the popularity of politicians and brands, the potential sales of a product, etc. Our insight is to leverage active learning techniques to find all the """"easy"""" documents, and then to use sampling techniques to infer the number of relevant documents in the residual collection. We propose a simple yet effective technique for determining this """"switchover"""" point, which intuitively can be understood as the """"knee"""" in an effort vs. recall gain curve, as well as alternative sampling strategies beyond the knee. We show on several TREC datasets and a collection of tweets that our best technique yields more accurate estimates (with the same effort) than several alternatives."""	active learning (machine learning);baseline (configuration management);extrapolation;media space;sampling (signal processing);social media;switchover;text retrieval conference	Haotian Zhang;Jimmy J. Lin;Gordon V. Cormack;Mark D. Smucker	2016		10.1145/2911451.2914685	computer science;data science;machine learning;data mining;world wide web;information retrieval	Web+IR	-22.451008364900424	-52.19689623102259	35996
57e0df9155ea84cf1fe0d4d159aa23fc22432c28	understanding common perceptions from online social media	selected works;bepress	Modern society habitually uses online social media services to publicly share observations, thoughts, opinions, and beliefs at any time and from any location. These geotagged social media posts may provide aggregate insights into people’s perceptions on a broad range of topics across a given geographical area beyond what is currently possible through services such as Yelp and Foursquare. This paper develops probabilistic language models to investigate whether collective, topic-based perceptions within a geographical area can be extracted from the content of geotagged Twitter posts. The capability of the methodology is illustrated using tweets from three areas of different sizes. An application of the approach to support power grid restoration following a storm is presented.	aggregate data;circuit restoration;depth perception;geographic coordinate system;geotagging;language model;social media	Derek Doran;Swapna S. Gokhale;Aldo Dagnino	2013			social science;computer science;multimedia;world wide web	Web+IR	-23.20623100282899	-53.83216988293251	36027
2c8ea39a662f0d5ac7768a99df592d194df86125	a web based visualization for documents	search engine;digital library;hardware software interface;information search and retrieval	Locating relevant information on the Internet is a challenging task. The use of commercial search engines such as Altavista and Hotbot simplify the problem [2], however, an user may scan many pages before finding a relevant document. This process is tedious and users may miss important documents if insufficient information is retrieved from a search engine. We have developed a set of tools to collect, organize, and display information from the Internet. Our tools range from experimental visualizations to a working prototype with multiple users. The tools reduce the time to obtain relevant information by automating the collection and organization of data.	hotbot;internet;multi-user;prototype;web search engine	Manu Konchady;Raymond J. D'Amore;Gary Valley	1998		10.1145/324332.324335	search engine indexing;database search engine;digital library;computer science;database;search analytics;web search query;world wide web;information retrieval;search engine;human–computer information retrieval	HCI	-30.70872690301895	-53.14685337243498	36028
0a4e0e7a2af448180f843a51705d4aa9bab94529	strategies for contiguous multiword expression analysis and dependency parsing		In this paper, we investigate various strategies to predict both syntactic dependency parsing and contiguous multiword expression (MWE) recognition, testing them on the dependency version of French Treebank (Abeillé and Barrier, 2004), as instantiated in the SPMRL Shared Task (Seddah et al., 2013). Our work focuses on using an alternative representation of syntactically regular MWEs, which captures their syntactic internal structure. We obtain a system with comparable performance to that of previous works on this dataset, but which predicts both syntactic dependencies and the internal structure of MWEs. This can be useful for capturing the various degrees of semantic compositionality of MWEs.	lexicon;minimal working example;parsing expression grammar;treebank	Marie Candito;Matthieu Constant	2014			natural language processing;computer science;linguistics;programming language	NLP	-23.95912540291932	-74.70253925351929	36029
4a6b4840eaf686d0545d5960a1742b2bdb4bda79	representation of hypertext documents based on terms, links and text compressibility	data compression;text compression;feature space;principal component analysis;support vector machine;document classification;cumulant;dimensional reduction	Three methods for representation of hypertext based on links, terms and text compressibility have been compared to check their usefulness in document classification. Documents for classification have been selected fr om the Wikipedia articles taken from five distinct categories. For each represen tatio dimensionality reduction by Principal Component Analysis has been perf ormed, providing rough visual presentation of the data. Compression-based f eature space representation needed about 5 times less PCA vectors than the term or linkbased representations to reach 90% cumulative variance, giving comparable results of classification by Support Vector Machines.	dimensionality reduction;document classification;hypertext;perf (linux);principal component analysis;support vector machine;wikipedia	Julian Szymanski;Wlodzislaw Duch	2010		10.1007/978-3-642-17537-4_35	data compression;support vector machine;feature vector;computer science;machine learning;pattern recognition;data mining;mathematics;cumulant;principal component analysis	Web+IR	-23.924727535098235	-61.83534913543308	36036
e1d1eaa28fcfa981f8ba2e5b30df2c5d897d995f	a study on irony within the context of 7x1-pt corpus		The increasing use of social networks to express consumer opinions yields a large amount of potentially useful information for organizations to gauge consumer perception of their products. Nevertheless, gauging information by assigning polarities to opinionated text is not a trivial task, especially when dealing with short and ironical text. In this paper, we evaluate the presence of irony at the sentence level within a Portuguese corpus extracted from Twitter.	social network	Silvia M. W. Moraes;Rackel M. Machado;Matheus Redecker;Rafael Cadaval;Felipe Meneguzzi	2017			linguistics;irony;psychology	NLP	-22.267734829182768	-59.48579105378362	36102
30d2c293c342027fa85db1c65da8351c4007c44b	facilitating understanding of large document collections	pattern clustering;digital archives;electronic mail;information retrieval;distributed processing;resource management;text analysis;density based clustering;digital archives density based clustering information retrieval distributed processing hadoop mapreduce;vectors;clustering algorithms;electronic mail noise clustering algorithms vectors resource management clustering methods educational institutions;clustering methods;hadoop mapreduce;noise;hadoop large document collection density based clustering document segmentation;text analysis information retrieval pattern clustering	Large document collections containing multiple topics can be overwhelming to understand, requiring librarians and archivists significant time and efforts to develop access points. Efficient computational methods can aid this process by uncovering groups of documents that can be described for access. We investigate the use of density based clustering with document segmentation to identify points of access as dense clusters of information. The method returns stories and classes of cohesive clusters that can be described as precise points of access. We found that our method performs more efficiently than K-means clustering and topic model using Latent Dirichlet Allocation (LDA). We use Hadoop to process a large document collection.	apache hadoop;archive;cluster analysis;computation;document layout analysis;k-means clustering;latent dirichlet allocation;librarian;topic model;wireless access point	Jae Hyeon Bae;Weijia Xu;Maria Esteva	2011	2011 International Conference on Document Analysis and Recognition	10.1109/ICDAR.2011.268	document clustering;computer science;noise;resource management;machine learning;data mining;cluster analysis;world wide web;information retrieval	DB	-26.323842283405824	-56.10186908241223	36111
978ac2b3a2ad4efa14a142c4ff839dd9da73f70e	the university of texas system submission for the code-switching workshop shared task 2018		This paper describes the system for the Named Entity Recognition Shared Task of the Third Workshop on Computational Approaches to Linguistic Code-Switching (CALCS) submitted by the Bilingual Annotations Tasks (BATs) research group of the University of Texas. Our system uses several features to train a Conditional Random Field (CRF) model for classifying input words as Named Entities (NEs) using the Inside-Outside-Beginning (IOB) tagging scheme. We participated in the Modern Standard Arabic-Egyptian Arabic (MSA-EGY) and English-Spanish (ENGSPA) tasks, achieving weighted average Fscores of 65.62 and 54.16 respectively. We also describe the performance of a deep neural network (NN) trained on a subset of the CRF features, which did not surpass CRF performance.	artificial neural network;baseline (configuration management);computation;conditional random field;deep learning;entity;feature engineering;inside outside beginning;named-entity recognition	Florian Janke;Tongrui Li;Eric Rincón;Gualberto A. Guzmán;Barbara Bullock;Almeida Jacqueline Toribio	2018			artificial intelligence;natural language processing;computer science;computer architecture;code-switching	NLP	-21.604135826997805	-70.16266480074184	36250
f85457dc413feff0c926f963d0ff57901c3c8ebd	combining heterogeneous knowledge resources for improved distributional semantic models	lexical semantics;semantic relatedness;semantic model;semantic relations;explicit semantic analysis	The Explicit Semantic Analysis (ESA) model based on term cooccurrences in Wikipedia has been regarded as state-of-the-art semantic relatedness measure in the recent years. We provide an analysis of the important parameters of ESA using datasets in five different languages. Additionally, we propose the use of ESA with multiple lexical semantic resources thus exploiting multiple evidence of term cooccurrence to improve over the Wikipedia-based measure. Exploiting the improved robustness and coverage of the proposed combination, we report improved performance over single resources in word semantic relatedness, solving word choice problems, classification of semantic relations between nominals, and text similarity.	customer support;esa;explicit semantic analysis;information retrieval;knowledge base;learning to rank;machine learning;natural language processing;semantic similarity;similarity measure;wikipedia;word embedding	György Szarvas;Torsten Zesch;Iryna Gurevych	2011		10.1007/978-3-642-19400-9_23	semantic data model;natural language processing;semantic similarity;semantic computing;lexical semantics;semantic integration;explicit semantic analysis;semeval;semantic search;computer science;social semantic web;data mining;semantic compression;semantic equivalence;linguistics;probabilistic latent semantic analysis;information retrieval	NLP	-27.31683741457322	-67.00422472732986	36299
12759e561e454ce9bd86eb4834520f104ccbac42	ibiomes: managing and sharing biomolecular simulation data in a distributed environment		Biomolecular simulations, which were once batch queue or compute limited, have now become data analysis and management limited. In this paper we introduce a new management system for large biomolecular simulation and computational chemistry data sets. The system can be easily deployed on distributed servers to create a mini-grid at the researcher's site. The system not only offers a simple data deposition mechanism but also a way to register data into the system without moving the data from their original location. Any registered data set can be searched and downloaded using a set of defined metadata for molecular dynamics and quantum mechanics and visualized through a dynamic Web interface.		Julien C. Thibault;Julio C. Facelli;Thomas E. Cheatham	2013	Journal of chemical information and modeling	10.1021/ci300524j	knowledge management;database;distributed computing	HPC	-6.7014377332451955	-57.891891565637344	36329
032c2861dd2af2df26ef48bce591a65397b08a87	hunter mt: a course for young researchers in wmt17		This paper documents an undergraduate course at Hunter College, in which one instructor, six undergraduates, and one high school student built 17 machine translation systems in six months from scratch. The team successfully participated in the second Conference on Machine Translation (WMT17) evaluation on the news task in Finnish-English and Latvian-English and on the bio-medical task in French-English, English-French, English-German, English-Romanian, and English-Polish.	baseline (configuration management);british informatics olympiad;machine translation;supervised learning	Jia Xu;Yi Zong Kuang;Shondell Baijoo;Jacob Hyun Lee;Uman Shahzad;Mir Ahmed;Meredith Lancaster;Chris Carlan	2017				NLP	-31.467411743360113	-64.46119723634496	36333
3c7ff33bd2ac27ea71475130519fe40e021ade42	capturing salience with a trainable cache model for zero-anaphora resolution	capturing salience;candidate antecedent;trainable cache model;zero-anaphora resolution;cache model;computational cost;japanese newspaper article;empirical evaluation;machine learning-based implementation;machine learning	This paper explores how to apply the notion of caching introduced by Walker (1996) to the task of zero-anaphora resolution. We propose a machine learning-based implementation of a cache model to reduce the computational cost of identifying an antecedent. Our empirical evaluation with Japanese newspaper articles shows that the number of candidate antecedents for each zero-pronoun can be dramatically reduced while preserving the accuracy of resolving it.	algorithmic efficiency;amiga walker;anaphora (linguistics);cpu cache;cache (computing);computation;entity;global optimization;inductive reasoning;machine learning;mathematical optimization	Ryu Iida;Kentaro Inui;Yuji Matsumoto	2009			natural language processing;speech recognition;computer science;artificial intelligence;machine learning	NLP	-19.215697133164525	-75.8620684719215	36379
76b25a52dea6110b65d075b04d6f940f40f06414	finding needles in the haystack: search and candidate generation	search problems encyclopedias semantics search engines syntactics internet electronic publishing computer architecture	haystack: Search and candidate generation J. Chu-Carroll J. Fan B. K. Boguraev D. Carmel D. Sheinwald C. Welty A key phase in the DeepQA architecture is Hypothesis Generation, in which candidate system responses are generated for downstream scoring and ranking. In the IBM Watsoni system, these hypotheses are potential answers to Jeopardy!i questions and are generated by two components: search and candidate generation. The search component retrieves content relevant to a given question from Watson’s knowledge resources. The candidate generation component identifies potential answers to the question from the retrieved content. In this paper, we present strategies developed to use characteristics of Watson’s different knowledge sources and to formulate effective search queries against those sources. We further discuss a suite of candidate generation strategies that use various kinds of metadata, such as document titles or anchor texts in hyperlinked documents. We demonstrate that a combination of these strategies brings the correct answer into the candidate answer pool for 87.17% of all the questions in a blind test set, facilitating high end-to-end question-answering performance.	2.5d;active set method;anchor text;carroll morgan (computer scientist);dbpedia;downstream (software development);encode;end-to-end principle;hyperlink;lookup table;mined;programming paradigm;question answering;software quality assurance;test set;text corpus;watson (computer);web search engine;web search query;wikipedia;word sense	Jennifer Chu-Carroll;James Fan;Branimir Boguraev;David Carmel;Dafna Sheinwald;Christopher A. Welty	2012	IBM Journal of Research and Development	10.1147/JRD.2012.2186682	computer science;data mining;world wide web;information retrieval	NLP	-31.191725066624894	-65.76376897026663	36389
90ac61ce2e7292f9b2e18ca0ce5747de3384ebb3	experiments with an ensemble of spanish dependency parsers	traitement automatique des langues naturelles;computacion informatica;maltparser;malt parser;filologias;analizadores sintacticos de dependencias;sistema combinado;info eu repo semantics article;informacion documentacion;linguistica;ciencias basicas y experimentales;aprendizaje basado en memoria;computational linguistics;memory based learning;dependency parsers;grupo a;ciencias sociales;linguistique informatique;grupo b;natural language processing;ensemble system	This article presents an ensemble system for dependency parsing of Spanish that combines three machine-learning-based dependency parsers. The system operates in two stages. In the first stage, each of the three parsers analyzes an input sentence and produces a dependency graph. In the second stage, a voting system distills a final dependency graph out of the three first-stage dependency graphs.	dependency relation;machine learning;parsing	Roser Morante	2008	Procesamiento del Lenguaje Natural		natural language processing;computer science;artificial intelligence;computational linguistics;algorithm	AI	-26.496066121011282	-78.08047982378733	36444
032bd834cf367fc56f99ff8b6850cac4f31a62c2	dealing with web data: history and look ahead	look ahead	The high rate of change and the unprecedented scale of the Web pose enormous challenges to search engines who wish to provide the most up-to-date and highly relevant information to its users. The VLDB 2000 paper ”The Evolution of the Web and Implications for an Incremental Crawler” tried to address part of this challenge by collecting and analyzing the Web history data and by describing the architecture and the associated algorithms for an incremental Web crawler that can provide more up-to-date data to users in a timely manner. Experiments and theoretical analysis showed — surprisingly at the time — that a policy that allocates more resources to more frequently changing items does not necessarily lead to better performance. In this paper, we discuss what has happened in the 10 years since and talk about the challenges that lie head.	algorithm;experiment;history of the world wide web;vldb;web crawler;web search engine	Junghoo Cho;Hector Garcia-Molina	2010	PVLDB	10.14778/1920841.1920846	simulation;computer science;look-ahead;data mining;database;world wide web	DB	-31.36320995639365	-53.795238728613334	36603
5a95219689d1d8e5dd6bba11e96a833fe4c1ec01	improve sentiment analysis of citations with author modelling		In this paper, we introduce a novel approach to sentiment polarity classification of citations, which integrates data about the authors’ reputation. More specifically, our method extends the h-index with citation polarities and utilizes it in sentiment classification of citation sentences. Our computational results show that our method yields significant improvement in terms of classification performance.	citation network;pagerank;sentiment analysis;social media	Zheng Ma;Jinseok Nam;Karsten Weihe	2016			sentiment analysis;data science;computer science;data mining	NLP	-19.96163221215323	-65.43562673242434	36617
99fb4b18e11c71e36de053aaa42f762af2aa1d09	modeling discourse segments in lyrics using repeated patterns		This study proposes a computational model of the discourse segments in lyrics to understand and to model the structure of lyrics. To test our hypothesis that discourse segmentations in lyrics strongly correlate with repeated patterns, we conduct the first large-scale corpus study on discourse segments in lyrics. Next, we propose the task to automatically identify segment boundaries in lyrics and train a logistic regression model for the task with the repeated pattern and textual features. The results of our empirical experiments illustrate the significance of capturing repeated patterns in predicting the boundaries of discourse segments in lyrics.	computation;computational model;experiment;logistic regression	Kento Watanabe;Yuichiroh Matsubayashi;Naho Orita;Naoaki Okazaki;Kentaro Inui;Satoru Fukayama;Tomoyasu Nakano;Jordan B. L. Smith;Masataka Goto	2016			natural language processing;lyrics;artificial intelligence;speech recognition;computer science	NLP	-14.18980771978932	-78.9855508048556	36642
685a320f80b09347e2524c0841b6e4de47637ce3	french-english multi-word term alignment based on lexical context analysis		This article presents a method of extracting bilingual lexica composed of single-word terms (SWTs) and multi-word terms (MWTs) from comparable corpora of a technical domain. First, this method extracts MWTs in each language, and then uses statistical methods to align single words and MWTs by exploiting the term contexts. After explaining the difficulties involved in aligning MWTs and specifying our approach, we show the adopted process for bilingual terminology extraction and the resources used in our experiments. Finally, we evaluate our approach and demonstrate its significance, particularly in relation to non-compositional MWT alignment.	align (company);experiment;lexicon;microsoft word for mac;sequence alignment;terminology extraction;text corpus	Béatrice Daille;Samuel Dufour-Kowalski;Emmanuel Morin	2004			speech recognition;artificial intelligence;natural language processing;lexical chain;computer science;context analysis;terminology extraction	NLP	-26.66142431623769	-74.2714382089037	36671
59c32bc6bcbe81d579d3a416a0e52b96a263394e	overview of the multimedia information processing for personality & social networks analysis contest		Progress in the autonomous analysis of human behavior from multimodal information has lead to very effective methods able to deal with problems like action/gesture/activity recognition, pose estimation, opinion mining, user tailored retrieval, etc. However, it is only recently that the community has been starting to look into related problems associated with more complex behavior, including personality analysis, deception detection, among others. We organized an academic contest co-located with ICPR2018 running two tasks in this direction. On the one hand, we organized an information fusion task in the context of multimodal image retrieval in social media. On the other hand, we ran another task in which we aim to infer personality traits from written essays, including textual and handwritten information. This paper describes both tasks, detailing for each of them the associated problem, data sets, evaluation metrics and protocol, as well as an analysis of the performance of simple baselines.		Gabriela Ramírez-de-la-Rosa;Esaú Villatoro-Tello;Bogdan Ionescu;Hugo Jair Escalante;Sergio Escalera;Martha Larson;Henning Müller;Isabelle Guyon	2018		10.1007/978-3-030-05792-3_12	deception;information processing;human–computer interaction;sentiment analysis;image retrieval;contest;artificial intelligence;social network;activity recognition;pattern recognition;gesture;computer science	ML	-19.092180021619352	-59.88254475563078	36699
70621d54171985e6a542c61492e31cb6e445b0e7	automatic translation lexicon extraction from czech-english parallel texts			lexicon	Jan Curín;Martin Cmejrek	1999	Prague Bull. Math. Linguistics		artificial intelligence;linguistics;natural language processing;lexicon;czech;computer science;example-based machine translation	NLP	-29.500539961697108	-77.8524355493199	36738
8531b84ca0cfd127d8e82f32d3dfaec568080e82	combining discrete lexicon probabilities with nmt for low-resource mongolian-chinese translation		Mongolian-Chinese neural machine translation (NMT) models often make mistakes in translating low-frequency words. We propose a method to alleviate this problem by improve NMT models with discrete translation lexicons that efficiently encode these low-frequency words. We describe a method to calcu-late the lexicon probability of generating the next word in the translation candi-date by using the attention vector of the NMT model to select which source word lexical probabilities the model should focus on. The method use this probabil-ity as a bias to combine with the stand-ard NMT probability. Experiments show an improvement of 4.02 BLEU score. We apply this method to large-scale corpus and improve the BLEU score. In addition, we also propose a novel approach to combine discrete probabilistic lexicons obtained from large-scale Mongolian - Chinese bilin-gual parallel corpus into NMT of small-scale corpus and enhance the perfor-mance of the system effectively.	lexicon	Jinting Li;Hongxu Hou;Jing Wu;Hongbin Wang;Wenting Fan;Zhong Ren	2017		10.1109/PDCAT.2017.00026	bleu;machine translation;distributed computing;encoding (memory);probabilistic logic;artificial neural network;computer science;vocabulary;artificial intelligence;pattern recognition;training set;lexicon	NLP	-19.31860609114408	-75.81942467856192	36741
c6905ead60c118c9a7740299a371d3014cbba5b2	centroid based summarization of multiple documents implemented using timestamps	document clustering;pattern clustering;document summarization;automatic text summarization multiple document summarization system time stamp user interaction document cluster centroid extractive summarization method chronological sentence order passage clustering mead extraction algorithm redundancy algorithm first sentence overlap;data mining feature extraction clustering algorithms natural languages redundancy us department of energy lead;redundancy algorithm;search engines;automatic text summarization;information retrieval;text analysis;passage clustering;natural languages;data mining;mead;centroid and timestamp;internet;redundancy;lead;feature extraction;text analysis pattern clustering;information processing;document cluster centroid;extractive summarization method;clustering algorithms;communications technology;mead extraction algorithm;centroid and timestamp mead multi document summariztion;computer science;us department of energy;user interaction;chronological sentence order;first sentence overlap;multiple document summarization system;time stamp;multi document summariztion	We propose a multiple-document summarization system with user interaction. We introduce a system that would extract a summary from multiple documents based on the document cluster centroids, which is effectively the distribution of terms in the multiple documents in the cluster. This summarization technique is a cluster- based, extractive summarization method, where passages are first clustered based on similarity, prior to the selection of passages that form the extractive summary of the documents. The sentences are then issued a timestamp based on the order of their occurrence in the original document, thereby ensuring the chronological order of sentences. Passage clustering forms a main component in this system that aims to extract the most relevant sentences of the documents at the same time keeping the summary non-redundant. The implementation is based on the MEAD extraction algorithm and redundancy based algorithm. MEAD extraction algorithm uses three features to compute the salience of the sentence. They are centroid value, positional value and first-sentence overlap. Redundancy algorithm checks for overlapping words in sentences and issues a redundancy penalty. Timestamps are issued to sentences to maintain the chronological order of the sentences and hence a coherent and free- flowing summary can be generated.	algorithm;automatic summarization;cluster analysis;coherence (physics);lamport timestamps	Ramanujam Nedunchelian	2008	2008 First International Conference on Emerging Trends in Engineering and Technology	10.1109/ICETET.2008.122	multi-document summarization;computer science;automatic summarization;pattern recognition;data mining;information retrieval	NLP	-25.947863903432676	-64.13521142481318	36788
10e06b88a5ab8d4bc5613f408c770c004687716c	integrating bilingual named entities lexicon with conditional random fields model for arabic named entities recognition		Named Entity Recognition plays an important role in locating and classifying atomic elements into predefined categories such as person names, locations, organizations, expression of times, temporal expressions etc. Several approaches with rule based and machine learning based techniques have been applied on English and some other Latin languages successfully. Arabic has a complex and rich morphology, which makes the named entities recognition a challenging process. In this paper we propose our hybrid NER system that applies conditional random fields (CRF), bilingual NE lexicon and grammar rules to the task of Named Entity Recognition in Arabic languages. The aim of our system is enhancing the overall performance of NER tasks. The empirical results indicate that the hybrid system outperforms the state-of-the-art of Arabic NER in terms of precision when applied to ANERcorp dataset, with f-measures 83.36 for Person, 89.58for Location, and 72.26 for Organization	conditional random field;dbpedia;galaxy morphological classification;hybrid system;lexicon;logic programming;machine learning;named entity;named-entity recognition;temporal expressions	Emna Hkiri;Souheyl Mallat;Mounir Zrigui	2017	2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)	10.1109/ICDAR.2017.105	pattern recognition;artificial intelligence;task analysis;rule-based system;morphology (linguistics);computer science;lexicon;named-entity recognition;conditional random field;arabic languages;grammar	NLP	-24.29027338622705	-71.12773273051155	36807
440e2ce5615c87b481272a806a586e670ec4bb47	technical review: sarcasm detection algorithms		In this paper, we want to review one of the challenging problems for the opinion mining task, which is sarcasm detection. To be able to do that, many researchers tried to explore such properties in sarcasm like theories of sarcasm, syntactical properties, psycholinguistic of sarcasm, lexical feature, semantic properties, etc. Studies conducted within last 15 years have not only made progress in semantic features but have also shown increasing amounts of methods of analysis using a machine-learning approach to process data. Therefore, this paper will try to explain the most currently used methods to detect sarcasm. Lastly, we will present a result of our finding, which might help other researchers to gain a better result in the future.	algorithm	Uraz Yavanoglu;Taha Yasin Ibisoglu;Setra Genyang Wicana	2018	Int. J. Semantic Computing	10.1142/S1793351X18300017	sentiment analysis;artificial intelligence;semantic property;machine learning;computer science;sarcasm;natural language processing	Web+IR	-21.859201338932913	-58.23249919917505	36843
ee7f0e5fe13c3b7c644648460db5026d5e511fde	transfer learning and sentence level features for named entity recognition on tweets		We present our system for the WNUT 2017 Named Entity Recognition challenge on Twitter data. We describe two modifications of a basic neural network architecture for sequence tagging. First, we show how we exploit additional labeled data, where the Named Entity tags differ from the target task. Then, we propose a way to incorporate sentence level features. Our system uses both methods and ranked second for entity level annotations, achieving an F1-score of 40.78, and second for surface form annotations, achieving an F1score of 39.33.	artificial neural network;deep learning;f1 score;named entity;network architecture	Pius von Däniken;Mark Cieliebak	2017			computer science;natural language processing;transfer of learning;entity linking;named-entity recognition;sentence;artificial intelligence;pattern recognition	NLP	-19.423700755539127	-71.94504063902394	36852
6b35e56768d567f75f05493361fa7191045434ce	rule-based logical forms extraction	article	In this paper, we present concise but robust rules for dependency-based logical form identification with high accuracy. We describe our approach from an intuitive and formalized perspective, which we believe overcomes much of the complexity. In comparison to previous work, we believe ours is more compact and involves less rules and exceptions. We also provide the reader with a comparison of the respective impacts of the most essential rules on the logical form identification task of the 2004 Senseval 3 test set.	complexity;exception handling;expert system;greedy algorithm;semeval;test set	Cenny Wenner	2007			history;media studies;operations research	NLP	-27.81925040436689	-72.84996499609383	36896
f587aa982b9371200a83121775642cbca1222c59	training datasets collection and evaluation of feature selection methods for web content filtering		WebSpy 1 : • 40% of Internet use is not related to business; • 64% of employees use the Internet for personal interest at work; • average employee spends 1‐2 hours per day for unproductive browsing; • wasting 1 hour per day employee yields approximately $ 7500 of losses per year.	content-control software;feature selection;geforce 7 series;internet;web content	Roman Suvorov;Ilya Sochenkov;Ilya Tikhomirov	2014		10.1007/978-3-319-10554-3_12	pattern recognition;data mining;information retrieval	Web+IR	-20.326520057026872	-52.73515930574884	36934
56a8c7489cbd0099eb52fc31bb2231ecafc88890	deep learning for detecting inappropriate content in text		"""Today, there are a large number of online discussion fora on the internet which are meant for users to express, discuss and exchange their views and opinions on various topics. For example, news portals, blogs, social media channels such as youtube. typically allow users to express their views through comments. In such fora, it has been often observed that user conversations sometimes quickly derail and become inappropriate such as hurling abuses, passing rude and discourteous comments on individuals or certain groups/communities. Similarly, some virtual agents or bots have also been found to respond back to users with inappropriate messages. As a result, inappropriate messages or comments are turning into an online menace slowly degrading the effectiveness of user experiences. Hence, automatic detection and filtering of such inappropriate language has become an important problem for improving the quality of conversations with users as well as virtual agents. In this paper, we propose a novel deep learning-based technique for automatically identifying such inappropriate language. We especially focus on solving this problem in two application scenarios—(a) Query completion suggestions in search engines and (b) Users conversations in messengers. Detecting inappropriate language is challenging due to various natural language phenomenon such as spelling mistakes and variations, polysemy, contextual ambiguity and semantic variations. For identifying inappropriate query suggestions, we propose a novel deep learning architecture called “Convolutional Bi-Directional LSTM (C-BiLSTM)"""" which combines the strengths of both Convolution Neural Networks (CNN) and Bi-directional LSTMs (BLSTM). For filtering inappropriate conversations, we use LSTM and Bi-directional LSTM (BLSTM) sequential models. The proposed models do not rely on hand-crafted features, are trained end-end as a single model, and effectively capture both local features as well as their global semantics. Evaluating C-BiLSTM, LSTM and BLSTM models on real-world search queries and conversations reveals that they significantly outperform both pattern-based and other hand-crafted feature-based baselines."""	artificial neural network;baseline (configuration management);bi-directional text;blog;comment (computer programming);convolution;deep learning;emoticon;intelligent agent;internet;long short-term memory;menace;natural language;portals;sensor;social media;web search engine;web search query	Harish Yenala;Ashish Jhanwar;Manoj Kumar Chinnakotla;Jay Goyal	2017	International Journal of Data Science and Analytics	10.1007/s41060-017-0088-4	web query classification;online discussion;artificial neural network;natural language processing;architecture;search engine;deep learning;natural language;computer science;artificial intelligence;phenomenon	AI	-19.65722706820124	-58.77893941575773	36979
64c265d245b7ca4c98eb16a636938cf8628b7ade	an improved crf based chinese language processing system for sighan bakeoff 2007		This paper describes three systems: the Chinese word segmentation (WS) system, the named entity recognition (NER) system and the Part-of-Speech tagging (POS) system, which are submitted to the Fourth International Chinese Language Processing Bakeoff. Here, Conditional Random Fields (CRFs) are employed as the primary models. For the WS and NER tracks, the ngram language model is incorporated in our CRFs based systems in order to take into account the higher level language information. Furthermore, to improve the performances of our submitted systems, a transformationbased learning (TBL) technique is adopted for post-processing.	conditional random field;heuristic;language model;linear model;log-linear model;n-gram;named-entity recognition;part-of-speech tagging;performance;system integration;text segmentation;video post-processing	Xihong Wu;Xiaojun Lin;Xinhao Wang;Chunyao Wu;Yaozhong Zhang;Dianhai Yu	2008			pattern recognition;artificial intelligence;crfs;computer science;named-entity recognition;language model;conditional random field;text segmentation	NLP	-22.33707486310054	-76.0204727479274	37010
0b9b987fdae057306835b9243c5b7cc54423bf28	textflow: a text similarity measure based on continuous sequences		Text similarity measures are used in multiple tasks such as plagiarism detection, information ranking and recognition of paraphrases and textual entailment. While recent advances in deep learning highlighted further the relevance of sequential models in natural language generation, existing similarity measures do not fully exploit the sequential nature of language. Examples of such similarity measures include ngrams and skip-grams overlap which rely on distinct slices of the input texts. In this paper we present a novel text similarity measure inspired from a common representation in DNA sequence alignment algorithms. The new measure, called TextFlow, represents input text pairs as continuous curves and uses both the actual position of the words and sequence matching to compute the similarity value. Our experiments on eight different datasets show very encouraging results in paraphrase detection, textual entailment recognition and ranking relevance.	algorithm;deep learning;experiment;gensim;grams;n-gram;natural language generation;relevance;semantic similarity;sequence alignment;similarity measure;skip list;textual entailment;tf–idf	Yassine Mrabet;Halil Kilicoglu;Dina Demner-Fushman	2017		10.18653/v1/P17-1071	machine learning;computer science;similarity measure;artificial intelligence;pattern recognition	NLP	-23.882682631709766	-68.65982120861382	37063
ac12ba5bf81de83991210b4cd95b4ad048317681	combining deep facial and ambient features for first impression estimation		First impressions influence the behavior of people towards a newly encountered person or a human-like agent. Apart from the physical characteristics of the encountered face, the emotional expressions displayed on it, as well as ambient information affect these impressions. In this work, we propose an approach to predict the first impressions people will have for a given video depicting a face within a context. We employ pre-trained Deep Convolutional Neural Networks to extract facial expressions, as well as ambient information. After video modeling, visual features that represent facial expression and scene are combined and fed to Kernel Extreme Learning Machine regressor. The proposed system is evaluated on the ChaLearn Challenge Dataset on First Impression Recognition, where the classification target is the ”Big Five” personality trait labels for each video. Our system achieved an accuracy of 90.94% on the sequestered test set, 0.36% points below the top system in the competition.	artificial neural network;convolutional neural network;emotion recognition;modality (human–computer interaction);multimodal interaction;test set;transfer-based machine translation	Furkan Gürpinar;Heysem Kaya;Albert Ali Salah	2016		10.1007/978-3-319-49409-8_30	artificial intelligence;convolutional neural network;video modeling;machine learning;deep learning;extreme learning machine;computer science;emotional expression;facial expression;first impression (psychology);test set	Vision	-12.296549849549482	-71.06485884635036	37064
8f0ebaa9ffd4018dbc3f76597a2f7e49ad593328	a semantic imitation model of social tag choices	probabilistic topic model;multi level social behavior modeling;computational cognitive model social tagging semantic imitation multi level social behavior modeling;document handling;tagging predictive models power system modeling stochastic processes stability computational modeling cognitive science human factors frequency dictionaries;human computer interaction;convergence;stochastic tag choice process;data mining;semantic interpretation;online social behavior semantic imitation model semantics formal representations stochastic tag choice process probabilistic topic model external word topic internal word concept relations random utility model social tagging system multi level modeling approach;social tagging system;computational modeling;internet;online social behavior;stochastic processes;random utility model;social behavior;semantic imitation model;semantic imitation;emergent behavior;social networking online;stability analysis;external word topic;computational cognitive model;semantics formal representations;internal word concept relations;is success;social networking online document handling human computer interaction internet;power law;probabilistic logic;social tagging;cognitive model;multi level modeling approach;tagging	We describe a semantic imitation model of social tagging that integrates formal representations of semantics and a stochastic tag choice process to explain and predict emergent behavioral patterns. The model adopts a probabilistic topic model to separately represent external word-topic and internal word-concept relations. These representations are coupled with a tag-based topic inference process that predicts how existing tags may influence the semantic interpretation of a document. The inferred topics influence the choice of tags assigned to a document through a random utility model of tag choices. We show that the model is successful in explaining the stability in tag proportions across time and power-law frequency-rank distributions of tag co-occurrences for semantically general and narrow tags. The model also generates novel predictions on how emergent behavioral patterns may change when users with different domain expertise interact with a social tagging system. The model demonstrates the weaknesses of single-level analyses and highlights the importance of adopting a multi-level modeling approach to explain online social behavior.	behavioral pattern;emergence;folksonomy;multi-level cell;multi-level governance;semantic interpretation;topic model	Wai-Tat Fu;Thomas George Kannampallil;Ruogu Kang	2009	2009 International Conference on Computational Science and Engineering	10.1109/CSE.2009.382	natural language processing;stochastic process;cognitive model;power law;semantic interpretation;von neumann stability analysis;the internet;convergence;social behavior;computer science;artificial intelligence;machine learning;data mining;database;probabilistic logic;computational model;statistics;emergence	AI	-23.735424844561585	-63.52256740732928	37070
08782e1bf9cac6c255a17f766809b135c910e018	dialog act classification using n-gram algorithms	support vector machine;natural language processing;classification system;human performance	Speech act classification remains one of the challenges in natural language processing. This paper evaluates a classification system that assigns one of twelve dialog acts to an utterance from the Map Task Corpus. The dialog act classification system chooses a dialog act based on n-grams from a training set. The system’s performance is comparable to other classification systems, like those using support vector machines. Performance is high given the fact that the system only considers an utterance out of context and from written input only. Moreover, the system’s performance is on par with human performance.	grams;human reliability;n-gram;natural language processing;support vector machine;test set;dialog	Max M. Louwerse;Scott A. Crossley	2006			n-gram;dialog act;natural language processing;artificial intelligence;support vector machine;speech recognition;training set;computer science;utterance;dialog box;dialog system	NLP	-22.675967666681778	-71.10051156446556	37116
6094265fb662090ac96265e43ccce7b2b838ad7a	using an intelligent agent to enhance search engine performance	intelligent agent;search engine	The amount of information available via networks and databases has rapidly increased and continues to increase. Existing search and retrieval engines provide limited assistance to users in locating the relevant information that they need. Autonomous, intelligent agents may prove to be the needed item in transforming passive search and retrieval engines into active, personal assistants. This proposal explores the quantity of information available that is driving the need for improved search and retrieval engines. It then reviews current information retrieval literature and agency literature. Following these reviews, it proposes that the combination of effective information retrieval techniques and autonomous, intelligent agents can improve the performance of short-term information retrieval in an existing search or retrieval engine. A review of the current status of agents in various areas including information retrieval is also presented. The proposal then presents the objectives of this research, the methodology to achieve these objectives, and concludes with the contributions of this research and a short summary.	intelligent agent	James Jansen	1997	First Monday		search engine indexing;query expansion;relevance;cognitive models of information retrieval;computer science;data mining;adversarial information retrieval;world wide web;information retrieval;search engine;human–computer information retrieval	AI	-33.5709386190055	-56.03507804659424	37136
77096191703ca7856c1a39a0726ee4ca434f1edb	achieving effective multi-term queries for fast dht information retrieval	dynamic programming;control theory;query processing;distributed hash table;information retrieval;feedback;system;user behavior;implicit relevance feedback;query expansion;data storage equipment;query logs;industrial engineering	Distributed Hash Tables (DHTs) are well-suited for exact match lookups using unique identifiers, but do not directly support multi-term queries. Related research of query expansion has shown that adding new terms to a query via ad hoc feedback improves the retrieval effectiveness of such query. In the paper, we propose an effective multi-term query processing algorithm for information retrieval in DHT systems. Given the significance of first term in a multi-term query, the query is sent to the peers containing the first term. To enhance the query effectiveness, we design two query expansion mechanisms and an implicit relevance feedback approach based on users’ behaviors. Additionally, we record the query log and the expansion terms for each query which can accelerate the future queries and improve the query accuracy. Experimental results show that our query methods yield substantial improvements in retrieval effectiveness in the following three aspects: recall, precision at 10 standard recall levels and precision histograms.	algorithm;browsing;database;digital library;distributed hash table;download;experiment;hoc (programming language);information retrieval;library (computing);query expansion;relevance feedback;unique identifier;web search engine	Quanqing Xu;Heng Tao Shen;Yafei Dai;Bin Cui;Xiaofang Zhou	2008		10.1007/978-3-540-85481-4_4	online aggregation;sargable;query optimization;query expansion;web query classification;ranking;boolean conjunctive query;computer science;chord;query by example;dynamic programming;concept search;data mining;feedback;database;system;rdf query language;web search query;view;world wide web;information retrieval;query language;object query language;spatial query	Web+IR	-31.353983241855886	-55.79043700202145	37187
a71b926af4d7158af9782c49a7f53abf161f8631	mg4j at trec 2006	indexation;query expansion	MG4J participated in the ad hoc task of the Terabyte Track (find all the relevant documents with high precision from 25.2 million pages from the .gov domain) at TREC 2006. It was the second time the MG4J group participated to TREC. For this year, we integrated standard techniques (such as stemming and BM25 scoring) into MG4J, and submitted also automatic runs based on trivial query expansion techniques.	hoc (programming language);query expansion;stemming;terabyte;text retrieval conference	Paolo Boldi;Sebastiano Vigna	2006			query expansion;computer science;data mining;world wide web;information retrieval	Web+IR	-31.773759215492266	-62.932861777497614	37243
52d6062ac968696c770e5192aa96d5428166da3d	query by semantic example	busqueda informacion;metodo caso peor;vocabulaire;image processing;recherche image;image databank;information retrieval;vocabulary;interrogation base donnee;procesamiento imagen;interrogacion base datos;proyeccion imagen;semantics;posterior probability;vocabulario;aprendizaje por ejemplos;probabilistic approach;semantica;semantique;traitement image;similitude;learning by example;recherche information;image projection;image representation;enfoque probabilista;approche probabiliste;banco imagen;probabilite a posteriori;banque image;similarity;probabilidad a posteriori;methode cas pire;projection image;experimental evaluation;similitud;query by example;similarity function;worst case method;database query;apprentissage a partir d exemple;semantic retrieval;image retrieval	A solution to the problem of image retrieval based on queryby-semantic-example (QBSE) is presented. QBSE extends the idea of query-by-example to the domain of semantic image representations. A semantic vocabulary is first defined, and a semantic retrieval system is trained to label each image with the posterior probability of appearance of each concept in the vocabulary. The resulting vector is interpreted as the projection of the image onto a semantic probability simplex, where a suitable similarity function is defined. Queries are specified by example images, which are projected onto the probability simplex. The database images whose projections on the simplex are closer to that of the query are declared its closest neighbors. Experimental evaluation indicates that 1) QBSE significantly outperforms the traditional queryby-visual-example paradigm when the concepts in the query image are known to the retrieval system, and 2) has equivalent performance even in the worst case scenario of queries composed by unknown concepts.	image retrieval;programming paradigm;query by example;similarity measure;vocabulary;worst-case scenario	Nikhil Rasiwasia;Nuno Vasconcelos;Pedro J. Moreno	2006		10.1007/11788034_6	computer vision;similarity;image processing;image retrieval;computer science;query by example;similitude;machine learning;pattern recognition;mathematics;semantics;linguistics;posterior probability;information retrieval;statistics	Vision	-12.773561443284684	-60.57578275450364	37247
02aaef9af0b7f866908f611c7e526f185aaceaad	what's with the attitude? identifying sentences with attitude in online discussions	online discussion;sentiment analysis;user generated content;natural language processing	Mining sentiment from user generated content is a very important task in Natural Language Processing. An example of such content is threaded discussions which act as a very important tool for communication and collaboration in the Web. Threaded discussions include e-mails, e-mail lists, bulletin boards, newsgroups, and Internet forums. Most of the work on sentiment analysis has been centered around finding the sentiment toward products or topics. In this work, we present a method to identify the attitude of participants in an online discussion toward one another. This would enable us to build a signed network representation of participant interaction where every edge has a sign that indicates whether the interaction is positive or negative. This is different from most of the research on social networks that has focused almost exclusively on positive links. The method is experimentally tested using a manually labeled set of discussion posts. The results show that the proposed method is capable of identifying attitudinal sentences, and their signs, with high accuracy and that it outperforms several other baselines.	baseline (configuration management);email;experiment;hidden markov model;interaction;markov chain;natural language processing;sentiment analysis;social network analysis;user-generated content;world wide web	Ahmed Hassan Awadallah;Vahed Qazvinian;Dragomir R. Radev	2010			natural language processing;computer science;machine learning;data mining;multimedia;user-generated content;world wide web;sentiment analysis	NLP	-21.95849336319354	-60.378254871462886	37275
748c1a0159924ff298d4868f0077015c569587a9	clustering of similar values, in spanish, for the improvement of search systems	search systems;similar values	The ability to correctly access electronically stored information is becoming increasingly important as stored information itself keeps growing continuously. One of the problems that face search systems is the inconsistency found among the stored values: i.e., the very same term may have different values, due to misspelling, a permuted word order, spelling variants and so on. The clustering of the values that refer to a given term solves this problem by replacing these clustered values with one single value. In this paper, we present a clustering method that allows us to reduce on the existing inconsistencies in databases and, thus, improve on the performance of both search and information retrieval systems. The method we propose here gives good results with a considerably low error rate.	algorithm;algorithmic efficiency;bit error rate;cluster analysis;coefficient;computation;computer cluster;data quality;database;experiment;information retrieval;jaccard index	Sergio Luján-Mora;Manuel Palomar	2000			artificial intelligence;natural language processing;machine learning;computer science;cluster analysis	Web+IR	-27.822072190390216	-60.2696473833184	37320
f23d18bc77f7180c1bee2e8e4ac2a845b1f65585	a system for query by example in image data base	query by example	Processing queries for databases containing multimedia information is a hard question. In this paper we describe a knowledge base approach to effectively resolve query by examples in image database. We present a prototype system that is at the moment under developing in our Labs. We consider wavelet descriptors for modeling color, shape and texture features, and we propose a knowledge base for assisting the user in the retrieval process. Results on a collection of about 1000 images are reported to provide a validation of the proposed strategy.	database;knowledge base;prototype;query by example;wavelet	Angelo Chianese;Antonio Picariello;Lucio Sansone	2001			query expansion;ranking (information retrieval);information retrieval;query language;query by example;rdf query language;view;query optimization;sargable;computer science	DB	-12.988406451619337	-57.99787977268382	37350
7f58b640c6da71658dd498a5e8fa039574867262	automated web site evaluation - an approach based on ranking svm	web site evaluation;evaluation function;support vector machines intelligent agent machine learning support vector machine classification internet usability voting web pages navigation conferences;ranking svm;web pages;support vector machines;web service;navigation;ranking svm web site evaluation;internet;machine learning;voting;intelligent agent;support vector machine classification;usability;conferences	This paper proposes an automated web site evaluation approach using machine learning to cope with ranking problems. Evaluating web sites is a significant task for web service because evaluated web sites provide useful information for users to estimate sites’ validation and popularity. Although many practical approaches have been taken to present a measuring stick for web sites, their evaluation functions are set up manually. Thus, we develop a method to obtain evaluation function using Ranking SVM and automatically rank web sites with the learned classifier. Also we conducted experiments and confirmed the effectiveness of our approach and its potential in performing high quality web site evaluation.	algorithm;categorization;discriminant;display resolution;evaluation function;experiment;machine learning;ranking svm;web service	Peng Li;Seiji Yamada	2009	2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2009.224	web service;support vector machine;navigation;the internet;usability;voting;computer science;artificial intelligence;machine learning;evaluation function;web page;data mining;law;ranking svm;world wide web;intelligent agent	Web+IR	-24.028221530035765	-57.812609798416005	37368
84416caeb6d0a464ae8d4d883654003cc4316fe0	automatic melodic segmentation of turkish makam music scores	presses;conferences music information retrieval signal processing computational modeling cognition music presses;computational modeling;signal processing;music information retrieval;cognition;music;conferences;melodic analysis makam music	Automatic melodic segmentation is one of the important steps in computational analysis of melodic content from symbolic data. This widely studied research problem has been very rarely considered for Turkish makam music. In this paper we first present test results for state-of-the-art techniques from literature on Turkish makam music data. Then, we present a statistical classification-based segmentation system that exploits the link between makam melodies and usual and makam scale hierarchies together with the well-known features in literature. We show through tests on a large dataset that the proposed system has a higher accuracy.	statistical classification;whole earth 'lectronic link	Baris Bozkurt;Mustafa Kemal Karaosmanoglu;Bilge Karaçali;Erdem Ünal	2014	2014 22nd Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2014.6830262	natural language processing;computer vision;speech recognition;cognition;computer science;signal processing;music	Web+IR	-16.898283520521954	-62.021893000361075	37376
cf6f0bb646e61971c6d33cf994c0ba0e72ceb2c4	intelius-nyu tac-kbp2012 cold start system		This paper describes the Intelius-NYU 2012 system for the KBP Cold Start task. The Cold Start task can be decomposed into two subtasks: slot filling and entity linking. For slot filling, we focus on the adaptation of the NYU 2011 regular slot filling system to the Cold Start task. For entity linking, we apply Intelius‘s commercial conflation engine to link person and organization entities with minimal tuning for the Cold Start task. We also developed a voting-based entity linking system for geo-political entities.	cold start;entity linking	Xin Wang;Ang Sun;Sen Xu;Yigit Kiran;Poornima Shakthi;Andrew Borthwick;Ralph Grishman	2012			simulation	NLP	-19.429714722082785	-71.2168153494141	37380
40c74d6cd28b7ca11dbea1b5531c2c7d55293474	wikikreator: improving wikipedia stubs automatically		Stubs on Wikipedia often lack comprehensive information. The huge cost of editing Wikipedia and the presence of only a limited number of active contributors curb the consistent growth of Wikipedia. In this work, we present WikiKreator, a system that is capable of generating content automatically to improve existing stubs on Wikipedia. The system has two components. First, a text classifier built using topic distribution vectors is used to assign content from the web to various sections on a Wikipedia article. Second, we propose a novel abstractive summarization technique based on an optimization framework that generates section-specific summaries for Wikipedia stubs. Experiments show that WikiKreator is capable of generating well-formed informative content. Further, automatically generated content from our system have been appended to Wikipedia stubs and the content has been retained successfully proving the effectiveness of our approach.	automatic summarization;experiment;information;mathematical optimization;semantic similarity;well-formed formula;wiki;wikipedia	Siddhartha Banerjee;Prasenjit Mitra	2015			internet privacy;world wide web;information retrieval	NLP	-29.26844677544727	-55.74640831805723	37420
717cf540b08e4b99942da70acee98832d3f34d96	an effective similarity propagation method for matching ontologies without sufficient or regular linguistic information	ontology matching	Most existing ontology matching methods are based on the linguistic information. However, some ontologies have not sufficient or regular linguistic information such as natural words and comments, so the linguistic-based methods can not work. Structure-based methods are more practical for this situation. Similarity propagation is a feasible idea to realize the structure-based matching. But traditional propagation does not take into consideration the ontology features and will be faced with effectiveness and performance problems. This paper analyzes the classical similarity propagation algorithm Similarity Flood and proposes a new structure-based ontology matching method. This method has two features: (1) It has more strict but reasonable propagation conditions which make matching process become more efficient and alignments become better. (2) A series of propagation strategies are used to improve the matching quality. Our method has been implemented in ontology matching system Lily. Experimental results demonstrate that this method performs well on the OAEI benchmark dataset.	algorithm;benchmark (computing);experiment;ontology (information science);ontology alignment;software propagation	Peng Wang;Baowen Xu	2009		10.1007/978-3-642-10871-6_8	ontology alignment;computer science;theoretical computer science;machine learning;data mining	AI	-28.104005988506408	-66.4499092057929	37475
13a583b967a738f2722fa275d9bf18cd6b0d983c	multi-label maximum entropy model for social emotion classification over short text	multi label maximum entropy model;short text analysis;social emotion classification;co training algorithm	Social media provides an opportunity for many individuals to express their emotions online. Automatically classifying user emotions can help us understand the preferences of the general public, which has a number of useful applications, including sentiment retrieval and opinion summarization. Short text is prevalent on the Web, especially in tweets, questions, and news headlines. Most of the existing social emotion classification models focus on the detection of user emotions conveyed by long documents. In this paper, we introduce a multi-label maximum entropy (MME) model for user emotion classification over short text. MME generates rich features by modeling multiple emotion labels and valence scored by numerous users jointly. To improve the robustness of the method on varied-scale corpora, we further develop a co-training algorithm for MME and use the L-BFGS algorithm for the generalized MME model. Experiments on real-world short text collections validate the effectiveness of these methods on social emotion classification over sparse features. We also demonstrate the application of generated lexicons in identifying entities and behaviors that convey different social emotions.	multi-label classification;principle of maximum entropy	Jun Li;Yanghui Rao;Fengmei Jin;Huijun Chen;Xiyun Xiang	2016	Neurocomputing	10.1016/j.neucom.2016.03.088	speech recognition;machine learning;data mining	ML	-21.606617111454593	-66.31695325041413	37495
99c79692be94e5b28ecfe0eb6fe2427516a8f346	towards bidirectional processing models of sign language: a constructional approach in fluid construction grammar		Sign languages (SL) require a fundamental rethinking of many basic assumptions about human language processing because instead of using linear speech, sign languages coarticulate facial expressions, shoulder and hand movements, eye gaze and usage of a three-dimensional space. SL researchers have therefore advocated SL-specific approaches that do not start from the biases of models that were originally developed for vocal languages. Unfortunately, there are currently no processing models that adequately achieve both language comprehension and formulation, and the SL-specific developments run the risk of becoming alienated from other linguistic research. This paper explores the hypothesis that a construction grammar architecture offers a solution to these problems because constructions are able to simultaneously access and manipulate information coming from many different sources. This claim is illustrated by a proof-of-concept implementation of a basic grammar for French Sign Language in Fluid Construction Grammar.	cognition;computational linguistics;fluid construction grammar;microsoft outlook for mac;sl (complexity)	Remi van Trijp	2015			architecture;natural language processing;artificial intelligence;eye tracking;facial expression;comprehension;french sign language;sign language;construction grammar;grammar;computer science	NLP	-10.66784431462723	-78.92847424624138	37502
c9a5fd8711e345c785c6cb2f43ccfcc97aaf805e	event participants and linguistic arguments		Although there is a clear and intuitive mapping between linguistic arguments of verbs and event participants, the mapping is not perfect. We review the linguistic evidence that indicates that the mapping is imperfect. We also present the results of a new experimental study that provides further support for a dissociation between event participants and linguistic arguments. The study consists of two tasks. The first task elicited intuitions on conceptual event participants, and the second task elicited intuitions on linguistic arguments in instrument verbs and transaction verbs. The results suggest that while instrument phrases and currency/price phrases are considered necessary event participants, they are not linguistic arguments.	experiment;nonlinear gameplay	Roxana-Maria Barbu;Ida Toivonen	2016			social psychology;cognitive psychology;psychology	NLP	-7.950625011093674	-78.84528517451571	37525
7785559fccddd0f54d47f3131cf16417fb7f658c	mining actionable information from security forums: the case of malicious ip addresses		The goal of this work is to systematically extract information from hacker forums, whose information would be in general described as unstructured: the text of a post is not necessarily following any writing rules. By contrast, many security initiatives and commercial entities are harnessing the readily public information, but they seem to focus on structured sources of information. Here, we focus on the problem of identifying malicious IP addresses, among the IP addresses which are reported in the forums. We develop a method to automate the identification of malicious IP addresses with the design goal of being independent of external sources. A key novelty is that we use a matrix decomposition method to extract latent features of the behavioral information of the users, which we combine with textual information from the related posts. A key design feature of our technique is that it can be readily applied to different language forums, since it does not require a sophisticated Natural Language Processing approach. In particular, our solution only needs a small number of keywords in the new language plus the user’s behavior captured by specific features. We also develop a tool to automate the data collection from security forums. Using our tool, we collect approximately 600K posts from 3 different forums. Our method exhibits high classification accuracy, while the precision of identifying malicious IP in post is greater than 88% in all three forums. We argue that our method can provide singificantly more information: we find up to 3 times more potentially malicious IP address compared to the reference blacklist VirusTotal. As the cyber-wars are becoming more intense, having early accesses to useful information becomes more imperative to remove the hackers first-move advantage, and our work is a solid step towards this direction.	entity;imperative programming;malware;natural language processing;virustotal	Joobin Gharibshah;Tai-Ching Li;Andre Castro;Konstantinos Pelechrinis;Evangelos E. Papalexakis;Michalis Faloutsos	2018	CoRR		computer science;data mining;hacker;novelty;data collection;blacklist;small number;constructed language	Web+IR	-19.989291923754095	-56.72105703278473	37584
2fc6b730924a3cadd3870bfd111148b8d351f441	sampling precision to depth 10000 at clef 2008	retrieval task;deeper rank;sampling precision;relevant item;particular document set;past test collection;ad hoc track;high precision;ad-hoc track;relevant document;information retrieval task;natural language query;retrieval method;hungarian information retrieval task;cross-language evaluation forum;natural language;information retrieval	We conducted an experiment to test the completeness of the relevance judgments for the monolingual Bulgarian, Czech and Hungarian information retrieval tasks of the Ad-Hoc Track of the Cross-Language Evaluation Forum (CLEF) 2007. In the ad hoc retrieval tasks, the system was given 50 natural language queries, and the goal was to find all of the relevant documents (with high precision) in a particular document set. For each language, we submitted a sample of the first 10000 retrieved items to investigate the frequency of relevant items at deeper ranks than the official judging depth (of 60 for Czech and 80 for Bulgarian and Hungarian). The results suggest that, on average, the percentage of relevant items assessed was less than 60% for Czech, 70% for Bulgarian and 85% for Hungarian. These levels of completeness are in line with the estimates that have been made for some past test collections which are still considered useful and fair for comparing retrieval methods.		Stephen Tomlinson	2008		10.1007/978-3-642-04447-2_20		Logic	-32.041426533533766	-63.28911989314939	37614
1aac772321a343a7887248acc62e2e158b47f309	context-orientated news riltering for web 2.0 and beyond	distance measure;information filtering;information overload;aggregation;web 2 0;rss;word senses;relevance feedback;tags;information search and retrieval;context	How can we solve the problem of information overload in news syndication? This poster outlines the path from keyword-based body text matching to distance-measurable taxonomic tag matching, on to context scale and practical uses.	information overload;web 2.0;web syndication	David Webster;Weihong Huang;Darren Mundy;Paul Warren	2006		10.1145/1135777.1135985	computer science;information filtering system;information overload;rss;internet privacy;web 2.0;world wide web;information retrieval	HCI	-28.4044343126934	-52.78380127806619	37644
52135fb8010c3d9554aaeb6756e355d2f43c220b	annotating character relationships in literary texts		We present a dataset of manually annotated relationships between characters in literary texts, in order to support the training and evaluation of automatic methods for relation type prediction in this domain (Makazhanov et al., 2014; Kokkinakis, 2013) and the broader computational analysis of literary character (Elson et al., 2010; Bamman et al., 2014; Vala et al., 2015; Flekova and Gurevych, 2015). In this work, we solicit annotations from workers on Amazon Mechanical Turk on four dimensions of interest: for a given pair of characters, we collect judgments as to the coarse-grained category (professional, social, familial), fine-grained category (friend, lover, parent, rival, employer), and affinity (positive, negative, neutral) that describes their primary relationship in a text. We do not assume that this relationship is static; we also collect judgments as to whether it changes at any point in the course of the text.	affinity analysis;amazon mechanical turk;the turk;vala	Philip Massey;Patrick Xia;David Bamman;Noah A. Smith	2015	CoRR		natural language processing;computer science;artificial intelligence;machine learning;linguistics	NLP	-22.080165764173056	-59.90712605889144	37674
5cdfad6755b54bde00c8dc0d0887233d8811971f	uncertainty detection in hungarian texts	p0 philology linguistics filologia;nyelveszet	Uncertainty detection is essential for many NLP applications. For instance, in information retrieval, it is of primary importance to distinguish among factual, negated and uncertain information. Current research on uncertainty detection has mostly focused on the English language, in contrast, here we present the first machine learning algorithm that aims at identifying linguistic markers of uncertainty in Hungarian texts from two domains: Wikipedia and news media. The system is based on sequence labeling and makes use of a rich feature set including orthographic, lexical, morphological, syntactic and semantic features as well. Having access to annotated data from two domains, we also focus on the domain specificities of uncertainty detection by comparing results obtained in indomain and cross-domain settings. Our results show that the domain of the text has significant influence on uncertainty detection.	algorithm;domain adaptation;experiment;information retrieval;lexicon;machine learning;natural language processing;orthographic projection;sequence labeling;supervised learning;wikipedia	Veronika Vincze	2014			natural language processing;computer science;linguistics	NLP	-25.199851173699173	-71.05320689894927	37711
365c9a5cd5d5a6d99948b2ba9dee9731fd8e7d23	exploring accumulative query expansion for relevance feedback	information retrieval;rocchio feedback;query expansion;relevance feedback;rf;qe	For the participation of Dublin City University (DCU) in the Relevance Feedback (RF) track of INEX 2010, we investigated the relation between the length of relevant text passages and the number of RF terms. In our experiments, relevant passages are segmented into non-overlapping windows of fixed length which are sorted by similarity with the query. In each retrieval iteration, we extend the current query with the most frequent terms extracted from these word windows. The number of feedback terms corresponds to a constant number, a number proportional to the length of relevant passages, and a number inversely proportional to the length of relevant passages, respectively. Retrieval experiments show a significant increase in MAP for INEX 2008 training data and improved precisions at early recall levels for the 2010 topics as compared to the baseline Rocchio feedback.	baseline (configuration management);experiment;iteration;microsoft windows;new general catalogue;query expansion;radio frequency;relevance feedback	Debasis Ganguly;Johannes Leveling;Gareth J. F. Jones	2010		10.1007/978-3-642-23577-1_29	computer science;theoretical computer science;data mining;information retrieval	Web+IR	-31.33062933637832	-62.86433933784787	37723
9275edeedcf1530b922e6bbf8fdb13b2cc09164f	improving the training and evaluation efficiency of recurrent neural network language models	boolean functions;indexes;speech recognition graphics processing units natural language processing pipeline processing recurrent neural nets;speech recognition language models recurrent neural network gpu;data structures;recurrent neural networks pipeline processing indexes data structures boolean functions;recurrent neural networks;pipelining recurrent neural network language model speech recognition gpu training time reduction class based model cpu test time rnnlm evaluation c rnnlm spliced sentence bunch technique variance regularisation softmax denominator f rnnlm;pipeline processing	Recurrent neural network language models (RNNLMs) are becoming increasingly popular for speech recognition. Previously, we have shown that RNNLMs with a full (non-classed) output layer (F-RNNLMs) can be trained efficiently using a GPU giving a large reduction in training time over conventional class-based models (C-RNNLMs) on a standard CPU. However, since test-time RNNLM evaluation is often performed entirely on a CPU, standard F-RNNLMs are inefficient since the entire output layer needs to be calculated for normalisation. In this paper, it is demonstrated that C-RNNLMs can be efficiently trained on a GPU, using our spliced sentence bunch technique which allows good CPU test-time performance (42× speedup over F-RNNLM). Furthermore, the performance of different classing approaches is investigated. We also examine the use of variance regularisation of the softmax denominator for F-RNNLMs and show that it allows F-RNNLMs to be efficiently used in test (56× speedup on a CPU). Finally the use of two GPUs for F-RNNLM training using pipelining is described and shown to give a reduction in training time over a single GPU by a factor of 1.6×.	artificial neural network;central processing unit;graphics processing unit;language model;pipeline (computing);recurrent neural network;softmax function;speech recognition;speedup	Xie Chen;Xunying Liu;Mark J. F. Gales;Philip C. Woodland	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7179003	database index;speech recognition;data structure;computer science;recurrent neural network;theoretical computer science;machine learning;time delay neural network;boolean function	Vision	-17.39905741314657	-78.41137547344822	37736
49bbdbb77caf05f060be35938cdb48b6f9d6d0bc	multiresolution semantic activity characterisation and abnormality discovery in videos	scene topology discovery;human behaviour;abnormality detection;activity reporting;endnotes;pubications;video understanding;semantic multimedia extraction	Graphical abstractDisplay Omitted HighlightsWe present a soft computing-based approach for automatic activity extraction from video.The proposed approach learns the activity model in an unsupervised way.Activities are characterised and analysed at different resolutions.Semantic information is delivered according to the resolution at which the activity is observed.The approach detects abnormalities based on analysis of statistics of the observed activities at different resolutions.The approach is generic and works both, indoors and outdoors. This paper addresses the issue of activity understanding from video and its semantics-rich description. A novel approach is presented where activities are characterised and analysed at different resolutions. Semantic information is delivered according to the resolution at which the activity is observed. Furthermore, the multiresolution activity characterisation is exploited to detect abnormal activity. To achieve these system capabilities, the focus is given on context modelling by employing a soft computing-based algorithm which automatically enables the determination of the main activity zones of the observed scene by taking as input the trajectories of detected mobiles. Such areas are learnt at different resolutions (or granularities). In a second stage, learned zones are employed to extract people activities by relating mobile trajectories to the learned zones. In this way, the activity of a person can be summarised as the series of zones that the person has visited. Employing the inherent soft relation properties, the reported activities can be labelled with meaningful semantics. Depending on the granularity at which activity zones and mobile trajectories are considered, the semantic meaning of the activity shifts from broad interpretation to detailed description. Activity information at different resolutions is also employed to perform abnormal activity detection.		Jose Luis Patino;James M. Ferryman	2014	Appl. Soft Comput.	10.1016/j.asoc.2014.08.039	computer vision;simulation;artificial intelligence;machine learning;human behavior	Robotics	-11.885777174186702	-52.23884530490106	37846
656779d40dd711b2b5bc83d12b8e656cec0b2a29	on the use of correspondence analysis to learn seed ontologies from text		In the present work we show our approach to generate hierarchies of concepts in the form of ontologies starting from free text. This approach relies on the statistical model of Correspondence Analysis to analyze term occurrences in text, identify the main concepts it refers to, and retrieve semantic relationships between them. We present a tool which is able to apply different methods for the generation of ontologies from text, namely hierarchy generation from hierarchical clustering representation, search for Hearst Patterns on the Web, and bootstrapping. Our evaluation shows that the precision in the generation of hierarchies of the tool is attested to be around 60% for the best automatic approach and around 90% for the best human-assisted	bootstrapping (compilers);cluster analysis;correspondence analysis;hierarchical clustering;ontology (information science);statistical model;world wide web	Davide Eynard;Fabio Marfia;Matteo Matteucci	2010			bootstrapping;correspondence analysis;statistical model;ontology (information science);data mining;hierarchical clustering;hierarchy;computer science	AI	-28.261314026135697	-66.9422136730268	37895
d23f1b8d67439d14f286bf708c63f0ade2efbd4d	tweet sentiment visualization and classification using manifold dimensionality reduction			dimensionality reduction	Francisco Grimaldo;Emilia López-Iñesta;Guilherme Ziolle;Juan Gómez-Sanchís;Melissa M Pearlstone;Tanakorn Kittikool;Emilio Soria-Olivas	2018		10.3233/978-1-61499-918-8-227		HPC	-23.88391425018263	-60.84901365077872	38045
202c3e82a7c6624bf7ed612d61dbb1307a133511	evaluation of sentiment polarity prediction using a dimensional and a categorical approach	databases;statistic metrics;motion pictures semantics support vector machines numerical models databases measurement feature extraction;movie review dataset;dominance;sentiment polarity prediction;machine learning algorithm;measurement;motion pictures;support vector machines;statistic metrics sentiment polarity vad model;bayes methods;naive bayes;semantics;text analysis;emotion recognition;word semantic orientation;dimensional approach;text instance classification sentiment polarity prediction dimensional approach categorical approach utterance sentiment polarity text expressiveness valence arousal dominance word semantic orientation chi square statistic metrics relevance factor statistic metrics feature extraction predictability potential machine learning algorithm naive bayes svm c4 5 emotional dataset semeval affective text isear international survey on emotional antecedents and reactions children fairy tales movie review dataset database content;text expressiveness;emotional dataset;relevance factor statistic metrics;international survey on emotional antecedents and reactions;statistical analysis;chi square statistic metrics;text instance classification;feature extraction;database content;semeval affective text;pattern classification;sentiment polarity;valence;predictability potential;children fairy tales;svm;arousal;categorical approach;numerical models;learning artificial intelligence;vad model;c4 5;text analysis bayes methods emotion recognition feature extraction learning artificial intelligence pattern classification statistical analysis;isear;utterance sentiment polarity	In this paper we evaluate two approaches for predicting the sentiment polarity of an utterance. The first method is based on a 3-dimensional model which takes into account text expressiveness in terms of valence, arousal and dominance. The second one determines the word's semantic orientation according to Chi-square and Relevance factor statistic metrics. We describe the general flow of the methods and their extracted features, as well as their predictability potential using different machine learning algorithms, Naïve Bayes, SVM and C4.5. The evaluation is performed on four emotional datasets: Semeval 2007 “Affective Text”, ISEAR (International Survey on Emotional Antecedents and Reactions), children's fairy-tales and a movie review dataset. The results show a high correlation of the prediction performance with the database content, as well as to the average number of words within the classified text instances.	c4.5 algorithm;chi;machine learning;naive bayes classifier;protein structure prediction;relevance;semeval;sentiment analysis	Ioana Muresan;Adriana Stan;Mircea Giurgiu;Rodica Potolea	2013	2013 7th Conference on Speech Technology and Human - Computer Dialogue (SpeD)	10.1109/SpeD.2013.6682645	computer science;machine learning;pattern recognition;data mining	NLP	-22.11058097680359	-65.41830771765547	38048
91ccd68f79408608e0e3f199cfbb1e9f7893c345	erp indices of situated reference in visual contexts		Violations of the maxims of Quantity occur when utterances provide more (over-specified) or less (under-specified) information than strictly required for referent identification. While behavioural data suggest that under-specified expressions lead to comprehension difficulty and communicative failure, there is no consensus as to whether over-specified expressions are also detrimental to comprehension. In this study we shed light on this debate, providing neurophysiological evidence supporting the view that extra information facilitates comprehension. We further present novel evidence that referential failure due to underspecification is qualitatively different from explicit cases of referential failure, when no matching referential candidate is available in the context.	erp;formal language;list comprehension;operating system;situated	Elli Tourtouri;Francesca Delogu;Matthew W. Crocker	2015			psychology;cognitive psychology;referent;social psychology;situated;underspecification;comprehension;expression (mathematics)	NLP	-7.174503269511028	-78.09235457377314	38064
ec749716a31b7daa6d3c1739324dc9dca4eb9944	still talking to machines (cognitively speaking).		This overview article reviews the structure of a fully statistical spoken dialogue system (SDS), using as illustration, various systems and components built at Cambridge over the last few years. Most of the components in an SDS are essentially classifiers which can be trained using supervised learning. However, the dialogue management component must track the state of the dialogue and optimise a reward accumulated over time. This requires techniques for statistical inference and policy optimisation using reinforcement learning. The potential advantages of a fully statistical SDS are the ability to train from data without hand-crafting, increased robustness to environmental noise and user uncertainty, and the ability to adapt and learn on-line.	dialog system;mathematical optimization;online and offline;reinforcement learning;spoken dialog systems;supervised learning	Steve J. Young	2010			statistical inference;robustness (computer science);supervised learning;reinforcement learning;environmental noise;artificial intelligence;pattern recognition;computer science	NLP	-14.526413221428289	-75.00322494660051	38071
7a24be92f23e7d2b07c7a1396d401a058870bb5f	using wordnet to complement training information in text categorization	vector space model;low frequency;lexical database;machine learning;natural language;relevance feedback;text categorization	Automatic Text Categorization (TC) is a complex and useful task for many natural language applications, and is usually performed through the use of a set of manually classified documents, a training collection. We suggest the utilization of additional resources like lexical databases to increase the amount of information that TC systems make use of, and thus, to improve their performance. Our approach integrates WordNet information with two training approaches through the Vector Space Model. The training approaches we test are the Rocchio (relevance feedback) and the Widrow-Hoff (machine learning) algorithms. Results obtained from evaluation show that the integration of WordNet clearly outperforms training approaches, and that an integrated technique can effectively address the classification of low frequency categories.	algorithm;artificial neural network;bluetooth;categorization;communications of the acm;dictionary;document classification;experiment;lexical analysis;lexical database;machine learning;natural language;relevance feedback;viable system model;wordnet	Manuel de Buenaga Rodríguez;José María Gómez Hidalgo;Belén Díaz-Agudo	1997	CoRR		natural language processing;wordnet;computer science;machine learning;pattern recognition;linguistics;low frequency;natural language;vector space model;information retrieval	NLP	-25.416168000939997	-68.35422496035989	38153
eb7a02968d126b1c93d05c88ccde4f10fb2556fa	a weighted maximum entropy language model for text classification	language model;natural language processing;text segmentation;maximum entropy;maximum entropy model	The Maximum entropy (ME) approach has been extensively used for various natural language processing tasks, such as language modeling, part-of-speech tagging, text segmentation and text classification. Previous work in text classification has been done using maximum entropy modeling with binary-valued features or counts of feature words. In this work, we present a method to apply Maximum Entropy modeling for text classification in a different way it has been used so far, using weights for both to select the features of the model and to emphasize the importance of each one of them in the classification task. Using the X square test to assess the contribution of each candidate feature from the obtained X square values we rank the features and the most prevalent of them, those which are ranked with the higher X square scores, they are used as the selected features of the model. Instead of using Maximum Entropy modeling in the classical way, we use the X square values to weight the features of the model and give thus a different importance to each one of them. The method has been evaluated on Reuters-21578 dataset for test classification tasks, giving very promising results and performing comparable to some of the “state of the art” systems in the classification field.	binary data;document classification;language model;maximum entropy spectral estimation;natural language processing;part-of-speech tagging;text segmentation	Kostas Fragos;Yannis Maistros;Christos Skourlas	2005			natural language processing;maximum-entropy markov model;principle of maximum entropy;machine learning;pattern recognition	NLP	-22.241145983996045	-65.91342317985684	38221
95e627efd0922759b82f5fb9025ef25aeab02592	proposition of improvement areas in most heavy an light stemmer algorithms novel stemmer: est.stemmer	computers;morphology;internet;syntactics;dictionaries;classification algorithms;java	Analysis of Arabic language has become a necessity because of its big evolution; we propose in this paper a novel classification algorithm called EST.Stemmer of Arabic text, it presents many important improvement areas which deals with some issues in Heavy Stemmers and Light Stemmers.	algorithm	Hanane El Manssouri;Soufiane Farrah;El Houssaine Ziyati;Mohammed Ouzzif	2016	2016 4th IEEE International Colloquium on Information Science and Technology (CiSt)	10.1109/CIST.2016.7804967	computer science;artificial intelligence;data mining;algorithm	Vision	-23.48212646682148	-68.7290842934307	38302
45fee968b80b770a4de9e5e572145fc752a71b92	the use of hierarchic clustering in information retrieval.	hierarchical clustering;organization;information retrieval;cluster grouping	Abstract We introduce information retrieval strategies which are based on automatic hierarchic clustering of documents. We discuss the evaluation of retrieval strategies and show, using a subset of the Cranfield Aeronautics document collection, that cluster-based retrieval strategies can be devised which are as effective as linear associative retrieval strategies and much more efficient. Finally, we outline how cluster-based retrieval may be extended to large growing document collections and indicate some ways in which the effectiveness of cluster-based retrieval strategies may be improved.		N. Jardine;C. J. van Rijsbergen	1971	Information Storage and Retrieval	10.1016/0020-0271(71)90051-9	correlation clustering;document clustering;fuzzy clustering;computer science;organization;pattern recognition;data mining;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;dendrogram;information retrieval;hierarchical clustering of networks;clustering high-dimensional data;conceptual clustering	Web+IR	-27.81975568000511	-60.26969797664641	38316
54fa2757ad9177a0384b14a2e48d235351be09e2	summary extraction from chinese text for data archives of online news	logographic text;data archives;sinogrammatic electronic news sentence;electronic news media;linguistic knowledge representation;efficient knowledge delivery;particular text sequence interconnects;chinese text;machine representation;summary extraction;online news distribution;machine comparison;human chosen sentence	logographic text;data archives;sinogrammatic electronic news sentence;electronic news media;linguistic knowledge representation;efficient knowledge delivery;particular text sequence interconnects;chinese text;machine representation;summary extraction;online news distribution;machine comparison;human chosen sentence		Nozomi Mikami;Lukas Pichl	2011		10.1007/978-3-642-25731-5_16	natural language processing;speech recognition;computer science;artificial intelligence;data mining;database;programming language;world wide web	NLP	-32.052454918278265	-75.89226122228337	38332
3e408acf397874d4859352246c4d485043a36561	second language acquisition modeling		We present the task of second language acquisition (SLA) modeling. Given a history of errors made by learners of a second language, the task is to predict errors that they are likely to make at arbitrary points in the future. We describe a large corpus of more than 7M words produced by more than 6k learners of English, Spanish, and French using Duolingo, a popular online language-learning app. Then we report on the results of a shared task challenge aimed studying the SLA task via this corpus, which attracted 15 teams and synthesized work from various fields including cognitive science, linguistics, and machine learning.	cognitive science;machine learning;service-level agreement;text corpus	Burr Settles;Chris Brust;Erin Gustafson;Masato Hagiwara;Nitin Madnani	2018			natural language processing;second-language acquisition;computer science;artificial intelligence	NLP	-20.83739044942809	-71.08060537583826	38345
8a12f3612a4fba4ac5a88e74190ab4e3dba602e9	fipsromanian: towards a romanian version of the fips syntactic parser	work in progress	We describe work in progress on the development of a full syntactic parser for Romanian. This work is part of a larger project of multilingual extension of the Fips parser (Wehrli, 2007), already available for French, English, German, Spanish, Italian, and Greek, to four new languages (Romanian, Romansh, Russian and Japanese). The Romanian version was built by starting with the Fips generic parsing architecture for the Romance languages and customising the grammatical component, in close relation to the development of the lexical component. We describe this process and report on preliminary results obtained for journalistic texts.	parser	Violeta Seretan;Eric Wehrli;Luka Nerima;Gabriela Soare	2010			syntax;work in process;natural language processing;architecture;artificial intelligence;parsing;linguistics;computer science;romanian;romance languages;german	NLP	-29.407320981891324	-76.70435650776574	38392
4bb51966222accaa2b28d93284095a76bb17f659	a discriminative kernel-based approach to rank images from text queries	busqueda informacion;modelizacion;anotacion;optimisation;kernel;text;learning algorithm;kernel based classifier;performance evaluation;optimizacion;support vector machines;recherche image;methode noyau;search engines;average precision;information retrieval;analisis forma;large margin;vocabulary;interrogation base donnee;image annotation task;interrogacion base datos;text analysis;photography;annotation;intelligence artificielle;ranking problem;texte;algorithme apprentissage;text analysis image retrieval support vector machines;online learning;image annotation;image contrast;wire;discriminant analysis;analyse discriminante;modelisation;model evaluation;photographie;analisis discriminante;hierarchical classification;algorithms artificial intelligence discriminant analysis image enhancement image interpretation computer assisted information storage and retrieval natural language processing pattern recognition automated vocabulary controlled;ranking;recherche information;contraste image;discrimination learning;metodo nucleo;classification hierarchique;fotografia;stock photography data;corel dataset;artificial intelligence;discriminative learning;kernel method;web search;predictive models;optimization;pattern analysis;inteligencia artificial;large margin image retrieval ranking discriminative learning kernel based classifier;discriminative ranking approach;texto;imagen contraste;algoritmo aprendizaje;modeling;clasificacion jerarquizada;database query;corel dataset text queries image retrieval ranking problem image annotation task kernel based classifiers stock photography data discriminative ranking approach;discriminative model;image retrieval vocabulary photography kernel performance evaluation web search search engines wire labeling predictive models;text queries;analyse forme;labeling;kernel based classifiers;image retrieval	This paper introduces a discriminative model for the retrieval of images from text queries. Our approach formalizes the retrieval task as a ranking problem, and introduces a learning procedure optimizing a criterion related to the ranking performance. The proposed model hence addresses the retrieval problem directly and does not rely on an intermediate image annotation task, which contrasts with previous research. Moreover, our learning procedure builds upon recent work on the online learning of kernel-based classifiers. This yields an efficient, scalable algorithm, which can benefit from recent kernels developed for image comparison. The experiments performed over stock photography data show the advantage of our discriminative ranking approach over state-of-the-art alternatives (e.g. our model yields 26.3% average precision over the Corel dataset, which should be compared to 22.0%, for the best alternative model evaluated). Further analysis of the results shows that our model is especially advantageous over difficult queries such as queries with few relevant pictures or multiple-word queries.	addresses (publication format);algorithm;appendix;automatic image annotation;benchmark (computing);body of uterus;computer vision;discriminative model;document;empirical risk minimization;evaluation procedure;experiment;image retrieval;information retrieval;inspiration function;isoelectric point;iteration;kernel;learning to rank;loss function;mathematical induction;question (inquiry);ranking svm;relevance;s100 calcium binding protein a10;scalability;semantics (computer science);silo (dataset);support vector machine;test data	David Grangier;Samy Bengio	2008	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2007.70791	support vector machine;kernel method;labeling theory;kernel;systems modeling;ranking;image retrieval;computer science;photography;machine learning;pattern recognition;predictive modelling;information retrieval;discriminative model;discrimination learning	Web+IR	-13.219087885871163	-61.2812058376165	38399
82c990aa15e2e35de8294b4a721785da1ede20d0	fine-grained pos tagging of german tweets		This paper presents the first work on POS tagging German Twitter data, showing that despite the noisy and often cryptic nature of the data a fine-grained analysis of POS tags on Twitter microtext is feasible. Our CRF-based tagger achieves an accuracy of around 89% when trained on LDA word clusters, features from an automatically created dictionary and additional out-of-domain training data.	brown corpus;conditional random field;dictionary;microprinting;part-of-speech tagging	Ines Rehbein	2013		10.1007/978-3-642-40722-2_17	information retrieval;user-generated content;training set;computer science;german	NLP	-22.40491360930429	-71.67845830006841	38445
5899d3cec5c02372ac2cd8eb77d3347d83aeab4a	using tweets to help sentence compression for news highlights generation		We explore using relevant tweets of a given news article to help sentence compression for generating compressive news highlights. We extend an unsupervised dependency-tree based sentence compression approach by incorporating tweet information to weight the tree edge in terms of informativeness and syntactic importance. The experimental results on a public corpus that contains both news articles and relevant tweets show that our proposed tweets guided sentence compression method can improve the summarization performance significantly compared to the baseline generic sentence compression method.	baseline (configuration management);sentence extraction;social media;text corpus;timeline;unsupervised learning	Zhongyu Wei;Yang Liu;Chen Li;Wei Gao	2015			natural language processing;multimedia;information retrieval	NLP	-25.80361434052195	-64.82468093225226	38473
5b1d78b160560db5f581e65289ce5e2f99eb9b1f	twitter100k: a real-world dataset for weakly supervised cross-media retrieval		This paper contributes a new large-scale dataset for weakly supervised cross-media retrieval, named Twitter100k. Current datasets, such as Wikipedia, NUS Wide, and Flickr30k, have two major limitations. First, these datasets are lacking in content diversity, i.e., only some predefined classes are covered. Second, texts in these datasets are written in well-organized language, leading to inconsistency with realistic applications. To overcome these drawbacks, the proposed Twitter100k dataset is characterized by two aspects: it has 100 000 image–text pairs randomly crawled from Twitter, and thus, has no constraint in the image categories; and text in Twitter100k is written in informal language by the users. Since strongly supervised methods leverage the class labels that may be missing in practice, this paper focuses on weakly supervised learning for cross-media retrieval, in which only text-image pairs are exploited during training. We extensively benchmark the performance of four subspace learning methods and three variants of the correspondence AutoEncoder, along with various text features on Wikipedia, Flickr30k, and Twitter100k. As a minor contribution, we also design a deep neural network to learn cross-modal embeddings for Twitter100k. Inspired by the characteristic of Twitter100k, we propose a method to integrate optical character recognition into cross-media retrieval. The experiment results show that the proposed method improves the baseline performance.	artificial neural network;autoencoder;baseline (configuration management);benchmark (computing);deep learning;experiment;high- and low-level;image retrieval;modal logic;optical character recognition;randomness;supervised learning;way to go;wikipedia	Yuting Hu;Liang Zheng;Yi Yang;Yongfeng Huang	2018	IEEE Transactions on Multimedia	10.1109/TMM.2017.2760101	artificial intelligence;supervised learning;autoencoder;optical character recognition;pattern recognition;computer science;the internet;visualization;artificial neural network;subspace topology	Vision	-15.988702016663066	-68.58582292384251	38563
124cf31b7ea925bdd5f852d5dd40189531b33b72	treebank conversion based self-training strategy for parsing		In this paper, we propose a novel selftraining strategy for parsing which is based on Treebank conversion (SSPTC). In SSPTC, we make full use of the strong points of Treebank conversion and self-training, and offset their weaknesses with each other. To provide good parse selection strategies which are needed in self-training, we score the automatically generated parse trees with parse trees in source Treebank as a reference. To maintain the constituency between source Treebank and conversion Treebank which is needed in Treebank conversion, we get the conversion trees with the help of self-training. In our experiments, SSPTC strategy is utilized to parse Tsinghua Chinese Treebank with the help of Penn Chinese Treebank. The results significantly outperform the baseline parser.	baseline (configuration management);brown corpus;ccir system b;digital media;experiment;f1 score;formal grammar;parsing;part-of-speech tagging;semantics (computer science);test set;treebank	Zhiguo Wang;Chengqing Zong	2010			natural language processing;speech recognition;computer science;linguistics	NLP	-22.825585460184726	-75.57417748061539	38602
652b19093aed0e0760752b5f5dac5f8a7cf9acdc	interrupting constructions in a rejuvenated amazon grammar	evaluation method;article in monograph or in proceedings	This paper reports on the latest rejuvenation of AMAZON , a structuralist parser for Dutch written sentences. Unlike older versions, the new AMAZON parser has been developed in a modular organization, with an empirical cycle containing evaluations on corpus material. This methodology facilitates the development by separate r es archers, and it gives more insight into the actual performance of the parser, providin g a useful means of measuring the improvement during development. In this paper, the evaluat ion method, and its outcome, is presented in general. As a more specific case study, the imp l mentation of a separate module for interruption constructions is discussed.	energy (psychological);experiment;f1 score;interrupt;modular design;parsing;partial evaluation;rough set;text corpus;treebank	Carla Schelfhout;Peter-Arno Coppen	2003			computer science;linguistics;literature	NLP	-28.757509809899865	-75.78953742917679	38611
f4d5cbeefaf1d5b81ebd673754a51890a9ae44a0	incorporating prior knowledge into word embedding for chinese word similarity measurement		Word embedding-based methods have received increasing attention for their flexibility and effectiveness in many natural language-processing (NLP) tasks, including Word Similarity (WS). However, these approaches rely on high-quality corpus and neglect prior knowledge. Lexicon-based methods concentrate on human’s intelligence contained in semantic resources, e.g., Tongyici Cilin, HowNet, and Chinese WordNet, but they have the drawback of being unable to deal with unknown words. This article proposes a three-stage framework for measuring the Chinese word similarity by incorporating prior knowledge obtained from lexicons and statistics into word embedding: in the first stage, we utilize retrieval techniques to crawl the contexts of word pairs from web resources to extend context corpus. In the next stage, we investigate three types of single similarity measurements, including lexicon similarities, statistical similarities, and embedding-based similarities. Finally, we exploit simple combination strategies with math operations and the counter-fitting combination strategy using optimization method. To demonstrate our system’s efficiency, comparable experiments are conducted on the PKU-500 dataset. Our final results are 0.561/0.516 of Spearman/Pearson rank correlation coefficient, which outperform the state-of-the-art performance to the best of our knowledge. Experiment results on Chinese MC-30 and SemEval-2012 datasets show that our system also performs well on other Chinese datasets, which proves its transferability. Besides, our system is not language-specific and can be applied to other languages, e.g., English.	algorithm;coefficient;experiment;lexicon;mathematical optimization;microsoft word for mac;natural language processing;ring counter;semeval;web resource;word embedding;wordnet	Degen Huang;Jiahuan Pei;Kaiyu Huang;Jianjun Ma	2018	ACM Trans. Asian & Low-Resource Lang. Inf. Process.	10.1145/3182622	word embedding;web resource;rank correlation;transferability;exploit;wordnet;artificial intelligence;pattern recognition;computer science;lexicon	NLP	-26.553043625527184	-67.27347091281084	38624
3cb0c2955d2c8d0e0cc217110a599e6001b39712	deep recurrent neural network vs. support vector machine for aspect-based sentiment analysis of arabic hotels' reviews		Abstract In this research, state-of-the-art approaches based on supervised machine learning are presented to address the challenges of aspect-based sentiment analysis (ABSA) of Arabic Hotels’ reviews. Two approaches of deep recurrent neural network (RNN) and support vector machine (SVM) are implemented and trained along with lexical, word, syntactic, morphological, and semantic features. The proposed approaches are evaluated using a reference dataset of Arabic Hotels’ reviews. Evaluation results show that the SVM approach outperforms the other deep RNN approach in the research investigated tasks ( T1: aspect category identification , T2: aspect opinion target expression (OTE) extraction , and T3: aspect sentiment polarity identification ). Whereas, when focusing on the execution time required for training and testing the models, the deep RNN execution time was faster, especially for the second task.	artificial neural network;recurrent neural network;sentiment analysis;support vector machine	Mohammad Al-Smadi;Omar Qawasmeh;Mahmoud Al-Ayyoub;Yaser Jararweh;Brij Gupta	2018	J. Comput. Science	10.1016/j.jocs.2017.11.006	arabic;deep learning;syntax;sentiment analysis;support vector machine;computer science;machine learning;recurrent neural network;pattern recognition;artificial intelligence	ML	-20.397515870989242	-70.79886381047633	38644
5b74d00e9b9b50b688112a213253fb6846b9c8b4	optimising task-based video quality	optimisation;quality;experiments;video;models	Development of techniques for assessing video quality is reviewed. Examples have been provided on the quality of video applications ranging from popular entertainment to new trends such as applications in wide-reaching public systems, not just those used by security forces but also for medical purposes. In particular, two typical usages of task-based video: surveillance video for accurate licence plate recognition, and medical video for credible diagnosis prior to bronchoscopic surgery were introduced by the author. The problem of task-based video quality assessment starting from subjective psychophysiological experiments to objective quality models is discussed. Example test results and models are provided alongside to the descriptions. Finally, a quality optimisation approach, driven by recognition rates is presented.	automatic number plate recognition;closed-circuit television;experiment;mathematical optimization;video	Mikolaj Leszczuk	2012	Multimedia Tools and Applications	10.1007/s11042-012-1161-6	subjective video quality;computer vision;simulation;video;telecommunications;computer science;multimedia;world wide web;computer security	Vision	-13.578211956128245	-53.29296414768738	38653
5d70e0d06db59241330272405e56ebfee6b43a96	adversarial reinforcement learning for chinese text summarization		This paper proposes a novel Adversarial Reinforcement Learning architecture for Chinese text summarization. Previous abstractive methods commonly use Maximum Likelihood Estimation (MLE) to optimize the generative models, which makes auto-generated summary less incoherent and inaccuracy. To address this problem, we innovatively apply the Adversarial Reinforcement Learning strategy to narrow the gap between the generated summary and the human summary. In our model, we use a generator to generate summaries, a discriminator to distinguish between generated summaries and real ones, and reinforcement learning (RL) strategy to iteratively evolve the generator. Besides, in order to better tackle Chinese text summarization, we use a character-level model rather than a word-level one and append Text-Attention in the generator. Experiments were run on two Chinese corpora, respectively consisting of long documents and short texts. Experimental Results showed that our model significantly outperforms previous deep learning models on rouge score.	append;automatic summarization;chinese wall;deep learning;discriminator;generative model;rouge (metric);reinforcement learning;text corpus	Hao Xu;Yanan Cao;Yanmin Shang;Yanbing Liu;Jianlong Tan;Li Guo	2018		10.1007/978-3-319-93713-7_47	generative grammar;distributed computing;automatic summarization;rouge;reinforcement learning;discriminator;deep learning;architecture;append;machine learning;computer science;artificial intelligence	AI	-18.663402439615773	-75.16859316675449	38660
053ab080a21b0278e4cd0c8ef60f68e52feed96f	improving synoptic quering for source retrieval: notebook for pan at clef 2015		Source retrieval is a part of a plagiarism discovery process, where only a selected set of candidate documents is retrieved from a large corpus of potential source documents and passed for detailed document comparison in order to highlight potential plagiarism. This paper describes a used methodology and the architecture of a source retrieval system, developed for PAN 2015 lab on uncovering plagiarism, authorship and social software misuse. The system is based on our previous systems used at PAN since 2012. The paper also discusses the queries performance and provides explanation for many implementation settings. The proposed methodology achieved the highest recall with usage of the least number of queries among other PAN 2015 softwares during the official test run. The source retrieval subsystem forms an integral part of a modern system for plagiarism discovery.	floor and ceiling functions	Simon Suchomel;Michal Brandejs	2015			world wide web;information retrieval;clef;computer science	Web+IR	-32.063808663588034	-62.01692642958832	38690
b0260e31db56ea5d606ce5e89189befe15f493e2	inferring users' gender from interests: a tag embedding approach				Peisong Zhu;Tieyun Qian;Ming Zhong;Xuhui Li	2016		10.1007/978-3-319-46681-1_11	pattern recognition;data mining;information retrieval	NLP	-24.171396780270165	-61.017013726331996	38713
d4eac8e9cabc48126502effb8179a0cc92438dbf	a framework for evaluating human action detection via multidimensional approach	video detection;front end;video signal processing c language graphical user interfaces hidden markov models inference mechanisms java mathematics computing ontologies artificial intelligence video retrieval;mathematics computing;knowledge representation scheme;legged locomotion;video signal processing;human action detection audio feature visual feature hidden markov model;hidden markov model;video retrieval;inference mechanisms;graphical user interface;data mining;ontologies artificial intelligence;audio feature;artificial intelligent;visualization;c language;visual feature;graphical user interfaces;hidden markov models;data extraction;image color analysis;feature extraction;process based ontology;visual features;component model;model classifier;human action detection;humans;humans multidimensional systems mathematical model layout artificial intelligence data mining ontologies information retrieval knowledge representation engines;inference engine;qualitative modeling;knowledge representation;multimodality features;video action scene detector tool;construction logic;matlab;multidimensional approach;java;multimodality features multidimensional approach human action detection evaluation data extraction process based ontology video retrieval video detection knowledge representation scheme model classifier inference engine video action scene detector tool construction logic java matlab c language graphical user interface;human action detection evaluation	This work discusses the application of an Artificial Intelligence technique called data extraction and a process-based ontology in constructing experimental qualitative models for video retrieval and detection. We present a framework architecture that uses multimodality features as the knowledge representation scheme to model the behaviors of a number of human actions in the video scenes. The main focus of this paper placed on the design of two main components (model classifier and inference engine) for a tool abbreviated as VASD (Video Action Scene Detector) for retrieving and detecting human actions from video scenes. The discussion starts by presenting the workflow of the retrieving and detection process and the automated model classifier construction logic. We then move on to demonstrate how the constructed classifiers can be used with multimodality features for detecting human actions. Finally, behavioral explanation manifestation is discussed. The simulator is implemented in bilingual; Math Lab and C++ are at the backend supplying data and theories while Java handles all front-end GUI and action pattern updating.	artificial intelligence;c++;graphical user interface;inference engine;java;knowledge representation and reasoning;sensor	Lili Nurliyana Abdullah	2009	2009 Sixth International Conference on Computer Graphics, Imaging and Visualization	10.1109/CGIV.2009.48	computer vision;computer science;machine learning;data mining	Vision	-12.871209239396215	-55.40402756149388	38726
6326b3ac9a6feae3c8d8083ac9c69b893e33d94a	a web-based medical video indexing environment	content management;video structure;histograms;synchronized multimedia integration language;component;browsing;web based medical video indexing environment;description display;medical videos;xml content management indexing internet;timed text xml component shot boundary detection video indexing smil;video indexing;media;html;timed text xml;image color analysis indexing xml media histograms html streaming media;internet;element generation;indexing;streaming media;image color analysis;embedded media player;xml;smil;video content management systems;media synchronization;html documents;web based indexing environment;timed text xml web based medical video indexing environment video content management systems web based indexing environment medical videos video structure element generation description display browsing html documents synchronized multimedia integration language media synchronization embedded media player;shot boundary detection	Video indexing is an important process in video content management systems. In this paper, we extend our previous work by presenting a new web-based indexing environment for medical videos. The proposed system provides two ways to describe a video structure: Element Generation and Description Display for Browsing. After shot boundaries are detected, HTML documents combined with SMIL (Synchronized Multimedia Integration Language) elements are generated. SMIL timing and media synchronization elements and attributes manage the location of shot boundaries and allow the users to browse and preview each shot within the video. The system enables browsing the entire content of the video displaying video descriptions for each segmented shot using an embedded media player and Timed Text XML. Through the use of the Timed Text XML file, textual information of the content in every shot is displayed on the media player.	browsing;content management system;digital video;embedded system;html;synchronized multimedia integration language;timed text;web application;xml	Engin Mendi;Coskun Bayrak	2010	2010 IEEE Fourth International Conference on Semantic Computing	10.1109/ICSC.2010.35	synchronized multimedia integration language;computer science;video capture;video tracking;database;multimedia;video processing;world wide web;information retrieval	DB	-14.677736806199457	-54.896782205810375	38761
96a989f3d3f0e67c834f53a3dec56d583fab1194	visualizing a correlative multi-level graph of biology entity interactions	visualization paradigm;graph theory;biology computing;interconnected networks;correlative core cancer terms network;terrain surface arrangement;data visualization intelligent networks bioinformatics surface treatment systems biology cancer large scale systems systematics biological information theory scalability;association mining;cancer;data mining;genetics;interconnection network;data visualisation;core gene terms network correlative multilevel graph biology entity interactions visualization paradigm interconnected networks term association mining terrain surface visualization units terrain surface arrangement terrain surface correlation correlative core cancer terms network;visualization;shape;term association mining;data visualization;diseases;biological information theory;terrain surface visualization units;biology entity interactions;core gene terms network;terrain surface correlation;correlative multilevel graph;graph theory biology computing cancer data mining data visualisation genetics	In this paper we present a new visualization paradigm to represent and assist the understanding of a correlative multi-level graph, a group of inter-connected networks. Such a graph is formed via term association mining, and the visualization paradigm consists of three components: terrain surface visualization units, terrain surface arrangement, and terrain surface correlation. We apply this paradigm to visualize and explore a pair of correlative core cancer terms network and core gene terms network. The results show that our visualization paradigm design is consistent with the derived associations, and is effective in preserving major features as the landmarks in the terrain surfaces.	association rule learning;biological network;biomedical text mining;entity;interaction;interpolation;numerical analysis;programming paradigm;visual paradigm for uml	Qian You;Shiaofen Fang;Snehasis Mukhopadhyay;Harsha Gopal Goud Vaka;Jake Yue Chen	2009	2009 International Conference on Network-Based Information Systems	10.1109/NBiS.2009.37	visualization;shape;computer science;bioinformatics;theoretical computer science;data mining;data visualization;cancer	Visualization	-7.3885865530486035	-52.141412594176515	38766
61c17d819a68daaef78f2db0dbb827d2ebde1f28	safe: a simple approach for feature extraction from app descriptions and app reviews		A main advantage of app stores is that they aggregate important information created by both developers and users. In the app store product pages, developers usually describe and maintain the features of their apps. In the app reviews, users comment these features. Recent studies focused on mining app features either as described by developers or as reviewed by users. However, extracting and matching the features from the app descriptions and the reviews is essential to bear the app store advantages, e.g. allowing analysts to identify which app features are actually being reviewed and which are not. In this paper, we propose SAFE, a novel uniform approach to extract app features from the single app pages, the single reviews and to match them. We manually build 18 part-of-speech patterns and 5 sentence patterns that are frequently used in text referring to app features. We then apply these patterns with several text pre-and post-processing steps. A major advantage of our approach is that it does not require large training and configuration data. To evaluate its accuracy, we manually extracted the features mentioned in the pages and reviews of 10 apps. The extraction precision and recall outperformed two state-of-the-art approaches. For well-maintained app pages such as for Google Drive our approach has a precision of 87% and on average 56% for 10 evaluated apps. SAFE also matches 87% of the features extracted from user reviews to those extracted from the app descriptions.	aggregate data;app store;design pattern;feature extraction;google drive;human-readable medium;information retrieval;machine learning;mathematical optimization;mobile app;natural language;precision and recall;preprocessor;safe-biopharma association;semantic similarity;user review;video post-processing	Timo Johann;Christoph Stanik;B. M. Alizadeh AlirezaM.Alizadeh;Walid Maalej	2017	2017 IEEE 25th International Requirements Engineering Conference (RE)	10.1109/RE.2017.71	requirements engineering;precision and recall;software;feature extraction;app store;data mining;computer science;sentence	SE	-23.832226294108462	-65.69191833725004	38850
033471c870410bdc34f9bf9d2b7704734f642092	extracting phrase patterns with minimum redundancy for unsupervised speaker role classification	redundancy removal;minimum redundancy;compact phrase pattern list;graph pruning algorithm;role classification accuracy;phrase pattern;n-gram lexical feature;phrase patterns result;extracting phrase pattern;unsupervised speaker role classification;mandarin talk shows;large corpus	This paper addresses the problem of learning phrase patterns for unsupervised speaker role classification. Phrase patterns are automatically extracted from large corpora, and redundant patterns are removed via a graph pruning algorithm. In experiments on English and Mandarin talk shows, the use of phrase patterns results in an increase of role classification accuracy over n-gram lexical features, and more compact phrase pattern lists are obtained due to the redundancy removal.	algorithm;document classification;experiment;grams;n-gram;semi-supervised learning;semiconductor industry;statistical classification;super robot monkey team hyperforce go!;supervised learning;text corpus	Bin Zhang;Brian Hutchinson;Wei Wu;Mari Ostendorf	2010			natural language processing;speech recognition;computer science;phrase search;pattern recognition	NLP	-23.275044254865627	-73.13605502710134	38877
51b14b1f876e820ebd73c3839af6272e846cbe19	part-of-speech tagging of portuguese based on variable length markov chains	long distance;part of speech tagging;part of speech;long range dependent;markov chain	Abstra t. Tagging is the task of attributing to words in ontext in a text, their orresponding Part-of-Spee h (PoS) lass. In this work, we have employed Variable Length Markov Chains (VLMC) for tagging, in the hope of apturing long distan e dependen ies. We obtained one of the best PoS tagging of Portuguese, with a pre ision of 95.51%. More surprisingly, we did that with a total time of training and exe ution of less than 3 minutes for a orpus of almost 1 million words. However, long distan e dependen ies are not well aptured by the VLMC tagger, and we investigate the reasons and limitations of the use of VLMCs. Future resear hes in statisti al linguisti s regarding long range dependen ies should on entrate in other ways of solving this limitation.	brill tagger;markov chain;part-of-speech tagging	Fábio Natanael Kepler;Marcelo Finger	2006		10.1007/11751984_32	natural language processing;speech recognition;computer science;communication	NLP	-24.271552909731994	-78.4894458471725	38896
08c3dbbd464012c56a5aa94d8b7ea41b27da1a7d	racai gec - a hybrid approach to grammatical error correction		This paper describes RACAI’s (Research Institute for Artificial Intelligence) hybrid grammatical error correction system. This system was validated during the participation into the CONLL’14 Shared Task on Grammatical Error Correction. We offer an analysis of the types of errors detected and corrected by our system, we present the necessary steps to reproduce our experiment and also the re-	artificial intelligence;error detection and correction;experiment;qr code;software bug	Tiberiu Boros;Stefan Daniel Dumitrescu;Adrian Zafiu;Verginica Barbu Mititelu;Ion Vaduva	2014				AI	-31.316157722464695	-72.93970337409115	38942
b1d7dedd45061ead2dbd8d543d75a80445e27350	instability in search engine results: lessons learned in the context of horizon scanning applications	search engines query processing;search engines google market research engines time series analysis educational institutions indexes;query processing;search engines;search engine instability;queries search engine results instability horizon scanning applications strategic decision making search engine interfaces information retrieval;horizon scanning search engines search engine instability web mining trend discovery and tracking;web mining;trend discovery and tracking;horizon scanning	Horizon scanning, the systematic search for information to identify potential threats, risks, emerging issues and opportunities, has become an increasingly important part of strategic decision making. Although horizon scanning has its roots in the pre-electronic information era, it has blossomed with the availability of Web-based information. Dedicated analysts responsible for scanning the horizon make frequent use of search engines to retrieve information. Regrettably, the results yielded by popular search engines are often inconsistent and redundant. Thus, post processing heuristics have to be employed to select the most relevant data. This paper focusses on the first steps of this process, and analyses the result counts provided by different search engine interfaces in response to a set of queries meant to gather information about new and emerging trends.	heuristic (computer science);horizon effect;instability;web search engine	Marco A. Palomino;Tim Taylor;Geoff McBride;Richard Owen	2013	2013 24th International Workshop on Database and Expert Systems Applications	10.1109/DEXA.2013.14	search-oriented architecture;search engine indexing;web mining;query expansion;database search engine;metasearch engine;semantic search;computer science;spamdexing;data mining;database;search analytics;web search query;world wide web;information retrieval;search engine	DB	-33.16194477598458	-53.899561047430716	38957
f4eb16df2a9fc248a1c70ec1a807eb47a0fa9db4	the university of amsterdam at weps3		In this paper we describe our participation in the Third Web People Search (WePS3) evaluation campaign. We took part in the Online Reputation Management (ORM) task. Ambiguity of organization names (e.g., “Amazon” or “Apple”) raises obvious difficulties for systems that attempt to trace mentions of and opinions about a specific company in Web data, in an unsupervised manner. Problems are further amplified in the context of user generated content, where proper capitalization of named entities is often absent. The ORM task, introduced this year, addresses this very problem, by setting out the following challenge: given a set of Twitter entries containing an (ambiguous) company name and given the homepage of the company, discriminate entries that do not refer the company. Given the above definition, it is natural to formulate the problem as a binary classification task. Our focus was on building a general organization classifier that predicts, for each tweet, whether it is about a company. Our goal is to assess how a system without external aid from other sources (the company’s homepage, Wikipedia entry, etc.) can perform. We, therefore, focus on extracting features that are organization-independent and build on the characteristics of Twitter, such as noisy text, abbreviations and Twitter-specific language. Specifically, we trained a J48 decision tree classifier using the following groups of features: (i) company name (matching based on character 3-grams), (ii) content value (whether the tweet contains URLs, hashtags or is part of a conversation), (iii) content quality (ratio of punctuation and capital characters), (iv) organizational context (ratio of words found in tweets labelled as positive). We submitted a single run that performed around the median of all submitted systems. One interesting observation that requires further investigation is that our F-score for the negative class was substantially higher than for the positive class (0.55 vs. 0.36); for other teams it was usually the other way around. In future work we plan to build company-specific models by exploiting content both from Twitter and from external sources. Acknowledgements This research was supported by the Netherlands Organisation for Scientific Research (NWO) under project number 612.061.815 and partially by the Center for Creation, Content and Technology (CCCT).	ambiguous grammar;binary classification;decision tree;exploit (computer security);hashtag;named entity;noisy text;reputation management;unsupervised learning;user-generated content;wikipedia	Manos Tsagkias;Krisztian Balog	2010			user-generated content;noisy text;conversation;information retrieval;computer science;ambiguity;decision tree learning;classifier (linguistics);c4.5 algorithm;reputation	Web+IR	-22.039051464204658	-69.00102801580643	38981
b14a4c22f64d9243b9566229797350d144249868	bringing common sense to wordnet with a word game	word games;common sense knowledge acquisition;wordnet	We present a tool for common sense knowledge acquisition in form of a twenty questions game. The described approach uses WordNet dictionary, which rich taxonomy allows to keep cognitive economy and accelerate knowledge propagation, although sometimes inferences made on hierarchical relations result in noise. We extend the dictionary with common sense assertions acquired during the games played with humans. The facts added to the knowledge base use eight new relation types. After 700 games played in the animals domain the average number of assertions per concept raised over 89%. Evaluation of the newly acquired facts indicates high quality of knowledge captured using proposed approach.	wordnet	Jacek Rzeniewicz;Julian Szymanski	2013		10.1007/978-3-642-40495-5_30	natural language processing;wordnet;computer science;artificial intelligence	NLP	-30.02717893592285	-68.16968075601797	39076
b577af6a273958d4f6e85d721f0e1c73f653be54	towards automatic retrieval of idioms in french newspaper corpora	institutional repositories;computacion informatica;fedora;historia y critica literaria;expression idiomatique;filologias;grupo de excelencia;distance semantique;linguistique appliquee;methode;segmentation;linguistique de corpus;vital;press;linguistica;semantic distance;francais;ciencias basicas y experimentales;presse;corpus linguistics;computational linguistics;vtls;grupo a;analyse semantique;latent semantic analysis;idiomatic expression;linguistique informatique;method;text segmentation;extraction;ils;semantic analysis;applied linguistics	The goal of this paper is to present a procedure for the automatic retrieval of idiomatic expressions from large text corpora. The procedure combines text segmentation techniques and Latent Semantic Analysis (Landauer, Foltz, Laham, 1998). Three indices were computed on the basis of the three-fold hypothesis that a) idiomatic expressions should have few neighbours, that b) idiomatic expressions should demonstrate low semantic proximity between the words composing them, and that c) idiomatic expressions should demonstrate low semantic proximity between the expression and the preceding and subsequent segments. The result of this procedure shows that we have not yet reached a fully automatic retrieval of idioms from large corpora, but this first trial has shown that we are on the way. The procedure reduces the amount of data to consider to less than a quarter (23.8%) of the original data, of which one fifth (20.9%) is idiomatic, and nearly 60% (58.8%) is phraseological in nature. In other words, this procedure drastically improves and facilitates hand-based retrieval. In addition, these first results already permit some linguistic exploitation of the retrieved idioms.	emoticon;landauer's principle;latent semantic analysis;programming idiom;semantic similarity;text corpus;text segmentation	Liesbeth Degand;Yves Bestgen	2003	LLC	10.1093/llc/18.3.249	natural language processing;text segmentation;semantic similarity;extraction;method;speech recognition;latent semantic analysis;computer science;computational linguistics;applied linguistics;corpus linguistics;linguistics;segmentation	Web+IR	-26.86553340815155	-76.46808876751138	39084
6f65e5798887ca1bba5407debc394fe30527031d	probabilistic coordination disambiguation in a fully-lexicalized japanese parser	probabilistic model;large scale	This paper describes a probabilistic model for coordination disambiguation integrated into syntactic and case structure analysis. Our model probabilistically assesses the parallelism of a candidate coordinate structure using syntactic/semantic similarities and cooccurrence statistics. We integrate these probabilities into the framework of fully-lexicalized parsing based on largescale case frames. This approach simultaneously addresses two tasks of coordination disambiguation: the detection of coordinate conjunctions and the scope disambiguation of coordinate structures. Experimental results on web sentences indicate the effectiveness of our approach.	experiment;parallel computing;parser;statistical model;word-sense disambiguation	Daisuke Kawahara;Sadao Kurohashi	2007			natural language processing;statistical model;speech recognition;computer science;pattern recognition;statistics	NLP	-23.250277476217008	-74.4986937298413	39120
a4d1d26a44d68e32485fa2d408828780b537723e	attempting automatic prosodic knowledge acquisition using a database			knowledge acquisition	L. Mortamet;Françoise Emerard;Laurent Miclet	1990			information retrieval;knowledge acquisition;natural language processing;database;artificial intelligence;computer science	NLP	-30.69094477190599	-77.8673408320139	39122
8f95ecbdf27b1e95e5eecd9cfbaf4f5c9820c5f0	it's always april fools' day!: on the difficulty of social network misinformation classification via propagation features		"""Given the huge impact that Online Social Networks (OSN) had in the way people get informed and form their opinion, they became an attractive playground for malicious entities that want to spread misinformation, and leverage their effect. In fact, misinformation easily spreads on OSN, and this is a huge threat for modern society, possibly influencing also the outcome of elections, or even putting people's life at risk (e.g., spreading """"anti-vaccines"""" misinformation). Therefore, it is of paramount importance for our society to have some sort of """"validation"""" on information spreading through OSN. The need for a wide-scale validation would greatly benefit from automatic tools. In this paper, we show that it is difficult to carry out an automatic classification of misinformation considering only structural properties of content propagation cascades. We focus on structural properties, because they would be inherently difficult to be manipulated, with the the aim of circumventing classification systems. To support our claim, we carry out an extensive evaluation on Facebook posts belonging to conspiracy theories (representative of misinformation), and scientific news (representative of fact-checked content). Our findings show that conspiracy content reverberates in a way which is hard to distinguish from scientific content: for the classification mechanism we investigated, classification F-score never exceeds 0.7."""	entity;malware;polarization (waves);social network;software propagation;theory	Mauro Conti;Daniele Lain;Riccardo Lazzeretti;Giulio Lovisotto;Walter Quattrociocchi	2017	2017 IEEE Workshop on Information Forensics and Security (WIFS)	10.1109/WIFS.2017.8267653	leverage (finance);data mining;artificial intelligence;computer science;machine learning;voting;social network;misinformation	Web+IR	-21.084631191595896	-54.97447321992943	39158
9d9d3d3cba94cd473c16fd19df76668281c57fe4	semantic similarity computation based on multi-feature combination using hownet	semantic similarity;semantic distance	Semantic similarity between words is becoming a generic problems for many applications of computational linguistics and artificial intelligence. The difficulty lies in how to develop a computational method that is capable of generating satisfactory results close to how humans perceive. This paper proposes a semantic similarity approach that is based on multi-feature combination. One of the benchmarks is Miller and Charles’ list of 30 noun pairs which had been manually designated similarity measurements. We correlate our experiments with those computed by several other methods. Experiments on Chinese word pairs show that our approach is close to human similarity judgments.	artificial intelligence;computation;computational linguistics;experiment;semantic similarity;word lens	Peiying Zhang;Zhanshan Zhang;Weishan Zhang;Chunlei Wu	2014	JSW		natural language processing;semantic similarity;semantic computing;computer science;data mining;information retrieval;similarity heuristic;dishin	NLP	-26.584792099680207	-69.0780028666798	39168
cadaea525ea2847a67e76444f86eb1a4182159f1	determining song similarity via machine learning techniques and tagging information		The task of determining item similarity is a crucial one in a recommender system. This constitutes the base upon which the recommender system will work to determine which items are more likely to be enjoyed by a user, resulting in more user engagement. In this paper we tackle the problem of determining song similarity based solely on song metadata (such as the performer, and song title) and on tags contributed by users. We evaluate our approach under a series of different machine learning algorithms. We conclude that tf-idf achieves better results than Word2Vec to model the dataset to feature vectors. We also conclude that k -NN models have better performance than SVMs and Linear Regression for this problem. 1 PROBLEM DESCRIPTION Various recommender systems use a metric known as song similarity to predict candidate songs users would be interested in listening to. Defining such a metric is somewhat subjective, though, and researchers use two different approaches for this: • The objective approach, in which similarity is based on content information, such as spectral or rhythmic analysis of songs, and the • subjective approach, in which user-generated data, such as tags—also known as collaborative filtering—is used. In this project we intend to use the subjective approach to define song similarity. In particular, we will define the similarity level between two songs ranging from zero (completely dissimilar) to one (identical) and will compute it using the co-occurrences of pairs of items in users’ histories using the cosine metric. This metric will also be our model of reality and, therefore, our ground truth. Such definition is plausible, since researchers of the field have used it with success [Linden et al., 2003]. 2 DATA The dataset used in this project was generated by calling Last.fm’sTM1 API and persisting the results. It contains more than 5M songs with all associated metadata (tags, artist, album, play count, number of listeners, duration, mbid2), the listening history of 380K users, and similarity metrics for 138M pair of songs in our dataset. A lot of Last.fmTM’s data is uploaded by users, for instance, users define tags for a song. The dataset contains tags that are written in different forms such as causing inconsistencies and different hyphenation or symbols (e.g. Guns & Roses versus Guns N’ Roses) duplicated songs and other noise forms that we will have to pre-process to achieve better results. During collection, the data was stored in a MongoDB database, where each API response was stored as a different JSON document in the database. Figure 1 shows an example of such a format. Its fields are: • name: The song’s name; • tags: An array of pairs consisting of (name, count), where “name” is a tag defined by a user and “count” represents how many users have applied that tag to that song. Notice that “count” is capped to 100. • album_mbid: The unique MusicBrainz ID assigned to the album that contains this particular song; • artist_name: The name of the artist that recorded this particular song; 1. http://last.fm–Last.fm is a trademark by Audioscrobbler Limited. 2. MusicBrainz ID—a reliable and unambiguous identifier in the MusicBrainz database (musicbrainz.org). ar X iv :1 70 4. 03 84 4v 1 [ cs .L G ] 1 2 A pr 2 01 7 { ’name’: ’headspin’, ’tags’: [[’idm’, 100], [’electronic’, 54], /* more tags */], ’album_mbid’: ’a960877b-0319-48ce-8658-c17b1e0dab9a’, ’artist_name’: ’plaid’, ’mbid’: ’3e34ad31-8fd2-4c6c-95a7-7c1fe2bb3dbf’, ’album_title’: ’not for threes’, ’artist_mbid’: ’7e54d133-2525-4bc0-ae94-65584145a386’ } Figure 1. Sample output of the last.fmTM API as stored in our database. • mbid: The song’s unique MusicBrainz ID; • album_title: The title of the album that contains this song; • artist_mbid: The artist’s MusicBrainz ID. Additionally, the computed co-occurrence model was computed between song pairs and stored in a comma-separated file in the format (song1, song2, similarity), which we had to parse to correctly build the similarity graph. 2.1 Data transformation To correctly model the data we needed to process it in different steps: after data collection, we had to extract data from MongoDB3, normalize text and integrate the similarity calculations into this data. Data normalization consisted of removing accents and all kinds of special characters from words, replacing numbers with words and converting Unicode characters to the closest latin characters that represented them. All strings in the dataset were normalized; namely: album title, artist name, song name, and tag name. Since MBIDs are unique, those were converted to integers sequentially in the order they appeared. Once all data was converted, we proceeded to create the feature vectors, which were created with using two different models: Word2Vec and tf-idf, described in the following. We also evaluate the various algorithms by artificially filtering songs with too low similarity: we produced two new datasets, one in which no songs with similarity smaller than 1% are found, and another in which no songs with similarity smaller than 2.5% are found.	16px|favicon of musicbrainzmusicbrainz;application programming interface;big data;caller id;collaborative filtering;computer;cosine similarity;cross-validation (statistics);economy of second life;ground truth;hyphenation algorithm;identifier;json;k-nearest neighbors algorithm;last.fm;machine learning;mongodb;natural language processing;parsing;preprocessor;recommender system;tf–idf;unicode;user-generated content;word2vec	Renato L. F. Cunha;Evandro Caldeira;Luciana Fujii	2017	CoRR		computer science;machine learning;world wide web;information retrieval	ML	-21.977642324370922	-60.69268729957346	39190
10a66966b70af0f6b716e919c96772d9fa98a353	service topic model with probability distance	databases;service topic;probability distance;mashups;lattices;collaboration;computational modeling;big data;quality of service;topic model	The number of Web services are growing rapidly on the Internet. Topics of services are becoming various. Semantic-based keyword search is used to retrieve proper services for service consumers. According to the semantic information implied in service database, we build a topic model to cluster and management related services. Our service recommendation approach can extract service patterns from correlated topics in semantic service descriptions. We use Latent Dirichlet Allocation to obtain the service patterns; and use Concept lattice to model the correlation between the extracted topics. Higher precision results are obtained in the experiments.	experiment;formal concept analysis;internet;latent dirichlet allocation;search algorithm;topic model;web service	Yu Lei;Philip S. Yu	2016	2016 IEEE/ACM 9th International Conference on Utility and Cloud Computing (UCC)	10.1145/2996890.3007863	big data;quality of service;computer science;lattice;data mining;topic model;data as a service;computational model;world wide web;information retrieval;mashup;collaboration	DB	-28.21607720714674	-57.58927549452902	39211
a56c348ed49dd41dc206e12bf59c1f5ac2906eee	politwi: early detection of emerging political topics on twitter and the impact on concept-level sentiment analysis	topic detection;social data analysis;big data;concept level sentiment analysis;twitter	In this work, we present a system called PoliTwi, which was designed to detect emerging political topics (Top Topics) in Twitter sooner than other standard information channels. The recognized Top Topics are shared via different channels with the wider public. For the analysis, we have collected about 4,000,000 tweets before and during the parliamentary election 2013 in Germany, from April until September 2013. It is shown, that new topics appearing in Twitter can be detected right after their occurrence. Moreover, we have compared our results to Google Trends. We observed that the topics emerged earlier in Twitter than in Google Trends. Finally, we show how these topics can be used to extend existing knowledge bases (web ontologies or semantic networks) which are required for concept-level sentiment analysis. For this, we utilized special Twitter hashtags, called sentiment hashtags, used by the German community during the parliamentary election. 2014 Published by Elsevier B.V.	hashtag;knowledge base;ontology (information science);semantic network;sentiment analysis	Sven Rill;Dirk Reinel;Jörg Scheidt;Roberto V. Zicari	2014	Knowl.-Based Syst.	10.1016/j.knosys.2014.05.008	big data;computer science;data mining;internet privacy;world wide web;sentiment analysis	Web+IR	-22.462779237306005	-55.44335345780461	39249
f2c41bacf152012c0b58c33794dba66bb422c2f2	on-line semantic analysis of english texts		This paper describes the use of an on-line system to do word-sense ambiguity resolution and content analysis of English paragraphs, using a system of semantic analysis programmed in Q32 LISP 1.5. The system of semantic analysis comprises dictionary codings for the text words, coded forms of permitted message, and rules producing message forms in combination on the basis of a criterion of semantic closeness. All these can be expressed as a single system of rules of phrase-structure form. In certain circumstances the system is able to enlarge its own dictionary in a real-time mode on the basis of information gained from the actual texts analyzed.	centrality;dictionary;lisp;online and offline;real-time computing;real-time transcription;semantic analysis (compilers);word sense;on-line system	Yorick Wilks	1968	Mech. Translat. & Comp. Linguistics		semantic field;natural language processing;semantic similarity;social semantic web;semantic equivalence;semantic search;content analysis;computer science;semantic network;explicit semantic analysis;artificial intelligence	Web+IR	-28.588932501011385	-77.68373845213995	39304
e48f9c67825ee7a53a87c27689e1783aa4c105ee	animacy annotation in the hindi treebank		In this paper, we discuss our efforts to annotate nominals in the Hindi Treebank with the semantic property of animacy. Although the treebank already encodes lexical information at a number of levels such as morph and part of speech, the addition of animacy information seems promising given its relevance to varied linguistic phenomena. The suggestion is based on the theoretical and computational analysis of the property of animacy in the context of anaphora resolution, syntactic parsing, verb classification and argument differentiation.	anaphora (linguistics);inter-rater reliability;java annotation;parsing;relevance;treebank	Itisree Jena;Riyaz Ahmad Bhat;Sambhav Jain;Dipti Misra Sharma	2013			natural language processing;computer science;treebank;linguistics;communication	NLP	-28.148567961411242	-75.27626267811468	39307
5f80ad2ca63bde8565993996f34bdf63d8cae848	comments-oriented blog summarization by sentence extraction	sentence extraction;requt;user study;blog;sentence selection;comments;information system	Much existing research on blogs focused on posts only, ignoring their comments. Our user study conducted on summarizing blog posts, however, showed that reading comments does change one's understanding about blog posts. In this research, we aim to extract representative sentences from a blog post that best represent the topics discussed among its comments. The proposed solution first derives representative words from comments and then selects sentences containing representative words. The representativeness of words is measured using ReQuT (i.e., Reader, Quotation, and Topic). Evaluated on human labeled sentences, ReQuT together with summation-based sentence selection showed promising results.	blog;comment (computer programming);sentence extraction;usability testing	Meishan Hu;Aixin Sun;Ee-Peng Lim	2007		10.1145/1321440.1321571	natural language processing;speech recognition;computer science;information retrieval;information system	NLP	-24.50452117044177	-58.93892989035426	39336
d5a418e1cabf4a9440b76b14337faf6bbcb16936	tagging and labelling portuguese modal verbs	part of book or chapter of book	We present in this paper an experiment in automatically tagging a set of Portuguese modal verbs with modal information. Modality is the expression of the speaker’s (or the subject’s) attitude towards the content of the sentences and may be marked with lexical clues such as verbs, adverbs, adjectives, but also by mood and tense. Here we focus exclusively on 9 verbal clues that are frequent in Portuguese and that may have more than one modal meaning. We use as our gold data set a corpus of 160.000 tokens manually annotated, according to a modality annotation scheme for Portuguese. We apply a machine learning approach to predict the modal meaning of a verb in context. This modality tagger takes into consideration all the features available from the parsed data (pos, syntactic and semantic). The results show that the tagger improved the baseline for all verbs, and reached macro-average F-measures between 35 and 81% depending on the modal verb and on the modal value.		Paulo Quaresma;Amália Mendes;Iris Hendrickx;Teresa Gonçalves	2014		10.1007/978-3-319-09761-9_7	natural language processing;art;linguistics;literature	NLP	-25.808660155530273	-74.21585785162087	39341
8acbdc1e0b05e9463017a0050c1f9d74e4b5b525	a comparative study of different sentiment lexica for sentiment analysis of tweets		We report on interoperability of different sentiment lexica with each other and with the linguistic notions negation and modality for sentiment analysis of tweets in a comprehensive ablation study and in competition results for SemEval 2015. Our approach performed well at the tweet level, but excelled in the presence of figurative language.	hoc (programming language);interoperability;lexicon;modality (human–computer interaction);semeval;sentiment analysis;structure of observed learning outcome	Canberk Özdemir;Sabine Bergler	2015			sentiment analysis;natural language processing;artificial intelligence;computer science	NLP	-28.745183744087484	-75.5053381174057	39455
95db3e569a45580d73505d4a65d37493e363874d	relevance of named entities in authorship attribution		Named entities (NE) are words that refer to names of people, locations, organization, etc. NE are present in every kind of documents: e-mails, letters, essays, novels, poems. Automatic detection of these words is very important task in natural language processing. Sometimes, NE are used in authorship attribution studies as a stylometric feature. The goal of this paper is to evaluate the effect of the presence of NE in texts for the authorship attribution task: are we really detecting the style of an author or are we just discovering the appearance of the same NE. We used the corpus that consists of 91 novels of 7 authors of XVIII century. These authors spoke and wrote English, their native language. All novels belong to fiction genre. The used stylometric features were character n-grams, word n-gram and n-gram of POS tags of various sizes (2-grams, 3-grams, etc.). Five novels were selected for each author, these novels contain between 4 and 7% of the NE. All novels were divided into blocks, each block contains 10,000 terms. Two kinds of experiment were conducted: automatic classification of blocks containing NE and of the same blocks without NE. In some cases, we use only the most frequent n-grams (500, 2,000 and 4,000 n-grams). Three machine learning algorithms were used for classification task: NB, SVM (SMO) and J48. The results show that as a tendency the presence of the NE helps to classify (improvements from 5% to 20%), but there are specific authors when NE do not help and even make the classification worse (about 10% of experimental data).	entity;named-entity recognition;relevance	Germán Ríos-Toledo;Grigori Sidorov;Noé Alejandro Castro-Sánchez;Alondra Nava-Zea;Liliana Chanona-Hernández	2016		10.1007/978-3-319-62434-1_1	experimental data;support vector machine;poetry;pattern recognition;c4.5 algorithm;artificial intelligence;first language;attribution;computer science	NLP	-23.770320538611482	-66.8607260494254	39469
55b40fbe3157cfd2162130d020e9b8e9001ad887	browsing with dynamic key frame collages in web-based entertainment video services	spatial temporal layout;video databases;information resources;web based entertainment;motion pictures laboratories context aware services content based retrieval tv production web and internet services computer aided instruction prototypes microscopy;interface dynamics;dynamic collage;internet;multimedia communication;multimedia video browsing dynamic key frame collages world wide web entertainment video services hierarchical browsing querying temporal layout spatial layout;content based retrieval information resources internet video databases entertainment multimedia communication;key frames;video browsing;content based retrieval;entertainment	We discuss video browsing in the context of Web-based entertainment video services. We propose dynamic collages of key frames as a suitable technique for rapid evaluation of moderately-sized sets of candidate videos that have been selected via hierarchical browsing or querying techniques elsewhere in the interface. Dynamic key frame collages integrate temporal and spatial layout, and thus potentially retain the advantages of each. We relate our designs to the research of Marchionini (1998) and colleagues, who have evaluated the effectiveness of other temporal and spatial key frame presentation techniques.	browsing;key frame	Kent Wittenburg;John R. Nicol;James Paschetto;Christopher Martin	1999		10.1109/MMCS.1999.778610	entertainment;the internet;computer science;multimedia;internet privacy;world wide web	Mobile	-15.009285556877773	-54.611202696113715	39513
8927faf20c319cde03ba14b588ecb95f1eb1484c	asap: a synchronous approach for photo sharing across multiple devices	new technology;reflective ui;synchronous photo sharing;asia handheld computers digital cameras automation mobile communication organizing image databases spatial databases visual databases navigation;synchronous image viewer synchronous photo sharing reflective ui synchronous ui mobile browsing experience;photo sharing;synchronous image viewer;image browsing;mobile communication;mobile browsing experience;experimental evaluation;synchronous ui;user interaction	Digital photos have become increasingly common and popular in mobile communications. However, due to the distribution of these photos captured in various devices, there is a need to develop new technologies to facilitate the sharing of large image collections across these devices for users. In this paper, we propose A Synchronous Approach for Photo sharing across multiple devices (ASAP). The ASAP provides a hierarchical two-level synchronization scheme, namely image-level and region-level. In the ASAP, a user’s interaction with any device automatically leads to a series of synchronous updates in other devices. Thus, the ASAP simultaneously presents similar images across devices in a way that allows automatic synchronization of images based on user interactions. Experimental evaluations indicate that it is effective and useful to improve image browsing experience across devices for users.	image viewer;interaction	Zhigang Hua;Xing Xie;Hanqing Lu;Wei-Ying Ma	2005	11th International Multimedia Modelling Conference	10.1109/MMMC.2005.21	computer vision;mobile telephony;computer science;operating system;multimedia;world wide web	HCI	-16.305570735482185	-55.09067240360947	39570
348252df2799c865ac433a3865dc5b65b023966b	problem solving environment approach to integrating diverse biological data sources	biology computing;experimental method;data collection;application server;mass spectrometry;software systems;interaction network;graphical user interfaces problem solving genetics arrays mass spectroscopy proteins molecular biophysics java public domain software cellular biophysics biology computing;genetics;arrays;public domain software;graphical user interfaces;proteins;molecular biophysics;biological systems;graphic user interface;data retrieval diverse biological data sources biological systems high throughput experimental methods mass spectrometry gene arrays public databases national center for biotechnology kyoto encyclopedia of genes and genomes protein interaction database biomolecular interaction network database cohesive datasets three tier software system client side graphical user interface data collection data integration data management java open source technology problem solving environment computational cell environment agile connectivity;mass spectroscopy;biological data;protein interaction;high throughput;data retrieval;cellular biophysics;problem solving environment;problem solving databases content management biological systems throughput mass spectroscopy merging biotechnology encyclopedias genomics;problem solving;java;open source;kyoto encyclopedia of genes and genomes	Scientists face an ever-increasing challenge in investigating biological systems with high throughput experimental methods such as mass spectrometry and gene arrays because of the scale and complexity of the data and the need to integrate results broadly with heterogeneous other types of information. Many analyses require merging the experimental results with datasets returned from public databases, such as those hosted by the National Center for Biotechnology (NCBI), Kyoto Encyclopedia of Genes and Genomes (KEGG), and protein interaction databases such as the Biomolecular Interaction Network Database (BIND). Because data sources such as these are constantly evolving the researcher is faced with hurdles to manually gather, integrate and manage the data into cohesive datasets. To overcome these technical problems, we have been building a three-tier software system that includes a client-side graphical user interface for rich interaction with the data, an application server that hides the messy technical details of data collection, integration, and management tasks from the researcher, and a flexible database schema that efficiently manages mixed data source content. The software is being developed using Java for portability and Open Source technology so that it can one day be freely distributed. This problem-solving environment is called the Computational Cell Environment (CCE) and is designed to provide scalable and agile connectivity to diverse data stores and eventually provide data retrieval, management, and analysis through all aspects of biological study.	agile software development;application server;bond;biological system;certified computer examiner;client-side;computation;data retrieval;data store;database schema;graphical user interface;interaction network;java;kegg;multitier architecture;problem solving environment;scalability;server (computing);software system;throughput	Eric G. Stephan;Kyle R. Klicker;Mudita Singhal;Heidi J. Sofia	2005	2005 IEEE Computational Systems Bioinformatics Conference - Workshops (CSBW'05)	10.1109/CSBW.2005.112	mass spectrometry;computer science;bioinformatics;machine learning;data mining;graphical user interface;world wide web	HPC	-4.573628357827521	-60.566806606800974	39590
40531e914a86b4501155136815f0f51e8304eb03	preschoolers infer contrast from adjectives if they can access lexical alternatives		When speakers use modified noun phrases (e.g. “the long book”), they provide information not only about a salient feature of a single item (that this book is long), but also about implicit contrasts with possible alternatives (books can vary by length: some may be short). We investigate the development of preschoolers’ ability to detect implicit contrasts from speakers’ use of adjectives and make inferences about category structure. In Experiment 1, we found that adults and preschoolers can make contrast inferences from adjective use in a supportive frame, and this ability improves over the preschool years. In Experiment 2, we reduced the cues to contrast and found that adults still inferred implied contrast from adjective use alone, but preschoolers did not. Perhaps the issue for preschoolers was an inability to consider alternatives from explicit descriptions (e.g. bringing to mind “short” from hearing “long”). Experiment 3 tested this hypothesis by reading preschoolers a book containing relevant opposite pairs immediately prior to the task. After reading the book, older 4-year-olds were able to make contrast inferences reliably, suggesting that increasing children’s access to lexical alternatives may boost their ability to make contrast inferences.	book;code reading;experiment	Alexandra Horowitz;Michael C. Frank	2014			social psychology;implicature;noun phrase;cognitive psychology;psychology;scalar implicature;referent;linguistics;utterance;adjective;pragmatics;lexical item	NLP	-8.739044307391438	-77.16925036964808	39622
fa514389243720846ddb3028c692b89a3a6029d2	study on key technologies of generator of q/a system	databases;generators;automatic question answer system;answer extraction;search engines;information retrieval;heterogeneous data;generator of q a natural language processing nlp;natural language processing information retrieval;shape measurement;data mining;engines;question answering system;games;generator of q a;web search;natural language processing nlp;information retrieval automatic question answer system natural language processing answer extraction answer translation method;answer translation method;natural languages information science shape measurement search engines conferences computational intelligence computer industry application software information management computer science;natural language processing	Automatic question-answer system is a very hot research in natural language processing realm. But it also exist some problems. The developing period of the Q/A system in different fields is too long, and the recycle rate is so low. To deal with it, the paper researches the generator of automatic Q/A system. First, the paper introduces the answer shape measure and the module of Q/A system. It brings elaborately forward the main idea of generator. Second, it puts forward the method of answer extraction and the translation method of the heterogeneous data in the automatic Q/A system. Automatic Q/A system make the corresponding answer by the principle of the maximal weigh. The sentences are regard as the basic unit of the result matching. It considerably enhances the intelligence degree of Q/A system. Finally, it gives an example of QA system.On the base of above theory and algorithms, we implement a generator of Q/A system.	algorithm;information management;information science;java platform, enterprise edition;lu decomposition;maximal set;natural language processing;yang	Haiyan Kang;Yangsen Zhang;Lei Yang;Wenhua Liu	2008	2008 IEEE Pacific-Asia Workshop on Computational Intelligence and Industrial Application	10.1109/PACIIA.2008.292	natural language processing;games;question answering;computer science;data mining;information retrieval;search engine	AI	-26.773460978094388	-68.01074287956615	39655
702c9f55cd980f651aa2b4429ede388df1bc05b0	determining the user intent of web search engine queries	search engine;user intent;classification algorithm;search engines;web queries;web searching;web search engine;web search;sparse data;information navigation;automatic classification	Determining the user intent of Web searches is a difficult problem due to the sparse data available concerning the searcher. In this paper, we examine a method to determine the user intent underlying Web search engine queries. We qualitatively analyze samples of queries from seven transaction logs from three different Web search engines containing more than five million queries. From this analysis, we identified characteristics of user queries based on three broad classifications of user intent. The classifications of informational, navigational, and transactional represent the type of content destination the searcher desired as expressed by their query. We implemented our classification algorithm and automatically classified a separate Web search engine transaction log of over a million queries submitted by several hundred thousand users. Our findings show that more than 80% of Web queries are informational in nature, with about 10% each being navigational and transactional. In order to validate the accuracy of our algorithm, we manually coded 400 queries and compared the classification to the results from our algorithm. This comparison showed that our automatic classification has an accuracy of 74%. Of the remaining 25% of the queries, the user intent is generally vague or multi-faceted, pointing to the need to for probabilistic classification. We illustrate how knowledge of searcher intent might be used to enhance future Web search engines.	algorithm;faceted classification;sparse matrix;transaction log;vagueness;web 2.0;web search engine;web search query	Bernard J. Jansen;Danielle L. Booth;Amanda Spink	2007		10.1145/1242572.1242739	web query classification;metasearch engine;web search engine;semantic search;computer science;data mining;database;search analytics;web search query;queries per second;world wide web;information retrieval;search engine	Web+IR	-32.07413117912129	-54.68802423729334	39680
3f6a4556769e819242d669d073b895f1e45a706f	image description using visual dependency representations		Describing the main event of an image involves identifying the objects depicted and predicting the relationships between them. Previous approaches have represented images as unstructured bags of regions, which makes it difficult to accurately predict meaningful relationships between regions. In this paper, we introduce visual dependency representations to capture the relationships between the objects in an image, and hypothesize that this representation can improve image description. We test this hypothesis using a new data set of region-annotated images, associated with visual dependency representations and gold-standard descriptions. We describe two template-based description generation models that operate over visual dependency representations. In an image description task, we find that these models outperform approaches that rely on object proximity or corpus information to generate descriptions on both automatic measures and on human judgements.	parser combinator;text corpus	Desmond Elliott;Frank Keller	2013			natural language processing;computer vision;machine learning	NLP	-13.693309106675631	-69.28664755893797	39805
f9483e72000699aa3331e1e93d45541d1eb4d7b9	a semantic and content-based search user interface for browsing large collections of foley sounds	sound effects;information visualization;multimedia information retrieval;search user interfaces	Sound designers select the sounds they use among massive collections of recordings. They usually rely on text-based queries to narrow down a subset from these collections when looking for specific content. However, when it comes to unknown collections, this approach can fail to precisely retrieve files according to their content. We investigate an audio search engine that associates content-based features and semantic meta-data using Apache Solr deployed in a fully integrated server architecture. In order to facilitate the task of browsing the sounds, we also propose a search user interface in which the user can perform both text-based queries and visual browsing in a window where sounds are organized according to their audio features. A preliminary evaluation of the performances helped to optimize the parameters of the system.	benchmark (computing);browsing;codebook;performance;server (computing);solr;text-based (computing);user interface;weatherstar;web search engine	Gabriel Urbain;Christian Frisson;Alexis Moinet;Thierry Dutoit	2016		10.1145/2986416.2986436	information visualization;computer science;multimedia;world wide web;information retrieval	Web+IR	-30.725398069778954	-52.54843365731428	39819
10f565dc5a2d7e6a220660a126ab789fbce59b56	automatic service categorisation through machine learning in emergent middleware	machine learning;middleware	The modern environment of mobile, pervasive, evolving services presents a great challenge to traditional solutions for enabling interoperability. Automated solutions appear to be the only way to achieve interoperability with the needed level of flexibility and scalability. While necessary, the techniques used to determine compatibility, as a precursor to interaction, come at a substantial computational cost, especially when checks are performed between systems in unrelated domains. To overcome this, we apply machine learning to extract high-level functionality information through text categorisation of a system’s interface description. This categorisation allows us to restrict the scope of compatibility checks, giving an overall performance gain when conducting matchmaking between systems. We have evaluated our approach on a corpus of web service descriptions, where even with moderate categorisation accuracy, a substantial performance benefit can be found. This in turn improves the applicability of our overall approach for achieving interoperability in the Connect project.	algorithmic efficiency;categorization;document classification;emergent;high- and low-level;interoperability;license compatibility;machine learning;middleware;pervasive informatics;run time (program lifecycle phase);scalability;support vector machine;text corpus;trustworthy computing;web service	Amel Bennaceur;Valérie Issarny;Richard Johansson;Alessandro Moschitti;Romina Spalazzese;Daniel Sykes	2011		10.1007/978-3-642-35887-6_7	computer science;middleware;data mining;database;distributed computing	Web+IR	-32.13909298720495	-71.9891468367841	39952
2ef4401362ebd7dcb8ee9cfeab5211ef063e1492	yuila at the ntcir-12 short text challenge: combining twitter data with dialogue system logs		The YUILA team participated in the Japanese subtask of the NTCIR-12 Short Text Challenge task. This report describes our approach to solving the responsiveness problem in STC task by using external dialogue log corpus and discusses the official results.	dialog system;dialog tree;responsiveness	Hiroshi Ueno;Takuya Yabuki;Masashi Inoue	2016			data science;data mining;computer science	NLP	-23.317384554972808	-69.56865535939748	39957
e70175a1b17d0b92b068d08a96785885ae19f0dc	bidirectional-isomorphic manifold learning at image semantic understanding & representation	modelizacion;analisis contenido;diversity;correlacion;structure topologique;optimisation;vision ordenador;contenu image;image content;text;diversidad;learning algorithm;keyword;instant messaging;donnee textuelle;red www;busqueda por contenidos;optimizacion;recherche image;redundancia;dimension reduction;dato textual;efficiency;reseau web;diversite;topological structure;semantics;modele ordre reduit;palabra clave;intelligence artificielle;manifold learning;multidimensional analysis;texte;algorithme apprentissage;mot cle;image annotation;semantica;semantique;computer vision;reduction dimension;modelisation;image interpretation;eficacia;content analysis;analyse n dimensionnelle;internet;redundancy;interpretacion imagen;indexing;indexation;modelo orden reducido;analyse correlation;indizacion;textual data;analisis n dimensional;reduced order model;efficacite;reduccion dimension;artificial intelligence;world wide web;messagerie instantanee;vision ordinateur;optimization;co training;busqueda de imagen;interpretation image;inteligencia artificial;analyse contenu;correlation;analisis semantico;analyse semantique;contenido imagen;texto;algoritmo aprendizaje;modeling;estructura topologica;content based retrieval;langage html;recherche par contenu;mensajeria instantanea;html language;analisis correlacion;redondance;lenguaje html;semantic analysis;correlation analysis;image retrieval	From relevant textual information to improve visual content understanding and representation is an effective way for deeply understanding web image content. However, the description of images is usually imprecise at the semantic level, which is caused by the noisy and redundancy information in both text (such as surrounding text in HTML pages) and visual (such as intra-class diversity) aspects. This paper considers the solution from the association analysis for image content and presents a Bidirectional- Isomorphic Manifold learning strategy to optimize both visual feature space and textual space, in order to achieve more accurate comprehension for image semantics and relationships. To achieve this optimization between two different models, Bidirectional-Isomorphic Manifold Learning utilizes a novel algorithm to unify adjustments in both models together to a topological structure, which is called the reversed Manifold mapping. We also demonstrate its correctness and convergence from a mathematical perspective. Image annotation and keywords correlation analysis are applied. Two groups of experiments are conducted: The first group is carried on the Corel 5000 image database to validate our method’s effectiveness by comparing with state-of-the-art Generalized Manifold Ranking Based Image Retrieval and SVM, while the second group carried on a web-downloaded Flickr dataset with over 6,000 images to testify the proposed method’s effectiveness in real-world application. The promising results show that our model attains a significant improvement over state-of-the-art algorithms.	algorithm;automatic image annotation;baseline (configuration management);co-training;computer vision;correctness (computer science);experiment;feature vector;flickr;html;image retrieval;mathematical optimization;nonlinear dimensionality reduction;nonlinear system;vergence	Xianming Liu;Hongxun Yao;Rongrong Ji;Pengfei Xu;Xiaoshuai Sun	2011	Multimedia Tools and Applications	10.1007/s11042-011-0947-2	multidimensional analysis;search engine indexing;the internet;systems modeling;html;content analysis;image retrieval;computer science;artificial intelligence;semantics;nonlinear dimensionality reduction;efficiency;redundancy;automatic image annotation;world wide web;correlation;algorithm;dimensionality reduction;manifold alignment	AI	-13.54646385596973	-61.23007506001823	39962
ed8266bfb46b875639532f519110d0e4b7eac0ec	xrce's participation in imageclef 2009	information retrieval;text processing;regularization method;large scale;mixture model;content based image retrieval;mean average precision;relevance feedback	This paper describes XRCE’s participation in Large Scale Visual Concept Detection and Annotation Task [16] and Photo Retrieval Task [17] of ImageCLEF 2009. Those tasks both use a new collection which is different and which is much larger than the ones used in past sessions. Moreover, new kinds of challenge to tackle were designed such as new categories to detect or new types of topic to deal with. Accordingly, our main motivations regarding our participation in this year’s session are two fold. First, we wanted to apply ongoing work in our team in image and text processing. Second, we wanted to figure out if our cross-media approach and our diversity re-ranking techniques developped in past sessions, can perform well on a new challenging corpus. It turns out that the material that we describe in this paper both made of new and already well-established techniques allow to perform very well on those two tasks since the results we obtained with the systems we designed are top ranked.	fold (higher-order function);information access;performance;query expansion;text corpus;text-based (computing)	Julien Ah-Pine;Stéphane Clinchant;Gabriela Csurka;Yan Liu	2009			visual word;computer science;machine learning;pattern recognition;information retrieval	NLP	-31.78333539261423	-63.15796170519797	39966
9de88f341a82a838ff409101e536d8fce1ee5b04	data fusion for japanese term and character n-gram search	n gram search;term segmentation;morphological analysis	Term segmentation plays a vital role in building effective information retrieval systems. In particular, languages such as Japanese and Chinese require a morphological analyzer or a word segmenter to identify potential terms. The alternative approach to indexing a segmented collection is n-gram search, where every n-length sequence of symbols is indexed. Both approaches have strengths and weaknesses when applied to non-English collections. In this study, we explore data fusion techniques to answer the following question: if there are multiple ranked lists of documents from both word and n-gram indexes, can we improve overall effectiveness by combining them? We consider three empirical methods for combining search results using eight different search indexes and twenty-one different search models with and without automatic query expansion. Our approach is language independent; however, we focus on Japanese test collections -- NTCIR IR4QA -- as our testbed for the current experiments. Our experimental results demonstrate that the combination of the two different segmentation approaches has the potential to significantly outperform the best word-segmented search methods.	alloy analyzer;experiment;information retrieval;n-gram;null character;query expansion;search algorithm;string (computer science);testbed	Michiko Yasukawa;J. Shane Culpepper;Falk Scholer	2015		10.1145/2838931.2838939	morphological analysis;computer science;pattern recognition;data mining;world wide web;information retrieval	NLP	-28.41418595011554	-67.69968220332109	39995
a26fc6c1d89a783cf59fc142d43b6a6111616155	a lexical approach for spanish question answering	information retrieval;machine learning;language processing;question answering	This paper discusses our system’s results at the Spanish Question Answering task of CLEF 2007. Our system is centered in a full data-driven approach that combines information retrieval and machine learning techniques. It mainly relies on the use of lexical information and avoids any complex language processing procedure. Evaluation results indicate that this approach is very effective for answering definition questions from Wikipedia. In contrast, they also reveal that it is very difficult to respond factoid questions from this resource solely based on the use of lexical overlaps and redundancy.	information retrieval;machine learning;question answering;redundancy (engineering);wikipedia	Alberto Téllez-Valero;Antonio Juárez;Gustavo Hernández;Claudia Denicia-Carral;Esaú Villatoro-Tello;Manuel Montes-y-Gómez;Luis Villaseñor Pineda	2007		10.1007/978-3-540-85760-0_40	natural language processing;question answering;computer science;data mining;information retrieval	NLP	-27.29439667832147	-71.01408259698887	40022
8cd595ee1d8b8ff5716f74b9223d736d21d12ce1	twowingos: a two-wing optimization strategy for evidential claim verification		Determining whether a given claim is supported by evidence is a fundamental NLP problem that is best modeled as Textual Entailment. However, given a large collection of text, finding evidence that could support or refute a given claim is a challenge in itself, amplified by the fact that different evidence might be needed to support or refute a claim. Nevertheless, most prior work decouples evidence identification from determining the truth value of the claim given the evidence. We propose to consider these two aspects jointly. We develop TwoWingOS (two-wing optimization strategy), a system that, while identifying appropriate evidence for a claim, also determines whether or not the claim is supported by the evidence. Given the claim, TwoWingOS attempts to identify a subset of the evidence candidates; given the predicted evidence, it then attempts to determine the truth value of the corresponding claim. We treat this challenge as coupled optimization problems, training a joint model for it. TwoWingOS offers two advantages: (i) Unlike pipeline systems, it facilitates flexible-size evidence set, and (ii) Joint training improves both the claim entailment and the evidence identification. Experiments on a benchmark dataset show state-of-the-art performance. Code: https://github.com/yinwenpeng/FEVER	benchmark (computing);end-to-end principle;experiment;mathematical optimization;natural language processing;textual entailment	Wenpeng Yin;Dan Roth	2018			textual entailment;machine learning;truth value;artificial intelligence;logical consequence;computer science;optimization problem	NLP	-15.136943201487377	-72.06045114582396	40046
156a38243cd1e0a59bb4f827b4a3850bd643f242	identifying and utilizing the class of monosemous japanese functional expressions in machine translation	conference paper	In the “Sandglass” machine translation architecture, we identify the class of monosemous Japanese functional expressions and utilize it in the task of translating Japanese functional expressions into English. We employ the semantic equivalence classes of a recently compiled large scale hierarchical lexicon of Japanese functional expressions. We then study whether functional expressions within a class can be translated into a single canonical English expression. Next, we introduce two types of ambiguities of functional expressions and identify monosemous functional expressions. In the evaluation of our translation rules for Japanese functional expressions, we directly apply those rules to monosemous functional expressions, and show that the proposed framework outperforms commercial machine translation software products. We further study how to extract rules for translating functional expressions in Japanese patent documents into English. In the result of this study, we show that translation rules manually developed based on the corpus for Japanese language grammar learners is reliable also in the patent domain.	compiler;lexicon;machine translation;turing completeness	Akiko Sakamoto;Taiji Nagasaka;Takehito Utsuro;Suguru Matsuyoshi	2009			natural language processing;speech recognition;computer science;algorithm	NLP	-25.53833015533885	-74.60646232151119	40071
ea23c8049e4b864853590680a0448c21f39adda0	saspy: a pymol plugin for manipulation and refinement of hybrid models against small angle x-ray scattering data		UNLABELLED Complex formation and conformational transitions of biological macromolecules in solution can be effectively studied using the information about overall shape and size provided by small angle X-ray scattering (SAXS). Hybrid modeling is often applied to integrate high-resolution models into SAXS data analysis. To facilitate this task, we present SASpy, a PyMOL plugin that provides an easy-to-use graphical interface for SAXS-based hybrid modeling. Through a few mouse clicks in SASpy, low-resolution models can be superimposed to high-resolution structures, theoretical scattering profiles and fits can be calculated and displayed on-the-fly. Mouse-based manual rearrangements of complexes are conveniently applied to rapidly check and interactively refine tentative models. Interfaces to automated rigid-body and flexible refinement of macromolecular models against the experimental SAXS data are provided.   AVAILABILITY AND IMPLEMENTATION SASpy is available as open source at: github.com/emblsaxs/saspy/. Working installations of both PyMOL (www.pymol.org) and ATSAS (www.embl-hamburg.de/biosaxs/download.html) are required.   CONTACT apanjkovich@embl-hamburg.de or svergun@embl-hamburg.de.	dna sequence rearrangement;fits;graphical user interface;image resolution;interactivity;interface device component;muscle rigidity;non-small cell lung carcinoma;open-source software;plug-in (computing);pymol;refinement (computing);two-hybrid system techniques;macromolecule	Alejandro Panjkovich;Dmitri I. Svergun	2016		10.1093/bioinformatics/btw071	computer science;bioinformatics	Graphics	-5.462076134866037	-58.01948479268008	40078
5ecb757b47d6fa38b5161de5d09d2a1c13627af5	automatic generation of link collections and their visualization	hyperlink analysis;automatic generation;visualization;link collection	In this paper, we describe a method of generating link collections in a user-specified category by comprehensively collecting existing link collections and analyzing their hyperlink references. Moreover, we propose a visualization method for a bird's-eye view of the generated link collections. Our methods are effective in grasping intuitively the trend of significant sites and keywords in a category.	bird's-eye view;hyperlink	Osamu Segawa;Jun Kawai;Kazuyuki Sakauchi	2005		10.1145/1062745.1062809	visualization;computer science;data mining;world wide web;information retrieval	Web+IR	-28.951638449752593	-53.58854703420087	40103
e36feb202a60d0e6e45980ed06993a072cb451ac	a probabilistic approach for extracting opinion-related word chains from texts				Rémi Lavalley;Chloé Clavel;Patrice Bellot	2010	TAL		divergence-from-randomness model;probabilistic logic;artificial intelligence;pattern recognition;mathematics	NLP	-25.299119212980255	-78.98965199777439	40166
7d6b1786eb44b1c0d10b818096ff300cd4b820a0	integration of semantic networks for corpus-based word sense disambiguation	linguistic model;linguistica matematica;analisis estadistico;noun;red semantica;integration information;frase;verbe;semantic network;tratamiento lenguaje;psicolinguistica;probabilistic approach;word sense disambiguation;method integration;sentence;reseau semantique;information integration;modele linguistique;statistical analysis;language processing;computational linguistic;polisemia;enfoque probabilista;approche probabiliste;analyse statistique;traitement langage;integracion informacion;modelo linguistico;polysemy;verbo;polysemie;linguistique mathematique;phrase;computational linguistics;psycholinguistique;psycholinguistics;verb	This paper presents an intelligent method for corpus-based word sense disambiguation (WSD), which utilizes the integrated noun and verb semantic networks through the selectional restriction relations in sentences. Experiments show that the presented intelligent method performs the verb translation better than the concept-based method and the statistics-based method. Integration of noun semantic networks into verb semantic networks will play an important role in both computational linguistic applications and psycholinguistic models of language processing.	semantic network;word sense;word-sense disambiguation	Yoo-Jin Moon;Kyongho Min;Youngho Hwang;Pankoo Kim	2003		10.1007/978-3-540-24599-5_38	natural language processing;noun;semantic computing;speech recognition;semeval;computer science;information integration;computational linguistics;linguistics;psycholinguistics;semantic network	NLP	-26.74757258538962	-77.3158029665204	40242
2fd30b6f81a1f94260fba90af184011411c89dc9	a corpus-based analysis for the ordering of clause aggregation operators	text generation application;annotated corpus;clause aggregation operator;relative clause;shorter surface expression;paratactic operator;hypotactic operator;corpus-based analysis;small corpus	To better understand the ordering of clause aggregation operators in a text generation application, we manually annotated a small corpus. The annotated corpus supports the preferred ordering of transformations that result in shorter surface expressions, such as adjectives over relative clauses. In addition, we were able to explain why paratactic operators are applied before and after hypotactic operators.	markup language;moser spindle;natural language generation;text corpus	James Shaw	2002			natural language processing;linguistics;algorithm	NLP	-27.009756360242402	-76.4847147290186	40256
6e029d200ed58703e33efc4d6c55e8c0e6c6f331	sentiment analysis of financial news articles using performance indicators	sentiment analysis;financial news;performance indicators;text mining;machine learning;classification	Mining financial text documents and understanding the sentiments of individual investors, institutions and markets is an important and challenging problem in the literature. Current approaches to mine sentiments from financial texts largely rely on domain-specific dictionaries. However, dictionary-based methods often fail to accurately predict the polarity of financial texts. This paper aims to improve the state-of-the-art and introduces a novel sentiment analysis approach that employs the concept of financial and non-financial performance indicators. It presents an association rule mining-based hierarchical sentiment classifier model to predict the polarity of financial texts as positive, neutral or negative. The performance of the proposed model is evaluated on a benchmark financial dataset. The model is also compared against other state-of-the-art dictionary and machine learning-based approaches and the results are found to be quite promising. The novel use of performance indicators for financial sentiment analysis offers interesting and useful insights.	association rule learning;benchmark (computing);business process;categorization;dictionary;domain-specific language;hierarchical classifier;lexicon;machine learning;numerical weather prediction;sentiment analysis	Srikumar Krishnamoorthy	2017	Knowledge and Information Systems	10.1007/s10115-017-1134-1	data mining;sentiment analysis;performance indicator;finance;association rule learning;classifier (linguistics);computer science;text mining	AI	-21.584704773786203	-58.03081397104733	40305
6ed884f92d3cd6559eb0a7ae2bab7ff8e4acd630	virtual clone fish experiment	behavior model;virtual clone fish experimental;behavior modeling;virtual reality;virtual reality technique;behavior model virtual reality virtual clone fish experimental artificial life virtual biology;virtual clone fish experimental system;cloning marine animals virtual reality embryo pharmaceutical technology biological system modeling cells biology microorganisms information technology computational biology;system simulation;artificial life virtual clone fish experimental system behavior model virtual reality technique virtual biology;artificial life;virtual biology	A virtual clone fish experimental system is designed and set up. The system simulates the clone fish experiment process using the technique of virtual reality based on behavior model. Through the experiment, users can further observe the results and make conclusion more conveniently and quickly. The virtual clone fish experiment can reduce unnecessary waste caused in a reality clone fish experiment and its long waiting process, and save a quantity of time for users. The paper also discusses the application of virtual reality technique in the area of artificial life, and its experiment development and application.	artificial life;behavior model;experimental system;virtual reality	Qian Xu;Wenyong Wang;Shaochun Zhong;Xiao-Lin Quan;Qingrong Zhang;Ying Liang	2006	16th International Conference on Artificial Reality and Telexistence--Workshops (ICAT'06)	10.1109/ICAT.2006.143	simulation;engineering;communication	Visualization	-8.420997086067516	-56.645368623365414	40370
e49a6b326062abe396495b9c1c9b0042f98a7d19	comics instance search with bag of visual words	visual instance search;comics;bag of visual words;lucene	Comics is rapidly developing and attracting a lot of people around the world. The problem is how a reader can find a translated version of a comics in his or her favorite language when he or she sees a certain comics page in another language. Therefore, in this paper, we propose a comics instance search based on Bag of Visual Words so that readers can find in a collection of translated versions of various comics with a single instance as a comics page in an arbitrary language. Our method is based on visual information and does not rely on textual information of comics. Our proposed system uses Apache Lucene to handle inverted index process to find comics pages with visual words and spatial verification using RANSAC to eliminate bad results. Experimental results on our dataset with 20 comics containing more than 270,000 images achieve the accuracy up to 77.5i¾?%. This system can be improved for building a commercial system that allows a reader easily search a multi-language collection of comics with a comics page as an input query.	bag-of-words model in computer vision	Duc-Hoang Nguyen;Minh-Triet Tran;Vinh-Tiep Nguyen	2015		10.1007/978-3-319-26135-5_22	computer science;multimedia;world wide web;computer graphics (images)	Vision	-17.16949002924539	-57.308935000898835	40377
a35db53edd8177b7c3ffed888f0d57fd7df29155	a synonym based approach of data mining in search engine optimization		In today’s era with the rapid growth of information on the web, makes users turn to search engines as a replacement of traditional media. This makes sorting of particular information through billions of webpages and displaying the relevant data makes the task tough for the search engine. Remedy for this is SEO (Search Engine Optimization), i.e. having a website optimized in such a way that it will display the relevant webpages based on ranking. This is the main reason that makes search engine optimization a prominent position in online world. This paper present a synonym based data mining approach for SEO that makes the task of improving the ranking of the website much easier way and user will get answer to their query easily through any of search engine available in market.	data mining;mathematical optimization;optimizing compiler;search engine optimization;sorting;web search engine	Palvi Arora;Tarun Bhalla	2014	CoRR	10.14445/22312803/IJCTT-V12P140	database search engine;organic search;metasearch engine;computer science;spamdexing;data mining;search analytics;world wide web;information retrieval;search engine	AI	-30.42104813504071	-54.73106963862308	40394
abcfb2d359d3a030e1487fd9be8b3556ba4d14d5	post rectifying methods to improve the accuracy of image annotation		Image annotation methods construct a Tag distance matrix, which entries show the relevancy of tags for each test image. More accuracy in calculating this matrix provides better annotation results. The aim of our two methods is to improve the accuracy of the Tag distance matrix using the class information already available in most datasets. If the class information is not available, extracting important tags from the trainset and using it like the class will do the work. We used the Tag distance matrix and constructed Class-tag relation matrix to predict image class. Then, we rectify Tag distance matrix by using the predicted class and the Class-tag relation matrix. The advantage of our methods is its independence regarding feature vector, dataset and annotation method. Our experiments showed improvement in all annotation methods tested. We show that this improvement can be obtained by currently available annotation methods which produce a Tag distance matrix. We recalculate the Tag distance matrix produced by these methods.	automatic image annotation;distance matrix;experiment;feature vector;rectifier;relevance;standard test image;tag cloud	Artin Ghostan Khatchatoorian;Mansour Jamzad	2017	2017 International Conference on Digital Image Computing: Techniques and Applications (DICTA)	10.1109/DICTA.2017.8227478	pattern recognition;automatic image annotation;artificial intelligence;computer science;logical matrix;feature vector;image retrieval;feature extraction;matrix (mathematics);standard test image;distance matrix	Vision	-14.293527455040978	-61.819324403994486	40399
1819ea83c1d8aa4c2d6f2eddc5f18dac20a78e41	university of waterloo at trec 2010: legal interactive		This year the University of Waterloo (UW) participated in the TREC Legal Interactive track and used the same process as last year except that this year we used three different human operators as opposed to only one as UW did last year. We participated in three topics: 301, 302, and 303. Relative to other participants, we performed well on one of the three topics. For two of the topics, low recall significantly hurt our F1 scores. Overall, we believe a contributing factor in our lower performance this year was with our interaction with the topic authorities, which resulted in our failing to understand the wide range of what constituted relevant material.	failure;interactivity	Mark D. Smucker;Charles L. A. Clarke;Gordon V. Cormack;Olga Vechtomova	2010			simulation;computer science;operations research;world wide web;information retrieval	Web+IR	-32.137832359941584	-63.211313272502274	40407
d5a01d0139fc406ccb716d1704aaf0a168c35e95	a bayesian framework for fusing multiple word knowledge models in videotext recognition	ocr bayesian framework multiple word knowledge model videotext recognition font style cluttered background multiple frame averaging image interpolation lexicon correction multimodality language model multiple knowledge combination mixture model learning approach expectation maximization em back off smoothing approach bayesian model british national corpus unique time distance distribution model videotext word closed caption word word recognition optical character recognition;bayesian framework;belief networks;lexicon correction;image recognition;unique time distance distribution model;videotext word;interpolation;cluttered background;image resolution;fuses;optical character recognition;low resolution;british national corpus;image interpolation;bayesian methods;videotext recognition;false alarm rate;text analysis;multimodality language model;multiple knowledge combination;layout;multiple frame averaging;computer vision;video indexing;optical character recognition software;video coding;smoothing methods;expectation maximization;mixture model;dictionaries;distributed models;word recognition;font style;ocr;video sharing;learning artificial intelligence;character recognition;closed caption word;bayesian methods character recognition layout image recognition interpolation smoothing methods fuses optical character recognition software video sharing dictionaries;knowledge modeling;em;image resolution belief networks learning artificial intelligence video coding text analysis optical character recognition computer vision;multiple word knowledge model;language model;bayesian model;learning approach;back off smoothing approach	Videotext recognition is challenging due to low resolution, diverse fonts/styles, and cluttered background. Past methods enhanced recognition by using multiple frame averaging, image interpolation and lexicon correction, but recognition using multi-modality language models has not been explored. In this paper, we present a formal Bayesian framework for videotext recognition by combining multiple knowledge using mixture models, and describe a learning approach based on Expectation-Maximization (EM). In order to handle unseen words, a back-off smoothing approach derived from the Bayesian model is also presented. We exploited a prototype that fuses the model from closed caption and that from the British National Corpus. The model from closed caption is based on a unique time distance distribution model of videotext words and closed caption words. Our method achieves a significant performance gain, with word recognition rate of 76.8% and character recognition rate of 86.7%. The proposed methods also reduce false videotext detection significantly, with a false alarm rate of 8.2% without substantial loss of recall.	british national corpus;expectation–maximization algorithm;image resolution;interpolation;knowledge representation and reasoning;language model;lexicon;mixture model;modality (human–computer interaction);optical character recognition;prototype;smoothing;speech recognition;teletext;word error rate	DongQing Zhang;Shih-Fu Chang	2003		10.1109/CVPR.2003.1211512	computer vision;speech recognition;image resolution;computer science;machine learning;pattern recognition;language model	Vision	-18.426461207567183	-78.97226458002685	40420
5623a21b4a954989702dc4d6718fd4b3d0c286df	a comprehensive benchmark of kernel methods to extract protein–protein interactions from literature	information extraction;text mining;rule based;data mining;performance metric;models molecular;evaluation metric;area under curve;proteins;reproducibility of results;life sciences;protein protein interaction;algorithms;kernel method;cross validation;protein interaction mapping;decision trees;information leakage;natural language processing;parameter optimization;extraction method;databases protein	The most important way of conveying new findings in biomedical research is scientific publication. Extraction of protein-protein interactions (PPIs) reported in scientific publications is one of the core topics of text mining in the life sciences. Recently, a new class of such methods has been proposed - convolution kernels that identify PPIs using deep parses of sentences. However, comparing published results of different PPI extraction methods is impossible due to the use of different evaluation corpora, different evaluation metrics, different tuning procedures, etc. In this paper, we study whether the reported performance metrics are robust across different corpora and learning settings and whether the use of deep parsing actually leads to an increase in extraction quality. Our ultimate goal is to identify the one method that performs best in real-life scenarios, where information extraction is performed on unseen text and not on specifically prepared evaluation data. We performed a comprehensive benchmarking of nine different methods for PPI extraction that use convolution kernels on rich linguistic information. Methods were evaluated on five different public corpora using cross-validation, cross-learning, and cross-corpus evaluation. Our study confirms that kernels using dependency trees generally outperform kernels based on syntax trees. However, our study also shows that only the best kernel methods can compete with a simple rule-based approach when the evaluation prevents information leakage between training and test corpora. Our results further reveal that the F-score of many approaches drops significantly if no corpus-specific parameter optimization is applied and that methods reaching a good AUC score often perform much worse in terms of F-score. We conclude that for most kernels no sensible estimation of PPI extraction performance on new text is possible, given the current heterogeneity in evaluation data. Nevertheless, our study shows that three kernels are clearly superior to the other methods.	area under curve;benchmark (computing);biological science disciplines;body of uterus;convolution;cross reactions;cross-validation (statistics);dependency grammar;drops - drug form;extravasation;f1 score;information extraction;information leakage;interaction;kernel method;linguistics;logic programming;mathematical optimization;parsing;pixel density;population parameter;proton pump inhibitors;real life;scientific publication;societies, scientific;spectral leakage;text corpus;text mining;trees (plant);sentence	Domonkos Tikk;Philippe Thomas;Peter Palaga;Jörg Hakenberg;Ulf Leser	2010		10.1371/journal.pcbi.1000837	protein–protein interaction;kernel method;text mining;integral;computer science;bioinformatics;data science;machine learning;decision tree;data mining;information extraction;cross-validation;statistics	NLP	-19.533025720498326	-70.06712641593185	40437
3f9aaf52704af2a3d9c33a7ea54792725d66956d	identifying pronominal verbs: towards automatic disambiguation of the clitic 'se' in portuguese		A challenging topic in Portuguese language processing is the multifunctional and ambiguous use of the clitic pronoun se, which impacts NLP tasks such as syntactic parsing, semantic role labeling and machine translation. Aiming to give a step forward towards the automatic disambiguation of se, our study focuses on the identification of pronominal verbs, which correspond to one of the six uses of se as a clitic pronoun, when se is considered a CONSTITUTIVE PARTICLE of the verb lemma to which it is bound, as a multiword unit. Our strategy to identify such verbs is to analyze the results of a corpus search and to rule out all the other possible uses of se. This process evidenced the features needed in a computational lexicon to automatically perform the disambiguation task. The availability of the resulting lexicon of pronominal verbs on the web enables their inclusion in broader lexical resources, such as the Portuguese versions of Wordnet, Propbank and VerbNet. Moreover, it will allow the revision of parsers and dictionaries already in use.	dictionary;lexicon;machine translation;multi-function printer;natural language processing;parsing;propbank;semantic role labeling;verbnet;word-sense disambiguation;wordnet	Magali Sanches Duran;Carolina Scarton;Sandra M. Aluísio;Carlos Ramisch	2013			natural language processing;computer science;linguistics	NLP	-28.146972606481356	-76.62525948723412	40485
34f3817dbbfffd94553ba9e39aded81697b37040	do words reveal concepts?		To study concepts, cognitive scientists must first identify some. The prevailing assumption is that they are revealed by words such as triangle, table, and robin. But languages vary dramatically in how they carve up the world by name. Either ordinary concepts must be heavily language-dependent or names cannot be a direct route to concepts. We asked English, Dutch, Spanish, and Japanese speakers to name videos of human locomotion and judge their similarities. We investigated what name inventories and scaling solutions on name similarity and on physical similarity for the groups individually and together suggest about the underlying concepts. Aggregated naming and similarity solutions converged on results distinct from the answers suggested by any single language. Words such as triangle, table, and robin help identify the conceptual space of a domain, but they do not directly reveal units of knowledge usefully considered “concepts.”	cognitive science;image scaling;inventory	Barbara Malt;Eef Ameel;Silvia P. Gennari;Mutsumi Imai;Noburo Saji;Asifa Majid	2011				HCI	-12.364134520674623	-76.93417361457674	40523
bb1f0e3782cf9e28db78eb27fd5f54e5d1f8a34f	interpretation of semantic tweet representations		Research in analysis of microblogging platforms is experiencing a renewed surge with a large number of works applying representation learning models for applications like sentiment analysis, semantic textual similarity computation, hashtag prediction, etc. Although the performance of the representation learning models has been better than the traditional baselines for such tasks, little is known about the elementary properties of a tweet encoded within these representations, or why particular representations work better for certain tasks. Our work presented here constitutes the first step in opening the black-box of vector embeddings for tweets.  Traditional feature engineering methods for high-level applications have exploited various elementary properties of tweets. We believe that a tweet representation is effective for an application because it meticulously encodes the application-specific elementary properties of tweets. To understand the elementary properties encoded in a tweet representation, we evaluate the representations on the accuracy to which they can model each of those properties such as tweet length, presence of particular words, hashtags, mentions, capitalization, etc.  Our systematic extensive study of nine supervised and four unsupervised tweet representations against most popular eight textual and five social elementary properties reveal that Bi-directional LSTMs (BLSTMs) and Skip-Thought Vectors (STV) best encode the textual and social properties of tweets respectively. FastText is the best model for low resource settings, providing very little degradation with reduction in embedding size. Finally, we draw interesting insights by correlating the model performance obtained for elementary property prediction tasks with the high-level downstream applications.	bi-directional text;black box;computation;downstream (software development);encode;elegant degradation;feature engineering;feature learning;hashtag;high- and low-level;list of sega arcade system boards;machine learning;sentiment analysis	J Ganesh;Manish Gupta;Vasudeva Varma	2017		10.1145/3110025.3110083	artificial intelligence;natural language processing;machine learning;sentiment analysis;data mining;robustness (computer science);microblogging;viral marketing;computation;computer science;social network;social media;feature engineering;feature learning	Web+IR	-17.54234979488235	-71.46041623779661	40540
64e880ce894ace4ecfe9bb93359d8a0f22e9249c	jabalin: a comprehensive computational model of modern standard arabic verbal morphology based on traditional arabic prosody		The computational handling of Modern Standard Arabic is a challenge in the field of natural language processing due to its highly rich morphology. However, several authors have pointed out that the Arabic morphological system is in fact extremely regular. The existing Arabic morphological analyzers have exploited this regularity to variable extent, yet we believe there is still some scope for improvement. Taking inspiration in traditional Arabic prosody, we have designed and implemented a compact and simple morphological system which in our opinion takes further advantage of the regularities encountered in the Arabic morphological system. The output of the system is a large-scale lexicon of inflected forms that has subsequently been used to create an Online Interface for a morphological analyzer of Arabic verbs. The Jabalín Online Interface is available at http://elvira.lllf.uam.es/jabalin/, hosted at the LLI-UAM lab. The generation system is also available under a GNU GPL 3 license.	algorithm;categorization;computation;computational model;floating-point unit;formal system;gnu;galaxy morphological classification;lexicon;mathematical morphology;natural language processing;orthographic projection;sap hana;semantic prosody;syllable;variable (computer science)	Alicia Gonzalez Martínez;Susana Lopez Hervas;Doaa Samy;Carlos G. Arques;Antonio Moreno-Sandoval	2013		10.1007/978-3-642-40486-3_3	natural language processing;speech recognition;computer science;linguistics	NLP	-29.966208004355945	-75.16979684435144	40565
750ee0c6342749a79a826e23f31ede77a48e85ea	automatic web query classification using large unlabeled web pages	web pages;query processing;relevant word query classification the domain knowledge base;data mining;classification;query classification;domain knowledge;large scale;large scale unlabeled web page;automatic web query classification;internet;query processing classification data mining internet knowledge based systems;relevant word;information management;the domain knowledge base;automatic domain knowledge base construction;knowledge based systems;data mining automatic web query classification large scale unlabeled web page automatic domain knowledge base construction	In this paper, a novel and simple method is employed to automatically construct domain knowledge base for query classification from large-scale Web pages. Besides, using context as the feature of words, the resource of relevant words is built automatically in order to extend the user's query. On the basis of domain knowledge base and extension of the query using relevant words, satisfactory performance in query classification is achieved. Experimental results demonstrate that our method achieves precision of 77.68% and recall of 75.34% in Chinese query classification. In English experiments, in spite of the scarcity of English Web pages and absence of stemming, precision achieves 58.83% and recall achieves 54.13%, which is a great improvement compared to state-of-the-art query classification algorithms.	algorithm;experiment;knowledge base;stemming;web page;web query classification	Jingbo Yu;Na Ye	2008	2008 The Ninth International Conference on Web-Age Information Management	10.1109/WAIM.2008.91	sargable;query optimization;query expansion;web query classification;the internet;biological classification;computer science;artificial intelligence;query by example;web page;data mining;database;rdf query language;information management;web search query;world wide web;information retrieval;domain knowledge;query language	DB	-25.589938224084786	-65.65564657767182	40573
b020d04d5b48ea4ccdf9fd56ca4afdbc7b40b4fa	a multilevel approach to pattern processing	pattern processing;speech recognition;descriptive technique;syntactic rules;multilevel;analysis/synthesis	This paper presents a methodology for describing multilevel pattern processing systems. It is suggested that any pattern processor can be adequately described in terms of multiple hierarchies of two types of fundamental mechanism: (1) a process which performs the pattern recognition functions of analysis and synthesis and (2) a process which performs the syntactic functions of parsing and generation. A computer implementation of these principles is outlined which enables a range of systems to be configured. Examples of speech and non-speech pattern processing are presented.		Roger K. Moore	1981	Pattern Recognition	10.1016/0031-3203(81)90070-4	state pattern;computer science;theoretical computer science;algorithm	Vision	-8.277461226109175	-72.69640938755923	40638
2bcd9bccd4a651e8879517005c346fce005c791d	natural language processing for dialectical arabic: a survey		This paper presents a wide literature review of natural language processing for dialectical Arabic. Four main research areas were identified and the dialect coverage in research work was outlined. The paper can be used as a quick reference to identify relevant contributions that address a specific NLP aspect for a specific dialect.	natural language processing	Abdulhadi Shoufan;Sumaya Alameri	2015		10.18653/v1/W15-3205	psychology;natural language processing;linguistics;communication	NLP	-31.796559934738756	-74.05031192259476	40670
8ae41cf75559e9adf7f107ee8be9c240f01e74eb	assisting translators in indirect lexical transfer	generative lexicon;parallel corpora	We present the design and evaluation of a translator’s amenuensis that uses comparable corpora to propose and rank nonliteral solutions to the translation of expressions from the general lexicon. Using distributional similarity and bilingual dictionaries, the method outperforms established techniques for extracting translation equivalents from parallel corpora. The interface to the system is available at: http://corpus.leeds.ac.uk/assist/v05/	bilingual dictionary;lexicon;parallel text;regular expression;text corpus	Bogdan Babych;Anthony Hartley;Serge Sharoff;Olga Mudraya	2007			natural language processing;speech recognition;generative lexicon;computer science;linguistics	NLP	-25.32217178443771	-75.22093948297541	40689
579799f0388df740651e87c032b7641a86fc9a4a	issues from corpus analysis that have influenced the on-going development of various haitian creole text- and speech-based nlp systems and applications			creole (markup);natural language processing	Marilyn Mason	2000			haitian creole;natural language processing;artificial intelligence;computer science;linguistics	NLP	-30.81624207302623	-77.6520645383107	40701
ad7d10c012c92f9ee1279b050c856250370429a4	ordering concepts based on common attribute intensity		This paper presents a novel task of ordering given concepts (e.g., London, Paris, and Rome) on the basis of common attribute intensity expressed by a given adjective (e.g., safe) and proposes statistical ordering methods that integrate heterogeneous evidence extracted from text on concept ordering. This study is aimed at deriving collective wisdom on concept ordering from social media text. Solving this task is not only interesting from a sociological perspective but also beneficial in the practical sense for those who want to order unfamiliar entities in terms of subjective attributes that are hard to quantify in order to make correct decisions. Experiments on real-world concepts revealed a strong correlation between orderings obtained by our methods and gold-standard orderings.	code;entity;experiment;neural coding;social media	Tatsuya Iwanari;Naoki Yoshinaga;Nobuhiro Kaji;Toshiharu Nishina;Masashi Toyoda;Masaru Kitsuregawa	2016			artificial intelligence;machine learning;adjective;collective wisdom;sociological imagination;social media;mathematics	AI	-17.90482213309266	-62.14389717320015	40713
f64a734f36b6ce2887ef50da8f18f376584ff8e8	topic-specific web searching based on a real-text dictionary		The contributions of this paper are twofold. First, we present a new type of dictionary that is intended as a search assistance in topic-specific Web searching. The method to construct the dictionary is a general method that can be applied to any reasonable topic. The first implementation deals with climate change. The dictionary has the following new features compared to standard dictionaries and thesauri: (A) It contains real-text phrases (e.g. rising sea levels) in addition to the standard dictionary forms (sea-level rise). The phrases were extracted automatically from the pages dealing with climate change, and are thus known to appear in the pages discussing climate change issues when used as search terms. (B) Synonyms, i.e., different spelling, syntactic, and short form variants of the phrase are grouped together into the same entry (synonym set) using approximate string matching. (C) Each phrase is assigned an importance score (IS) which is calculated based on the frequencies of the phrase in relevant pages (i.e., pages on climate change) and nonrelevant pages. Second, we investigate how effective the IS is for indicating the best phrase among synonymous phrases and for indicating effective phrases in general from the viewpoint of search results. The experimental results showed that the best phrases have higher ISs than the other phrases of a synonym set, and that the higher the IS is the better the search results are. This paper also describes the crawler used to fetch the source data for the climate change dictionary and discusses the benefits of using the dictionary in Web	academy;approximate string matching;data dictionary;information needs;source data;string searching algorithm;synonym ring;thesaurus;web crawler;web search engine;world wide web	Ari Pirkola	2012			data mining;world wide web;computer science;machine-readable dictionary	Web+IR	-29.410803963597743	-68.4962700641385	40715
414cad6578f0b91f86e99c618dc6d2070a0d90b3	a maximum entropy-based word sense disambiguation system	supervised learning;semantics;semantica;word sense disambiguation;aprendizaje automatico;machine learning;natural language processing;info eu repo semantics bookpart;maximum entropy;procesamiento del lenguaje natural;desambiguacion del sentido de las palabras;knowledge base	In this paper, a supervised learning system of word sense disambiguation is presented. It is based on conditional maximum entropy models. This system acquires the linguistic knowledge from an annotated corpus and this knowledge is represented in the form of features. Several types of features have been analyzed using the SENSEVAL-2 data for the Spanish lexical sample task. Such analysis shows that instead of training with the same kind of information for all words, each one is more e ectively learned using a di erent set of features. This bestfeature-selection is used to build some systems based on di erent maximum entropy classi ers, and a voting system helped by a knowledgebased method.	feature selection;gene ontology term enrichment;information source;internationalized domain name;maximum entropy spectral estimation;newton's method;principle of maximum entropy;supervised learning;synonym ring;text corpus;universal media disc;user agent;word sense;word-sense disambiguation;wordnet	Armando Suárez;Manuel Palomar	2002		10.3115/1072228.1072343	natural language processing;knowledge base;speech recognition;semeval;computer science;artificial intelligence;principle of maximum entropy;machine learning;semantics;linguistics;supervised learning	NLP	-24.363449336112726	-72.41192395837643	40732
b04ccd461a63a849b273fa72b00e4788f80eabf7	augmenting dublin core digital library metadata with dewey decimal classification	online catalogues;online databases;digital libraries;cataloguing;classification;classification schemes	Purpose – The purpose of this paper is to describe a new approach to a well-known problem for digital libraries, how to search across multiple unrelated libraries with a single query. Design/methodology/approach – The approach involves creating new Dewey Decimal Classification terms and numbers from existing Dublin Core records. In total, 263,550 records were harvested from three digital libraries. Weighted key terms were extracted from the title, description and subject fields of each record. Ranked DDC classes were automatically generated from these key terms by considering DDC hierarchies via a series of filtering and aggregation stages. A mean reciprocal ranking evaluation compared a sample of 49 generated classes against DDC classes created by a trained librarian for the same records. Findings – The best results combined weighted key terms from the title, description and subject fields. Performance declines with increased specificity of DDC level. The results compare favorably with similar studies. R...		Michael Khoo;Jae-wook Ahn;Ceri Binding;Hilary Jane Jones;Xia Lin;Diane Massam;Douglas Tudhope	2015	Journal of Documentation	10.1108/JD-07-2014-0103	library science;digital library;biological classification;classification scheme;computer science;database;world wide web;information retrieval	NLP	-31.690339947900195	-59.584315878721746	40755
8a8f5429c7cb749be3bba42eba95e79c3fb3b4a8	predicting interesting things in text		While reading a document, a user may encounter concepts, entities, and topics that she is interested in exploring more. We propose models of “interestingness”, which aim to predict the level of interest a user has in the various text spans in a document. We obtain naturally occurring interest signals by observing user browsing behavior in clicks from one page to another. We cast the problem of predicting interestingness as a discriminative learning problem over this data. We leverage features from two principal sources: textual context features and topic features that assess the semantics of the document transition. We learn our topic features without supervision via probabilistic inference over a graphical model that captures the latent joint topic space of the documents in the transition. We train and test our models on millions of realworld transitions between Wikipedia documents as observed from web browser session logs. On the task of predicting which spans are of most interest to users, we show significant improvement over various baselines and highlight the value of our latent semantic model.	entity;flickr;graphical model;local interconnect network;named entity;named-entity recognition;observable;topic model;web content;wikipedia	Michael Gamon;Arjun Mukherjee;Patrick Pantel	2014			natural language processing;computer science;data mining;world wide web;information retrieval	ML	-23.32150893728562	-52.7634577777674	40760
b767b84c915c498c0fae80ea86b9d56c0a7cb516	bilingual indexing for information retrieval with autindex.		AUTINDEX is a bilingual automatic indexing system for the two languages German and English. It is being developed within the EU-funded BINDEX project. The aim of the system is to automatically index large quantities of abstracts of scientific and technical papers from several areas of engineering. Automatic indexing takes place using a controlled vocabulary provided in monolingual and bilingual thesauri. AUTINDEX produces for a given abstract a list of descriptors as well as a list of classification codes using these thesauri. It also allows for free indexing indexing with an unrestricted vocabulary (delivering so called 'free descriptors ́). These free descriptors are used to enhance and extend the thesauri. The bilingual AUTINDEX module indexes German abstracts in English and	autindex;code;controlled vocabulary;dictionary;documentation;information architecture institute;information retrieval;lexicon;mock object;requirement;thesaurus (information retrieval)	Dieter Maas;Nuebel Rita;Catherine Pease;Paul Schmidt	2002			information retrieval;human–computer information retrieval	Vision	-33.42253402245217	-75.49397422475161	40805
d3e7c3207df7e9e6f17fee49ff9de32dfc93a9cc	automatic paper-to-reviewer assignment, based on the matching degree of the reviewers	hungarian algorithm	There are a number of issues which are involved with organizing a conference. Among these issues, assigning conference-papers to reviewers is one of the most difficult tasks. Assigning conference-papers to reviewers is automatically the most crucial part. In this paper, we address this issue of paper-to-reviewer assignment, and we propose a method to model the reviewers, based on the matching degree between the reviewers and the papers by combining a preference-based approach and a topic-based approach. We explain the assignment algorithm and show the evaluation results in comparison with the Hungarian algorithm.	assignment problem;feedback;hungarian algorithm;organizing (structure)	Xinlian Li;Toyohide Watanabe	2013		10.1016/j.procs.2013.09.144	computer science;machine learning;data mining;information retrieval	Robotics	-26.897857864576928	-61.29020455197974	40812
0483f4a33790b3eccfe59b1a818cacae597cbd6a	unsupervised induction of tree substitution grammars for dependency parsing	dependency parsing	Inducing a grammar directly from text is one of the oldest and most challenging tasks in Computational Linguistics. Significant progress has been made for inducing dependency grammars, however the models employed are overly simplistic, particularly in comparison to supervised parsing models. In this paper we present an approach to dependency grammar induction using tree substitution grammar which is capable of learning large dependency fragments and thereby better modelling the text. We define a hierarchical non-parametric Pitman-Yor Process prior which biases towards a small grammar with simple productions. This approach significantly improves the state-of-the-art, when measured by head attachment accuracy.	attachments;computation;computational linguistics;dependency grammar;grammar induction;parsing;pitman–yor process	Phil Blunsom;Trevor Cohn	2010			natural language processing;parser combinator;l-attributed grammar;link grammar;parsing expression grammar;computer science;parsing;s-attributed grammar;pattern recognition;phrase structure rules;linguistics;attribute grammar;top-down parsing;immediate constituent analysis;dependency grammar	NLP	-22.20728013335643	-75.81031584356344	40892
941d2b7986dcc17935275250ee1e39170a240c17	the logical diversity of explanations in owl ontologies	debugging;explanation;owl;semantic web;ontologies;bioinformatics	Given the high expressivity of the Web Ontology Language OWL 2, there is a potential for great diversity in the logical content of OWL ontologies. The fact that many naturally occurring entailments of such ontologies have multiple justifications indicates that ontologies often overdetermine their consequences, suggesting a diversity in supporting reasons. On closer inspection, however, we often find that justifications---even for multiple entailments---appear to be structurally similar, suggesting that their multiplicity might be due to diverse material, not formal grounds for an entailment.  In this paper, we introduce and explore several equivalence relations over justifications for entailments of OWL ontologies which partition a set of justifications into structurally similar subsets. These equivalence relations range from strict isomorphism to looser notions of similarity, covering justifications which contain different class expressions, or even different numbers of axioms. We present the results of a survey of 78 ontologies from the biomedical domain which shows that OWL ontologies used in practice often contain large numbers of structurally similar justifications. We find that a large justification corpus can be reduced by 97% of its original size to a small core of frequently occurring justification templates.	debugger;debugging;loose coupling;ontology (information science);process (computing);text corpus;turing completeness;web ontology language	Samantha Bail;Bijan Parsia;Ulrike Sattler	2013		10.1145/2505515.2505536	computer science;ontology;artificial intelligence;semantic web;data mining;database;debugging;algorithm	Web+IR	-28.09485150025495	-70.7302439561502	40942
0cbc08adb7a85770d53ac3ce9b2dce2f8f4871ef	the inside-outside recursive neural network model for dependency parsing		We propose the first implementation of an infinite-order generative dependency model. The model is based on a new recursive neural network architecture, the Inside-Outside Recursive Neural Network. This architecture allows information to flow not only bottom-up, as in traditional recursive neural networks, but also topdown. This is achieved by computing content as well as context representations for any constituent, and letting these representations interact. Experimental results on the English section of the Universal Dependency Treebank show that the infinite-order model achieves a perplexity seven times lower than the traditional third-order model using counting, and tends to choose more accurate parses in k-best lists. In addition, reranking with this model achieves state-of-the-art unlabelled attachment scores and unlabelled exact match scores.	artificial neural network;attachments;bottom-up parsing;direct inward dial;generative model;network architecture;network model;perplexity;random neural network;recursion (computer science);recursive neural network;top-down and bottom-up design;treebank	Phong Le;Willem H. Zuidema	2014			natural language processing;speech recognition;computer science;machine learning	NLP	-17.648232682356394	-75.3282407329924	40950
31dbac0d8ff8867f51ff075d3457c7375ec0dd88	investigating the use of extractive summarisation in sentiment classification		In online reviews, authors often use a short passage to describe the overall feeling about a product or a service. A review as a whole can mention many details not in line with the overall feeling, so capturing this key passage is important to understand the overall sentiment of the review. This paper investigates the use of extractive summarisation in the context of sentiment classification. The aim is to find the summary sentence, or the short passage, which gives the overall sentiment of the review, filtering out potential noisy information. Experiments are carried out on a movie review data-set. The main finding is that subjectivity detection plays a central role in building summaries for sentiment classification. Subjective extracts carry the same polarity of the full text reviews, while statistical and positional approaches are not able to capture this aspect.	document classification;entity;naive bayes classifier;sentence extraction	Marco Bonzanini;Miguel Martinez-Alvarez;Thomas Roelleke	2012			data mining;feeling;subjectivity;sentence;computer science	NLP	-22.79458498514284	-60.00677096773872	40964
b9af91e3da0fd3c84829798ca47fab1e5999f673	a very large database of collocations and semantic links	dependency grammar;specification language;inference rule;very large database;foreign language learning;semantic relations;system management;word processing	A computational system manages a very large database of collocations (word combinations) and semantic links. The collocations are related (in the meaning of a dependency grammar) word pairs, joint immediately or through prepositions. Synonyms, antonyms, subclasses, superclasses, etc. represent semantic relations and form a thesaurus. The structure of the system is universal, so that its language-dependent parts are easily adjustable to any specific language (English, Spanish, Russian, etc.). Inference rules for prediction of highly probable new collocations automatically enrich the database at runtime. The inference is assisted by the available thesaurus links. The aim of the system is word processing, foreign language learning, parse filtering, and lexical disambiguation.	collocation;database;dependency grammar;lexicon;parsing;run time (program lifecycle phase);thesaurus;word-sense disambiguation	Igor A. Bolshakov;Alexander F. Gelbukh	2000		10.1007/3-540-45399-7_9	natural language processing;systems management;specification language;computer science;linguistics;programming language;rule of inference;dependency grammar;very large database	NLP	-30.246233948190323	-71.12295632035588	40989
471c76aff7607770a5d4f364577148803044409e	scalable discovery of contradictions on the web	second order;opinion mining;interaction analysis;large scale;contradiction analysis;data extraction;computational efficiency;data structure	Our study addresses the problem of large-scale contradiction detection and management, from data extracted from the Web. We describe the first systematic solution to the problem, based on a novel statistical measure for contradictions, which exploits first- and second-order moments of sentiments. Our approach enables the interactive analysis and online identification of contradictions under multiple levels of time granularity. The proposed algorithm can be used to analyze and track opinion evolution over time and to identify interesting trends and patterns. It uses an incrementally updatable data structure to achieve computational efficiency and scalability. Experiments with real datasets show promising time performance and accuracy.	algorithm;data structure;scalability;stationary process;world wide web	Mikalai Tsytsarau;Themis Palpanas;Kerstin Denecke	2010		10.1145/1772690.1772871	data structure;computer science;data science;machine learning;data mining;database;programming language;world wide web;second-order logic	ML	-23.82373999909554	-56.200152619101495	40998
f5876e6b74b1e0f9f416ed32561c5b9e5f85d630	cvte at ijcnlp-2017 task 1: character checking system for chinese grammatical error diagnosis task		Grammatical error diagnosis is an important task in natural language processing. This paper introduces CVTE Character Checking System in the NLP-TEA-4 shared task for CGED 2017, we use Bi-LSTM to generate the probability of every character, then take two kinds of strategies to decide whether a character is correct or not. This system is probably more suitable to deal with the error type of bad word selection, which is one of four types of errors, and the rest are words redundancy, words missing and words disorder. Finally the second strategy achieves better F1 score than the first one at all of detection level, identification level, position level.	f1 score;long short-term memory;natural language processing	Xian Li;Suixue Wang;Guanyu Jiang;Tianyuan You	2017			artificial intelligence;computer science;natural language processing	NLP	-24.058916407336355	-76.40450860599475	41029
230560506fb88db32c6d2f00dd92918aa3b09613	automatic acquisition of context-specific lexical paraphrases	natural lan guage processing;information extraction;web mining;machine translation;question answering	Lexical paraphrasing aims at acquiring word-level paraphrases. It is critical for many Natural Language Processing (NLP) applications, such as Question Answering (QA), Information Extraction (IE), and Machine Translation (MT). Since the meaning and usage of a word can vary in distinct contexts, different paraphrases should be acquired according to the contexts. However, most of the existing researches focus on constructing paraphrase corpora, in which little contextual constraints for paraphrase application are imposed. This paper presents a method that automatically acquires context-specific lexical paraphrases. In this method, the obtained paraphrases of a word depend on the specific sentence the word occurs in. Two stages are included, i.e. candidate paraphrase extraction and paraphrase validation, both of which are mainly based on web mining. Evaluations are conducted on a news title corpus and the presented method is compared with a paraphrasing method that exploits a Chinese thesaurus of synonyms -Tongyi Cilin (Extended) (CilinE for short). Results show that the f-measure of our method (0.4852) is significantly higher than that using CilinE (0.1127). In addition, over 85% of the correct paraphrases derived by our method cannot be found in CilinE, which suggests that our method is effective in acquiring out-of-thesaurus paraphrases.	f1 score;information extraction;lexicon;machine translation;natural language processing;question answering;text corpus;thesaurus;web mining	Shiqi Zhao;Ting Liu;Xincheng Yuan;Sheng Li;Yu Zhang	2007			natural language processing;web mining;question answering;computer science;machine translation;information extraction;information retrieval	AI	-28.230952537190895	-72.11851930668085	41032
5674ace2c666f6af53a2a58279ade6ebd271e8c7	exploiting visual-based intent classification for diverse social image retrieval		In the 2017 MediaEval Retrieving Diverse Social Images task, we (TUD-MMC team) propose a novel method, namely an intent-based approach, for social image search result diversification. The underlying assumption is that the visual appearance of social images is impacted by the underlying photographic act, i.e., why the images were taken. Better understanding the rationale behind the photographic act could potentially benefit social image search result diversification. To investigate this idea, we employ a manual content analysis approach to create a taxonomy of intent classes. Our experiments show that a CNN-based neural network classifier is able to capture the visual difference between the classes in the intent taxonomy. We cluster images of the Flickr baseline based on predicted intent class and generate a re-ranked list by alternating images from different clusters. Our results reveal that, compared to conventional diversification strategies, intent-based search result diversification is able to bring a considerable improvement in terms of cluster recall with several extra benefits.		Bo Wang;Martha Larson	2017			natural language processing;artificial intelligence;information retrieval;artificial neural network;computer science;image retrieval;recall;content analysis;classifier (linguistics);visual appearance;diversification (marketing strategy)	ML	-17.239965957486923	-60.665164383402804	41157
cd750ba9e3bdbc54c8506c6ddb807a5b6b19ccee	the role of implicit learning in the acquisition of generative knowledge	conceptual model;implicit learning	This paper reports two experiments using a modified artificial grammar paradigm to study the acquisition of generative knowledge. It employs a starship game and a computerized form of motherese to study acquisition of generative knowledge of a grammar. It was found that initial exposure to a small set of representative cases and an opportunity to organize the cases enhanced ability to generate grammatical strings. Neither seeing a large number of cases nor building a conceptual model of the grammar enhanced acquisition of generative knowledge. However, building a conceptual model did increase learners' confidence in their ability to generate strings. The quality of strings generated by learners remained fairly constant across attempts, suggesting that their performance reflects true generative knowledge rather than memories of exemplars.		Robert C. Mathews;Lewis G. Roussel;Barbara P. Cochran;Ann E. Cook;Deborah L. Dunaway	2000	Cognitive Systems Research	10.1016/S1389-0417(00)00007-3	psychology;natural language processing;generative systems;computer science;artificial intelligence;conceptual model;machine learning	AI	-10.574696064903346	-76.0619144098127	41171
ff598c166a87bd31a204f27b2545df633bdf0b53	content-based information retrieval for vrml 3d objects	3d information;web pages;3d imaging;image databases;information extraction;multiple subsystems;virtual reality languages;information retrieval;vrml 3d objects;object parser system;shape comparison;object database;virtual reality;3d object;oct tree;information retrieval content based retrieval layout image databases virtual reality web sites web pages image retrieval shape data mining;graphical user interface;layout;interior design;data mining;graphical user interface content based information retrieval vrml 3d objects world wide web virtual reality modeling language web page 3d information cbr shape comparison color comparison similarity method object database multiple subsystems object parser system normalization system octtree database management system;normalization system;content based information retrieval;virtual reality modeling language;graphical user interfaces;web design;shape;feature extraction;web sites;octtree;vrml;graphic user interface;cbr;world wide web;similarity method;web page;color comparison;object oriented databases;graphical user interfaces virtual reality languages content based retrieval octrees web design object oriented databases;database management system;content based retrieval;octrees;image retrieval	Because of the interest of 3D images and the popularity of World Wide Web, the number of 3D scene/objects and model database throughout the world is growing both in number and in size. VRML (Virtual Reality Modeling Language) is used to model the 3D object of the Web page, and become as a standard for 3D information. Most content-based retrieval (CBR) techniques such as shape and color comparison among objects are for image and video. The mechanisms are designed based on 2D information. In this paper, we propose a new similarity method, based on 3D information extracted from a VRML object database. The system includes multiple subsystems, which are conducted in two main parts. The first part includes the VRML object parser system and the normalization system. The second part includes the comparison system, database management system and a friendly graphical user interface. The method is the first step of our project, which aims to incorporate feature extraction of virtual reality objects, such as chairs, car, and others in 3D space. House interior designers can use the proposed system. The user can select proper scenes and furniture in order to meet the requirement of potential customers.		Ching-Sheng Wang;Timothy K. Shih;Chun-Hung Huang;Jia-Fu Chen	2003		10.1109/AINA.2003.1192910	image retrieval;computer science;graphical user interface;database;virtual reality;world wide web;information extraction;information retrieval	Vision	-12.796268409453415	-57.4786563651912	41202
90fceb8838fdcf3642893cca63be9945db042e16	the romanian neuter examined through a two-gender n-gram classification system		In this paper we look at the gender system of Romanian and investigate, using machine learning techniques, the validity of the traditional analysis according to which Romanian is a three gender language. We offer strong evidence in favor of the two gender system analysis proposed in (Bateman and Polinsky, 2010) with classification accuracy higher than the one previously obtained and diverge from the approaches found in the works of (Cucerzan and Yarowsky, 2003) and (Nastase and Popescu, 2009) on automated classification of Romanian nouns according to gender, which leads us to the best accuracy in discriminating the neuter.	machine learning;n-gram;system analysis	Liviu P. Dinu;Vlad Niculae;Octavia-Maria Sulea	2012			artificial intelligence;natural language processing;n-gram;computer science;romanian	NLP	-21.73097496710772	-71.23286231650287	41208
8856ebdbf449de4c7a5b1a57b6f2160097f21f1c	shahmukhi to gurmukhi transliteration system	word frequency;corpus analysis;statistical analysis	The existence of two scripts for Punjabi language has created a script barrier between the Punjabi literature written in India and Pakistan. This research has developed a new system for the first time of its kind for Shahmukhi text without diacritical marks. The purposed system for Shahmukhi to Gurmukhi transliteration has been implemented with various research techniques based on language corpus. The corpus analysis of both scripts is performed for generating statistical data of different types like character and word frequencies and bi-gram frequencies. This statistical analysis is used in different phases of transliteration. Potentially, all members of the substantial Punjabi community will benefit vastly from this transliteration system.	bi-directional text;text corpus;word lists by frequency	Tejinder Singh Saini;Gurpreet Singh Lehal;Virinder S. Kalra	2008			natural language processing;speech recognition;computer science;word lists by frequency;linguistics	NLP	-25.44031383324332	-80.1151960566343	41333
f7e8d134bf192c42f5913cb293be7ca779e9161f	evalution-man 2.0: expand the evaluation dataset for vector space models	conference paper	We introduce EVALution 2.0, a simplified Mandarin dataset for the evaluation of Vector Space Models. We take a psycholinguistics-based methodology through the use of a verbal association task, which differs from previous datasets that use corpus and ontology to construct word relation pairs. Semantic neighbors were created for 100 target words and surprisingly, to which participants produced 1129 word relation pairs. In a separate agreement-rating task, only 62 pairs showed were rejected. The methodology has proven to be a way to expand the existing resources quickly while maintaining a high level of quality.		Hongchao Liu;Chu-Ren Huang	2016		10.1007/978-3-319-49508-8_25	computer science;data science;data mining;operations research	HCI	-27.439582918223568	-69.15313978180822	41373
7f46118b259743062e13e84df81ea7f990991b46	biosumm: a novel summarizer oriented to biological information	availability data mining proteins performance analysis petroleum data analysis dictionaries navigation indexing information retrieval;grading function;gene protein relationships;search engine;document handling;domain dictionary;data integrity;information retrieval system;database management systems;document handling bioinformatics data mining database management systems dictionaries;national institute of health;biology;biological information summarizer;data mining;digital archive;unstructured information management;indexes;domain dictionary biosumm biological information summarizer biomedical text repository biological text repository unstructured information management document summaries data mining analysis gene protein relationships grading function;document summaries;proteins;biosumm;dictionaries;life sciences;automatic indexing;biomedical text repository;clustering algorithms;biological text repository;information seeking;breast cancer;data mining analysis;bioinformatics	The availability of increasingly wider repositories of biomedical and biological texts requires effective techniques to manage the huge mass of unstructured information there contained. The availability of ad-hoc document summaries, targeted to specific topics, may assist researchers in inferring previously undisclosed knowledge and in performing the biological validation of the results of data mining analysis. This paper presents BioSumm, a flexible framework which analyzes large collections of unclassified biomedical texts and produces ad-hoc summaries oriented to inferring knowledge of gene/protein relationships. Summary generation is driven by a novel grading function, which biases sentence selection by means of an appropriate domain dictionary.	data dictionary;data mining;hoc (programming language)	Elena Baralis;Alessandro Fiori;Lorenzo Montrucchio	2008	2008 8th IEEE International Conference on BioInformatics and BioEngineering	10.1109/BIBE.2008.4696750	database index;computer science;bioinformatics;data science;breast cancer;data integrity;data mining;cluster analysis;information retrieval;search engine	Visualization	-27.09093376528962	-57.81840461534387	41383
f00e8b6e3fbcd20012fbf5baa03a510b106aa7ae	high-performance unsupervised relation extraction from large corpora	unsupervised learning;web relation extraction system;bag of words representation unsupervised relation extraction unsupervised relation identification pattern based representation web relation extraction system;unsupervised relation identification;unsupervised relation extraction;pattern based representation;relation extraction;unsupervised learning internet knowledge acquisition;internet;bag of words representation;knowledge acquisition;binary relation;data mining strontium machine learning gallium nitride relays knowledge engineering humans;high performance;bag of words	We present URIES - an unsupervised relation identification and extraction system. The system automatically identifies interesting binary relations between entities in the input corpus, and then proceeds to extract a large number of instances of these relations. The system discovers relations by clustering frequently co- occuring pairs of entities, based on the contexts in which they appear. Its complex pattern-based representation of the contexts allows the clustering step to achieve very high precision, sufficient for the clusters to perform as sets of seeds for bootstrapping a high-recall relation extraction process. In a series of experiments we demonstrate the successful performance of URIES and compare it to the two existing systems - a weakly supervised high-recall Web relation extraction system called SRES, and an unsupervised relation identification system that uses a simpler bag-ofwords representation of contexts. The experiments show that URIES performs comparably to SRES, but without any supervision, and that such performance is due to the power of its complex contexts representation and to its novel candidate selection method.	bag-of-words model;cluster analysis;entity;experiment;relationship extraction;text corpus	Binyamin Rosenfeld;Ronen Feldman	2006	Sixth International Conference on Data Mining (ICDM'06)	10.1109/ICDM.2006.82	unsupervised learning;the internet;computer science;bag-of-words model;machine learning;pattern recognition;binary relation;data mining	DB	-25.31509918726043	-66.41959822545262	41389
8b0df2837df22faca9f55de89b10f8cea9c5e618	a study on estimating the attractiveness of food photography	framing;framing food photo attractiveness;electronic mail;electronic mail estimation error conferences multimedia communication big data information science;food images food photography attractiveness estimation feature extraction color features shape features;information science;big data;food photo;multimedia communication;attractiveness;estimation error;shape recognition feature extraction image colour analysis photography;conferences	This paper proposes a method for estimating the attractiveness of food photos in order to assist a user to shoot them attractively. The proposed method extracts both color and shape features from input food images, and then integrates them according to a regression scheme. By this way, the proposed method estimates the attractiveness of an unknown food photo. We also created a food image dataset taken from various 3D-angles for each food category, and set target values of their attractiveness through subjective experiments. Then, we evaluated the performance of the proposed method in two different ways of constructing the attractiveness estimator: One that constructs it for each food category, and the other that constructs a common attractiveness estimator for all food categories. Experimental results showed the effectiveness of the proposed method in addition to the necessity for adaptively selecting the estimator depending on the appearance of foods for further performance improvement.	attractiveness principle;color;experiment	Kazuma Takahashi;Keisuke Doman;Yasutomo Kawanishi;Takatsugu Hirayama;Ichiro Ide;Daisuke Deguchi;Hiroshi Murase	2016	2016 IEEE Second International Conference on Multimedia Big Data (BigMM)	10.1109/BigMM.2016.41	computer vision;geography;multimedia;advertising	Robotics	-17.10900307720958	-52.484792499414816	41539
6b394f25d583640c1c9f8f5aeec0d27b0ed99e9f	fast supervised dimensionality reduction algorithm with applications to document categorization & retrieval	latent semantic indexing;k nearest neighbor;document categorization;experimental evaluation;support vector machine;dimensional reduction;statistical probabilistic models;text categorization;text data mining;machine learning and ir	Retriev al techniques based on dimensionalit y reduction, such as Latent Semantic Indexing (LSI), have been shown to improve the quality of the information being retrieved by capturing the latent meaning of the words present in the documents. Unfortunately, the high computational and memory requirements of LSI and its inabilit yto compute an e ective dimensionality reduction in a supervised setting limits its applicability. In this paper we present a fast supervised dimensionality reduction algorithm that is derived from the recen tly dev eloped cluster-based unsupervised dimensionality reduction algorithms. We experimentally evaluate the quality of the low er dimensional spaces both in the context of document categorization and improvements in retrieval performance on a variety of di erent document collections. Our experiments show that the lower dimensional spaces computed by our algorithm consistently improve the performance of traditional algorithms such as C4.5, k-nearestneigh bor, and Support Vector Machines (SVM), by an average of 2% to 7%. Furthermore, the supervised lower dimensional space greatly improves the retriev al performance when compared to LSI.	c4.5 algorithm;categorization;dimensionality reduction;document classification;experiment;requirement;supervised learning;support vector machine	Eui-Hong Han;George Karypis	2000		10.1145/354756.354772	support vector machine;text mining;latent semantic indexing;document clustering;boosting methods for object categorization;computer science;machine learning;pattern recognition;k-nearest neighbors algorithm;information retrieval	ML	-19.95463572693364	-64.366907511418	41551
282b7e3aa56577854e5da7edccddd9f1c80d32d5	evaluating novel features for aggressive language detection		The widespread use and abuse of social media and other platforms to voice opinions online has necessitated the development of tools to regulate this exchange of opinions in light of ethical and legal considerations. In this work, we aim to detect patterns of aggressive language to gain insight into what differentiates it from non-inflammatory language. Of particular interest are features of comments that, taken together, allow this distinction to be made automatically. To that end, we employ feature selection techniques to find optimal feature subsets.		Tina Schuh;Stephan Dreiseitl	2018		10.1007/978-3-319-99579-3_60	language identification;natural language processing;feature selection;social media;artificial intelligence;computer science	NLP	-20.98048556664538	-56.06865442668048	41607
64bb2055a5e83f6b2a106edfe9872c370d9cd62e	tf-icf: a new term weighting scheme for clustering dynamic data streams	unsupervised learning;document clustering;computers;pattern clustering;general and miscellaneous mathematics computing and information science;unsupervised document clustering;computational complexity data engineering frequency conversion machine learning software engineering laboratories computer science vectors parallel algorithms information filtering;learning;text analysis;term frequency;classification;machine learning application;term weighting scheme;dynamic data stream clustering;dynamic data;vectors;machine learning;computational complexity;term weighting;linear time;unsupervised learning computational complexity pattern clustering text analysis;term frequency inverse corpus frequency;information;unsupervised document clustering term weighting scheme dynamic data stream clustering term frequency inverse corpus frequency machine learning application	In this paper, we propose a new term weighting scheme called term frequency-inverse corpus frequency (TF-ICF). It does not require term frequency information from other documents within the document collection and thus, it enables us to generate the document vectors of N streaming documents in linear time. In the context of a machine learning application, unsupervised document clustering, we evaluated the effectiveness of the proposed approach in comparison to five widely used term weighting schemes through extensive experimentation. Our results show that TF-ICF can produce document clusters that are of comparable quality as those generated by the widely recognized term weighting schemes and it is significantly faster than those methods	archive;cluster analysis;document;dynamic data;machine learning;tf–idf;time complexity;windows firewall	Joel W. Reed;Yu Jiao;Thomas E. Potok;Brian A. Klump;Mark T. Elmore;Ali R. Hurson	2006	2006 5th International Conference on Machine Learning and Applications (ICMLA'06)	10.1109/ICMLA.2006.50	unsupervised learning;time complexity;dynamic data;document clustering;information;biological classification;computer science;machine learning;pattern recognition;data mining;computational complexity theory;tf–idf	Web+IR	-21.740047720698612	-63.66255361124028	41630
4d1b508202f160281d45a48f8bef6fad68d4cfcc	predicting stance in ideological debate with rich linguistic knowledge		Debate stance classification, the task of classifying an author's stance in a two-sided debate, is a relatively new and challenging problem in opinion mining. One of its challenges stems from the fact that it is not uncommon to find words and phrases in a debate post that are indicative of the opposing stance, owing to the frequent need for an author to re-state other people's opinions so that she can refer to and contrast with them when establishing her own arguments. We propose a machine learning approach to debate stance classification that leverages two types of rich linguistic knowledge, one exploiting contextual information and the other involving the determination of the author's stances on topics. Experimental results on debate posts involving two popular debate domains demonstrate the effectiveness of our two types of linguistic knowledge when they are combined in an integer linear programming framework. Title and Abstract in Bengali উ ত ভাষািবদ ার সাহােয ভাবাদিশক িবতেকর প িনণয় িবতেকর প িনণয় তথা একিট ি পাি ক িবতেক একজন তািকক কান প িনে ন সিট িনধারণ করা ওিপিনয়ন মাইিনং-এ একিট অেপ াকৃত নতুন এবং জিটল সম া। এে ে একিট অ তম িতব ক হেলা একজন তািকেকর লখায় ায়ই িবপে র ব ব ত শ এবং বাক াংশ পাওয়া যায় যা ঐ তািকক অ পে র যুি পুন ে খ এবং খ ডেনর মাধ েম িনজ যুি উপ াপেনর জ ব বহার কেরন। িবতেকর প িনণেয়র জ আমরা একিট মিশন লািনং প িত াব করিছ যােত ই ধরেণর উ ত ভাষািবদ া েয়াগ করা হেয়েছ, থমিট াসংিগক তথ এবং অ িট িবিভ আেলাচ িবষেয়র ে তািকেকর অিভমেতর উপর িভি কের িতি ত। িট ব ল আেলািচত িবষেয়র পে -িবপে লখা রচনার উপর চালােনা পরী ার ফলাফল ইি টজার িলিনয়ার া ািমং-এর সােথ যু াব া এই ই ধরেণর উ ত ভাষািবদ ার কাযকািরতা মাণ কের।	integer programming;linear programming;machine learning	Kazi Saidul Hasan;Vincent Ng	2012			psychology;artificial intelligence;communication;social psychology	NLP	-21.42756057278381	-67.75951718188712	41657
f4f28b7911636d0897c40b03f39bb2be633a72bf	sentiment classification in english from sentence-level annotations of emotions regarding models of affect		This paper presents a text classifier for automatically tagging the sentiment of input text according to the emotion that is being conveyed. This system has a pipelined framework composed of Natural Language Processing modules for feature extraction and a hard binary classifier for decision making between positive and negative categories. To do so, the Semeval 2007 dataset composed of sentences emotionally annotated is used for training purposes after being mapped into a model of affect. The resulting scheme stands a first step towards a complete emotion classifier for a future automatic expressive text-to-speech synthesizer.	binary classification;emotion markup language;feature extraction;natural language processing;semeval;speech synthesis;text-based user interface	Alexandre Trilla;Francesc Alías	2009			sentiment analysis;artificial intelligence;binary classification;semeval;pattern recognition;feature extraction;computer science;classifier (linguistics);sentence	NLP	-25.247615667690997	-76.42610809322774	41660
d4cc6fb1b81dd6cffde542f60b001129e3175b93	the helsinki neural machine translation system		We introduce the Helsinki Neural Machine Translation system (HNMT) and how it is applied in the news translation task at WMT 2017, where it ranked first in both the human and automatic evaluations for English–Finnish. We discuss the success of English–Finnish translations and the overall advantage of NMT over a strong SMT baseline. We also discuss our submissions for English–Latvian, English– Chinese and Chinese–English.	bleu;baseline (configuration management);chinese wall;experiment;neural machine translation;satisfiability modulo theories;text segmentation	Robert Östling;Yves Scherrer;Jörg Tiedemann;Gongbo Tang;Tommi Nieminen	2017			language technology;artificial intelligence;natural language processing;machine translation;language and communication technologies;computational linguistics;computer science	NLP	-22.908018217634677	-75.91203342178834	41703
4a1457858fb71a48e55b3175b4eb420875f6f3d3	scalable structured prediction for richly structured socio-behavioral data		Online recommender systems, content-provider sites, and social media platforms provide richly structured socio-behavioral data. However, using this noisy and incomplete data to make decisions and recommendations is challenging. It often requires complex forms of structured prediction that rely on both the logical structure in the domain and probabilistic dependencies among interlinked entities. In this talk, I will describe some common inference patterns that are useful for socio-behavioral networks and introduce probabilistic soft logic (PSL). PSL is a highly scalable open-source probabilistic programming language being developed within my group that is well-suited for structured prediction over socio-behavioral data. Finally, I will review some of our recent work using PSL for hybrid recommender systems, explanation, and fair decision making.	entity;open-source software;probabilistic soft logic;programming language;recommender system;scalability;social media;structured prediction	Lise Getoor	2018		10.1145/3240323.3267109	machine learning;data mining;recommender system;scalability;probabilistic logic;artificial intelligence;structure (mathematical logic);probabilistic programming language;social media;inference;computer science;structured prediction	AI	-12.965320725883162	-65.66439829046165	41719
020597e90b87003835395217afb0dda1013f16c8	estimation of stochastic attribute-value grammars using an informative sample	training set;informative subset;infeasibly large number;corpus show;minimal difference;available material;better estimation result;full training set;computational complexity;stochastic attribute-value;informative sample	We argue that some of the computational complexity associated with estimation of stochastic attributevalue grammars can be reduced by training upon an informative subset of the full training set. Results using the parsed Wall Street Journal corpus show that in some circumstances, it is possible to obtain better estimation results using an informative sample than when training upon all the available material. Further experimentation demonstrates that with unlexicalised models, a Gaussian prior can reduce over tting. However, when models are lexicalised and contain overlapping features, over tting does not seem to be a problem, and a Gaussian prior makes minimal di erence to performance. Our approach is applicable for situations when there are an infeasibly large number of parses in the training set, or else for when recovery of these parses from a packed representation is itself computationally expensive.	analysis of algorithms;computational complexity theory;information;parsing;test set;the wall street journal	Miles Osborne	2000			computer science;artificial intelligence;machine learning;data mining;computational complexity theory;statistics	Vision	-19.84249743696602	-77.42736125209598	41738
3ef77ffa1522cd2f8781041b654a7a3d28dd9302	grouping and aggregate queries over semantic web databases	time warp simulation data mining knowledge representation text analysis;text analysis;data mining;text knowledge representation;semantic information;time measurement humans knowledge representation speech analysis computer science knowledge engineering length measurement application software area measurement text mining;sentence similarity;time warp simulation;knowledge representation;semantic information sentence similarity dynamic time warping text knowledge representation knowledge discovery;dynamic time warping;knowledge discovery	As a growing number of applications represent data as semantic graphs like RDF (Resource Description Format) and the many entity-attribute-value formats, query languages for such data are being required to support operations beyond graph pattern matching and inference queries. Specifically the ability to express aggregate queries is an important feature which is either lacking or is implemented with little attention to the peculiarities of the data model. In this paper, we study the meaning and implementation of grouping and aggregate queries over RDF graphs. We first define grouping and aggregate operators algebraically and then show how the SPARQL query language can be extended to express grouping and aggregate queries.	aggregate data;aggregate function;computation;data model;database;mathematical optimization;pattern matching;query language;rdf schema;resource description framework;sparql;sql;semantic web;vocabulary;xslt/muenchian grouping	Dawit Yimam Seid;Sharad Mehrotra	2007	International Conference on Semantic Computing (ICSC 2007)	10.1109/ICSC.2007.91	natural language processing;knowledge representation and reasoning;text mining;computer science;artificial intelligence;dynamic time warping;data mining;knowledge extraction;information retrieval	DB	-32.37576901338248	-69.32489933137958	41758
90da46fa966b3d7385129e28e25f3b6b31aa2085	bi-directional differentiable input reconstruction for low-resource neural machine translation		We aim to better exploit the limited amounts of parallel text available in low-resource settings by introducing a differentiable reconstruction loss for neural machine translation (NMT). We reconstruct the input from sampled translations and leverage differentiable sampling and bi-directional NMT to build a compact model that can be trained end-to-end. This approach achieves small but consistent BLEU improvements on four language pairs in both translation directions, and outperforms an alternative differentiable reconstruction strategy based on hidden states.	bleu;end-to-end principle;neural machine translation;parallel text;sampling (signal processing);semi-supervised learning;semiconductor industry;softmax function;unsupervised learning	Xing Niu;Weijia Xu;Marine Carpuat	2018	CoRR		machine learning;artificial intelligence;differentiable function;machine translation;sampling (statistics);bleu;computer science;exploit	NLP	-17.92122530548415	-75.6715530334769	41816
aeb98ca502ef631a10ea31fe762e6f2a485df277	some experiments in two-dimensional grammatical inference	inference grammaticale;grammaire tableau;markov random field model;champ aleatoire markov;grammaire formelle;two dimensional space;pattern recognition;formal grammar;espace 2 dimensions;grammatical inference;array grammars;reconnaissance forme	This paper presents results of experiments conducted on a scheme for inferring two-dimensional, probabilistic Siromoney array grammars incorporating Markov random field distortion of binary images.	binary image;distortion;experiment;grammar induction;markov chain;markov random field	Gautam Biswas;Richard C. Dubes	1984	Pattern Recognition Letters	10.1016/0167-8655(84)90042-4	natural language processing;two-dimensional space;computer science;machine learning;pattern recognition;mathematics;formal grammar	Vision	-25.70319391638395	-79.49599332648233	41857
165e3aa6cab7a56fe377a146fffb929386ef8f34	designing a realistic evaluation of an end-to-end interactive question answering system		We report on the development of material for an eva luation exercise designed to assess the overall des ign and usability of HITIQA, an interactive question-answering system for preparing broad ranging reports on complex issues. The two b asic objectives of the evaluation were (1) To perform a realistic assessme nt of the usefulness and usability of HITIQA as an end-to-end system, from the information seeker’s initial questions to completio n f a draft report; and (2) To develop metrics to compare the answers obtained by different analysts and evaluate the quality of the support that HITIQA provides. We used qualitative a nd quantitative tools to obtain data about analyst’s comfort with the HITIQA system , especially its novel features such as the ability to answer complex questions and the interactive dialogue. Because of the impractica lity of measuring the quality of HITIQA output with the standard metrics of precision and recall, we developed a new task –cros s-evaluation--to indirectly measure the quality of the answers obtained using HITIQA; in this black-box assessment, analysts rate the quality of their own and their colleagues’ rep orts.	black box;end system;end-to-end principle;precision and recall;question answering;usability	Nina Wacholder;Sharon G. Small;Bing Bai;Diane Kelly;Robert Rittman;Sean Ryan;Robert Salkin;Peng Song;Ying Sun;Ting Liu;Paul B. Kantor;Tomek Strzalkowski	2004			artificial intelligence;information retrieval;precision and recall;natural language processing;usability;ranging;question answering;end-to-end principle;computer science	HCI	-30.972960774077272	-71.87908547375051	41894
4637f6e73a6095d44fd3c0efda2315131704cf0d	improved robustness of signature-based near-replica detection via lexicon randomization	duplicate detection;web pages;information filtering;memory performance;general techniques;data mining;spam filtering;deduplication;web mining;data cleaning;document similarity	Detection of near duplicate documents is an important problem in many data mining and information filtering applications. When faced with massive quantities of data, traditional duplicate detection techniques relying on direct inter-document similarity computation (e.g., using the cosine measure) are often not feasible given the time and memory performance constraints. On the other hand, fingerprint-based methods, such as I-Match, are very attractive computationally but may be brittle with respect to small changes to document content. We focus on approaches to near-replica detection that are based upon large-collection statistics and present a general technique of increasing their robustness via multiple lexicon randomization. In experiments with large web-page and spam-email datasets the proposed method is shown to consistently outperform traditional I-Match, with the relative improvement in duplicate-document recall reaching as high as 40-60%. The large gains in detection accuracy are offset by only small increases in computational requirements.	address space layout randomization;computation;data mining;email;experiment;fingerprint;information filtering system;lexicon;requirement;spamming	Aleksander Kolcz;Abdur Chowdhury;Joshua Alspector	2004		10.1145/1014052.1014127	web mining;data deduplication;computer science;machine learning;web page;data mining;world wide web;information retrieval	Web+IR	-30.19256448956751	-56.10195107586709	41987
3cf480f3f9c559fdb0a705d3f9954e484c399a2a	soccer video summarization system based on hidden markov model with multiple mpeg-7 descriptors	hidden markov model		hidden markov model;mpeg-7;markov chain	Ngoc Thanh Nguyen;Truong Cong Thang;Tae Meon Bae;Yong Man Ro	2003			automatic summarization;hidden markov model;machine learning;artificial intelligence;maximum-entropy markov model;computer science;pattern recognition	Vision	-8.859136108452994	-69.66172354787967	41992
aad03480c30c0a3d917d171d8d6b914026fe5105	affordances provide a fundamental categorization principle for visual scenes		How do we know that a kitchen is a kitchen by looking? Relatively little is known about how we conceptualize and categorize different visual environments. Traditional models of visual perception posit that scene categorization is achieved through the recognition of a scene's objects, yet these models cannot account for mounting evidence that human observers are relatively insensitive to the local details in an image. Psychologists have long theorized that the affordances, or the actionable possibilities of a stimulus are pivotal to its perception. To what extent are scene categories created from similar affordances? Using a large-­‐scale experiment using hundreds of scene categories, we show that the activities afforded by a visual scene provide a fundamental categorization principle. Affordance-­‐based similarity explained the majority of the structure in human scene categorization patterns, outperforming alternative similarities based on objects or visual features. When all these models are combined, affordances provide the majority of the predictive power in the combined model, and nearly half of the total explained variance is captured only by affordances. These results challenge many existing models of high-­‐level visual perception, and provide immediately testable hypotheses for the functional organization of the human perceptual system. Significance Statement How do we know that a kitchen is a kitchen by looking? Models of visual perception assume that scene identification is facilitated through object recognition. However, these models fail to account for observers' relative insensitivity to local image details. We explore an alternative view that posits that a scene's identity is determined by the possibilities for actions that a scene affords (its affordances). In a large-­‐scale experiment using hundreds of scene categories, we found that human scene similarity ratings were more closely related to affordance-­‐based similarity than to object or visual feature-­‐based models. Combining models revealed that nearly half of the explained variance was captured only by affordances. This work demonstrates that affordances provide a fundamental grouping principle for scenes.	categorization;color vision;outline of object recognition;scene graph;theory	Michelle R. Greene;Christopher Baldassano;Andre Esteva;Diane M. Beck;Li Fei-Fei	2014	CoRR		psychology;computer vision;communication;social psychology	Vision	-6.267727129918801	-75.68454786452098	42034
e9efff07afc300a24663fd8d55a2605668bc2d1b	mining adverse events of dietary supplements from product labels by topic modeling	dietary supplements;natural language processing;pharmacovigilance	The adverse events of the dietary supplements should be subject to scrutiny due to their growing clinical application and consumption among U.S. adults. An effective method for mining and grouping the adverse events of the dietary supplements is to evaluate product labeling for the rapidly increasing number of new products available in the market. In this study, the adverse events information was extracted from the product labels stored in the Dietary Supplement Label Data-base (DSLD) and analyzed by topic modeling techniques, specifically Latent Dirichlet Allocation (LDA). Among the 50 topics generated by LDA, eight topics were manually evaluated, with topic relatedness ranging from 58.8% to 100% on the product level, and 57.1% to 100% on the ingredient level. Five out of these eight topics were coherent groupings of the dietary supplements based on their adverse events. The results demonstrated that LDA is able to group supplements with similar adverse events based on the dietary supplement labels. Such information can be potentially used by consumers to more safely use dietary supplements.	adverse event;adverse reaction to drug;anterior descending branch of left coronary artery;coherence (physics);dietary supplements;effective method;extraction;latent dirichlet allocation;topic model	Yefeng Wang;Divya R. Gunashekar;Terrence Adam;Rui Zhang	2017		10.3233/978-1-61499-830-3-614	risk analysis (engineering);topic model;pharmacovigilance;environmental science;adverse effect	Web+IR	-21.56137185847146	-56.048241242334655	42051
6652e374c621b62de9bd251ceea5e764880dd9ba	a confidence-based hierarchical feature clustering algorithm for text classification	databases;p2p system;data sharing;pattern clustering;information systems;information security;resource management;p2p;text analysis;family physician;protection;medical services;healthcare system;feature extraction;resource sharing;merging;management information systems;authorization;classification algorithms clustering algorithms text categorization feature extraction merging clustering methods pervasive computing text mining software libraries gain measurement;healthcare information system;privacy;policy management;data security	In this paper, we propose a novel feature reduction ap- proach to group words hierarchically into clusters which can then be used as new features for document classifica- tion. Initially, each word constitutes a cluster. We calculate the mutual confidence between any two different words. The pair of clusters containing the two words with the highest mutual confidence are combined into a new cluster. This process of merging is iterated until all the mutual confi- dences between the un-processed pair of words are smaller than a predefined threshold or only one cluster exists. In this way, a hierarchy of word clusters is obtained. The user can decide the clusters, from a certain level, to be used as new features for document classification. Experimental re- sults have shown that our method can perform better than other methods.	algorithm;cluster analysis;computer cluster;document classification;iteration	Jung-Yi Jiang;Kai-Tai Yin;Shie-Jue Lee	2007	The 2007 International Conference on Intelligent Pervasive Computing (IPC 2007)	10.1109/IPC.2007.35	knowledge management;data mining;database;business	DB	-10.27658713758404	-60.386981062132605	42067
b20eaf26e7cc4ebfbb70c74fca5a30c9f0c3eade	incremental bayesian category learning from natural language	cognitive science;incrementality;bayesian statistics;learning;classification;natural language;bayesian models;models;monte carlo methods;categorization	Models of category learning have been extensively studied in cognitive science and primarily tested on perceptual abstractions or artificial stimuli. In this paper, we focus on categories acquired from natural language stimuli, that is, words (e.g., chair is a member of the furniture category). We present a Bayesian model that, unlike previous work, learns both categories and their features in a single process. We model category induction as two interrelated subproblems: (a) the acquisition of features that discriminate among categories, and (b) the grouping of concepts into categories based on those features. Our model learns categories incrementally using particle filters, a sequential Monte Carlo method commonly used for approximate probabilistic inference that sequentially integrates newly observed data and can be viewed as a plausible mechanism for human learning. Experimental results show that our incremental learner obtains meaningful categories which yield a closer fit to behavioral data compared to related models while at the same time acquiring features which characterize the learned categories. (An earlier version of this work was published in Frermann and Lapata .).		Lea Frermann;Mirella Lapata	2016	Cognitive science	10.1111/cogs.12304	natural language processing;variable-order bayesian network;biological classification;computer science;machine learning;pattern recognition;linguistics;natural language;bayesian statistics;statistics;monte carlo method;categorization	AI	-7.8923626029423986	-74.14458717838562	42083
aca0c805ed09ae219bbe6deba304bf16745fa4ff	automatic detection of lexicalised phrases in swedish		I wiIrpresent a system under development, called LP-DETECT. The system detects and analyses Swedish lexicalised phrases (LPs) in order to enhance subsequent parsing. LPs are one of a number of stumbling blocks related to word sequences that must be dealt with when parsing unrestricted text. LPs include semantic idioms, syntactic idioms and morphological idioms and so called valency breaking LPs. The system reported on consists of an LP lexicon of some 8000 LPs with analyses, a detection program written in perl and rules for disambiguating between and discarding LP analyses. A small evaluation of the system is also presented.	lexicon;parsing;perl;programming idiom;stumbleupon;syntactic predicate	Janne Lindberg	1999			computer science	NLP	-27.789214331847134	-78.08395257369109	42093
ea0aee68be756e9540bb01a2bd80fb6ee826c5d2	hpi question answering system in the bioasq 2015 challenge		I describe my participation on the 2015 edition of the BioASQ challenge in which I submitted results for the concept matching, document retrieval, passage retrieval, exact answer and ideal answer subtasks. My approach relies on a in-memory based database (IMDB) and its built-in text analysis features, as well as on PubMed for retrieving relevant citations, and on predefined ontologies and terminologies necessary for matching concepts to the questions. Although results are far below the ones obtained by other groups, I present an novel approach for answer extraction based on sentiment analysis.	canonical account;document retrieval;hardware platform interface;in-memory database;internet movie database (imdb);ontology (information science);pubmed;question answering;sentiment analysis	Mariana L. Neves	2015			computer science;data science;data mining;information retrieval	AI	-29.629443475022438	-64.8569084572528	42104
19a50fa877c9fb90d889bd502d5e674494b15b85	towards generating colour terms for referents in photographs: prefer the expected or the unexpected?		Colour terms have been a prime phenomenon for studying language grounding, though previous work focussed mostly on descriptions of simple objects or colour swatches. This paper investigates whether colour terms can be learned from more realistic and potentially noisy visual inputs, using a corpus of referring expressions to objects represented as regions in real-world images. We obtain promising results from combining a classifier that grounds colour terms in visual input with a recalibration model that adjusts probability distributions over colour terms according to contextual and object-specific preferences.	boolean expression;commonsense knowledge (artificial intelligence);regular expression;statistical classification;text corpus	Sina Zarrieß;David Schlangen	2016			natural language processing;artificial intelligence;computer science	NLP	-13.6050356021636	-69.4180027444817	42136
c84b2cb9c76d8c377094d465b57b35110ff0246d	automatic long sentence segmentation for neural machine translation	long sentence;bleu point;target clause;nist chinese-english translation task	Neural machine translation (NMT) is an emerging machine translation paradigm that translates texts with an encoder-decoder neural architecture. Very recent studies find that translation quality drops significantly when NMT translates long sentences. In this paper, we propose a novel method to deal with this issue by segmenting long sentences into several clauses. We introduce a split and reordering model to collectively detect the optimal sequence of segmentation points for a long source sentence. Each segmented clause is translated by the NMT system independently into a target clause. The translated target clauses are then concatenated without reordering to form the final translation for the long sentence. On NIST Chinese-English translation tasks, our segmentation method achieves a substantial improvement of 2.94 BLEU points over the NMT baseline on translating long sentences with more than 30 words, and 5.43 BLEU points on sentences of over 40 words.	neural machine translation	Shaohui Kuang;Deyi Xiong	2016		10.1007/978-3-319-50496-4_14	natural language processing;speech recognition;pattern recognition;rule-based machine translation	NLP	-19.26034990536729	-75.89245682029285	42151
4d788ed9ee2fdbc60a539c4d3ca8428d545e87fa	adaptive co-training svm for sentiment classification on tweets	sentiment classification;multiclass svm;会议论文;semi supervised learning;topic adaptive;co training;tweet sentiment	Sentiment classification is an important problem in tweets mining. There lack labeled data and rating mechanism for generating them in Twitter service. And topics in Twitter are more diverse while sentiment classifiers always dedicate themselves to a specific domain or topic. Thus it is a challenge to make sentiment classification adaptive to diverse topics without sufficient labeled data. Therefore we formally propose an adaptive multiclass SVM model which transfers an initial common sentiment classifier to a topic-adaptive one. To tackle the tweet sparsity, non-text features are explored besides the conventional text features, which are intuitively split into two views. An iterative algorithm is proposed for solving this model by alternating among three steps: optimization, unlabeled data selection and adaptive feature expansion steps. The algorithm alternatively minimizes the margins of two independent objectives on different views to learn coefficient matrices, which are collaboratively used for unlabeled tweets selection from the topic that the algorithm is adapting to. And then topic-adaptive sentiment words are expended based on the above selection, in turn to help the first two steps find more confident and unlabeled tweets and boost the final performance. Comparing with the well-known supervised sentiment classifiers and semi-supervised approaches, our algorithm achieves promising increases in accuracy averagely on the 6 topics from public tweet corpus.	algorithm;co-training;coefficient;iterative method;linear classifier;mathematical optimization;semi-supervised learning;semiconductor industry;sentiment analysis;sparse matrix;statistical classification;whole earth 'lectronic link	Shenghua Liu;Fuxin Li;Fangtao Li;Xueqi Cheng;Huawei Shen	2013		10.1145/2505515.2505569	semi-supervised learning;computer science;machine learning;pattern recognition;data mining;sentiment analysis	NLP	-19.74138719654913	-66.04862943279453	42285
1324cbbc7937ec56b13d91525fa4e1f3769bfc10	swim: a prototype environment for visual media retrieval	video retrieval;video browsing	This paper presents our work in developing a system, SWIM (Show What I Mean), an prototyping environment for development of content-based image and video retrieval tools. A variety of features for representing image and video content implemented in SWIM has been presented. A set of functional modules of SWIM with unique features, including visual query formation, video parsing and abstraction, image and video retrieval, and video browsing, has been presented with example of their applications.		HongJiang Zhang	1995		10.1007/3-540-60793-5_106	computer vision;computer science;multimedia;computer graphics (images)	Web+IR	-15.144525702080365	-55.42827912493172	42292
4f1686ae2b42bb01c950f5d72cc147515abe4560	image retrieval in the unstructured data management system audr	video retrieval content based retrieval search engines text analysis;search engines;image retrieval feature extraction visualization data models indexes engines computer architecture;video retrieval;text analysis;computer architecture;indexes;visualization;engines;feature extraction;imageclef medical dataset image data multimodal retrieval unstructured data management system image management uniform data model audr subengine function video data audio data text data visual features imagenet dataset;content based retrieval;data models;image retrieval	The explosive growth of image data leads to severe challenges to the traditional image retrieval methods. In order to manage massive images more accurate and efficient, this paper firstly proposes a scalable architecture for image retrieval based on a uniform data model and makes this function a sub-engine of AUDR, an advanced unstructured data management system, which can simultaneously manage several kinds of unstructured data including image, video, audio and text. The paper then proposes a new image retrieval algorithm, which incorporates rich visual features and two text models for multi-modal retrieval. Experiments on both ImageNet dataset and ImageCLEF medical dataset show that our proposed architecture and the new retrieval algorithm are appropriate for efficient management of massive image.	algorithm;data hub;data model;image retrieval;imagenet;modal logic;scalability;text mining	Junwu Luo;Bo Lang;Chao Tian;Danchen Zhang	2012	2012 IEEE 8th International Conference on E-Science	10.1109/eScience.2012.6404474	document retrieval;visual word;image retrieval;computer science;concept search;data mining;automatic image annotation;world wide web;data retrieval;information retrieval;human–computer information retrieval	Vision	-14.67764642441888	-56.642797389470324	42313
ac1301aea12705ea55c27a931be965b03521d1d2	a clinical text classification paradigm using weak supervision and deep representation	clinical text classification;electronic health records;machine learning;natural language processing;weak supervision	BACKGROUND Automatic clinical text classification is a natural language processing (NLP) technology that unlocks information embedded in clinical narratives. Machine learning approaches have been shown to be effective for clinical text classification tasks. However, a successful machine learning model usually requires extensive human efforts to create labeled training data and conduct feature engineering. In this study, we propose a clinical text classification paradigm using weak supervision and deep representation to reduce these human efforts.   METHODS We develop a rule-based NLP algorithm to automatically generate labels for the training data, and then use the pre-trained word embeddings as deep representation features for training machine learning models. Since machine learning is trained on labels generated by the automatic NLP algorithm, this training process is called weak supervision. We evaluat the paradigm effectiveness on two institutional case studies at Mayo Clinic: smoking status classification and proximal femur (hip) fracture classification, and one case study using a public dataset: the i2b2 2006 smoking status classification shared task. We test four widely used machine learning models, namely, Support Vector Machine (SVM), Random Forest (RF), Multilayer Perceptron Neural Networks (MLPNN), and Convolutional Neural Networks (CNN), using this paradigm. Precision, recall, and F1 score are used as metrics to evaluate performance.   RESULTS CNN achieves the best performance in both institutional tasks (F1 score: 0.92 for Mayo Clinic smoking status classification and 0.97 for fracture classification). We show that word embeddings significantly outperform tf-idf and topic modeling features in the paradigm, and that CNN captures additional patterns from the weak supervision compared to the rule-based NLP algorithms. We also observe two drawbacks of the proposed paradigm that CNN is more sensitive to the size of training data, and that the proposed paradigm might not be effective for complex multiclass classification tasks.   CONCLUSION The proposed clinical text classification paradigm could reduce human efforts of labeled training data creation and feature engineering for applying machine learning to clinical text classification by leveraging weak supervision and deep representation. The experimental experiments have validated the effectiveness of paradigm by two institutional and one shared clinical text classification tasks.		Yanshan Wang;Sunghwan Sohn;Sijia Liu;Feichen Shen;Liwei Wang;Elizabeth J. Atkinson;Shreyasee Amin;Hongfang Liu	2019		10.1186/s12911-018-0723-6		AI	-19.75366296727184	-70.78299230147786	42327
756462aff77a451792778a3f6e3ebc09bcee6c1a	domain adapted word embeddings for improved sentiment classification		Supplemental material provides details about word tokens, embedding dimensions and hyperparameter details. 1 Dimensions of CCA and KCCA projections. Using both KCCA and CCA, generic embeddings and DS embeddings are projected onto their d largest correlated dimensions. By construction, d ≤ min (d1, d2). The best d for each data set is obtained via 10 fold cross validation on the sentiment classification task. Table 2 provides dimensions of all word embeddings considered. Note that for LSA and DA, average word embedding dimension across all four data sets are reported. Generic word embeddings such as GloVe and word2vec are of fixed dimensions across all four data sets. 2 Kernel parameter estimation. Parameter σ of the Gaussian kernel used in KCCA is obtained empirically from the data. The median (μ) of pairwise distances between data points mapped by the kernel function is used to determine σ. Typically σ = μ or σ = 2μ. In this section both values are considered for σ and results with the best performing σ are reported. 3 Word tokens and word embeddings dimensions Data Set Word Tokens Yelp 2049 Amazon 1865 IMDB 3075		Prathusha K. Sarma;Yingyu Liang;William A. Sethares	2018			computer science;machine learning;artificial intelligence;kernel (linear algebra);encoding (memory);nonlinear system;sentence;canonical correlation	NLP	-17.874468574345098	-77.72078088748945	42375
4586c41d3807fccd7902b2fc05755276e7683c78	evaluating supervised semantic parsing methods on application-independent data		While supervised statistical semantic parsing methods have received a good amount of attention in recent years, this research has largely been done on small and specialized data sets. This paper introduces a work-in-progress with the objective of examining the applicability of supervised statistical semantic parsing to application-independent data with linguistically motivated meaning representations. The approach discussed in this paper has three key aspects: The circumvention of data scarcity using automatic annotation, experimentation with different types of meaning representations, and the design of a suitable graded evaluation measure.	parsing	Sebastian Beschke	2013		10.1007/978-3-662-44116-9_2	programming language	NLP	-28.05631837984346	-71.7529741175005	42376
07d880be170afdca2ca9a7f58553806631f0ff4c	index structures of user profiles for efficient web page filtering services	user information needs;information resources;user needs;web pages;search engines;information retrieval;distributed processing;information filtering;index structure;information needs;user information needs index structures user profiles web page filtering services information filtering web pages indexing;web page filtering services;data mining;metasearch;user profile;internet;keyword search;indexing;indexation;signature;user profiles;world wide web;web page;web pages information filtering information filters search engines metasearch world wide web data mining keyword search distributed processing computer science;index structures;computer science;information need;information filters;information resources indexing information needs search engines information retrieval internet	Contact author Abstract Searching information from the WWW efficiently and effectively has become a very important issue. In this paper, we apply the information filtering concept to finding good matches of web pages for what the users need. Furthermore, we propose four new methods for indexing the users’ information needs and show their efficiency in comparison with two well-known approaches.	information filtering system;information needs;www;web page	Yi-Hung Wu;Arbee L. P. Chen	2000		10.1109/ICDCS.2000.840981	information needs;static web page;user modeling;metasearch engine;computer science;information filtering system;web page;database;world wide web;information retrieval	Web+IR	-29.882045520546708	-52.76085967871783	42408
b04f2c8411944df83cf495d434a6be8994457f70	integrating hard and soft information sources for d2d using controlled natural language	information resources;groupware;sensor systems;political alliances;english language;access;information systems;surveillance;information retrieval;text analysis;speech;data mining;symposia;roads;unstructured text information sources soft information sources hard information sources controlled natural language coalition data to decision chains sensor based sources intelligence surveillance and reconnaissance isr assets sensing assets controlled english ce native english speaker information representation unambiguous form d2d tasks sharing collaborating users coalition environment automatic information extraction semistructured text information sources;data structures;natural language;vehicles cameras roads sensor systems ontologies data mining;control;ontologies;vehicles;intelligence;reconnaissance;natural language processing;cameras;text analysis data structures groupware information resources information retrieval natural language processing surveillance	We introduce an approach to integrating access to hard and soft information sources to provide better exploitation of all available sources in the context of coalition data-to-decision (D2D) chains. In terms of hard (sensor-based) sources we show how intelligence, surveillance, and reconnaissance (ISR) assets can be represented at a relatively high level in controlled natural language, and how this allows the automatic assignment of sensing assets to D2D tasks. We demonstrate how the use of Controlled English (CE) - a type of controlled natural language designed to be readable by a native English speaker whilst representing information in a structured, unambiguous form - supports the informed sharing of D2D tasks and assets between collaborating users in a coalition environment. Moreover, we show how CE can be used in the automatic extraction of information from unstructured and semi-structured text information sources, providing us with a uniform way to integrate these soft sources with the aforementioned hard sources.	compute node linux;controlled natural language;design rationale;formal language;high-level programming language;human-readable medium;information systems research;resource description framework;semiconductor industry;structured text;tracing (software);web ontology language;while;xml	Alun D. Preece;Diego Pizzocaro;Dave Braines;David H. Mott;Geeth de Mel;Tien Pham	2012	2012 15th International Conference on Information Fusion		computer science;data mining;communication;information retrieval	NLP	-32.91337842591842	-71.31825604160959	42435
49bd64704934dedbfddabec530ec18bdb254d2f1	using a chinese lexicon to learn sense embeddings and measure semantic similarity		Word embeddings have recently been widely used to model words in Natural Language Processing (NLP) tasks including semantic similarity measurement. However, word embeddings are not able to capture polysemy, because a polysemous word is represented by a single vector. To address this problem, learning multiple embedding vectors for different senses of a word is necessary and intuitive. We present a novel approach based on a Chinese lexicon to learn sense embeddings. Every sense is represented by a vector that consists of semantic contributions made by senses explaining it. To make full use of the lexicon’s advantages and address its drawbacks, we perform representation expansion to make sparse embedding vectors dense and disambiguate in gloss polysemous words by semantic contribution allocation. Thanks to the use of an intuitive way of noise filtering, we achieve noticeable improvement both in dimensionality reduction and semantic similarity measurement. We perform experiments on a translated version of Miller-Charles dataset and report state-of-the-art performance on semantic similarity measurement. We also apply our approach to SemEval-2012 Task4: Evaluating Chinese Word Similarity, which uses a translated version of wordsim353 as the standard dataset, and our approach also noticeably outperforms conventional approaches.	lexicon;semantic similarity	Zhuo Zhen;Yuquan Chen	2018		10.1007/978-3-030-01716-3_17	semantic similarity;gloss (annotation);filter (signal processing);dimensionality reduction;polysemy;embedding;lexicon;artificial intelligence;pattern recognition;computer science	NLP	-20.71383148005513	-73.0105292501138	42440
04c73bed777524d85683bca728c03234d85f21d7	towards context-sensitive domain ontology extraction	intelligent information retrieval context sensitive domain ontology extraction lexico syntactic approach statistical learning approach;ontologies data mining humans computational efficiency context surges learning systems large scale systems statistical learning information retrieval;empirical study;information retrieval;contextual information;ontologies artificial intelligence;large scale;statistical learning;real world application;learning methods;statistical analysis;statistical analysis information retrieval learning artificial intelligence ontologies artificial intelligence;learning artificial intelligence;computational efficiency;domain ontology;extraction method	Although there has been a surge of interest in applying domain ontologies to facilitate communications among computers and human users, engineering of these ontologies turns out to be very labor intensive and time consuming. Recently, some learning methods have been proposed for automatic or semi-automatic extraction of ontologies. Nevertheless, the accuracy and computational efficiency of these methods should be improved to support large scale ontology extraction for real-world applications. This paper illustrates a novel domain ontology extraction method. In particular, contextual information of the knowledge sources is exploited for the extraction of high quality domain ontologies. By combining lexico-syntactic and statistical learning approaches, the accuracy and the computational efficiency of the extraction process can be improved. Empirical studies have confirmed that the proposed method can extract reliable domain ontology to improve the performance of information retrieval and facilitate human users to discover and refine domain ontology	computation;computer;display resolution;e-commerce;experiment;information retrieval;information system;knowledge management;lexico;machine learning;ontology (information science);ontology engineering;ontology learning;refinement (computing);semantic web;semiconductor industry;web ontology language	Raymond Y. K. Lau;Jin-Xing Hao;Maolin Tang;Xujuan Zhou	2007	2007 40th Annual Hawaii International Conference on System Sciences (HICSS'07)	10.1109/HICSS.2007.570	natural language processing;idef5;computer science;data mining;ontology-based data integration;empirical research;information retrieval;process ontology	AI	-31.697964557945014	-67.34763522008475	42454
4ed371f23a8e80f72ee0c3fa92dbe04975111997	the best trail algorithm for assisted navigation of web sites	information retrieval;hypermedia;computational complexity;web sites;tree searching;web site best trail algorithm hypertext navigation problem probabilistic best first expansion filtering algorithm potential gain metric scoring method;data structure;navigation filtering algorithms motion planning hypertext systems computer science information systems educational institutions gain measurement web sites topology;tree searching web sites hypermedia computational complexity	We present an algorithm called the Best Trail Algorithm, which helps solve the hypertext navigation problem by automating the construction of memex-like trails through the corpus. The algorithm performs a probabilistic best-first expansion of a set of navigation trees to find relevant and compact trails. We describe the implementation of the algorithm, scoring methods for trails, filtering algorithms and a new metric called potential gain which measures the potential of a page for future navigation opportunities.	algorithm;hypertext;memex;motion planning	Richard Wheeldon;Mark Levene	2003		10.1109/LAWEB.2003.1250294	data structure;computer science;theoretical computer science;machine learning;programming language;computational complexity theory;world wide web;information retrieval;algorithm	ML	-30.39540013284234	-56.01776067144942	42461
451682765b26edbdfb1785c5b7b2eadd94118707	assessing violent group culpability and classifications using open-source intelligence	computational method;pattern classification government data processing;subnational group violence violent group culpability open source intelligence secret intelligence subnational violent group computational method;open source software terrorism information resources computational intelligence ontologies computer science blogs collaboration government bayesian methods;pattern classification;government data processing;open source	Because traditional secret intelligence can be difficult to obtain regarding subnational violent groups, open-source intelligence is considered a potentially valuable source of information. Open-source intelligence, obtained from news reports, blogs, and other publically available resources, is a rich, but noisy source of information about violent group activities. In an era during which collaborations across departments, agencies, or governments bring together experts with diverse training, resources, and objectives, developing methods for aggregating and objectively interpreting such sources of information is essential. In this paper, we explore the capability of combining open-source intelligence with computational methods to solve two problems relevant to understanding and responding to subnational group violence.	blog;information source;open-source intelligence;open-source software;regular expression	Derek Ruths	2010	2010 International Symposium on Collaborative Technologies and Systems	10.1109/CTS.2010.5478469	intelligence cycle;artificial intelligence;data mining;management;law;computer security	AI	-20.875811912697856	-55.96799307357283	42476
c5daad3dc84a09cf247ab1ac8009473a5128fa86	what happens next? future subevent prediction using contextual hierarchical lstm	event prediction;lstm;subevent sequence	Events are typically composed of a sequence of subevents. Predicting a future subevent of an event is of great importance for many real-world applications. Most previous work on event prediction relied on hand-crafted features and can only predict events that already exist in the training data. In this paper, we develop an end-to-end model which directly takes the texts describing previous subevents as input and automatically generates a short text describing a possible future subevent. Our model captures the two-level sequential structure of a subevent sequence, namely, the word sequence for each subevent and the temporal order of subevents. In addition, our model incorporates the topics of the past subevents to make context-aware prediction of future subevents. Extensive experiments on a real-world dataset demonstrate the superiority of our model over several state-of-the-art methods.	ch c/c++ interpreter;end-to-end principle;experiment;language model;long short-term memory;n-gram	Linmei Hu;Juan-Zi Li;Liqiang Nie;Xiaoli Li;Chao Shao	2017			computer science;data science;machine learning;data mining;long short term memory	AI	-16.622622829160424	-72.2167247526685	42484
f5c67f250d99817470a9f5d42e2f34c48e88b70b	improving opinionated blog retrieval effectiveness with quality measures and temporal features	impact;information retrieval;blog;blogger;influence;search;ranking;post;opinionated retrieval	The massive acceptance and usage of the blog communities by a significant portion of the Web users has rendered knowledge extraction from blogs a particularly important research field. One of the most interesting related problems is the issue of the opinionated retrieval, that is, the retrieval of blog entries which contain opinions about a topic. There has been a remarkable amount of work towards the improvement of the effectiveness of the opinion retrieval systems. The primary objective of these systems is to retrieve blog posts which are both relevant to a given query and contain opinions, and generate a ranked list of the retrieved documents according to the relevance and opinion scores. Although a wide variety of effective opinion retrieval methods have been proposed, to the best of our knowledge, none of them takes into consideration the issue of the importance of the retrieved opinions. In this work we introduce a ranking model which combines the existing retrieval strategies with query-independent information to enhance the ranking of the opinionated documents. More specifically, our model accounts for the influence of the blogger who authored an opinion, the reputation of the blog site which published a specific blog post, and the impact of the post itself. Furthermore, we expand the current proximity-based opinion scoring strategies by considering the physical locations of the query and opinion terms within a document. We conduct extensive experiments with the TREC Blogs08 dataset which demonstrate that the application of our methods enhances retrieval precision by a significant margin.	blog;blogger;document;experiment;information retrieval;lexicon;purchasing;query throughput;relevance;software quality assurance;synchronous backplane interconnect;text retrieval conference;world wide web	Leonidas Akritidis;Panayiotis Bozanis	2013	World Wide Web	10.1007/s11280-013-0237-1	ranking;ranking;computer science;data mining;impact;world wide web;information retrieval	Web+IR	-27.257199691080302	-52.70811460839343	42485
c9c19da153c4d681762d3b0ef0d4b4d271435b41	syntactic dependency-based n-grams: more evidence of usefulness in classification	sn grams;syntactic n grams;authorship attribution task;svm classifier;syntactic paths	The paper introduces and discusses a concept of syntactic n-grams (sn-grams) that can be applied instead of traditional n-grams in many NLP tasks. Sn-grams are constructed by following paths in syntactic trees, so sngrams allow bringing syntactic knowledge into machine learning methods. Still, previous parsing is necessary for their construction. We applied sn-grams in the task of authorship attribution for corpora of three and seven authors with very promising results.	grams;machine learning;n-gram;natural language processing;parsing;stylometry;text corpus	Grigori Sidorov;Francisco Velasquez;Efstathios Stamatatos;Alexander F. Gelbukh;Liliana Chanona-Hernández	2013		10.1007/978-3-642-37247-6_2	natural language processing;computer science;syntactic predicate;pattern recognition;linguistics	NLP	-25.279477010861726	-72.75750204413478	42498
a789de57751003ad89e6e9cea63f86881b4112bc	an sla corpus annotated with pedagogically relevant grammatical structures		The evaluation of a language learner’s proficiency in second language is a task that normally involves comparing the learner’s production with a learning framework of the target language. One of the most well known frameworks is the Common European Framework for Languages (CEFR), which addresses language learning in general and is broadly used in the European Union, while serving as reference in countries outside the EU as well. In this study, we automatically annotated a corpus of texts produced by language learners with pedagogically relevant grammatical structures and observed how these structures are being employed by learners from different proficiency levels. We analyzed the use of structures both in terms of evolution along the levels and in terms of level in which the structures are used the most. The annotated resource, SGATe, presents a rich source of information for teachers that wish to compare the production of their students with those of already certified language learners.	compiler;information source;service-level agreement;text corpus	Leonardo Zilio;Rodrigo Wilkens;Cédrick Fairon	2018			artificial intelligence;natural language processing;speech recognition;computer science	NLP	-29.54253852459	-74.78028879534328	42512
bf0f5ce95791542fb80a787de26102be857c2db5	a dialogue annotation scheme for weight management chat using the trans-theoretical model of health behavior change		In this study we collect and annotate human-human role-play dialogues in the domain of weight management. There are two roles in the conversation: the “seeker” who is looking for ways to lose weight and the “helper” who provides suggestions to help the “seeker” in their weight loss journey. The chat dialogues collected are then annotated with a novel annotation scheme inspired by a popular health behavior change theory called “trans-theoretical model of health behavior change”. We also build classifiers to automatically predict the annotation labels used in our corpus. We find that classification accuracy improves when oracle segmentations of the interlocutors’ sentences are provided compared to directly classifying unsegmented sentences.	amazon mechanical turk;crowdsourcing;dialog system;experiment;interaction;oracle nosql db;source-to-source compiler;sparse matrix;system on a chip;the third manifesto;theory	Ramesh R. Manuvinakurike;Sumanth Bharadwaj;Kallirroi Georgila	2018	CoRR		artificial intelligence;machine learning;conversation;computer science;oracle;natural language processing;behavior change;annotation	NLP	-20.95803827646083	-68.74334473070297	42532
4b53f660eb6cfe9180f9e609ad94df8606724a3d	text mining of news-headlines for forex market prediction: a multi-layer dimension reduction algorithm with semantics and sentiment	forex prediction;market sentiment analysis;market prediction;news mining;news semantic analysis	In this paper a novel approach is proposed to predict intraday directional-movements of a currency-pair in the foreign exchange market based on the text of breaking financial news-headlines. The motivation behind this work is twofold: First, although market-prediction through text-mining is shown to be a promising area of work in the literature, the text-mining approaches utilized in it at this stage are not much beyond basic ones as it is still an emerging field. This work is an effort to put more emphasis on the text-mining methods and tackle some specific aspects thereof that are weak in previous works, namely: the problem of high dimensionality as well as the problem of ignoring sentiment and semantics in dealing with textual language. This research assumes that addressing these aspects of text-mining have an impact on the quality of the achieved results. The proposed system proves this assumption to be right. The second part of the motivation is to research a specific market, namely, the foreign exchange market, which seems not to have been researched in the previous works based on predictive text-mining. Therefore, results of this work also successfully demonstrate a predictive relationship between this specific market-type and the textual data of news. Besides the above two main components of the motivation, there are other specific aspects that make the setup of the proposed system and the conducted experiment unique, for example, the use of news article-headlines only and not news article-bodies, which enables usage of short pieces of text rather than long ones; or the use of general financial breaking news without any further filtration. In order to accomplish the above, this work produces a multi-layer algorithm that tackles each of the mentioned aspects of the text-mining problem at a designated layer. The first layer is termed the Semantic Abstraction Layer and addresses the problem of co-reference in text mining that is contributing to sparsity. Co-reference occurs when two or more words in a text corpus refer to the same concept. This work produces a custom approach by the name of Heuristic-Hypernyms Feature-Selection which creates a way to recognize words with the same parent-word to be regarded as one entity. As a result, prediction accuracy increases significantly at this layer which is attributed to appropriate noise-reduction from the feature-space. The second layer is termed Sentiment Integration Layer, which integrates sentiment analysis capability into the algorithm by proposing a sentiment weight by the name of SumScore that reflects investors’ sentiment. Additionally, this layer reduces the dimensions by eliminating those that are of zero value in terms of sentiment and thereby improves prediction accuracy. The third layer encompasses a dynamic model creation algorithm, termed Synchronous Targeted Feature Reduction (STFR). It is suitable for the challenge at hand whereby the mining of a stream of text is concerned. It updates the models with the most recent information available and, more importantly, it ensures that the dimensions are reduced to the absolute minimum. The algorithm and each of its layers are extensively evaluated using real market data and news content across multiple years and have proven to be solid and superior to any other comparable solution. The proposed techniques implemented in the system, result in significantly high directional-accuracies of up to 83.33%. On top of a well-rounded multifaceted algorithm, this work contributes a much needed research framework for this context with a test-bed of data that must make future research endeavors more convenient. The produced algorithm is scalable and its modular design allows improvement in each of its layers in future research. This paper provides ample details to reproduce the entire system and the conducted experiments. 2014 Elsevier Ltd. All rights reserved. http://dx.doi.org/10.1016/j.eswa.2014.08.004 0957-4174/ 2014 Elsevier Ltd. All rights reserved. ⇑ Corresponding author. E-mail address: armankhnt@gmail.com (A. Khadjeh Nassirtoussi). Expert Systems with Applications 42 (2015) 306–324	abstraction layer;algorithm;dimensionality reduction;experiment;expert system;feature selection;foreign exchange service (telecommunications);heuristic;layer (electronics);mathematical model;mechatronics;modular design;predictive text;scalability;semantic data model;sentiment analysis;sparse matrix;testbed;text corpus;text mining	Arman Khadjeh Nassirtoussi;Saeed Reza Aghabozorgi;Ying Wah Teh;David Chek Ling Ngo	2015	Expert Syst. Appl.	10.1016/j.eswa.2014.08.004	simulation;computer science;artificial intelligence;machine learning;data mining;mathematics	AI	-17.814425295304012	-71.23972598735338	42558
a0c01e01f11fe30e22240e7e7883df580d5bc68c	the clef 2003 cross language image retrieval task			image retrieval	Paul D. Clough;Mark Sanderson	2003			image retrieval;information retrieval;clef;computer science	Vision	-30.16966812301829	-63.21603415151152	42569
6700a09bf8dffd4d8807ebd471545e3ed0ef1ef6	affective music information retrieval		Much of the appeal of music lies in its power to convey emotions/moods and to evoke them in listeners. In consequence, the past decade witnessed a growing interest in modeling emotions from musical signals in the music information retrieval (MIR) community. In this article, we present a novel generative approach to music emotion modeling, with a specific focus on the valence-arousal (VA) dimension model of emotion. The presented generative model, called acoustic emotion Gaussians (AEG), better accounts for the subjectivity of emotion perception by the use of probability distributions. Specifically, it learns from the emotion annotations of multiple subjects a Gaussian mixture model in the VA space with prior constraints on the corresponding acoustic features of the training music pieces. Such a computational framework is technically sound, capable of learning in an online fashion, and thus applicable to a variety of applications, including user-independent (general) and user-dependent (personalized) emotion recognition and emotion-based music retrieval. We report evaluations of the aforementioned applications of AEG on a largerscale emotion-annotated corpora, AMG1608, to demonstrate the effectiveness of AEG and to showcase how evaluations are conducted for research on emotion-based MIR. Directions of future work are also discussed. 1 ar X iv :1 50 2. 05 13 1v 1 [ cs .I R ] 1 8 Fe b 20 15	acoustic cryptanalysis;acoustic model;computation;emotion recognition;field electron emission;generative model;information retrieval;mixture model;modern valence bond theory;personalization;text corpus	Ju-Chiang Wang;Yi-Hsuan Yang;Hsin-Min Wang	2017		10.1007/978-3-319-31413-6_12	speech recognition;multimedia;music and emotion	Web+IR	-12.811073408426319	-72.85686916074634	42604
1e9be812b295e98e8ef3add76a57674aabea72e1	hybrid recommender system of biomedical ontologies	hybrid recommender system;biomedical ontology;web service semantic	This paper presents a semantic personalized recommender system for biomedical ontologies. To do this, we design and implement a semantic repository of biomedical ontologies, containing metadata associated with each biomedical ontology. The knowledge stored in the metadata of the semantic repository is used for the recommender system, in order to give prioritized recommendations of the different biomedical ontologies that meet certain search criteria. The proposed recommender also considers hybridization, considering the calibration of the impact generated by two aspects: customization (adaptability) for each user, and the quality evaluation of each ontology to recommend.	ontology (information science);personalization;recommender system;web search engine	José Aguilar;Junior Altamiranda;Omar Portilla	2016	2016 XLII Latin American Computing Conference (CLEI)	10.1109/CLEI.2016.7833392	upper ontology;open biomedical ontologies;computer science;database;world wide web;information retrieval	Web+IR	-29.336732021226272	-58.82929721687181	42649
988856232ba4ac92b8ca40461039ca7d784765e3	overview of the inex 2010 xml mining track: clustering and classification of xml documents	inex;wikipedia;xml document mining;classification;content;custering;structure	This report explains the objectives, datasets and evaluation criteria of both the clustering and classification tasks set in the INEX 2010 XML Mining track. The report also describes the approaches and results obtained by participants.	algorithm;cluster analysis;data mining;information retrieval;interdependence;machine learning;xml	Christopher M. De Vries;Richi Nayak;Sangeetha Kutty;Shlomo Geva;Andrea Tagarelli	2010		10.1007/978-3-642-23577-1_35	computer science;data mining;database;information retrieval	Web+IR	-27.06418218442341	-61.11797052222199	42667
0e9f4d5d3d4efba1aeca069ec27d613ec02ddee0	a web service search engine for large-scale web service discovery based on the probabilistic topic modeling and clustering		With the ever increasing number of Web services, discovering an appropriate Web service requested by users has become a vital yet challenging task. We need a scalable and efficient search engine to deal with the large volume of Web services. The aim of this approach is to provide an efficient search engine that can retrieve the most relevant Web services in a short time. The proposed Web service search engine (WSSE) is based on the probabilistic topic modeling and clustering techniques that are integrated to support each other by discovering the semantic meaning of Web services and reducing the search space. The latent Dirichlet allocation (LDA) is used to extract topics from Web service descriptions. These topics are used to group similar Web services together. Each Web service description is represented as a topic vector, so the topic model is an efficient technique to reduce the dimensionality of word vectors and to discover the semantic meaning that is hidden in Web service descriptions. Also, the Web service description is represented as a word vector to address the drawbacks of the keyword-based search system. The accuracy of the proposed WSSE is compared with the keyword-based search system. Also, the precision and recall metrics are used to evaluate the performance of the proposed approach and the keyword-based search system. The results show that the proposed WSSE based on LDA and clustering outperforms the keyword-based search system.	cluster analysis;experiment;k-means clustering;latent dirichlet allocation;precision and recall;scalability;semantic matching;service discovery;tf–idf;topic model;ws-security;web search engine;web service;word embedding;world wide web	Afnan Bukhari;Xumin Liu	2018	Service Oriented Computing and Applications	10.1007/s11761-018-0232-6	latent dirichlet allocation;computer science;data mining;topic model;distributed computing;probabilistic logic;cluster analysis;web service;search engine;scalability;curse of dimensionality	Web+IR	-28.302571842951448	-57.901554993036356	42678
dd23b6366b06fea40bdf3921c9b3193faf84fe2b	a knowledge graph-based content selection model for data-driven text generation		Content selection is a critical task for natural language generation. A novel approach based on knowledge graph is proposed. Structure data is mapping to the graph and combined with user defined knowledge. The model analyses the content selection features on the graph, and automatically learns the content selection rules. The model was evaluated in the domain of weather forecasting.	knowledge graph;natural language generation	Jun-Peng Gong;Juan Cao;Peng-Zhou Zhang	2017	IJRIS	10.1504/IJRIS.2017.10011231	artificial intelligence;machine learning;computer science;natural language generation;data-driven;graph	NLP	-27.772280121586775	-64.05170555419615	42736
b849600aaeb0645be7fb8295afd531af5502962f	incorporating prepositional phrase classification knowledge in prepositional phrase identification		This paper proposes a method of prepositional phrase (PP) identification by incorporating PP classification knowledge. When PPs act as different syntactic constituents, they have different characteristics in terms of location and context. In this paper, PPs are classified based on the context in which they appear. We select features based on the category of PPs to train multiple machine learning models for PP identification, and recombine these identification results. In this way, we can make full use of the complementary advantage of multiple models.		Qiaoli Zhou;Ling Zhang;Na Ye;Dongfeng Cai	2015		10.1007/978-3-319-27194-1_57	natural language processing;prepositional pronoun;noun phrase;determiner phrase;linguistics	NLP	-24.860246131070586	-72.30849924379153	42738
22358c1e6f371db45a0d237baff6052e0a50e498	implicit distortion and fertility models for attention-based encoder-decoder nmt model		Neural machine translation has shown very promising results lately. Most NMT models follow the encoder-decoder framework. To make encoder-decoder models more flexible, attention mechanism was introduced to machine translation and also other tasks like speech recognition and image captioning. We observe that the quality of translation by attention-based encoder-decoder can be significantly damaged when the alignment is incorrect. We attribute these problems to the lack of distortion and fertility models. Aiming to resolve these problems, we propose new variations of attention-based encoderdecoder and compare them with other models on machine translation. Our proposed method achieved an improvement of 2 BLEU points over the original attentionbased encoder-decoder.	bleu;distortion;encoder;end-to-end principle;neural machine translation;speech recognition;trix (operating system)	Shi Feng;Shujie Liu;Mu Li;Ming Zhou	2016	CoRR		speech recognition;artificial intelligence;machine learning	NLP	-18.71840912779971	-75.58195362040416	42743
307e36662defe8e8903008afd8492fcede8ccb90	combining lexical, syntactic, and semantic features with maximum entropy models for information extraction		Extracting semantic relationships between entities is challenging because of a paucity of annotated data and the errors induced by entity detection modules. We employ Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text. Our system obtained competitive results in the Automatic Content Extraction (ACE) evaluation. Here we present our general approach and describe our ACE results.	ace;automatic differentiation;entity;information extraction;principle of maximum entropy	Nanda Kambhatla	2004			principle of maximum entropy;artificial intelligence;natural language processing;information extraction;syntax;computer science	NLP	-25.147122178571927	-70.6617335108088	42754
553fa529ba615e4bddea81e9a231ae19d5a870a4	estimating upper and lower bounds on the performance of word-sense disambiguation programs	lower bound;upper and lower bounds;upper bound	We have recently reported on two new word-sense disambiguation systems, one trained on bilingual material (the Canadian Hansards) and the other trained on monolingual material (Roget's Thesaurus and Grolier's Encyclopedia). After using both the monolingual and bilingual classifiers for a few months, we have convinced ourselves that the performance is remarkably good. Nevertheless, we would really like to be able to make a stronger statement, and therefore, we decided to try to develop some more objective evaluation measures. Although there has been a fair amount of literature on sense-disambiguation, the literature does not offer much guidance in how we might establish the success or failure of a proposed solution such as the two systems mentioned in the previous paragraph. Many papers avoid quantitative evaluations altogether, because it is so difficult to come up with credible estimates of performance. This paper will attempt to establish upper and lower bounds on the level of performance that can be expected in an evaluation. An estimate of the lower bound of 75% (averaged over ambiguous types) is obtained by measuring the performance produced by a baseline system that ignores context and simply assigns the most likely sense in all cases. An estimate of the upper bound is obtained by assuming that our ability to measure performance is largely limited by our ability obtain reliable judgments from human informants. Not surprisingly, the upper bound is very dependent on the instructions given to the judges. Jorgensen, for example, suspected that lexicographers tend to depend too much on judgments by a single informant and found considerable variation over judgments (only 68% agreement), as she had suspected. In our own experiments, we have set out to find word-sense disambiguation tasks where the judges can agree often enough so that we could show that they were outperforming the baseline system. Under quite different conditions, we have found 96.8% agreement over judges.	approximation algorithm;baseline (configuration management);computation;computational linguistics;computer performance;definition;design of experiments;dictionary;dynamic range;expect;experiment;judgment (mathematical logic);lexicography;microsoft word for mac;parallel text;part-of-speech tagging;protologism;roget's thesaurus;text corpus;word sense;word-sense disambiguation	William A. Gale;Kenneth Ward Church;David Yarowsky	1992			speech recognition;artificial intelligence;upper and lower bounds	NLP	-26.592910841544676	-73.95085892747053	42756
3f3d62d6e3ac629581a00d95cef90f865de27941	a re-ranking model for dependency parsing with knowledge graph embeddings	paints;knowledge graph embedding dependency parsing re ranking;artificial neural networks;natural language sentences re ranking model dependency parsing dependencies semantic plausibilities dependencies syntactic plausibilities parse tree knowledge graph embedding penn treebank corpus;pipelines;artificial neural networks paints pipelines;trees mathematics grammars natural language processing probability	Re-ranking models of parse trees have been focused on re-ordering parse trees with a syntactic view. However, also a semantic view should be considered in re-ranking parse trees, because the fact that a word pair has a dependency implies that the pair has both syntactic and semantic relations. This paper proposes a re-ranking model for dependency parsing based on a combination of syntactic and semantic plausibilities of dependencies. The syntactic probability is used as a syntactic plausibility of a parse tree, and a knowledge graph embedding is adopted to represent its semantic plausibility. The knowledge graph embedding allows the semantic plausibility of parse trees to be expressed effectively with ease. The experiments on the standard Penn Treebank corpus prove that the proposed model improves the base parser regardless of the number of candidate parse trees.	dependency relation;experiment;graph embedding;knowledge graph;parse tree;parsing;plausibility structure;stochastic context-free grammar;treebank	A.-Yeong Kim;Hyun-Je Song;Seong-Bae Park;Sang-Jo Lee	2015	2015 International Conference on Asian Language Processing (IALP)	10.1109/IALP.2015.7451560	natural language processing;computer science;machine learning;pattern recognition;pipeline transport;linguistics;top-down parsing;artificial neural network	NLP	-20.310038132290817	-74.02975403076219	42764
c52fb925b1a158f066e92a2802cb4c3cc011ced3	identification of tweets that mention books: an experimental comparison of machine learning methods	book title identification;named entity recognition on twitter;machine learning;japanese text classification	"""In this paper, we address the task of the identification of tweets on Twitter that mention books TMB among tweets that contain the same strings as full book titles. Although this task can be treated as a kind of Named Entity Recognition, the fact that book titles consist of ordinary expressions such as """"The Girl on the Train"""" makes the task harder. Furthermore, if tweets are gathered through a dictionary-based search, the tweets that contain the same strings as full book titles are often spam. However, assuming a complete list of book titles i.e. from a union catalogue from a library or commercial bibliographic data from a book store, this task can be solved by text classification. Thus, we proposed a two-step pipeline consisting of spam filtering and TMB classification based on supervised learning with a small amount of labelled data. We constructed optimal classifiers by comparing combinations of four proven supervised learning methods with different features. Given the difficulty of the task, our pipeline performed highly about 0.7 in terms of F-score."""	machine learning	Shuntaro Yada;Kyo Kageura	2015		10.1007/978-3-319-27974-9_30	computer science;data science;data mining;world wide web	AI	-23.970837328762713	-66.56903759169359	42765
ce38060be5a5b709b9c09b20f22bb414a0e28ef0	a new improved term weighting scheme for text categorization		In text categorization, term weighting is the task to assign weights to terms during the document presentation phase. Thus, it affects the classification performance. In this paper, we propose a new term weighting scheme logtf.rf max . It is an improvement to tf.rf − one of the most effective term weighting schemes to date. We conducted experiments to compare the new term weighting scheme to tf.rf and others on common text categorization benchmark data sets. The experimental results show that logtf.rf max consistently outperforms tf.rf as well as other schemes. Furthermore, our new scheme is simpler than tf.rf.	categorization;document classification	Nguyen Pham Xuan;Hieu Le Quang	2013		10.1007/978-3-319-02741-8_23	pattern recognition;information retrieval	NLP	-20.844348333093315	-63.64008713132889	42771
a2d1e70f65150ff68801352212d47a6d8e34b981	what we really want to find by sentiment analysis: the relationship between computational models and psychological state		As the first step to model emotional state of a person, we build sentiment analysis models with existing deep neural network algorithms and compare the models with psychological measurements to enlighten the relationship. In the experiments, we first examined psychological state of 64 participants and asked them to summarize the story of a book, Chronicle of a Death Foretold (Márquez, 1981). Secondly, we trained models using crawled 365,802 movie review data; then we evaluated participants’ summaries using the pretrained model as a concept of transfer learning. With the background that emotion affects on memories, we investigated the relationship between the evaluation score of the summaries from computational models and the examined psychological measurements. The result shows that although CNN performed the best among other deep neural network algorithms (LSTM, GRU), its results are not related to the psychological state. Rather, GRU shows more explainable results depending on the psychological state. The contribution of this paper can be summarized as follows: (1) we enlighten the relationship between computational models and psychological measurements. (2) we suggest this framework as objective methods to evaluate the emotion; the real sentiment analysis of a person.	algorithm;artificial neural network;computation;computational model;deep learning;experiment;long short-term memory;mental state;sentiment analysis	Hwiyeol Jo;Soo-Min Kim;Jeong Am Ryu	2017	CoRR		artificial intelligence;machine learning;natural language processing;transfer of learning;sentiment analysis;artificial neural network;computational model;computer science	NLP	-12.889267368369905	-71.10207958137384	42802
63b38cf5a080e70e1118575083a8c2e207af138b	an expansion and reranking approach for annotation-based image retrieval from web	web pages;reranking;document expansion;information retrieval;indexing terms;wordnet;query expansion;image retrieval	In this paper, we introduce an expansion and reranking approach for annotation based image retrieval from Web pages. Our suggestion considers an image retrieval system using the surrounding texts nearby the image in a Web page as annotations. However, annotations may include too much and uninformative text such as copyright notice, date, author. In order to choose indexing terms effectively, we propose a term selection approach, which first expands the document using WordNet, and then selects descriptive terms among them. Notably, we applied this term selection methodology to both document and query. This is because applying either of documents or query does not help to increase retrieval performance. On the other hand, term selection process increases the number of terms per documents, and both documents and queries become more exhaustive than original. Consequently, this results high recall with low precision in retrieval. Thus, we also proposed a two-level reranking approach. In order to evaluate our approaches we have participated ImageCLEF2009 WikipediaMM subtask. The results we obtained are superior to any participating approaches and our approach has obtained the best four ranks, in text-only image retrieval. The results also showed that document expansion and effective term selection to annotations plays an important role in text-based image retrieval.	image retrieval	Deniz Kilinç;Adil Alpkocak	2011	Expert Syst. Appl.	10.1016/j.eswa.2011.04.118	wordnet;query expansion;visual word;index term;image retrieval;computer science;web page;data mining;term discrimination;automatic image annotation;world wide web;vector space model;information retrieval	Vision	-30.932824449258167	-58.78966285615247	42824
84d265f518b41212943940b03c8183a077a7c5d6	extracting domain knowledge by complex networks analysis of wikipedia entries	internet encyclopedias electronic publishing computer languages complex networks communities;network structure complex networks analysis wikipedia entries domain knowledge extraction centrality measures;web sites	In this paper we describe a complex networks analysis of Wikipedia. We construct 10 different networks from Wikipedia entries (articles) related to the chosen domain. The goal of the experiment is to extract domain knowledge in terms of identifying entries that are centrally positioned and entries that are strongly related. We apply complex networks analysis on all acquired networks and examine the networks' structure. We employ centrality measures in order to find centrally positioned entries in the network. Furthermore we identify communities and find which entries are densely connected according to the network structure.	centrality;complex network;wikipedia	Neven Matas;Sanda Martincic-Ipsic;Ana Mestrovic	2015	2015 38th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)	10.1109/MIPRO.2015.7160531	computer science;data science;data mining;world wide web	ML	-25.921505696345477	-57.80386768701287	42828
746730018f1bf607a24e2d0d161b4df47920bbaa	moving code-switching research toward more empirically grounded methods		As our world becomes more globalized and interconnected, the boundaries between languages become increasingly blurred (Bullock, Hinrichs & Toribio, 2014). But to what degree? To date, researchers have no objective way to measure the frequency and extent to which languages might be mixed. While Natural Language Processing (NLP) tools process monolingual texts with very high accuracy, they perform poorly when multiple languages are involved. In this paper, we offer an automated language identification system and intuitive metrics–the Integration, Burstiness, and Memory indices–that allow us to characterize how corpora are mixed.	language identification;natural language processing;our world;text corpus	Gualberto A. Guzmán;Joseph Ricard;Jacqueline Serigos;Barbara Bullock;Almeida Jacqueline Toribio	2017			code-switching;theoretical computer science;computer science	NLP	-28.146544892115845	-73.89889104816677	42836
7a9c5758a5f1118bf8e1abf8f2668735b6017fec	fa3l at semeval-2017 task 3: a three embeddings recurrent neural network for question answering			question answering;recurrent neural network	Giuseppe Attardi;Antonio Carta;Federico Errica;Andrea Madotto;Ludovica Pannitto	2017		10.18653/v1/S17-2048	natural language processing;machine learning;computer science;artificial intelligence;question answering;deep learning;semeval;recurrent neural network	NLP	-19.65488803538093	-71.56487359764024	42848
3e35ee5e30473a65915db942ae20b2b3400a7db6	representations for multi-document event clustering	text mining;clustering;probabilistic content models	We study several techniques for representing, fusing and comparing content representations of news documents. As underlying models we consider the vector space model (both in a term setting and in a latent semantic analysis setting) and probabilistic topic models based on latent Dirichlet allocation. Content terms can be classified as topical terms or named entities, yielding several models for content fusion and comparison. All used methods are completely unsupervised. We find that simple methods can still outperform the current state-of-the-art techniques.	cluster analysis;computation;cosine similarity;high- and low-level;language model;latent dirichlet allocation;latent semantic analysis;linear algebra;linear discriminant analysis;local-density approximation;named entity;perplexity;point of view (computer hardware company);requirement;similarity measure;time complexity;unsupervised learning	Wim De Smet;Marie-Francine Moens	2012	Data Mining and Knowledge Discovery	10.1007/s10618-012-0270-1	text mining;computer science;machine learning;pattern recognition;data mining;cluster analysis;probabilistic latent semantic analysis	Web+IR	-23.571888209591425	-68.68042763019977	42852
eea3b22b97589eba1925b653034acbafffa1e3e0	quantitative aspects of pdtb-style discourse relations across languages		Frequency distribution of words, syntax and semantics in many languages abides by certain laws. However, because of the shortage of discourse corpora, few studies have examined whether the frequency of discourse relations follows some distributional patterns. Although there is some research based on the Rhetorical Structure Theory discourse treebank (RST-DT), each of these studies is limited to a single language. Otherwise to the RST-DT, the Penn Discourse Treebank (PDTB), adopting another annotation system, has had an enormous influence on the study of discourse structure and discourse annotation. Discourse corpora in other languages, such as Chinese, Hindi, Turkish, Czech and Arabic have been annotated following PDTB style. With the data from these discourse treebanks, we find that the rank-frequency of discourse relations follow the same pattern and that these languages share significant similarities in using semantic relations to organize the discourse. It is evidenced in our research that humans assume the relationship between two consecutive sentences is a causal connection or expansion link for fewer connectives used, but the relation of contrast is the most marked by connectives. This research will be of significance for understanding the homogeneity of discourse structure across languages.		Kun Sun;Lili Zhang	2018	Journal of Quantitative Linguistics	10.1080/09296174.2017.1390934	treebank;linguistics;natural language processing;turkish;artificial intelligence;hindi;syntax;czech;semantics;rhetorical structure theory;computer science;annotation	NLP	-27.84063869815822	-75.03067305730065	42873
de87bc678bd6a3f500e22cf114b1f786a48a7abc	a novel information search and recommendation services platform based on an indexing network (short paper)	internet users recommendation services platform internet technology information resources heterogeneity redundancy keyword matching web pages semantic association graph cloud distributed systems sugon tongji cloud platform indexing network model interaction interface system demonstration valuable information search related services;portals;search engines;information retrieval;semantics;data mining;web sites cloud computing indexing information retrieval portals recommender systems redundancy search engines;navigation;internet;redundancy;indexing;semantics navigation educational institutions internet indexing data mining;web sites;recommender systems;cloud computing	With the rapid development of Internet technology, information resources on the Internet become more abundant, but also bring some problems like diversity, heterogeneity, disorder, and redundancy. Given a brief expression like search keywords only, users' needs are ambiguous. Therefore, current technologies of search applications relying on direct keyword matching cannot meet the requirements of users exactly. Service applications are hoped to be more intelligent and knowledgeable. To solve such challenge, this paper attempts to organize web pages into a semantic association graph based on a novel model - indexing network, which can provide more valuable information and services for users. An information search and recommendation services platform is implemented using cloud distributed systems (Hadoop + Habse + Zookeeper) based on Sugon-Tongji cloud platform that is located at Tongji University. 70 million web pages are crawled on the Internet and their corresponding indexing network model are constructed. Several novel services are implemented and provided on the platform, such as search location, search navigation, category/keyword recommendation, and the interaction interface of the indexing network. System demonstration and experimental analysis show that the proposed platform can provide and support more knowledgeable and valuable information search-related services, thereby better meeting the growing needs of Internet users.	ambiguous grammar;apache hadoop;distributed computing;google cloud platform;internet;inverted index;network model;prototype;requirement;web page	Xiaodong Deng;Ming Jiang;Haichun Sun;Yangjie Zhang;Junjun Liu;Yu Guo;Xin Wang;Dajie Ge;Pengwei Wang;Zhijun Ding;Hongzhong Chen	2013	2013 IEEE 6th International Conference on Service-Oriented Computing and Applications	10.1109/SOCA.2013.62	search engine indexing;navigation;the internet;cloud computing;computer science;data mining;database;semantics;redundancy;world wide web;information retrieval	DB	-29.965073789876175	-55.44569733704677	42910
77e5768a4ac9cd3076e9f970c912058604944fb8	extraction of semantic relation based on feature vector from wikipedia	parse tree;relation extraction;feature based;dependency tree;k nearest neighbor;support vector machine	In this paper, we propose a feature vector to extract semantic relations using dependency tree and parse tree. We exploit relation descriptions from infoboxes on Wikipedia documents. The features include part-of-speech, phrase label in dependency tree, and grammatical structure of phrase label, path of phrase label inherent in parse tree. In our experi ments, support vector machine and k-nearest neighbor are applied to extract relations from Wikipedia documents.	feature vector;ontology components;wikipedia	Duc-Thuan Vo;Cheol-Young Ock	2012		10.1007/978-3-642-32695-0_78	natural language processing;relationship extraction;support vector machine;computer science;machine learning;pattern recognition;k-nearest neighbors algorithm	NLP	-25.628290297530523	-66.07922104889867	42924
ecabf74904c90504dc9d554d4f0f82cb39ad10e3	a method of chinese coreference resolution combined multi-features in discourse	coreferential forms;decision tree;semantics training machine learning decision trees support vector machine classification chromium;rule based;training;crf;semantics;text analysis;text analysis natural language processing;linguistic rules based approaches;multi features;machine learning;chromium;chinese coreference resolution;support vector machine classification;f measure;ubiquitous language phenomenon;chinese oriented text discourse;coreference resolution;decision trees;ner;natural language processing;linguistic rules based approaches chinese coreference resolution ubiquitous language phenomenon natural language processing coreferential forms f measure chinese oriented text discourse;decision tree coreference resolution multi features ner crf	Coreference that is a kind of ubiquitous language phenomenon makes the topic more highlighted and the narration more concise and coherent in discourse. Conversely, it leads to ambiguity in Natural Language Processing as well. Coreference resolution is the process that eliminates the indeterminacy caused by coreferential forms. To improve the current system, a method of coreference resolution combined with multi-features, mainly including clause's or full-sentence's distance, semantic class, and shorten-form features, is proposed in this paper. Experiments show that those features are valuable and have certain effects on the performance of resolution. It can be verified by both precision and F-measure in Chinese-oriented text discourse. In addition, a novelty point absorbed somewhat domain ontological idea in our previous work and embodied further in this paper is rather than usual manual linguistic rules-based approaches for processing the resolution.	coherence (physics);computer science;domain-driven design;entity;experiment;f1 score;information system;natural language processing;natural language understanding;nondeterministic algorithm;ontology (information science);resolution (logic)	Shumin Shi;Heyan Huang;Rui-Yang Chen	2010	2010 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2010.5580883	natural language processing;coreference;computer science;machine learning;decision tree;pattern recognition;semantics	NLP	-26.016736524355398	-70.81667066458	42980
be0a8f9758b64eab30a26dab93560a7c124afcd5	a unified model for opinion target extraction and target sentiment prediction		Target-based sentiment analysis involves opinion target extraction and target sentiment classification. However, most of the existing works usually studied one of these two sub-tasks alone, which hinders their practical use. This paper aims to solve the complete task of target-based sentiment analysis in an end-to-end fashion, and presents a novel unified model which applies a unified tagging scheme. Our framework involves two stacked recurrent neural networks: The upper one predicts the unified tags to produce the final output results of the primary target-based sentiment analysis; The lower one performs an auxiliary target boundary prediction aiming at guiding the upper network to improve the performance of the primary task. To explore the inter-task dependency, we propose to explicitly model the constrained transitions from target boundaries to target sentiment polarities. We also propose to maintain the sentiment consistency within an opinion target via a gate mechanism which models the relation between the features for the current word and the previous word. We conduct extensive experiments on three benchmark datasets and our framework achieves consistently superior results.	air traffic control radar beacon system;artificial neural network;benchmark (computing);end-to-end principle;experiment;recurrent neural network;sentiment analysis;unified model	Xin Li;Lidong Bing;Piji Li;Wai Lam	2018	CoRR			NLP	-18.395793808483393	-71.84716965259618	43033
1479169a5095d8b8daed5d8ba7c8bc308577b370	chinese subjectivity detection using a sentiment density-based naive bayesian classifier	opinion mining;subjectivity classification;subjectivity detection;bayes methods;training;opinion summarization systems;bayesian methods;naive bayesian classifier;data mining;sentiment classifiers;subjectivity classification chinese subjectivity detection naive bayesian classifier opinion mining systems sentiment classifiers opinion summarization systems chinese subjectivity classification chi square technique sentiment density subintervals;chinese subjectivity detection;training data;sentiment density subintervals;machine learning;feature extraction;classification algorithms;sentiment density subinterval subjectivity detection sentiment density naive bayesian classifier;pattern classification;sentiment density;bayesian methods training data classification algorithms data mining feature extraction training machine learning;opinion mining systems;chi square technique;chinese subjectivity classification;sentiment density subinterval;pattern classification bayes methods data mining	Subjectivity detection plays an important role in many opinion mining systems such as sentiment classifiers and opinion summarization systems. In this paper we present a sentiment density-based naive Bayesian classifier for Chinese subjectivity classification. In this study, we first employ the chi-square technique to automatically extract subjective cues from training data. To represent sentence subjectivity, we calculate sentiment density using the extracted subjective cues and thus construct a set of sentiment density subintervals. Finally, we implement a naive Bayesian classifier with sentiment density subintervals as features for subjectivity classification. We also conduct several experiments on the NTCIR-6 Chinese opinion data, showing the feasibility of the proposed method.	bayesian network;experiment;naive bayes classifier;sentence boundary disambiguation;sentiment analysis	Xin Wang;Guohong Fu	2010	2010 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2010.5580700	statistical classification;training set;feature extraction;bayesian probability;computer science;machine learning;pattern recognition;data mining;sentiment analysis	ML	-22.605054861998276	-65.86319294967127	43075
cb7bbb65dd470c6120b12bb5d9c7e63080be48e6	extracting biomolecular interactions using semantic parsing of biomedical text		We advance the state of the art in biomolecular interaction extraction with three contributions: (i) We show that deep, Abstract Meaning Representations (AMR) significantly improve the accuracy of a biomolecular interaction extraction system when compared to a baseline that relies solely on surfaceand syntax-based features; (ii) In contrast with previous approaches that infer relations on a sentence-by-sentence basis, we expand our framework to enable consistent predictions over sets of sentences (documents); (iii) We further modify and expand a graph kernel learning framework to enable concurrent exploitation of automatically induced AMR (semantic) and dependency structure (syntactic) representations. Our experiments show that our approach yields interaction extraction systems that are more robust in environments where there is a significant mismatch between training and test conditions.	adaptive multi-rate audio codec;algorithm;baseline (configuration management);big mechanism;design rationale;experiment;glossary of computer graphics;graph kernel;interaction;parsing;ver (command)	Sahil Garg;Aram Galstyan;Ulf Hermjakob;Daniel Marcu	2016			natural language processing;computer science;machine learning	NLP	-19.52109447932305	-74.35330719770754	43079
7cf0bca03691889b48a7a2a61aabbebd783443fd	affirmative cue words in task-oriented dialogue	contextual information;computational method;spoken dialogue system;american english	We present a series of studies of affirmative cue words—a family of cue words such as “okay” or “alright” that speakers use frequently in conversation. These words pose a challenge for spoken dialogue systems because of their ambiguity: They may be used for agreeing with what the interlocutor has said, indicating continued attention, or for cueing the start of a new topic, among other meanings. We describe differences in the acoustic/prosodic realization of such functions in a corpus of spontaneous, task-oriented dialogues in Standard American English. These results are important both for interpretation and for production in spoken language applications. We also assess the predictive power of computational methods for the automatic disambiguation of these words. We find that contextual information and final intonation figure as the most salient cues to automatic disambiguation.	acoustic cryptanalysis;dialog system;spontaneous order;text corpus;word-sense disambiguation	Agustín Gravano;Julia Hirschberg;Stefan Benus	2011	Computational Linguistics	10.1162/COLI_a_00083	natural language processing;linguistics	NLP	-12.318184510464244	-80.09158429761952	43080
32abe927a5c8d8a6056d15160423700ff03d2279	learning language using genetic algorithms	language use;statistical method;conference contribution;genetic algorithm;computer science;grammar induction	"""Strict pattern-based methods of grammar induction are often frustrated by the apparently inexhaustible variety of novel word combinations in large corpora. Statistical methods ooer a possible solution by allowing frequent well-formed expressions to overwhelm the infrequent ungrammatical ones. They also have the desirable property of being able to construct robust grammars from positive instances alone. Unfortunately , the \zero-frequency"""" problem entails assigning a small probability to all possible word patterns, thus ungrammatical n-grams become as probable as unseen grammatical ones. Further, such grammars are unable to take advantage of inherent lexical properties that should allow infrequent words to inherit the syntactic properties of the class to which they belong. This paper describes a genetic algorithm (GA) that adapts a population of hypothesis grammars towards a more eeective model of language structure. The GA is statistically sensitive in that the utility of frequent patterns is reinforced by the persistence of eecient substructures. It also supports the view of language learning as a \bootstrapping problem,"""" a learning domain where it appears necessary to simultaneously discover a set of categories and a set of rules deened over them. Results from a number of tests indicate that the GA is a robust, fault-tolerant method for inferring grammars from positive examples of natural language."""	fault tolerance;genetic algorithm;grammar induction;grams;n-gram;natural language;persistence (computer science);robustness (computer science);sensitivity and specificity;software release life cycle;text corpus;well-formed element	Tony C. Smith;Ian H. Witten	1995		10.1007/3-540-60925-3_43	grammar systems theory;natural language processing;language identification;genetic algorithm;grammar induction;computer science;machine learning	AI	-10.908366750441772	-74.63127794548225	43141
d7238c2d244eb15b44d6afb85f16e5307ce4b260	computational model of speech understanding	connectionist models;syntactic parsing;human computer interfaces;computer model;semantic mapping;syntactic analysis;speech understanding;natural language processing;human computer interface;spoken language understanding	This paper proposes a speech comprehension computational model based on neurocognitive researches. The computational representation uses techniques as wavelets transform and connectionist models. The speech signal codification and data prosodic extraction are derived from wavelet coefficients. Moreover, the connectionist models are used to perform syntactic parsing and prosodic-semantic mapping. Thus, the computational model applies three approaches: the application of wavelet coefficients as input in connectionist language analysis, the use of SARDSRNRAAM system to syntactic analysis as well as the proposition of prosodic-semantic maps to language contexts definition.	coefficient;computation;computational model;connectionism;map;natural language processing;parsing;semantic mapper;simulation;speech recognition;wavelet transform	Daniel Nehme Müller;Philippe Olivier Alexandre Navaux	2006			natural language processing;trace;speech recognition;computer science;linguistics	NLP	-30.35600067642415	-79.83358057727057	43146
c8b0e78cc8c018b2f3de72d02d8ac460d10cca5c	normalization of dutch user-generated content	languages and literatures	This paper describes a phrase-based machine translation approach to normalize Dutch user-generated content (UGC). We compiled a corpus of three different social media genres (text messages, message board posts and tweets) to have a sample of this recent domain. We describe the various characteristics of this noisy text material and explain how it has been manually normalized using newly developed guidelines. For the automatic normalization task we focus on text messages, and find that a cascaded SMT system where a token-based module is followed by a translation at the character level gives the best word error rate reduction. After these initial experiments, we investigate the system’s robustness on the complete domain of UGC by testing it on the other two social media genres, and find that the cascaded approach performs best on these genres as well. To our knowledge, we deliver the first proof-of-concept system for Dutch UGC normalization, which can serve as a baseline for future work.	baseline (configuration management);compiler;experiment;machine translation;noisy text;social media;text corpus;user-generated content;word error rate	Orphée De Clercq;Sarah Schulz;Bart Desmet;Els Lefever;Véronique Hoste	2013			natural language processing;computer science;information retrieval	NLP	-22.968800150306425	-74.1125304351891	43170
af5bc40544983bddd425e8e89e14af3562cc02ba	interpolative self-training approach for sentiment analysis	interpolation;supervised learning;training;prediction algorithms;sentiment analysis;algorithm design and analysis;semisupervised learning	Sentiment analysis has become one of the fundamental research areas with an objective of estimating the polarity of text documents. While sentiment analysis requires rich training resources, the number of available labeled documents is limited. The proposed interpolative self-training model is an extension of self-training as one of the most common semi-supervised learning algorithms. The proposed method is based on enlarging learning documents by interpolating data in both the training and the test phase. The method also includes a weighting strategy for data selection in each iteration. The method is evaluated using four Twitter datasets for the task of sentiment analysis. The results indicate that the proposed self-training model successfully outperforms the baseline and the standard self-training approach.	algorithm;baseline (configuration management);interpolation;iteration;machine learning;semi-supervised learning;semiconductor industry;sentiment analysis;supervised learning	Somayyeh Aghababaei;Masoud Makrehchi	2016	2016 International Conference on Behavioral, Economic and Socio-cultural Computing (BESC)	10.1109/BESC.2016.7804475	computer science;machine learning;pattern recognition;data mining	ML	-20.170504566526763	-66.22002127426893	43178
96072306758d9964d12d885339b7c03ec97de37b	fst based morphological analyzer for hindi language		Hindi being a highly inflectional language, FST (Finite State Transducer) based approach is most efficient for developing a morphological analyzer for this language. The work presented in this paper uses the SFST (Stuttgart Finite State Transducer) tool for generating the FST. A lexicon of root words is created. Rules are then added for generating inflectional and derivational words from these root words. The Morph Analyzer developed was used in a Part Of Speech (POS) Tagger based on Stanford POS Tagger. The system was first trained using a manually tagged corpus and MAXENT (Maximum Entropy) approach of Stanford POS tagger was then used for tagging input sentences. The morphological analyzer gives approximately 97% correct results. POS tagger gives an accuracy of approximately 87% for the sentences that have the words known to the trained model file, and 80% accuracy for the sentences that have the words unknown to the trained model file.	alloy analyzer;brill tagger;finite-state transducer;lexicon;morphological pattern;part-of-speech tagging;principle of maximum entropy	Deepak Kumar;Manjeet Singh;Seema Shukla	2012	CoRR		natural language processing;speech recognition;computer science;linguistics	NLP	-25.212189004507316	-77.77750126162233	43194
2e0038ec86913810c138262debe7227791214bb8	ambiguity correction method for free-form annotation in xml documents	forma libre;anotacion;marking;information structure;fiabilidad;reliability;donnee textuelle;structure information;dato textual;xml language;free form;estructura informacion;annotation;forme libre;marcacion;fiabilite;textual data;xml document;reperage;ambiguity;ambiguedad;langage xml;lenguaje xml;ambiguite	In this paper, current dependencybased treebanks are introduced and analyzed. The methods used for building the resources, the annotation schemes applied, and the tools used (such as POS taggers, parsers and annotation software) are discussed.	alloy analyzer;comment (computer programming);parsing;point of sale;spell checker;treebank;xml	Tuomo Kakkonen	2003		10.1007/3-540-45036-X_19	xml;computer science;database;programming language;world wide web;information retrieval	NLP	-27.872105610756332	-77.85632735984683	43201
09efde3bd0a380e8cbcd55a13694648276c2c166	customized image narrative generation via interactive visual question generation and answering		Image description task has been invariably examined in a static manner with qualitative presumptions held to be universally applicable, regardless of the scope or target of the description. In practice, however, different viewers may pay attention to different aspects of the image, and yield different descriptions or interpretations under various contexts. Such diversity in perspectives is difficult to derive with conventional image description techniques. In this paper, we propose a customized image narrative generation task, in which the users are interactively engaged in the generation process by providing answers to the questions. We further attempt to learn the user's interest via repeating such interactive stages, and to automatically reflect the interest in descriptions for new images. Experimental results demonstrate that our model can generate a variety of descriptions from single image that cover a wider range of topics than conventional models, while being customizable to the target user of interaction.		Andrew Shin;Yoshitaka Ushiku;Tatsuya Harada	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00930	knowledge extraction;task analysis;natural language processing;feature extraction;visualization;human–computer interaction;artificial intelligence;narrative;computer science;recurrent neural network	Vision	-14.518317167239898	-68.71484122486066	43211
dac39e90c2d0184c567401a7a52bcd9eb17f8cdc	unsupervised summarization of threads in online discussion groups				Heba Elfardy;Ahmed Rafea	2010			automatic summarization;online discussion;information retrieval;thread (computing);multimedia;computer science	NLP	-22.69639079509979	-68.10016547690293	43250
4c09f6b3c759cf6a6d6161d4df1bd22eac5518b1	improving translation selection with a new translation model trained by independent monolingual corpora	statistical approach;判解;ming;statistical machine translation;huang;法律詞典;changning;論文;大陸法學;large scale;zhou;法規;cross language word similarity;月旦法學;法律題庫;裁判時報;ding;月旦知識庫;法學資料庫;yuan;tssci;教學;language model;machine translation;chinese english machine translation;translation selection	We propose a novel statistical translation model to improve translation selection of collocation. In the statistical approach that has been popularly applied for translation selection, bilingual corpora are used to train the translation model. However, there exists a formidable bottleneck in acquiring large-scale bilingual corpora, in particular for language pairs involving Chinese. In this paper, we propose a new approach to training the translation model by using unrelated monolingual corpora. First, a Chinese corpus and an English corpus are parsed with dependency parsers, respectively, and two dependency triple databases are generated. Then, the similarity between a Chinese word and an English word can be estimated using the two monolingual dependency triple databases with the help of a simple Chinese-English dictionary. This cross-language word similarity is used to simulate the word translation probability. Finally, the generated translation model is used together with the language model trained with the English dependency database to realize translation of Chinese collocations into English. To demonstrate the effectiveness of this method, we performed various experiments with verb-object collocation translation. The experiments produced very promising results.	collocation;data dictionary;database;experiment;language model;microsoft word for mac;parsing;simulation;statistical machine translation;text corpus	Ming Zhou;Ding Yuan;Changning Huang	2001	IJCLCLP		natural language processing;speech recognition;computer science;linguistics	NLP	-22.543169109312558	-77.09345756969114	43315
61b633591a177edcb13e8be3588646f4c23f9d94	selecting indexing strings using adaptation	noun;agglutinative language;word segmentation;selection index;adaptation;language model	It is not easy to tokenize agglutinative languages like Japanese and Chinese into words. Many IR systems start with a dictionary-based morphology program like ChaSen [4]. Unfortunately, dictionaries cannot cover all possible words; unknown words such as proper nouns are important for IR. This paper proposes a statistical dictionary-free method for selecting index strings based on recent work on adaptive language modeling.	chasen;dictionary;galaxy morphological classification;language model;lexical analysis	Yoshiyuki Takeda;Kyoji Umemura	2002		10.1145/564376.564477	natural language processing;noun;text segmentation;speech recognition;computer science;agglutinative language;language model;adaptation	NLP	-24.653896275821566	-77.62562694973343	43335
179c5137c6e6122862ed776eea8e0ffa23ad93d4	a multi-view intelligent editor for digital video libraries	video library;multimedia authoring;digital library;interactive video;information technology;digital video editing;multimedia materials;time synchronization;editing;authoring aids programming;video equipment;electronic libraries;silver;informedia;digital video;digital video library;authoring tool;audiovisual aids	Silver is an authoring tool that aims to allow novice users to edit di gital video. The goal is to make editing of digital video as easy as text editing. Silver provides multiple coordinated views, including project, source, outline, subject, storyboard, textual transcript and timeline views. Selections and edits in any view are synchronized with all other views. A variety of recognition algorithms are applied to the video and audio content and then are used to aid in the editing tasks. The Informedia Digital Library supplies the recognition algorithms and metadata used to support intelligent editing, and Informedia also provides search and a repository. The metadata includes shot boundaries and a time-synchronized transcript, which are used to support intelligent selection and intelligent cut/copy/paste.	algorithm;as-easy-as;cut, copy, and paste;digital video;free viewpoint television;informedia digital library;library (computing);storyboard;text editor;timeline	Brad A. Myers;Juan P. Casares;Scott M. Stevens;Laura A. Dabbish;Dan Yocum;Albert T. Corbett	2001		10.1145/379437.379461	digital library;editing;post-production;computer science;silver;multimedia;information technology;world wide web;computer graphics (images);non-linear editing system	HCI	-14.605986638516752	-54.606438456475004	43337
366b916b6f5772b5a188c6a68c6b88093c5dd5fc	alternative objective functions for training mt evaluation metrics		MT evaluation metrics are tested for correlation with human judgments either at the sentenceor the corpus-level. Trained metrics ignore corpus-level judgments and are trained for high sentence-level correlation only. We show that training only for one objective (sentence or corpus level), can not only harm the performance on the other objective, but it can also be suboptimal for the objective being optimized. To this end we present a metric trained for corpus-level and show empirical comparison against a metric trained for sentencelevel exemplifying how their performance may vary per language pair, type and level of judgment. Subsequently we propose a model trained to optimize both objectives simultaneously and show that it is far more stable than–and on average outperforms– both models on both objectives.	evaluation function;loss function;optimization problem;rapid refresh;text corpus	Milos Stanojevic;Khalil Sima'an	2017		10.18653/v1/P17-2004	artificial intelligence;computer science;machine learning;sentence	NLP	-15.38239922045103	-72.22126599325841	43344
6a22f6e6dae685a37b1ee0dc3d9b7ec96394c8a6	a common solution for tokenization and part-of-speech tagging	viterbi algorithm;part of speech tagging;space complexity	Current taggers assume that input texts are already tokenized, i.e. correctly segmented in tokens or high level information units that identify each individual component of the texts. This working hypothesis is unrealistic, due to the heterogeneous nature of the application texts and their sources. The greatest troubles arise when this segmentation is ambiguous. The choice of the correct segmentation alternative depends on the context, which is precisely what taggers study.In this work, we develop a tagger able not only to decide the tag to be assigned to every token, but also to decide whether some of them form or not the same term, according to different segmentation alternatives. For this task, we design an extension of the Viterbi algorithm able to evaluate streams of tokens of different lengths over the same structure. We also compare its time and space complexities with those of the classic and iterative versions of the algorithm.	part-of-speech tagging;tokenization (data security)	Jorge Graña Gil;Miguel A. Alonso;Manuel Vilares Ferro	2002		10.1007/3-540-46154-X_1	speech recognition;viterbi algorithm;computer science;artificial intelligence;machine learning;dspace;algorithm	NLP	-25.110258728351404	-78.01847754972756	43345
18f985f4df51065b6caf03ed14fb18bb3de022b0	applying morphology generation models to machine translation	generic model;statistical machine translation;machine translation	We improve the quality of statistical machine translation (SMT) by applying models that predict word forms from their stems using extensive morphological and syntactic information from both the source and target languages. Our inflection generation models are trained independently of the SMT system. We investigate different ways of combining the inflection prediction component with the SMT system by training the base MT system on fully inflected forms or on word stems. We applied our inflection generation models in translating English into two morphologically complex languages, Russian and Arabic, and show that our model improves the quality of SMT over both phrasal and syntax-based SMT systems according to BLEU and human judgements.	bleu;error analysis (mathematics);lexicon;mathematical morphology;preprocessor;statistical machine translation;text segmentation	Kristina Toutanova;Hisami Suzuki;Achim Ruopp	2008			natural language processing;speech recognition;transfer-based machine translation;example-based machine translation;computer science;linguistics;machine translation;rule-based machine translation	NLP	-23.28351482043201	-77.09194037739825	43422
292bf3162eb9f471392e72ea155581cea6e9c6f1	improving statistical machine translation with processing shallow parsing	conference paper	Reordering is of essential importance for phrase based statistical machine translation (SMT). In this paper, we would like to present a new method of reordering in phrase based SMT. We inspired from (Xia and McCord, 2004) using preprocessing reordering approaches. We used shallow parsing and transformation rules to reorder the source sentence. The experiment results from English-Vietnamese pair showed that our approach achieves significant improvements over MOSES which is the state-of-the art phrase based system.	monotone polygon;moses;preprocessor;shallow parsing;statistical machine translation	Hoai-Thu Vuong;Vinh Van Nguyen;Viet-Hong Tran;Akira Shimazu	2012			natural language processing;speech recognition;computer science;programming language	NLP	-21.981414162985924	-77.20730401861533	43449
10fb1e22698123f63719ff7fd2e9e1ffcd8147f3	research on information retrieval system based on ant clustering algorithm	ant-based clustering;clustering result;information retrieval;information retrieval system	Internet is more and more widely used, which provide a valuable information resource for users. How retrieve the information users prefers rapidly and accurately become the focus nowadays. With introducing ant-based clustering and sorting, it makes a more precise and rapid clustering result. Consequently, it increases the speed and efficiency of information retrieval.	algorithm;cluster analysis;information retrieval;internet;pile (abstract data type);sorting	Peiyu Liu;Zhenfang Zhu;Lina Zhao	2009	JSW		data stream clustering;document clustering;computer science;canopy clustering algorithm;cure data clustering algorithm;data mining;cluster analysis;brown clustering;world wide web;information retrieval;clustering high-dimensional data	Web+IR	-28.296056464457624	-56.86378950438573	43451
56ea86bc7fe656e0b512f9e3efd165ba2f78d3f3	semantic-based lightweight ontology learning framework: a case study of intrusion detection ontology		Building ontology for wireless network intrusion detection is an emerging method for the purpose of achieving high accuracy, comprehensive coverage, self-organization and flexibility for network security. In this paper, we leverage the power of Natural Language Processing (NLP) and Crowdsourcing for this purpose by constructing lightweight semi-automatic ontology learning framework which aims at developing a semantic-based solution-oriented intrusion detection knowledge map using documents from Scopus. Our proposed framework uses NLP as its automatic component and Crowdsourcing is applied for the semi part. The main intention of applying both NLP and Crowdsourcing is to develop a semi-automatic ontology learning method in which NLP is used to extract and connect useful concepts while in uncertain cases human power is leveraged for verification. This heuristic method shows a theoretical contribution in terms of lightweight and timesaving ontology learning model as well as practical value by providing solutions for detecting different types of intrusions.	crowdsourcing;heuristic;intrusion detection system;knowledge management;lightweight ontology;natural language processing;network security;ontology learning;scopus;self-organization;semiconductor industry;sensor	Yu Zhang;Morteza Saberi;Elizabeth Chang	2017		10.1145/3106426.3109053	ontology learning;process ontology;ontology-based data integration;data mining;ontology (information science);upper ontology;lightweight ontology;suggested upper merged ontology;intrusion detection system;computer science	Web+IR	-32.64017004389949	-67.60959618809598	43454
3273235b8687ffc9fe2776a5ba2df294e7ffad45	graph-based semi-supervised conditional random fields for spoken language understanding using unaligned data		We experiment graph-based SemiSupervised Learning (SSL) of Conditional Random Fields (CRF) for the application of Spoken Language Understanding (SLU) on unaligned data. The aligned labels for examples are obtained using IBM Model. We adapt a baseline semisupervised CRF by defining new feature set and altering the label propagation algorithm. Our results demonstrate that our proposed approach significantly improves the performance of the supervised model by utilizing the knowledge gained from the graph.	baseline (configuration management);conditional random field;information;international conference on machine learning;label propagation algorithm;semi-supervised learning;semiconductor industry;software propagation;supervised learning	Mohammad Aliannejadi;Masoud Kiaeeha;Shahram Khadivi;Saeed Shiry Ghidary	2014			natural language processing;speech recognition;computer science;machine learning	NLP	-19.471038776922686	-74.16549027965327	43474
4a30e569473c1401d501c4b6372ba8d7737206c7	improving text retrieval accuracy by using a minimal relevance feedback	probabilistic topic model;computer science all;text retrieval;query expansion	In this paper we have demonstrated that the accuracy of a text retrieval system can be improved if we employ a query expansion method based on explicit relevance feedback that expands the initial query with a structured representation instead of a simple list of words. This representation, named a mixed Graph of Terms, is composed of a directed and an a-directed subgraph and can be automatically extracted from a set of documents using a method for term extraction based on the probabilistic Topic Model. The evaluation of the method has been conducted on a web repository collected by crawling a huge number of web pages from the website ThomasNet.com. We have considered several topics and performed a comparison with a baseline and a less complex structure that is a simple list of words.	baseline (configuration management);document retrieval;mixed graph;query expansion;relevance feedback;terminology extraction;topic model;web page	Francesco Colace;Massimo De Santo;Luca Greco;Paolo Napoletano	2011		10.1007/978-3-642-37186-8_8	natural language processing;document retrieval;query expansion;relevance;computer science;concept search;pattern recognition;okapi bm25;information retrieval;human–computer information retrieval;divergence-from-randomness model	Web+IR	-27.765900078158424	-63.51247959238449	43502
cf335e461889899877aff8ae5b83803573202626	when image indexing meets knowledge discovery	cluster algorithm;retrieval;content based indexing;image;image indexing;indexing;content;indexation;relations;similarity;visual features;knowledge discovery	In our paper, we deal with the challenge of extending automatically the classic image indexing by visual relationship features. The visual relationship features are discovered automatically from images. They contribute to make more efficient the content-based indexing. More particularly, we develop an advanced content-based indexing articulated around the following notions : classic indexing, clustering algorithm, visual feature book and relationship qualification.	algorithm;cluster analysis	Chabane Djeraba	2000			search engine indexing;similarity;computer science;data science;image;data mining;knowledge extraction;information retrieval	Vision	-13.884439176206042	-58.80006812468325	43515
c549cfef2082e6a4835c2ff45e4111cd2ac3b140	using role determination and expert mining in the enterprise environment	information retrieval;words language;scientists;integration;scoring;symposia;ranking;analysts;documents;specialists;intranet;methodology;china;roles behavior	In real world, expert search is not just only name matching. Since each kind of people has their own features, we try two methods to judge whether the person we have found is more likely to be an expert. One method is to determine the role of a person by the context of the pages; the other is to judge the authority of a person by the forms of pages where he appears considering the structure of the Intranet. The evaluation results show these new methodologies have been helpful to improve the performance of the expert search on TREC 08 queries.	intranet	Jing Yao;Jun Xu;Junyu Niu	2008			natural language processing;legal expert system;ranking;computer science;data science;methodology;data mining;subject-matter expert;world wide web;china;information retrieval	Web+IR	-25.359734292247307	-56.90419257323878	43526
1cc76492ce4550163e6c5a2f76d5b3f97ceddb91	web classification using support vector machine	web classification;web pages;web mining;svm;support vector machine	In web classification, web pages from one or more web sites are assigned to pre-defined categories according to their content. Since web pages are more than just plain text documents, web classification methods have to consider using other context features of web pages, such as hyperlinks and HTML tags. In this paper, we propose the use of Support Vector Machine (SVM) classifiers to classify web pages using both their text and context feature sets. We have experimented our web classification method on the WebKB data set. Compared with earlier Foil-Pilfs method on the same data set, our method has been shown to perform very well. We have also shown that the use of context features especially hyperlinks can improve the classification performance significantly.	foil (programming language);html element;hyperlink;support vector machine;web page	Aixin Sun;Ee-Peng Lim;Wee Keong Ng	2002		10.1145/584931.584952	support vector machine;web mining;site map;web query classification;data web;web mapping;computer science;machine learning;social semantic web;data mining;world wide web;website parse template;information retrieval	ML	-28.43555131892465	-55.68920310336594	43551
c48af73b6f184636794372b6714a5c5adc3b1154	gp on spmd parallel graphics hardware for mega bioinformatics data mining	nvidia geforce 8800 gtx;paper;genetic pro gramming;qa mathematics;genetic programming;rapidmind;data mining;graphics hardware;package;nvidia;opengl;computer science;breast cancer;qh301 biology	We demonstrate a SIMD C++ genetic programming system on a single 128 node parallel nVidia GeForce 8800 GTX GPU under RapidMind’s GPGPU Linux software by predicting ten year+ outcome of breast cancer from a dataset containing a million inputs. NCBI GEO GSE3494 contains hundreds of Affymetrix HG-U133A and HG-U133B GeneChip biopsies. Multiple GP runs each with a population of 5 million programs winnow useful variables from the chaff at more than 500 million GPops per second. Sources available via FTP.	admissible numbering;bioinformatics;c++;data mining;gpops-ii;geforce 8 series;general-purpose computing on graphics processing units;genetic programming;geo (microformat);graphics hardware;graphics processing unit;linear discriminant analysis;linear model;linux;nonlinear system;simd;spmd;soft computing;statistical classification;support vector machine;while	William B. Langdon;Andrew P. Harrison	2008	Soft Comput.	10.1007/s00500-008-0296-x	genetic programming;parallel computing;computer science;operating system;breast cancer;package;graphics hardware;computer graphics (images)	ML	-6.692227658163349	-53.01202669883855	43576
867f4755c5e09188f2cdd082d0116c254cf46eef	feature reduction for neural network based text categorization	ducts;text classifier;neural network based text categorization;neural networks;high dimension feature space;multilayer perceptrons;vocabulary;reuters 22173 test collection;text analysis;testing;neural network classifier;dimensionality reduction techniques;feature space;classification;input space;categorization effectiveness;artificial neural networks;feature reduction;principal component analysis neural network based text categorization feature reduction text categorization model artificial neural network text classifier high dimension feature space dimensionality reduction techniques input space reuters 22173 test collection categorization effectiveness precision recall;precision;principal component analysis;recall;neural networks text categorization artificial neural networks scalability space technology testing vocabulary principal component analysis computer science ducts;full text databases;feedforward neural nets;space technology;scalability;computer science;text categorization model;high dimension;dimensional reduction;text categorization;test collection;artificial neural network;neural network;full text databases principal component analysis feedforward neural nets multilayer perceptrons classification text analysis	In a text categorization model using an artificial neural network as the text classifier, scalability is poor if the neural network is trained using the raw feature space since textural data has a very high-dimension feature space. We proposed and compared four dimensionality reduction techniques to reduce the feature space into an input space of much lower dimension for the neural network classifier. To test the effectiveness of the proposed model, experiments were conducted using a subset of the Reuters-22173 test collection for text categorization. The results showed that the proposed model was able to achieve high categorization effectiveness as measured by precision and recall. Among the four dimensionality reduction techniques proposed, Principal Component Analysis was found to be the most effective in reducing the dimensionality of the feature space.	artificial neural network;backpropagation;categorization;dimensionality reduction;document classification;experiment;feature vector;feedforward neural network;precision and recall;principal component analysis;scalability;text corpus	Savio L. Y. Lam;Dik Lun Lee	1999		10.1109/DASFAA.1999.765752	duct;scalability;feature vector;biological classification;boosting methods for object categorization;computer science;machine learning;pattern recognition;data mining;recall;accuracy and precision;software testing;space technology;artificial neural network;dimensionality reduction;principal component analysis	ML	-20.72204662016168	-64.00874580324687	43632
2128c2e4e016e5daa5fefafc180d993dc8dbde0c	quality information retrieval for the world wide web	quality assessment crawling web page retrieval quality information;quality assurance;information retrieval web sites web pages web search search engines quality control quality assurance web and internet services data mining machine learning algorithms;web pages;selected works;search engines;information retrieval;training;user survey;information services;web search engine;web crawler;web repository world wide web quality assurance internet information retrieval web pages quality evaluation mechanisms machine learning web search engine web crawlers;search engines information retrieval internet learning artificial intelligence quality assurance;quality assessment;internet;machine learning;quality evaluation;assessment methods;world wide web;bepress;quality information;quality criteria;crawlers;electronic publishing;learning artificial intelligence;quality control;encyclopedias;web page retrieval;crawling	The World Wide Web is an unregulated communication medium which exhibits very limited means of quality control. Quality assurance has become a key issue for many information retrieval services on the Internet, e.g. web search engines. This paper introduces some quality evaluation and assessment methods to assess the quality of web pages. The proposed quality evaluation mechanisms are based on a set of quality criteria which were extracted from a targeted user survey. A weighted algorithmic interpretation of the most significant user quoted quality criteria is proposed. In addition, the paper utilizes machine learning methods to produce a prediction of quality for web pages before they are downloaded. The set of quality criteria allows us to implement a web search engine with quality ranking schemes, leading to web crawlers which can crawl directly quality web pages. The proposed approaches produce some very promising results on a sizeable web repository.	information retrieval;machine learning;web crawler;web page;web search engine;world wide web	Milly Kc;Markus Hagenbuchner;Ah Chung Tsoi	2008	2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WIIAT.2008.378	web service;web application security;quality assurance;quality control;web development;web modeling;the internet;web analytics;web mapping;web design;web search engine;web accessibility initiative;web standards;computer science;web crawler;web navigation;crawling;web page;data mining;database;information quality;electronic publishing;web intelligence;world wide web;information retrieval;information system;web server;encyclopedia	Web+IR	-28.26821370216698	-52.87938118330557	43679
a1eeffd278089415ca9fa09aa3041297df118ad7	a system for the semantic interpretation of unrestricted domains using wordnet	semantic interpretation	"""In this demonstration, we show an algorithm for the semantic interpretation of unrestricted texts. The algorithm presents a solution for the following interpretation problems: determination of the meaning of the verb, identification of thematic roles and adjuncts, and attachments of prepositional phrases (PPs). An interesting aspect of the algorithm is that the solution of all these problems is interdependent. The interpretation algorithm uses WordNet (Miller et al. 1993) as its lexical knowledge-base. Predicates, or verbal concepts, have been defined for WordNet verb classes (Fellbaum 1993), which have been reorganized considerably following the criteria imposed by the interpretation algorithm. The WordNet ontology for nouns has also undergone some reorganization and redefinition to conform with the entries in the thematic roles of the predicates. We have taken a top-down approach that defines generic abstract predicates subsuming semantically and syntactically a large class of verbs. WordNet verb classes have been mapped into these generic predicates. Some of this mapping has required us to define new classes and to reclassify and/or redefine some WordNet classes and subclasses (Gomez 1998a). The predicates form a hierarchy in which thematic roles and inferences are inherited by subpredicates from their superpredicates. Two major consequences derive from anchoring verb classes in abstract semantic predicates: coalescing several WordNet senses into a predicate, which reduces the systemic polysemy in some WordNet senses, and mapping the same WordNet synset into distinct predicates. For instance, all the 5 synsets listed by WordNet for """"travel"""": """"travell, go, move, locomote;"""" """"travel2, journey; .... travel3, take a trip, make a trip;"""" """"travel4, journey;"""" and """"travel5 (undergo transportation, as in vehicle)"""" are coalesced into the abstract semantic predicate change-of-location-by-animate. This predicate defines a class of verbs containing the most generic properties shared by all members of the class. The differentia between this predicate and its subpredicates are given by one or more of the following: a) specific selectional restrictions for the thematic roles, b) different syntactic realizations of the thematic roles, and c) specific sets of inferences associated with the subpredicates. For instance, the instrument of drive is always a vehicle, while the instrument of change-of-location-by-animate can be an animate, an animate body part, etc. The instrument of drive is never realized by a subject, but the instrument of the generic predicate can be realized by a subject, e.g., """"This bus goes to Cambridge every Wednesday."""" On the other hand, migrate differs from change-of-location-by-animate only by the …"""	algorithm;attachments;differentia;emoticon;interdependence;semantic interpretation;synonym ring;top-down and bottom-up design;wordnet	Fernando Gomez;Carlos Segami	1999			semantic interpretation;semantic similarity;semantic computing;computer science;semantic equivalence	AI	-9.221524635432342	-74.8706606378459	43680
4f69a78395724d045136d8cbc551d3ac28df3a88	structural opinion mining for graph-based sentiment representation	graph representation;sentence level sentiment;input sentence;structural opinion mining;experimental evaluation;complicated opinion structure;on-line review corpus;linear programming-based structural learning;novel graph-based representation;chinese corpus;graph-based sentiment representation	Based on analysis of on-line review corpus we observe that most sentences have complicated opinion structures and they cannot be well represented by existing methods, such as frame-based and feature-based ones. In this work, we propose a novel graph-based representation for sentence level sentiment. An integer linear programming-based structural learning method is then introduced to produce the graph representations of input sentences. Experimental evaluations on a manually labeled Chinese corpus demonstrate the effectiveness of the proposed approach.	algorithm;corpus linguistics;directed graph;integer programming;linear programming;online and offline	Yuanbin Wu;Qi Zhang;Xuanjing Huang;Lide Wu	2011			natural language processing;computer science;machine learning;data mining	NLP	-20.634222614977865	-73.21573860097227	43686
39c1ef8b440dafbd298d56e031111ebb8c7184d0	bootstrapping word alignment via word packing	word alignment;machine translating;machine translation	We introduce a simple method to pack words for statistical word alignment. Our goal is to simplify the task of automatic word alignment by packing several consecutive words together when we believe they correspond to a single word in the opposite language. This is done using the word aligner itself, i.e. by bootstrapping on its output. We evaluate the performance of our approach on a Chinese-to-English machine translation task, and report a12.2% relative increase in BLEU score over a state-of-the art phrasebased SMT system.	bleu;bitext word alignment;booting;bootstrapping (compilers);data structure alignment;effective method;hidden markov model;machine translation;programming language;satisfiability modulo theories;set packing;simultaneous multithreading;text corpus	Yanjun Ma;Nicolas Stroppa;Andy Way	2007			natural language processing;speech recognition;word error rate;computer science;pattern recognition;linguistics;machine translation	NLP	-21.771204427605895	-77.61672579739603	43775
7abf32099f7599f5a05da7dbc1dde646536e5a25	integrating a large domain ontology of species into wordnet	conference report;domain ontology	With the proliferation of applications sharing information represented in multip le ontologies, the development of automatic methods for robust and accurate ontology matching will be crucial to their success. C onnecting and merging already existing semantic networks is perhaps one of the most challenging task related to knowledge engineerin g. This paper presents a new approach for aligning automatically a very large domain ontology of Species to WordNet. The approach relies on the use of knowledge-based Word Sense Disambiguation algorithm which accurately assigns WordNet synsets to the concepts repr esented in Species 2000.	algorithm;ontology (information science);ontology alignment;semantic network;synonym ring;word sense;word-sense disambiguation;wordnet	Montse Cuadros;Egoitz Laparra;German Rigau;Piek T. J. M. Vossen;Wauter Bosma	2010			natural language processing;upper ontology;open biomedical ontologies;wordnet;computer science;ontology;data mining;ontology-based data integration;information retrieval;process ontology;suggested upper merged ontology	AI	-31.221627931663996	-67.90614106811505	43855
04cc59f48af16d3ecccbea2937d3cf77d693363d	typology of comparatives	conference paper	It is well-known that an adjective is not a universal part of speech. As pointed out in the previous researches, many oceanic languages do not have a morphologically distinguished adjective class. The only criteria which distinguish adjectives from verbs and nouns could be whether they appear in comparative construction or not. However, even comparative construction is not universal. Whereas most languages have a specific morpheme to construct comparatives like inflection, some languages do not. In this paper I investigate various languages in the Pacific Asia region and discuss the difference in comparative construction in the framework of formal semantics. I suggest two parameters to distinguish the four language groups and propose a new typology of comparative construction.	biological anthropology;semantics (computer science)	Satomi Ito	2008			development economics;geography;environmental protection;cartography	PL	-28.716815330321392	-76.58401814921599	43875
2645a627e8671b479694992ede3ff3d022ab90c4	reasoning human emotional responses from large-scale social and public media		The basic characteristics of extreme events are their infrequence and potential damages to the human–nature system. It is difficult for people to design comprehensive policies for dealing with such events due to time pressure and their limit knowledge about rare and uncertain sequential impacts. Recently, online media provides digital source of individual and public information to study collective human responses to extreme events, which can help us reduce the damages of an extreme event and improve the efficiency of disaster relief. More specifically, there are different emotional responses (e.g., anxiety and anger) to an event and its subevents during a whole event, which can be reflected in the contents of public news and social media to a certain degree. Therefore, an online computational method for extracting these contents can help us better understand human emotional states at different stages of an event, reveal underlying reasons, and improve the efficiency of event relief. Here, we first employ tweets and reports extracted from Twitter and ReliefWeb for text analysis on three distinct events. Then, we detect textual contents by sentiment lexicon to find out human emotional responses over time. Moreover, a clustering-based method is proposed to detect emotional responses to a certain episode during events based on the co-occurrences of words as used in tweets and/or articles. Taking Japanese earthquake in 2011, Haiti earthquake in 2010 and Swine influenza A (H1N1) pandemic in 2009 as case studies, we reveal the underlying reasons of distinct patterns of human emotional responses to the whole events and their episodes.		Xianghua Li;Zhen Wang;Chao Gao;Lei Shi	2017	Applied Mathematics and Computation	10.1016/j.amc.2017.03.031	data mining	HCI	-21.79246821474812	-54.993245489430045	43906
2d19eb1f63de9331e5033e9beb1a518c414e9bd4	parallel fragments : measuring their impact on translation performance		Lack of parallel corpora have diverted the direction of research towards exploring other arenas to fill in the dearth. Comparable corpora have proved to be a valuable resource in this regard. Interestingly other than the parallel sentences extracted from comparable corpora, parallel phrase fragments have also proved to be beneficial for statistical machine translation. We present a novel approach based on an efficient framework for parallel fragment extraction from comparable corpora. Using the fragments as additional corpus for translation, we are able to obtain an improvement of 0.88 and 0.89 BLEU points on test data for Arabic English and French English systems respectively. We have also conducted a detailed analysis of impact of fragments extracted from related vs non-related corpus. A comparison of impact of parallel fragments vs. parallel sentences is also presented highlighting the significance of parallel segments for statistical machine translation. The article concludes with a crude comparative analysis of our approach with an existing fragment extraction technique at various stages of the fragment extraction pipeline. 2016 Elsevier Ltd. All rights reserved.	bleu;baseline (configuration management);experiment;formal language;information extraction;natural language generation;natural language processing;parallel text;qualitative comparative analysis;simultaneous multithreading;statistical machine translation;test data;text corpus;theodor schwenk;usability	Sadaf Abdul-Rauf;Holger Schwenk;Mohammad Nawaz	2017	Computer Speech & Language	10.1016/j.csl.2016.12.002	natural language processing;speech recognition;computer science;algorithm	NLP	-27.3978717054788	-74.91273289963613	43924
5e5930a1ac0404b79e2a23037a19e727fbb15f4f	churn identification in microblogs using convolutional neural networks with structured logical knowledge		For brands, gaining new customer is more expensive than keeping an existing one. Therefore, the ability to keep customers in a brand is becoming more challenging these days. Churn happens when a customer leaves a brand to another competitor. Most of the previous work considers the problem of churn prediction using CDRs. In this paper, we use micro-posts to classify customers into churny or nonchurny. We explore the power of CNNs since they achieved state-of-the-art in various computer vision and NLP applications. However, the robustness of end-toend models has some limitations such as the availability of a large amount of labeled data and uninterpretability of these models. We investigate the use of CNNs augmented with structured logic rules to overcome or reduce this issue. We developed our system called Churn teacher by using an iterative distillation method that transfers the knowledge, extracted using just the combination of three logic rules, directly into the weight of Deep Neural Networks (DNNs). Furthermore, we used weight normalization to speed up training our convolutional neural networks. Experimental results showed that with just these three rules, we were able to get state-of-the-art on publicly available Twitter dataset about three Telecom brands.	artificial neural network;computer vision;concatenation;convolutional neural network;gene prediction;iterative method;lookup table;named-entity recognition;natural language processing;neural networks;text corpus;word embedding	Mourad Gridach;Hatem Haddad;Hala Mulki	2017			convolutional neural network;artificial intelligence;machine learning;microblogging;computer science;natural language processing;social media	AI	-18.08471217600286	-70.76548967355603	43929
60907b3456ccdbccf440102657204531f22211a0	detecting concept mentions in biomedical text using hidden markov model: multiple concept types at once or one at a time?	biological patents;biomedical journals;text mining;europe pubmed central;citation search;data mining and knowledge discovery;data mining;citation networks;computational biology bioinformatics;research articles;abstracts;open access;life sciences;clinical guidelines;algorithms;full text;combinatorial libraries;computer appl in life sciences;information storage and retrieval;natural language processing;rest apis;electronic health records;orcids;europe pmc;biomedical research;bioinformatics;literature search	BACKGROUND Identifying phrases that refer to particular concept types is a critical step in extracting information from documents. Provided with annotated documents as training data, supervised machine learning can automate this process. When building a machine learning model for this task, the model may be built to detect all types simultaneously (all-types-at-once) or it may be built for one or a few selected types at a time (one-type- or a-few-types-at-a-time). It is of interest to investigate which strategy yields better detection performance.   RESULTS Hidden Markov models using the different strategies were evaluated on a clinical corpus annotated with three concept types (i2b2/VA corpus) and a biology literature corpus annotated with five concept types (JNLPBA corpus). Ten-fold cross-validation tests were conducted and the experimental results showed that models trained for multiple concept types consistently yielded better performance than those trained for a single concept type. F-scores observed for the former strategies were higher than those observed for the latter by 0.9 to 2.6% on the i2b2/VA corpus and 1.4 to 10.1% on the JNLPBA corpus, depending on the target concept types. Improved boundary detection and reduced type confusion were observed for the all-types-at-once strategy.   CONCLUSIONS The current results suggest that detection of concept phrases could be improved by simultaneously tackling multiple concept types. This also suggests that we should annotate multiple concept types in developing a new corpus for machine learning models. Further investigation is expected to gain insights in the underlying mechanism to achieve good performance when multiple concept types are considered.	body of uterus;concept map;confusion;cross reactions;cross-validation (statistics);hidden markov model;machine learning;markov chain;phrases;sensor;supervised learning;uterine corpus carcinosarcoma	Manabu Torii;Kavishwar B. Wagholikar;Hongfang Liu	2014		10.1186/2041-1480-5-3	natural language processing;text mining;medical research;computer science;bioinformatics;data science;data mining;information retrieval	NLP	-23.956631211190135	-68.05754304312201	43957
ef6b87929d67c2a2c69d3c0f18dfd60ead60be33	a logistic regression model of irony detection in chinese internet texts		The research of sentiment analysis has become fascinating with the support of emerging Internet language material. In this paper, irony in Chinese is investigated as a sentiment that has not been meticulously studied. We describe here a set of features and their computational formalization for detecting irony at a linguistic level. Comments from online forum are collected and detected whether ironical or not, with a logistic regression model. The efficacy and validity of our model is proved by comparison with other popular learning methods and statistical testing. The model achieves a performance close to state-of-the-art results in English and Italian from recall and accuracy perspective.	irony;logistic regression;part-of-speech tagging;sensor;sentiment analysis	Frank Z. Xing	2015	Research in Computing Science		the internet;logistic regression;natural language processing;speech recognition;irony;psychology;artificial intelligence	NLP	-21.594815662677508	-68.0097625407332	43970
d55040cc13198dcea6c42d8c6c7d7c1fa4484939	statistical approach to noisy-parallel and comparable corpora filtering for the extraction of bi-lingual equivalent data at sentence-level		Text alignment and text quality are critical to the accuracy of Machine Translation (MT) systems and other text processing tasks requiring bilingual data. In this study, we propose a language independent bi-sentence filtering approach based on Polish to English translation. This approach was developed using a noisy TED Talks corpus and tested on a Wikipedia-based comparable corpus; however, it can be extended to any text domain or language pair. The proposed method uses various statistical measures for sentence comparison and can be used for in-domain data adaptation tasks as well. Minimization of data loss was ensured by parameter adaptation. An improvement in MT system score using the text processed using our tool is discussed and in-domain data adaptation results are presented. We also discuss measures to improve performance such as bootstrapping and comparison model pruning. The results show significant improvement in filtering in terms of MT quality.	text corpus	Krzysztof Wolk;Emilia Zawadzka;Agnieszka Wolk	2018		10.1007/978-3-319-77703-0_79	machine translation;minification;filter (signal processing);data loss;bootstrapping;pattern recognition;sentence;text processing;computer science;artificial intelligence	NLP	-21.2575488788689	-77.87875992047556	43992
bdbc834a9454f4fe9c85feecc1bb513a3b9fd221	tweet classification based on their lifetime duration	filtering;real time;microblog;time dependency;twitter	Many microblog messages remain useful only within a short time, and users often find such a message after its informational value has vanished. Users also sometimes miss old but still useful messages buried among outdated ones. To solve these problems, we develop a method of classifying messages into the following three categories: (1) messages that users should read now because their value will diminish soon, (2) messages that users may read later because their value will not largely change soon, and (3) messages that are not useful anymore because their value has vanished. Our method uses an error correcting output code consisting of binary classifiers each of which determines whether a given message has value at specific time point. Our experiments on Twitter data confirmed that it outperforms naive methods.	binary classification;experiment	Hikaru Takemura;Keishi Tajima	2012		10.1145/2396761.2398642	filter;computer science;microblogging;data mining;internet privacy;world wide web	DB	-22.95139543235302	-53.40812712999705	44066
15883024a483864784bd5818c7ec4219a55aec17	medical information extracting system by bootstrapping of nttdrdh at ntcir-10 mednlp task		We participated in a complaint and diagnosis task of MedNLP in NTCIR10. We extracted words of complaint/diagnosis by using a hybrid approach with bootstrapping and pattern matching with a medical term dictionary. It was possible that part of the complaint’s or diagnosis’s expressions are present in the extracted words. Therefore, our system concatenated the extracted words and their surrounding words by heuristic rules and determined the final complaint’s or diagnosis’s words. And our system estimated the modality attribute of the extracted complaint/diagnosis by heuristic rules also.	bootstrapping (compilers);concatenation;dictionary;heuristic;modality (human–computer interaction);pattern matching	Yuji Nomura;Takashi Suenaga;Daisuke Satoh;Megumi Ohki;Toru Takaki	2013			theoretical computer science;complaint;concatenation;bootstrapping;pattern matching;heuristic;artificial intelligence;expression (mathematics);pattern recognition;computer science	NLP	-24.842999305675328	-79.8240438313463	44147
7cd4312086ced697d64c3c9c5f25421502e1464d	a fpga-based parallel semi-naive bayes classifier implementation	fpga;parallel semi naive bayes classification		field-programmable gate array;naive bayes classifier;semiconductor industry	Sun-Wook Choi;Chong Ho Lee	2013	IEICE Electronic Express	10.1587/elex.10.20130673	computer science;theoretical computer science;machine learning;pattern recognition;field-programmable gate array	HCI	-9.99048436103344	-65.30286215154703	44202
43f753008b8791d6173791fb7411b1fff868edd0	a general framework for managing and processing live video data with privacy protection		Though a large body of existing work on video surveillance focuses on image and video processing techniques, few address the usability of such systems, and in particular privacy issues. This study fuses concepts from stream processing and content-based image retrieval to construct a privacy-preserving framework for rapid development and deployment of video surveillance applications. Privacy policies, instantiated to as privacy filters, may be applied both granularly and hierarchically. Privacy filters are granular as they are applicable to specific objects appearing in the video streams. They are hierarchal because they can be specified at specific objects in the framework (e.g., users, cameras) and are combined such that the disseminated video stream adheres to the most stringent aspect specified in the cascade of all privacy filters relevant to a video stream or query. To support this privacy framework, we extend our Live Video Database Model with an informatics-based approach to object recognition and tracking and add an intrinsic privacy model that provides a level of privacy protection not previously available for real-time streaming video data. The proposed framework also provides a formal approach to implement and enforce privacy policies that are verifiable, an important step towards privacy certification of video surveillance systems through a standardized privacy specification language.	closed-circuit television;content-based image retrieval;database model;declarative programming;experiment;fifo (computing and electronics);formal verification;general-purpose markup language;general-purpose modeling;high- and low-level;image processing;in-memory database;informatics;match moving;mechatronics;outline of object recognition;particle filter;privacy policy;query language;rapid application development;real-time transcription;software deployment;software development;specification language;stream processing;streaming media;systems architecture;usability;video processing	Alexander J. Aved;Kien A. Hua	2011	Multimedia Systems	10.1007/s00530-011-0245-x	privacy software;computer science;video tracking;internet privacy;world wide web;computer security;computer network	Security	-12.078406981298835	-55.19381822070538	44248
0ff68de1bc8ac7eea1bcf8be2dee8b1d6cbfa460	topic model methods for automatically identifying out-of-scope resources	digital library;digital libraries;performance metric;machine learning;topics;relevance;support vector machine;digital library for earth system education;text classication;scope	Recent years have seen the rise of subject-themed digital libraries, such as the NSDL pathways and the Digital Library for Earth System Education (DLESE). These libraries often need to manually verify that contributed resources cover topics that fit within the theme of the library. We show that such scope judgments can be automated using a combination of text classification techniques and topic modeling. Our models address two significant challenges in making scope judgments: only a small number of out-of-scope resources are typically available, and the topic distinctions required for digital libraries are much more subtle than classic text classification problems. To meet these challenges, our models combine support vector machine learners optimized to different performance metrics and semantic topics induced by unsupervised statistical topic models. Our best model is able to distinguish resources that belong in DLESE from resources that don't with an accuracy of around 70%. We see these models as the first steps towards increasing the scalability of digital libraries and dramatically reducing the workload required to maintain them.	document classification;library (computing);national science digital library;scalability;support vector machine;topic model;unsupervised learning	Steven Bethard;Soumya Ghosh;James H. Martin;Tamara Sumner	2009		10.1145/1555400.1555405	support vector machine;digital library;relevance;computer science;data science;data mining;world wide web;information retrieval	Web+IR	-23.58168965195309	-64.45140263108084	44279
5c763e289ca11a76486a0e4e92cc203bcea85c33	babel: an eastern european multi-language database	database management systems;linguistics;natural languages;research initiatives;babel;bulgarian;copernicus project;esprit sam project;eurom databases;eastern european multi-language database;estonian;hungarian;polish;romanian;data collection;data formatting	BABEL is a joint European project under the COPERNICUS scheme (Project #1304) comprising partners from five Eastern European countries and three Western ones. The project is producing a multi-language database of five of the most widelydiffering Eastern European languages. The collection and formatting of the data conforms to the protocols established by the ESPRIT SAM project and the resulting EUROM databases.	database	Peter Roach;Simon Arnfield;William J. Barry;J. Baltova;Marian Boldea;Adrian Fourcin;W. Gonet;Ryszard Gubrynowicz;E. Hallum;Lori Lamel;Krzysztof Marasek;Alain Marchal;Einar Meister;Klára Vicsi	1996			engineering;data mining;database;world wide web	DB	-33.55558562865231	-75.52534511129967	44280
1f16bd00c859d3251c5beb4cf1c84d1ec1004133	comparing values and sentiment using mechanical turk	statistical significance;text classification;human values;sentiment analysis	Human values can help to explain people's sentiment toward current events. In this experiment, we compare people's values with their agreement or disagreement with paragraphs that were classified as either supporting or opposing a specific topic. We found that five value types have statistically significant agreement (p<0.001) for both the supporting and opposing paragraphs, in opposite directions. We hope to use these paragraph ratings to train an automatic text classifier to agree or disagree with paragraphs based on a specific value profile.	amazon mechanical turk;text-based user interface;the turk;value (ethics)	Thomas Clay Templeton;Kenneth R. Fleischmann;Jordan L. Boyd-Graber	2011		10.1145/1940761.1940903	psychology;data science;data mining;social psychology	HCI	-22.124984625968622	-59.73931436388128	44334
f243bb4a321abcca6b1dcff0f3165723e55c2980	discovering relations between named entities from a large raw corpus using tree similarity-based clustering	unsupervised learning;hierarchical clustering;lenguaje natural;similarity metric;entity relationship model;analyse amas;raisonnement base sur cas;text;razonamiento fundado sobre caso;classification non supervisee;speech processing;langage naturel;tratamiento palabra;traitement parole;metric;modelo entidad relacion;texte;apprentissage non supervise;modele entite relation;similitude;relation extraction;hierarchical classification;cluster analysis;natural language;clasificacion no supervisada;similarity;classification hierarchique;unsupervised classification;metrico;analisis cluster;similitud;case based reasoning;texto;clasificacion jerarquizada;named entity;metrique;shallow parsing	We propose a tree-similarity-based unsupervised learning method to extract relations between Named Entities from a large raw corpus. Our method regards relation extraction as a clustering problem on shallow parse trees. First, we modify previous tree kernels on relation extraction to estimate the similarity between parse trees more efficiently. Then, the similarity between parse trees is used in a hierarchical clustering algorithm to group entity pairs into different clusters. Finally, each cluster is labeled by an indicative word and unreliable clusters are pruned out. Evaluation on the New York Times (1995) corpus shows that our method outperforms the only previous work by 5 in F-measure. It also shows that our method performs well on both high-frequent and lessfrequent entity pairs. To the best of our knowledge, this is the first work to use a tree similarity metric in relation clustering.	ace;cluster analysis;entity;expect;expectation–maximization algorithm;hierarchical clustering;named-entity recognition;parse tree;relationship extraction;sensor;shallow parsing;similarity measure;sparse matrix;the new york times;thesaurus;unsupervised learning;wordnet	Min Zhang;Jian Su;Danmei Wang;Guodong Zhou;Chew Lim Tan	2005		10.1007/11562214_34	unsupervised learning;correlation clustering;case-based reasoning;similarity;metric;fuzzy clustering;entity–relationship model;computer science;similitude;machine learning;pattern recognition;data mining;speech processing;mathematics;hierarchical clustering;cluster analysis;natural language;single-linkage clustering	NLP	-24.925768641114193	-67.28120079214433	44451
92ec5884145d8de15711c15ef535cfa3b20cfa75	an auditory display in playlist generation	music streaming services auditory display playlist generation music information retrieval music playlists mobile devices;information retrieval;auditory displays;mobile handsets music mobile communication visualization signal processing algorithms auditory displays algorithm design and analysis music information retrieval;visualization;music information retrieval mobile computing mobile handsets;signal processing;music information retrieval;mobile communication;mobile handsets;auditory display;playlist generation;signal processing algorithms;mobile computing;music;algorithm design;algorithm design and analysis;mobile devices;music streaming services;music playlists	A current area of research within music information retrieval is the automatic formation of music playlists: automatically creating a meaningful arrangement of music tracks. It is an application area particularly suited for accessing music on mobile devices as music collections are becoming too large to manually navigate. The growth of music streaming services means that even more vast collections of music are accessible from a mobile device. Much energy has been spent constructing the best algorithms to create a playlist, but less has been spent on how that algorithm is presented and accessed, particularly on a mobile device.	algorithm;auditory display;binaural beats;feedback;information retrieval;library (computing);mobile device;multimodal interaction;pervasive informatics;standard library;streaming media;surround sound;user interface	Rebecca Stewart;Mark B. Sandler	2011	IEEE Signal Processing Magazine	10.1109/MSP.2011.940883	algorithm design;speech recognition;computer science;operating system;signal processing;multimedia;mobile computing	Web+IR	-16.72152792930018	-54.63468372621214	44491
90a0318869be0f662e78e7d87733de7effc26d00	bridging taxonomic semantics to accurate hierarchical classification		The unregulated and open nature of the Internet and the explosive growth of the Web create a pressing need to provide various services for content categorization. The hierarchical classification attempts to achieve both accurate classification and increased comprehensibility. It has also been shown in literature that hierarchical models outperform flat models in training efficiency, classification efficiency, and classification accuracy (Koller & Sahami, 1997; McCallum, Rosenfeld, Mitchell & Ng, 1998; Ruiz & Srinivasan ,1999; Dumais & Chen, 2000; Yang, Zhang & Kisiel, 2003; Cai & Hofmann, 2004; Liu, Yang, Wan, Zeng, Cheng & Ma, 2005). However, the quality of the taxonomy attracted little attention in past works. Actually, different taxonomies can result in differences in classification. So the quality of the taxonomy should be considered for real-world classifications. Even a semantically sound taxonomy does not necessarily lead to the intended classification performance (Tang, Zhang & Liu 2006). Therefore, it is desirable to construct or modify a hierarchy to better suit the hierarchical content classification task.		Lei Tang;Huan Liu;Jianping Zhang	2009		10.4018/978-1-60566-010-3.ch029	cluster analysis;bridging (networking);hierarchical database model;semantics;artificial intelligence;computer science;pattern recognition	AI	-18.971659431030833	-60.88844649034697	44510
673a12f60e6bb69160d6ff7380fdfb7a7fde64f3	parametric description of a scan-display system	computers;image processing;data processing;television data display systems description of digital scan display system parametric;data acquisition system;interactive display;digital systems;pattern recognition;data acquisition systems;n26400 instrumentation miscellaneous instruments components;photographic film;images;central processing unit;recording systems;data acquisition systems description of digital scan display system parametric	Automatic Pattern Recognition and graphical data processing have recently received considerable attention. In addition to analysis and processing of pictorial information, there is a need for interactive display systems to present both intermediate and final processed data. The subject of graphic display terminals has been extensively discussed in the literature. Scan-display systems oriented towards image processing, however, with particular attention to bypassing the central processing unit for as many tasks as possible have not had comparable development. This paper is focused on this latter area.	central processing unit;graphical user interface;image processing;pattern recognition	Lawrence A. Dunn;Lakshmi N. Goyal;Bruce H. McCormick;Val G. Tareski	1969		10.1145/1476793.1476826	computer vision;computer hardware;computer science;digital image processing;computer graphics (images)	Graphics	-10.097082744697547	-53.374729783542236	44518
0f475c37047ec75298a71036bdcde18039d5599c	content-based analysis method for sentiment scoring in microblogging mining			sentiment analysis	Nurfadhlina Mohd Sharef;Fartash Haghanikhameneh	2014		10.3233/978-1-61499-434-3-398	microblogging;sentiment analysis;data mining;computer science;social media	NLP	-22.528494784100065	-57.31301209329993	44536
1f6dcd31854a6216e3327b9e54a41f8162efec02	a multitude of linguistically-rich features for authorship attribution - notebook for pan at clef 2011		This paper reports on the procedure and learning models we adopted for the ‘PAN 2011 Author Identification’ challenge targetting real-world email messages. The novelty of our approach lies in a design which combines shallow characteristics of the emails (words and trigrams frequencies) with a large number of ad hoc linguistically-rich features addressing different language levels. For the author attribution tasks, all these features were used to train a maximum entropy model which gave very good results. For the single author verification tasks, a set of features exclusively based on the linguistic description of the emails’ messages was considered as input for symbolic learning techniques (rules and decision trees), and gave weak results. This paper presents in detail the features extracted from the corpus, the learning models and the results obtained.	decision tree;email;hoc (programming language);principle of maximum entropy;text corpus;trigram	Ludovic Tanguy;Assaf Urieli;Basilio Calderone;Nabil Hathout;Franck Sajous	2011			natural language processing;multitude;clef;attribution;art;artificial intelligence	Web+IR	-22.148450057202982	-70.93016481683146	44549
2539794c88ad244d212c5cebe81c171357bb088d	semanticrank: ranking keywords and sentences using semantic graphs	co-occurrence information;feature extraction;semantic graph;different ranking algorithm;text summary;inverse document frequency;graph-based ranking algorithm;text node;sentence extraction;ranking keyword;candidate keyword	The selection of the most descriptive terms or passages from text is crucial for several tasks, such as feature extraction and summarization. In the majority of the cases, research works propose the ranking of all candidate keywords or sentences and then select the top-ranked items as features, or as a text summary respectively. Ranking is usually performed using statistical information from text (i.e., frequency of occurrence, inverse document frequency, co-occurrence information). In this paper we present SemanticRank, a graph-based ranking algorithm for keyword and sentence extraction from text. The algorithm constructs a semantic graph using implicit links, which are based on semantic relatedness between text nodes and consequently ranks nodes using different ranking algorithms. Comparative evaluation against related state of the art methods for keyword and sentence extraction shows that SemanticRank performs favorably in previously used data sets.	shebang (unix)	George Tsatsaronis;Iraklis Varlamis;Kjetil Nørvåg	2010			natural language processing;text graph;computer science;pattern recognition;ranking svm;information retrieval	NLP	-26.57474827944025	-64.3375048515762	44551
a0f692b3d4e4d0273726ca5508ca147e47ec3a4c	an intelligent system for sentence retrieval and novelty mining	relevant information;information retrieval;software engineering;blended metrics;evaluation measure;novel information;sentence retrieval;intelligent systems;novelty mining;intelligent system;knowledge engineering	This paper describes the development of an intelligent system for sentence retrieval and novelty mining. Knowledge and software engineering techniques are used for the development, implementation and evaluation strategies for relevant sentence retrieval and novelty mining using blended metrics. Moreover, we describe our novelty mining system with result aggregation, which can greatly facilitate users to find novel information. The experimental results on TREC 2003 and TREC 2004 novelty track data demonstrate the usefulness of our intelligent novelty mining system in a variety of performance situations and user settings. By considering the special issues based on domain knowledge, our intelligent novelty mining system can help to discover novelty that can support business people who are making domain-pertinent actionable decisions.		Flora S. Tsai;Kap Luk Chan	2011	IJKEDM	10.1504/IJKEDM.2011.037645	computer science;artificial intelligence;data science;knowledge engineering;data mining;information retrieval	NLP	-24.253509824336085	-58.1189704753143	44554
4f45cb2fa22a6bf8d49cc2021a624c1ae73b78f5	an audio indexing system for election video material	user interface;search engines;information retrieval;large vocabulary automatic speech recognition;speech;audio indexing system;presidential election;automatic speech recognition;indexes;navigation;hidden markov models;user interfaces information retrieval search engines speech recognition;indexing;design and implementation;election video material;indexation;nominations and elections;speech recognition;indexing nominations and elections automatic speech recognition youtube user interfaces navigation video compression information retrieval web server databases;audio indexing;user interfaces;large vocabulary automatic speech recognition audio indexing system election video material information retrieval user interfaces;user interfaces large vocabulary automatic speech recognition information retrieval	In the 2008 presidential election race in the United States, the prospective candidates made extensive use of YouTube to post video material. We developed a scalable system that transcribes this material and makes the content searchable (by indexing the meta-data and transcripts of the videos) and allows the user to navigate through the video material based on content. The system is available as an iGoogle gadget1 as well as a Labs product (labs.google.com/gaudi). Given the large exposure, special emphasis was put on the scalability and reliability of the system. This paper describes the design and implementation of this system.	prospective search;scalability	Christopher Alberti;Michiel Bacchiani;Ari Bezman;Ciprian Chelba;Anastassia Drofa;Hank Liao;Pedro J. Moreno;Ted Power;Arnaud Sahuguet;Maria Shugrina;Olivier Siohan	2009	2009 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2009.4960723	speech recognition;computer science;multimedia;user interface;world wide web;information retrieval;hidden markov model	Robotics	-15.163229207303825	-55.63656898064022	44567
d3c80063a02e3d813644a4b6030cc57d83a0b059	personalized semantic based blog retrieval	godfrey winster sathianesan swamynathan sankaranarayanan 个性化 博客 检索 语义 基于内容 语料库 xml 系统 personalized semantic based blog retrieval	Blog retrieval is a complex task because of the informal language usage. Blogs deviate from the language which is used in traditional corpora largely due to various reasons. Spelling errors, grammatical irregularity, over use of abbreviations and symbolic characters like emotions are a few reasons of irregular corpus blogs. To make the retrieval of blogs easier, the novel idea of personalized semantic based blog retrieval (PSBBR) system is discussed in this paper. The blogs are tagged with a relationship to one another with reference to ontology. The meanings of the blog content and key term are tagged as XML tags. The query term accesses the XML tags to retrieve entire blog content. The system is evaluated with a huge number of blogs extracted from various blog sources. Relevance score is calculated for every blog associated with keywords and content-based importance (CBI) gives the content similarity to the query word. The experimental result shows the system performs well for the blog retrieval process.	blog;grammar induction;importance sampling;personalization;relevance;semantic web;text corpus;web mining;web service;wordnet;xml	Godfrey Winster Sathianesan;Swamynathan Sankaranarayanan	2012	Journal of Computer Science and Technology	10.1007/s11390-012-1246-8	computer science;multimedia;world wide web;information retrieval	Web+IR	-29.81723468567507	-57.787384759693744	44625
375fbb7ff8164eda2461d633dec856871020d060	pazesh: a graph-based approach to increase readability of automatic text summaries	automatic text summarization challenge;readability factor;important content;benchmark datasets;language-independent graph-based approach;automatic text summary;topic centroid sentence;special path;salient sentence;themost important aspect	Today, research on automatic text summarization challenges on readability factor as one of the most important aspects of summarizers’ performance. In this paper, we present Pazesh: a language-independent graph-based approach for increasing the readability of summaries while preserving the most important content. Pazesh accomplishes this task by constructing a special path of salient sentences which passes through topic centroid sentences. The results show that Pazesh compares approvingly with previously published results on benchmark datasets.	automatic summarization;benchmark (computing);human-readable medium;language-independent specification	Nasrin Mostafazadeh;Seyed Abolghasem Mirroshandel;Gholamreza Ghassem-Sani;Omid Bakhshandeh Babarsad	2011		10.1007/978-3-642-21043-3_38	natural language processing;computer science;data mining;information retrieval	NLP	-26.67046921448775	-65.64762324434697	44639
3f141bd11f7c87c15184ecba9fe20c0637775dc1	classification of comments by tree kernels using the hierarchy of wikipedia for tree structures	tree kernel;wikipedia;naive bayes;tourism video retrieval system comments classification tree kernels wikipedia hierarchy tree structures web services automatic text classification accurate text classification bag of words tweets classification wikipedia categories;encyclopedias electronic publishing internet videos kernel support vector machines;support vector;twitter;web services pattern classification social networking online text analysis travel industry tree data structures video retrieval;naive bayes support vector tree kernel wikipedia twitter	Many web services posting short comments such as product reviews and Twitter have been provided. We consider that automatic and accurate text classification may lead to develop new web services and system. In the past, the frequency of appearance of words by bag-of-words have been often used for text classification as a basic technique. In contrast, we propose a technique to classify tweets using tree kernels created by the categories of Wikipedia in this study. In addition, we developed a retrieval system for tourism videos by applying the technique to tweets related to tourism.	bag-of-words model in computer vision;document classification;web service;wikipedia	Masahiro Takeda;Nobuyuki Kobayashi;Fumio Kitagawa;Hiromitsu Shiina	2016	2016 5th IIAI International Congress on Advanced Applied Informatics (IIAI-AAI)	10.1109/IIAI-AAI.2016.62	computer science;pattern recognition;data mining;tree kernel;world wide web	Web+IR	-22.539154233760467	-60.58316153715822	44677
660341b18fbc2fd2a8b0092c476e87ae6e74e99d	selecting a reference frame	spatial relations;selection;reference frame;reference frames;spatial language;inhibition;spatial configuration	Reference frames are representations that parse space. In the case of spatial terms, reference frames mediate the mapping of linguistic expressions onto spatial configurations of objects. In the sentence ``The fly is above the cat,'' ``above'' is defined with respect to a reference frame that is imposed on the cat. Different types of reference frames can be used to define spatial terms, each based on a different source of information. For example, gravity, the orientation of objects in the scene or the orientation of the viewer can all be used to set the orientation of a reference frame. When these reference frames disagree (because the viewer is reclining or because the objects in the scene are overturned), there are competing definitions for the spatial term, resulting in the need for reference frame selection. The purpose of this paper is to review a line of research that examines reference frame selection in the context of spatial language. This work shows that all reference frames are initially active and assign a direction to a spatial term. Moreover, this activation is automatic, and is followed by the selection of a single reference frame, with selection accompanied by inhibition of the non-selected frames. Parallels between reference frame selection in language and in perception and attention are discussed.	reference frame (video)	Laura A. Carlson	1999	Spatial Cognition & Computation	10.1023/A:1010071109785	reference frame;computer vision;computer science;artificial intelligence;mathematics;communication	Vision	-8.729537993710363	-77.15720856486355	44733
270421d1d682d2e447269fbefa5f0766690af348	clustering-based analysis of semantic concept models for video shots	entropy based method;pattern clustering;multimedia lexicon;video signal processing;information science;video shot;information retrieval;probability density function;trecvid 2005;semantic concept model;feature space;multimedia systems;training data;feature extraction technique;shape;pattern matching;feature extraction;signal processing;trecvid 2005 clustering based analysis semantic concept model video shot entropy based method multimedia lexicon feature extraction technique;ontologies;clustering based analysis;video signal processing entropy feature extraction multimedia systems pattern clustering;entropy;digital video;information analysis;ontologies feature extraction entropy shape probability density function training data information analysis pattern matching large scale systems information science;large scale systems	In this paper we present a clustering-based method for representing semantic concepts on multimodal low-level feature spaces and study the evaluation of the goodness of such models with entropy-based methods. As different semantic concepts in video are most accurately represented with different features and modalities, we utilize the relative model-wise confidence values of the feature extraction techniques in weighting them automatically. The method also provides a natural way of measuring the similarity of different concepts in a multimedia lexicon. The experiments of the paper are conducted using the development set of the TRECVID 2005 corpus together with a common annotation for 39 semantic concepts	cluster analysis;experiment;feature extraction;high- and low-level;lexicon;multimodal interaction	Markus Koskela;Alan F. Smeaton	2006	2006 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2006.262546	training set;entropy;probability density function;semantic similarity;semantic computing;feature vector;feature extraction;information science;shape;computer science;ontology;machine learning;pattern matching;signal processing;pattern recognition;data mining;data analysis;information retrieval;statistics	Robotics	-15.451149332441148	-58.22897646793517	44754
eb0722b80484756ed9f1cee7498a962cb76a5f81	a video browser based on closed cap	video databases;multimedia systems streaming media digital tv video sharing databases information resources tv broadcasting technology management video compression displays;video index data video browser browse recorded video closed caption text keyword selection web browser video clips audio synchronized transcript;keyword selection;digital television;indexing terms;video indexing;online front ends;closed caption text;audio synchronized transcript;video index data;web browser;video databases online front ends;video browser;video clips;browse recorded video	This paper proposes a way to browse recorded video using closed caption text and chosen keywords. It shows examples of keyword selection for news and sports program. The keyword can be chosen using a Web browser to see the recent news keywords. This paper describes a two-step procedure to browse recorded video. The first step is generating a video index for video clips using closed caption data in video as the synchronized transcript of audio. Then, the viewer may search and play those recorded videos, using the saved closed caption text and video index data. This paper also provides details how to setup database from video and browse video with those indices	browsing;digital video;embedded system;video clip	Janghwan Lee	2006	IEEE Transactions on Consumer Electronics	10.1109/TCE.2006.1706517	video compression picture types;index term;video production;digital television;telecommunications;computer science;video capture;video tracking;multimedia;video processing;smacker video;internet privacy;world wide web;non-linear editing system	Vision	-15.047774429156606	-54.18746739701187	44860
23d2b82005741237714c5d21e4ad71b464dfa2e1	layering and merging linguistic annotations	anc data;output format;annotations in-line;linguistic annotation framework;stand-off markup;stand-off xml format compliant;linguistic annotation;iso tc37 sc4 wg1;american national corpus	The American National Corpus and its annotations are represented in a stand-off XML format compliant with the specifications of ISO TC37 SC4 WG1’s Linguistic Annotation Framework. Because few systems that enable search and access of the corpus currently support stand-off markup, the project has developed a SAX like parser that generates ANC data with annotations in-line, in a variety of output formats.	american national corpus;markup language;pipeline (computing);text corpus;xml	Keith Suderman;Nancy Ide	2006			natural language processing;computer science;database;information retrieval	NLP	-32.85833258741014	-74.99737198376303	44909
aff61bfbac75dbd3a4a5bd111f5bdaddefa60d69	a formal characterization of parsing word alignments by synchronous grammars with empirical evidence to the itg hypothesis		Deciding whether a synchronous grammar formalism generates a given word alignment (the alignment coverage problem) depends on finding an adequate instance grammar and then using it to parse the word alignment. But what does it mean to parse a word alignment by a synchronous grammar? This is formally undefined until we define an unambiguous mapping between grammatical derivations and word-level alignments. This paper proposes an initial, formal characterization of alignment coverage as intersecting two partially ordered sets (graphs) of translation equivalence units, one derived by a grammar instance and another defined by the word alignment. As a first sanity check, we report extensive coverage results for ITG on automatic and manual alignments. Even for the ITG formalism, our formal characterization makes explicit many algorithmic choices often left underspecified in earlier work.	bitext word alignment;data structure alignment;formal grammar;formal system;linker (computing);microsoft word for mac;parsing;sanity check;semantics (computer science);stochastic context-free grammar;turing completeness;undefined behavior	Gideon Maillette de Buy Wenniger;Khalil Sima'an	2013			natural language processing;synchronous context-free grammar;computer science;regular tree grammar;programming language;algorithm	NLP	-24.42527898883763	-75.57948657514974	44981
3b721259531045d5b45f37e0eaf52015e4628504	incremental recognition and referential categorization of french proper names		This paper presents Nemesis, a French proper name (PN) recognizer for Large-scale Information Extraction (IE), whose specifications have been elaborated through corpus investigation both in terms of referential categories and graphical structures. The graphical criteria are used to identify proper names and the referential classification to categorize them. The system is a classical one: it is rule-based and uses specialized lexicons without any linguistic preprocessing. Its originality consists on a modular architecture which includes a learning process. The system up to now recognizes anthroponyms and toponyms with performance achieving 95 % of precision and 90 % of recall.	categorization;finite-state machine;graphical user interface;information extraction;lexicon;logic programming;name;nemesis;preprocessor	Nordine Fourour;Emmanuel Morin;Béatrice Daille	2002			architecture;information extraction;categorization;artificial intelligence;natural language processing;proper noun;recall;preprocessor;computer science;modular design;originality	NLP	-31.030982070215398	-70.34704884050443	45009
db1c9e6f7d2f2d891ae94f8b4f10d8af32be9683	integrating information retrieval & neural networks		Due to the proliferation of information in databases and on the Internet, users are overwhelmed leading to Information Overload. It is impossible for humans to index and search such a wealth of information by hand so automated indexing and searching techniques are required. In this dissertation, we explore current Information Retrieval (IR) techniques and their shortcomings and we consider how more sophisticated approaches can be developed to aid retrieval. Current techniques can be slow due to the sheer volume of the search space although faster ones are being developed. Matching is often poor, as the quantity of retrievals does not necessarily indicate quality retrievals. Many current approaches simply return the documents containing the greatest number of `query words'. A methodology is desired to: process documents unsupervised; generate an index using a data structure that is memory efficient, speedy, incremental and scalable; identify spelling mistakes in the query and suggest alternative spellings; handle paraphrasing of documents and synonyms for both indexing and searching; to focus retrieval by minimising the search space; and, finally calculate the query-document similarity from statistics autonomously derived from the text corpus. We describe our IR system named MinerTaur, developed using both the AURA modular neural system and a hierarchical, growing self-organising neural technique based on Growing Cell Structures which we call TreeGCS. We integrate three modules in MinerTaur: a spell checker; a hierarchical thesaurus generated from corpus statistics inferred by the system; and, a word-document matrix to efficiently store the associations between the documents and their constituent words. We describe each module individually and evaluate each against comparative data structures and benchmark implementations. We identify improved memory usage, spelling recall accuracy, cluster quality and training and recall times for the modules. Finally we compare MinerTaur against a benchmark IR system, SMART developed at Cornell University, and reveal superior recall and precision for MinerTaur versus SMART.	artificial neural network;information retrieval	Victoria J. Hodge	2001			computer science;data science;data mining;information retrieval	ML	-33.41866292474974	-59.97885168729675	45010
a744dbf4f9618c5ee851fcf92bc44b1ef6938310	uc3m at trecvid 2010 semantic indexing task		This paper describes the experiments carried out by the UC3M team for the TRECVID 2010 high-level feature extraction task. In our previous participations in TRECVID, we have developed a modular system to facilitate the testing of several functionalities. This year we have selected a sim ple system configuration and we have added some elements expected to provide an additional advantage. For instance, 1) different kinds of histogram based features have been included in the system, both with a late fusion scheme and with a spatial pyramid matching configuration; 2) according to th e nature of the low level features, different kernels have bee n used to train a set of non-linear SVMs; and, 3) to combine the outputs of the trained SVMs, two different fusion strate gies have been taken into account, an average combination and a linear 1-norm SVM. Additionally, one of our runs exploits the relationships between the different categories by taking into account the provided taxonomy relationships, t hus modifying the final system output for some categories. To check the advantages provided by these new system elements, we have submitted the following runs for their evaluation in TRECVID 2010: • RUN 1 (“F A UC3M 1 1” ): this run is the baseline system configuration, where the local features have been included with a Spatial Pyramid Matching scheme and an average combination of the SVMs outputs has been considered. • RUN 2 (“F A UC3M 2 2” ): this run has the same configuration as RUN 1, but histogram based features have been included with a late fusion configuration. • RUN 3 (“F A UC3M 3 3” ): this run replicates the configuration of RUN 2, but a 1-norm SVM has been used for the late fusion stage. • RUN 4 (“F A UC3M 4 4” ): this last run modifies the final output of RUN 3 for a subset of classes according to the provided taxonomy relationships. This work is part of the i3media Project (CDTI 2007 1012) and i s partially funded by the Centro para el Desarrollo Tecnológico Industrial (CDTI), within the Ingenio 2010 Program. The four submitted runs have achieved average InfAP values from0.0457 (RUN 1) to0.0528 (RUN 3). Thus, our best run performed24 out of all87 submitted runs for this task.	baseline (configuration management);experiment;feature extraction;high- and low-level;nonlinear system;support vector machine;system configuration	Iván González-Díaz;Vanessa Gómez-Verdejo;Fernando Díaz-de-María;Jerónimo Arenas-García	2010			computer science;artificial intelligence;machine learning;data mining	ML	-9.558912190583554	-68.35016767631295	45040
6bd76f3cc19f8d7d1ba46d3d06297517545d9769	a comparative analysis of the search feature effectiveness of the major english and chinese search engines	google;google china;search feature;search engines;information retrieval;retrieval effectiveness;evaluation;baidu	Purpose – The purpose of this paper to investigate the effectiveness of selected search features in the major English and Chinese search engines and compare the search engines’ retrieval effectiveness. Design/approach/methodology – The search engines Google, Google China, and Baidu were selected for this study. Common search features such as title search, basic search, exact phrase search, PDF search, and URL search, were identified and used. Search results from using the five features in the search engines were collected and compared. One-way ANOVA and regression analysis were used to compare the retrieval effectiveness of the search engines. Findings – It was found that Google achieved the best retrieval performance with all five search features among the three search engines. Moreover Google achieved the best webpage ranking performance. Practical implications – The findings of this study improve the understanding of English and Chinese search engines and the differences between them in terms of search features, and can be used to assist users in choosing appropriate and effective search strategies when they search for information on the internet. Originality/value – The original contributions of this paper are that the Chinese and English search engines in both languages are compared for retrieval effectiveness. Five search features were evaluated, compared, and analysed in the two different language environments by using the discounted cumulative gain method.	phrase search;portable document format;qualitative comparative analysis;web page;web search engine	Jin Zhang;Wei Fei;Taowen Le	2013	Online Information Review	10.1108/OIR-07-2011-0099	google penalty;search engine indexing;full text search;organic search;metasearch engine;search engine optimization;computer science;spamdexing;evaluation;data mining;search analytics;world wide web;information retrieval;search engine	Web+IR	-33.55971526162955	-57.4498169136726	45048
a311383e10e6cf7cae38360e6ca2fb6075a5d0ef	influence of features discretization on accuracy of random forest classifier for web user identification		Web user identification based on linguistic or stylometric features helps to solve several tasks in computer forensics and cybersecurity, and can be used to prevent and investigate high-tech crimes and crimes where computer is used as a tool. In this paper we present research results on influence of features discretization on accuracy of Random Forest classifier. To evaluate the influence were carried out series of experiments on text corpus, contains Russian online texts of different genres and topics. Was used data sets with various level of class imbalance and amount of training texts per user. The experiments showed that the discretization of features improves the accuracy of identification for all data sets. We obtained positive results for extremely low amount of online messages per one user, and for maximum imbalance level.	computer forensics;computer security;discretization;experiment;random forest;stylometry;text corpus	Alisa A. Vorobeva	2017	2017 20th Conference of Open Innovations Association (FRUCT)	10.23919/FRUCT.2017.8071354	random forest;data mining;discretization;text corpus;computer forensics;machine learning;data set;computer science;artificial intelligence	ML	-21.258357244711146	-57.62858530447318	45070
fb6330fe5404f6e7ccc5461d69af1c90d2e591ce	grock: high-throughput docking using lcg grid tools	distributed computing;resource allocation;proteins;high throughput;object oriented programming;high availability;grid computing;resource manager;indexing terms	The study of interactions of proteins with other molecules is a major task to understand living organisms and design new drugs. GROCK is a portal that facilitates mass screening of potential molecular interactions in the life sciences. The main purpose for developing GROCK has been to facilitate users the performance of huge amounts of computational tasks using the power of the grid. In GROCK we have considered issues of high availability, redundancy, failure recovery and maximal exploitation of available grid resources. After trying various approaches we have settled for LCG-submitter, a tool developed for the physics LHC project to solve some of our goals. In this paper we introduce GROCK and analyze its design goals, the challenges found and the solutions we came up with to overcome them.	docking (molecular);high availability;high-throughput computing;interaction;large hadron collider;maximal set;throughput;worldwide lhc computing grid	David J. Garcia Aristegui;Patricia Méndez Lorenzo;José Ramón Valverde	2005	The 6th IEEE/ACM International Workshop on Grid Computing, 2005.		high-throughput screening;index term;resource allocation;computer science;bioinformatics;resource management;theoretical computer science;operating system;database;distributed computing;high availability;object-oriented programming;world wide web;grid computing	HPC	-5.284473280504929	-53.20495099100566	45130
c5d62893548a90dc371bd8a36c456510da24888a	normalized compression distance for visual analysis of document collections	multi dimensional projection;automatic generation;information presentation;multi dimensional;kolmogorov complexity;visualization technique;text collection visualization;visual analysis;normalized compression distance;similarity measure;document visualization;intermediate representation	In a world flooded by text of various sources, it is of strategic importance to find ways to map information present in written documents in a form that helps users locate and associate important information within a particular text data set. Content-based maps can support extremely useful explorations of text data sets. This paper proposes and evaluates the use of Kolmogorov complexity approximations as a means to detect similarity between general textual documents, in order to support mapping and visualization techniques for corpora exploration. The calculation of this similarity measure requires no intermediate representation of a corpus (such as vector representation) and therefore no pre-processing or parametrization steps. That makes it very attractive for a wider range of exploratory applications compared to conventional measures that need vector-based text representations. The visual layout used here is based on fast distance multi-dimensional projections. It is shown that the similarity measure and the resulting maps present very good precision and that the approach can be used successfully for visual analysis of automatically generated text maps.		Guilherme P. Telles;Rosane Minghim;Fernando Vieira Paulovich	2007	Computers & Graphics	10.1016/j.cag.2007.01.024	computer vision;visual analytics;information visualization;computer science;normalized compression distance;data mining;programming language;intermediate language;information retrieval	Vision	-17.95732951559554	-60.43843402394875	45215
ac54bc8adcc39e8903ec2a3a0185a071d6dfd33d	on the impact of semantic roles on text comprehension for question answering		New challenges for question answering are introduced by texts whose understanding require inference and commonsense knowledge. Task 11 - Machine comprehension using Commonsense Knowledge - from SemEval 2018 proposes a corpus of such texts, questions and answers. Since the predicates identified by Semantic Role Labeling aim to capture the semantic of a sentence, they seem appropriate to the task of text comprehension. We propose a Context-Novelty based model for identification of the correct answer for a question. This model relies on the SRL predicates of the text, question and answers and (i) it targets identification of the parts from the text which are relevant to the current question and (ii) it measures how well the answer matches that parts. The performance of the model was evaluated directly by counting the number of correctly answered questions and by its integration to a classical machine learning process.		Anca Marginean;Gabriela Pricop	2018		10.1007/978-3-030-05918-7_6	natural language processing;semeval;semantic role labeling;commonsense knowledge;question answering;comprehension;commonsense reasoning;inference;artificial intelligence;computer science;sentence	NLP	-27.290571998364168	-71.68867809894024	45226
c183fc3c197a34f9c1d6cbe1a810a37b00bf00ee	codeswitching language identification using subword information enriched word vectors.		Codeswitching is a widely observed phenomenon among bilingual speakers. By combining subword information enriched word vectors with linear-chain Conditional Random Field, we develop a supervised machine learning model that identifies languages in a English-Spanish codeswitched tweets. Our computational method achieves a tweet-level weighted F1 of 0.83 and a token-level accuracy of 0.949 without using any external resource. The result demonstrates that named entity recognition remains a challenge in codeswitched texts and warrants further work.	conditional random field;language identification;machine learning;named-entity recognition;substring;supervised learning;word embedding	Meng Xuan Xia	2016		10.18653/v1/W16-5818	natural language processing;linguistics	NLP	-22.76855120465145	-71.77151178739653	45264
b85c4d54fd4e3158a336e9eec15dbf5551cdd6b2	extractive summarization with swap-net: sentences and words from alternating pointer networks		We present a new neural sequence-tosequence model for extractive summarization called SWAP-NET (Sentences and Words from Alternating Pointer Networks). Extractive summaries comprising a salient subset of input sentences, often also contain important key words. Guided by this principle, we design SWAP-NET that models the interaction of key words and salient sentences using a new twolevel pointer network based architecture. SWAP-NET identifies both salient sentences and key words in an input document, and then combines them to form the extractive summary. Experiments on large scale benchmark corpora demonstrate the efficacy of SWAP-NET that outperforms state-of-the-art extractive summarizers.	benchmark (computing);experiment;pointer (computer programming);text corpus	Aishwarya Jadhav;Vaibhav Rajan	2018			natural language processing;automatic summarization;pointer (computer programming);computer science;artificial intelligence;machine learning;swap (finance)	NLP	-16.695224300628084	-73.47061147579463	45266
f6ec1fa2def777848485f13e4f9fcda04d6b9330	a unified neural architecture for joint dialog act segmentation and recognition in spoken dialog system		In spoken dialog systems (SDSs), dialog act (DA) segmentation and recognition provide essential information for response generation. A majority of previous works assumed ground-truth segmentation of DA units, which is not available from automatic speech recognition (ASR) in SDS. We propose a unified architecture based on neural networks, which consists of a sequence tagger for segmentation and a classifier for recognition. The DA recognition model is based on hierarchical neural networks to incorporate the context of preceding sentences. We investigate sharing some layers of the two components so that they can be trained jointly and learn generalized features from both tasks. An evaluation on the Switchboard Dialog Act (SwDA) corpus shows that the jointly-trained models outperform independently-trained models, single-step models, and other reported results in DA segmentation, recognition, and joint tasks.	artificial neural network;brill tagger;dialog system;ground truth;jsp model 2 architecture;sentence boundary disambiguation;speech recognition;spoken dialog systems;telephone switchboard;word embedding	Tianyu Zhao;Tatsuya Kawahara	2018			artificial intelligence;dialog act;natural language processing;architecture;computer science;dialog system;segmentation	NLP	-21.90153696457971	-75.37385909242298	45296
af00f835eecb988c88c418f0cb4f8ce96508dff0	fuzzy mining of multimedia genre applied to television archives	fuzzy multimedia mining;genre characterisation;multimodal content analysis;multimedia communication feature extraction data mining classification algorithms face streaming media tv;distribution channels;multimedia genre;multimedia mining;data mining;multimedia systems;television archives;fuzzy clustering;content analysis;user oriented access;distribution channel;streaming media;genre characterisation multimedia genre television archives multimedia content distribution channels user oriented access fuzzy multimedia mining genre classification;feature extraction;genre characterisation multimodal content analysis fuzzy clustering multimedia mining;multimedia communication;classification algorithms;classification system;pattern classification;information retrieval systems;face;tv;genre classification;multimedia content;pattern classification content based retrieval data mining information retrieval systems multimedia systems;content based retrieval	The quantity of multimedia content available either through the traditional distribution channels, such as broadcast television, or through new platforms like the Internet, is continuously increasing. Tools for user-oriented access to desired information are thus needed. In this paper, we illustrate a novel fuzzy multimedia mining technique for genre characterisation, aimed at overcoming limitations of conventional crisp classification systems. Effective results in genre classification and characterisation are presented based on the extraction of structural and cognitive properties of the content.	archive;internet;terrestrial television	Alberto Messina;Maurizio Montagnuolo	2008	2008 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2008.4607385	face;content analysis;fuzzy clustering;feature extraction;computer science;machine learning;multimedia;world wide web;information retrieval	DB	-15.471723154180017	-56.739557026972	45332
3163da3936317c3b977bc1e7028149d89c3c20ac	constructing a practical constituent parser from a japanese treebank with function labels		We present an empirical study on constructing a Japanese constituent parser, which can output function labels to deal with more detailed syntactic information. Japanese syntactic parse trees are usually represented as unlabeled dependency structure between bunsetsu chunks, however, such expression is insufficient to uncover the syntactic information about distinction between complements and adjuncts and coordination structure, which is required for practical applications such as syntactic reordering of machine translation. We describe a preliminary effort on constructing a Japanese constituent parser by a Penn Treebank style treebank semi-automatically made from a dependency-based corpus. The evaluations show the parser trained on the treebank has comparable bracketing accuracy as conventional bunsetsu-based parsers, and can output such function labels as the grammatical role of the argument and the type of adnominal phrases.	machine translation;natural language processing;parse tree;parser;semiconductor industry;treebank	Takaaki Tanaka;Masaaki Nagata	2013			natural language processing;treebank;programming language	NLP	-24.3207921548527	-75.56189888974205	45334
01425ee02723ec84fb21485a3bb069cd57e1da98	a syntactically and semantically tagged corpus of russian: state of the art and prospects		We describe a project aimed at creating a deeply annotated corpus of Russian texts. The annotation consists of comprehensive morphological marking, syntactic tagging in the form of a complete dependency tree, and semantic tagging within a restricted semantic dictionary. Syntactic tagging is using about 80 dependency relations. The syntactically annotated corpus counts more than 28,000 sentences and makes an autonomous part of the Russian National Corpus (www.ruscorpora.ru). Semantic tagging is based on an inventory of semantic features (descriptors) and a dictionary comprising about 3,000 entries, with a set of tags assigned to each lexeme and its argument slots. The set of descriptors assigned to words has been designed in such a way as to construct a linguistically relevant classification for the whole Russian vocabulary. This classification serves for discovering laws according to which the elements of various lexical and semantic classes interact in the texts. The inventory of semantic descriptors consists of two parts, object descriptors (about 90 items in total) and predicate descriptors (about a hundred). A set of semantic roles is thoroughly elaborated and contains about 50 roles. 1 The paper was partially supported by a grant No. 04-07-90179 from the Russian Foundation of Basic Research, which is gratefully acknowledged. In addition to the authors’ of the paper, Valentina Apresjan, Olga Boguslavskaya, Tatyana Krylova, Irina Levontina and Elena Uryson have contributed to the creation of the semantic dictionary and the system of descriptors. 1. Syntactic Tagging The paper is a progress report on a project aimed at creating a deeply annotated corpus of Russian texts. This corpus, jointly developed by two Moscow teams, is largely based on the ideology of an advanced MT system, ETAP-3 (Apresjan et al. 2003), and is so far the only corpus of Russian supplied with comprehensive morphological annotation and syntactic tagging in the form of a complete dependency tree provided for every sentence. Fig. 1 is a screenshot of the dependency tree for the sentence (1) Наибольшее возмущение участников митинга вызвал продолжающийся рост цен на бензин, устанавливаемых нефтяными компаниями ‘It was the continuing growth of petrol prices set by oil companies that caused the greatest indignation of the participants of the meeting’. Fig.1. A syntactically tagged sentence Here, nodes represent words assigned morphological and part-of-speech tags, whilst branches are labeled with names of syntactic links. The tagging uses about 80 surface-syntactic links; half of these were proposed in Mel’čuk’s Meaning ⇔ Text Theory (see e.g. Mel’čuk 1988) and the rest were adopted from the ETAP-3 system or specifically designed for the project. Annotation is produced semi-automatically: sentences are first processed by the rule-based Russian parser of ETAP-3 and then edited manually by linguists who handle all hard cases, including the cases of ambiguity that cannot be reliably resolved without extralinguistic knowledge, as well as versatile elliptical constructions, syntactic idiomaticity, and the like. Currently, the syntactically tagged corpus exceeds 28,000 sentences belonging to modern Russian texts of a variety of genres (fiction, popular science, newspaper and journal articles etc.) and is steadily growing. It is an integral but fully autonomous part of the Russian National Corpus developed in a nationwide research project and available on the Web (www.ruscorpora.ru).	autonomous robot;data descriptor;data dictionary;etap-3;item unique identification;lexicon;logic programming;olga (technology);part-of-speech tagging;russian national corpus;screenshot;semantic data model;semiconductor industry;syntactic predicate;tag (metadata);text corpus;treebank;vocabulary;while;world wide web	Juri D. Apresjan;Igor Boguslavsky;Boris Iomdin;Leonid L. Iomdin;Andrei Sannikov;Victor G. Sizov	2006			computer science;syntax;natural language processing;artificial intelligence;predicate (grammar);semantic role labeling;parsing;ambiguity;lexeme;annotation;sentence	NLP	-31.869628018122068	-75.02788920786863	45395
1a4791871fc3fdae02d4b53506d3f2ecefca36be	polish n-grams and their correction process	software;internet encyclopedias electronic publishing speech recognition software dictionaries;polish language word n gram statistic supervised correction;software engineering;polish language;internet;speech recognition software engineering;dictionaries;speech recognition;word n gram statistic;electronic publishing;encyclopedias;supervised correction	Word n-gram statistics collected from over 1 300 000 000 words are presented. Eventhough they were collected from various good sources, they contain several types of errors. The paper focuses on the process of partly supervised correction of the n- grams. Types of errors are described as well as our software allowing efficient and fast corrections.	grams;microsoft word for mac;n-gram;programming tool	Bartosz Ziólko;Dawid Skurzok;Ma&#x0142;gorzata Michalska	2010	2010 4th International Conference on Multimedia and Ubiquitous Engineering	10.1109/MUE.2010.5575068	natural language processing;the internet;speech recognition;computer science;machine learning;pattern recognition;polish;electronic publishing;encyclopedia	SE	-24.924436706994655	-79.00428176121433	45410
bbb9e23a9635215c937bc617fd875ae10321ff36	semantic video indexing and summarization using subtitles	semantic indexing;video summarization;retrieval;video retrieval;multimedia information system;video indexing;script vector;multimedia data;digital video;subtitles	How to build semantic index for multimedia data is an important and challenging problem for multimedia information systems. In this paper, we present a novel approach to build a semantic video index for digital videos by analyzing the subtitle files of DVD/DivX videos. The proposed approach for building semantic video index consists of 3 stages, viz., script extraction, script partition and script vector representation. First, the scripts are extracted from the subtitle files that are available in the DVD/DivX videos. Then, the extracted scripts are partitioned into segments. Finally, the partitioned script segments are converted into a tfidf vector based representation, which acts as the semantic index. The efficiency of the semantic index is demonstrated through video retrieval and summarization applications. Experimental results demonstrate that the proposed approach is very promising.	acm-mm;algorithm;automatic summarization;concept learning;digital video;filter (signal processing);image processing;information retrieval;information system;linear algebra;mpeg-7;tf–idf;viz: the computer game	Haoran Yi;Deepu Rajan;Liang-Tien Chia	2004		10.1007/978-3-540-30541-5_78	computer science;multimedia;video processing;world wide web;information retrieval	AI	-14.185549244578384	-56.84043493186778	45464
3d0adc6fca3a0669c108958c5d5204e2695ea4db	towards conversation entailment: an empirical investigation	long distance	While a significant amount of research has been devoted to textual entailment, automated entailment from conversational scripts has received less attention. To address this limitation, this paper investigates the problem of conversation entailment: automated inference of hypotheses from conversation scripts. We examine two levels of semantic representations: a basic representation based on syntactic parsing from conversation utterances and an augmented representation taking into consideration of conversation structures. For each of these levels, we further explore two ways of capturing long distance relations between language constituents: implicit modeling based on the length of distance and explicit modeling based on actual patterns of relations. Our empirical findings have shown that the augmented representation with conversation structures is important, which achieves the best performance when combined with explicit modeling of long distance relations.	explicit modeling;parsing;textual entailment	Joyce Yue Chai	2010			natural language processing;computer science;machine learning;linguistics	NLP	-27.996123499281346	-73.32475389844826	45468
19cf9495533ccb3e5cb4b2dab78815c97493c76d	fast annotation of video objects for interactive tv	video object;video processing;interactive tv;eurecom ecole d ingenieur telecommunication centre de recherche graduate school research center communication systems;object tracking;video annotation;video annotation tool;generic object tracker;object re detection;interactive television;real time application	In this demonstration, we present the Annotation Tool that is being developed in the porTiVity project to annotate video objects for Interactive Television programs. This tool includes various video processing components to structure and speed-up the annotation process, such as shot segmentation, key-frame extraction, object tracking and object redetection. A specific feature is that the tool includes a preprocessing phase where a quantity of information is precomputed, so that the annotation itself can be done quite rapidly.	key frame;precomputation;preprocessor;video processing	Helmut Neuschmied;Rémi Trichet;Bernard Mérialdo	2007		10.1145/1291233.1291268	computer vision;telecommunications;computer science;video capture;video tracking;multimedia;video processing;interactive television;computer graphics (images)	Vision	-15.39594291273156	-55.439173347734275	45508
ea030fcebcb42c11c886e474ec3052971ea0ea72	predicting correlations between lexical alignments and semantic inferences		While there is a strong intuition that word alignments (e.g. synonymy, hyperonymy) play a relevant role in recognizing textto-text semantic inferences (e.g. textual entailment, semantic similarity), this intuition is often not reflected in the system performances and there is a general need of a deeper comprehension of the role of lexical resources. This paper provides an empirical analysis of the dependencies between data-sets, lexical resources and algorithms that are commonly used in text-to-text inference tasks. We define a resource impact index, based on lexical alignments between pairs of texts, and show that such index is significantly correlated with the performance of different textual entailment algorithms. The result is an operational, algorithm-independent, procedure for predicting the performance of a class of available RTE algorithms.	approximation algorithm;lexical substitution;performance;semantic similarity;textual entailment	Simone Magnolini;Bernardo Magnini	2015			natural language processing;textual entailment;machine learning;semantic similarity;artificial intelligence;comprehension;computer science;synonym;inference;intuition	NLP	-25.774978500349313	-71.28371917130409	45519
cbcd6ce6b44f729acaf7b0e345c77901177c8ac7	pke: an open source python-based keyphrase extraction toolkit		We describe pke, an open source python-based keyphrase extraction toolkit. It provides an end-to-end keyphrase extraction pipeline in which each component can be easily modified or extented to develop new approaches. pke also allows for easy benchmarking of state-of-the-art keyphrase extraction approaches, and ships with supervised models trained on the SemEval-2010 dataset (Kim et al., 2010).	end-to-end principle;open-source software;python	Florian Boudin	2016			artificial intelligence;data mining;natural language processing;python (programming language);benchmarking;computer science	NLP	-32.75223882018126	-73.49326713460988	45582
0f5ca9de25ed028ac92a8f8950b0df1c789d01a4	a vertical search engine for school information based on heritrix and lucene	search engine;information retrieval;indexing;web crawling	The contents on the web are increasing exponentially as the rapid development of the Internet applications and services continues to expand. A problem in obtaining useful information from vast contents quickly and accurately is facing us while people are enjoying the convenience of the Internet. The immediate response to this problem is a Web Search Engine. We developed a vertical search engine for a certain domain like university. The search engine consists of Crawler, Indexer, and Searcher. The crawler component is implemented with Heritrix crawler based on the mechanism of recursion and archiving. A reusable, extensible index establishment and management subsystem are designed and implemented by open-source package named Lucene in the indexer component. An experiment has been done for Chungbuk National University web sites, and the number of documents the system retrieves is more than 4 hundred times on the average for typical keywords set than those from Google or university's search engines.	heritrix	Hyo-Bong Lee;Franco Nazareno;Seung-Hyun Jung;Wan-Sup Cho	2011		10.1007/978-3-642-24082-9_42	search engine indexing;database search engine;web search engine;search engine optimization;qwant;computer science;web crawler;database;search analytics;world wide web;information retrieval;search engine	AI	-31.338768802918047	-55.63504363895672	45584
8da5ef67a2aa3719e1d776f075b339ff71976bb9	fouille de graphes sous contraintes linguistiques pour l'exploration de grands textes (graph mining under linguistic constraints to explore large texts) [in french]		Graph Mining Under Linguistic Constraints to Explore Large Texts In this paper, we propose an approach to explore large texts by highlighting coherent sub-parts. The exploration method relies on a graph representation of the text according to the Hoey linguistic model which allows the selection and the binding of sentences in the graph. Our contribution relates to using graph mining techniques under constraints to extract relevant subparts of the text (i.e., collections of homogeneous sentence sub-networks). We have conducted some experiments on two large English texts to show the interest of the proposed approach. MOTS-CLÉS : Fouille de graphes, réseaux phrastiques, analyse textuelle, navigation textuelle.	coherence (physics);experiment;graph (abstract data type);structure mining	Solen Quiniou;Peggy Cellier;Thierry Charnois;Dominique Legallois	2012			art;art history;linguistics;literature	NLP	-26.679579896489273	-64.49712472080421	45614
1a502c802a321e85f2878acf116be298be2a948b	harvesting multi-word expressions from parallel corpora		The paper presents a set of approaches to extend the automatically created Slovene wordnet with nominal multi­word expressions. In the first approach multi­word expressions from Princeton WordNet are translated with a technique that is based on word­alignment and lexico­syntactic  patterns.  This  is  followed  by  extracting  new  terms  from  a  monolingual  corpus  using  keywordness  ranking  and contextual patterns. Finally, the multi­word expressions are assigned a hypernym and added to our wordnet. Manual evaluation and comparison of the results shows that  the translation approach is the most straightforward and accurate. However,  it  is successfully complemented by the two monolingual approaches which are able to identify more term candidates in the corpus that would otherwise go unnoticed. Some weaknesses of the proposed wordnet extension techniques are also addressed.	parallel text;text corpus;wordnet	Spela Vintar;Darja Fiser	2008			artificial intelligence;natural language processing;contextual patterns;expression (mathematics);computer science;wordnet;ranking	NLP	-28.104297470908083	-70.93446979778669	45648
6163f0af41268722cf57ded21013ae7a19c0c511	semantic web for content based video retrieval	pediatrics;video retrieval content based retrieval feature extraction indexing search engines semantic web;search engines;video retrieval;resource description framework;data mining;video indexing;video storage semantic web content based video retrieval video search engine automatic feature extraction video indexing;video storage;indexing;video annotation semantic web content based video retrieval;streaming media;feature extraction;semantic web;video annotation;content based video retrieval;video search engine;semantic web content based retrieval feature extraction indexing search engines data mining relational databases mpeg 7 standard videoconference multimedia databases;content based retrieval;automatic feature extraction	This paper aims to provide a semantic web based video search engine. Currently, we do not have scalable integration platforms to represent extracted features from videos, so that they could be indexed and searched. The task of indexing extracted features from videos is a difficult challenge, due to the diverse nature of the features and the temporal dimensions of videos. We present a semantic web based framework for automatic feature extraction, storage, indexing and retrieval of videos. Videos are represented as interconnected set of semantic resources. Also, we suggest a new ranking algorithm for finding related resources which could be used in a semantic web based search engine.	algorithm;feature extraction;scalability;semantic web;web search engine	Sancho C. Sebastine;Bhavani M. Thuraisingham;B. Prabhakaran	2009	2009 IEEE International Conference on Semantic Computing	10.1109/ICSC.2009.49	search engine indexing;semantic computing;semantic search;feature extraction;semantic grid;computer science;semantic web;rdf;social semantic web;semantic web stack;multimedia;world wide web;information retrieval;semantic analytics	Vision	-14.728107923576328	-56.46404022659688	45649
23d166fa4a9422da573cf5373e6a3c0c70a8abcb	issues and challenges in developing statistical pos taggers for sambalpuri		Low-density languages are also known as lesser-known, poorly-described, less-resourced, minority or less-computerized language because they have fewer resources available. Collection and annotation of a voluminous corpus for the purpose of NLP application for these languages prove to be quite challenging. For the development of any NLP application for a low-density language, one needs to have an annotated corpus and a standard scheme for annotation. Because of their non-standard usage in text and other linguistic nuances, they pose significant challenges that are of linguistic and technical in nature. The present paper highlights some of the underlying issues and challenges in developing statistical POS taggers applying SVM and CRF++ for Sambalpuri, a less-resourced Eastern Indo-Aryan language. A corpus of approximately 121 k is collected from the web and converted into Unicode encoding. The whole corpus is annotated under the BIS (Bureau of Indian Standards) annotation scheme devised for Odia under the ILCI (Indian Languages Corpora Initiative) Project. Both the taggers are trained and tested with approximately 80 k and 13 k respectively. The SVM tagger provides 83% accuracy while the CRF++ has 71.56% which is less in comparison to the former.		Pitambar Behera;Atul Kr. Ojha;Girish Nath Jha	2015		10.1007/978-3-319-93782-3_28	support vector machine;natural language processing;unicode;annotation;computer science;artificial intelligence	EDA	-29.38516891849354	-74.35435400670002	45671
0290ceace79be396b603b294994c0879999aac13	"""the """"la sapienza"""" question answering system at trec 2006"""	question answering;question answering system;system performance	This report describes the system developed at the University of Rome “La Sapienza” for the TREC-2006 question answering evaluation exercise. The backbone of this QA system is linguistically-principled: Combinatory Categorial Grammar is used to generate syntactic analyses of questions and potential answer snippets, and Discourse Representation Theory is employed as formalism to match the meanings of questions and answers. The key idea of the La Sapienza system is to use semantics to prune answer candidates, thereby exploiting lexical resources such as WordNet and NomLex to facilitate the selection of answers. The system performed reasonably well at TREC2006: in the per-series evaluation it performed slightly above the median accuracy score of all participating systems.	combinatory categorial grammar;document retrieval;google questions and answers;internet backbone;linear algebra;location-based game;mark steedman;named entity;question answering;san diego supercomputer center;semantics (computer science);wordnet	Johan Bos	2006			combinatory categorial grammar;information retrieval;discourse representation theory;data mining;syntax;question answering;semantics;natural language processing;formalism (philosophy);wordnet;computer science;artificial intelligence	NLP	-27.154820611570454	-72.97212187988382	45751
08e08db374823c611065b17c1bc57f9cf5eb8027	unsupervised learning of arabic stemming using a parallel corpus	unsupervised learning;parallel text;proprietary stemmer;unsupervised learning approach;resource-frugal approach result;english stemmer;proprietary arabic stemmer;unstemmed text;parallel corpus;arabic information retrieval;unannotated text;human annotated text;information retrieval	This paper presents an unsupervised learning approach to building a non-English (Arabic) stemmer. The stemming model is based on statistical machine translation and it uses an English stemmer and a small (10K sentences) parallel corpus as its sole training resources. No parallel text is needed after the training phase. Monolingual, unannotated text can be used to further improve the stemmer by allowing it to adapt to a desired domain or genre. Examples and results will be given for Arabic , but the approach is applicable to any language that needs affix removal. Our resource-frugal approach results in 87.5% agreement with a state of the art, proprietary Arabic stemmer built using rules, affix lists, and human annotated text, in addition to an unsupervised component. Task-based evaluation using Arabic information retrieval indicates an improvement of 38% in average precision over unstemmed text, and 96% of the performance of the proprietary stemmer above.	ibm notes;information retrieval;parallel text;statistical machine translation;stemming;unsupervised learning	Monica Rogati;J. Scott McCarley;Yiming Yang	2003			natural language processing;unsupervised learning;speech recognition;computer science;machine learning;pattern recognition	NLP	-23.102551261612216	-74.99314970346335	45766
6730f754ab0a84f88d9f4aa3510d056d356c439c	classification of short comments by weighted tree kernels using the hierarchy of wikipedia	tree kernel;wikipedia;naive bayes;support vector;twitter	With the rise of Web services for posting tourist facility and product reviews, and short comments such as Twitter, there are increasing calls for a method that alleviates the burden of creating corpus and identifying text both automatically and accurately. Further, in the lecture questionnaires used for university lectures, the free reply answers are all short text, and classification of the short text could be used in evaluating lectures. Until now, most methods have used the appearance frequency of vocabulary, based on Bag-of-Words, as the basic method of classifying text. In this study, however, we converted short comments, such as tweets, into tree structure data for category classification, and classified according to a weighted tree kernel, in which weight is further attached to the tree node sections on the tree kernel.	bag-of-words model in computer vision;tree structure;vocabulary;web service;wikipedia	Mashahiro Takeda;Nobuyuki Kobayashi;Hiromitsu Shiina	2017		10.1145/3022227.3022310	support vector machine;naive bayes classifier;computer science;machine learning;pattern recognition;incremental decision tree;data mining;brand;database;tree kernel;world wide web	NLP	-22.515595035761947	-60.864435423704144	45791
b3fa409752bcf04485b47acd9f2796c8bf310e17	medical image retrieval using bag of meaningful visual words: unsupervised visual vocabulary pruning with plsa	medical image retrieval;language modelling;bag of visual words	Content--based medical image retrieval has been proposed as a technique that allows not only for easy access to images from the relevant literature and electronic health records but also for training physicians, for research and clinical decision support. The bag-of-visual-words approach is a widely used technique that tries to shorten the semantic gap by learning meaningful features from the dataset and describing documents and images in terms of the histogram of these features. Visual vocabularies are often redundant, over--complete and noisy. Larger than required vocabularies lead to high--dimensional feature spaces, which present important disadvantages with the curse of dimensionality and computational cost being the most obvious ones. In this work a visual vocabulary pruning technique is presented. It enormously reduces the amount of required words to describe a medical image dataset with no significant effect on the accuracy. Results show that a reduction of up to 90% can be achieved without impact on the system performance. Obtaining a more compact representation of a document enables multimodal description as well as using classifiers requiring low--dimensional representations.	accessibility;algorithmic efficiency;bag-of-words model in computer vision;clinical decision support system;computation;curse of dimensionality;image retrieval;medical imaging;multimodal interaction;probabilistic latent semantic analysis;vocabulary	Antonio Foncubierta-Rodríguez;Alba Garcia Seco de Herrera;Henning Müller	2013		10.1145/2505323.2505336	natural language processing;visual word;computer science;pattern recognition;bag-of-words model in computer vision;information retrieval	Vision	-10.745014342079646	-65.96200036694941	45792
c111d243e103269cbc792950a601b4cb0ac10494	fint: find images and text	feature vector	In this article, we describe the FINT system, which stands for Find Images aNd Text. This system is built within the VindIT project, that focuses on handling large amounts of multi-media data. The current approach concentrates on searching in a combination of textual and visual data. The system described here is an iterative system that computes distances between the images. From each image (and corresponding case), a feature vector is extracted. The distances are now computed using these feature vectors. The distance computation can be different in each step of the iterative system. Here we will describe the system and settings that were used in the medical retrieval task of the ImageCLEF 2004 competition.	computation;feature vector;iteration;iterative method;machine learning	Menno van Zaanen;Guido C. H. E. de Croon	2004			computation;artificial intelligence;feature vector;pattern recognition;computer science	ML	-7.93577410088479	-69.16600722412974	45800
2c46522aaf2ab17780bd928a8ef7e069acf57b9c	lexical ambiguity resolution for turkish in direct transfer machine translation models	metodo directo;modelizacion;traduccion automatica;analisis estadistico;q science general;probabilistic approach;lengua blanco;machine transfert;desambiguisacion;modelisation;analyse syntaxique;traduction automatique;statistical analysis;analisis sintaxico;polisemia;enfoque probabilista;approche probabiliste;syntactic analysis;target language;langue cible;analyse statistique;ambiguity resolution;disambiguation;polysemy;polysemie;ambiguity;statistical language model;transfer machine;maquina transferencia;desambiguisation;discriminacion;ambiguedad;modeling;methode directe;direct method;machine translation;discrimination;ambiguite;automatic translation	This paper presents a statistical lexical ambiguity resolution method in direct transfer machine translation models in which the target language is Turkish. Since direct transfer MT models do not have full syntactic information, most of the lexical ambiguity resolution methods are not very helpful. Our disambiguation model is based on statistical language models. We have investigated the performances of some statistical language model types and parameters in lexical ambiguity resolution for our direct transfer MT system.	compiler;language model;machine translation;performance;word-sense disambiguation	A. Cüneyd Tantug;Esref Adali;Kemal Oflazer	2006		10.1007/11902140_26	direct method;natural language processing;discrimination;speech recognition;systems modeling;transfer-based machine translation;computer science;parsing;machine translation;programming language;statistics	NLP	-24.041200205090654	-79.94741788023858	45842
c75fb6e52597d26143cd9e2310c5c4e69edf0814	syntactic and semantic knowledge in the delphi unification grammar	delphi unification grammar;harc system;integrated syntax;semantic analysis;semantics unification grammar;bottom-up parser;recent natural language work;byblos system;semantic knowledge;bbn spoken language system;discourse module;natural language system	This paper presents recent natural language work on HARC, the BBN Spoken Language System. The HARC system incorporates the Byblos system [6] as its speech recognition component and the natural language system Delphi, which consists of a bottom-up parser paired with an integrated syntax/semantics unification grammar, a discourse module, and a database question-answering backend. The paper focuses on the syntactic and semantic analyses made in the grammar.	bottom-up parsing;han unification;natural language;question answering;speech recognition;unification (computer science)	Robert J. Bobrow;Robert Ingria;David Stallard	1990			natural language processing;generative grammar;computer science;syntactic predicate;emergent grammar;linguistics;relational grammar;programming language;attribute grammar;mildly context-sensitive grammar formalism	NLP	-29.5666450995736	-77.84451557990836	45858
f0b501f50cc19dcfa30805bd56bc5241d8b15f11	pos-rs: a random subspace method for sentiment classification based on part-of-speech analysis	sentiment classification;ensemble learning;random subspace;part of speech	The rise of social media has fueled interest in sentiment classification.POS-RS is proposed for sentiment analysis based on part-of-speech analysis.Ten public datasets were investigated to verify the effectiveness of POS-RS.Experimental results reveal POS-RS can be used as a viable method. With the rise of Web 2.0 platforms, personal opinions, such as reviews, ratings, recommendations, and other forms of user-generated content, have fueled interest in sentiment classification in both academia and industry. In order to enhance the performance of sentiment classification, ensemble methods have been investigated by previous research and proven to be effective theoretically and empirically. We advance this line of research by proposing an enhanced Random Subspace method, POS-RS, for sentiment classification based on part-of-speech analysis. Unlike existing Random Subspace methods using a single subspace rate to control the diversity of base learners, POS-RS employs two important parameters, i.e. content lexicon subspace rate and function lexicon subspace rate, to control the balance between the accuracy and diversity of base learners. Ten publicly available sentiment datasets were investigated to verify the effectiveness of proposed method. Empirical results reveal that POS-RS achieves the best performance through reducing bias and variance simultaneously compared to the base learner, i.e., Support Vector Machine. These results illustrate that POS-RS can be used as a viable method for sentiment classification and has the potential of being successfully applied to other text classification problems.	random subspace method;voice analysis	Gang Wang;Zhu Zhang;Jianshan Sun;Shanlin Yang;Catherine A. Larson	2015	Inf. Process. Manage.	10.1016/j.ipm.2014.09.004	random subspace method;part of speech;computer science;machine learning;pattern recognition;data mining;linguistics;ensemble learning;world wide web;information retrieval;sentiment analysis;statistics	NLP	-20.179061023319488	-64.76181315225877	45869
a1b5984f597407348c54c4afd7d297d4fb7f7e73	on privacy preservation in text and document-based active learning for named entity recognition	active learning;privacy preservation;supervised machine learning;active machine learning;named entity recognition;machine learning;named entity recognizer;data annotation;named entity	The preservation of the privacy of persons mentioned in text requires the ability to automatically recognize and identify names. Named entity recognition is a mature field and most current approaches are based on supervised machine learning techniques. Such learning requires the presence of labeled examples on which to train; training examples are usually provided to the learner on the form of annotated corpora. Creating and annotating corpora is a tedious, meticulous and error prone process; obtaining good training examples is a hard task in itself. This paper describes the development and in-depth empirical investigation of a method, called BootMark, for bootstrapping the marking up of named entities in textual documents. Experimental results show that BootMark requires a human annotator to manually annotate fewer documents in order to produce a named entity recognizer with a given performance, than would be needed if the documents forming the basis for the recognizer were randomly drawn from the same corpus. The investigation further indicates that the primary gain obtained by BootMark compared to passive learning is in terms of higher recall. Thus, it is argued, the recognizers are suitable for use in privacy preservation applications.	active learning (machine learning);case preservation;cognitive dimensions of notations;document;finite-state machine;item unique identification;machine learning;named entity;privacy;randomness;supervised learning;text corpus	Fredrik Olsson	2009		10.1145/1651449.1651460	semi-supervised learning;natural language processing;computer science;pattern recognition;data mining;entity linking;active learning;active learning	NLP	-26.390942576652044	-69.7291350414181	45873
b6bc6bffcbd08af527d5d0c7a8e0fbe20631b980	video editing support system based on video grammar and content analysis	digital video broadcasting;video databases;material videos;history;metadata;content based retrieval video recording video databases knowledge based systems computational complexity;layout;systems engineering and theory;support system;conducting materials;content analysis;intelligent support system;history cameras intelligent systems layout informatics systems engineering and theory digital video broadcasting production systems conducting materials joining processes;computational complexity;video editing;video recording;intelligent systems;joining processes;production systems;video grammar;informatics;metadata video editing support system video grammar content analysis material videos intelligent support system;video editing support system;content based retrieval;knowledge based systems;cameras	Video editing is the work to produce the final videos with certain duration by finding and selecting appropriate shots from the material videos and connecting them. In order to produce the excellent videos, this process is generally conducted according to the special rules called “video grammar”. In this paper, we propose an intelligent support system for the video editing where metadata are extracted automatically and then the video grammars are applied to the extracted metadata.		Masahito Kumano;Yasuo Ariki;Miki Amano;Kuniaki Uehara;Kenji Shunto;Kiyoshi Tsukada	2002		10.1109/ICPR.2002.1048481	video compression picture types;layout;computer vision;post-production;content analysis;computer science;video tracking;multimedia;video processing;production system;smacker video;computational complexity theory;informatics;metadata;world wide web;digital video broadcasting;non-linear editing system	HCI	-14.206721284737247	-55.14256866138912	45879
1b2ae3cdfe4304f271c4cfff9fa623a7f0c4d802	detection of web subsites: concepts, algorithms, and evaluation issues	cluster algorithm;information retrieval clustering algorithms size measurement internet distortion measurement intelligent structures world wide web data visualization web search testing;information retrieval;size measurement;testing;distortion measurement;user profiling;internet;web information retrieval;clustering;data visualization;clustering algorithms;world wide web;web search;incremental web mining;intelligent structures	The Internet is a vast resource of information. Unfortunately, finding and accessing this information is often a very cumbersome task even with existing information platforms. Searching on the WWW suffers from the fact that almost every word is ambiguous to a certain degree in the information-rich environment of the Internet. Clustering search results is a way to solve this problem. This paper demonstrates how to employ novel Information Retrieval measures to derive optimal parametrizations for a cluster algorithm.	algorithm;information retrieval;internet;www	Eduarda Mendes Rodrigues;Natasa Milic-Frayling;Blaz Fortuna	2007	IEEE/WIC/ACM International Conference on Web Intelligence (WI'07)	10.1109/WI.2007.107	cognitive models of information retrieval;computer science;machine learning;data mining;cluster analysis;world wide web;information retrieval;data visualization;human–computer information retrieval	DB	-30.010554238341854	-56.542860286396206	45889
2fba730f3b9e97df1bd14f819cf8af297b675360	effective web-service discovery using k-means clustering		Web Services are proving to be a convenient way to integrate distri- buted software applications. As service-oriented architecture is getting popular, vast numbers of web services have been developed all over the world. But it is a challenging task to find the relevant or similar web services using web servic- es registry such as UDDI. Current UDDI search uses keywords from web ser- vice and company information in its registry to retrieve web services. This in- formation cannot fully capture user's needs and may miss out on potential matches. Underlying functionality and semantics of web services need to be considered. In this study, we explore the resemblance among web services us- ing WSDL document features such as WSDL Content and Web Services name. We compute the similarity of web services and use this data to generate clusters using K-means clustering algorithm. This approach has really yielded good re- sults and can be efficiently used by any web service search engine to retrieve similar or related web services.	computer cluster;k-means clustering;service discovery	A. Santhana Vijayan;S. R. Balasundaram	2013		10.1007/978-3-642-36071-8_36	web service;web application security;web development;web modeling;data web;web analytics;web mapping;web design;web standards;computer science;ws-policy;web navigation;social semantic web;data mining;ws-addressing;database;web intelligence;ws-i basic profile;web 2.0;world wide web;universal description discovery and integration	ML	-28.593284266231656	-57.26035524599745	45947
6406dda4141addfb6686479c9a316e89f1482d37	natural language processing with neural networks	unsupervised learning;unsupervised learning natural languages neural nets;neural nets;semantic maps learning based natural language processing neural networks parallel distributed learning processing machines unsupervised learning devices supervised learning devices part of speech tagging error detection annotated corpora self organization;natural languages;semantic mapping;distributed learning;natural language processing neural networks;part of speech;self organization;error detection;natural language processing;neural network	With learning-based natural language processing (NLP) becoming the main-stream of NLP research, neural networks (NNs), which are powerful parallel distributed learning/processing machines, should attract more attention from both NN and NLP researchers and can play more important roles in many areas of NLP. This paper tries to reveal the true power of NNs for NLP applications as supervised or unsupervised learning devices by concretely introducing three practical applications: part of speech (POS) tagging, error detection in annotated corpora, and self-organization of semantic maps.	artificial neural network;natural language processing	Qing Ma	2002		10.1109/LEC.2002.1182290	semi-supervised learning;natural language processing;unsupervised learning;computer science;machine learning;pattern recognition;deep learning;competitive learning;artificial neural network	ML	-19.990761977378288	-71.7179289966272	45954
5fbb13945350aa00b9ac55637a53d41b2bfb7bd6	multimodal analysis of user behavior and browsed content under different image search intents		The motivation or intent of a search for content may vary between users and use-cases. Knowledge and understanding of these underlying objectives may therefore be important in order to return appropriate search results, and studies of user search intent are emerging in information retrieval to understand why a user is searching for a particular type of content. In the context of image search, our work targets automatic recognition of users’ intent in an early stage of a search session. We have designed seven different search scenarios under the intent conditions of finding items, re-finding items and entertainment. Moreover, we have collected facial expressions, physiological responses, eye gaze and implicit user interactions from 51 participants who performed seven different search tasks on a custom-built image retrieval platform, and we have analyzed the users’ spontaneous and explicit reactions under different intent conditions. Finally, we trained different machine learning models to predict users’ search intent from the visual content of the visited images, the user interactions and the spontaneous responses. Our experimental results show that after fusing the visual and user interaction features, our system achieved the F-1 score of 0.722 for classifying three classes in a user-independent cross-validation. Eye gaze and implicit user interactions, including mouse movements and keystrokes are the most informative features for intent recognition. In summary, the most promising results are obtained by modalities that can be captured unobtrusively and online, and the results therefore demonstrate the potential of including intent-based methods in multimedia retrieval platforms.	browsing;categorization;cross-validation (statistics);event (computing);image retrieval;information retrieval;machine learning;multimodal interaction;session (web analytics);software deployment;spontaneous order	Mohammad Soleymani;Michael Riegler;Pål Halvorsen	2018	International Journal of Multimedia Information Retrieval	10.1007/s13735-018-0150-6	modalities;machine learning;artificial intelligence;image retrieval;eye tracking;facial expression;computer science	Web+IR	-6.319823248810959	-72.683940620584	45978
beed54dde24149697d12a88b2a3b22ebb85f8717	clarit experiments in batch filtering: term selection and threshold optimization in ir and svm filters		The Clairvoyance team participated in the Filtering Track, submitting two runs in the Batch Filtering category. While we have been exploring the question of both topic modeling and ensemble filter construction (as in our previous TREC filtering experiments [5]), we had one distinct objective this year, to explore the viability of monolithic filters in classification-like tasks. This is appropriate to our work, in part, because monolithic filters are a crucial starting point for ensemble filtering, and it is possible for them to contribute substantially in the ensemble approach. Our primary goal in experiments this year, thus, was to explore two issues in monolithic filter construction: (1) term count selection and (2) filter threshold optimization.	experiment;kalman filter;mathematical optimization;particle filter;texture filtering;topic model	David A. Evans;James G. Shanahan;Norbert Roma;Jeffrey L Bennett;Victor Sheftel;Emilia Stoica;Jesse Montgomery;David A. Hull;Waibhav Tembe	2002			topic model;data mining;filter (signal processing);support vector machine;machine learning;computer science;algorithm;artificial intelligence	Web+IR	-8.518949247624695	-65.46722118390286	46014
2f46588b45dafd7c0fbe728e96a887f1d0cd69bf	bridging concept identification for constructing information networks from text documents	magnesium;text mining;bridging concept identification;information networks;pubmed;migraine;knowledge discovery	A major challenge for next generation data mining systems is creative knowledge discovery from diverse and distributed data sources. In this task an important challenge is information fusion of diverse mainly unstructured representations into a unique knowledge format. This chapter focuses on merging information available in text documents into an information network --- a graph representation of knowledge. The problem addressed is how to efficiently and effectively produce an information network from large text corpora from at least two diverse, seemingly unrelated, domains. The goal is to produce a network that has the highest potential for providing yet unexplored cross domain links which could lead to new scientific discoveries. The focus of this work is better identification of important domain bridging concepts that are promoted as core nodes around which the rest of the network is formed. The evaluation is performed by repeating a discovery made on medical articles in the migraine magnesium domain.	bridging (networking)	Matjaz Jursic;Borut Sluban;Bojan Cestnik;Miha Grcar;Nada Lavrac	2012		10.1007/978-3-642-31830-6_6	computer science;data science;data mining;information retrieval	ML	-26.597123342706283	-57.79069355899103	46081
3fa2b1ea36597f2b3055844dcf505bacd884f437	confabulation based sentence completion for machine reading	unsupervised learning;human information processing;hash table confabulation based sentence completion words filling reading comprehension indispensible component machine reading cogent confabulation bioinspired computational model human information processing confabulation knowledge base unsupervised machine learning algorithm cogent confabulation model training algorithm;hash table cogent confabulation machine reading sentence completion unsupervised learning;word processing file organisation unsupervised learning;knowledge based system;complexity theory;computer model;reading;training;reading comprehension;learning machines;symposia;performance improvement;computational modeling;shape;unsupervised machine learning;hash table;information processing;training knowledge based systems neurons merging complexity theory computational modeling shape;merging;cogent confabulation;algorithms;neurons;machine reading;knowledge based systems;sentence completion;training algorithm;word processing;knowledge base;file organisation	Sentence completion and prediction refers to the capability of filling missing words in any incomplete sentences. It is one of the keys to reading comprehension, thus making sentence completion an indispensible component of machine reading. Cogent confabulation is a bio-inspired computational model that mimics the human information processing. The building of confabulation knowledge base uses an unsupervised machine learning algorithm that extracts the relations between objects at the symbolic level. In this work, we propose performance improved training and recall algorithms that apply the cogent confabulation model to solve the sentence completion problem. Our training algorithm adopts a two-level hash table, which significantly improves the training speed, so that a large knowledge base can be built at relatively low computation cost. The proposed recall function fills missing words based on the sentence context. Experimental results show that our software can complete trained sentences with 100% accuracy. It also gives semantically correct answers to more than two thirds of the testing sentences that have not been trained before.	algorithm;british informatics olympiad;computation;computational model;display resolution;hash table;information processing;knowledge base;machine learning;natural language understanding;open reading frame	Qinru Qiu;Qing Wu;Daniel J. Burns;Michael J. Moore;Robinson E. Pino;Morgan Bishop;Richard W. Linderman	2011	2011 IEEE Symposium on Computational Intelligence, Cognitive Algorithms, Mind, and Brain (CCMB)	10.1109/CCMB.2011.5952109	natural language processing;computer science;artificial intelligence;machine learning	AI	-19.03584361274713	-75.2800014913671	46086
9963550dff4bae2d7074d92c75cb3802fc7c7126	the contribution of the university of alicante to ave 2007		In this paper we discuss a system used to recognize entailment relations within the AVE framework. This system creates representations of text snippets by means of a variety of lexical measures and syntactic structures. Once these representations have been created, we compare the corresponding to the text and to the hypothesis and we try to determine if there is an entailment relation between the text and the hypothesis. The hypotheses have been generated by merging the answers with their corresponding questions, applying a set of regular expression aimed at this issue. In the performed experiments our system obtained a maximum F-measure score of 0.40 and 0.39 for the development and test English corpora, respectively.	experiment;f1 score;regular expression;text corpus	Óscar Ferrández;Daniel Micol;Rafael Muñoz;Manuel Palomar	2007			syntax;merge (version control);natural language processing;regular expression;logical consequence;mathematics;artificial intelligence	NLP	-25.833960443791067	-73.34765586757209	46169
3dc3a4a83e2bfe08c6a24a24ae11103477cbd4c7	hierarchical topic segmentation of websites	website segmentation;gain ratio;efficient algorithm;kl distance;classification;tree partitioning;website hierarchy;facility location	In this paper, we consider the problem of identifying and segmenting topically cohesive regions in the URL tree of a large website. Each page of the website is assumed to have a topic label or a distribution on topic labels generated using a standard classifier. We develop a set of cost measures characterizing the benefit accrued by introducing a segmentation of the site based on the topic labels. We propose a general framework to use these measures for describing the quality of a segmentation; we also provide an efficient algorithm to find the best segmentation in this framework. Extensive experiments on human-labeled data confirm the soundness of our framework and suggest that a judicious choice of cost measures allows the algorithm to perform surprisingly accurate topical segmentations.	algorithm;experiment;image segmentation;off topic;statistical classification;text segmentation	Ravi Kumar;Kunal Punera;Andrew Tomkins	2006		10.1145/1150402.1150433	biological classification;computer science;information gain ratio;facility location problem;machine learning;pattern recognition;data mining;kullback–leibler divergence;scale-space segmentation;world wide web;statistics	Web+IR	-18.541470235664907	-61.77749893695457	46172
4d58fbb388884caf78a0652a02ee279883d86f75	extraction of semantic information from an ordinary english dictionary and its evaluation	magnetic tape version;ordinary english dictionary;extraction program;automatic extraction;odinary english dictionary;contemporary english;semantic marker;definition sentence;semantic relationship;semantic information;longman dictionary;relational database system;pattern matching	The ;~ ~tomatlc e~tractimt o~ scntar~tie ilthlrmatio~, ca-. pec{a.]ly ~emrmtic rel~tionships be~wemt words, from sat o~:di~l~'y I~h~glish dictionaty it~ described. For the extra,trim b the mag,~etic tape re:eaton or' I, DOCE (l,oagman f)ictic,~ r y of (Jmttempotary E~glish, 1978 editimt) is loaded b~to a ~:elatk,aa] database system. Developed exgractimt pro!.,,~:;u~ts a.mdyze a definition se:uteltce in I,I)OCE with a pa,~te:r~t ~m~tching based algo*ithm. Si,tce this ~lgofithm is no t pe*[e(:t, the *esalt of ¢]te extra.el.trot h a~; been corn IJa.~:ed with sem~/ltic b, formatimt (sema.ntic markers) which gba ma.gnetic tape version o~ LI)OCI~ eontaiim. The zesnlt oi comparismt i~'~ a,b;o discussed for evaluating the !:e]iability of aucl~ ;,.tt alttontatic e×traetion.	data dictionary;database;disjunctive normal form;mag (cryptography);relational database management system;thesaurus	Jun-ichi Nakamura;Makoto Nagao	1988			natural language processing;relationship extraction;semantic computing;relational database management system;speech recognition;computer science;pattern matching;programming language;information retrieval	DB	-30.618081970110275	-78.13006758792376	46175
1b0c6769422716f77e22e3ff2bbb1553840b9fbf	a system for extracting top-k lists from the web	web pages;top k lists;list extraction;web mining;high frequency;web information extraction;structured data;knowledge base	"""List data is an important source of structured data on the web. This paper is concerned with """"top-k"""" pages, which are web pages that describe a list of k instances of a particular topic or concept. Examples include """"the 10 tallest persons in the world"""" and """"the 50 hits of 2010 you don't want to miss"""". Compared to normal web list data, """"top-k"""" lists contain richer information and are easier to understand. Therefore the extraction of such lists can help enrich existing knowledge bases about general concepts, or act as a preprocessing step to produce facts for a fact answering engine. We present an efficient system that extracts the target lists from web pages with high accuracy. We have used the system to process up to 160 million, or 1/10 of a high-frequency web snapshot from Bing, and obtained over 140,000 lists with 90.4% precision."""	preprocessor;question answering;snapshot (computer storage);web page	Zhixian Zhang;Kenny Q. Zhu;Haixun Wang	2012		10.1145/2339530.2339780	web service;knowledge base;web mining;static web page;web development;site map;data web;web design;web search engine;page view;data model;computer science;high frequency;social semantic web;web page;data mining;web 2.0;world wide web;information retrieval;web server	ML	-29.416498453615883	-54.245495268475246	46176
7dcf693cd289b00a4c1bf81fc213d4dd9989c3d8	effects of query complexity and learning on novice user query performance with conceptual and logical database interfaces	conceptual database interfaces;databases;user interfaces query complexity learning novice users query performance logical database interfaces conceptual database interfaces relational database system semantic communication visual languages query languages;logical database interfaces;query language;database system;learning;semantic communication;user interface;databases speech processing hidden markov models speech recognition signal processing algorithms pattern recognition bayesian methods error analysis mutual information;helium;novice users;speech processing;bayesian methods;relational database;indexing terms;data model;query languages;error analysis;visual languages;hidden markov models;query complexity;data models user interfaces relational databases query languages visual languages;visual language;pattern recognition;speech recognition;relational database system;mutual information;relational databases;signal processing algorithms;user interfaces;query performance;data models	Users see the database interface as the database system. A good interface enables them to formulate queries better. The semantics communicated through the interface can be classified according to abstraction levels, such as the conceptual and logical levels. With the conceptual interface, interaction is in terms of real-world concepts such as entities, objects and relationships. Current user-database interaction is mainly based on the logical interface, where interaction is in terms of abstract database concepts such as relations and joins. Many researchers argue that end users will perform better with the conceptual interface. This research tested this claim, as well as the effects of query complexity and learning, on the visual query performance of users. The experiment involved three tests: an initial test, a retention test and a relearning test. The results showed that, for complex queries, conceptual interface users achieved higher accuracy, were more confident in their answers, and spent less time on the queries. This is persistent across retention and relearning tests.	database;decision tree model;entity;persistence (computer science)	Keng Siau;Hock Chuan Chan;Kwok Kee Wei	2004	IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans	10.1109/TSMCA.2003.820581	natural language processing;relational database;computer science;conceptual schema;data mining;speech processing;database;user interface;query language	DB	-10.043044886564838	-72.34714970289194	46221
b870e99a239f80a8e3a2e52ad061e516da462ff0	mining the sentiment expectation of nouns using bootstrapping method		We propose an unsupervised bootstrapping method to generate a new type of affect knowledge base: the sentiment expectation of nouns (e.g., “high salary” is desirable while “high price” is usually undesirable, because people have opposite sentiment expectation towards “salary” and “price”). A bootstrapping framework is designed to retrieve patterns that might be used to express complaints from the Web. The sentiment expectation of a noun could be automatically predicted with the output patterns. We evaluate the retrieved patterns and show that our method yields good results. Also, they are applied to improve both sentence and document level sentiment analysis results.	ambiguous grammar;baseline (configuration management);bootstrapping (compilers);knowledge base;lexicon;sentiment analysis;supervised learning;unsupervised learning;webserver directory index;world wide web	Miaomiao Wen;Yunfang Wu	2011			natural language processing;computer science;machine learning;pattern recognition;data mining	NLP	-24.792730962755062	-70.08997642612492	46234
35ccdcea6a329bcd546f682d80c00edfd0aaceb8	mizar 40 for mizar 40	formal mathematics;large theories;premise selection;automated reasoning;article letter to editor;machine learning;artificial intelligence;mizar	As a present to Mizar on its 40th anniversary, we develop an AI/ATP system that in 30 seconds of real time on a 14-CPU machine automatically proves 40 % of the theorems in the latest official version of the Mizar Mathematical Library (MML). This is a considerable improvement over previous performance of large-theory AI/ATP methods measured on the whole MML. To achieve that, a large suite of AI/ATP methods is employed and further developed. We implement the most useful methods efficiently, to scale them to the 150000 formulas in MML. This reduces the training times over the corpus to 1–3 seconds, allowing a simple practical deployment of the methods in the online automated reasoning service for the Mizar users (Miz AR $\mathbb {A}\mathbb {R}$ ).	artificial general intelligence;automated reasoning;automated theorem proving;dreams;enumerated type;iteration;mips magnum;machine learning;mizar;software deployment;text corpus	Cezary Kaliszyk;Josef Urban	2015	Journal of Automated Reasoning	10.1007/s10817-015-9330-8	computer science;artificial intelligence;mathematics;mizar system;automated reasoning;programming language;algorithm	PL	-15.737757305871146	-70.25114581347171	46235
2c4b612ec6b7f5bc47d6c00485016e52dfd057b0	finding parallel passages in cultural heritage archives		It is of great interest to researchers and scholars in many disciplines (particularly those working on cultural heritage projects) to study parallel passages (i.e., identical or similar pieces of text describing the same thing) in digital text archives. Although there exist a few software tools for this purpose, they are restricted to a specific domain (e.g., the Bible) or a specific language (e.g., Hebrew). In this article, we present in detail how we build a digital infrastructure that can facilitate the search and discovery of parallel passages for any domain in any language. It is at the core of our Samtla (Search And Mining Tools with Linguistic Analysis) system designed in collaboration with historians and linguists. The system has already been used to support research on five large text corpora that span a number of different domains and languages. The key to such a domain-independent and language-independent digital infrastructure is a novel combination of a character-based n-gram language model, space-optimized suffix tree, and generalized edit distance. A comprehensive evaluation through crowdsourcing shows that the effectiveness of our system’s search functionality is on par with the human-level performance.		Martyn Harris;Mark Levene;Dell Zhang;Dan Levene	2018	JOCCH	10.1145/3195727	multimedia;edit distance;suffix tree;hebrew;software;cultural heritage;language model;text corpus;crowdsourcing;computer science	Web+IR	-32.21981581978985	-72.137237263451	46252
22916d503ca18e345f653e3f0599d09097ba164b	bridging languages for question answering	question answering;question answering system	Abstract. This paper presents the extension of the ITC-irst DIOGENE Question  Answering system,towards,multilinguality. DIOGENE  relies on a well tested  three-componentsarchitecture built in the framewor,k of our participation in the  QA track at the Text Retrieval Conference (TREC 2002). The novelty factors  are represented by the enhancement,of the system wi th language-specific tools  targeted to the Italian language,( e.g.  a module,in charge of the answer-type  extraction, and a named entities recognizer) and th e introduction of a module  for the translation of Italian queries into English  queries. The overall  architecture of the extended system, as well as the results obtained in the CLEF- 2003 Monolingual,Italian and,Bilingual Italian/Engl ish QA tracks will be  presented and discussed throughout the paper.	bridging (networking);question answering	Bernardo Magnini;Matteo Negri;Hristo Tanev;Milen Kouylekov	2005	KI		information retrieval;natural language processing;artificial intelligence;bridging (networking);question answering;computer science	NLP	-31.89864304782358	-73.30526706428229	46257
61c0f9d5b184bfd9f6e483a77c6d984d3923d657	tag-weighted dirichlet allocation	analytical models;pattern clustering;tag weighted;probability;resource management;text analysis;inference mechanisms;data mining;text analysis data mining expectation maximisation algorithm inference mechanisms parameter estimation pattern classification pattern clustering probability recommender systems;data models vectors resource management mathematical model predictive models probabilistic logic analytical models;text classification tag weighted dirichlet allocation tag information plain text metadata html files author information venue information research articles semistructured document data model fitness model efficiency tagged document modelling twda framework document tag leveraging document word leveraging topic components document topic distribution learning topic word distribution learning tag topic distribution inference text mining text clustering text recommendations probabilistic weights variational inference method em algorithm model parameter estimation tag prediction;vectors;variational inference;pattern classification;mathematical model;variational inference tag weighted dirichlet allocation topic model;predictive models;parameter estimation;probabilistic logic;dirichlet allocation;recommender systems;topic model;data models;expectation maximisation algorithm	In the past two decades, there has been a huge amount of document data with rich tag information during the evolution of the Internet, which can be called semi-structured data. These semi-structured data contain both unstructured features (e.g., plain text) and metadata, such as tags in html files or author and venue information in research articles. It's of great interest to model such kind of data. Most previous works focused on modeling the unstructured data. Some other methods have been proposed to model the unstructured data with specific tags. To build a general model for semi-structured documents remains an important problem in terms of both model fitness and efficiency. In this paper, we propose a novel method to model the tagged documents by a so-called Tag-Weighted Dirichlet Allocation (TWDA). TWDA is a framework that leverages both the tags and words in each document to infer the topic components for the documents. This allows not only to learn the document-topic and topic-word distributions, but also to infer the tag-topic distributions for text mining (e.g., classification, clustering, and recommendations). Moreover, TWDA can automatically infer the probabilistic weights of tags for each document, that can be used to predict the tags in one document. We present an efficient variational inference method with an EM algorithm for estimating the model parameters. The experimental results show the effectiveness, efficiency and robustness of our TWDA approach by comparing it with the state-of-the-art methods on four corpora in document modeling, tags prediction and text classification.	calculus of variations;cluster analysis;computer vision;document classification;expectation–maximization algorithm;experiment;html;internet;semi-structured data;semiconductor industry;tag (metadata);text corpus;text mining;topic model;venue (sound system)	Shuangyin Li;Guan Huang;Ruiyang Tan;Rong Pan	2013	2013 IEEE 13th International Conference on Data Mining	10.1109/ICDM.2013.11	data modeling;text mining;computer science;resource management;machine learning;pattern recognition;probability;mathematical model;data mining;predictive modelling;probabilistic logic;topic model;estimation theory;information retrieval;statistics	DB	-16.796821166527995	-63.17868593712194	46317
aa527b3f0d3350b292bd6bc74eedf48d5e33e72d	interactive analysis and visualization of macromolecular interfaces between proteins	interface contact matrix;human computer interaction;protein complex;interaction analysis;3d visualization;macromolecular interfaces;hot spot;tumour necrosis factor;molecular surface;3d structure;bioinformatics	Molecular interfaces between proteins are of high importance for understanding their interactions and functions. In this paper protein complexes in the PDB database are used as input to calculate an interface contact matrix between two proteins, based on the distance between individual residues and atoms of each protein. The interface contact matrix is linked to a 3D visualization of the macromolecular structures in that way, that mouse clicking on the appropriate part of the interface contact matrix highlights the corresponding residues in the 3D structure. Additionally, the identified residues in the interface contact matrix are used to define the molecular surface at the interface. The interface contact matrix allows the end user to overview the distribution of the involved residues and an evaluation of interfacial binding hot spots. The interactive visualization of the selected residues in a 3D view via interacting windows allows realistic analysis of the macromolecular interface.		Marco Wiltgen;Andreas Holzinger;Gernot P. Tilz	2007		10.1007/978-3-540-76805-0_17	crystallography;materials science;human–computer interaction;bioinformatics	Visualization	-6.117681543001345	-58.240742937442846	46319
c7aafcad82269652b343775726fd8fdbfef54c11	a domain-specific compiler theory based framework for automated reaction network generation	kinetic model;chemical reaction network;reaction networks;compiler;abstract syntax tree;catalysis;automatic network generation;chemistry rules;text generation;domain specificity;reaction mechanism;intermediate representation;knowledge base	Catalytic chemical reaction networks are often very complicated because of the numerous species and reactions involved. Hence, automating the network generation process is necessary as it is quite labor intensive and error prone to write down all the reactions manually. We present an automated integrated framework for reaction network generation based on domain-specific compiler theory using a knowledge base of chemistry rules. The chemistry rules represent basic reaction mechanisms that the reactants can undergo. The system's domain-specific compiler takes the rules and initial reactants as inputs, parses the rule text, generates the intermediate representation, and finally produces the reaction network by interpreting the intermediate representation. We chose the Abstract Syntax Tree (AST) as the intermediate representation because of its transparency and ease of search. The system executes the AST using the initial reactants, and generates the reaction network. The Reaction Description Language (RDL) has been extended to describe the chemistry rules for catalytic systems, and the molecules are represented by Simplified Molecular Input Line Entry System (SMILES). This framework separates the molecules and the behavior of catalysts, represented by the chemistry rules. This approach accelerates the speed of generating hypotheses for building the kinetic models for catalytic systems.	compiler	Shuo-Huan Hsu;Balachandra Krishnamurthy;Prathima Rao;Chunhua Zhao;Suresh Jagannathan;Venkat Venkatasubramanian	2008	Computers & Chemical Engineering	10.1016/j.compchemeng.2008.01.007	knowledge base;compiler;catalysis;simulation;computer science;artificial intelligence;theoretical computer science;machine learning;reaction mechanism;intermediate language;algorithm;abstract syntax tree	PL	-4.578688295577925	-65.12853825466979	46327
3c6d4321513625958c764a3ff8d17c76d95d35b5	improving text segmentation using latent semantic analysis: a reanalysis of choi, wiemer-hastings, and moore (2001)	institutional repositories;computacion informatica;fedora;filologias;grupo de excelencia;vital;experience report;linguistica;ciencias basicas y experimentales;computational linguistics;vtls;grupo a;latent semantic analysis;linguistique informatique;text segmentation;ils	Choi, Wiemer-Hastings, and Moore (2001) proposed to use Latent Semantic Analysis (LSA) to extract semantic knowledge from corpora in order to improve the accuracy of a text segmentation algorithm. By comparing the accuracy of the very same algorithm, depending on whether or not it takes into account complementary semantic knowledge, they were able to show the benefit derived from such knowledge. In their experiments, semantic knowledge was, however, acquired from a corpus containing the texts to be segmented in the test phase. If this hyper-specificity of the LSA corpus explains the largest part of the benefit, one may wonder if it is possible to use LSA to acquire generic semantic knowledge that can be used to segment new texts. The two experiments reported here show that the presence of the test materials in the LSA corpus has an important effect, but also that the generic semantic knowledge derived from large corpora clearly improves the segmentation accuracy.	algorithm;experiment;hyper cd-rom;latent semantic analysis;meteorological reanalysis;sensitivity and specificity;tag (game);text corpus;text segmentation	Yves Bestgen	2006	Computational Linguistics	10.1162/coli.2006.32.1.5	natural language processing;text segmentation;semantic similarity;speech recognition;explicit semantic analysis;latent semantic analysis;computer science;artificial intelligence;computational linguistics;linguistics;probabilistic latent semantic analysis	NLP	-28.433397179390525	-72.47791161846362	46379
76a22632eea28baaf304aecc7cee94da628fa23f	corpus-based translation of ontologies for improved multilingual semantic annotation		Ontologies have proven to be useful to enhance NLP-based applications such as information extraction. In the biomedical domain rich ontologies are available and used for semantic annotation of texts. However, most of them have either no or only few non-English concept labels and cannot be used to annotate non-English texts. Since translations need expert review, a full translation of large ontologies is often not feasible. For semantic annotation purpose, we propose to use the corpus to be annotated to identify high occurrence terms and their translations to extend respective ontology concepts. Using our approach, the translation of a subset of ontology concepts is sufficient to significantly enhance annotation coverage. For evaluation, we automatically translated RadLex ontology concepts from English into German. We show that by translating a rather small set of concepts (in our case 433), which were identified by corpus analysis, we are able to enhance the amount of annotated words from 27.36 % to 42.65 %.	algorithm;compiler;dictionary;display resolution;domain-specific language;grams;information extraction;n-gram;natural language processing;ontology (information science);sim lock;text corpus	Claudia Bretschneider;Heiner Oberkampf;Sonja Zillner;Bernhard Bauer;Matthias Hammon	2014		10.3115/v1/W14-6201	natural language processing;computer science;data mining;information retrieval	NLP	-29.495443032015878	-71.55566469091112	46389
0c6ccaa7899c8226e4b7a6bfbcb6458c6be4cc9b	donâ€™t until the final verb wait: reinforcement learning for simultaneous machine translation		We introduce a reinforcement learningbased approach to simultaneous machine translation—producing a translation while receiving input words— between languages with drastically different word orders: from verb-final languages (e.g., German) to verb-medial languages (English). In traditional machine translation, a translator must “wait” for source material to appear before translation begins. We remove this bottleneck by predicting the final verb in advance. We use reinforcement learning to learn when to trust predictions about unseen, future portions of the sentence. We also introduce an evaluation metric to measure expeditiousness and quality. We show that our new translation model outperforms batch and monotone translation strategies.	medial graph;refinement (computing);reinforcement learning;sparse voxel octree;statistical machine translation;tokenization (data security);world-system;monotone	Alvin Grissom;He He;Jordan L. Boyd-Graber;John Morgan;Hal Daumé	2014		10.3115/v1/D14-1140	natural language processing;synchronous context-free grammar;speech recognition;example-based machine translation;computer science;machine learning;linguistics;rule-based machine translation	NLP	-19.710358522784425	-76.12644348050257	46400
6e6dd9e8e022a1e227c8d23138b73e7de81d664d	lemmatization technique in bahasa: indonesian language	language;algorithm;lemmatization;stemmer	many researches and inventions have been made in the field of linguistics and technology. Even so, the integration between linguistics and technology is not always reliable to all language. Every language is unique in its linguistic nature and rules. In this paper, a lemmatization technique in Bahasa (Indonesian language) is presented. It has achieved good precision by using The Indonesian Dictionary and a set of rules to remove affixes. The lemmatization technique is developed based on the previous algorithm, Indonesian stemmer. Both Indonesian stemming and lemmatization method have the same characteristics but a little bit different in its implementation. The way to reach its own goal/purpose is defined as a core difference and therefore possible to modify. The result shows that the algorithm achieved roughly 98% precision on a collection consisting 57,261 valid words with 7,839 unique valid words gathered from Kompas.com, an Indonesian online news article.	algorithm;dictionary;lemmatisation;stemming	Derwin Suhartono;David Christiandy;Rolando Rolando	2014	JSW		natural language processing;lemmatisation;computer science;stemming;language	PL	-28.84948446112038	-74.23533488396119	46432
7fd9f091c3e2b6df6659619d9a43078a0e9294a8	support and centrality: learning weights for knowledge graph embedding models		Computing knowledge graph (KG) embeddings is a technique to learn distributional representations for components of a knowledge graph while preserving structural information. The learned embeddings can be used in multiple downstream tasks such as question answering, information extraction, query expansion, semantic similarity, and information retrieval. Over the past years, multiple embedding techniques have been proposed based on different underlying assumptions. The most actively researched models are translation-based which treat relations as translation operations in a shared (or relation-specific) space. Interestingly, almost all KG embedding models treat each triple equally, regardless of the fact that the contribution of each triple to the global information content differs substantially. Many triples can be inferred from others, while some triples are the foundational (basis) statements that constitute a knowledge graph, thereby supporting other triples. Hence, in order to learn a suitable embedding model, each triple should be treated differently with respect to its information content. Here, we propose a data-driven approach to measure the information content of each triple with respect to the whole knowledge graph by using rule mining and PageRank. We show how to compute triple-specific weights to improve the performance of three KG embedding models (TransE, TransR and HolE). Link prediction tasks on two standard datasets, FB15K and WN18, show the effectiveness of our weighted KG embedding model over other more complex models. In fact, for FB15K our TransE-RW embeddings model outperforms models such as TransE, TransM, TransH, and TransR by at least 12.98% for measuring the Mean Rank and at least 1.45% for HIT@10. Our HolE-RW model also outperforms HolE and ComplEx by at least 14.3% for MRR and about 30.4% for HIT@1 on FB15K. Finally, TransR-RW show an improvement over TransR by 3.90% for Mean Rank and 0.87% for HIT@10.	basis (linear algebra);bottom-up proteomics;centrality;downstream (software development);graph embedding;information extraction;information retrieval;kasparov's gambit;knowledge graph;pagerank;query expansion;question answering;read-write memory;self-information;semantic similarity;www.renderware.com	Gengchen Mai;Krzysztof Janowicz;Bo Yan	2018		10.1007/978-3-030-03667-6_14	graph embedding;semantic similarity;query expansion;data mining;information extraction;theoretical computer science;pagerank;centrality;embedding;question answering;computer science	NLP	-16.276361943510302	-66.62478770550763	46436
040800e88fbdff598fb85ea82c12f94c3939989f	reverse tdnn: an architecture for trajectory generation	trajectory generation;backpropagation algorithm	"""The backpropagation algorithm can be used for both recognition and generation of time trajectories. When used as a recognizer, it has been shown that the performance of a network can be greatly improved by adding structure to the architecture. The same is true in trajectory generation. In particular a new architecture corresponding to a \reversed"""" TDNN is proposed. Results show dramatic improvement of performance in the generation of hand-written characters. A combination of TDNN and reversed TDNN for compact encoding is also suggested."""	algorithm;backpropagation;finite-state machine;time delay neural network	Patrice Y. Simard;Yann LeCun	1991			simulation;computer science;artificial intelligence;backpropagation;machine learning	ML	-15.325629534112233	-76.96236433433957	46499
00538c72c45d0c87038daaa9b0707bbe40c02ca0	visual analysis and knowledge discovery for text		Providing means for effectively accessing and exploring large textual data sets is a problem attracting the attention of text mining and information visualization experts alike. The rapid growth of the data volume and heterogeneity, as well as the richness of metadata and the dynamic nature of text repositories, add to the complexity of the task. This chapter provides an overview of data visualization methods for gaining insight into large, heterogeneous, dynamic textual data sets. We argue that visual analysis, in combination with automatic knowledge discovery methods, provides several advantages. Besides introducing human knowledge and visual pattern recognition into the analytical process, it provides the possibility to improve the performance of automatic methods through user feedback.	data visualization;information visualization;pattern recognition;text corpus;text mining	Christin Seifert;Vedran Sabol;Wolfgang Kienreich;Elisabeth Lex;Michael Granitzer	2014		10.1007/978-1-4614-9242-9_7	information retrieval	ML	-25.47242403961644	-57.62411331289524	46505
41e5f6072f48f91d1dc70672bcb3ba40b1545ea0	visual indexing of large scale train-borne video for rail condition perceiving				Peng Dai;Shengchun Wang;Yaping Huang;Hao Wang;Xinyu Du;Qiang Han	2017	IEICE Transactions		computer science;artificial intelligence;computer vision;search engine indexing;multimedia	Visualization	-15.152993492031309	-56.20345476457335	46596
af24e068b25e098812c6027a3c71fb101958a00a	ubc-upc: sequential srl using selectional preferences. an approach with maximum entropy markov models		We present a sequential Semantic Role Labeling system that describes the tagging problem as a Maximum Entropy Markov Model. The system uses full syntactic information to select BIO-tokens from input data, and classifies them sequentially using state-of-the-art features, with the addition of Selectional Preference features. The system presented achieves competitive performance in the CoNLL-2005 shared task dataset and it ranks first in the SRL subtask of the Semeval-2007 task 17.	markov chain;maximum-entropy markov model;semantic role labeling;universal product code	Beñat Zapirain;Eneko Agirre;Lluís Màrquez i Villodre	2007			maximum-entropy markov model;computer science;machine learning;pattern recognition;data mining	NLP	-22.894346275814442	-72.50295305994683	46646
f697f984b597efd272cdf5309eb6fd7f440f4283	"""the ergativization of the verb """"死""""[si](die) in chinese language history"""				Yichen Zhang	2014		10.1007/978-3-319-14331-6_12	natural language processing;speech recognition;linguistics	NLP	-30.56600663480602	-78.22949577169311	46671
4aa836552d17b945ca964864c229919bb3aeba4c	an inclusive survey on data preprocessing methods used in web usage mining		Several data mining techniques applied in Web usage mining applications for discovering user access pattern from web log data. To understand and provide better services it will require Web-based applications. Web usage mining is one of the types of Web mining. Web mining is the technique to extract knowledge from web content, structure and usage. It is the collection of technologies to accomplish the possible of extracting valuable knowledge from the World Wide Web and its usage pattern. Web mining enables to find out relevant result from Web data including web document, hyperlink between documents, usage log of website etc. There are three main areas of web mining research –content, structure and usage. This paper provide an overview of previous and existing work in all three areas, and also define an overview of data preprocessing process like Data Cleaning, User Identification, Session Identification, Transaction Identification, Path Completion used in Web usage mining.	blog;data mining;data pre-processing;hyperlink;preprocessor;web application;web content;web mining;web page;world wide web	Brijesh Bakariya;Krishna Kumar Mohbey;Ghanshyam Singh Thakur	2012		10.1007/978-81-322-1041-2_35	web service;web application security;web mining;web development;web modeling;data web;web analytics;web mapping;web design;web standards;computer science;web navigation;social semantic web;web page;data mining;semantic web stack;web intelligence;web 2.0;world wide web;information retrieval	Web+IR	-29.488273252091123	-52.41921717650052	46728
2e9bc3ff64a4e771bce6bd426e2bc90a0be1ce35	real-time predicting bursting hashtags on twitter	hashtag;burst;real time prediction	A large number of hashtags are generated every day on Twit- ter. Only a few hashtags can become bursting topics. It is quite challeng- ing to predict such bursting hashtags in real-time. In this paper, we pro- vide the first definition of a bursting hashtag, and propose a solution to the real-time prediction problem of bursting hashtags. The experimen- tal results show that the proposed method outperforms other related methods.	hashtag;real-time transcription	Shoubin Kong;Qiaozhu Mei;Ling Feng;Zhe Zhao	2014		10.1007/978-3-319-08010-9_29	simulation;computer science;artificial intelligence;data science	NLP	-23.503731654498804	-55.20809378500615	46758
7106459fafaa33ab203ed2a5fe5e6db41ee7c0b4	an axiomatic approach to ir--uiuc trec 2005 robust track experiments	information retrieval;mixture model	In this paper, we report our experiments in the TREC 2005 Robust Track. Our focus is to explore the use of a new axiomatic approach to information retrieval. Most existing retrieval models make the assumption that terms are independent of each other. Although such simplifying assumption has facilitated the construction of successful retrieval systems, the assumption is not true; words are related by use, and their similarity of occurrence in documents can reflect underlying semantic relations between terms. Our new method aims at incorporating term dependency relations into the axiomatic retrieval model in a natural way. In this paper, we describe the method and present analysis of our Robust-2005 evaluation results. The results show that the proposed method works equally well as the KL-divergence retrieval model with a mixture model feedback method. The performance can be further improved by using the external resources such as Google.	axiomatic system;experiment;information retrieval;kl-one;kullback–leibler divergence;language model;mixture model	Hui Fang;ChengXiang Zhai	2005				Web+IR	-27.727990680400886	-66.97421205075518	46796
55983f4a2927fbb2768b62f27bf69610679812f5	incremental hierarchical clustering of text documents	document clustering;hierarchical clustering;incremental clustering;text clustering	Incremental hierarchical text document clustering algorithms are important in organizing documents generated from streaming on-line sources, such as, Newswire and Blogs. However, this is a relatively unexplored area in the text document clustering literature. Popular incremental hierarchical clustering algorithms, namely Cobweb and Classit, have not been widely used with text document data. We discuss why, in the current form, these algorithms are not suitable for text clustering and propose an alternative formulation that includes changes to the underlying distributional assumption of the algorithm in order to conform with the data. Both the original Classit algorithm and our proposed algorithm are evaluated using Reuters newswire articles and Ohsumed dataset.	algorithm;blog;cluster analysis;hierarchical clustering;online and offline;organizing (structure)	Nachiketa Sahoo;James P. Callan;Ramayya Krishnan;George T. Duncan;Rema Padman	2006		10.1145/1183614.1183667	correlation clustering;data stream clustering;document clustering;fuzzy clustering;flame clustering;computer science;data science;canopy clustering algorithm;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;hierarchical clustering;cluster analysis;brown clustering;world wide web;dbscan;biclustering;affinity propagation;hierarchical clustering of networks;clustering high-dimensional data;conceptual clustering	Web+IR	-25.385988133252464	-59.43776207344454	46821
0225785c966821c08f40345988a1e852ee27a862	a model-based em method for topic person name multi-polarization	person name clustering;text mining;sentiment analysis	In this paper, we propose an unsupervised approach for multi-polarization of topic person names. We employ a model-based EM method to polarize individuals into positively correlated groups. In addition, we present off-topic block elimination and weighted correlation coefficient techniques to eliminate the off-topic blocks and reduce the text sparseness problem respectively. Our experiment results demonstrate that the proposed method can identify multi-polar person groups of topics correctly.	expectation–maximization algorithm;polarization (waves)	Chien Chin Chen;Zhong-Yong Chen	2011		10.1007/978-3-642-25631-8_37	text mining;computer science;machine learning;pattern recognition;data mining;sentiment analysis	NLP	-22.556901028262082	-65.19237548595062	46855
0a10d64beb0931efdc24a28edaa91d539194b2e2	efficient estimation of word representations in vector space		We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.	algorithmic efficiency;artificial neural network;computation;display resolution;test set;word embedding	Tomas Mikolov;Kai Chen;Gregory S. Corrado;Jeffrey Dean	2013	CoRR		natural language processing;computer science;machine learning	NLP	-19.989287538192865	-73.74555732796864	46859
398833d5db74d5517d28f1d8ce0758ad38972ff2	towards standardized lexical semantic corpus annotation: components of the semantic annotation framework, semaf		Spatial annotations form part of the Semantic Annotation Framework (SemAF). The current development SemAF-Space (ISO 24617-7) provides a formal specification but does not provide annotation guidelines. In my talk I will compare this approach with the approach developed for the annotation of preposition senses in Müller et al. (2011), where the annotation guidelines form the annotation specification.	formal specification	Tibor Kiss	2012			semantic similarity;semantic computing;formal specification;natural language processing;temporal annotation;artificial intelligence;annotation;semantic web stack;computer science	NLP	-31.277151835273916	-76.05530225787562	46860
cda8e440f4928708e348464d6b50b43fb9507b8f	perception of complex coda clusters and the role of the ssp		Modern Persian permits coda clusters, many of which violate the Sonority Sequencing Principle. In a syllable counting task, Persian speakers consistently perceived clusters in CVCC target items as monosyllabic, whereas English speakers generally perceived clusters existing in English as monosyllabic but those not existing in English as bi-syllabic. Moreover, the latter were perceived as monosyllabic more frequently if they adhered to the SSP than if they did not. In a follow-up experiment, French speakers performed a similar task, related to the clusters of that language. It is anticipated that the French speakers will exhibit similar perceptual behavior demonstrating the influence of the native language when the cluster exists in French, and the influence of the SSP if it does not.	coda (file system);computer cluster;syllable	Irene Vogel;Robin Aronow-Meredith	2006			psychology;speech recognition;communication;social psychology	NLP	-10.82996577587246	-80.07528896982937	46871
99f79e3509dee928483a199ad3876aa6d8055c37	interpretation of chinese discourse connectives for explicit discourse relation recognition		This paper addresses the specific features of Chinese discourse connectives, including types (word-pair and single-word), linking directions (forward and backward linking), positions and ambiguous degrees, and discusses how they affect the discourse relation recognition. A semisupervised learning method is proposed to learn the probability distributions of discourse functions of connectives from a small labeled dataset and a big unlabeled dataset. The statistics learned from the dataset demonstrates some interesting linguistic phenomena such as connective synonyms sharing similar distributions, multiple discourse functions of connectives, and couple-linking elements providing strong clues for discourse relation resolution.	ambiguous grammar;chinese room;chinese wall;discourse relation;experiment;finite-state machine;logical connective;semi-supervised learning;semiconductor industry;supervised learning;text corpus	Hen-Hsen Huang;Tai-Wei Chang;Huan-Yuan Chen;Hsin-Hsi Chen	2014			natural language processing;linguistics	NLP	-25.056049118839773	-72.3051519682203	46944
b2ec8a38760618169ef2ec0948404e7ed33d469d	improving machine translation quality estimation with neural network features		Machine translation quality estimation is a challenging task in the WMT evaluation campaign. Feature extraction plays an important role in automatic quality estimation, and in this paper, we propose neural network features, including embedding features and cross-entropy features of source sentences and machine translations, to improve machine translation quality estimation. The sentence embedding features are extracted through global average pooling from word embedding and are trained by the word2vec toolkits, while the sentence crossentropy features are calculated by the recurrent neural network language model. The experimental results on the development set of WMT17 machine translation quality estimation tasks show that the neural network features gain significant improvements over the baseline. Furthermore, when combining the neural network features and the baseline features, the system performance obtains further improvement.	artificial neural network;baseline (configuration management);cross entropy;end-to-end principle;feature extraction;language model;list of toolkits;machine translation;network model;quadratic equation;recurrent neural network;the sentence;word embedding;word2vec	Zhiming Chen;Yiming Tan;Chenlin Zhang;Qingyu Xiang;Lilin Zhang;Maoxi Li;Mingwen Wang	2017			machine translation;artificial neural network;time delay neural network;machine learning;artificial intelligence;pattern recognition;computer science	NLP	-18.77625180002137	-75.24473672673594	46953
025f424e03e782ed94e18800168d374c4001f927	maximum spanning tree algorithm for non-projective labeled dependency parsing	on-line fashion;average result;dependency parsing;shared task;tree algorithm	Following (McDonald et al., 2005), we present an application of a maximum spanning tree algorithm for a directed graph to non-projective labeled dependency parsing. Using a variant of the voted perceptron (Collins, 2002; Collins and Roark, 2004; Crammer and Singer, 2003), we discriminatively trained our parser in an on-line fashion. After just one epoch of training, we were generally able to attain average results in the CoNLL 2006 Shared Task.	algorithm;directed graph;discriminative model;file spanning;iteration;minimum spanning tree;online and offline;parsing;perceptron	Nobuyuki Shimizu	2006			speech recognition;computer science;pattern recognition;algorithm	NLP	-17.72722644556927	-77.20466167490667	46983
09311fef1c773ee0df5f02d11473018f21187177	melodi: a supervised distributional approach for free paraphrasing of noun compounds	intelligence artificielle;logique en informatique;free paraphrasing of noun compounds;apprentissage;informatique et langage	This paper describes the system submitted by the MELODI team for the SemEval-2013 Task 4: Free Paraphrases of Noun Compounds (Hendrickx et al., 2013). Our approach combines the strength of an unsupervised distributional word space model with a supervised maximum-entropy classification model; the distributional model yields a feature representation for a particular compound noun, which is subsequently used by the classifier to induce a number of appropriate paraphrases.	semeval;unsupervised learning	Tim Van de Cruys;Stergos D. Afantenos;Philippe Muller	2013			natural language processing;speech recognition;computer science;linguistics	NLP	-22.873324344651206	-71.93771197080586	46995
013348c6f0bc2ab2a41223f4524ef1c4f0966f56	efficient fuzzy search in large text collections	approximate text search;inverted index;approximate dictionary search;fuzzy search;hyb index;error tolerant autocomplete	We consider the problem of fuzzy full-text search in large text collections, that is, full-text search which is robust against errors both on the side of the query as well as on the side of the documents. Standard inverted-index techniques work extremely well for ordinary full-text search but fail to achieve interactive query times (below 100 milliseconds) for fuzzy full-text search even on moderately-sized text collections (above 10 GBs of text). We present new preprocessing techniques that achieve interactive query times on large text collections (100 GB of text, served by a single machine). We consider two similarity measures, one where the query terms match similar terms in the collection (e.g., algorithm matches algoritm or vice versa) and one where the query terms match terms with a similar prefix in the collection (e.g., alori matches algorithm). The latter is important when we want to display results instantly after each keystroke (search as you type). All algorithms have been fully integrated into the CompleteSearch engine.	algorithm;approximate string matching;event (computing);incremental search;inverted index;preprocessor	Hannah Bast;Marjan Celikik	2013	ACM Trans. Inf. Syst.	10.1145/2457465.2457470	beam search;full text search;query expansion;inverted index;approximate string matching;computer science;phrase search;machine learning;concept search;data mining;database;programming language;world wide web;information retrieval;search engine	Web+IR	-32.36756139879058	-57.625033069315045	47040
ac8d7fb5d5a4121146512e8bc08d84725a93b84b	review on the existing language resources for languages of france		With the support of the DGLFLF, ELDA conducted an inventory of existing language resources for the regional languages of France. The main aim of this inventory was to assess the exploitability of the identified resources within technologies. A total of 2,299 Language Resources were identified. As a second step, a deeper analysis of a set of three language groups (Breton, Occitan, overseas languages) was carried out along with a focus of their exploitability within three technologies: automatic translation, voice recognition/synthesis and spell checkers. The survey was followed by the organisation of the TLRF2015 Conference which aimed to present the state of the art in the field of the Technologies for Regional Languages of France. The next step will be to activate the network of specialists built up during the TLRF conference and to begin the organisation of a second TLRF conference. Meanwhile, the French Ministry of Culture continues its actions related to linguistic diversity and technology, in particular through a project with Wikimedia France related to contributions to Wikipedia in regional languages, the upcoming new version of the “Corpus de la Parole” and the reinforcement of the DGLFLF’s Observatory of Linguistic Practices.	linear algebra;machine translation;spell checker;wikipedia	Thibault Grouas;Valérie Mapelli;Quentin Samier	2016			natural language processing;artificial intelligence;linguistics;computer science	NLP	-30.457859670721962	-75.76046065378115	47041
5e6ce04815bc726326314bb167e9b12590f99f85	recursive alignment block classification technique for word reordering in statistical machine translation	alignement;statistique;learning;corpus bilingue;statistical classification;word reordering;statistical machine translation;automatic evaluation;word order;apprentissage;evaluation metric;traduction automatique;word alignment;target language;inductive learning;statistics;bilingual corpus;alignment;machine translation	Statistical machine translation (SMT) is based on alignment models which learn from bilingual corpora the word correspondences between source and target language. These models are assumed to be capable of learning reorderings. However, the difference in word order between two languages is one of the most important sources of errors in SMT. In this paper, we show that SMT can take advantage of inductive learning in order to solve reordering problems. Given a word alignment, we identify those pairs of consecutive source blocks (sequences of words) whose translation is swapped, i.e. those blocks which, if swapped, generate a correct monotonic translation. Afterwards, we classify these pairs into groups, following recursively a co-occurrence block criterion, in order to infer reorderings. Inside the same group, we allow new internal combination in order to generalize the reorder to unseen pairs of blocks. Then, we identify the pairs of blocks in the source corpora (both training and test) which belong to the same group. We swap them and we use the modified source training corpora to realign and to build the final translation system. We have evaluated our reordering approach both in alignment and translation quality. In addition, we have used two state-of-the-art SMT systems: a Phrased-based and an Ngram-based. Experiments are reported on the EuroParl task, showing improvements almost over 1 point in the standard MT evaluation metrics (mWER and BLEU). M. R. Costa-jussà (&) Barcelona Media Innovation Center, Av. Diagonal 177, 08018 Barcelona, Spain e-mail: marta.ruiz@barcelonamedia.org J. A. R. Fonollosa E. Monte Universitat Politècnica de Catalunya, TALP Research Center, Jordi Girona 1-3, 08034 Barcelona, Spain J. A. R. Fonollosa e-mail: jose.fonollosa@upc.edu E. Monte e-mail: enric.monte@upc.edu 123 Lang Resources & Evaluation (2011) 45:165–179 DOI 10.1007/s10579-010-9133-9	bleu;bitext word alignment;compiler;email;experiment;inductive reasoning;n-gram;paging;recursion (computer science);statistical machine translation;text corpus	Marta R. Costa-Jussà;José A. R. Fonollosa;Enric Monte-Moreno	2011	Language Resources and Evaluation	10.1007/s10579-010-9133-9	word order;statistical classification;natural language processing;speech recognition;transfer-based machine translation;computer science;pattern recognition;linguistics;machine translation	NLP	-25.70395010307298	-77.67619411882886	47117
65de051be102e08d58a2d5118668a01eb0884e40	unsupervised multi-lingual cold start slot filler ensembling with the knowledge resolver system for tac-kbp 2016		This paper describes a new slot filler ensembling component we developed as part of our Knowledge Resolver relation extraction and knowledge base population toolkit (KRes), and its performance on the TAC-KBP 2016 ensembling task. At the core of our approach is a process of anti-error compounding which identifies likely redundant extractions that are based on at least partially independent evidence, and aggregates them into more reliable results exploiting the redundancies. We use mention overlap, string matching and within and cross-document coreference provided by an EDL system to determine likely equivalent extractions. We also exploit relative system performance estimates based on preliminary scoring or past performance where available. Our system does not require any training. By merging the top-5 recall KBs created by different teams based on preliminary score estimates distributed with the TAC-KBP 2016 SFV evaluation package, our system improved the best overall English SF-All-Micro f1 score for Hop-0 queries from 0.319 to 0.350, and for Hop-0+1 queries from 0.270 to 0.286. For Spanish and Chinese we could not create any improvements due to a lack of run diversity for Spanish and a token-based name and value matcher that was not working sufficiently for Chinese.	cold start;f1 score;knowledge base;like button;relationship extraction;simple file verification;star filler;string searching algorithm	Hans Chalupsky	2016			data mining;information retrieval;computer science;cold start (automotive);resolver	NLP	-27.50637197528052	-68.83267797004986	47124
3da8ae6ea3b4320f03528af0a74ce434fbecd341	semi-supervised vs. cross-domain graphs for sentiment analysis		The lack of labeled data always poses challenges for tasks where machine learning is involved. Semi-supervised and cross-domain approaches represent the most common ways to overcome this difficulty. Graph-based algorithms have been widely studied during the last decade and have proved to be very effective at solving the data limitation problem. This paper explores one of the most popular stateof-the-art graph-based algorithms label propagation, together with its modifications previously applied to sentiment classification. We study the impact of modified graph structure and parameter variations and compare the performance of graph-based algorithms in cross-domain and semi-supervised settings. The results provide a strategy for selecting the most favourable algorithm and learning paradigm on the basis of the available labeled and unlabeled data.	algorithm;baseline (configuration management);compiler description language;experiment;mad;machine learning;programming paradigm;semi-supervised learning;semiconductor industry;sentiment analysis;software propagation;statistical classification;transport layer security	Natalia Ponomareva;Mike Thelwall	2013			natural language processing;computer science;theoretical computer science;machine learning;data mining	NLP	-18.419928618909623	-66.5954781242286	47145
ca85300bd7ab05a38e4e87142866c660b23795ec	twitter user gender inference using combined analysis of text and image processing		Profile inference of SNS users is valuable for marketing, target advertisement, and opinion polls. Several studies examining profile inference have been reported to date. Although information of various types is included in SNS, most such studies only use text information. It is expected that incorporating information of other types into text classifiers can provide more accurate profile inference. As described in this paper, we propose combined method of text processing and image processing to improve gender inference accuracy. By applying the simple formula to combine two results derived from a text processor and an image processor, significantly increased accuracy was confirmed.	central processing unit;feature vector;final (java);image processing;image processor;semiconductor consolidation;user profile	Shigeyuki Sakaki;Yasuhide Miura;Xiaojun Ma;Keigo Hattori;Tomoko Ohkuma	2014		10.3115/v1/W14-5408	computer science;data science;data mining;information retrieval	NLP	-26.43548106079198	-55.23007166173863	47153
946f3bf6c02fd12e1a2db1fb90c5202305a8055a	semantics-driven event clustering in twitter feeds	event detection;ibcn;semantic information;technology and engineering;clustering;twitter;social media	Detecting events using social media such as Twitter has many useful applications in real-life situations. Many algorithms which all use di↵erent information sources—either textual, temporal, geographic or community features—have been developed to achieve this task. Semantic information is often added at the end of the event detection to classify events into semantic topics. But semantic information can also be used to drive the actual event detection, which is less covered by academic research. We therefore supplemented an existing baseline event clustering algorithm with semantic information about the tweets in order to improve its performance. This paper lays out the details of the semantics-driven event clustering algorithms developed, discusses a novel method to aid in the creation of a ground truth for event detection purposes, and analyses how well the algorithms improve over baseline. We find that assigning semantic information to every individual tweet results in just a worse performance in F 1 measure compared to baseline. If however semantics are assigned on a coarser, hashtag level the improvement over baseline is substantial and significant in both precision and recall.	algorithm;baseline (configuration management);cluster analysis;ground truth;hashtag;precision and recall;real life;social media	Cedric De Boom;Steven Van Canneyt;Bart Dhoedt	2015			computer science;data mining;world wide web;information retrieval	Web+IR	-23.72965845763552	-54.76935332585088	47197
397c6adc85e4fa20f87924d65a9c0c41a40c1762	keyword-guided word spotting in historical printed documents using synthetic data and user feedback	busqueda informacion;word spotting;keyword;document imprime;recherche image;index words;information retrieval;extraction forme;caracter impreso;user feedback;printed character;palabra clave;segmentation;article δημοσίeυση πeριοδικού;mot cle;historical document indexing;printed document;col;documento impreso;extraccion forma;indexing;recherche information;feature extraction;indexation;indizacion;pattern recognition;reconnaissance forme;extraction caracteristique;synthetic data;reconocimiento patron;caractere imprime;pattern extraction;segmentacion;image retrieval	In this paper, we propose a novel technique for word spotting in historical printed documents combining synthetic data and user feedback. Our aim is to search for keywords typed by the user in a large collection of digitized printed historical documents. The proposed method consists of the following stages: (1) creation of synthetic image words; (2) word segmentation using dynamic parameters; (3) efficient feature extraction for each word image and (4) a retrieval procedure that is optimized by user feedback. Experimental results prove the efficiency of the proposed approach.	feature extraction;feedback;historical document;printing;synthetic data;text segmentation	Thomas Konidaris;Basilios Gatos;Kostas Ntzios;Ioannis Pratikakis;Sergios Theodoridis;Stavros J. Perantonis	2007	International Journal of Document Analysis and Recognition (IJDAR)	10.1007/s10032-007-0042-4	search engine indexing;speech recognition;feature extraction;image retrieval;computer science;pattern recognition;segmentation;world wide web;information retrieval;synthetic data	Vision	-12.128444370371517	-61.575809621058916	47226
ce06df9ea97ec2746f36f4ce5ec835795d72b92d	hljit at trec 2017 real-time summarization		This paper describes the approaches used at the TREC 2017 Real-Time Summarization. This task contains two scenarios: push notifications and email digest. For the scenario of push notifications, three filtering models, which are based on the hyperlink-extended retrieval model, the Learning to Rank and the hybrid filtering model, are proposed to filter the relevant tweets for a given topic. A novelty verification method is given for further filter the tweets for push notification. For the scenario of email digest, three ranking models, the hyperlink-extended retrieval model, the retrieval model based on learning to rank, and the personal retrieval model, are presented to rank the relevant tweets. Similarly, a novelty verification is proposed for filtering the redundant tweets. The evaluation results of TREC 2017 Real-Time Summarization show that the performance of our models is competitive.	automatic summarization;cryptographic hash function;email;hyperlink;learning to rank;push technology;real-time transcription	Zhongyuan Han;Song Li;Leilei Kong;Liuyang Tian;Haoliang Qi	2017			data mining;information retrieval;automatic summarization;computer science	Web+IR	-25.892035258781526	-54.04531461540674	47276
d72bdea563e3b92bd51a1d5c83f5b968b5b08be6	romanized arabic and berber detection using prediction by partial matching and dictionary methods		Arabic is one of the Semitic languages written in Arabic script in its standard form. However, the recent rise of social media and new technologies has contributed considerably to the emergence of a new form of Arabic, namely Arabic written in Latin scripts, often called Romanized Arabic or Arabizi. While Romanized Arabic is an informal language, Berber or Tamazight uses Latin script in its standard form with some orthography differences depending on the country it is used in. Both these languages are under-resourced and unknown to the state-of-the-art language identiüers. In this paper, we present a language automatic identifier for both Romanized Arabic and Romanized Berber. We also describe the built linguistic resources (large dataset and lexicons) including a wide range of Arabic dialects (Algerian, Egyptian, Gulf, Iraqi, Levantine, Moroccan and Tunisian dialects) as well as the most popular Berber varieties (Kabyle, Tashelhit, Tarifit, Tachawit and Tamzabit). We use the Prediction by Partial Matching (PPM) and dictionary-based methods. The methods reach a macro-average F-Measure of 98.74% and 97.60% respectively.	arabic chat alphabet;dictionary;emergence;identifier;lexicon;prediction by partial matching;social media	Wafia Adouane;Nasredine Semmar;Richard Johansson	2016	2016 IEEE/ACS 13th International Conference of Computer Systems and Applications (AICCSA)	10.1109/AICCSA.2016.7945668	orthography;romanization;computer science;prediction by partial matching;natural language processing;semitic languages;modern arabic mathematical notation;artificial intelligence;arabic languages;latin script;arabic script	NLP	-29.352118397604237	-74.59062005912423	47297
921e6817f25397dc7b9c4bee9f8d491c37ab71f9	frozen out: molecular modeling in the age of cryocrystallography		As molecular modellers we need to remember that the flexibility of a protein is necessary for it to function. Unfortunately, this flexibility is not readily apparent from the seductive molecular graphics rendering of cryocrystallographic results.		Yvonne C. Martin;Steven W. Muchmore	2012	Journal of computer-aided molecular design	10.1007/s10822-011-9537-7	simulation;computer science;artificial intelligence;computer graphics (images)	EDA	-8.256845082365567	-56.3828104293347	47305
255a28dcd9d40dc5ea66806c5f495603e8a0dfde	bridging source and target word embeddings for neural machine translation		Neural machine translation systems encode a source sequence into a vector from which a target sequence is generated via a decoder. Different from the traditional statistical machine translation, source and target words are not directly mapped to each other in translation rules. They are at the two ends of a long information channel in the encoder-decoder neural network, separated by source and target hidden states. This may lead to translations with inconceivable word alignments. In this paper, we try to bridge source and target word embeddings so as to shorten their distance. We propose three strategies to bridge them: 1) a source state bridging model that moves source word embeddings one step closer to their counterparts, 2) a target state bridging model that explores relevant source word embeddings for target state prediction, and 3) a direct link bridging model that directly connects source and target word embeddings so as to minimize their discrepancy. Experiments and analysis demonstrate that the proposed bridging models are able to significantly improve quality of both translation and word alignments.	artificial neural network;baseline (configuration management);bitext word alignment;bridging (networking);bridging model;dictionary;discrepancy function;encode;encoder;experiment;inline linking;neural machine translation;statistical machine translation;word embedding	Shaohui Kuang;Junhui Li;Deyi Xiong	2017	CoRR		machine translation;artificial neural network;bridging (networking);transfer-based machine translation;bridging model;machine learning;rule-based machine translation;communication channel;artificial intelligence;computer science	NLP	-17.602181742818484	-75.00915957254189	47341
c7abc654270112c88bb02d28531e4286c45f6012	hyperlink of men	encyclopedias internet electronic publishing navigation context feature extraction;user study;recommendations;hyperlink;text analysis;recommendations hyperlink hypertext hypersonomy user study;text analysis recommender systems;recommender systems;hypertext;recommendation strategies hyperlink of men hand made hyperlinks text similarity recommendation algorithm human created links;hypersonomy	"""Hand-made hyperlinks are increasingly outnumbered by automatically generated links, which are usually based on text similarity or some sort of recommendation algorithm. In this paper we explore the current linking and appreciation of automatically generated links. To what extent do they prevail on the Web, in what forms do they appear, and do users think those generated links are just as good as human-created links? To answer these questions we first propose a model for extracting contextual information of a hyperlink. Second, we developed a hyperlink ranker to assigned relevance to each existing human generated link. With the outcomes of the hyperlink ranker, together with another two recommendation strategies, we performed a user study with over 100 participants. Results indicate that automated links are """"good enough"""", and even preferred in some user contexts. Still, they do not provide the deeper knowledge as expressed by human authors."""	abandonware;algorithm;hyperlink;principle of good enough;randomness extractor;recommender system;relevance;semantic similarity;usability testing;wikipedia;world wide web	Ricardo Kawase;Patrick Siehndel;Eelco Herder;Wolfgang Nejdl	2012	2012 Eighth Latin American Web Congress	10.1109/LA-WEB.2012.12	text mining;hypertext;computer science;data mining;database;hyperlink;multimedia;world wide web;information retrieval	Web+IR	-31.507300451389376	-53.20830806060844	47360
e267958b4f25114eeb772917509fe21d6fe72e43	a latent semantic analysis-based approach to geographic feature categorization from text	vegetation mapping;information retrieval;semantics;text analysis;vegetation;latent semantic analysis geographic feature categorization;ontologies feature extraction semantics vegetation mapping matrix decomposition ecosystems vegetation;ecosystems;text analysis geography information retrieval;matrix decomposition;feature extraction;domain knowledge latent semantic analysis based approach geographic feature categorization text documents text classification techniques;geographic feature;ontologies;latent semantic analysis;categorization;geography	Geographic feature categorization from text addresses the need for querying and finding geographic features from text documents. Although many text classification techniques have been developed, there are limitations to apply to geographic features due to the uniqueness of the geography features. In this paper we propose a method to classify geographic features based on latent semantic analysis and domain knowledge. The empirical experiment indicates that the proposed method achieves satisfactory categorizing effectiveness.	categorization;document classification;feature vector;latent semantic analysis	Yuxia Huang	2011	2011 IEEE Fifth International Conference on Semantic Computing	10.1109/ICSC.2011.15	ecosystem;latent semantic analysis;feature extraction;computer science;ontology;pattern recognition;data mining;semantics;linguistics;matrix decomposition;information retrieval;vegetation;categorization	DB	-25.363604459261627	-60.4148398497624	47364
363bb81558c5508f2530ae7287e3aa98a8fecb1e	semantic tuples for evaluation of image sentence generation	computer vision;conference report;natural language processing	The automatic generation of image captions has received considerable attention. The problem of evaluating caption generation systems, though, has not been that much explored. We propose a novel evaluation approach based on comparing the underlying visual semantics of the candidate and ground-truth captions. With this goal in mind we have defined a semantic representation for visually descriptive language and have augmented a subset of the Flickr-8K dataset with semantic annotations. Our evaluation metric (BAST) can be used not only to compare systems but also to do error analysis and get a better understanding of the type of mistakes a system does. To compute BAST we need to predict the semantic representation for the automatically generated captions. We use the Flickr-ST dataset to train classifiers that predict STs so that evaluation can be fully automated 1.	coupling (computer programming);error analysis (mathematics);evaluation function;flickr;formal language;ground truth;mind	Lily D. Ellebracht;Arnau Ramisa;Pranava Swaroop Madhyastha;Jose Cordero-Rama;Francesc Moreno-Noguer;Ariadna Quattoni	2015		10.18653/v1/W15-2806	natural language processing;semantic computing;computer science;data mining;information retrieval	NLP	-27.5254653663339	-65.66674846350506	47365
29cf22d5f8876d861eac0f157945d8ee8139a296	priberam: a turbo semantic parser with second order features		This paper presents our contribution to the SemEval-2014 shared task on BroadCoverage Semantic Dependency Parsing. We employ a feature-rich linear model, including scores for first and second-order dependencies (arcs, siblings, grandparents and co-parents). Decoding is performed in a global manner by solving a linear relaxation with alternating directions dual decomposition (AD3). Our system achieved the top score in the open challenge, and the second highest score in the closed track.	interaction;international symposium on fundamentals of computation theory;lagrangian relaxation;linear model;linear programming relaxation;parser;runtime system;semeval;software feature	André F. T. Martins;Mariana S. C. Almeida	2014			speech recognition;computer science;theoretical computer science;algorithm	NLP	-23.557591432577585	-75.45024777191826	47390
3261aa3c9d5fd1ceeaf25a880fc18848c1da832f	roomba: an extensible framework to validate and build dataset profiles	dataset profile;linked data;metadata;linked data dataset prole metadata data quality;eurecom ecole d ingenieur telecommunication centre de recherche graduate school research center communication systems;data quality	Linked Open Data (LOD) has emerged as one of the largest collections of interlinked datasets on the web. In order to benefit from this mine of data, one needs to access to descriptive information about each dataset (or metadata). This information can be used to delay data entropy, enhance datasets discovery, exploration and reuse as well as helping data portal administrators in detecting and eliminating spam. However, such metadata information is currently very limited to a few data portals where they are usually provided manually, thus being often incomplete and inconsistent in terms of quality. To address these issues, we propose a scalable automatic approach for extracting, validating, correcting and generating descriptive linked dataset profiles. This approach applies several techniques in order to check the validity of the metadata provided and to generate descriptive and statistical information for a particular dataset or for an entire data portal.	ckan;entropy (information theory);experiment;information;linked data;portals;profiling (computer programming);scalability;search engine indexing;sensor;spamming	Ahmad Assaf;Raphaël Troncy;Aline Senart	2015		10.1007/978-3-319-25639-9_46	data quality;computer science;linked data;data mining;database;metadata;world wide web;data element;metadata repository	DB	-28.47776949976842	-56.07912389943193	47428
8cd0468502e5f8f93df57ae46895e291fa1508da	a context-dependent sentiment analysis of online product reviews based on dependency relationships	text mining;pagerank;feature extraction;sentiment analysis;online product reviews	Consumers often view online consumer product review as a main channel for obtaining product quality information. Existing studies on product review sentiment analysis usually focus on identifying sentiments of individual reviews as a whole, which may not be effective and helpful for consumers when purchase decisions depend on specific features of products. This study proposes a new feature-level sentiment analysis approach for online product reviews. The proposed method uses an extended PageRank algorithm to extract product features and construct expandable context-dependent sentiment lexicons. Moreover, consumers’ sentiment inclinations toward product features expressed in each review can be derived based on term dependency relationships. The empirical evaluation using consumer reviews of two different products shows a higher level of effectiveness of the proposed method for sentiment	algorithm;context-sensitive language;lexicon;pagerank;sentiment analysis	Zhijun Yan;Meiming Xing;Dongsong Zhang;Baizhang Ma;Tianmei Wang	2014			text mining;feature extraction;computer science;data mining;world wide web;information retrieval;sentiment analysis	Web+IR	-22.50948579870953	-57.31743771498697	47431
c1307158add51eb4cb6fd64d22fb4ca1890bf136	implementing a formal model of inflectional morphology		Inflectional morphology as a research topic lies on the crossroads of many a linguistic subfield, such as linguistic description, linguistic typology, formal linguistics and computational linguistics. However, the subject itself is tackled with diverse objectives and approaches each time. In this paper, we describe the implementation of a formal model of inflectional morphology capturing typological generalisations that aims at combining efforts made in each subfield giving access to every one of them to valuable methods and/or data that would have been out of range otherwise. We show that both language description and studies in formal morphology and linguistic typology on the one hand, as well as NLP tool and resource development on the other benefit from the availability of such a model and an implementation thereof.	angular defect;biological anthropology;computational linguistics;formal grammar;formal language;formal system;galaxy morphological classification;mathematical morphology;natural language processing;relevance	Benoît Sagot;Géraldine Walther	2013		10.1007/978-3-642-40486-3_7	natural language processing;linguistic typology;computer science;linguistics;communication;linguistic description	NLP	-31.566289069424926	-74.51306617682658	47449
5e8e223dc9b978be272b66d4e2dddb1ec677e048	alignment in vision-oriented syntactic language games for teams of robots using stochastic regular grammars and reinforcement learning		This paper approaches the syntactic alignment of a robot team by means of dialogic language games by applying online probabilistic reinforcement learning algorithms. The main contribution of the paper is the application of stochastic regular grammars, with learning capability, to generate the robots’ language. First, the paper describes the syntactic language games, in particular the type of grammar and syntactic rules of the robots’ language and the dynamic process of the language games which are based on dialogic communicative acts and a reinforcement learning policy that allows the robot team to converge to a common language. Afterwards, the experimental results are presented and discussed. The experimental work has been organized around the linguistic description of visual scenes of the blocks world type.		Jack Mario Mingo;Darío Maravall Gómez-Allende;Javier de Lope Asiaín	2013		10.1007/978-3-642-38622-0_8	natural language processing;computer science;machine learning	AI	-12.867913832195791	-69.29233565026288	47473
6cf3bdeccd519a8c94ddd998777d26d40d84aa83	query segmentation for web search	web search	This paper describes a query segmentation method for search engines supporting inverse lookup of words and phrases. Data mining in query logs and document corpora is used to produce segment candidates and compute connexity measures. Candidates are considered in context of the whole query, and a list of the most likely segmentations is generated, with each segment attributed with a connexity value. For each segmentation a segmentation score is computed from connexity values of non-trivial segments, which can be used as a sorting criterion for the segmentations. We also point to a relevancy improvement in query evaluation model by means of proximity penalty.	data mining;lookup table;relevance;sorting;text corpus;web search engine	Knut Magne Risvik;Tomasz Mikolajewski;Peter Boros	2003			query optimization;query expansion;web query classification;computer science;pattern recognition;data mining;database;web search query;information retrieval	Web+IR	-31.728988215791546	-58.488967713010545	47616
7ab2280821e92a236ebcec714a860950fd2de0eb	a video summarization approach based on the emulation of bottom-up mechanisms of visual attention	computational visual attention model;automatic video summarization;content-independent approach;television	This work addresses the development of a computational model of visual attention to perform the automatic summarization of digital videos from television archives. Although the television system represents one of the most fascinating media phenomena ever created, we still observe the absence of effective solutions for content-based information retrieval from video recordings of programs produced by this media universe. This fact relates to the high complexity of the content-based video retrieval problem, which involves several challenges, among which we may highlight the usual demand on video summaries to facilitate indexing, browsing and retrieval operations. To achieve this goal, we propose a new computational visual attention model, inspired on the human visual system and based on computer vision methods (face detection, motion estimation and saliency map computation), to estimate static video abstracts, that is, collections of salient images or key frames extracted from the original videos. Experimental results with videos from the Open Video Project show that our approach represents an effective solution to the problem of automatic video summarization, producing video summaries with similar quality to the ground-truth manually created by a group of 50 users.	archive;automatic summarization;bottom-up parsing;computation;computational model;computer vision;emulator;face detection;information retrieval;key frame;motion estimation	Hugo Jacob;Flávio L. C. Pádua;Anísio Lacerda;Adriano M. Pereira	2016	Journal of Intelligent Information Systems	10.1007/s10844-016-0441-4	video compression picture types;computer vision;simulation;computer science;automatic summarization;video tracking;data mining;multimedia	Vision	-14.268125028835996	-56.35729667855952	47643
877811746da57bd12fa68eaa7d9013a5af52ef6b	separation of overlapping and touching lines within handwritten arabic documents	overlapping and touching lines;calligraph morphology;arabic documents;handwriting line segmentation	In this paper, we propose an approach for the separation of overlapping and touching lines within handwritten Arabic documents. Our approach is based on the morphology analysis of the terminal letters of Arabic words. Starting from 4 categories of possible endings, we use the angular variance to follow the connection and separate the endings. The proposed separation scheme has been evaluated on 100 documents contains 640 overlapping and touching occurrences reaching an accuracy of about 96.88%.	angularjs;connected component (graph theory);experiment;mathematical morphology;statistical model	Nazih Ouwayed;Abdel Belaïd	2009		10.1007/978-3-642-03767-2_29	arithmetic;natural language processing;speech recognition	Vision	-24.77631077424156	-77.85483938179091	47715
1eb5a8b3ffd749b78b46c3b7eee285bfcdcc1e32	identifying fraudulently promoted online videos	decision tree;online videos;supervised machine learning;classifier;youtube;opinion spam;svm	Fraudulent product promotion online, including online videos, is on the rise. In order to understand and defend against this ill, we engage in the fraudulent video economy for a popular video sharing website, YouTube, and collect a sample of over 3,300 fraudulently promoted videos and 500 bot profiles that promote them. We then characterize fraudulent videos and profiles and train supervised machine learning classifiers that can successfully differentiate fraudulent videos and profiles from legitimate ones.	machine learning;supervised learning;video clip	Vlad Bulakh;Christopher W. Dunn;Minaxi Gupta	2014		10.1145/2567948.2578996	support vector machine;classifier;computer science;machine learning;decision tree;multimedia;internet privacy;world wide web	AI	-20.08531454761988	-54.98299862258039	47728
3117347dde0045bb6693e4be50e38a5ca1f733bf	exploring biological data : mappings between ontology- and cluster-based representations	hierarchical clustering;journal_article;mappings;computer and information science;informations och programvisualisering;visualization;computer and information sciences computer science;data och informationsvetenskap;information and software visualization;ontology;gene ontology	Ontologies and hierarchical clustering are both important tools in biology and medicine to study high-throughput data such as transcriptomics and metabolomics data. Enrichment of ontology terms in the data is used to identify statistically overrepresented ontology terms, giving insight into relevant biological processes or functional modules. Hierarchical clustering is a standard method to analyze and visualize data to find relatively homogeneous clusters of experimental data points. Both methods support the analysis of the same data set, but are usually considered independently. However, often a combined view is desired: visualizing a large data set in the context of an ontology under consideration of a clustering of the data. This article proposes new visualization methods for this task. They allow for interactive selection and navigation to explore the data under consideration as well as visual analysis of mappings between ontologyand cluster-based space-filling representations. In this context, we discuss our approach together with specific properties of the biological input data and identify features that make our approach easily usable for domain experts.	algorithm;binary tree;cluster analysis;clutter;data point;directed acyclic graph;event (computing);feedback;gene ontology term enrichment;hierarchical clustering;high-throughput computing;interactivity;internet backbone;metabolomics;ontology (information science);preprocessor;prototype;scrolling;self-balancing binary search tree;subject-matter expert;throughput;unbalanced circuit	Ilir Jusufi;Andreas Kerren;Falk Schreiber	2013	Information Visualization	10.1177/1473871612468880	upper ontology;ontology alignment;ontology components;visualization;bibliographic ontology;computer science;bioinformatics;ontology;ontology;data mining;hierarchical clustering;ontology-based data integration;information retrieval;process ontology;suggested upper merged ontology	Visualization	-6.364748673054477	-61.682483753787096	47747
6eecdc54ffd09ac3af2f92eb7dbcd5ca812957b8	learning grammatical categories using paradigmatic representations: substitute words for language acquisition		Learning word categories is a fundamental task in language acquisition. Previous studies show that co-occurrence patterns of preceding and following words are essential to group words into categories. However, the neighboring words, or frames, are rarely repeated exactly in the data. This creates data sparsity and hampers learning for frame based models. In this work, we propose a paradigmatic representation of word context which uses probable substitutes instead of frames. Our experiments on child-directed speech show that models based on probable substitutes learn more accurate categories with fewer examples compared to models based on frames.	bigram;claire;emoticon;experiment;language model;n-gram;perplexity;sampling (signal processing);sparse matrix;xb machine	Mehmet Ali Yatbaz;Volkan Cirik;Aylin C. Küntay;Deniz Yuret	2016			natural language processing;language acquisition;artificial intelligence;linguistics;grammatical category;computer science	NLP	-10.66090061296294	-76.90709176248228	47749
c97cb1afab4490185c2ebb90646c23ac646040f5	titpi: web people search task using semi-supervised clustering approach	web people search task;similar cluster;similar document;certain criterion;web search result;accurate cluster;semi-supervised clustering approach;agglomerative clustering approach;previous work;disambiguate personal name	Most of the previous works that disambiguate personal names in Web search results employ agglomerative clustering approaches. However, these approaches tend to generate clusters that contain a single element depending on a certain criterion of merging similar clusters. In contrast to such previous works, we have adopted a semisupervised clustering approach to integrate similar documents into a labeled document. Moreover, our proposed approach is characterized by controlling the fluctuation of the centroid of a cluster in order to generate more accurate clusters.	cluster analysis;quantum fluctuation;semi-supervised learning;semiconductor industry	Kazunari Sugiyama;Manabu Okumura	2007			correlation clustering;fuzzy clustering;computer science;machine learning;data mining;cluster analysis;brown clustering;world wide web	Web+IR	-26.64519563502517	-59.28055212194666	47759
50f4dce0af8749447692a987a86b12b43693d65a	task-oriented word embedding for text classification		Distributed word representation plays a pivotal role in various natural language processing tasks. In spite of its success, most existing methods only consider contextual information, which is suboptimal when used in various tasks due to a lack of task-specific features. The rational word embeddings should have the ability to capture both the semantic features and task-specific features of words. In this paper, we propose a task-oriented word embedding method and apply it to the text classification task. With the function-aware component, our method regularizes the distribution of words to enable the embedding space to have a clear classification boundary. We evaluate our method using five text classification datasets. The experiment results show that our method significantly outperforms the state-of-the-art methods.	document classification;experiment;natural language processing;word embedding	Qian Liu;Heyan Huang;Yang Gao;Xiaochi Wei;Yuxin Tian;Luyang Liu	2018			artificial intelligence;pattern recognition;natural language processing;computer science;word embedding	NLP	-19.425824400816825	-72.35184158210268	47779
a07ef551227d29cfd4fbdaf729bf43077b7bf1bf	improving the multimodal probabilistic semantic model by elm classifiers		Abstract The multi-modal retrieval is considered as performing information retrieval among different modalities of multimedia information. Nowadays, it becomes increasingly important in the information science field. However, it is so difficult to bridge the meanings of different multimedia modalities that the performance of multimodal retrieval is deteriorated now. In this paper, we propose a new mechanism to build the relationship between visual and textual modalities and to verify the multimodal retrieval. Specifically, this mechanism depends on the multimodal binary classifiers based on the Extreme Learning Machine (ELM) to verify whether the answers are related to the query examples. Firstly, we propose the multimodal probabilistic semantic model to rank the answers according to their generative probabilities. Furthermore, we build the multimodal binary classifiers to filter out unrelated answers. The multimodal binary classifiers are called the word classifiers. It can improve the performance of the multimodal probabilistic semantic model. The experimental results show that the multimodal probabilistic semantic model and the word classifiers are effective and efficient. Also they demonstrate that the word classifiers based on ELM not only can improve the performance of the probabilistic semantic model but also can be easily applied to other probabilistic semantic models.	elm;multimodal interaction	Yu Zhang;Ye Yuan;Fangda Guo;Yishu Wang;Guoren Wang	2018	J. Franklin Institute	10.1016/j.jfranklin.2017.08.019	modalities;semantic data model;divergence-from-randomness model;generative grammar;probabilistic logic;extreme learning machine;machine learning;artificial intelligence;computer science;pattern recognition;information science	NLP	-15.889658557863275	-61.74040680763308	47833
8613fef2738325a5697e253276b160099100528d	time-varying lstm networks for action recognition		We describe an architecture of Time-Varying Long Short-Term Memory recurrent neural networks (TV-LSTMs) for human action recognition. The main innovation of this architecture is the use of hybrid weights, shared weights and non-shared weights which we refer to as varying weights. The varying weights can enhance the ability of LSTMs to represent videos and other sequential data. We evaluate TV-LSTMs on UCF-11, HMDB-51, and UCF-101 human action datasets and achieve the top-1 accuracy of 99.64%, 57.52%, and 85.06% respectively. This model performs competitively against the models that use both RGB and other features, such as optical flows, improved Dense Trajectory, etc. In this paper, we also propose and analyze the methods of selecting varying weights.	artificial neural network;chen–ho encoding;computation;entity–relationship model;experiment;long short-term memory;natural language;recurrent neural network;video	Zichao Ma;Zhixin Sun	2018	Multimedia Tools and Applications	10.1007/s11042-018-6260-6	architecture;rgb color model;computer science;pattern recognition;artificial intelligence;recurrent neural network	ML	-16.331182247046595	-72.63701416808247	47846
7fae5f54887151c241f14f92ca4e6b66cde74aaa	the hyperonym problem revisited: conceptual and lexical hierarchies in language generation	lexical hierarchy;symbolic representation;fixed level;language production process;lexical item;language generation;hyperonym problem;production model;cognitive modelling;various approach;applicability condition;lexical semantics	"""When a lexical item is selected in the language production process, it needs to be explained why none of its superordinates gets selected instead, since their applicability conditions are fulfilled all the same. This question has received much attention in cognitive modelling and not as much in other branches of NLG. This paper describes the various approaches taken, discusses the reasons why they are so different, and argues that production models using symbolic representations should make a distinction between conceptual and lexical hierarchies, which can be organized along fixed levels as studied in (some branches of) lexical semantics. 1 I n t r o d u c t i o n Representations used in language processing owe much to the tradition of 'semantic networks', which nowadays have been successfully formalized and organized especially around one particular kind of link between nodes: the ISAlink, which connects entities to subordinate entities. This link is, by definition, the root of the so-called 'hyperonym 1 problem': When a speaker utters a word, she presumably needs to retrieve a lemma from her mental lexicon, and the 'applicability conditions"""" of the lemma automatically render the lemma's hyperonyms also applicable, thus raising the question how the choice among a set of more or less specific words is made. In this paper, I briefly review approaches to the hyperonym problem in psycholinguistics, natural language generation, and lexical semantics. In doing that, I will refer to different branches of NLG according to their roots I Alternat ively called 'hypernym' in many publications: 'hyperonym"""" seems preferable, as the Greek root is 'hyper"""" (super) + ' onoma ' (name). . . . . ~ . . . . . . . . . . . . . . . . . . . . . . . . . • . . . . . . . : . . . . . . . . ~ . : : . . . . . . and main motivations. Generally acknowledged are the two poles of 'cognition-inspired' and 'engineering-inspired' language production: Cognition-inspired work (CI-NLG, for short) seeks to build models that replicate performance data and explain phenomena of human language production with the help of psychological experiments; engineering-inspired work (EINLG) seeks to build programs that provide linguistic output to some particular computer application. These goals are extremely different, and it seems that the gap between the respective methodologies will persist for quite some time. In between the two, however, I would situate a third category, which may be called 'linguistics-inspired'. For this branch, here abbreviated as LI-NLG, the primary motivation is neither in modelling human performance nor in efficiently performing a technical application; rather, LI-NLG seeks production models that replicate 'competence data', i.e. that account for observed linguistic regularities, without con> miting to statements about the human production p~vcess. Arguing that progress hinges on a better understanding of the structure of the mental vocabulary, which includes a clear picture of the nature of the ISA-link, I will sketch a framework of distinct (but related) conceptual and lexical hierarchies, which offers possibilities to account for at least some of the phenomena to be discussed. 2 T h e h y p e r o n y m p r o b l e m Following tile psycholinguistics literature, the hyperonym problem is regarded as all aspect of lemrna retrieval. Roelofs [1996, p. 308] describes a 'lemma' as a representation of the meaning and the syntactic properties of a word, and the task of lemma retrieval as a crucial step in the"""	cognitive model;content-based image retrieval;denotational semantics;entity;experiment;human reliability;lexicon;li-chen wang;name;naruto shippuden: clash of ninja revolution 3;natural language generation;race condition;self-replicating machine;semantic network;situated cognition;vocabulary	Manfred Stede	2000			natural language processing;lexical item;computer science;linguistics;lexical choice	NLP	-12.522823221602572	-76.81776596268242	47891
97d1237d7927fd6a3c0b4a33dafd09368159424d	coupled local-global adaptation for multi-source transfer learning			multi-source	Jieyan Liu;Jingjing Li;Ke Lu	2018	Neurocomputing	10.1016/j.neucom.2017.06.051		ML	-8.811663831605843	-71.2103350511334	47939
e93b6c1a0b3ae77b2ae7685b58b4c7176054b01b	a brief survey of text mining: classification, clustering and extraction techniques		The amount of text that is generated every day is increasing dramatically. This tremendous volume of mostly unstructured text cannot be simply processed and perceived by computers. Therefore, efficient and effective techniques and algorithms are required to discover useful patterns. Text mining is the task of extracting meaningful information from text, which has gained significant attentions in recent years. In this paper, we describe several of the most fundamental text mining tasks and techniques including text pre-processing, classification and clustering. Additionally, we briefly explain text mining in biomedical and health care domains.	algorithm;cluster analysis;computer;preprocessor;text mining	Mehdi Allahyari;Seyed Amin Pouriyeh;Mehdi Assefi;Saied Safaei;Elizabeth D. Trippe;Juan B. Gutierrez;Krys J. Kochut	2017	CoRR		data science;cluster analysis;noisy text analytics;concept mining;data mining;computer science;biomedical text mining;co-occurrence networks;text graph;text mining	ML	-25.163430853107826	-57.38881041947275	47980
7eba0de164fce212dfa476708bdbd2d89ef3942c	improved parsing for arabic by combining diverse dependency parsers	arabic language processing;maltparser;system combination;mstparser;dependency parsing;natural language processing	Recently there has been a considerable interest in dependency parsing for many reasons. First, it works accurately for a wide range of typologically different languages. Second, it can be useful for semantics, since it can be easier to attach compositional rules directly to lexical items than to assign them to large numbers of phrase structure rules. Third, robust machine-learning based parsers are available. In this paper, we investigate two techniques for combining multiple data-driven dependency parsers for parsing Arabic, where we are faced with an exceptional level of lexical and structural ambiguity. Experimental results show that combined parsers can produce more accurate results, even for imperfectly tagged text, than each parser produces by itself for texts with the gold-standard tags.	parsing	Maytham Alabbas;Allan Ramsay	2011		10.1007/978-3-319-08958-4_4	natural language processing;parser combinator;parsing expression grammar;top-down parsing language;computer science;parsing;linguistics;programming language;top-down parsing;lr parser	NLP	-24.402876361518704	-74.64825646166281	47999
5e25e27c31f5f025d8e2f7892f95df708fe3ac0b	effective video annotation by mining visual features and speech features	mining visual features;multimedia content;additional manual cost;multimedia understanding;speech-association model modelsass;speech-association pattern;multimedia processing;effective video annotation;speech features;visual-statistical model;prediction model;innovative method;data mining;machine learning;statistical model;statistical analysis;speech processing	In the area of multimedia processing, a number of studies have been devoted to narrowing the gap between multimedia content and human sense. In fact, multimedia understanding is a difficult and challenging task even using machine-learning techniques. To deal with this challenge, in this paper, we propose an innovative method that employs data mining techniques and content-based paradigm to conceptualize videos. Mainly, our proposed method puts the focus on: (I) Construction of prediction models, namely speech-association model ModelSass and visual-statistical model ModelCRM, and (2) Fusion of prediction models to annotate unknown videos automatically. Without additional manual cost, discovered speech-association patterns can show the implicit relationships among the sequential images. On the other hand, visual features can atone for the inadequacy of speech-association patterns. Empirical evaluations reveal that our approach makes, on the average, the promising results than other methods for annotating videos.	data mining;existential quantification;logic programming;machine learning;national supercomputer centre in sweden;programming paradigm;statistical model	Vincent S. Tseng;Ja-Hwung Su;Chih-Jen Chen	2007	Third International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP 2007)	10.1109/IIH-MSP.2007.392	statistical model;computer vision;speech recognition;computer science;machine learning;pattern recognition;data mining;speech processing;predictive modelling;human visual system model;statistics	Robotics	-16.79515871005698	-61.91913885624487	48012
3f437d1a01b79c340bc5cb86276e063b6e635f01	acquisition of translation lexicons for historically unwritten languages via bridging loanwords		With the advent of informal electronic communications such as social media, colloquial languages that were historically unwritten are being written for the first time in heavily code-switched environments. We present a method for inducing portions of translation lexicons through the use of expert knowledge in these settings where there are approximately zero resources available other than a language informant, potentially not even large amounts of monolingual data. We investigate inducing a Moroccan DarijaEnglish translation lexicon via French loanwords bridging into English and find that a useful lexicon is induced for humanassisted translation and statistical machine translation.	bilingual dictionary;bridging (networking);lexicon;social media;statistical machine translation	Michael Bloodgood;Benjamin Strauss	2017			natural language processing;machine translation;computer science;artificial intelligence;bridging (networking);lexicon;social media	NLP	-25.117855566944034	-74.65002089697079	48038
97beef3f5727a74660e7338cdcec2680dd835ae2	interactive ontology matching based on partial reference alignment		Abstract The technique that enables the user and the automatic ontology matching tool to cooperate with each other to generate high-quality alignments in a reasonable amount of time is referred to as the interactive ontology matching. Interactive ontology matching poses a new challenge in a way of how to efficiently leverage user validation to improve the ontology alignment. To address this challenge, this paper presents an innovative interactive ontology matching technique based on Partial Reference Alignment (PRA) to better balance between the large workload posed on users and the demand of improving the quality of ontology alignment. In particular, a PRA-based Interactive Compact Hybrid Evolutionary Algorithm (ICHEA) is proposed to reduce user workload, by adaptively determining the timing of involving users, showing them the most problematic mappings, and helping them to deal with multiple conflicting mappings simultaneously. Meanwhile, it increases the value of user involvement by propagating the confidences of validated mappings, as well as reducing the negative effects brought by the erroneous user validations. The well-known OAEI 2016u0027s benchmark track and interactive track are utilized to test the performance of this approach. The experimental results on benchmark track show that both the f -measure and the f -measure per second of this approach outperform those of the OAEI participants and three state-of-the-art Evolutionary Algorithm (EA) based ontology matching techniques. In addition, the experimental results of three interactive testing cases further show that ICHEA can efficiently determine high-quality ontology alignments under different cases of user error rates, and the performance of the approach is generally better than that of state-of-the-art interactive ontology matching systems.	ontology alignment	Xingsi Xue;Xin Yao	2018	Appl. Soft Comput.	10.1016/j.asoc.2018.08.003	machine learning;artificial intelligence;mathematics;workload;user error;ontology alignment;ontology;evolutionary algorithm	NLP	-12.251246482259669	-66.77784105305022	48052
185a37a7ff76116c6cde92ed39e30c9a7e7fb618	linguistic redundancy in twitter	settore inf 01 informatica;settore ing inf 05 sistemi di elaborazione delle informazioni	In the last few years, the interest of the research community in micro-blogs and social media services, such as Twitter, is growing exponentially. Yet, so far not much attention has been paid on a key characteristic of microblogs: the high level of information redundancy. The aim of this paper is to systematically approach this problem by providing an operational definition of redundancy. We cast redundancy in the framework of Textual Entailment Recognition. We also provide quantitative evidence on the pervasiveness of redundancy in Twitter, and describe a dataset of redundancy-annotated tweets. Finally, we present a general purpose system for identifying redundant tweets. An extensive quantitative evaluation shows that our system successfully solves the redundancy detection task, improving over baseline systems with statistical significance.	baseline (configuration management);blog;high-level programming language;operational definition;redundancy (engineering);redundancy (information theory);social media;textual entailment;triple modular redundancy	Fabio Massimo Zanzotto;Marco Pennacchiotti;Kostas Tsioutsiouliklis	2011			computer science;artificial intelligence;machine learning;data mining	NLP	-27.474232771873826	-68.87739989687155	48071
872cdbbae200a58bb69b5d98e6c8da68deb65bf8	leveraging probabilistic season and location context models for scene understanding	image features;spatial context;context aware;context information;object region detection;probabilistic graphical model;context model;seasonality;semantic gap;context aware scene understanding;spatial relationships;scene understanding;season and location proximity;image retrieval	Recent research has shown the power of context-aware scene understanding in bridging the semantic gap between high-level semantic concepts and low-level image features. In this paper, we present a new method to exploit nonvisual context information from the season and location proximity in which pictures were taken to facilitate region (object) annotation in consumer photos. Our method does not require precise time and location from the capture device or user input. Instead, it learns from rough location (e.g., states in the US) and time (e.g., seasons) information, which can be obtained through picture metadata automatically or through minimal user input (e.g., grouping). In addition, the visual context within the image is obtained by analyzing the spatial relationships between different regions (objects) in the scene. Both visual and nonvisual context information are fused using a probabilistic graphical model to improve the accuracy of object region recognition. Our method has been evaluated on a database that consists of over 10,000 regions in more than 1000 images collected from both the Web and consumers. Experimental results show that incorporating the season and location context significantly improves the performance of region recognition.	bridging (networking);database;graphical model;high- and low-level;world wide web	Jie Yu;Jiebo Luo	2008		10.1145/1386352.1386379	spatial relation;computer vision;image retrieval;computer science;spatial contextual awareness;pattern recognition;data mining;context model;feature;information retrieval;semantic gap;seasonality;statistics	Vision	-15.522218917179103	-59.75910557662175	48077
1b92aac6e1562d2b243dea2799fdda73c035edab	parts-of-speech tagger errors do not necessarily degrade accuracy in extracting information from biomedical text	information extraction;information retrieval;text processing;text analysis;part of speech;protein protein interaction;shallow parsing;entity relationship;semantic analysis	Background: An ongoing assessment of the literature is difficult with the rapidly increasing volume of research publications and limited effective information extraction tools which identify entity relationships from text. A recent study reported development of Muscorian, a generic text processing tool for extracting proteinprotein interactions from text that achieved comparable performance to biomedicalspecific text processing tools. This result was unexpected since potential errors from a series of text analysis processes is likely to adversely affect the outcome of the entire process. Most biomedical entity relationship extraction tools have used biomedicalspecific parts-of-speech (POS) tagger as errors in POS tagging and are likely to affect subsequent semantic analysis of the text, such as shallow parsing. This study aims to evaluate the parts-of-speech (POS) tagging accuracy and attempts to explore whether a comparable performance is obtained when a generic POS tagger, MontyTagger, was used in place of MedPost, a tagger trained in biomedical text. Results: Our results demonstrated that MontyTagger, Muscorian's POS tagger, has a POS tagging accuracy of 83.1% when tested on biomedical text. Replacing MontyTagger with MedPost did not result in a significant improvement in entity relationship extraction from text; precision of 55.6% from MontyTagger versus 56.8% from MedPost on directional relationships and 86.1% from MontyTagger compared to 81.8% from MedPost on nondirectional relationships. This is unexpected as the potential for poor POS tagging by MontyTagger is likely to affect the outcome of the information extraction. An analysis of POS tagging errors demonstrated that 78.5% of tagging errors are being compensated by shallow parsing. Thus, despite 83.1% tagging accuracy, MontyTagger has a functional tagging accuracy of 94.6%. Conclusions: The POS tagging error does not adversely affect the information extraction task if the errors were resolved in shallow parsing through alternative POS tag use.	brill tagger;downstream (software development);entity–relationship model;information extraction;interaction;part-of-speech tagging;point of sale;relationship extraction;shallow parsing	Maurice H. T. Ling;Christophe M Lefèvre;Kevin R. Nicholas	2008	CoRR		protein–protein interaction;natural language processing;entity–relationship model;part of speech;computer science;pattern recognition;linguistics;information extraction;information retrieval	NLP	-28.45749041340437	-68.87387357179897	48106
b9964cc660f742600fd2310912c0069dcc5254bc	address standardization with latent semantic association	data standardization;customer orientation;data integrity;supervised learning;customer relationship management;active learning;rule based;data distribution;large scale;reference data;data cleansing;business intelligence;address;domain specificity;semantic association;concept space	Address standardization is a very challenging task in data cleansing. To provide better customer relationship management and business intelligence for customer-oriented cooperates, millions of free-text addresses need to be converted to a standard format for data integration, de-duplication and householding. Existing commercial tools usually employ lots of hand-craft, domain-specific rules and reference data dictionary of cities, states etc. These rules work better for the region they are designed. However, rule-based methods usually require more human efforts to rewrite these rules for each new domain since address data are very irregular and varied with countries and regions. Supervised learning methods usually are more adaptable than rule-based approaches. However, supervised methods need large-scale labeled training data. It is a labor-intensive and time-consuming task to build a large-scale annotated corpus for each target domain. For minimizing human efforts and the size of labeled training data set, we present a free-text address standardization method with latent semantic association (LaSA). LaSA model is constructed to capture latent semantic association among words from the unlabeled corpus. The original term space of the target domain is projected to a concept space using LaSA model at first, then the address standardization model is active learned from LaSA features and informative samples. The proposed method effectively captures the data distribution of the domain. Experimental results on large-scale English and Chinese corpus show that the proposed method significantly enhances the performance of standardization with less efforts and training data.	corpus linguistics;customer relationship management;data deduplication;data dictionary;information;latent semantic analysis;logic programming;rewrite (programming);supervised learning;test set;text corpus	Honglei Guo;Huijia Zhu;Zhili Guo;Xiaoxun Zhang;Zhong Su	2009		10.1145/1557019.1557144	reference data;computer science;data science;machine learning;data integrity;data mining;database;active learning;business intelligence;supervised learning;data cleansing	AI	-29.453685231644304	-70.99852185054954	48126
31626d5cbff61b8cfc53e31fb0dc7b6d83874c9d	the extended dirndl corpus as a resource for coreference and bridging resolution			bridging (networking)	Anders Björkelund;Kerstin Eckart;Arndt Riester;Nadja Schauffler;Katrin Schweitzer	2014			natural language processing;speech recognition;computer science	NLP	-30.59693694778194	-77.64944197332709	48131
b598983a32c96850c571381a8e6d1f64a10c7881	foreign english accent adjustment by learning phonetic patterns		State-of-the-art automatic speech recognition (ASR) systems struggle with the lack of data for rare accents. For sufficiently large datasets, neural engines tend to outshine statistical models in most natural language processing problems. However, a speech accent remains a challenge for both approaches. Phonologists manually create general rules describing a speaker’s accent, but their results remain underutilized. In this paper, we propose a model that automatically retrieves phonological generalizations from a small dataset. This method leverages the difference in pronunciation between a particular dialect and General American English (GAE) and creates new accented samples of words. The proposed model is able to learn all generalizations that previously were manually obtained by phonologists. We use this statistical method to generate a million phonological variations of words from the CMU Pronouncing Dictionary and train a sequence-to-sequence RNN to recognize accented words with 59% accuracy.	cmu pronouncing dictionary;google app engine;natural language processing;random neural network;speech recognition;statistical model	Fedor Kitashov;Elizaveta Svitanko;Debojyoti Dutta	2018	CoRR		statistical model;american english;computer science;speech recognition;pronunciation;generalization	NLP	-20.087744957316968	-78.99072535662059	48141
dd17ae8df3b03a5eb7fc4b78108167adfe7a572d	collection-integral source selection for uncooperative distributed information retrieval environments	information sources;distributed information retrieval;source selection;linear interpolation;indexation;federated search	We propose a new integral-based source selection algorithm for uncooperative distributed information retrieval environments. The algorithm functions by modeling each source as a plot, using the relevance score and the intra-collection position of its sampled documents in reference to a centralized sample index. Based on the above modeling, the algorithm locates the collections that contain the most relevant documents. A number of transformations are applied to the original plot, in order to reward collections that have higher scoring documents and dampen the effect of collections returning an excessive number of documents. The family of linear interpolant functions that pass through the points of the modified plot is computed for each available source and the area that they cover in the rank-relevance space is calculated. Information sources are ranked based on the area that they cover. Based on this novel metric for collection relevance, the algorithm is tested in a variety of testbeds in both recall and precision oriented settings and its performance is found to be better or at least equal to previous state-of-the-art approaches, overall constituting a very effective and robust solution.	information retrieval	Georgios Paltoglou;Michail Salampasis;Maria Satratzemi	2010	Inf. Sci.	10.1016/j.ins.2010.03.020	mathematical optimization;computer science;artificial intelligence;machine learning;data mining;database;linear interpolation;information retrieval;statistics	Web+IR	-30.585784442536756	-60.76646805470765	48176
21b26f2bbce8d03ba8529d3a5ade44b2c60631db	on applying an image processing technique to detecting spams	filtering;unsolicited electronic mail;image processing unsolicited electronic mail bayesian methods filtering html computer science image databases detection algorithms matched filters pattern matching;image processing;detection algorithms;image databases;bayesian methods;html;pattern matching;bayesian filtering;matched filters;image processing techniques;computer science;spam detection	In this paper we describe a preliminary study on applying an image processing technique to spam detection. We propose a simple method of processing email messages as image data for detecting spams out of them, and show some experimental results obtained from comparing our scheme with a Bayesian filter system. Although the image processing technique we adopted in this paper is quite simple, the results indicate that our method must be potentially useful for spam detection.	anti-spam techniques;email;image processing;sensor	Nobuo Kumagai;Masayoshi Aritsugi	2005	21st International Conference on Data Engineering Workshops (ICDEW'05)	10.1109/ICDE.2005.252	filter;html;image processing;bayesian probability;computer science;pattern matching;digital image processing;data mining;matched filter;world wide web;information retrieval	DB	-19.2831404943615	-58.218855904167526	48201
71de809f4b2d8a76a144d582405f2a7653df513f	hume: human ucca-based evaluation of machine translation		Human evaluation of machine translation normally uses sentence-level measures such as relative ranking or adequacy scales. However, these provide no insight into possible errors, and do not scale well with sentence length. We argue for a semantics-based evaluation, which captures what meaning components are retained in the MT output, thus providing a more fine-grained analysis of translation quality, and enabling the construction and tuning of semantics-based MT. We present a novel human semantic evaluation measure, Human UCCA-based MT Evaluation (HUME), building on the UCCA semantic representation scheme. HUME covers a wider range of semantic phenomena than previous methods and does not rely on semantic annotation of the potentially garbled MT output. We experiment with four language pairs, demonstrating HUME’s broad applicability, and report good inter-annotator agreement rates and correlation with human adequacy scores.	evaluation of machine translation;formal language;human-based computation;inter-rater reliability	Alexandra Birch;Omri Abend;Ondrej Bojar;Barry Haddow	2016			natural language processing;computer science;artificial intelligence;machine learning;data mining;linguistics	NLP	-27.457269969674087	-72.12212022080607	48206
8645453225462a144dd40e84df6d900a538b0198	identifying, attributing and describing spatial bursts	social network;large-scale social media warehousing;information burst;challenging task;multi-terabyte text collection;different geographical location;notable geographically;demographic factor;descriptive keyword;efficient algorithm;spatial burst	User generated content that appears on weblogs, wikis and social networks has been increasing at an unprecedented rate. The wealth of information produced by individuals from different geographical locations presents a challenging task of intelligent processing. In this paper, we introduce a methodology to identify notable geographically focused events out of this collection of user generated information. At the heart of our proposal lie efficient algorithms that identify geographically focused information bursts, attribute them to demographic factors and identify sets of descriptive keywords. We present the results of a prototype evaluation of our algorithms on BlogScope, a large-scale social media warehousing platform. We demonstrate the scalability and practical utility of our proposal running on top of a multi-terabyte text collection.	algorithm;blog;prototype;scalability;social media;social network;terabyte;user-generated content;wiki	Michael Mathioudakis;Nilesh Bansal;Nick Koudas	2010	PVLDB	10.14778/1920841.1920978	simulation;computer science;data mining;database;world wide web	DB	-24.253266461255567	-52.29760147982193	48222
51a9ba166ed24323e43dc0381d29751e36ff1d3a	multi-feature fusion for predicting social media popularity		This paper presents the method that underlies our submission to the popularity prediction task of Social Media Prediction Challenge 2017. The task is designed to predict the impact of sharing different posts for a publisher on social media. There are many factors that influence image popularity; these include not only the visual features of the image, but also the social features, such as user characteristics of its poster and even the upload time. In this project, we propose a fast and effective framework for popularity prediction. First, we investigate and extract visual and social features of images. For the visual feature, we introduce 1) global feature descriptors, such as Local Binary Pattern and Color Names, 2) local feature descriptors, such as Local Maximal Occurrence, and 3) deep features. For the social feature, we adopt users features (average views, group count, and member count), post features (title length, description length, and tag count), and time features (month, weekday, day, and hour). Furthermore, we fed a fusion of multi-feature to Linear Regression, Matrix Factorization based on Time and feature Cluster, and Support Vector Regression models respectively, and present comparative analysis of the prediction results. Finally, we choose the best model to predict the popularity scores of the test images. Experimental results demonstrate that our method can achieve 0.8581, 1.4062 and 0.8625 in terms of Spearman Ranking Correlation, Mean Absolute Error, and Mean Squared Error, respectively.	approximation error;belief propagation;best practice;lr parser;maximal set;mean squared error;performance;qualitative comparative analysis;semiconductor research corporation;social media;support vector machine;upload	Jinna Lv;Wu Liu;Meng Zhang;He Gong;Bin Wu;Huadong Ma	2017		10.1145/3123266.3127897	support vector machine;local binary patterns;linear regression;computer science;machine learning;popularity;matrix decomposition;mean squared error;artificial intelligence;upload;ranking	NLP	-18.079215635225896	-69.15736486186434	48248
4b8784231ee2909d7ad58771a7cab2d414ed823a	a classification tree approach to automatic segmentation of japanese compound sentences	conference paper	It is well known that direct parsing of a long Japanese compound sentence is extremely difficult. Various pre-processing methods have been proposed to segment such a sentence into shorter, simpler ones prior to parsing. The problem with the conventional methods is that some kind of segmentation patterns or heuristic preference scores must be given manually, hence no guarantee for optimality. This paper proposes a new method of sentence segmentation based on a classification tree technique. In this method, optimal segmentation patterns and the optimal order of their application are automatically acquired from training data, linguistic phenomena together with their occurence frequencies being taken into account. Generation of a classification tree is conducted on an EDR corpus, and evaluation results are reported. It is shown that pruning reduces the tree size by a factor of about 1/4 without affecting the performance.	bluetooth;classification chart;decision tree learning;heuristic;parsing;preprocessor;sentence boundary disambiguation	Yujie Zhang;Kazuhiko Ozeki	1999			natural language processing;speech recognition;computer science;pattern recognition	NLP	-24.340081233484902	-77.43278440408913	48268
4bcbebb242bd0125922c21594fcf7dacfc50b3f8	generating succinct titles for web urls	search engine;web pages;web page title generation;automatic generation;quicklinks;sitemaps	How can a search engine automatically provide the best and most appropriate title for a result URL (link-title) so that users will be persuaded to click on the URL? We consider the problem of automatically generating link-titles for URLs and propose a general statistical framework for solving this problem. The framework is based on using information from a diverse collection of sources, each of which can be thought of as contributing one or more candidate link-titles for the URL. It can also incorporate the context in which the link-title will be used, along with constraints on its length. Our framework is applicable to several scenarios: obtaining succinct titles for displaying quicklinks, obtaining titles for URLs that lack a good title, constructing succinct sitemaps, etc. Extensive experiments show that our method is very effective, producing results that are at least 20% better than non-trivial baselines.	aggregate data;baseline (configuration management);display resolution;experiment;html;sitemaps;web search engine	Deepayan Chakrabarti;Ravi Kumar;Kunal Punera	2008		10.1145/1401890.1401905	semantic url;url normalization;site map;computer science;web page;data mining;world wide web;rewrite engine;information retrieval;search engine	AI	-30.768166513112043	-54.59227180475796	48284
11b9d27c58482a769ae8c3f82c7dda2cf8f8327f	the hidden tag model: synchronous grammars for parsing resource-poor languages	resource-poor language;training data;levantine arabic;synchronous grammar;synchronous tree substitution grammar;related language;resource-rich side;synchronous tag formalism;resource-poor side;standard arabic;hidden tag model;abundant training data;empirical natural language processing	This paper discusses a novel probabilistic synchronous TAG formalism, synchronous Tree Substitution Grammar with sister adjunction (TSG+SA). We use it to parse a language for which there is no training data, by leveraging off a second, related language for which there is abundant training data. The grammar for the resource-rich side is automatically extracted from a treebank; the grammar on the resource-poor side and the synchronization are created by handwritten rules. Our approach thus represents a combination of grammar-based and empirical natural language processing. We discuss the approach using the example of Levantine Arabic and Standard Arabic. 1 Parsing Arabic Dialects and Tree Adjoining Grammar The Arabic language is a collection of spoken dialects and a standard written language. The standard written language is the same throughout the Arab world, Modern Standard Arabic (MSA), which is also used in some scripted spoken communication (news casts, parliamentary debates). It is based on Classical Arabic and is not a native language of any Arabic speaking people, i.e., children do not learn it from their parents but in school. Thus most native speakers of Arabic are unable to produce sustained spontaneous MSA. The dialects show phonological, morphological, lexical, and syntactic differences comparable to This work was primarily carried out while the first author was at the University of Maryland Institute for Advanced Computer Studies. those among the Romance languages. They vary not only along a geographical continuum but also with other sociolinguistic variables such as the urban/rural/Bedouin dimension. The multidialectal situation has important negative consequences for Arabic natural language processing (NLP): since the spoken dialects are not officially written and do not have standard orthography, it is very costly to obtain adequate corpora, even unannotated corpora, to use for training NLP tools such as parsers. Furthermore, there are almost no parallel corpora involving one dialect and MSA. The question thus arises how to create a statistical parser for an Arabic dialect, when statistical parsers are typically trained on large corpora of parse trees. We present one solution to this problem, based on the assumption that it is easier to manually create new resources that relate a dialect to MSA (lexicon and grammar) than it is to manually create syntactically annotated corpora in the dialect. In this paper, we deal with Levantine Arabic (LA). Our approach does not assume the existence of any annotated LA corpus (except for development and testing), nor of a parallel LA-MSA corpus. The approach described in this paper uses a special parameterization of stochastic synchronous TAG (Shieber, 1994) which we call a “hidden TAG model.” This model couples a model of MSA trees, learned from the Arabic Treebank, with a model of MSA-LA translation, which is initialized by hand and then trained in an unsupervised fashion. Parsing new LA sentences then entails simultaneously building a forest of MSA trees and the corresponding forest of LA trees. Our implementation uses an extension of our monolingual parser (Chiang, 2000) based on tree-substitution grammar with sister adjunction (TSG+SA). The main contributions of this paper are as follows: 1. We introduce the novel concept of a hidden TAG model. 2. We use this model to combine statistical approaches with grammar engineering (specifically motivated from the linguistic facts). Our approach thus exemplifies the specific strength of a grammar-based approach. 3. We present an implementation of stochastic synchronous TAG that incorporates various facilities useful for training on real-world data: sister-adjunction (needed for generating the flat structures found in most treebanks), smoothing, and Inside-Outside reestimation. This paper is structured as follows. We first briefly discuss related work (Section 2) and some of the linguistic facts that motivate this work (Section 3). We then present the formalism, probabilistic model, and parsing algorithm (Section 4). Finally, we discuss the manual grammar engineering (Section 5) and evaluation (Section 6). 2 Related Work This paper is part of a larger investigation into parsing Arabic dialects (Rambow et al., 2005; Chiang et al., 2006). In that investigation, we examined three different approaches: Sentence transduction, in which a dialect sentence is roughly translated into one or more MSA sentences and then parsed by an MSA parser. Treebank transduction, in which the MSA treebank is transduced into an approximation of a LA treebank, on which a LA parer is then trained. Grammar transduction, which is the name given in the overview papers to the approach discussed in this paper. The present paper provides for the first time a complete technical presentation of this approach. Overall, grammar transduction outperformed the other two approaches. In other work, there has been a fair amount of interest in parsing one language using another language, see for example (Smith and Smith, 2004; Hwa et al., 2004). Much of this work, like ours, relies on synchronous grammars (CFGs). However, these approaches rely on parallel corpora. For MSA and its dialects, there are no naturally occurring parallel corpora. It is this fact that has led us to investigate the use of explicit linguistic knowledge to complement machine learning.	algorithm;approximation;arabic chat alphabet;formal grammar;lexicon;machine learning;natural language processing;parallel text;parse tree;parsing;semantics (computer science);smoothing;spontaneous order;statistical model;tag system;text corpus;transduction (machine learning);tree-adjoining grammar;treebank;triune continuum paradigm;unsupervised learning;word lists by frequency	David Chiang;Owen Rambow	2006			natural language processing;context-sensitive grammar;synchronous context-free grammar;speech recognition;computer science;affix grammar;parsing;extended affix grammar;context-free language;linguistics;mildly context-sensitive grammar formalism	NLP	-29.314074156150326	-75.54932149065517	48368
2040a18ef36c616378fd3795a9c862d6283b46d3	turn the page: automated traversal of paginated websites	paginated web;feature selection;pagination link;feature extraction;automated traversal;wide settings range;data extraction system;exhaustive exploration;content-intensive web site;broad range;wide range;textsc l	Content-intensive web sites, such as Google or Amazon, paginate their results to accommodate limited screen sizes. Thus, human users and automatic tools alike have to traverse the pagination links when they crawl the site, extract data, or automate common tasks, where these applications require access to the entire result set. Previous approaches, as well as existing crawlers and automation tools, rely on simple heuristics (e.g., considering only the link text), falling back to an exhaustive exploration of the site where those heuristics fail. In particular, focused crawlers and data extraction systems target only fractions of the individual pages of a given site, rendering a highly accurate identification of pagination links essential to avoid the exhaustive exploration of irrelevant pages. We identify pagination links in a wide range of domains and sites with near perfect accuracy (99%). We obtain these results with a novel framework for web block classification, BERyL, that combines rule-based reasoning for feature extraction and machine learning for feature selection and classification. Through this combination, BERyL is applicable in a wide settings range, adjusted to maximise either precision, recall, or speed. We illustrate how BERyL minimises the effort for feature extraction and evaluate the impact of a broad range of features (content, structural, and visual).	anchor text;feature extraction;feature model;feature selection;first-order predicate;focused crawler;heuristic (computer science);logic programming;machine learning;relevance;rendering (computer graphics);result set;rewriting;traverse;tree traversal	Tim Furche;Giovanni Grasso;Andrey Kravchenko;Christian Schallhart	2012		10.1007/978-3-642-31753-8_27	computer science;machine learning;data mining;world wide web	DB	-30.67976393985963	-54.60883628306251	48437
d70aedeafc16123085b1ac08b4b94288222942d6	estimating numerical attributes by bringing together fragmentary clues		This work is an attempt to automatically obtain numerical attributes of physical objects. We propose representing each physical object as a feature vector and representing sizes as linear functions of feature vectors. We train the function in the framework of the combined regression and ranking with many types of fragmentary clues including absolute clues (e.g., A is 30cm long) and relative clues (e.g., A is larger than B).	feature vector;lexical definition;linear function;numerical analysis;rare events;thesaurus;word-sense disambiguation;wordnet	Hiroya Takamura;Jun'ichi Tsujii	2015			linear function;machine learning;artificial intelligence;feature vector;computer science;ranking	DB	-20.294276982027696	-71.78730979661479	48463
517c19e152fc279f051e2203148cd1c6b7682e09	stringlish: improved english string searching in binary files	string searching;binary images;english	When analyzing binary files, printable strings are easy to find, but a naive approach yields a large number of false positives: 'uninteresting' string-like sequences that occur by chance in the binary. We present a lightweight yet surprisingly effective method of filtering printable strings for English or English-like sequences. Copyright © 2015 John Wiley & Sons Ltd.	binary file;string searching algorithm	John Aycock	2015	Softw., Pract. Exper.	10.1002/spe.2327	computer science;theoretical computer science;english;world wide web;algorithm	SE	-32.079810613422396	-57.65310680840417	48472
43adf3130dacb88e09d34e5ca00429bd5142235a	learning times for large lexicons through cross-situational learning	slow mapping;cross situational learning;retention;words;journal article;infants;statistics;fast mapping;situated learning;lexicon learning time;language;labels;world learning	Cross-situational learning is a mechanism for learning the meaning of words across multiple exposures, despite exposure-by-exposure uncertainty as to a word's true meaning. Doubts have been expressed regarding the plausibility of cross-situational learning as a mechanism for learning human-scale lexicons in reasonable timescales under the levels of referential uncertainty likely to confront real word learners. We demonstrate mathematically that cross-situational learning facilitates the acquisition of large vocabularies despite significant levels of referential uncertainty at each exposure, and we provide estimates of lexicon learning times for several cross-situational learning strategies. This model suggests that cross-situational word learning cannot be ruled out on the basis that it predicts unreasonably long lexicon learning times. More generally, these results indicate that there is no necessary link between the ability to learn individual words rapidly and the capacity to acquire a large lexicon.	estimated;learning disorders;lexicon;observation result uncertainty;plausibility structure;vocabulary	Richard A. Blythe;Kenny Smith;Andrew D. M. Smith	2010	Cognitive science	10.1111/j.1551-6709.2009.01089.x	psychology;situated learning;natural language processing;multi-task learning;instance-based learning;preference learning;algorithmic learning theory;sequence learning;computer science;linguistics;language;communication;fast mapping	ML	-10.475792326740454	-76.00578619405901	48482
f0c1cd493dec605c9d4b33178eeb9c92a11ef30f	contrast perception across human cognitive style		In the paper we tested hypothesis that subjects’ preferences to images with different contrast level is not only a function of image content but also of an individual pattern of perceptual organisation, which in terms of cognitive psychology is called a cognitive style. As reference to subjects’ perceptual preferences we used FRIS(textregistered ) [15] that is a psychometric model and also an inventory for cognitive styles measurement. To test the hypothesis, the perceptual experiments were performed on a database included different content images. The received results confirm the given hypothesis, especially the obtained statistically significant results between idea-based thinking group and the rest of the thinking groups given for low contrast level.		Anna Lewandowska;Anna Samborska-Owczarek;Malwina Dzisko	2018		10.1007/978-3-319-93000-8_39	computer science;artificial intelligence;pattern recognition;perception;cognitive style;cognitive psychology	HCI	-7.661549286006549	-76.72717819969772	48595
19f698dc53e8e4e805371aaa3590fb45544a1f2e	carbbuilder: software for building molecular models of complex oligo- and polysaccharide structures	shigella flexneri;polysaccharide conformation;3d model;c albicans;carbohydrate	CarbBuilder is a portable software tool for producing three-dimensional molecular models of carbohydrates from the simple text specification of a primary structure. CarbBuilder can generate a wide variety of carbohydrate structures, ranging from monosaccharides to large, branched polysaccharides. Version 2.0 of the software, described in this article, supports monosaccharides of both mammalian and bacterial origin and a range of substituents for derivatization of individual sugar residues. This improved version has a sophisticated building algorithm to explore the range of possible conformations for a specified carbohydrate molecule. Illustrative examples of models of complex polysaccharides produced by CarbBuilder demonstrate the capabilities of the software. CarbBuilder is freely available under the Artistic License 2.0 from https://people.cs.uct.ac.za/~mkuttel/Downloads.html. © 2016 Wiley Periodicals, Inc.		Michelle M. Kuttel;Jonas Ståhle;Göran Widmalm	2016	Journal of computational chemistry	10.1002/jcc.24428	biochemistry	Comp.	-4.648188011949253	-57.95839451909344	48628
b35ba82ed916143390c6a14f750b87e7763fddbb	using argument mining to assess the argumentation quality of essays		Argument mining aims to determine the argumentative structure of texts. Although it is said to be crucial for future applications such as writing support systems, the benefit of its output has rarely been evaluated. This paper puts the analysis of the output into the focus. In particular, we investigate to what extent the mined structure can be leveraged to assess the argumentation quality of persuasive essays. We find insightful statistical patterns in the structure of essays. From these, we derive novel features that we evaluate in four argumentation-related essay scoring tasks. Our results reveal the benefit of argument mining for assessing argumentation quality. Among others, we improve the state of the art in scoring an essay’s organization and its argument strength.	algorithm;automated essay scoring;ground truth;mined;persuasive technology;statistical model	Henning Wachsmuth;Khalid Al Khatib;Benno Stein	2016			artificial intelligence;argumentation theory;natural language processing;computer science;data science	NLP	-23.67232440784168	-59.1543960749028	48668
7a684045afae2ccf40338ff07b8fa429bad93a57	c4corpus: multilingual web-size corpus with free license		Large Web corpora containing full documents with permissive licenses are crucial for many NLP tasks. In this article we present the construction of 12 million-pages Web corpus (over 10 billion tokens) licensed under CreativeCommons license family in 50+ languages that has been extracted from CommonCrawl, the largest publicly available general Web crawl to date with about 2 billion crawled URLs. Our highly-scalable Hadoop-based framework is able to process the full CommonCrawl corpus on 2000+ CPU cluster on the Amazon Elastic Map/Reduce infrastructure. The processing pipeline includes license identification, state-of-the-art boilerplate removal, exact duplicate and near-duplicate document removal, and language detection. The construction of the corpus is highly configurable and fully reproducible, and we provide both the framework (DKPro C4CorpusTools) and the resulting data (C4Corpus) to the research community.	amazon elastic compute cloud (ec2);apache hadoop;central processing unit;elastic map;free license;language identification;natural language processing;scalability;text corpus	Ivan Habernal;Omnia Zayed;Iryna Gurevych	2016			license;artificial intelligence;natural language processing;world wide web;boilerplate text;language identification;elastic map;web crawler;computer science	NLP	-31.821464738150645	-72.21847257873208	48681
2b52982ca4d1e0e487557ac0e6b9f021aea14f00	advances in multilingual and multimodal information retrieval: 8th workshop of the cross-language evaluation forum, clef 2007, budapest, hungary, september 19-21, 2007, revised selected papers	multimodal information retrieval;selected papers;cross-language evaluation forum;information retrieval	This book constitutes the thoroughly refereed proceedings of the 8th Workshop of the Cross-Language Evaluation Forum, CLEF 2007, held in Budapest, Hungary, September 2007. The revised and extended papers were carefully reviewed and selected for inclusion in the book. There are 115 contributions in total and an introduction. The seven distinct evaluation tracks in CLEF 2007, are designed to test the performance of a wide range of multilingual information access systems or system components. The papers are organized in topical sections on Multilingual Textual Document Retrieval (Ad Hoc), Domain-Specific Information Retrieval (Domain-Specific), Multiple Language Question Answering (QA@CLEF), cross-language retrieval in image collections (Image CLEF), cross-language speech retrieval (CL-SR), multilingual Web retrieval (WebCLEF), cross-language geographical retrieval (GeoCLEF), and CLEF in other evaluations.		Carol Peters;Valentin Jijkoun;Thomas Mandl;Henning Müller;Douglas W. Oard;Anselmo Peñas;Vivien Petras;Diana Santos	2008		10.1007/978-3-540-85760-0	information retrieval	Web+IR	-32.029328077611105	-63.09025924341951	48760
00fe09e4c97e4f4056fdd5301e22d12255d18353	autonomous news clustering and classification for an intelligent web portal	statistical natural language processing;classification;text classification;machine learning;web portal;intelligent agent;text clustering;automatic classification;natural language processing;news portal	The paper presents an autonomous text classification module for a news web portal for the Romanian language. Statistical natural language processing techniques are combined in order to achieve a completely autonomous functionality of the portal. The news items are automatically collected from a large number of news sources using web syndication. Afterward, machine-learning techniques are used for achieving an automatic classification of the news stream. Firstly, the items are clustered using an agglomerative algorithm and the resulting groups correspond to the main news topics. Thus, more information about each of the main topics is acquired from various news sources. Secondly, text classification algorithms are applied to automatically label each cluster of news items in a predetermined number of classes. More than a thousand news items were employed for both the training and the evaluation of the classifiers. The paper presents a complete comparison of the results obtained for each method.		Traian Rebedea;Stefan Trausan-Matu	2008		10.1007/978-3-540-68123-6_52	biological classification;computer science;artificial intelligence;machine learning;data mining;database;world wide web;intelligent agent;information retrieval	ML	-28.399394231777915	-65.38750109484431	48761
98012f2c789ffafbceb0a7bfc4e8e1446f8ed2be	semantic relations established by specialized processes expressed by nouns and verbs: identification in a corpus by means of syntactico-semantic annotation		This article presents the methodology and results of the analysis of terms referring to processes expressed by verbs or nouns in a corpus of specialized texts dealing with ceramics. Both noun and verb terms are explored in context in order to identify and represent the semantic roles held by their participants (arguments and circumstants), and therefore explore some of the relations established by these terms. We present a methodology for the identification of related terms that take part in the development of specialized processes and the annotation of the semantic roles expressed in these contexts. The analysis has allowed us to identify participants in the process, some of which were already present in our previous work, but also some new ones. This method is useful in the distinction of different meanings of the same verb. Contexts in which processes are expressed by verbs have proved to be very informative, even if they are less frequent in the corpus. This work is viewed as a first step in the implementation – in ontologies – of conceptual relations in which activities are involved.	information;ontology (information science);text corpus	Nava Maroto;Marie-Claude L'Homme;Amparo Alcina	2012			natural language processing;linguistics	NLP	-28.49484208540462	-72.26385929118953	48765
6646e631cb31f63c61eff249075a0736df26c2ff	chniese document classification using field association knowledge base	statistical methods chniese document classification field association knowledge base fakb discriminating terms document fields;document handling;data collection testing accuracy statistical analysis training data niobium knowledge based systems;statistical analysis document handling knowledge based systems natural language processing pattern classification;statistical analysis;pattern classification;field association terms document classification field asociation knowledge base;natural language processing;knowledge based systems	Field Association (FA) terms are a limited set of discriminating terms that offer human knowledge to identify document (text) fields. Field association knowledge base (FAKB) is composed of FA terms and their potential hierarchical relationship of the fields belongs to. The primary goal of this research is to build a system that can imitate the process whereby humans recognize the fields by looking at a few Chinese FA terms in a document (text). The documents classification experiment is made on two data collections under different circumstances, including 4000 and 1300 documents respectively. FAKB outperforms all the other statistical methods (SVMs, kNN, and NB) with the average accuracies of 97.7% and 89%. All the experimental results clearly prove that the presented novel method is effective in Chinese document classification.	document classification;knowledge base;knowledge management;naive bayes classifier;weatherstar	Li Wang;Kui Jiang;Xingyun Geng;Yuanpeng Zhang;Dong Zhou;Jiancheng Dong	2012	2012 IEEE 2nd International Conference on Cloud Computing and Intelligence Systems	10.1109/CCIS.2012.6664616	computer science;data science;knowledge-based systems;pattern recognition;data mining	DB	-24.236655050483364	-64.51962689333966	48801
1d34857d8a2abbdb7479defdcd40f54e19e5827d	supervised phrase table triangulation with neural word embeddings for low-resource languages		In this paper, we develop a supervised learning technique that improves noisy phrase translation scores obtained by phrase table triangulation. In particular, we extract word translation distributions from small amounts of source-target bilingual data (a dictionary or a parallel corpus) with which we learn to assign better scores to translation candidates obtained by triangulation. Our method is able to gain improvement in translation quality on two tasks: (1) On Malagasy-to-French translation via English, we use only 1k dictionary entries to gain +0.5 Bleu over triangulation. (2) On Spanish-to-French via English we use only 4k sentence pairs to gain +0.7 Bleu over triangulation interpolated with a phrase table extracted from the same 4k sentence pairs.	algorithm;artificial neural network;bleu;dictionary;information sciences institute;interpolation;parallel text;polygon triangulation;supervised learning	Tomer Levinboim;David Chiang	2015			natural language processing;speech recognition;computer science;pattern recognition	NLP	-20.216238154927346	-77.68365106646048	48938
546b9b2f09bb5fcfd079f2f2ebd353777065734e	the automatic generation of templates for automatic abstracting	information content;automatic generation;research paper	Our goal is the automatic abstraction of journal articles, initially in the field of crop protection. We build a set of templates against which the original text is compared. The templates are designed so that they match the text at points of high information content, where inferences can be made about which expressions best reflect the content of the document. Strings found by matching templates are assigned roles specific to each template. These roles correspond to slots in a frame which is used to represent the document as a whole. An abstract is generated which contains the concept-strings selected from the text.	self-information;string (computer science)	Michael P. Oakes;Chris D. Paice	1999			self-information;computer science;data mining;world wide web;information retrieval	NLP	-29.825100628324584	-65.95522127978221	48950
5edb917731a383e74290eb8507fd1f20b60b67d6	deep graph search based disease related knowledge summarization from biomedical literature	diseases data mining semantics bioinformatics mutual information genetics information filtering;information filtering;semantics;data mining;genetics;correlation extraction methods deep graph search method disease related knowledge summarization biomedical literature kullback leibler divergence mutual information metric disease salient information extraction depth first search random walk algorithm;tree searching bioinformatics diseases random processes;diseases;mutual information;random walk knowledge summarization kullback leibler divergence mutual information;bioinformatics	In this paper, we present an approach to automatically construct disease related knowledge summarization from biomedical literature. In this approach, first Kullback-Leibler divergence combined with mutual information metric is used to extract disease salient information. Then deep search based on depth first search (DFS) is applied to find hidden relations between biomedical entities. Finally random walk algorithm is exploited to filter out the weak relations. The experimental results show that our approach achieves a precision of 60% and a recall of 61% on salient information extraction, and outperforms the method of Combo. In addition, the method of deep search obtains more hidden relations than the original correlation extraction methods.	algorithm;depth-first search;entity;graph traversal;information extraction;kullback–leibler divergence;mutual information	Xiaofang Wu;Zhihao Yang;Zhiheng Li;Yuanyuan Sun;Hongfei Lin;Jian Wang	2014	2014 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)	10.1109/BIBM.2014.6999210	computer science;bioinformatics;automatic summarization;machine learning;pattern recognition;data mining;semantics;mutual information;genetics;statistics	AI	-26.3870396697483	-63.49875642791359	49030
758ec3a7a5f0bf94c02f913cfa32df79a6707ec3	knowledge-based word sense disambiguation using topic models		Word Sense Disambiguation is an open problem in Natural Language Processing which is particularly challenging and useful in the unsupervised setting where all the words in any given text need to be disambiguated without using any labeled data. Typically WSD systems use the sentence or a small window of words around the target word as the context for disambiguation because their computational complexity scales exponentially with the size of the context. In this paper, we leverage the formalism of topic model to design a WSD system that scales linearly with the number of words in the context. As a result, our system is able to utilize the whole document as the context for a word to be disambiguated. The proposed method is a variant of Latent Dirichlet Allocation in which the topic proportions for a document are replaced by synset proportions. We further utilize the information in the WordNet by assigning a non-uniform prior to synset distribution over words and a logistic-normal prior for document distribution over synsets. We evaluate the proposed method on Senseval-2, Senseval-3, SemEval-2007, SemEval2013 and SemEval-2015 English All-Word WSD datasets and show that it outperforms the state-of-the-art unsupervised knowledge-based WSD system by a significant margin.	computational complexity theory;latent dirichlet allocation;natural language processing;semantics (computer science);synonym ring;topic model;unsupervised learning;web services for devices;word sense;word-sense disambiguation;wordnet	Devendra Singh Chaplot;Ruslan Salakhutdinov	2018			machine learning;computer science;open problem;artificial intelligence;topic model;latent dirichlet allocation;word-sense disambiguation;wordnet;computational complexity theory;formalism (philosophy);sentence	NLP	-23.610965041619306	-73.18020922840915	49190
8360b1f19f637c91ccc561588cd5dd0cd9f7960d	iconicity vs. systematicity in artificial language learning		A foundational assumption in linguistics has been that words and their meanings are arbitrarily related; however, this position has been challenged recently. Experiments have shown that both systematic (where similar objects have similar labels) and iconic (words ‘resemble’ the objects they label) associations between words and objects facilitate learning. However, these two literatures remain confounded: the degree to which increased learnability is driven by iconicity rather than systematicity has not been disentangled. Here we present the results of two studies testing the differences in learnability between artificial lexica that are either conventionally systematic, or both systematic and cross-modally iconic. In the first study we find that both conventional and iconic systematic lexicons are equally learnable, but iconic mappings provide an early learnability advantage. In the second study we find that the presence of sound-symbolic associations for one dimension can interfere with the learning of conventional associations on another dimension.	experiment;learnability;lexicon	Alan Ks Nielsen;Julia Simner;Simon Kirby;Kenny Smith	2017			cognitive psychology;psychology;constructed language;iconicity	NLP	-10.340949490886992	-75.7379319931715	49240
45f0115613b9c79572af9365daafc58bede9851e	acquiring common sense spatial knowledge through implicit spatial templates		Spatial understanding is a fundamental problem with widereaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., “on”, “below”, etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., “glass on table”), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., “man riding horse”). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (“where is the man w.r.t. a horse when the man is walking the horse?”). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,“man walking dog”) have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., “dog”). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.	cisco pix;ground truth;parsing;question answering;structured text;supervised learning;word embedding	Guillem Collell;Luc Van Gool;Marie-Francine Moens	2018			form of the good;artificial intelligence;machine learning;structured text;computer science;template	AI	-15.656861058323807	-70.33829593399057	49258
b39f1540e985a3bde097b1488c5df5cd7abb77b8	two stage semantic relation extraction	association rules semantic relation extraction ontology web intelligent question answering system pc troubleshooting domain product trouble relation product attribute relation information extraction;semantic relation extraction;computers;kernel;information extraction;information retrieval;product attribute relation;association rules;data mining;ontologies artificial intelligence;relation extraction;data mining ontologies association rules taxonomy hybrid intelligent systems computer science intelligent systems computer architecture clustering methods;ontology learning;web intelligence;internet;ontologies artificial intelligence data mining information retrieval internet;association rule;question answering system;pc troubleshooting domain;web intelligent question answering system;feature extraction;two stage extraction ontology learning semantic relation extraction;ontologies;two stage extraction;semantic relations;ontology;context;extraction method;product trouble relation	"""Ontology is applied to a real-world web intelligent question answering system in PC troubleshooting domain as an improvement, so semantic relations need to be extracted to construct ontology semi-automatically. The paper mainly pays attention to """"product-trouble"""" relation and """"product-attribute"""" relation. A two stage semantic relation extraction method is proposed. In stage one, relation identification is executed to produce high-precision relations instance as the seed for next stage; stage two is the actual relation extraction process, the related terms which describe troubleshooting information and attribute information are extracted based on association rules, then the terms is clustered to find target semantic relation. The relation extraction integrated the advantage of pattern-based and clustering-based methods. The experimental results show it is superior to individual pattern-based and clustering-based methods in precision and recall."""	association rule learning;cluster analysis;ontology components;precision and recall;question answering;relationship extraction;semiconductor industry	Jibin Fu;Xiaozhong Fan;Jintao Mao;Xiaoming Liu	2009	2009 Ninth International Conference on Hybrid Intelligent Systems	10.1109/HIS.2009.71	semantic similarity;computer science;data mining;database;information retrieval	AI	-28.389049842251534	-65.44948297207965	49277
2e63d572ad16d3ead10f37b93b47706de705771c	graphical models for social behavior modeling in face-to face interaction	structure learning;behavioral model;multimodal generation;dbn;face to face interaction	The goal of this paper is to model the coverbal behavior of a subject involved in face-to-face social interactions. For this end, we present a multimodal behavioral model based on a Dynamic Bayesian Network (DBN). The model was inferred from multimodal data of interacting dyads in a specific scenario designed to foster mutual attention and multimodal deixis of objects and places in a collaborative task. The challenge for this behavioral model is to generate coverbal actions (gaze, hand gestures) for the subject given his verbal productions, the current phase of the interaction and the perceived actions of the partner. In our work, the structure of the DBN was learned from data, which revealed an interesting causality graph describing precisely how verbal and coverbal human behaviors are coordinated during the studied interactions. Using this structure, DBN exhibits better performances compared to classical baseline models such as Hidden Markov Models (HMMs) and Hidden Semi-Markov Models (HSMMs). We outperform the baseline in both measures of performance, i.e. interaction unit recognition and behavior generation. DBN also reproduces more faithfully the coordination patterns between modalities observed in ground truth compared to the baseline models.	baseline (configuration management);behavior model;causality;computer performance;conditional (computer programming);dynamic bayesian network;graphical model;ground truth;heart rate variability;hidden markov model;hidden semi-markov model;icub;instruction unit;latent variable;long short-term memory;markov chain;multimodal interaction;relevance;robot;semiconductor industry;ws-coordination	Alaeddine Mihoub;Gérard Bailly;Christian Wolf;Frédéric Elisei	2016	Pattern Recognition Letters	10.1016/j.patrec.2016.02.005	behavioral modeling;computer vision;face-to-face interaction;computer science;artificial intelligence;machine learning	ML	-11.151821111220215	-70.52701953128043	49369
1eb28b2b6c686945e7b9433f101c66730bddc0d1	motifs and motif generalization in chinese word networks	complex networks;motif	The most significant semantic unit of Chinese language is words composed of individual characters. This compositional structure produces great variability and representability compared to individual characters, which is quite distinct from other languages. In this paper we utilized complex networks to model the composition of words from characters. We focus on network motifs, the local pattern which appears more often in a statistically significant sense. Network motifs describe the most significant connection pattern between these nodes. We investigated their functions and semantical relationship. We also investigated different generalizations of network motifs and analyzed the larger pattern in the complex networks. As the word network is quite huge and the motif detection is very slow when motifs are much larger, for larger pattern in the network we used topology generalization of simple motifs rather than carry out a thorough motif detection task. The results on motifs and motif generalization in this paper not only offer us a big picture how Chinese words are formed, but also support the conclusion that motifs play a very important role in research of complex systems.	complex network;complex systems;motif;social network;spatial variability	Jianyu Li;Feng Xiao;Jie Zhou;Zhanxin Yang	2012		10.1016/j.procs.2012.04.059	artificial intelligence;network motif;machine learning;mathematics	AI	-13.372749784810045	-75.31074146283534	49370
ab4db405af79e960754e67bfe941dbe8052da710	zero pronoun resolution based on automatically constructed case frames and structural preference of antecedents	case analysis;zero pronoun detection;structural preference;case frame;zero pronoun;close position;zero pronoun resolution;japanese text;appropriate antecedent;antecedent estimation	This paper describes a method to detect and resolve zero pronouns in Japanese text. We detect zero pronouns by case analysis based on automatically constructed case frames, and rank candidate antecedents of a zero pronoun based on similarity to examples in the case frames. We also introduce an order of antecedent location preference to precisely capture the tendency that a zero pronoun has its antecedent in its close position. Large experimental results on 100 articles indicate that the precision and recall of zero pronoun detection are 87.1% and 74.8% respectively and the accuracy of antecedent estimation is 61.8%.	anaphora (linguistics);baseline (configuration management);precision and recall	Daisuke Kawahara;Sadao Kurohashi	2004		10.1007/978-3-540-30211-7_2	natural language processing	NLP	-25.355348272341196	-70.71765665321236	49413
b87b0b8b2f4f71e2a8730e81e289502cddb826d6	an easter egg hunting approach to test collection building in dynamic domains		Test collections for offline evaluation remain crucial for information retrieval research and industrial practice, yet the classical Sparck Jones and Van Rijsbergen approach to test collection building based on the pooling of runs on a large collection is expensive and being pushed beyond its limits with the ever increasing size and dynamic nature of the collections. We experiment with a novel approach to reusable test collection building, where we inject judged pages into an existing corpus, and have systems retrieve pages from the extended corpus with the aim to create a reusable test collection. In a metaphorical way, we hide the Easter eggs for systems to retrieve. Our experiments exploit the unique setup of the TREC Contextual Suggestion Track, which allowed both submissions from a fixed corpus (ClueWeb12) as well as from the open web. We conduct an extensive analysis of the reusability of the test collection based on ClueWeb12, and find it too low for reliable offline testing. Then, we detail the expansion with judged pages from the open web, and do extensive analysis on the reusability of the resulting expanded test collection, and observe a dramatic increase in reusability. Our approach offers novel and cost effective ways to build new test collections, and to refresh and update existing test collections. This explores new ways of effective maintenance of offline test collections for dynamic domains such as the web.	easter egg (media);experiment;information retrieval;jones calculus;online and offline;open web	Seyyed Hadi Hashemi;Charles L. A. Clarke;Adriel Dean-Hall;Jaap Kamps;Julia Kiseleva	2016			cartography;archaeology;geography	Web+IR	-31.340128329479658	-62.035372889056596	49542
40f9a26022ff400c6678b0a1566d7ebcbe7e4dcc	domain representation using possibility theory: an exploratory study	performance evaluation;information extraction;uncertainty;information retrieval;schoolnet corpus domain representation possibility theory natural language processing domain specific information information extraction natural language documents mathematical process semantic distances possibility distributions possibilistic domain classifier;prototypes;text analysis;uncertainty handling information retrieval natural language processing pattern classification possibility theory statistical distributions text analysis;natural languages;schoolnet corpus;uncertainty handling;mathematical process;data mining;fuzzy set theory;statistical distributions;possibility distributions;fuzzy;natural language;possibility theory natural language processing data mining natural languages fuzzy set theory text analysis uncertainty humans prototypes performance evaluation;possibilistic domain classifier;pattern classification;domain representation;possibility theory;natural language processing nlp;humans;semantic distances;exploratory study;domain specific information;natural language processing;domain specificity;language model;natural language documents;probabilistic reasoning;uncertainty fuzzy language models natural language processing nlp probabilistic reasoning text analysis;possibility distribution;language models	This study explores a new domain representation method for natural language processing based on an application of possibility theory. In our method, domain-specific information is extracted from natural language documents using a mathematical process based on Rieger's notion of semantic distances, and represented in the form of possibility distributions. We implement the distributions in the context of a possibilistic domain classifier, which is trained using the SchoolNet corpus.	authorization;domain-specific language;exploratory testing;ieee xplore;natural language processing;possibility theory;statistical classification	Richard Khoury;Fakhri Karray;Mohamed S. Kamel	2008	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2008.2005011	natural language processing;computer science;artificial intelligence;machine learning;pattern recognition;natural language;language model	NLP	-24.02696910411634	-64.31943486385856	49556
5b09e36ee4431277833202bcbf413bee98cf8a4d	toward a robust spell checker for arabic text		Spell checking is the process of detecting misspelled words in a written text and recommending alternative spellings. The first stage consists of detecting real-word errors and non-word errors in a given text. The second stage consists of error correction. In this paper we propose a novel method for spell checking Arabic text. Our system is a sequential combination of approaches including lexicon based, rule based, and statistical based. The experimental results show that the proposed method achieved good performance in terms of recall rate or precision rate in error detection, and correction comparing to other systems.	spell checker	Mourad Mars	2016		10.1007/978-3-319-42092-9_24	natural language processing;speech recognition;programming language	Robotics	-23.69381481617194	-76.51137493947874	49582
c3a4efd350b5218d841a6010c707f4ca74f457b6	appraisal navigator	opinion mining;appraisal theory;appraisal extraction;sentiment analysis;shallow parsing	Much interesting text n the web consists largely of opinionated or evaluative text, as opposed to directly informative text. The new field of 'sentiment analysis' seeks to characterize such aspects of natural language text, as opposed to just the bare facts. We suggest that 'appraisal expression extraction' should be viewed as a fundamental task for sentiment analysis. We define an 'appraisal expression' to be a piece of text expressing some evaluative stance towards a particular object. The task is to find these elements and characterize the type and orientation (positive or negative) of the evaluative stance, as well as its target and possibly its source. Potential applications of these methods include new approaches to the now-traditional tasks of sentiment classification and pinion mining, as well as possibly for adversarial textual analysis and intention detection for intelligence applications.	information;natural language;sentiment analysis	Navendu Garg;Kenneth Bloom;Shlomo Argamon	2006		10.1145/1148170.1148340	natural language processing;appraisal theory;text mining;computer science;artificial intelligence;machine learning;data mining;world wide web;information retrieval;sentiment analysis	NLP	-23.070386898650458	-60.20690554796351	49603
079cbf73b9a2b26f5f3842baad1b349487d8430b	prompsit's submission to wmt 2018 parallel corpus filtering shared task		This paper describes Prompsit Language Engineering’s submissions to the WMT 2018 parallel corpus filtering shared task. Our four submissions were based on an automatic classifier for identifying pairs of sentences that are mutual translations. A set of hand-crafted hard rules for discarding sentences with evident flaws were applied before the classifier. We explored different strategies for achieving a training corpus with diverse vocabulary and fluent sentences: language model scoring, an active-learning-inspired data selection algorithm and n-gram saturation. Our submissions were very competitive in comparison with other participants on the 100 million word training corpus.	language model;n-gram;parallel text;selection algorithm;statistical classification;text corpus;the 100;vocabulary	Víctor M. Sánchez-Cartagena;Marta Bañón;Sergio Ortiz-Rojas;Gema Ramírez-Sánchez	2018				NLP	-23.045892347708083	-75.60700630383354	49651
c45636cc7ecf92e7bbb1cb57687b18d0373a35e4	ancor, the first large french speaking corpus of conversational speech annotated in coreference to be freely available (ancor, premier corpus de français parlé d'envergure annoté en coréférence et distribué librement) [in french]			text corpus	Judith Muzerelle;Anaïs Lefeuvre;Jean-Yves Antoine;Emmanuel Schang;Denis Maurel;Jeanne Villaneau;Iris Eshkol-Taravella	2013				NLP	-30.872678739422796	-77.2423859674725	49674
4c94a32429ed903ff2d794572c9f2769ff5f1dd0	hybrid approach of situation-oriented classification of sightseeing spot images based on visual and tag information	color feature extraction;classification;geotag;timestamp;tourism informatics	Recent trend on the web is to share their traveling experience via uploading photos to web albums. Shared photos of sightseeing spots are important resources for those who are going to visit there. As sightseeing spot scenes vary with different situations, such as weather and season, automatic classification of photos into different situations is expected to be beneficial for tourists to plan when to visit there. This paper proposes a hybrid approach of combining content-based image classification with filtering based on tag information of image. By using geotag information when retrieving images from web albums, collected images can be limited to a reasonable boundary to eliminate outliers. Content-based image classification groups collected images into night, sunrise/sunset, cloudy, and shine situations. Moreover, by using the timestamp of images, the four situation categories are further verified to increase the accuracy. Experimental results show that the hybrid approach of content-based image classification and tag-based filtering is effective for classifying image into situations with high precision and recall.	artificial intelligence;chi;cloud computing;computer vision;entity–relationship model;feature extraction;geotagging;image processing;information processing;information visualization;institute of electronics, information and communication engineers;javaserver pages;nan;precision and recall;region of interest;tag cloud;upload;web intelligence	Chia-Huang Chen;Yasufumi Takama	2014	J. Inf. Sci. Eng.		timestamp;computer vision;biological classification;computer science;artificial intelligence;operating system;data mining;database;geotagging;computer security;information retrieval	AI	-25.3429007888164	-53.06595636820546	49720
d38fa02b5659ceeaf32a22bb6bcf6d3d4ffb9d44	using knowledge-based relatedness for information retrieval	semantic similarity;information retrieval;semantic relatedness;query and document expansion;knowledge based systems	Traditional information retrieval (IR) systems use keywords to index and retrieve documents. The limitations of keywords were recognized since the early days, specially when different but closely related words are used in the query and the relevant document. Query expansion techniques like pseudo-relevance feedback (PRF) and document clustering techniques rely on the target document set in order to bridge the gap between those words. This paper explores the use of knowledge-based semantic relatedness techniques to overcome the vocabulary mismatch between the query and documents, both on IR and Passage Retrieval for question answering. We performed query expansion and document expansion using WordNet, with positive effects over a language modeling baseline on three datasets, and over PRF on two of those datasets. Our analysis shows that our models and PRF are complementary; in that, PRF is better for easy queries, and our models are stronger for difficult queries and that our models generalize better to other collections, being more robust to parameter adjustments. In addition, we show that our method has a positive impact in an end-to-end question answering system for Basque and that it can be readily applied to other knowledge bases, as our good results using Wikipedia show, paving the way for the use of other knowledge structures such as medical ontologies and linked data repositories.	baseline (configuration management);biological anthropology;blog;cluster analysis;database normalization;end-to-end principle;gene ontology term enrichment;google questions and answers;graph (abstract data type);hoc (programming language);information retrieval;knowledge base;language model;linked data;ontology (information science);primitive recursive function;query expansion;question answering;relevance feedback;semantic similarity;sinclair ql;vocabulary mismatch;web services for devices;wikipedia;wordnet	Arantxa Otegi;Xabier Arregi;Olatz Ansa;Eneko Agirre	2014	Knowledge and Information Systems	10.1007/s10115-014-0785-4	natural language processing;semantic similarity;query expansion;computer science;knowledge-based systems;data mining;database;information retrieval;query language	Web+IR	-30.664082919849502	-66.85387726310523	49728
094a6b6157d58af4b3031e8b78f3f8fbb609553d	dynamic summarization: another stride towards summarization	set theory;stride towards summarization;dynamic summarization;model improvement;dynamic content;model improvement solution;algorithm improvement;dynamic summarization research;classic summarization;information analysis	In order to produce summaries from dynamic content, we address the definition of the dynamic summarization. In this paper, the issue of modeling of dynamic summarization is discussed, and then two solutions of model improvement with set theory and algorithm improvement with reranking are proposed for dynamic summarization from classic summarization. Finally, the performances of these two solutions are evaluated on the dataset of DUC 2007. Our results demonstrate that the model improvement solution is more effective, but as another stride towards summarization, dynamic summarization research still need further study.	algorithm;automatic summarization;dynamic web page;performance;set theory;upsampling	Jin Zhang;Xueqi Cheng;Hongbo Xu	2007	2007 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology - Workshops		multi-document summarization;computer science;data science;dynamic web page;automatic summarization;data mining;data analysis;world wide web;information retrieval;set theory	AI	-27.10165610434671	-56.83016340913923	49731
74e4beccd5411e676a4b6ce83743dd2694670051	relevance feedback for content-based image retrieval using bayesian network	bayesian network;relevant image adoption;content based image retrieval;relevance feedback	Relevance feedback is a powerful query modification technique in the field of content-based image retrieval. The key issue in relevance feedback is how to effectively utilize the feedback information to improve the retrieval performance. This paper presents a relevance feedback scheme using Bayesian network model for feedback information adoption. Relevant images during previous iterations are reasonably incorporated into the current iteration and the chosen relevant images can better capture user’s information need.	bayesian network;content-based image retrieval;information needs;iteration;network model;relevance feedback	Jing Xin;Jesse S. Jin	2003			relevance;computer science;machine learning;data mining;information retrieval	Vision	-30.459652836279172	-58.69458083352761	49845
e918408a39f7304353b11e8168a8205eba16a4a8	textual characteristics for language engineering		Language statistics are widely used to characterize and better understand language. In parallel, the amount of text mining and information retrieval methods grew rapidly within the last decades, with many algorithms evaluated on standardized corpora, often drawn from newspapers. However, up to now there were almost no attempts to link the areas of natural language processing and language statistics in order to properly characterize those evaluation corpora, and to help others to pick the most appropriate algorithms for their particular corpus. We believe no results in the field of natural language processing should be published without quantitatively describing the used corpora. Only then the real value of proposed methods can be determined and the transferability to corpora originating from different genres or domains can be estimated. We lay ground for a language engineering process by gathering and defining a set of textual characteristics we consider valuable with respect to building natural language processing systems. We carry out a case study for the analysis of automotive repair orders and explicitly call upon the scientific community to provide feedback and help to establish a good practice of corpus-aware evaluations.	algorithm;information retrieval;natural language processing;text corpus;text mining	Mathias Bank;Robert Remus;Martin Schierle	2012			natural language processing;speech recognition;linguistics	Web+IR	-29.677324822053198	-73.11441582293239	49855
1c2c161dcc2113b1319520f84a90719a39ed38b8	ahpa-calculating hub and authority for information retrieval	search and retrieval;social network services;web pages;search engines;information retrieval;html;information retrieval web pages web sites social network services humans search engines informatics computer science explosions html;web sites;social network analysis;world wide web;explosions;informatics;humans;computer science	The fast development of the World Wide Web makes searching and retrieving of information to become a not easy task. Two algorithms proposed around the fall of 1996, Page Rank [3] and HITS [9], became the center of majority research efforts. They try to remedy the abundance of results, bringing order with the help of notions related to prestige in social network analysis. Some approaches use the link structure of the web to find the importance of the web pages (Page Rank method [3]) or to determine their authority related to a particular topic (Hub and Authority concept). In this paper we propose a new method for calculating the authority of a web page.	algorithm;information retrieval;pagerank;social network analysis;web page;world wide web	George Stephanides;Mirel Cosulschi;Mihai Gabroveanu;Nicolae Constantinescu	2006	22nd International Conference on Data Engineering Workshops (ICDEW'06)	10.1109/ICDEW.2006.26	web service;web application security;web development;social network analysis;web modeling;site map;web mapping;html;web design;web search engine;web accessibility initiative;web standards;computer science;web navigation;social semantic web;web page;data mining;database;web intelligence;hits algorithm;informatics;web 2.0;world wide web;information retrieval;web server;search engine	DB	-28.37149349765143	-56.00138305055819	49933
9aa9041511b0e1a409a7a9db365b5f1602cb563d	lexical characteristics analysis of chinese clinical documents		Understanding lexical characteristics of clinical documents is the foundation of sublanguage based Medical Language Processing (MLP) approach. However, there are limited studies focused on the lexical characters of Chinese clinical documents. In this study, a lexical characteristics analysis on both syntactic and semantic levels was conducted in a clinical corpus which contains 3,500 clinical documents generated during daily practices. The analysis was based on the automatic tagging results of a lexicon-based part-of-speech (POS) and semantic tagging method. The medical lexicon contains 237,291 entries annotated with both semantic and syntactic classes. The normalized frequency of different terms, syntactic and semantic classes was calculated and visualized. Major contribution of this paper is providing a wide-coverage Chinese medical semantic lexicon and presenting the lexical characteristics of Chinese clinical documents. Both of these will lay a good foundation for sublanguage based MLP studies in China.	lexicon;memory-level parallelism;natural language processing;normalized frequency (fiber optics);part-of-speech tagging;sublanguage	Meizhi Ju;Haomin Li;Huilong Duan	2015	2015 7th International Conference on Information Technology in Medicine and Education (ITME)	10.18653/v1/W15-3813	natural language processing;semantic computing;speech recognition;computer science;semantic compression;linguistics;lexical choice;lexical functional grammar;lexical grammar	NLP	-29.94232034725679	-72.45520104735184	49937
7f8ebd4a495925d70dc89ae9ee5f8ed24ac0f98b	pseudo relevance feedback using fast xml retrieval	bottom up;vector space model;pseudo relevance feedback;xml retrieval;large scale	This paper reports the result of experimentation of our approach using the vector space model for retrieving large-scale XML data. The purposes of the experiments are to improve retrieval precision on the INitiative for the Evaluation of XML Retrieval (INEX) 2008 Adhoc Track, and to compare the retrieval time of our system to other systems on the INEX 2008 Efficiency Track. For the INEX 2007 Adhoc Track, we developed a system using a relative inverted-path (RIP) list and a Bottom-UP approach. The system achieved reasonable retrieval time for XML data. However the system has a room for improvement in terms of retrieval precision. So for INEX 2008, the system uses CAS titles and Pseudo Relevance Feedback (PRF) to improve retrieval precision.	relevance feedback;xml retrieval	Hiroki Tanioka	2008		10.1007/978-3-642-03761-0_22	computer science;data mining;database;information retrieval	Web+IR	-32.615036611572485	-63.37720552965647	49946
03622ed4c0d3e754998153c4c8d2c44f086f4a60	retrieving nasa problem reports with natural language	traitement automatique des langues naturelles;textual database;independent set;information retrieval;rule based;linguistique appliquee;automaton;methode;base de donnees textuelles;automata;automatic recognition;evaluative study;etiquetage automatique;natural language;part of speech tagging;automate;part of speech;computational linguistics;partie du discours;linguistique informatique;question;method;natural language processing;deterministic finite state automata;reconnaissance automatique;recherche d information;etude evaluative;tagging;applied linguistics	A system that retrieves problem reports from a NASA database is described. The database is queried with natural language questions. Part-of-speech tags are first assigned to each word in the question using a rule-based tagger. A partial parse of the question is then produced with independent sets of deterministic finite state automata. Using partial parse information, a look up strategy searches the database for problem reports relevant to the question. A bigram stemmer and irregular verb conjugates have been incorporated into the system to improve accuracy. The system is evaluated by a set of fifty five questions posed by NASA engineers. A discussion of future research is also presented.	automata theory;bigram;brill tagger;finite-state machine;logic programming;natural language;parsing;part-of-speech tagging	Sebastian van Delden;Fernando Gomez	2002		10.1007/3-540-36271-1_13	natural language processing;computer science;computational linguistics;applied linguistics;automaton;linguistics;algorithm	NLP	-27.660743143493338	-77.57744244753498	49952
74c75fbdcb496f8847305ccfc74e30e3e57d9e6a	domain independent sentiment classification with many lexicons	databases;opinion mining;sentiment classification;sentiment lexicon;classifier ensemble;supervised learning;unsupervised sentiment classification;multiple classifier systems;frequency domain analysis;multiple classifier systems opinion mining sentiment lexicon sentiment classification natural language processing;semantics;sentiment lexicons;prior knowledge;data mining;thesauri;language resources;accuracy;frequency of occurrence;domain independent sentiment classification;classification decisions;robust method;pattern classification;multiple classifier system;accuracy films supervised learning semantics thesauri databases frequency domain analysis;natural language processing;classification decisions domain independent sentiment classification sentiment lexicons opinion mining unsupervised sentiment classification;films;pattern classification data mining	Sentiment lexicons are language resources widely used in opinion mining and important tools in unsupervised sentiment classification. We present a comparative study of sentiment classification of reviews on six different domains using sentiment lexicons from different sources. Our results highlight the tendency of a lexicon’s performance to be imbalanced towards one class, and indicate lexicon accuracy varies with the target domain. We propose an approach that combines information from different lexicons to make classification decisions and achieve more robust results that consistently improve our baseline across all domains tested. These are further refined by a domain independent score adjustment that mitigates the effect of the precision imbalance seen on some of the results.	algorithm;baseline (configuration management);bundle adjustment;kerrison predictor;lexicon	Bruno Ohana;Brendan Tierney;Sarah Jane Delany	2011	2011 IEEE Workshops of International Conference on Advanced Information Networking and Applications	10.1109/WAINA.2011.103	natural language processing;computer science;pattern recognition;data mining;semantics;accuracy and precision;supervised learning;frequency domain	SE	-20.73257384899469	-69.25261923653908	49977
c3253c3ba5b95ad89b99524acac1e9f0ee56add3	comparative study of indexing and search strategies for the hindi, marathi, and bengali languages	search engine;natural language processing with indo european languages;measurement;information retrieval;performance;vector space;search strategy;statistical significance;indexing method;search engines for asian languages;indexation;hindi language;divergence from randomness;algorithms;statistical language model;indic languages;mean average precision;natural language processing;marathi language;bengali language;test collection;stemmer	The main goal of this article is to describe and evaluate various indexing and search strategies for the Hindi, Bengali, and Marathi languages. These three languages are ranked among the world’s 20 most spoken languages and they share similar syntax, morphology, and writing systems. In this article we examine these languages from an Information Retrieval (IR) perspective through describing the key elements of their inflectional and derivational morphologies, and suggest a light and more aggressive stemming approach based on them.  In our evaluation of these stemming strategies we make use of the FIRE 2008 test collections, and then to broaden our comparisons we implement and evaluate two language independent indexing methods: the n-gram and trunc-n (truncation of the first n letters). We evaluate these solutions by applying our various IR models, including the Okapi, Divergence from Randomness (DFR) and statistical language models (LM) together with two classical vector-space approaches: tf idf and Lnu-ltc.  Experiments performed with all three languages demonstrate that the I(ne)C2 model derived from the Divergence from Randomness paradigm tends to provide the best mean average precision (MAP). Our own tests suggest that improved retrieval effectiveness would be obtained by applying more aggressive stemmers, especially those accounting for certain derivational suffixes, compared to those involving a light stemmer or ignoring this type of word normalization procedure. Comparisons between no stemming and stemming indexing schemes shows that performance differences are almost always statistically significant. When, for example, an aggressive stemmer is applied, the relative improvements obtained are ~28% for the Hindi language, ~42% for Marathi, and ~18% for Bengali, as compared to a no-stemming approach. Based on a comparison of word-based and language-independent approaches we find that the trunc-4 indexing scheme tends to result in performance levels statistically similar to those of an aggressive stemmer, yet better than the 4-gram indexing scheme. A query-by-query analysis reveals the reasons for this, and also demonstrates the advantage of applying a stemming or a trunc-4 indexing scheme.	divergence-from-randomness model;galaxy morphological classification;information retrieval;language model;language-independent specification;n-gram;programming paradigm;randomness;stemming;tf–idf;truncation	Ljiljana Dolamic;Jacques Savoy	2010	ACM Trans. Asian Lang. Inf. Process.	10.1145/1838745.1838748	natural language processing;speech recognition;hindi;performance;vector space;computer science;machine learning;stemming;statistical significance;linguistics;information retrieval;bengali;search engine;measurement	Web+IR	-28.524316661306504	-68.26700850994403	49985
7b7dc331db849d696a48f035974ce222bd438a1f	mapsss results for oaei 2011		MapSSS is very preliminary work on the feasibility of matching OWL -based ontologies in a manner that involves using only limited reasoning over the ontologies. This is the first year MapSSS has been a part of the OAEI competition, and it is hoped that these results will serve as a baseline for comparison with a more mature version of the algorithm a year from now. 1 Presentation of the system MapSSS is an OWL ontology alignment algorithm designed to explore what can be accomplished using very simple similarity metrics rather than (or at least before) resorting to complex reasoning algorithms. OWL ontologies are treated as simple directed graphs with edges representing OWL relations and nodes representing classes, properties, and individuals. The basic algorithm consists of a syntactic, structural, and semantic metric. The metrics are conservative in the sense that a node that may match more than one node is ignored rather than risking making an error. These metrics are applied one after the other, and a positive result from any one of them is treated as a match. When a match is found, MapSSS attempts to capitalize on this by immediately recursing to look for matches among the immediate neighbors of the newly matched nodes. 1.1 State, purpose, general statement MapSSS is meant to provided automated alignments of OWL-based ontologies. The basic algorithm is only two-thirds complete but still produces surprisingly good results on several OAEI test sets. All mappings found by MapSSS are currently considered to have a confidence level of 1.0. It would be relatively straightforward to adapt the metrics to use thresholds instead of requiring exact matches. MapSSS currently only supports finding equivalence relations – finding subsumption and other types of relations is a goal of future work. 1.2 Specific techniques used The three 'S'es in MapSSS correspond to the three types of metrics the algorithm will eventually use: syntactic, structural, and semantic. The syntactic metric is a simple lexical comparison. The structural metric is a graph-based metric based on the direct neighbors of an entity and the edges that connect the entity to those neighbors. The semantic metric has not yet been implemented. We plan to use a Google Research account (http://research.google.com/university/search/) to determine whether or not a pair of entity labels are synonyms. The main processing loop of MapSSS compares each entity in the first ontology to each entity in the second, first based on the syntactic metric, then the semantic, and finally the semantic metric. Whenever a match is found, the algorithm pauses its main loop execution and instead recurses on the newly matched nodes. At this point the metric treats the direct neighborhoods of the newly matched nodes as if they were the entire ontology. This improves recall because while there may be several matches for a given node within the entire opposing ontology, the one that is closest to an already-matched neighbor is most likely to be correct. The main loop repeats until no new mappings are added by any of the metrics. Syntactic Metric Levenstein distance is generally used for the syntactic mapping, though for the OAEI contest only exact matches are considered valid (i.e. the threshold is 1.0). Some minor pre-processing is done on the entity labels prior to running this metric. All labels are converted to lower case and camel case and underscores are converted to spaced words. For instance “oneTwo” and “one_two” are both converted to “one two”. Structural Metric As mentioned previously, MapSSS treats OWL ontologies as simple graphs. The structural metric acts on the direct neighborhood of the nodes in a candidate match. The metric only adds the candidate nodes to the mapping if they are the ONLY possible matches within the graph of subgraph being considered. The metric has the following constraints: • The entities must be the same type to be considered possible matches (i.e. classes are only matched with classes, properties with properties, etc). • Edge labels (e.g. subclass, domain, range, instance, allValuesFrom, someValuesFrom, minCardinality, maxCardinality, cardinality) must match exactly for the entities to be considered possible matches, and the corresponding neighbors at the end of those edges must be of the same type. • If an entity in one ontology has a neighbor that is already part of the mapping, then the node that neighbor is mapped to must be a neighbor of any prospective match for this entity. This is similar to how the VF2 graph matching algorithm works. This constraint helps to ensure that the generated mapping is internally consistent and coherent. Semantic Metric We plan to use the Google Research API to query Google based on the potentially matching entity labels together with configurable search terms such as “synonym” and “translation” and consider the number and quality of the results that are returned. This has several benefits over using WordNet or a domain-specific dictionary: • It will work with jargon, slang, and other words not likely to be in WordNet. • It will work with non-English labels. • It will always be up-to-date with the way words are currently being used by real people. 1.3 Adaptations made for the evaluation Because the OAEI competition does not consider instance matches in the results, these types of matches are removed from alignment after the algorithm has completed (so that they can still be used by the metrics) but before the final results are stored. Due to the nature of the OAEI test sets (particularly the benchmark test set), the syntactic (lexical) metric is set to only consider exact lexical matches rather than a Levenstein distance greater than some threshold. 1.4 Link to the system and parameters file The source code for MapSSS can be downloaded at https://github.com/mcheatham/MapSSS. No parameters file is required. 1.5 Link to the set of provided alignments The results produced by the system are also available at https://github.com/mcheatham/MapSSS.	algorithm;baseline (configuration management);benchmark (computing);coherence (physics);dictionary;directed graph;edit distance;entity;event loop;graph (discrete mathematics);jargon;levenshtein distance;locality of reference;matching (graph theory);ontology (information science);ontology alignment;pattern matching;preprocessor;prospective search;subsumption architecture;test set;test-and-set;turing completeness;web ontology language;wordnet	Michelle Cheatham	2011			simulation;computer science;knowledge management;data mining	Web+IR	-27.883839799121606	-70.15584144157759	50014
246e6d2088a7b64c2c4763f08bd9d336b3ac7b0a	modeling syntactic structures of topics with a nested hmm-lda	analytical models;topic modeling method;structural model;text analysis data mining hidden markov models;text mining;hidden markov model;probability density function;training;text analysis;background functional words syntactic structure modeling latent dirichlet allocation topic modeling method text analysis text mining hidden markov models topic specific content words;topic specific content words;data mining;latent dirichlet allocation;hybrid model;evaluation metric;hidden markov models;hidden markov models linear discriminant analysis data mining text analysis management information systems conference management content management information management text categorization information retrieval;background functional words;syntactic structure modeling;markov processes;information system;bag of words;data models	Latent Dirichlet Allocation (LDA) is a commonly used topic modeling method for text analysis and mining. Standard LDA treats documents as bags of words, ignoring the syntactic structures of sentences. In this paper, we propose a hybrid model that embeds hidden Markov models (HMMs) within LDA topics to jointly model both the topics and the syntactic structures within each topic. Our model is general and subsumes standard LDA and HMM as special cases. Compared with standard LDA and HMM, our model can simultaneously discover both topic-specific content words and background functional words shared among topics. Our model can also automatically separate content words that play different roles within a topic. Using perplexity as evaluation metric, our model returns lower perplexity for unseen test documents compared with standard LDA, which shows its better generalization power than LDA.	data mining;hidden markov model;latent dirichlet allocation;markov chain;perplexity;state transition table;text corpus;topic model	Jing Jiang	2009	2009 Ninth IEEE International Conference on Data Mining	10.1109/ICDM.2009.144	latent dirichlet allocation;natural language processing;data modeling;probability density function;text mining;computer science;bag-of-words model;machine learning;pattern recognition;data mining;markov process;information system;hidden markov model;statistics	ML	-16.656778097920885	-63.44677292697435	50028
6a1bc4694f7055c9237262029e61548e57997ffb	evaluation of contextualization and diversification approaches in aggregated search		The combination of different knowledge bases in the field of information retrieval is called federated or aggregated search. It has several benefits over single source retrieval but poses some challenges as well. This work focuses on the challenge of result aggregation; especially in a setting where the final result list should include some level of diversity and serendipity. Both concepts have been shown to have an impact on how user perceive an information retrieval system. In particular, we want to assess if conventional procedures for result list aggregation can be utilised to introduce diversity and serendipity. Furthermore, we study whether blocking or interleaving for result aggregation yields better results. In a cross vertical aggregated search the so-called verticals could be news, multimedia content or text. Block ranking is one approach to combine such heterogeneous result. It relies on the idea that these verticals are combined into a single result list as blocks of several adjacent items. An alternative approach for this is interleaving. Here the verticals are blended into one result list on an item by item basis, i.e. adjacent items in the result list may come from different verticals. To generate the diverse and serendipitous results we relied on a query reformulation technique which we showed to be beneficial to produce diversified results in previous work. To conduct this evaluation we created a dedicated dataset. This dataset served as a basis for three different evaluation settings on a crowdsourcing platform, with over 300 participants. Our results show that query based diversification can be adapted to generate serendipitous results in a similar manner. Further, we discovered that both methods, interleaving and block ranking, appear to be beneficial to introduce diversity and serendipity. Though it seems that queries either benefit from one approach, or the other one, but not from both.	baseline (configuration management);blocking (computing);crowdsourcing;diversification (finance);forward error correction;information retrieval;recommender system;web search engine	Hermann Ziak;Roman Kern	2017	2017 28th International Workshop on Database and Expert Systems Applications (DEXA)	10.1109/DEXA.2017.37	data mining;computer science;database;contextualization;metasearch engine;knowledge-based systems;information retrieval;interleaving;ranking;crowdsourcing;diversification (marketing strategy)	Web+IR	-33.47061391455855	-55.011919706623125	50059
831bdb1ecbe3753cdc28e15dc1895ecaa6b8798d	detecting strategic moves in hearthstone matches		In this paper, we demonstrate how to extract strategic knowledge from gaming data collected among players of the popular video game HearthStone. Our methodology is as follows. First we train a series of classifiers to predict the outcome of the game during a match, then we demonstrate how to spot key strategic events by tracking sudden changes in the classifier prediction. This methodology is applied to a large collection of HeathStone matches that we have collected from top ranked European players. Expert analysis shows that the events identified with this approach are both important and easy to interpret with the corresponding data.		Boris Doux;Clément Gautrais;Benjamin Négrevergne	2016			simulation;computer science;machine learning;data mining	AI	-22.101244067156276	-54.85056122316723	50113
bc12cf9b8a3cb95740c11dc57926b732033cc13a	modernizing historical slovene words with character-based smt		We propose a language-independent word normalization method exemplified on modernizing historical Slovene words. Our method relies on character-based statistical machine translation and uses only shallow knowledge. We present the relevant lexicons and two experiments. In one, we use a lexicon of historical word– contemporary word pairs and a list of contemporary words; in the other, we only use a list of historical words and one of contemporary ones. We show that both methods produce significantly better results than the baseline.	ambiguous name resolution;baseline (configuration management);common gateway interface;digital humanities;enlightenment foundation libraries;experiment;language model;language-independent specification;lexicon;satisfiability modulo theories;statistical machine translation;stemming;text-based (computing);tokenization (data security);unsupervised learning	Yves Scherrer;Tomaz Erjavec	2013			natural language processing;speech recognition;computer science;linguistics	NLP	-30.40682946721764	-75.56600486973817	50175
f641b4e61bee60a617e90007aece3d191c67f740	visualization and interpretation of siamese style convolutional neural networks for sound search by vocal imitation		Designing systems that allow users to search sounds through vocal imitation augments the current text-based search engines and advances human-computer interaction. Previously we proposed a Siamese style convolutional network called IMINET for sound search by vocal imitation, which jointly addresses feature extraction by Convolutional Neural Network (CNN) and similarity calculation by Fully Connected Network (FCN), and is currently the state of the art. However, how such architecture works is still a mystery. In this paper, we try to answer this question. First, we visualize the input patterns that maximize the activation of different neurons in each CNN tower; this helps us understand what features are extracted from vocal imitations and sound candidates. Second, we visualize the imitation-sound input pairs that maximize the activation of different neurons in the FCN layers; this helps us understand what kind of input pattern pairs are recognized during the similarity calculation. Interesting patterns are found to reveal the local-to-global and simple-to-conceptual learning mechanism of TL-IMINET. Experiments also show how transfer learning helps to improve TL-IMINET performance from the visualization aspect.	convolutional neural network;feature extraction;human–computer interaction;internal market information system;network topology;text-based (computing);transform, clipping, and lighting;web search engine	Yichi Zhang;Zhiyao Duan	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8461729	task analysis;convolutional neural network;search engine;feature extraction;visualization;architecture;spectrogram;artificial intelligence;pattern recognition;imitation;computer science	Visualization	-12.682812479544634	-71.04440430223016	50238
c90c7c902ae0615f7c76702e0f5b743657d13a89	relevance feedback using a bayesian classifier in content-based image retrieval	bayesian classifier;user feedback;feedback;negative feedback;content based image retrieval;relevance feedback	As an effective solution of the content-based image retrieval (CBIR) problems, relevance feedback has been put on many efforts for the past few years. In this paper, we propose a new relevance feedback approach with progressive leaning capability. It is based on a Bayesian classifier and treats positive and negative feedback examples with different strategies. It can utilize previous users’ feedback information to help the current query. Experimental results show that our algorithm is achieves high accuracy and effectiveness on real-world image collections.	algorithm;bayesian network;content-based image retrieval;naive bayes classifier;negative feedback;progressive enhancement;relevance feedback;world file	Zhong Su;HongJiang Zhang;Shao-peng Ma	2001		10.1117/12.410918	naive bayes classifier;computer science;machine learning;data mining;feedback;negative feedback;information retrieval	Vision	-30.42821104584765	-58.543004771141455	50444
860a08d3d403985b0a55a2dd5d4a258531571956	flexible ontology population from text: the owlexporter	text analysis;ontology population;natural language;general architecture for text engineering;language engineering;knowledge base	Ontology population from text is becoming increasingly important for NLP applications. Ontologies in OWL format provide for a standardized means of modeling, querying, and reasoning over large knowledge bases. Populated from natural language texts, they offer significant advantages over traditional export formats, such as plain XML. The development of text analysis systems has been greatly facilitated by modern NLP frameworks, such as the General Architecture for Text Engineering (GATE). However, ontology population is not currently supported by a standard component. We developed a GATE resource called the OwlExporter that allows to easily map existing NLP analysis pipelines to OWL ontologies, thereby allowing language engineers to create ontology population systems without requiring extensive knowledge of ontology APIs. A particular feature of our approach is the concurrent population and linking of a domainand NLP-ontology, including NLP-specific features such as safe reasoning over coreference chains.	gate;knowledge base;natural language processing;ontology (information science);pipeline (computing);population;web ontology language;xml	René Witte;Ninus Khamis;Juergen Rilling	2010			natural language processing;upper ontology;open biomedical ontologies;knowledge base;bibliographic ontology;ontology inference layer;computer science;ontology;data mining;linguistics;ontology-based data integration;natural language;process ontology;suggested upper merged ontology	AI	-33.08678719301471	-68.6131639357795	50451
90086a2c847c2e2bc50354404adb8f0cd84712ca	spatial role labeling with convolutional neural networks	spatial semantics;spatial role labeling;convolutional neural networks;natural language processing	Many natural language processing applications require information about the spatial locations of objects referenced in text, or spatial relations between these objects in space. For example, the phrase a book on the shelf contains information about the location of the object book, corresponding to a trajector, with respect to the object shelf, which in turn corresponds to a landmark. Spatial role labeling concerns with the task of automatically processing textual sentences and identifying objects of spatial scenes and relations between them. In this paper, we describe the application of modern machine learning methods to extract spatial roles and their relations, specifically by adapting a pre-existing system based on a convolutional neural network architecture that has been recently proposed for the more general task of semantic role labeling. We report on experiments with datasets from the SemEval challenges on spatial role labeling, showing that our method can achieve results in line with the current state-of-the-art. We therefore argue that that spatial role labeling can leverage on recent developments in semantic role labeling, requiring only minimal adaptations.	artificial neural network;convolutional neural network;experiment;machine learning;natural language processing;network architecture;semeval;semantic role labeling	Alexey Mazalov;Bruno Martins;David Martins de Matos	2015		10.1145/2837689.2837706	natural language processing;computer science;machine learning;data mining;convolutional neural network;information retrieval	AI	-15.344428490568651	-69.34831131045075	50453
c443679b0f11e966330cdc2ba7e4fe7112a961b9	computational mechanisms for metaphor in languages: a survey	computational mechanics;english language;logic;周昌乐 杨芸 黄孝喜 隐喻 自然语言处理 中文信息处理 计算模型 逻辑学 computational mechanisms for metaphor in languages a survey;expressive power;natural language;metaphor understanding;chinese language;computational model;article;natural language processing;machine translation	Metaphor computation has attracted more and more attention because metaphor, to some extent, is the focus of mind and language mechanism. However, it encounters problems not only due to the rich expressive power of natural language but also due to cognitive nature of human being. Therefore machine-understanding of metaphor is now becoming a bottle-neck in natural language processing and machine translation. This paper first suggests how a metaphor is understood and then presents a survey of current computational approaches, in terms of their linguistic historical roots, underlying foundations, methods and techniques currently used, advantages, limitations, and future trends. A comparison between metaphors in English and Chinese languages is also introduced because compared with development in English language Chinese metaphor computation is just at its starting stage. So a separate summarization of current progress made in Chinese metaphor computation is presented. As a conclusion, a few suggestions are proposed for further research on metaphor computation especially on Chinese metaphor computation.	artificial intelligence;computation;machine translation;natural language processing	Changle Zhou;Yun Yang;Xiaoxi Huang	2007	Journal of Computer Science and Technology	10.1007/s11390-007-9038-2	natural language processing;computer science;artificial intelligence;computational mechanics;english;machine translation;natural language;computational model;chinese;logic;expressive power	NLP	-11.158315008200248	-73.40657306325122	50516
301e0ea05f1045475ce0140247aee06719b82345	statistical evaluation of similarity measures on multi-lingual text corpora	linguistique;analisis estadistico;similitude;statistical evaluation;linguistica;statistical analysis;analyse statistique;similarity;similitud;similarity measure;linguistics	Subject of this paper is the investigation of a similarity measure considering different large contexts of words. A multi-lingual bibel corpus is used to verify the results.	corpus linguistics;text corpus	Robert Neumann;Rudolf Schmidt	1999		10.1007/3-540-48239-3_70	natural language processing;speech recognition;similarity;similitude;mathematics;linguistics	NLP	-27.11785413437299	-76.96475175534798	50541
1ba84863e2685c45c5c41953444d9383dc7aa13b	efficient support vector classifiers for named entity recognition	efficient support vector classifier;similar task;support vector machines;better score;key technology;entity recognition;open-domain question answering;information extraction;svm-based feature selection method;efficient training method;conventional system;ne recognizer;feature selection;support vector machine;noun;question answering	NamedEntity (NE) recognitionis a task in which proper nouns and numerical information are extractedfrom documentsandareclassifiedinto categoriessuchas person,organization,and date. It is a key technologyof InformationExtractionand Open-DomainQuestionAnswering.First,weshow thatanNE recognizerbasedonSupportVectorMachines(SVMs)givesbetterscoresthanconventional systems.However, off-the-shelfSVM classifiersare too inefficient for this task.Therefore,we presenta methodthat makes the systemsubstantiallyfaster . This approachcan also be applied to other similar taskssuchaschunkingandpart-of-speechtagging. We alsopresentanSVM-basedfeatureselection methodandanefficient trainingmethod.	finite-state machine;named-entity recognition;numerical analysis	Hideki Isozaki;Hideto Kazawa	2002			natural language processing;noun;support vector machine;question answering;computer science;machine learning;pattern recognition;data mining;feature selection;information extraction	NLP	-23.235059879789908	-71.80394990587617	50554
d5abf93a1d5a15d135c7ba65bf715bcc5db5d210	an effective hybrid classifier based on rough sets and neural networks	support vector machines rough sets hybrid classifier internet documents automated categorization automatic clustering vector space model artificial neural network documents classification bayesian classifiers complex classification problems;bayesian classifier;fault tolerant;vector space model;support vector machines;neural nets;documents classification;rough set theory;information agents;autonomous auctions and negotiation;autonomous knowledge;hybrid classifier;intrusion detection;autonomy oriented computing;documents automated categorization;infrastructure security;complex classification problems;feature vector;bayesian classifiers;internet;automatic clustering;intelligent response;rough sets neural networks artificial neural networks internet clustering algorithms stability fault tolerance bayesian methods support vector machines support vector machine classification;rough set theory neural nets pattern classification;pattern classification;rough sets;distributed problem solving;exponential growth;document classification;rough set;applications;artificial neural network;neural network	Due to the exponential growth of documents on the Internet and the emergent need to organize them, the automated categorization of documents into predefined labels has received an ever-increased attention in the recent years. This paper describes a method developed for the automatic clustering of documents by using a hybrid classifier based on rough sets and neural networks, which we called as Rough-Ann,. First, the documents are denoted by vector space model and the feature vectors are reduced by using rough sets. Then using those feature vectors we reduced that are training set for artificial neural network and clustering the documents. The experimental results show that the algorithm Rough-Ann is effective for the documents classification, and has the better performance in classification precision, stability and fault-tolerance comparing with the traditional classification methods, Bayesian classifiers SVM and kNN, especially for the complex classification problems with many feature vectors.	algorithm;artificial neural network;bayesian network;categorization;cluster analysis;emergence;fault tolerance;feature vector;internet;naive bayes classifier;rough set;test set;time complexity	Rujiang Bai;Xiaoyue Wang	2006	2006 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology Workshops	10.1109/WI-IATW.2006.36	rough set;computer science;machine learning;pattern recognition;data mining;artificial neural network	AI	-23.880458221915553	-57.63340129558367	50560
35100b377932ce3ee7f4e99a83ef0c0e5c94c1e5	dependency treelet translation: syntactically informed phrasal smt	novel approach;statistical machine translation;phrasal smt;phrasal translation;source language;source-language dependency parser;dependency treelet translation pair;source dependency parse;promising approach;conventional smt model;word segmentation;dependency parsing	We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation. We depend on a source-language dependency parser and a word-aligned parallel corpus. The only target language resource assumed is a word breaker. These are used to produce treelet (“phrase”) translation pairs as well as several models, including a channel model, an order model, and a target language model. Together these models and the treelet translation pairs provide a powerful and promising approach to MT that incorporates the power of phrasal SMT with the linguistic generality available in a parser. We evaluate two decoding approaches, one inspired by dynamic programming and the other employing an A* search, comparing the results under a variety of settings.	a* search algorithm;channel (communications);compiler;dynamic programming;language model;parallel text;statistical machine translation	Chris Quirk;Arul Menezes;Colin Cherry	2005			natural language processing;text segmentation;speech recognition;example-based machine translation;computer science;linguistics;machine translation;rule-based machine translation;dependency grammar	NLP	-21.737264431433037	-77.13072691394979	50621
1406cae06084f3a8e8174529d74115600cb3c88b	sentence similarity measures revisited: ranking sentences in pubmed documents		"""While various measures are available for computing sentence similarity, few studies have examined their performance in the biomedical domain. Motivated by BIOSSES, an earlier study for biomedical sentence similarity, we here explore the effectiveness of multiple similarity measures via sentence ranking in PubMed abstracts. Ranking sentences is a crucial component for text summarization and biocuration evidence attribution. Applied to the """"natural language processing"""" and """"computational biology"""" datasets, our experimental results show that the off-the-shelf measures for sentence similarity may not be effective for ranking sentences. Neither lexical nor semantic measures provided more than 0.60 NDCG scores at the top 1 ranked document. It necessitates the development of a large-scale benchmark set and more effective measures."""	automatic summarization;benchmark (computing);biocurator;computation;computational biology;natural language processing;pubmed;semantic similarity	Qingyu Chen;Sun Kim;W. John Wilbur;Zhiyong Lu	2018		10.1145/3233547.3233640	machine learning;learning to rank;information retrieval;automatic summarization;artificial intelligence;ranking;computer science;sentence	NLP	-27.72385054682056	-67.39570215606578	50643
2b244d30f0442dc12ec5a99a10a56bb27f8bba4b	efficient machine learning methods for document image analysis	dissertation	Title of dissertation: EFFICIENT MACHINE LEARNING METHODS FOR DOCUMENT IMAGE ANALYSIS Jayant Kumar, Doctor of Philosophy, 2013 Dissertation directed by: Professor Larry Davis Department of Computer Science Dr. David Doermann University of Maryland Institute for Advanced Computer Studies With the exponential growth in volume of multimedia content on the internet, there has been an increasing interest for developing more efficient and scalable algorithms to learn directly from data without excessive restrictions on nature of the content. In the context of document images, many large scale digitization projects have called for reliable and scalable triage methods for enhancement, segmentation, grouping and categorization of captured images. Current approaches, however, are typically limited to a specific class of documents such as scanned books, newspapers, journal articles or forms for example, and analysis and processing of more unconstrained and noisy heterogeneous document collections has not been as widely addressed. Additionally, existing machine-learning based approaches for document processing need to be carefully applied to handle the challenges associated with large and imbalanced training data. In this thesis, we address these challenges in three primary applications of document image analysis low-level document enhancement, mid-level handwritten line segmentation, and high-level classification and retrieval. We first present a data selection method for training Support Vector Machines (SVM) on large-scale data sets. We apply the proposed approach to pixel-level document image enhancement, and show promising results with a relatively small number of training samples. Second, we present a graph-based method for segmentation of handwritten document images into text-lines which is more efficient and adaptive than previous approaches. Our approach demonstrates that combining results from local and global methods enhances the final performance of text-line segmentation. Third, we present an approach to compute structural similarities between images for classification and retrieval. Results on real-world data sets show that the approach is more effective than earlier approaches when the labeled data is limited. We extend our classification approach to a completely unsupervised setting, where both the number of classes and representative samples from each class is assumed to be unknown. We present a method for computing similarities based on learned structural patterns and correlations from the given data. Experiments with four different data sets show that our approach can estimate number of classes in large document collections and group structurally similar images with a high-accuracy. EFFICIENT MACHINE METHODS FOR DOCUMENT IMAGE ANALYSIS	algorithm;book;categorization;computer science;document processing;experiment;fagan inspection;high- and low-level;image analysis;image editing;machine learning;pixel;scalability;scope (computer science);structural pattern;support vector machine;time complexity;unsupervised learning	Jayant Kumar	2013			computer science;data science;machine learning;active learning;information retrieval	ML	-17.323939021916416	-60.58673726282113	50658
cbc5775f54b14a613b9f7e8a0286eb887d67c404	using mutual information to identify new features for text documents of various domains	conference paper	The task of identifying proper names, unknown words and new terms, is an important step in text processing systems. This paper describes a method of using mutual information to collect possible segments as candidates of these three feature types in a document scope. Then the construction and context of each possible feature is examined to determine its type, canonical form and meaning. Adding very little domain-specific knowledge, this method adapts to various domains easily.	mutual information	Zhi Li Guo	2003			data mining;world wide web;information retrieval	ML	-29.86210976542693	-68.7757742015189	50716
0c9f3344d2a852edb9e1841b277ed0e8748ccdd7	experiments of uned at the third recognising textual entailment challenge	remarkable result;qa task;recognising textual entailment challenge;entity recognition;sophisticated treatment;textual entailment;third recognising textual entailment;ie task	This paper describes the experiments developed and the results obtained in the participation of UNED in the Third Recognising Textual Entailment (RTE) Challenge. The experiments are focused on the study of the effect of named entities in the recognition of textual entailment. While Named Entity Recognition (NER) provides remarkable results (accuracy over 70%) for RTE on QA task, IE task requires more sophisticated treatment of named entities such as the identification of relations between them.	experiment;named entity;software quality assurance;textual entailment	Álvaro Rodrigo;Anselmo Peñas;Jesús Herrera;M. Felisa Verdejo	2007			natural language processing;computer science;data mining;linguistics	NLP	-27.87096667258038	-73.73492899750475	50722
d04d646fdbb7561ac02161378103c709f4d02b46	cricket team prediction with hadoop: statistical modeling approach		Abstract Cricket is one of the most popular team games in the world. In this paper, we embark on predicting the best suitable Team to be lined for a particular match. We propose statistical modeling approach to predict the perfect players for the match to be played. As cricket is not a very simple sport, there are many factors affecting the line-up and selection of players for a particular match such as Player’s Overall Stats, Player Performances with different Teams and the most important Last 5 Performances. All these factors have been considered for selection of players in playing 11 from the Team of 16. This work suggests that the relative team strength between the competing teams forms a distinctive feature for predicting the winner. Modeling the team strength boils down to modeling individual player batting and bowling performances, forming the basis of approach used. Career statistics as well as the recent performances of a player have been used to model. Player independent factors have also been considered in order to predict the outcome of a match. Experimental analysis was performed using Hadoop and Hive for Indian players. Results establish that proposed approach is able to obtain up to 91% accuracy as compared to the real results available over WWW.	apache hadoop;statistical model	Shubham Agarwal;Lavish Yadav;Shikha Mehta	2017		10.1016/j.procs.2017.11.402	artificial intelligence;distinctive feature;machine learning;cricket;statistical model;computer science	ML	-10.754495628735807	-68.84882961223647	50723
5a165bfa865166d91e381ec69facfc9593f1f699	criteria for identifying and annotating caused motion constructions in corpus data		While natural language processing performance has been improved through the recognition that there is a relationship between the semantics of the verb and the syntactic context in which the verb is realized, sentences where the verb does not conform to the expected syntax-semantic patterning behavior remain problematic. For example, in the sentence “The crowed laughed the clown off the stage”, a verb of non-verbal communication laugh is used in a caused motion construction and gains a motion entailment that is atypical given its inherent lexical semantics. This paper focuses on our efforts at defining the semantic types and varieties of caused motion constructions (CMCs) through an iterative annotation process and establishing annotation guidelines based on these criteria to aid in the production of a consistent and reliable annotation. The annotation will serve as training and test data for classifiers for CMCs, and the CMC definitions developed throughout this study will be used in extending VerbNet to handle representations of sentences in which a verb is used in a syntactic context that is atypical for its lexical semantics.	corpus linguistics;iterative method;natural language processing;test data;verbnet	Jena D. Hwang;Annie Zaenen;Martha Palmer	2014			computer science;theoretical computer science;pattern recognition;data mining	NLP	-26.19376308715336	-74.25932444202132	50788
372ed4a594674eb7f11eaa5534a115c3d3bd895e	bilingually motivated domain-adapted word segmentation for statistical machine translation	segmentation approach;statistical machine translation;segmented training data;segmented monolingual domain-specific corpus;word segmentation approach;domain-adapted word segmentation;different data condition;approach score;different domain;pb-smt task;word boundary;statistical word alignment technique;machine translation;word segmentation	We introduce a word segmentation approach to languages where word boundaries are not orthographically marked, with application to Phrase-Based Statistical Machine Translation (PB-SMT). Instead of using manually segmented monolingual domain-specific corpora to train segmenters, we make use of bilingual corpora and statistical word alignment techniques. First of all, our approach is adapted for the specific translation task at hand by taking the corresponding source (target) language into account. Secondly, this approach does not rely on manually segmented training data so that it can be automatically adapted for different domains. We evaluate the performance of our segmentation approach on PB-SMT tasks from two domains and demonstrate that our approach scores consistently among the best results across different data conditions.	bilingual dictionary;bitext word alignment;named entity;satisfiability modulo theories;statistical machine translation;text corpus;text segmentation;vocabulary	Yanjun Ma;Andy Way	2009			natural language processing;text segmentation;speech recognition;transfer-based machine translation;example-based machine translation;word error rate;computer science;pattern recognition;machine translation;rule-based machine translation	NLP	-22.97595362772823	-77.08891139785851	50798
51b84d5254b3148495338e8ae5d28ca7adcb376e	experimental design to improve topic analysis based summarization		We use efficient screening experiments to investigate and improve topic analysis based multi-document extractive summarization. In our summarization process, topic analysis determines the weighted topic content vectors that characterize the corpora, and then Jensen-Shannon divergence extracts sentences that best match the weighted content vectors to assemble the summaries. We use screening experiments to investigate several control parameters in this process, gaining better understanding of and improving the topic analysis based summarization process.	aggregate data;automatic summarization;design of experiments;experiment;jensen's inequality;rouge (metric);shannon (unit);text corpus	John Miller;Kathleen F. McCoy	2014			multi-document summarization;computer science;data science;automatic summarization;data mining;information retrieval	NLP	-26.362801729076303	-62.71782864543996	50816
df4b472d70af0203a7ec84ff7448a50eb700d561	web information systems engineering – wise 2013	computers	Entity Correspondence seeks to find instances that refer to the same real world entity. Usually, a fixed set of properties exists, for each of which the similarity score is computed to support entity correspondence. However, in a knowledge base that has properties incrementally recognized, we can no longer rely only on the belief that two instances sharing value for the same property are likely to correspond with each other: a pair of different properties that are of hierarchies or specific relations can also be evidential to corresponding instances. This paper proposes the use of second-order Markov Logic to perform entity correspondence. With second-order Markov Logic, we regard properties as variables, explicitly define and exploit relations between properties and enable interaction between entity correspondence and property relation discovery. We also prove that second-order Markov Logic can be rephrased to first-order in practice. Experiments on a real world knowledge base show promising entity correspondence results, particularly in recall.	commonsense knowledge (artificial intelligence);experiment;first-order predicate;knowledge base;markov chain;markov logic network;systems engineering	Xuemin Lin;Yannis Manolopoulos;Divesh Srivastava;Guangyan Huang;Gerhard Goos;Juris Hartmanis;Jan van Leeuwen;David Hutchison	2013		10.1007/978-3-642-41230-1	computer science;data mining;database;world wide web;information retrieval	AI	-16.363163255293586	-66.65146461668813	50827
5cfb54f655602eb1fc128645426b25d45221f724	a stereochemically accurate chemical substance database based on the systematic names of organic compounds. 1. low molecular weight organic compounds	computer program;compuesto organico;stereochimie;organic compounds;etude theorique;systeme base donnee;low molecular weight;estructura quimica;estudio teorico;compose organique;estereoquimica;chemical structure;theoretical study;structure chimique;programa computador;faible masse moleculaire;programme ordinateur;stereochemistry;organic compound	A chemical substance database, containing chemical structures generated automatically from their IUPAC systematic names, is described. The structure generation program can convert the names of natural products, peptides, and stereochemically complex names into stereochemically correct connection tables. More than 300,000 commonly encountered compounds are stored in the database and are publicly accessible through the JOIS-F online system, which also offers mass spectral, thermochemical, and other factual databases.		Keisuke Araki;Masanori Kaji	1991	Journal of chemical information and computer sciences	10.1021/ci00003a001	stereochemistry;chemistry;organic chemistry;chemical structure;mineralogy	DB	-5.334181307549705	-55.67284606615602	50920
a491c5aa6d68345736f8e3106883ae03940585d6	bionotes: a system for biosequence annotation	biology computing;data mining biology computing;genomics bioinformatics data warehouses documentation control systems relational databases xml conferences image databases expert systems;data mining;functional requirement;data source bionotes biosequence annotation genome projects knowledge derivation biological knowledge data searching	One of the most important tasks of genome projects is the interpretation of experimental data in order to derive biological knowledge from the data. To achieve this goal, researchers typically search external data sources, execute analysis programs on the biosequences, analyze previous annotations and add new annotations to register their interpretation of the data. This paper first elicits the functional requirements of biosequence annotation systems. Then, it describes BioNotes, a tool that meets these requirements, stressing the advantages it brings to the researchers in this area.	functional requirement	Melissa Lemos;Luiz Fernando Bessa Seibel;Marco A. Casanova	2003		10.1109/DEXA.2003.1231991	computer science;bioinformatics;data mining;database;world wide web;functional requirement	Comp.	-4.790663234610057	-62.044256003557244	50936
14564ea3be8833d15654a49055a41651772c2e93	multimodal bag-of-words for cross domains sentiment analysis		The advantages of using cross domain data when performing text-based sentiment analysis have been established; however, similar findings have yet to be observed when performing multimodal sentiment analysis. A potential reason for this is that systems based on feature extracted from speech and facial features are susceptible to confounding effecting caused by different recording conditions associated with data collected in different locations. In this regard, we herein explore different Bag-of-Words paradigms to aid sentiment detection by providing training material from an additional dataset. Key results presented indicate that using a Bag-of-Words extraction paradigm that takes into account information from both the test domain and the out of domain datasets yields gains in system performance.	bag-of-words model in computer vision;multimodal interaction;programming paradigm;sentiment analysis;text-based (computing)	Nicholas Cummins;Shahin Amiriparian;Sandra Ottl;Maurice Gerczuk;Maximilian Schmitt;Björn W. Schuller	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8462660	sentiment analysis;feature extraction;bag-of-words model;artificial intelligence;pattern recognition;confounding;computer science	SE	-6.725199126969003	-72.96175702983098	50975
468ef22fc4931fad37acba3e0433783a27972f48	learning simplifications for specific target audiences		Text simplification (TS) is a monolingual text-to-text transformation task where an original (complex) text is transformed into a target (simpler) text. Most recent work is based on sequence-to-sequence neural models similar to those used for machine translation (MT). Different from MT, TS data comprises more elaborate transformations, such as sentence splitting. It can also contain multiple simplifications of the same original text targeting different audiences, such as school grade levels. We explore these two features of TS to build models tailored for specific grade levels. Our approach uses a standard sequenceto-sequence architecture where the original sequence is annotated with information about the target audience and/or the (predicted) type of simplification operation. We show that it outperforms stateof-the-art TS approaches (up to 3 and 12 BLEU and SARI points, respectively), including when training data for the specific complex-simple combination of grade levels is not available, i.e. zero-shot learning.	bleu;computer multitasking;encoder;level of detail;machine translation;multi-task learning;statistical classification;text simplification	Carolina Scarton;Lucia Specia	2018			machine learning;artificial intelligence;natural language processing;computer science	NLP	-17.925182907766274	-74.77095045512797	50979
05d7aa0815d3218edac16f052297599c2c8cd4c3	mining text for word senses using independent component analysis	second order;higher order;independent component analysis	The assumption that the problem of ambiguity in text analysis can only be solved if statistical dependencies of higher than second order are considered leads us to independent component analysis (ICA), a statistical formalism that takes higher-order dependencies into account. By assuming independence, ICA is capable of detecting a set of hidden vectors if only different linear mixtures of these vectors are observable. As a test case for ICA’s applicability to natural language processing we look at the task of word sense induction. Our starting point is that we consider the co-occurrence vector of an ambiguous word as a linear mixture of its unknown sense vectors. If corpora from different domains are available, this should give us the different linear mixtures that are required for ICA. It turns out that the independent sense vectors derived by ICA from the distributional differences of word usage reflect a word’s meanings surprisingly well.	independent computing architecture;independent component analysis;natural language processing;observable;semantics (computer science);sensor;test case;text corpus;word sense;word-sense induction	Reinhard Rapp	2004		10.1137/1.9781611972740.39	pattern recognition;computer science;artificial intelligence;independent component analysis;natural language processing	ML	-25.11888776072652	-72.48713111417487	50996
b0e83e90d057444d9c70db1e7db2975a611dcbe1	potsdam commentary corpus 2.0: annotation for discourse research		We present a revised and extended version of the Potsdam Commentary Corpus, a collection of 175 German newspaper commentaries (op-ed pieces) that has been annotated with syntax trees and three layers of discourse-level information: nominal coreference, connectives and their arguments (similar to the PDTB, (Prasad et al., 2008)), and trees reflecting discourse structure according to Rhetorical Structure Theory (Mann and Thompson, 1988). Connectives have been annotated with the help of a semi-automatic tool (Conano, (Stede and Heintze, 2004)) that identifies most connectives and suggests arguments based on their syntactic category. The other layers have been created manually with dedicated annotation tools. The corpus is made available on the one hand as a set of original XML files produced with the annotation tools, based on identical tokenization. On the other hand, it will be distributed together with the open-source linguistic database ANNIS3 (Chiarcos et al., 2008; Zeldes et al., 2009), which provides multi-layer search functionality and layer-specific visualization modules. This allows for comfortable qualitative evaluation of the correlations between annotation layers.	daisy digital talking book;layer (electronics);logical connective;open-source software;semiconductor industry;tokenization (data security);xml	Manfred Stede;Arne Neumann	2014			linguistics	NLP	-32.08024755776645	-75.29520029158199	51007
99bec22932e6a3e84d786a31d36a0e9217b91ad5	samsung poland nlp team at semeval-2016 task 1: necessity for diversity; combining recursive autoencoders, wordnet and ensemble methods to measure semantic similarity		This paper describes our proposed solutions designed for a STS core track within the SemEval 2016 English Semantic Textual Similarity (STS) task. Our method of similarity detection combines recursive autoencoders with a WordNet award-penalty system that accounts for semantic relatedness, and an SVM classifier, which produces the final score from similarity matrices. This solution is further supported by an ensemble classifier, combining an aligner with a bi-directional Gated Recurrent Neural Network and additional features, which then performs Linear Support Vector Regression to determine another set of scores.	autoencoder;ensemble learning;natural language processing;overfitting;preprocessor;recurrent neural network;recursion;semeval;semantic similarity;support vector machine;word embedding;wordnet	Barbara Rychalska;Katarzyna Pakulska;Krystyna Chodorowska;Wojciech Walczak;Piotr Andruszkiewicz	2016			semantic similarity;natural language processing;machine learning;computer science;artificial intelligence;semeval;wordnet;ensemble learning;recursion;diversity combining	NLP	-21.837443665465145	-72.02251138312286	51090
b6398f6e9386730d22c1af681b38863aa77f2f0e	unsupervised learning of morphology with graph sampling		We introduce a language-independent, graph-based probabilistic model of morphology, which uses transformation rules operating on whole words instead of the traditional morphological segmentation. The morphological analysis of a set of words is expressed through a graph having words as vertices and structural relationships between words as edges. We define a probability distribution over such graphs and develop a sampler based on the Metropolis-Hastings algorithm. The sampling is applied in order to determine the strength of morphological relationships between words, filter out accidental similarities and reduce the set of rules necessary to explain the data. The model is evaluated on the task of finding pairs of morphologically similar words, as well as generating new words. The results are compared to a state-of-the-art segmentation-based approach.	approximation algorithm;brown corpus;error-tolerant design;filter (signal processing);galaxy morphological classification;language model;language-independent specification;markov chain monte carlo;mathematical morphology;metropolis;metropolis–hastings algorithm;monte carlo method;point of sale;protologism;sampling (signal processing);semantics (computer science);statistical model;unsupervised learning;word embedding	Maciej Sumalvico	2017		10.26615/978-954-452-049-6_093	morphology (linguistics);machine learning;sampling (statistics);artificial intelligence;computer science;unsupervised learning;graph	Web+IR	-23.605323187025682	-75.222192331741	51115
12c82314e5d868c43e10b0da0a7a20aed3c797d8	self-organizing map analysis consistent with neuroimaging for chinese noun, verb and class-ambiguous word	grammar;brain;systeme nerveux central;noun;relation semantique;verbe;relacion semantica;semantics;hombre;intelligence artificielle;chino;encefalo;semantica;semantique;sistema nervioso central;cerebro;self consistency;encephale;autocoherencia;grammaire;cerveau;semantic description;human;verbo;autoorganizacion;autocoherence;artificial intelligence;self organization;self organized map;semantic relation;encephalon;inteligencia artificial;reseau neuronal;chinois;chinese;gramatica;red neuronal;human brain;autoorganisation;central nervous system;homme;neural network;verb	In the paper we discussed the semantic distinction between Chinese noun, verb, and class-ambiguous word by using SOM (self-organizing map) neural networks. Comparing neuroimaging method with neural network method, our result shows neural network technique can be used to study lexical meaning, syntax relation and semantic description for the three kinds of words. After all, the response of human brain to Chinese lexical information is based mainly on conceptual and semantic attributes, seldom uses Chinese syntax and grammar features. Our experimental results are coincident with human brain's neuroimaging, our analysis will help to understand the role of feature description and relation of syntax and semantic features.		Minghu Jiang;Huiying Cai;Bo Zhang	2005		10.1007/11427469_153	natural language processing;noun;computer science;artificial intelligence;central nervous system;grammar;semantics;chinese;cerebro;artificial neural network	NLP	-11.49518252518734	-76.1986486661092	51143
092236560c1b93f971f3f3d8390a6764b1a172e2	data challenges at yahoo!	data mining;mobile advertising;clustering;mobile customers targeting	In this short paper we describe the data that Yahoo! handles, the current trends in Web applications, and the many challenges that this poses for Yahoo! Research. These challenges have led to the development of new data systems and novel data mining techniques.	data mining;data system;web application	Ricardo A. Baeza-Yates;Raghu Ramakrishnan	2008		10.1145/1353343.1353421	computer science;data mining;internet privacy;cluster analysis;world wide web	DB	-21.150064176035585	-53.41635718680891	51157
599c3bf9a0a6a284381b3d0ed6a02f93b0787ffc	how a french tts system can describe loanwords.		We propose here an approach of the morphophonological treatment of borrowed lexical items in the general French lexicon, through the mecanims of the functionning of the TTS Toph. We will show how the methodology we adopted in this TTS allows us to distinguish two levels of processing, adapted to the nature of the items. After the display of the typology of phonetic transfers we will focus on the inflexional systems of loanwords, taking into account the graphic and the phonetic dimensions. The last part will be dedicated to the phonetic inflected forms generation, which would find applications in concept-to-speech synthesis.	biological anthropology;galaxy morphological classification;lexicon;netware file system;orthographic projection;speech synthesis	Fred Sannier;Rabia Belrhali;Véronique Aubergé	1998			speech recognition;computer science	NLP	-28.632939916210393	-79.89576217916425	51188
0bdb77530ca8c09dde9c7288971535d56f4f0478	exploring weakly supervised latent sentiment explanations for aspect-level review analysis	opinion mining;sentiment classification;text mining;structural learning;sentiment analysis	In sentiment analysis, aspect-level review analysis has been an important task because it can catalogue, aggregate, or summarize various opinions according to a product's properties. In this paper, we explore a new concept for aspect-level review analysis, latent sentiment explanations, which are defined as a set of informative aspect-specific sentences whose polarities are consistent with that of the review. In other words, sentiment explanations best represent a review in terms of both aspect and polarity. We formulate the problem as a structure learning problem, and sentiment explanations are modeled with latent variables. Training samples are automatically identified through a set of pre-defined aspect signature terms (i.e., without manual annotation on samples), which we term the way weakly supervised.  Our major contributions lie in two folds: first, we formalize the use of aspect signature terms as weak supervision in a structural learning framework, which remarkably promotes aspect-level analysis; second, the performance of aspect analysis and document-level sentiment classification are mutually enhanced through joint modeling. The proposed method is evaluated on restaurant and hotel reviews respectively, and experimental results demonstrate promising performance in both document-level and aspect-level sentiment analysis.	aggregate data;information;latent variable;sentiment analysis	Lei Fang;Minlie Huang;Xiaoyan Zhu	2013		10.1145/2505515.2505538	natural language processing;computer science;data science;machine learning;data mining;sentiment analysis	NLP	-19.615611830154858	-68.13446887025147	51211
5c3347ce547db0f94dfa786c75dddea59475d155	overview of the nlpcc 2015 shared task: open domain qa	question answering;knowledge base	In this paper, we give the overview of the open domain Question Answering or open domain QA shared task in NLPCC 2015. We first review the background of QA, and then describe open domain QA shared task in this year's NLPCC, including the construction of the benchmark datasets, the auxiliary dataset, and the evaluation metrics. The evaluation results of submissions from participating teams are presented in the experimental part, together with a brief introduction to the techniques used in each participating team's QA system.	software quality assurance	Nan Duan	2015		10.1007/978-3-319-25207-0_53	computer science;data mining;database;world wide web	NLP	-31.8537166179311	-63.93353637188611	51218
29bf10dd12444f0fbdc6cb3f539fc34ccbeb4d0f	experiments varying semantic similarity measures and reference ontologies for ontology alignment		Semantic similarity measures within a reference ontology have been used in a few ontology alignment (OA) systems. Most use a single reference ontology, typically WordNet, and a single similarity measure within it. The mediating matcher with semantic similarity (MMSS) was added to AgreementMaker to incorporate the selection of a semantic similarity measure and the combination of multiple reference ontologies in an adaptable fashion. The results of experiments using the MMSS on the anatomy track of the Ontology Alignment Evaluation Initiative (OAEI) are reported. A variety of semantic similarity measures are applied within multiple reference ontologies. Using multiple reference ontologies with the MMSS improved alignment results. All information-content based semantic similarity measures produced better alignment results than a path-based semantic similarity measure.	ontology (information science);ontology alignment;semantic similarity	Valerie V. Cross;Pramit Silwal;Xi Chen	2013		10.1007/978-3-642-41242-4_42	ontology alignment;semantic similarity;ontology-based data integration	NLP	-32.28196325670626	-65.94181683286338	51223
9dcc2f7f48b1335d53743be315e6e4919452c80b	on the vector representation of utterances in dialogue context		In recent years, the representation of words as vectors in a vector space, also known as word embeddings, has achieved a high degree of attention in the research community and the benefits of such a representation can be seen in the numerous applications that utilise it. In this work, we introduce dialogue vector models, a new language resource that represents dialogue utterances in vector space and captures the semantic meaning of those utterances in the dialogue context. We examine how the word vector approach can be applied to utterances in a dialogue to generate a meaningful representation of them in vector space. Utilising existing dialogue corpora and word vector models, we create dialogue vector models and show that they capture relevant semantic information by comparing them to manually annotated dialogue acts. Furthermore, we discuss potential areas of application for dialogue vector models, such as dialogue act annotation, learning of dialogue strategies, intent detection and paraphrasing.	text corpus;word embedding	Louisa Pragst;Niklas Rach;Wolfgang Minker;Stefan Ultes	2018			artificial intelligence;speech recognition;natural language processing;computer science	NLP	-20.64756492375245	-72.11501722974197	51242
288b3dfb4d5b3a186bf46abf2db9bc4f0a41474d	digesting commercial clips from tv streams	text categorization tv commercial multimodal analysis semantics video segmentation video classification;video streaming television applications;tv streams;tv commercial;video streaming;nova;tv advertising break;television applications;multimodal analysis;semantics;video segmentation;research repository;tv layout advertising indexing gunshot detection systems performance analysis monitoring innovation management costs video recording;university of newcastle;hidden markov models;image edge detection;commercial clips;video classification;image color analysis;feature extraction;syntactic analysis;production;tv;digesting commercial;set top box commercial clips tv streams commercial system syntactic analysis semantic analysis tv advertising break;commercial system;text categorization;set top box;institutional repository;semantic analysis;research online;harmonic analysis	A commercial system that performs syntactic and semantic analysis during a TV advertising break could facilitate innovative new applications, such as an intelligent set-top box that enhances the ability of viewers to monitor and manage commercials from TV streams.	binary classification;categorization;computable function;document classification;set-top box;sparse matrix	Ling-yu Duan;Jinqiao Wang;Yantao Zheng;Hanqing Lu;Jesse S. Jin	2008	IEEE MultiMedia	10.1109/MMUL.2008.4	feature extraction;computer science;parsing;harmonic analysis;semantics;multimedia;nova;world wide web	DB	-14.43833536050073	-55.68679028369027	51258
41a0fa22649c9d2f239fff933a6238c94c85d6a4	writing stories with help from recurrent neural networks		This thesis explores the use of a recurrent neural network model for a novel story generation task. In this task, the model analyzes an ongoing story and generates a sentence that continues the story.	neural networks;recurrent neural network	Melissa Roemmele	2016			computer science;artificial intelligence	AI	-14.93359856791448	-73.56428197606694	51293
9a55a63ce8fc4d9723db92f2f27bcd900e93d1da	a simple and effective unsupervised word segmentation approach	null	In this paper, we propose a new unsupervised approach for word segmentation. The core idea of our approach is a novel word induction criterion called WordRank, which estimates the goodness of word hypotheses (character or phoneme sequences). We devise a method to derive exterior word boundary information from the link structures of adjacent word hypotheses and incorporate interior word boundary information to complete the model. In light of WordRank, word segmentation can be modeled as an optimization problem. A Viterbistyled algorithm is developed for the search of the optimal segmentation. Extensive experiments conducted on phonetic transcripts as well as standard Chinese and Japanese data sets demonstrate the effectiveness of our approach. On the standard Brent version of BernsteinRatner corpora, our approach outperforms the state-ofthe-art Bayesian models by more than 3%. Plus, our approach is simpler and more efficient than the Bayesian methods. Consequently, our approach is more suitable for real-world applications.	bayesian network;experiment;mathematical induction;mathematical optimization;microsoft word for mac;optimization problem;text corpus;text segmentation;unsupervised learning;viterbi algorithm	Songjian Chen;Yabo Xu;HuiYou Chang	2011			natural language processing;speech recognition;machine learning;pattern recognition	NLP	-20.495597983428375	-77.19885365632852	51336
9fd8bbdd4aa2ebb9c1e85b0d418b83f1af5d10c8	a proxy server supporting document categorization			categorization;document classification;proxy server	Cheng-Kang Wen;Shang-Rong Tsai;Feng-Shiuh Song	1999			world wide web;computer science;database;categorization;proxy server	Web+IR	-24.711648862463218	-61.37332789831742	51398
f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97	recurrent neural network regularization		We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, and machine translation.	artificial neural network;dropout (neural networks);language model;long short-term memory;machine translation;matrix regularization;neural networks;overfitting;recurrent neural network;speech recognition	Wojciech Zaremba;Ilya Sutskever;Oriol Vinyals	2014	CoRR		computer science;artificial intelligence;machine learning;pattern recognition	NLP	-17.514516093579676	-75.63838983851446	51440
ab5ada120821c9bee09c281d7c8154be69d0c70b	a markovian approach to distributional semantics with application to semantic compositionality	hidden markov model;distributional semantics;word representation;word embedding	In this article, we describe a new approach to distributional semantics. This approach relies on a generative model of sentences with latent variables, which takes the syntax into account by using syntactic dependency trees. Words are then represented as posterior distributions over those latent classes, and the model allows to naturally obtain in-context and out-of-context word representations, which are comparable. We train our model on a large corpus and demonstrate the compositionality capabilities of our approach on different datasets.	consistency model;dependency grammar;distributional semantics;expectation–maximization algorithm;generative model;latent dirichlet allocation;latent semantic analysis;latent variable;spectral method;statistical model;text corpus;topic model;web page	Edouard Grave;Guillaume Obozinski;Francis R. Bach	2014			natural language processing;statistical semantics;componential analysis;computer science;pattern recognition;linguistics;hidden markov model	NLP	-19.952816052810316	-76.47010805950026	51454
567882b269ae1202e9f5ce4e1e6d2f1d62355509	ranking optimization with constraints	ranking optimization;post ranking;bradley terry model	This paper addresses the problem of post-processing of ranking in search, referred to as post ranking. Although important, no research seems to have been conducted on the problem, particularly with a principled approach, and in practice ad-hoc ways of performing the task are being adopted. This paper formalizes the problem as constrained optimization in which the constraints represent the post-processing rules and the objective function represents the trade-off between adherence to the original ranking and satisfaction of the rules. The optimization amounts to refining the original ranking result based on the rules. We further propose a specific probabilistic implementation of the general formalization on the basis of the Bradley-Terry model, which is theoretically sound, effective, and efficient. Our experimental results, using benchmark datasets and enterprise search dataset, show that the proposed method works much better than several baseline methods of utilizing rules.	baseline (configuration management);benchmark (computing);bradley–terry model;constrained optimization;hoc (programming language);loss function;mathematical optimization;optimization problem;video post-processing	Fangzhao Wu;Jun Xu;Hang Li;Xin Jiang	2014		10.1145/2661829.2661895	ranking;computer science;artificial intelligence;data science;machine learning;data mining;database;ranking svm;information retrieval	Web+IR	-26.796541562438758	-61.141758529195975	51486
be3917d9642d7e67d6ea2a1433dab5099e83cd8b	improving asr processing of ungrammatical utterances through grammatical error modeling	article in monograph or in proceedings	Automatic speech recognition (ASR) of non-native utterances with grammatical errors is problematic. A new method which makes it possible to better recognize such utterances is presented in the current paper. It can be briefly summarized as follows: extract error patterns automatically from a learner corpus, formulate rewrite rules for these syntactic and morphological errors, build finite state grammars (FSGs), and use these FSGs as language models in ASR systems. All rules used in isolation and in different combinations yield lower word error rates (WERs).	automated system recovery;context-free grammar;language model;microsoft word for mac;rewrite (programming);rewriting;speech recognition	Helmer Strik;Joost van Doremalen;Janneke van de Loo;Catia Cucchiarini	2011			natural language processing;speech recognition;art;history;computer science;linguistics;physics	NLP	-27.95890822337344	-78.80965287309044	51520
2c07ef64464d618efdd050d48d521c1635258364	a machine learning approach to portuguese clause identification	transformative learning;brazilian portuguese;machine learning;part of speech	In this work, we apply and evaluate a machine-learningbased system to Portuguese clause identification. To the best of our knowledge, this is the first machine-learning-based approach to this task. The proposed system is based on Entropy Guided Transformation Learning. In order to train and evaluate the proposed system, we derive a clause annotated corpus from the Bosque corpus of the Floresta Sintá(c)tica Project – an European and Brazilian Portuguese treebank. We include part-of-speech (POS) tags to the derived corpus by using an automatic state-of-the-art tagger. Additionally, we use a simple heuristic to derive a phrase-chunk-like (PCL) feature from phrases in the Bosque corpus. We train an extractor to this sub-task and use it to automatically include the PCL feature in the derived clause corpus. We use POS and PCL tags as input features in the proposed clause identifier. This system achieves a Fβ=1 of 73.90, when using the golden values of the PCL feature. When the automatic values are used, the system obtains Fβ=1 = 69.31. These are promising results for a first machine learning approach to Portuguese clause identification. Moreover, these results are achieved using a very simple PCL feature, which is generated by a PCL extractor developed with very little modeling effort.	brill tagger;brown corpus;heuristic;identifier;machine learning;part-of-speech tagging;randomness extractor;treebank	Eraldo Rezende Fernandes;Cícero Nogueira dos Santos;Ruy Luiz Milidiú	2010		10.1007/978-3-642-12320-7_8	natural language processing;speech recognition;engineering;linguistics	NLP	-23.669306585583545	-71.6523842223938	51542
58466af48fd206e2cb4fb031c2d881980f5cdbaf	jointly modeling embedding and translation to bridge video and language	video signal processing computer vision language translation natural language processing;svo triplets bridge video embedding language translation automatic video content description natural language computer vision recurrent neural networks rnn visual interpretation sentence semantics visual content long short term memory with visual semantic embedding lstm e lstm learning word generation probability maximization visual semantic embedding space youtube2text dataset natural sentence generation bleu 4 meteor movie description datasets m vad mpii md subject verb object triplet prediction;semantics visualization coherence neural networks feature extraction loss measurement computer vision	Automatically describing video content with natural language is a fundamental challenge of computer vision. Re-current Neural Networks (RNNs), which models sequence dynamics, has attracted increasing attention on visual interpretation. However, most existing approaches generate a word locally with the given previous words and the visual content, while the relationship between sentence semantics and visual content is not holistically exploited. As a result, the generated sentences may be contextually correct but the semantics (e.g., subjects, verbs or objects) are not true. This paper presents a novel unified framework, named Long Short-Term Memory with visual-semantic Embedding (LSTM-E), which can simultaneously explore the learning of LSTM and visual-semantic embedding. The former aims to locally maximize the probability of generating the next word given previous words and visual content, while the latter is to create a visual-semantic embedding space for enforcing the relationship between the semantics of the entire sentence and visual content. The experiments on YouTube2Text dataset show that our proposed LSTM-E achieves to-date the best published performance in generating natural sentences: 45.3% and 31.0% in terms of BLEU@4 and METEOR, respectively. Superior performances are also reported on two movie description datasets (M-VAD and MPII-MD). In addition, we demonstrate that LSTM-E outperforms several state-of-the-art techniques in predicting Subject-Verb-Object (SVO) triplets.	artificial neural network;computer vision;digital video;experiment;exploit (computer security);holism;long short-term memory;meteor;natural language;performance;sparse voxel octree;unified framework;voice activity detection	Yingwei Pan;Tao Mei;Ting Yao;Houqiang Li;Yong Rui	2016	2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2016.497	natural language processing;computer vision;speech recognition;computer science;machine learning;pattern recognition	Vision	-14.753137898929218	-69.8704544637723	51572
f4169f6ddae2af04d13859ad8de434890d82dc77	chinese discourse segmentation using bilingual discourse commonality		Discourse segmentation aims to segment Elementary Discourse Units (EDUs) and is a fundamental task in discourse analysis. For Chinese, previous researches identify EDUs just through discriminating the functions of punctuations. In this paper, we argue that Chinese EDUs may not end at the punctuation positions and should follow the definition of EDU in RST-DT. With this definition, we conduct Chinese discourse segmentation with the help of English labeled data. Using discourse commonality between English and Chinese, we design an adversarial neural network framework to extract common language-independent features and language-specific features which are useful for discourse segmentation, when there is no or only a small scale of Chinese labeled data available. Experiments on discourse segmentation demonstrate that our models can leverage common features from bilingual data, and learn efficient Chinese-specific features from a small amount of Chinese labeled data, outperforming the baseline models.	artificial neural network;baseline (configuration management);chinese room;experiment;intel matrix raid;language-independent specification;memory segmentation	Jingfeng Yang;Sujian Li	2018	CoRR		labeled data;discourse analysis;natural language processing;artificial neural network;artificial intelligence;punctuation;computer science;segmentation	NLP	-20.940572825023363	-73.80410286155721	51611
68b8173c251f4b4fd8ae9da5f6750ca8cc4edeb4	classify sina weibo users into high or low happiness groups using linguistic and behavior features		It’s of great importance to measure happiness of social network users, but the existing method based on questionnaires suffers from high costs and low efficiency. This paper aims at identifying social network users’ happiness level based on their Web behav ior. We recruited 548 participants to fill in the Oxford Happiness Inventory (OHI) and divided them into two groups with high/low OHI score. We downloaded each Weibo user’s data by calling API, and extracted 103 linguistic and behavior features. 24 features are identified with significant difference between high and low happiness groups. We trained a Decision Tree on these 24 features to make the prediction of high/low happiness group. The decision tree can be used to identify happiness level of any new social network user based on linguistic and behavior features. The Decision Tree can achieve 67.7% on precision. Although the capability of our Decision Tree is not ideal, classifying happiness via linguistic and behavior features on the Internet is proved to be feasible. Keyword: happiness; social network sites; linguistic feature; behavior feature	application programming interface;decision tree;interaction;kerrison predictor;lexicon;money;perceptron;population;sampling (signal processing);social media;social network	Jingying Wang;Tianli Liu;Tingshao Zhu;Bibo Hao;Zhenxiang Chen	2015	CoRR		data mining;social psychology	Web+IR	-19.78234883373487	-52.56818086557211	51677
6f0b32b1a1062c77e2fb2d7895030ffdd19f4011	a hybrid method for chinese entity relation extraction		Entity relation extraction is an important task for information extraction, which refers to extracting the relation between two entities from input text. Previous researches usually converted this problem to a sequence labeling problem and used statistical models such as conditional random field model to solve it. This kind of method needs a large, high-quality training dataset. So it has two main drawbacks: 1) for some target relations, it is not difficult to get training instances, but the quality is poor; 2) for some other relations, it is hardly to get enough training data automatically. In this paper, we propose a hybrid method to overcome the shortcomings. To solve the first drawback, we design an improved candidate sentences selecting method which can find out high-quality training instances, and then use them to train our extracting model. To solve the second drawback, we produce heuristic rules to extract entity relations. In the experiment, the candidate sentences selecting method improves the average F1 value by 78.53% and some detailed suggestions are given. And we submitted 364944 triples with the precision rate of 46.3% for the competition of Sougou Chinese entity relation extraction and rank the 4th place in the platform.	chinese wall;conditional random field;entity;heuristic;information extraction;knowledge base;relationship extraction;sequence labeling;statistical model	Hao Wang;Zhenyu Qi;Hongwei Hao;Bo Xu	2014		10.1007/978-3-662-45924-9_32	computer science;artificial intelligence;data mining;algorithm	NLP	-26.3142279130176	-66.71562840996413	51691
2c3abf6b210f29ae996b0a0d5ce5f712c5bd2da2	assembling personal speech collections by monologue scene detection from a news video archive	dynamic model;dynamic speech modeling;prior knowledge;closed caption text;personal namannotation;face detection	Monologue scenes in news shows are important since they contain non-verbal information that could not be expressed through text media. In this paper, we propose a method that detects monologue scenes by individuals in news shows (news subjects) without external or prior knowledge on the show. The method first detects monologue scene candidates by face detection in the frame images, and then excludes scenes overlapped with speech by anchor-persons or reporters (news persons) by dynamically modeling them according to clues obtained from the closed-caption text and from the audio stream. As an application of monologue scene detection, we also propose a method which assembles personal speech collections per individual that appear in the news. Although the methods still need further improvement for realistic use, we confirmed the effectiveness of employing multimodal information for the tasks, and also saw interesting outputs from the automatically assembled speech collections.	archive;face detection;multimodal interaction;streaming media	Ichiro Ide;Naoki Sekioka;Tomokazu Takahashi;Hiroshi Murase	2006		10.1145/1178677.1178708	computer vision;face detection;speech recognition;computer science;multimedia;world wide web	NLP	-17.428267627974538	-56.13972695621513	51719
c3e4ab04f548cd4bfc4c941705d9e73b3c4492de	constituent parsing as sequence labeling		We introduce a method to reduce constituent parsing to sequence labeling. For each word wt, it generates a label that encodes: (1) the number of ancestors in the tree that the words wt andwt+1 have in common, and (2) the nonterminal symbol at the lowest common ancestor. We first prove that the proposed encoding function is injective for any tree without unary branches. In practice, the approach is made extensible to all constituency trees by collapsing unary branches. We then use the PTB and CTB treebanks as testbeds and propose a set of fast baselines. We achieve 90% F-score on the PTB test set, outperforming the Vinyals et al. (2015) sequence-to-sequence parser. In addition, sacrificing some accuracy, our approach achieves the fastest constituent parsing speeds reported to date on PTB by a wide margin.	baseline (configuration management);coding tree unit;f1 score;fastest;lowest common ancestor;parsing expression grammar;programming paradigm;sequence labeling;terminal and nonterminal symbols;test set;treebank;unary operation	Carlos Gómez-Rodríguez;David Vilares	2018			machine learning;sequence labeling;artificial intelligence;terminal and nonterminal symbols;parsing;computer science;injective function;symbol;unary operation;pattern recognition;test set;lowest common ancestor	NLP	-21.300019790484345	-76.26514845781585	51776
be04e90482e406a4547b145dc48fcc5d19f23aaa	an effective trust-based search approach in peer-to-peer network	p2p system;peer to peer network;search mechansim;p2p;platform;computer networks;network servers;computational modeling;querycycle platform trust based search approach peer to peer network peer to peer platform p2p platform recommendation p2p system forwarding messages downloading resources search quality;machine vision;displays;recommendation value peer to peer search mechansim platform;system testing;computer science;peer to peer computing;response rate;peer to peer;man machine systems;recommender systems;security of data;peer to peer computing network servers machine vision man machine systems educational institutions computer science computer networks computational modeling system testing displays;recommendation value;security of data peer to peer computing recommender systems	In this paper, a novel optimized trust-based search approach was proposed for the peer-to-peer (P2P) platform. In traditional trust-based or recommendation P2P system, the trust or recommendation value of a peer is considered when selecting it for forwarding messages or downloading resources, while the other factors are ignored, such as good download rate, good response rate, and on-line time similarity. In the proposed optimized approach, we consider all possible factors which affect the search quality in the P2P system. The proposed algorithm is developed on the well-known QueryCycle platform. Experiment results show the efficiency of our approach.	algorithm;download;online and offline;peer-to-peer;whole earth 'lectronic link	Junqing Li;Shengxian Xie;Quan-Ke Pan;Kai-Zhou Gao;Baoxian Jia;Yuting Wang	2010	2010 International Conference on Machine Vision and Human-machine Interface	10.1109/MVHI.2010.133	computer vision;machine vision;computer science;response rate;peer-to-peer;multimedia;internet privacy;platform;computational model;system testing;world wide web	AI	-28.25781867274078	-52.163891509442756	51792
dbaca186b2712d82472ba04d8f7dd80acf8e0fc6	extracting food names from food reviews	manuals;animals;semantics;dairy products;gold;internet;shape	Chinese food names are important language resources, which can be used in analysis of food reviews. Since the naming of Chinese food names are quite flexible and food reviews are typical spoken language, it is not easy to construct a general list for them. In this paper, we propose an approach to extracting Chinese food names from a large unlabeled Chinese corpus. At first, we construct character-level clues for foods, and then we select word candidates which could be a part of a food name, and perform manual annotation on them. Based on the annotation result, we used heuristic rules to extract food names from food reviews. Experiments are performed to evaluate our approach.	gnome;heuristic;hill climbing;raw image format;text corpus	Ge Xu;Likun Qiu	2015	2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)	10.1109/WI-IAT.2015.100	gold;natural language processing;the internet;shape;computer science;data mining;semantics	NLP	-30.740019962146846	-68.69918327318527	51796
a0403302f171cfdc3a89f64d50936f6fcfc1b94f	one sentence one model for neural machine translation		Neural machine translation (NMT) becomes a new state-ofthe-art and achieves promising translation results using a simple encoder-decoder neural network. This neural network is trained once on the parallel corpus and the fixed network is used to translate all the test sentences. We argue that the general fixed network cannot best fit the specific test sentences. In this paper, we propose the dynamic NMT which learns a general network as usual, and then fine-tunes the network for each test sentence. The fine-tune work is done on a small set of the bilingual training data that is obtained through similarity search according to the test sentence. Extensive experiments demonstrate that this method can significantly improve the translation performance, especially when highly similar sentences are available.	artificial neural network;curve fitting;experiment;neural machine translation;parallel text;priority encoder;similarity search	Xiaoqing Li;Jiajun Zhang;Chengqing Zong	2018	CoRR		natural language processing;speech recognition;example-based machine translation;computer science;machine learning;rule-based machine translation	NLP	-18.588847034148504	-75.5417962781429	51817
8973c7d8a0f57dc5e4a8a54322951ca8858f32b0	abbreviation disambiguation: experiments with various variants of the one sense per discourse hypothesis	statistical method;machine learning	Abbreviations are widely used in many languages and disambiguation of abbreviations is critical. In this research, a structured process that attempts to solve the problem of abbreviation ambiguity is presented. Various baseline methods have been explored, including context-related methods and statistical methods. Almost all methods are domain-independent and language independent. The application domain is Jewish Law documents written in Hebrew, which are known to be rich in ambiguous abbreviations. Several implementations of the one sense per discourse hypothesis are used, improving the baseline methods with new variants. Several common machine learning methods have been tested to find a successful integration of the baseline method variants. The best results have been achieved by LIBSVM, with 96.09% accuracy.	a library for support vector machines;application domain;baseline (configuration management);machine learning;word-sense disambiguation	Yaakov HaCohen-Kerner;Ariel Kass;Ariel Peretz	2008		10.1007/978-3-540-69858-6_5	natural language processing;computer science;linguistics	NLP	-25.9766117430853	-71.96397738519703	51880
dcc054d846d117a7430dec9342877976d0240335	an evaluation method for segmental accommodation reviews with text mining			text mining	Koichi Tsujii;Takashi Ikoma;Kazuhiko Tsuda	2012			data mining;information retrieval;accommodation;text mining;computer science	ML	-31.717716110009036	-77.45877082044697	51887
2e227c14b7a944884cd8eb02d3c1e459693a34c2	fast very large vocabulary recognition based on compact dawg-structured language models			language model;vocabulary	Kallirroi Georgila;Kyriakos N. Sgarbas;Nikos Fakotakis;George K. Kokkinakis	2000			speech recognition;universal networking language;structured english;natural language processing;language identification;language model;computer science;artificial intelligence;vocabulary;structured prediction	NLP	-28.0438698903525	-79.69019817972735	51899
ee095a7a3732fe50e5eea537245c26034fb9bee9	chinese image character recognition using dnn and machine simulated training samples		Inspired by the success of deep neural network (DNN) models in solving challenging visual problems, this paper studies the task of Chinese Image Character Recognition (ChnICR) by leveraging DNN model and huge machine simulated training samples. To generate the samples, clean machine born Chinese characters are extracted and are plus with common variations of image characters such as changes in size, font, boldness, shift and complex backgrounds, which in total produces over 28 million character images, covering the vast majority of occurrences of Chinese character in real life images. Based on these samples, a DNN training procedure is employed to learn the appropriate Chinese character recognizer, where the width and depth of DNN, and the volume of samples are empirically discussed. Parallel to this, a holistic Chinese image text recognition system is developed. Encouraging experimental results on text from 13 TV channels demonstrate the effectiveness of the learned recognizer, from which significant performance gains are observed compared to the baseline system.		Jinfeng Bai;Zhineng Chen;Bailan Feng;Bo Xu	2014		10.1007/978-3-319-11179-7_27	speech recognition;machine learning;pattern recognition	Vision	-17.14367178337788	-78.79541459139409	51904
d73fd47c58906b115377a53ed02c0dedab16dcac	a new method to compose long unknown chinese keywords	unknown chinese word;keyword retrieval;chinese word segmentation	There is now a huge amount of electronic documents stored on the internet. In order to retrieve information from this data, each document is commonly represented as a set of keywords, and then all documents are analysed based on the set of discriminative words. In information retrieval the recognition of words in articles is an essential step; however, unlike English, Chinese words are not distinguished by spaces. Therefore, many approaches have been devised to parse Chinese words. The dictionary-based approach is commonly used in most current systems for text segmentation. However, general purpose dictionaries are not always able to provide proper references to accurately parse the domain-specific words, especially with unknown words. This paper aims to propose a new method for classifying longer keywords from Chinese documents by incorporating previously unknown keywords into a keyword list without the effort of building domain-specific dictionaries. Our method first utilizes the parsed words from existing parsers and filters the keywords utilizing term frequency-inverse document frequency (TF-IDF) values; further, based on the parsed words and keywords, a T tree is used to store the candidates for composing unknown words. The candidates are evaluated by an unknown word (UW) coefficient threshold, i.e. newly composed words are deemed as newly discovered unknown words if their UW coefficient is higher than a pre-defined threshold. Finally, the parsed words and newly composed words are re-filtered to form long keywords. The results of several experiments comparing the results with Google and Yahoo show that, regardless of recall rates, precision rates and F-measures, our proposed method significantly outperforms other methods.		Yu-Chin Liu;Chun-Wei Lin	2012	J. Information Science	10.1177/0165551512442481	natural language processing;speech recognition;computer science;database;world wide web;information retrieval;stop words	NLP	-29.63506837760436	-68.31400067978085	51982
e9979bf9c85127738e0b9bf0820754ac4dbd126e	user-centered evaluation for ir: ranking annotated document algorithms	human ranking;semantic web;annotation document	Since the introduction of Semantic Web, the practice of seeking and retrieving documents had been evolved. In this paper, the retrieved documents are ranked based on their annotated documents. We adopt two IR algorithms; Lucene Luke and ComFFICF. In order to verify the generated rankings, we run a user-centered evaluation, where it involved 10 human judges. Then, we assess the performance of ranking using NDCG metric. The assessment shows a ranking by ComFFICF algorithm outperforms a ranking by Lucene Luke. This method is proven to be one of preferable IR algorithms for searching and ranking annotated document.	algorithm	Syarifah Bahiyah Rahayu;Shahrul Azman Mohd. Noah;Andrianto Arfan Wardhana	2011		10.1007/978-3-642-22203-0_27	ranking;computer science;semantic web;data mining;ranking svm;world wide web;information retrieval	Web+IR	-30.106310812373433	-61.54526307003558	51994
91adb59cd127b474cee4a9a75d720b54a7848766	how similar is it? towards personalized similarity measures in ontologies	gold standard;schema matching;human subjects;machine learning;semantic web;similarity measure;clustered data	Finding a good similarity assessment algorithm for the use in ontologies is central to the functioning of techniques such as retrieval, matchmaking, clustering, datamining, ontology translations, automatic database schema matching, and simple object comparisons. This paper assembles a catalogue of ontology based similarity measures, which are experimentally compared with a “similarity gold standard” obtained by surveying 50 human subjects. Results show that human and algorithmic similarity predications varied substantially, but could be grouped into cohesive clusters. Addressing this variance we present a personalized similarity assessment procedure, which uses a machine learning component to predict a subject’s cluster membership, providing an excellent prediction of the gold standard. We conclude by hypothesizing ontology dependent similarity measures.	algorithm;cluster analysis;data mining;database schema;experiment;handbook;machine learning;matchmaking (video games);natural language processing;ontology (information science);personalization;word-sense disambiguation	Abraham Bernstein;Esther Kaufmann;Christoph Buerki;Mark Klein	2005		10.1007/3-7908-1624-8_71	semantic similarity;gold standard;computer science;machine learning;semantic web;data mining;database;world wide web;information retrieval;similarity heuristic	Web+IR	-32.93454263912134	-61.61262785546233	52031
098a780c8a9384d3bb237df3b7444026fe0e9508	improving discourse relation projection to build discourse annotated corpora		The naive approach to annotation projection is not effective to project discourse annotations from one language to another because implicit discourse relations are often changed to explicit ones and vice-versa in the translation. In this paper, we propose a novel approach based on the intersection between statistical word-alignment models to identify unsupported discourse annotations. This approach identified 65% of the unsupported annotations in the English-French parallel sentences from Europarl. By filtering out these unsupported annotations, we induced the first PDTB-style discourse annotated corpus for French from Europarl. We then used this corpus to train a classifier to identify the discourse-usage of French discourse connectives and show a 15% improvement of F1-score compared to the classifier trained on the non-filtered annotations.	daisy digital talking book;data structure alignment;discourse relation;f1 score;logical connective;parallel text;parsing;text corpus	Majid Laali;Leila Kosseim	2017		10.26615/978-954-452-049-6_054	artificial intelligence;natural language processing;discourse relation;computer science;classifier (linguistics);annotation	NLP	-25.04548173807666	-73.91972963792549	52094
54ebd80096a7ec7d8be3c14fd448896214717610	cswhmm: a novel context switching hidden markov model for biological sequence analysis	data mining;hidden markov models;bioinformatics	In this work we created a sequence model that goes beyond simple linear patterns to model a specific type of higher-order relationship possible in biological sequences. Particularly, we seek models that can account for partially overlaid and interleaved patterns in biological sequences. Our proposed context-switching model (cswHMM) is designed as a variable-order hidden Markov model (HMM) with a specific structure that allows switching control between two or more sub-models. An important feature of our model is the ability of its sub-models to store their last active state, so when each sub-model resumes control it can continue uninterrupted. This is a fundamental variation on the closely related jumping HMMs. A combination of as few as two simple linear HMMs can describe sequences with complicated mixed dependencies. Tests of this approach suggest that a combination of HMMs for protein sequence analysis, such as pattern mining based HMMs or profile HMMs, with the context-switching approach can improve the descriptive ability and performance of the models.	algorithm;context switch;data mining;hidden markov model;international conference on bioinformatics;markov chain;mined;sequence analysis;unsupervised learning	Vojtech Bystrý;Matej Lexa	2012			maximum-entropy markov model;computer science;bioinformatics;machine learning;hidden semi-markov model;pattern recognition;data mining;markov model;hidden markov model;variable-order markov model	ML	-5.994787255126318	-66.31004367340583	52164
cfeda4a133ecaed5c44e106770da3d326301d3f1	supervised learning for building stemmers	text analysis and indexing;stemming algorithms;information storage and retrieval;stemmer builder	This work is part of a project aiming to define a methodology for building simple but robust stemmers, having primitive knowledge of the stemmer's target language. The methodology starts with a very simple primary stemmer that simply removes the longest suffix using the primitive knowledge - the list of available suffixes that matches the ending of the examined word. Information retrieval IR experts express their arguments against the results of the primary stemmer. These the experts' arguments are valuable knowledge that offer us the ability to apply supervised learning in order to automatically produce better stemmers that conform to the arguments expressed by the IR experts. We also conduct an evaluation of our supervised learning-based methodology that builds stemmers for languages that the experts do not have knowledge on.	supervised learning	Nikitas N. Karanikolas	2015	J. Information Science	10.1177/0165551515572528	computer science;artificial intelligence;machine learning;data mining;database;world wide web;information retrieval	NLP	-27.924354750219177	-70.96897596142992	52185
538c0d22739ea9f61bcb062573f4753856411613	a proposal for social search system design	libraries;eigenvalues and eigenfunctions;social search system design;parallel computing;parallel computing library;search engine;slepc;eigenvalue problem;social communication;probability;telecommunication links;web pages;history;query processing;search engines;probability transition matrix social search system design social links social communication web page url search engine query processing linked page page ranking algorithm hyperlink structure social network structure real time user activity parallel computing library open mpi slepc eigenvector calculation eigenvalue calculation;real time systems web pages eigenvalues and eigenfunctions program processors symmetric matrices libraries history;page ranking algorithm;matrix algebra;symmetric matrices;linked page;eigenvalue calculation;system design;application program interfaces;social networking online;social network structure;parallel computer;message passing;open mpi;eigenvector calculation;probability transition matrix;web page;real time user activity;social search;social links;telecommunication links application program interfaces eigenvalues and eigenfunctions matrix algebra message passing parallel processing probability query processing search engines social networking online;program processors;parallel computing social search eigenvalue problem;parallel processing;hyperlink structure;url;real time systems	"""We have developed a search method that uses both hyperlinks and social links and thus combines the merits of searching and social communication. A user can quickly search for not only web pages but also for other users currently accessing those pages (""""real-time users""""). Each URL on the search engine results page is annotated with the number of real-time users. On each page linked from the search engine results page, each hyperlink is annotated with the number of real-time users. By using these links, a user can communicate with other real-time users who may have more knowledge about the topic of the linked page. All linked pages provide a window for sending queries in real time and for communicating with other real-time users. The window also shows a log of previous communications through that page, enabling a user to see if a similar query was previously sent and answered, which would obviate the need to send it again. Furthermore, users can highlight text on a linked page, enabling other users to quickly find important information as the system can automatically scroll down and present the highlighted text after the linked page is accessed. We have also developed a page ranking algorithm based on a hyperlink structure and a social network structure. The social network structure reflects the """"quality"""" and number of real-time users. A user can thus access the most popular web pages and experts in real time. In our ranking algorithm, how to timely reflect real-time user activities becomes a key point for our system validity. In this paper, we especially focus on it. As a first phase, in order to shorten the calculation time of our ranking method, we investigated SLEPc, a parallel computing library based on Open MPI and PETSc. We evaluated three calculation methods, Lanzcos, Arnoldi and Krylov-Schur, implemented in SLEPC. As a result, we confirmed that we can complete the eigenvalue and the eigenvector calculation of a probability transition matrix with hundreds of thousands rows and columns in dozens of seconds. Based on the investigation of this paper, we will implement our ranking algorithm on the prototype system."""	arnoldi iteration;column (database);hyperlink;knowledge acquisition;krylov subspace;linked data;linked list;open mpi;petsc;pagerank;parallel computing;prototype;real-time clock;real-time computing;real-time transcription;slepc;search algorithm;search engine results page;social network;social search;stochastic matrix;systems design;web page	Toyokazu Akiyama;Yukiko Kawai;Yuya Matsui;Yoshinori Kubota;Takuya Osaki	2011	2011 IEEE/IPSJ International Symposium on Applications and the Internet	10.1109/SAINT.2011.24	parallel processing;page view;computer science;theoretical computer science;operating system;web page;database;distributed computing;world wide web;search engine	Networks	-31.791231649421025	-54.88094594046085	52273
6574bc49e855efde3ec3fccaeb1dfed8e3aa7c93	untangling compound documents on the web	web documents;information retrieval;text analysis;large scale;semantic web;world wide web;composites;hypertext;wasted space	"""Most text analysis is designed to deal with the concept of a """"document"""", namely a cohesive presentation of thought on a unifying subject. By contrast, individual nodes on the World Wide Web tend to have a much smaller granularity than text documents. We claim that the notions of """"document"""" and """"web node"""" are not synonymous, and that authors often tend to deploy documents as collections of URLs, which we call """"compound documents"""". In this paper we present new techniques for identifying and working with such compound documents, and the results of some large-scale studies on such web documents. The primary motivation for this work stems from the fact that information retrieval techniques are better suited to working on documents than individual hypertext nodes."""	expect;heuristic (computer science);hyperlink;hypertext;information retrieval;semantic web;text mining;web page;world wide web	Nadav Eiron;Kevin S. McCurley	2003		10.1145/900051.900070	web service;text mining;web development;web modeling;hypertext;html;web standards;computer science;semantic web;web navigation;web page;data mining;semantic web stack;topic model;web intelligence;web 2.0;world wide web;information retrieval;web server	Web+IR	-29.020927915039493	-56.63255766295714	52302
27159d587ed4a6ba26089d78639cec3baea614f6	recognizing irregular entities in biomedical text via deep neural networks		Abstract Named entity recognition (NER) is an important task for biomedical text mining. Most prior work focused on recognizing regular entities that consist of continuous word sequences and are not overlapped with each other. In this paper, we propose a neural network model called Bi-LSTM-CRF that consists of bidirectional (Bi) long short-term memories (LSTMs) and conditional random fields (CRFs) to identify regular entities and the components of irregular entities. Then the components are combined to build final irregular entities according to manually designed rules. Furthermore, we propose a novel model called NerOne that consists of the Bi-LSTM-CRF network and another Bi-LSTM network. The Bi-LSTM-CRF network performs the same task as the aforementioned model, and the Bi-LSTM network determines whether two components should be combined. Therefore, NerOne automatically combines the components instead of using manually designed rules. We evaluate our models on two datasets for recognizing regular and irregular biomedical entities. Experimental results show that, with less feature engineering, the performances of our models are comparable with those of state-of-the-art systems. We show that the method of automatically combining the components is as effective as the method of manually designing rules. Our work can facilitate the research on biomedical text mining.		Fei Li;Meishan Zhang;Bo Tian;Bo Chen;Guohong Fu;Dong-Hong Ji	2017	Pattern Recognition Letters	10.1016/j.patrec.2017.06.009	pattern recognition;crfs;artificial neural network;biomedical text mining;machine learning;feature engineering;conditional random field;named-entity recognition;computer science;artificial intelligence	Vision	-18.622143711844824	-71.6496861974351	52307
154c3f0e86d349a3598925c3389332741850f67a	exploiting the human computational effort dedicated to message reply formatting for training discursive email segmenters	text segmentation	In the context of multi-domain and multimodal online asynchronous discussion analysis, we propose an innovative strategy for manual annotation of dialog act (DA) segments. The process aims at supporting the analysis of messages in terms of DA. Our objective is to train a sequence labelling system to detect the segment boundaries. The originality of the proposed approach is to avoid manually annotating the training data and instead exploit the human computational efforts dedicated to message reply formatting when the writer replies to a message by inserting his response just after the quoted text appropriate to his intervention. We describe the approach, propose a new electronic mail corpus and report the evaluation of segmentation models we built.	computation;email;multimodal interaction;dialog	Nicolas Hernandez;Soufian Salim	2014			computer science;data mining;multimedia;world wide web	NLP	-23.48109182931451	-73.78790678491181	52334
66fcb7f62aac9b75aa8046de8475527a024d4528	dtatg: an automatic title generator based on dependency trees		We study automatic title generation for a given block of text and present a method called DTATG to generate titles. DTATG first extracts a small number of central sentences that convey the main meanings of the text and are in a suitable structure for conversion into a title. DTATG then constructs a dependency tree for each of these sentences and removes certain branches using a Dependency Tree Compression Model we devise. We also devise a title test to determine if a sentence can be used as a title. If a trimmed sentence passes the title test, then it becomes a title candidate. DTATG selects the title candidate with the highest ranking score as the final title. Our experiments showed that DTATG can generate adequate titles. We also showed that DTATG-generated titles have higher F1 scores than those generated by the previous methods.	experiment;jing;lu decomposition;unsupervised learning	Liqun Shao;Jie Wang	2016		10.5220/0006035101660173	programming language	NLP	-25.944264317353632	-66.10507564934123	52362
906f4fd412b2f854a346465014d8d95e9c2108ee	comparing fifty natural languages and twelve genetic languages using word embedding language divergence (weld) as a quantitative measure of language distance		We introduce a new measure of distance between languages based on word embedding, called word embedding language divergence (WELD). WELD is defined as divergence between unified similarity distribution of words between languages. Using such a measure, we perform language comparison for fifty natural languages and twelve genetic languages. Our natural language dataset is a collection of sentence-aligned parallel corpora from bible translations for fifty languages spanning a variety of language families. Although we use parallel corpora, which guarantees having the same content in all languages, interestingly in many cases languages within the same family cluster together. In addition to natural languages, we perform language comparison for the coding regions in the genomes of 12 different organisms (4 plants, 6 animals, and two human subjects). Our result confirms a significant high-level difference in the genetic language model of humans/animals versus plants. The proposed method is a step toward defining a quantitative measure of similarity between languages, with applications in languages classification, genre identification, dialect identification, and evaluation of translations.	file spanning;high- and low-level;language model;natural language;parallel text;text corpus;vergence;word embedding	Ehsaneddin Asgari;Mohammad R. K. Mofrad	2016	CoRR		natural language processing;computer science;syntax;linguistics;programming language;comparison of multi-paradigm programming languages;algorithm	NLP	-26.127870129660355	-74.9569443104315	52445
